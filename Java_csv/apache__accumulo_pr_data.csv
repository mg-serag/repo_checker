metadata
"{""instance_id"": ""apache__accumulo-5670"", ""pr_id"": 5670, ""issue_id"": 5660, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Avoid creating a conditional writer per compaction commit\n**Is your feature request related to a problem? Please describe.**\n\nMain has a similar issue to #5543 except that instead of creating a batch writer it will create a conditional writer [here](https://github.com/apache/accumulo/blob/439ea97c7b12487c62f04c57eb8cc362e8bcf5a7/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java#L794) when committing an external compaction.  This conditional writer will write to the fate table and it will creates threads, so this could lead to the same problem that #5543 solved which is creating an excessive number of threads when completing lots of small external compactions.\n\n**Describe the solution you'd like**\n\nThe solution to #5543 introduced a new SharedBatchWriter class.  This was done because the batch writer is designed to be used by a single thread writing lots of data.  The ConditionalWriter is designed to support many threads using a single ConditionalWriter object to write lots of data.  So do not need to create a shared conditional writer, multiple threads can directly use a shared conditinal writer  The user fate store could be modified to use a single conditional writer instance for all of its writes.  This may be generally beneficial outside of the compaction commit use case.  A single shared conditional writer for fate operations would allow RPC batching of conditional mutations for all threads in the manger doing fate table updates at around the same time.\n\nHopefully the following could be done to avoid creating lots of conditional writer threads in the manager.\n\n * Create a conditional writer in the UserFateStore constructor.  Add a new property to configure how many threads this conditional writer has.\n * Chang the places in FateMutatorImpl and UserFateStore that call AccumuloClient.createConditionalWriter to use the conditional writer created in the construcor.\n * When fate is closed, close the conditional writer.\n"", ""issue_word_count"": 305, ""test_files_count"": 21, ""non_test_files_count"": 6, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/Admin.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java""], ""base_commit"": ""22c3e23695d3c4c6c8b8812b69f877df161c8615"", ""head_commit"": ""795c88b529e92d9fb9195248b33ba48351b87997"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5670"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5670"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-24T20:35:46.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 798afcc97c8..b3c6f7d1eab 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -442,6 +442,10 @@ public enum Property {\n   MANAGER_WAL_CLOSER_IMPLEMENTATION(\""manager.wal.closer.implementation\"",\n       \""org.apache.accumulo.server.manager.recovery.HadoopLogCloser\"", PropertyType.CLASSNAME,\n       \""A class that implements a mechanism to steal write access to a write-ahead log.\"", \""2.1.0\""),\n+  MANAGER_FATE_CONDITIONAL_WRITER_THREADS_MAX(\""manager.fate.conditional.writer.threads.max\"", \""3\"",\n+      PropertyType.COUNT,\n+      \""Maximum number of threads to use for writing data to tablet servers of the FATE system table.\"",\n+      \""4.0.0\""),\n   MANAGER_FATE_METRICS_MIN_UPDATE_INTERVAL(\""manager.fate.metrics.min.update.interval\"", \""60s\"",\n       PropertyType.TIMEDURATION, \""Limit calls from metric sinks to zookeeper to update interval.\"",\n       \""1.9.3\""),\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 560c7da0b2b..6a577333a1c 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -532,6 +532,9 @@ public void shutdown(long timeout, TimeUnit timeUnit) {\n     if (deadResCleanerExecutor != null) {\n       deadResCleanerExecutor.shutdownNow();\n     }\n+\n+    // ensure store resources are cleaned up\n+    store.close();\n   }\n \n   private boolean anyFateExecutorIsAlive() {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\nindex 264198cf93a..4910e1e757a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n@@ -24,6 +24,7 @@\n import static org.apache.accumulo.core.fate.user.UserFateStore.invertRepo;\n \n import java.util.Objects;\n+import java.util.function.Supplier;\n \n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n@@ -58,14 +59,17 @@ public class FateMutatorImpl<T> implements FateMutator<T> {\n   private final String tableName;\n   private final FateId fateId;\n   private final ConditionalMutation mutation;\n+  private final Supplier<ConditionalWriter> writer;\n   private boolean requiredUnreserved = false;\n   public static final int INITIAL_ITERATOR_PRIO = 1000000;\n \n-  public FateMutatorImpl(ClientContext context, String tableName, FateId fateId) {\n+  public FateMutatorImpl(ClientContext context, String tableName, FateId fateId,\n+      Supplier<ConditionalWriter> writer) {\n     this.context = Objects.requireNonNull(context);\n     this.tableName = Objects.requireNonNull(tableName);\n-    this.fateId = fateId;\n+    this.fateId = Objects.requireNonNull(fateId);\n     this.mutation = new ConditionalMutation(new Text(getRowId(fateId)));\n+    this.writer = Objects.requireNonNull(writer);\n   }\n \n   @Override\n@@ -237,8 +241,8 @@ public Status tryMutate() {\n \n         return Status.ACCEPTED;\n       } else {\n-        try (ConditionalWriter writer = context.createConditionalWriter(tableName)) {\n-          ConditionalWriter.Result result = writer.write(mutation);\n+        try {\n+          ConditionalWriter.Result result = writer.get().write(mutation);\n \n           switch (result.getStatus()) {\n             case ACCEPTED:\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\nindex 29ba43fa0a1..331401fc6bb 100644\n--- a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n+++ b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n@@ -193,6 +193,11 @@ public Map<FateId,FateReservation> getActiveReservations() {\n       public void deleteDeadReservations() {\n         store.deleteDeadReservations();\n       }\n+\n+      @Override\n+      public void close() {\n+        store.close();\n+      }\n     };\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\nindex d5bb3f4261d..0d7d30d350a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n@@ -959,7 +959,6 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n     var zTableLocksPath = context.getServerPaths().createTableLocksPath();\n     var zk = context.getZooSession();\n     ServiceLock adminLock = null;\n-    Map<FateInstanceType,FateStore<Admin>> fateStores;\n     Map<FateInstanceType,ReadOnlyFateStore<Admin>> readOnlyFateStores = null;\n \n     try {\n@@ -967,20 +966,22 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n         cancelSubmittedFateTxs(context, fateOpsCommand.fateIdList);\n       } else if (fateOpsCommand.fail) {\n         adminLock = createAdminLock(context);\n-        fateStores = createFateStores(context, zk, adminLock);\n-        for (String fateIdStr : fateOpsCommand.fateIdList) {\n-          if (!admin.prepFail(fateStores, fateIdStr)) {\n-            throw new AccumuloException(\""Could not fail transaction: \"" + fateIdStr);\n+        try (var fateStores = createFateStores(context, zk, adminLock)) {\n+          for (String fateIdStr : fateOpsCommand.fateIdList) {\n+            if (!admin.prepFail(fateStores.getStoresMap(), fateIdStr)) {\n+              throw new AccumuloException(\""Could not fail transaction: \"" + fateIdStr);\n+            }\n           }\n         }\n       } else if (fateOpsCommand.delete) {\n         adminLock = createAdminLock(context);\n-        fateStores = createFateStores(context, zk, adminLock);\n-        for (String fateIdStr : fateOpsCommand.fateIdList) {\n-          if (!admin.prepDelete(fateStores, fateIdStr)) {\n-            throw new AccumuloException(\""Could not delete transaction: \"" + fateIdStr);\n+        try (var fateStores = createFateStores(context, zk, adminLock)) {\n+          for (String fateIdStr : fateOpsCommand.fateIdList) {\n+            if (!admin.prepDelete(fateStores.getStoresMap(), fateIdStr)) {\n+              throw new AccumuloException(\""Could not delete transaction: \"" + fateIdStr);\n+            }\n+            admin.deleteLocks(zk, zTableLocksPath, fateIdStr);\n           }\n-          admin.deleteLocks(zk, zTableLocksPath, fateIdStr);\n         }\n       }\n \n@@ -991,7 +992,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n             getCmdLineStatusFilters(fateOpsCommand.states);\n         EnumSet<FateInstanceType> typesFilter =\n             getCmdLineInstanceTypeFilters(fateOpsCommand.instanceTypes);\n-        readOnlyFateStores = createReadOnlyFateStores(context, zk, Constants.ZFATE);\n+        readOnlyFateStores = createReadOnlyFateStores(context, zk);\n         admin.print(readOnlyFateStores, zk, zTableLocksPath, new Formatter(System.out),\n             fateIdFilter, statusFilter, typesFilter);\n         // print line break at the end\n@@ -1000,7 +1001,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n \n       if (fateOpsCommand.summarize) {\n         if (readOnlyFateStores == null) {\n-          readOnlyFateStores = createReadOnlyFateStores(context, zk, Constants.ZFATE);\n+          readOnlyFateStores = createReadOnlyFateStores(context, zk);\n         }\n         summarizeFateTx(context, fateOpsCommand, admin, readOnlyFateStores, zTableLocksPath);\n       }\n@@ -1011,20 +1012,19 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n     }\n   }\n \n-  private Map<FateInstanceType,FateStore<Admin>> createFateStores(ServerContext context,\n-      ZooSession zk, ServiceLock adminLock) throws InterruptedException, KeeperException {\n+  private FateStores createFateStores(ServerContext context, ZooSession zk, ServiceLock adminLock)\n+      throws InterruptedException, KeeperException {\n     var lockId = adminLock.getLockID();\n     MetaFateStore<Admin> mfs = new MetaFateStore<>(zk, lockId, null);\n     UserFateStore<Admin> ufs =\n         new UserFateStore<>(context, SystemTables.FATE.tableName(), lockId, null);\n-    return Map.of(FateInstanceType.META, mfs, FateInstanceType.USER, ufs);\n+    return new FateStores(FateInstanceType.META, mfs, FateInstanceType.USER, ufs);\n   }\n \n-  private Map<FateInstanceType,ReadOnlyFateStore<Admin>>\n-      createReadOnlyFateStores(ServerContext context, ZooSession zk, String fateZkPath)\n-          throws InterruptedException, KeeperException {\n-    MetaFateStore<Admin> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n-    UserFateStore<Admin> readOnlyUFS =\n+  private Map<FateInstanceType,ReadOnlyFateStore<Admin>> createReadOnlyFateStores(\n+      ServerContext context, ZooSession zk) throws InterruptedException, KeeperException {\n+    ReadOnlyFateStore<Admin> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n+    ReadOnlyFateStore<Admin> readOnlyUFS =\n         new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null);\n     return Map.of(FateInstanceType.META, readOnlyMFS, FateInstanceType.USER, readOnlyUFS);\n   }\n@@ -1343,4 +1343,27 @@ private static boolean depsFailed(CheckCommand.Check check,\n     System.out.println(\""-\"".repeat(50));\n     System.out.println();\n   }\n+\n+  /**\n+   * Wrapper around the fate stores\n+   */\n+  private static class FateStores implements AutoCloseable {\n+    private final Map<FateInstanceType,FateStore<Admin>> storesMap;\n+\n+    private FateStores(FateInstanceType type1, FateStore<Admin> store1, FateInstanceType type2,\n+        FateStore<Admin> store2) {\n+      storesMap = Map.of(type1, store1, type2, store2);\n+    }\n+\n+    private Map<FateInstanceType,FateStore<Admin>> getStoresMap() {\n+      return storesMap;\n+    }\n+\n+    @Override\n+    public void close() {\n+      for (var fs : storesMap.values()) {\n+        fs.close();\n+      }\n+    }\n+  }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\nindex 47f9e90e4b3..a771f595a92 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\n@@ -61,54 +61,55 @@ private static Admin.CheckCommand.CheckStatus checkTableLocks(ServerContext cont\n     final AdminUtil<Admin> admin = new AdminUtil<>();\n     final var zTableLocksPath = context.getServerPaths().createTableLocksPath();\n     final var zk = context.getZooSession();\n-    final MetaFateStore<Admin> mfs = new MetaFateStore<>(zk, null, null);\n-    final UserFateStore<Admin> ufs =\n-        new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null);\n+    try (final MetaFateStore<Admin> mfs = new MetaFateStore<>(zk, null, null); final UserFateStore<\n+        Admin> ufs = new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null)) {\n \n-    log.trace(\""Ensuring table and namespace locks are valid...\"");\n+      log.trace(\""Ensuring table and namespace locks are valid...\"");\n \n-    var tableIds = context.tableOperations().tableIdMap().values();\n-    var namespaceIds = context.namespaceOperations().namespaceIdMap().values();\n-    List<String> lockedIds =\n-        context.getZooSession().asReader().getChildren(zTableLocksPath.toString());\n-    boolean locksExist = !lockedIds.isEmpty();\n+      var tableIds = context.tableOperations().tableIdMap().values();\n+      var namespaceIds = context.namespaceOperations().namespaceIdMap().values();\n+      List<String> lockedIds =\n+          context.getZooSession().asReader().getChildren(zTableLocksPath.toString());\n+      boolean locksExist = !lockedIds.isEmpty();\n \n-    if (locksExist) {\n-      lockedIds.removeAll(tableIds);\n-      lockedIds.removeAll(namespaceIds);\n-      if (!lockedIds.isEmpty()) {\n-        status = Admin.CheckCommand.CheckStatus.FAILED;\n-        log.warn(\""...Some table and namespace locks are INVALID (the table/namespace DNE): \""\n-            + lockedIds);\n+      if (locksExist) {\n+        lockedIds.removeAll(tableIds);\n+        lockedIds.removeAll(namespaceIds);\n+        if (!lockedIds.isEmpty()) {\n+          status = Admin.CheckCommand.CheckStatus.FAILED;\n+          log.warn(\""...Some table and namespace locks are INVALID (the table/namespace DNE): \""\n+              + lockedIds);\n+        } else {\n+          log.trace(\""...locks are valid\"");\n+        }\n       } else {\n-        log.trace(\""...locks are valid\"");\n+        log.trace(\""...no locks present\"");\n       }\n-    } else {\n-      log.trace(\""...no locks present\"");\n-    }\n \n-    log.trace(\""Ensuring table and namespace locks are associated with a FATE op...\"");\n+      log.trace(\""Ensuring table and namespace locks are associated with a FATE op...\"");\n \n-    if (locksExist) {\n-      final var fateStatus =\n-          admin.getStatus(Map.of(FateInstanceType.META, mfs, FateInstanceType.USER, ufs), zk,\n-              zTableLocksPath, null, null, null);\n-      if (!fateStatus.getDanglingHeldLocks().isEmpty()\n-          || !fateStatus.getDanglingWaitingLocks().isEmpty()) {\n-        status = Admin.CheckCommand.CheckStatus.FAILED;\n-        log.warn(\""The following locks did not have an associated FATE operation\\n\"");\n-        for (Map.Entry<FateId,List<String>> entry : fateStatus.getDanglingHeldLocks().entrySet()) {\n-          log.warn(\""fateId: \"" + entry.getKey() + \"" locked: \"" + entry.getValue());\n-        }\n-        for (Map.Entry<FateId,List<String>> entry : fateStatus.getDanglingWaitingLocks()\n-            .entrySet()) {\n-          log.warn(\""fateId: \"" + entry.getKey() + \"" locking: \"" + entry.getValue());\n+      if (locksExist) {\n+        final var fateStatus =\n+            admin.getStatus(Map.of(FateInstanceType.META, mfs, FateInstanceType.USER, ufs), zk,\n+                zTableLocksPath, null, null, null);\n+        if (!fateStatus.getDanglingHeldLocks().isEmpty()\n+            || !fateStatus.getDanglingWaitingLocks().isEmpty()) {\n+          status = Admin.CheckCommand.CheckStatus.FAILED;\n+          log.warn(\""The following locks did not have an associated FATE operation\\n\"");\n+          for (Map.Entry<FateId,List<String>> entry : fateStatus.getDanglingHeldLocks()\n+              .entrySet()) {\n+            log.warn(\""fateId: \"" + entry.getKey() + \"" locked: \"" + entry.getValue());\n+          }\n+          for (Map.Entry<FateId,List<String>> entry : fateStatus.getDanglingWaitingLocks()\n+              .entrySet()) {\n+            log.warn(\""fateId: \"" + entry.getKey() + \"" locking: \"" + entry.getValue());\n+          }\n+        } else {\n+          log.trace(\""...locks are valid\"");\n         }\n       } else {\n-        log.trace(\""...locks are valid\"");\n+        log.trace(\""...no locks present\"");\n       }\n-    } else {\n-      log.trace(\""...no locks present\"");\n     }\n \n     return status;\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex 3f5a8ec0402..c73d5768bf6 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -41,7 +41,7 @@\n  * transaction's operation, possibly pushing more operations onto the transaction as each step\n  * successfully completes. If a step fails, the stack can be unwound, undoing each operation.\n  */\n-public interface FateStore<T> extends ReadOnlyFateStore<T> {\n+public interface FateStore<T> extends ReadOnlyFateStore<T>, AutoCloseable {\n \n   /**\n    * Create a new fate transaction id\n@@ -269,4 +269,6 @@ public int hashCode() {\n    */\n   FateTxStore<T> reserve(FateId fateId);\n \n+  @Override\n+  void close();\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex d3d117c9fa4..f38c50a2e9f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -41,9 +41,11 @@\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.ConditionalWriter;\n+import org.apache.accumulo.core.client.ConditionalWriterConfig;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n+import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n@@ -73,6 +75,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n+import com.google.common.base.Suppliers;\n \n public class UserFateStore<T> extends AbstractFateStore<T> {\n \n@@ -80,6 +83,7 @@ public class UserFateStore<T> extends AbstractFateStore<T> {\n \n   private final ClientContext context;\n   private final String tableName;\n+  private final Supplier<ConditionalWriter> writer;\n \n   private static final FateInstanceType fateInstanceType = FateInstanceType.USER;\n   private static final com.google.common.collect.Range<Integer> REPO_RANGE =\n@@ -108,6 +112,14 @@ public UserFateStore(ClientContext context, String tableName, ZooUtil.LockID loc\n     super(lockID, isLockHeld, maxDeferred, fateIdGenerator);\n     this.context = Objects.requireNonNull(context);\n     this.tableName = Objects.requireNonNull(tableName);\n+    this.writer = Suppliers.memoize(() -> {\n+      try {\n+        return createConditionalWriterForFateTable(this.tableName);\n+      } catch (TableNotFoundException e) {\n+        throw new IllegalStateException(\n+            \""Incorrect use of UserFateStore, table \"" + tableName + \"" does not exist.\"");\n+      }\n+    });\n   }\n \n   @Override\n@@ -383,7 +395,7 @@ public static String getRowId(FateId fateId) {\n   }\n \n   private FateMutatorImpl<T> newMutator(FateId fateId) {\n-    return new FateMutatorImpl<>(context, tableName, fateId);\n+    return new FateMutatorImpl<>(context, tableName, fateId, writer);\n   }\n \n   private <R> R scanTx(Function<Scanner,R> func) {\n@@ -491,15 +503,15 @@ private Map<FateId,ConditionalWriter.Status> tryMutateBatch() {\n       }\n \n       final Map<FateId,ConditionalWriter.Status> resultsMap = new HashMap<>();\n-      try (ConditionalWriter writer = context.createConditionalWriter(tableName)) {\n-        Iterator<ConditionalWriter.Result> results = writer\n+      try {\n+        Iterator<ConditionalWriter.Result> results = writer.get()\n             .write(pending.values().stream().map(pair -> pair.getFirst().getMutation()).iterator());\n         while (results.hasNext()) {\n           var result = results.next();\n           var row = new Text(result.getMutation().getRow());\n           resultsMap.put(FateId.from(FateInstanceType.USER, row.toString()), result.getStatus());\n         }\n-      } catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {\n+      } catch (AccumuloException | AccumuloSecurityException e) {\n         throw new IllegalStateException(e);\n       }\n       return resultsMap;\n@@ -689,4 +701,17 @@ static Integer restoreRepo(Text invertedPosition) {\n         \""Position %s is not in the valid range of [0,%s]\"", position, MAX_REPOS);\n     return position;\n   }\n+\n+  private ConditionalWriter createConditionalWriterForFateTable(String tableName)\n+      throws TableNotFoundException {\n+    int maxThreads =\n+        context.getConfiguration().getCount(Property.MANAGER_FATE_CONDITIONAL_WRITER_THREADS_MAX);\n+    ConditionalWriterConfig cwConfig = new ConditionalWriterConfig().setMaxWriteThreads(maxThreads);\n+    return context.createConditionalWriter(tableName, cwConfig);\n+  }\n+\n+  @Override\n+  public void close() {\n+    writer.get().close();\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex 2d769bce6a5..72ce79e1cc3 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -653,6 +653,11 @@ private byte[] mutate(FateId fateId, UnaryOperator<FateData<T>> fateDataOp)\n     }\n   }\n \n+  @Override\n+  public void close() {\n+    // no-op\n+  }\n+\n   protected static class FateData<T> {\n     final TStatus status;\n     final Optional<FateKey> fateKey;\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\nindex 8e881fddad5..e4d057fc108 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n@@ -291,4 +291,9 @@ public FateInstanceType type() {\n   public boolean isDeferredOverflow() {\n     return false;\n   }\n+\n+  @Override\n+  public void close() {\n+    // no-op\n+  }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\nindex 6365df4a80a..7c9a4c5d568 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n@@ -252,10 +252,10 @@ public void testExternalCompaction() throws Exception {\n   @Test\n   public void testCompactionCommitAndDeadDetectionRoot() throws Exception {\n     var ctx = getCluster().getServerContext();\n-    FateStore<Manager> metaFateStore =\n-        new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null);\n \n-    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<Manager> metaFateStore =\n+            new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null)) {\n       var tableId = ctx.getTableId(SystemTables.ROOT.tableName());\n       var allCids = new HashMap<TableId,List<ExternalCompactionId>>();\n       var fateId = createCompactionCommitAndDeadMetadata(c, metaFateStore,\n@@ -271,10 +271,10 @@ public void testCompactionCommitAndDeadDetectionRoot() throws Exception {\n   @Test\n   public void testCompactionCommitAndDeadDetectionMeta() throws Exception {\n     var ctx = getCluster().getServerContext();\n-    FateStore<Manager> metaFateStore =\n-        new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null);\n \n-    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<Manager> metaFateStore =\n+            new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null)) {\n       // Metadata table by default already has 2 tablets\n       var tableId = ctx.getTableId(SystemTables.METADATA.tableName());\n       var allCids = new HashMap<TableId,List<ExternalCompactionId>>();\n@@ -293,9 +293,9 @@ public void testCompactionCommitAndDeadDetectionUser() throws Exception {\n     var ctx = getCluster().getServerContext();\n     final String tableName = getUniqueNames(1)[0];\n \n-    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n-      UserFateStore<Manager> userFateStore =\n-          new UserFateStore<>(ctx, SystemTables.FATE.tableName(), testLock.getLockID(), null);\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build();\n+        UserFateStore<Manager> userFateStore =\n+            new UserFateStore<>(ctx, SystemTables.FATE.tableName(), testLock.getLockID(), null)) {\n       SortedSet<Text> splits = new TreeSet<>();\n       splits.add(new Text(row(MAX_DATA / 2)));\n       c.tableOperations().create(tableName, new NewTableConfiguration().withSplits(splits));\n@@ -317,11 +317,11 @@ public void testCompactionCommitAndDeadDetectionAll() throws Exception {\n     var ctx = getCluster().getServerContext();\n     final String userTable = getUniqueNames(1)[0];\n \n-    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n-      UserFateStore<Manager> userFateStore =\n-          new UserFateStore<>(ctx, SystemTables.FATE.tableName(), testLock.getLockID(), null);\n-      FateStore<Manager> metaFateStore =\n-          new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null);\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<Manager> userFateStore =\n+            new UserFateStore<>(ctx, SystemTables.FATE.tableName(), testLock.getLockID(), null);\n+        FateStore<Manager> metaFateStore =\n+            new MetaFateStore<>(ctx.getZooSession(), testLock.getLockID(), null)) {\n \n       SortedSet<Text> splits = new TreeSet<>();\n       splits.add(new Text(row(MAX_DATA / 2)));\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java b/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java\nindex 70fa4549650..ae6d485d05e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresITBase.java\n@@ -73,53 +73,55 @@ private void testReserveUnreserve(TestStoreFactory<SleepingTestEnv> testStoreFac\n     final ZooUtil.LockID lock1 = new ZooUtil.LockID(\""/locks\"", \""L1\"", 50);\n     final ZooUtil.LockID lock2 = new ZooUtil.LockID(\""/locks\"", \""L2\"", 52);\n     Map<FateId,FateStore.FateReservation> activeReservations;\n-    final FateStore<SleepingTestEnv> store1 = testStoreFactory.create(lock1, null);\n-    final FateStore<SleepingTestEnv> store2 = testStoreFactory.create(lock2, null);\n-    final FateId fakeFateId = FateId.from(store1.type(), UUID.randomUUID());\n+    try (final FateStore<SleepingTestEnv> store1 = testStoreFactory.create(lock1, null);\n+        final FateStore<SleepingTestEnv> store2 = testStoreFactory.create(lock2, null)) {\n+      final FateId fakeFateId = FateId.from(store1.type(), UUID.randomUUID());\n \n-    // Create the fate ids using store1\n-    for (int i = 0; i < numFateIds; i++) {\n-      assertTrue(allIds.add(store1.create()));\n-    }\n-    assertEquals(numFateIds, allIds.size());\n-\n-    // Reserve half the fate ids using store1 and rest using store2, after reserving a fate id in\n-    // one, should not be able to reserve the same in the other. Should also not matter that all the\n-    // ids were created using store1\n-    int count = 0;\n-    for (FateId fateId : allIds) {\n-      if (count % 2 == 0) {\n-        reservations.add(store1.reserve(fateId));\n-        assertTrue(store2.tryReserve(fateId).isEmpty());\n-      } else {\n-        reservations.add(store2.reserve(fateId));\n-        assertTrue(store1.tryReserve(fateId).isEmpty());\n+      // Create the fate ids using store1\n+      for (int i = 0; i < numFateIds; i++) {\n+        assertTrue(allIds.add(store1.create()));\n       }\n-      count++;\n-    }\n-    // Try to reserve a non-existent fate id\n-    assertTrue(store1.tryReserve(fakeFateId).isEmpty());\n-    assertTrue(store2.tryReserve(fakeFateId).isEmpty());\n-    // Both stores should return the same reserved transactions\n-    activeReservations = store1.getActiveReservations();\n-    assertEquals(allIds, activeReservations.keySet());\n-    activeReservations = store2.getActiveReservations();\n-    assertEquals(allIds, activeReservations.keySet());\n-\n-    // Test setting/getting the TStatus and unreserving the transactions\n-    for (int i = 0; i < allIds.size(); i++) {\n-      var reservation = reservations.get(i);\n-      assertEquals(ReadOnlyFateStore.TStatus.NEW, reservation.getStatus());\n-      reservation.setStatus(ReadOnlyFateStore.TStatus.SUBMITTED);\n-      assertEquals(ReadOnlyFateStore.TStatus.SUBMITTED, reservation.getStatus());\n-      reservation.delete();\n-      reservation.unreserve(Duration.ofMillis(0));\n-      // Attempt to set a status on a tx that has been unreserved (should throw exception)\n-      assertThrows(IllegalStateException.class,\n-          () -> reservation.setStatus(ReadOnlyFateStore.TStatus.NEW));\n+      assertEquals(numFateIds, allIds.size());\n+\n+      // Reserve half the fate ids using store1 and rest using store2, after reserving a fate id in\n+      // one, should not be able to reserve the same in the other. Should also not matter that all\n+      // the\n+      // ids were created using store1\n+      int count = 0;\n+      for (FateId fateId : allIds) {\n+        if (count % 2 == 0) {\n+          reservations.add(store1.reserve(fateId));\n+          assertTrue(store2.tryReserve(fateId).isEmpty());\n+        } else {\n+          reservations.add(store2.reserve(fateId));\n+          assertTrue(store1.tryReserve(fateId).isEmpty());\n+        }\n+        count++;\n+      }\n+      // Try to reserve a non-existent fate id\n+      assertTrue(store1.tryReserve(fakeFateId).isEmpty());\n+      assertTrue(store2.tryReserve(fakeFateId).isEmpty());\n+      // Both stores should return the same reserved transactions\n+      activeReservations = store1.getActiveReservations();\n+      assertEquals(allIds, activeReservations.keySet());\n+      activeReservations = store2.getActiveReservations();\n+      assertEquals(allIds, activeReservations.keySet());\n+\n+      // Test setting/getting the TStatus and unreserving the transactions\n+      for (int i = 0; i < allIds.size(); i++) {\n+        var reservation = reservations.get(i);\n+        assertEquals(ReadOnlyFateStore.TStatus.NEW, reservation.getStatus());\n+        reservation.setStatus(ReadOnlyFateStore.TStatus.SUBMITTED);\n+        assertEquals(ReadOnlyFateStore.TStatus.SUBMITTED, reservation.getStatus());\n+        reservation.delete();\n+        reservation.unreserve(Duration.ofMillis(0));\n+        // Attempt to set a status on a tx that has been unreserved (should throw exception)\n+        assertThrows(IllegalStateException.class,\n+            () -> reservation.setStatus(ReadOnlyFateStore.TStatus.NEW));\n+      }\n+      assertTrue(store1.getActiveReservations().isEmpty());\n+      assertTrue(store2.getActiveReservations().isEmpty());\n     }\n-    assertTrue(store1.getActiveReservations().isEmpty());\n-    assertTrue(store2.getActiveReservations().isEmpty());\n   }\n \n   @Test\n@@ -132,11 +134,12 @@ private void testReserveNonExistentTxn(TestStoreFactory<SleepingTestEnv> testSto\n     // Tests that reserve() doesn't hang indefinitely and instead throws an error\n     // on reserve() a non-existent transaction.\n     final ZooUtil.LockID lock = new ZooUtil.LockID(\""/locks\"", \""L1\"", 50);\n-    final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null);\n-    final FateId fakeFateId = FateId.from(store.type(), UUID.randomUUID());\n+    try (final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null)) {\n+      final FateId fakeFateId = FateId.from(store.type(), UUID.randomUUID());\n \n-    var err = assertThrows(IllegalStateException.class, () -> store.reserve(fakeFateId));\n-    assertTrue(err.getMessage().contains(fakeFateId.canonical()));\n+      var err = assertThrows(IllegalStateException.class, () -> store.reserve(fakeFateId));\n+      assertTrue(err.getMessage().contains(fakeFateId.canonical()));\n+    }\n   }\n \n   @Test\n@@ -150,30 +153,32 @@ private void testReserveReservedAndUnreserveUnreserved(\n     final Set<FateId> allIds = new HashSet<>();\n     final List<FateStore.FateTxStore<SleepingTestEnv>> reservations = new ArrayList<>();\n     final ZooUtil.LockID lock = new ZooUtil.LockID(\""/locks\"", \""L1\"", 50);\n-    final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null);\n-\n-    // Create some FateIds and ensure that they can be reserved\n-    for (int i = 0; i < numFateIds; i++) {\n-      FateId fateId = store.create();\n-      assertTrue(allIds.add(fateId));\n-      var reservation = store.tryReserve(fateId);\n-      assertFalse(reservation.isEmpty());\n-      reservations.add(reservation.orElseThrow());\n-    }\n-    assertEquals(numFateIds, allIds.size());\n+    try (final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null)) {\n \n-    // Try to reserve again, should not reserve\n-    for (FateId fateId : allIds) {\n-      assertTrue(store.tryReserve(fateId).isEmpty());\n-    }\n+      // Create some FateIds and ensure that they can be reserved\n+      for (int i = 0; i < numFateIds; i++) {\n+        FateId fateId = store.create();\n+        assertTrue(allIds.add(fateId));\n+        var reservation = store.tryReserve(fateId);\n+        assertFalse(reservation.isEmpty());\n+        reservations.add(reservation.orElseThrow());\n+      }\n+      assertEquals(numFateIds, allIds.size());\n \n-    // Unreserve all the FateIds\n-    for (var reservation : reservations) {\n-      reservation.unreserve(Duration.ofMillis(0));\n-    }\n-    // Try to unreserve again (should throw exception)\n-    for (var reservation : reservations) {\n-      assertThrows(IllegalStateException.class, () -> reservation.unreserve(Duration.ofMillis(0)));\n+      // Try to reserve again, should not reserve\n+      for (FateId fateId : allIds) {\n+        assertTrue(store.tryReserve(fateId).isEmpty());\n+      }\n+\n+      // Unreserve all the FateIds\n+      for (var reservation : reservations) {\n+        reservation.unreserve(Duration.ofMillis(0));\n+      }\n+      // Try to unreserve again (should throw exception)\n+      for (var reservation : reservations) {\n+        assertThrows(IllegalStateException.class,\n+            () -> reservation.unreserve(Duration.ofMillis(0)));\n+      }\n     }\n   }\n \n@@ -188,39 +193,40 @@ private void testReserveAfterUnreserveAndReserveAfterDeleted(\n     final Set<FateId> allIds = new HashSet<>();\n     final List<FateStore.FateTxStore<SleepingTestEnv>> reservations = new ArrayList<>();\n     final ZooUtil.LockID lock = new ZooUtil.LockID(\""/locks\"", \""L1\"", 50);\n-    final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null);\n-\n-    // Create some FateIds and ensure that they can be reserved\n-    for (int i = 0; i < numFateIds; i++) {\n-      FateId fateId = store.create();\n-      assertTrue(allIds.add(fateId));\n-      var reservation = store.tryReserve(fateId);\n-      assertFalse(reservation.isEmpty());\n-      reservations.add(reservation.orElseThrow());\n-    }\n-    assertEquals(numFateIds, allIds.size());\n+    try (final FateStore<SleepingTestEnv> store = testStoreFactory.create(lock, null)) {\n \n-    // Unreserve all\n-    for (var reservation : reservations) {\n-      reservation.unreserve(Duration.ofMillis(0));\n-    }\n+      // Create some FateIds and ensure that they can be reserved\n+      for (int i = 0; i < numFateIds; i++) {\n+        FateId fateId = store.create();\n+        assertTrue(allIds.add(fateId));\n+        var reservation = store.tryReserve(fateId);\n+        assertFalse(reservation.isEmpty());\n+        reservations.add(reservation.orElseThrow());\n+      }\n+      assertEquals(numFateIds, allIds.size());\n \n-    // Ensure they can be reserved again, and delete and unreserve this time\n-    for (FateId fateId : allIds) {\n-      // Verify that the tx status is still NEW after unreserving since it hasn't been deleted\n-      assertEquals(ReadOnlyFateStore.TStatus.NEW, store.read(fateId).getStatus());\n-      var reservation = store.tryReserve(fateId);\n-      assertFalse(reservation.isEmpty());\n-      reservation.orElseThrow().delete();\n-      reservation.orElseThrow().unreserve(Duration.ofMillis(0));\n-    }\n+      // Unreserve all\n+      for (var reservation : reservations) {\n+        reservation.unreserve(Duration.ofMillis(0));\n+      }\n+\n+      // Ensure they can be reserved again, and delete and unreserve this time\n+      for (FateId fateId : allIds) {\n+        // Verify that the tx status is still NEW after unreserving since it hasn't been deleted\n+        assertEquals(ReadOnlyFateStore.TStatus.NEW, store.read(fateId).getStatus());\n+        var reservation = store.tryReserve(fateId);\n+        assertFalse(reservation.isEmpty());\n+        reservation.orElseThrow().delete();\n+        reservation.orElseThrow().unreserve(Duration.ofMillis(0));\n+      }\n \n-    for (FateId fateId : allIds) {\n-      // Verify that the tx is now unknown since it has been deleted\n-      assertEquals(ReadOnlyFateStore.TStatus.UNKNOWN, store.read(fateId).getStatus());\n-      // Attempt to reserve a deleted txn, should throw an exception and not wait indefinitely\n-      var err = assertThrows(IllegalStateException.class, () -> store.reserve(fateId));\n-      assertTrue(err.getMessage().contains(fateId.canonical()));\n+      for (FateId fateId : allIds) {\n+        // Verify that the tx is now unknown since it has been deleted\n+        assertEquals(ReadOnlyFateStore.TStatus.UNKNOWN, store.read(fateId).getStatus());\n+        // Attempt to reserve a deleted txn, should throw an exception and not wait indefinitely\n+        var err = assertThrows(IllegalStateException.class, () -> store.reserve(fateId));\n+        assertTrue(err.getMessage().contains(fateId.canonical()));\n+      }\n     }\n   }\n \n@@ -239,42 +245,43 @@ private void testMultipleFateInstances(TestStoreFactory<SleepingTestEnv> testSto\n     final ZooUtil.LockID lock2 = new ZooUtil.LockID(\""/locks\"", \""L2\"", 52);\n     final Set<ZooUtil.LockID> liveLocks = new HashSet<>();\n     final Predicate<ZooUtil.LockID> isLockHeld = liveLocks::contains;\n-    final FateStore<SleepingTestEnv> store1 = testStoreFactory.create(lock1, isLockHeld);\n-    final FateStore<SleepingTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld);\n+    try (final FateStore<SleepingTestEnv> store1 = testStoreFactory.create(lock1, isLockHeld);\n+        final FateStore<SleepingTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld)) {\n \n-    liveLocks.add(lock1);\n-    liveLocks.add(lock2);\n-\n-    Fate<SleepingTestEnv> fate1 = new Fate<>(testEnv1, store1, true, Object::toString,\n-        DefaultConfiguration.getInstance(), new ScheduledThreadPoolExecutor(2));\n-    Fate<SleepingTestEnv> fate2 = new Fate<>(testEnv2, store2, false, Object::toString,\n-        DefaultConfiguration.getInstance(), new ScheduledThreadPoolExecutor(2));\n+      liveLocks.add(lock1);\n+      liveLocks.add(lock2);\n \n-    try {\n-      for (int i = 0; i < numFateIds; i++) {\n-        FateId fateId;\n-        // Start half the txns using fate1, and the other half using fate2\n-        if (i % 2 == 0) {\n-          fateId = fate1.startTransaction();\n-          fate1.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n-        } else {\n-          fateId = fate2.startTransaction();\n-          fate2.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+      Fate<SleepingTestEnv> fate1 = new Fate<>(testEnv1, store1, true, Object::toString,\n+          DefaultConfiguration.getInstance(), new ScheduledThreadPoolExecutor(2));\n+      Fate<SleepingTestEnv> fate2 = new Fate<>(testEnv2, store2, false, Object::toString,\n+          DefaultConfiguration.getInstance(), new ScheduledThreadPoolExecutor(2));\n+\n+      try {\n+        for (int i = 0; i < numFateIds; i++) {\n+          FateId fateId;\n+          // Start half the txns using fate1, and the other half using fate2\n+          if (i % 2 == 0) {\n+            fateId = fate1.startTransaction();\n+            fate1.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+          } else {\n+            fateId = fate2.startTransaction();\n+            fate2.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+          }\n+          allIds.add(fateId);\n         }\n-        allIds.add(fateId);\n-      }\n-      assertEquals(numFateIds, allIds.size());\n+        assertEquals(numFateIds, allIds.size());\n \n-      // Should be able to wait for completion on any fate instance\n-      for (FateId fateId : allIds) {\n-        fate2.waitForCompletion(fateId);\n+        // Should be able to wait for completion on any fate instance\n+        for (FateId fateId : allIds) {\n+          fate2.waitForCompletion(fateId);\n+        }\n+        // Ensure that all txns have been executed and have only been executed once\n+        assertTrue(Collections.disjoint(testEnv1.executedOps, testEnv2.executedOps));\n+        assertEquals(allIds, Sets.union(testEnv1.executedOps, testEnv2.executedOps));\n+      } finally {\n+        fate1.shutdown(1, TimeUnit.MINUTES);\n+        fate2.shutdown(1, TimeUnit.MINUTES);\n       }\n-      // Ensure that all txns have been executed and have only been executed once\n-      assertTrue(Collections.disjoint(testEnv1.executedOps, testEnv2.executedOps));\n-      assertEquals(allIds, Sets.union(testEnv1.executedOps, testEnv2.executedOps));\n-    } finally {\n-      fate1.shutdown(1, TimeUnit.MINUTES);\n-      fate2.shutdown(1, TimeUnit.MINUTES);\n     }\n   }\n \n@@ -304,73 +311,79 @@ private void testDeadReservationsCleanup(TestStoreFactory<LatchTestEnv> testStor\n     final AccumuloConfiguration config = FateTestUtil.createTestFateConfig(numThreads);\n     Map<FateId,FateStore.FateReservation> reservations;\n \n-    final FateStore<LatchTestEnv> store1 = testStoreFactory.create(lock1, isLockHeld);\n-    liveLocks.add(lock1);\n-    Fate<LatchTestEnv> fate1 = null;\n-    Fate<LatchTestEnv> fate2 = null;\n-\n-    try {\n-      fate1 = new FastFate<>(testEnv1, store1, true, Object::toString, config);\n-      // Ensure nothing is reserved yet\n-      assertTrue(store1.getActiveReservations().isEmpty());\n-\n-      // Create transactions\n-      for (int i = 0; i < numFateIds; i++) {\n-        FateId fateId;\n-        fateId = fate1.startTransaction();\n-        fate1.seedTransaction(TEST_FATE_OP, fateId, new LatchTestRepo(), true, \""test\"");\n-        allIds.add(fateId);\n-      }\n-      assertEquals(numFateIds, allIds.size());\n-\n-      // Wait for all the fate worker threads to start working on the transactions\n-      Wait.waitFor(() -> testEnv1.numWorkers.get() == numFateIds);\n-      // Each fate worker will be hung up working (IN_PROGRESS) on a single transaction\n+    try (final FateStore<LatchTestEnv> store1 = testStoreFactory.create(lock1, isLockHeld)) {\n+      liveLocks.add(lock1);\n+      Fate<LatchTestEnv> fate1 = null;\n+      Fate<LatchTestEnv> fate2 = null;\n \n-      // Verify store1 has the transactions reserved and that they were reserved with lock1\n-      reservations = store1.getActiveReservations();\n-      assertEquals(allIds, reservations.keySet());\n-      reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n-\n-      final FateStore<LatchTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld);\n-\n-      // Verify store2 can see the reserved transactions even though they were reserved using\n-      // store1\n-      reservations = store2.getActiveReservations();\n-      assertEquals(allIds, reservations.keySet());\n-      reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n-\n-      // Simulate what would happen if the Manager using the Fate object (fate1) died.\n-      // isLockHeld would return false for the LockId of the Manager that died (in this case, lock1)\n-      // and true for the new Manager's lock (lock2)\n-      liveLocks.remove(lock1);\n-      liveLocks.add(lock2);\n+      try {\n+        fate1 = new FastFate<>(testEnv1, store1, true, Object::toString, config);\n+        // Ensure nothing is reserved yet\n+        assertTrue(store1.getActiveReservations().isEmpty());\n \n-      // Create the new Fate/start the Fate threads (the work finder and the workers).\n-      // Don't run another dead reservation cleaner since we already have one running from fate1.\n-      fate2 = new Fate<>(testEnv2, store2, false, Object::toString, config,\n-          new ScheduledThreadPoolExecutor(2));\n-\n-      // Wait for the \""dead\"" reservations to be deleted and picked up again (reserved using\n-      // fate2/store2/lock2 now).\n-      // They are considered \""dead\"" if they are held by lock1 in this test. We don't have to worry\n-      // about fate1/store1/lock1 being used to reserve the transactions again since all\n-      // the workers for fate1 are hung up\n-      Wait.waitFor(() -> {\n-        Map<FateId,FateStore.FateReservation> store2Reservations = store2.getActiveReservations();\n-        boolean allReservedWithLock2 =\n-            store2Reservations.values().stream().allMatch(entry -> entry.getLockID().equals(lock2));\n-        return store2Reservations.keySet().equals(allIds) && allReservedWithLock2;\n-      }, fate1.getDeadResCleanupDelay().toMillis() * 2);\n-    } finally {\n-      // Finish work and shutdown\n-      testEnv1.workersLatch.countDown();\n-      testEnv2.workersLatch.countDown();\n-      if (fate1 != null) {\n-        fate1.shutdown(1, TimeUnit.MINUTES);\n-      }\n-      if (fate2 != null) {\n-        fate2.shutdown(1, TimeUnit.MINUTES);\n+        // Create transactions\n+        for (int i = 0; i < numFateIds; i++) {\n+          FateId fateId;\n+          fateId = fate1.startTransaction();\n+          fate1.seedTransaction(TEST_FATE_OP, fateId, new LatchTestRepo(), true, \""test\"");\n+          allIds.add(fateId);\n+        }\n+        assertEquals(numFateIds, allIds.size());\n+\n+        // Wait for all the fate worker threads to start working on the transactions\n+        Wait.waitFor(() -> testEnv1.numWorkers.get() == numFateIds);\n+        // Each fate worker will be hung up working (IN_PROGRESS) on a single transaction\n+\n+        // Verify store1 has the transactions reserved and that they were reserved with lock1\n+        reservations = store1.getActiveReservations();\n+        assertEquals(allIds, reservations.keySet());\n+        reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n+\n+        try (final FateStore<LatchTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld)) {\n+\n+          // Verify store2 can see the reserved transactions even though they were reserved using\n+          // store1\n+          reservations = store2.getActiveReservations();\n+          assertEquals(allIds, reservations.keySet());\n+          reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n+\n+          // Simulate what would happen if the Manager using the Fate object (fate1) died.\n+          // isLockHeld would return false for the LockId of the Manager that died (in this case,\n+          // lock1)\n+          // and true for the new Manager's lock (lock2)\n+          liveLocks.remove(lock1);\n+          liveLocks.add(lock2);\n+\n+          // Create the new Fate/start the Fate threads (the work finder and the workers).\n+          // Don't run another dead reservation cleaner since we already have one running from\n+          // fate1.\n+          fate2 = new Fate<>(testEnv2, store2, false, Object::toString, config,\n+              new ScheduledThreadPoolExecutor(2));\n+\n+          // Wait for the \""dead\"" reservations to be deleted and picked up again (reserved using\n+          // fate2/store2/lock2 now).\n+          // They are considered \""dead\"" if they are held by lock1 in this test. We don't have to\n+          // worry\n+          // about fate1/store1/lock1 being used to reserve the transactions again since all\n+          // the workers for fate1 are hung up\n+          Wait.waitFor(() -> {\n+            Map<FateId,FateStore.FateReservation> store2Reservations =\n+                store2.getActiveReservations();\n+            boolean allReservedWithLock2 = store2Reservations.values().stream()\n+                .allMatch(entry -> entry.getLockID().equals(lock2));\n+            return store2Reservations.keySet().equals(allIds) && allReservedWithLock2;\n+          }, fate1.getDeadResCleanupDelay().toMillis() * 2);\n+        }\n+      } finally {\n+        // Finish work and shutdown\n+        testEnv1.workersLatch.countDown();\n+        testEnv2.workersLatch.countDown();\n+        if (fate1 != null) {\n+          fate1.shutdown(1, TimeUnit.MINUTES);\n+        }\n+        if (fate2 != null) {\n+          fate2.shutdown(1, TimeUnit.MINUTES);\n+        }\n       }\n     }\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java\nindex bddab38cc61..b890f09f1a0 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateExecutionOrderIT_SimpleSuite.java\n@@ -25,6 +25,7 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n@@ -44,11 +45,13 @@ public void executeTest(FateTestExecutor<FeoTestEnv> testMethod, int maxDeferred\n     try (var zk = new ZooSession(getClass().getSimpleName() + \"".mkdirs\"", conf)) {\n       zk.asReaderWriter().mkdirs(ZK_ROOT);\n     }\n-    try (var zk = new ZooSession(getClass().getSimpleName() + \"".fakeroot\"",\n-        conf.get(Property.INSTANCE_ZK_HOST) + ZK_ROOT,\n-        (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT),\n-        conf.get(Property.INSTANCE_SECRET))) {\n-      testMethod.execute(new MetaFateStore<>(zk, createDummyLockID(), null), sctx);\n+    try (\n+        var zk = new ZooSession(getClass().getSimpleName() + \"".fakeroot\"",\n+            conf.get(Property.INSTANCE_ZK_HOST) + ZK_ROOT,\n+            (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT),\n+            conf.get(Property.INSTANCE_SECRET));\n+        FateStore<FeoTestEnv> fs = new MetaFateStore<>(zk, createDummyLockID(), null)) {\n+      testMethod.execute(fs, sctx);\n     }\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\nindex dd7082709c3..cfd28ecda1d 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\n@@ -30,6 +30,7 @@\n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.fate.AbstractFateStore.FateIdGenerator;\n import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n@@ -64,8 +65,10 @@ public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n     expect(sctx.getZooSession()).andReturn(zk).anyTimes();\n     replay(sctx);\n \n-    testMethod.execute(\n-        new MetaFateStore<>(zk, createDummyLockID(), null, maxDeferred, fateIdGenerator), sctx);\n+    try (FateStore<TestEnv> fs =\n+        new MetaFateStore<>(zk, createDummyLockID(), null, maxDeferred, fateIdGenerator)) {\n+      testMethod.execute(fs, sctx);\n+    }\n   }\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java\nindex 2f53ee76973..4f81d2857ed 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateOpsCommandsIT.java\n@@ -42,7 +42,9 @@ public void executeTest(FateTestExecutor<LatchTestEnv> testMethod, int maxDeferr\n     ServerContext context = getCluster().getServerContext();\n     var zk = context.getZooSession();\n     // test should not be reserving txns or checking reservations, so null lockID and isLockHeld\n-    testMethod.execute(new MetaFateStore<>(zk, null, null), context);\n+    try (FateStore<LatchTestEnv> fs = new MetaFateStore<>(zk, null, null)) {\n+      testMethod.execute(fs, context);\n+    }\n   }\n \n   /**\n@@ -62,7 +64,9 @@ public void stopManagerAndExecuteTest(FateTestExecutor<LatchTestEnv> testMethod)\n       ZooUtil.LockID lockID = testLock.getLockID();\n       Predicate<ZooUtil.LockID> isLockHeld =\n           lock -> ServiceLock.isLockHeld(context.getZooCache(), lock);\n-      testMethod.execute(new MetaFateStore<>(zk, lockID, isLockHeld), context);\n+      try (FateStore<LatchTestEnv> fs = new MetaFateStore<>(zk, lockID, isLockHeld)) {\n+        testMethod.execute(fs, context);\n+      }\n     } finally {\n       if (testLock != null) {\n         testLock.unlock();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\nindex 1fc61c1c003..5c5b26d2e0a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\n@@ -26,6 +26,7 @@\n import java.io.File;\n \n import org.apache.accumulo.core.fate.AbstractFateStore;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.fate.FatePoolsWatcherITBase;\n@@ -56,7 +57,9 @@ public void executeTest(FateTestExecutor<PoolResizeTestEnv> testMethod, int maxD\n     expect(sctx.getZooSession()).andReturn(zk).anyTimes();\n     replay(sctx);\n \n-    testMethod.execute(\n-        new MetaFateStore<>(zk, createDummyLockID(), null, maxDeferred, fateIdGenerator), sctx);\n+    try (FateStore<PoolResizeTestEnv> fs =\n+        new MetaFateStore<>(zk, createDummyLockID(), null, maxDeferred, fateIdGenerator)) {\n+      testMethod.execute(fs, sctx);\n+    }\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\nindex 9f7ad526aea..4735e9ee6ea 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\n@@ -26,6 +26,7 @@\n import org.apache.accumulo.test.fate.FateStatusEnforcementITBase;\n import org.apache.accumulo.test.fate.FateTestUtil;\n import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.io.TempDir;\n@@ -50,4 +51,9 @@ public void beforeEachSetup() throws Exception {\n     fateId = store.create();\n     txStore = store.reserve(fateId);\n   }\n+\n+  @AfterEach\n+  public void afterEachTeardown() {\n+    store.close();\n+  }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\nindex 755ba42d7d0..f41f993d995 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\n@@ -73,13 +73,13 @@ public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n     ServerContext sctx = createMock(ServerContext.class);\n     expect(sctx.getZooSession()).andReturn(FateTestUtil.MetaFateZKSetup.getZk()).anyTimes();\n     replay(sctx);\n-    MetaFateStore<TestEnv> store = new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(),\n-        createDummyLockID(), null, maxDeferred, fateIdGenerator);\n-\n-    // Check that the store has no transactions before and after each test\n-    assertEquals(0, store.list().count());\n-    testMethod.execute(store, sctx);\n-    assertEquals(0, store.list().count());\n+    try (FateStore<TestEnv> store = new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(),\n+        createDummyLockID(), null, maxDeferred, fateIdGenerator)) {\n+      // Check that the store has no transactions before and after each test\n+      assertEquals(0, store.list().count());\n+      testMethod.execute(store, sctx);\n+      assertEquals(0, store.list().count());\n+    }\n   }\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java\nindex 0260827dbf8..acfe46589d2 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT_SimpleSuite.java\n@@ -25,9 +25,12 @@\n \n import java.time.Duration;\n import java.util.UUID;\n+import java.util.function.Supplier;\n \n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.ConditionalWriter;\n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n import org.apache.accumulo.core.client.admin.TabletAvailability;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n@@ -46,6 +49,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Suppliers;\n+\n public class FateMutatorImplIT_SimpleSuite extends SharedMiniClusterBase {\n \n   Logger log = LoggerFactory.getLogger(FateMutatorImplIT_SimpleSuite.class);\n@@ -67,6 +72,16 @@ protected Duration defaultTimeout() {\n     return Duration.ofMinutes(5);\n   }\n \n+  private Supplier<ConditionalWriter> createWriterSupplier(AccumuloClient client, String table) {\n+    return Suppliers.memoize(() -> {\n+      try {\n+        return client.createConditionalWriter(table);\n+      } catch (TableNotFoundException e) {\n+        throw new IllegalStateException();\n+      }\n+    });\n+  }\n+\n   @Test\n   public void putRepo() throws Exception {\n     final String table = getUniqueNames(1)[0];\n@@ -77,20 +92,22 @@ public void putRepo() throws Exception {\n \n       var fateId = FateId.from(FateInstanceType.USER, UUID.randomUUID());\n \n+      Supplier<ConditionalWriter> writer = createWriterSupplier(client, table);\n+\n       // add some repos in order\n-      var fateMutator = new FateMutatorImpl<TestEnv>(context, table, fateId);\n+      var fateMutator = new FateMutatorImpl<TestEnv>(context, table, fateId, writer);\n       fateMutator.putRepo(100, new TestRepo(\""test\"")).mutate();\n-      var fateMutator1 = new FateMutatorImpl<TestEnv>(context, table, fateId);\n+      var fateMutator1 = new FateMutatorImpl<TestEnv>(context, table, fateId, writer);\n       fateMutator1.putRepo(99, new TestRepo(\""test\"")).mutate();\n-      var fateMutator2 = new FateMutatorImpl<TestEnv>(context, table, fateId);\n+      var fateMutator2 = new FateMutatorImpl<TestEnv>(context, table, fateId, writer);\n       fateMutator2.putRepo(98, new TestRepo(\""test\"")).mutate();\n \n       // make sure we cant add a repo that has already been added\n-      var fateMutator3 = new FateMutatorImpl<TestEnv>(context, table, fateId);\n+      var fateMutator3 = new FateMutatorImpl<TestEnv>(context, table, fateId, writer);\n       assertThrows(IllegalStateException.class,\n           () -> fateMutator3.putRepo(98, new TestRepo(\""test\"")).mutate(),\n           \""Repo in position 98 already exists. Expected to not be able to add it again.\"");\n-      var fateMutator4 = new FateMutatorImpl<TestEnv>(context, table, fateId);\n+      var fateMutator4 = new FateMutatorImpl<TestEnv>(context, table, fateId, writer);\n       assertThrows(IllegalStateException.class,\n           () -> fateMutator4.putRepo(99, new TestRepo(\""test\"")).mutate(),\n           \""Repo in position 99 already exists. Expected to not be able to add it again.\"");\n@@ -107,67 +124,71 @@ public void requireStatus() throws Exception {\n \n       var fateId = FateId.from(FateInstanceType.USER, UUID.randomUUID());\n \n-      // use require status passing all statuses. without the status column present this should fail\n+      Supplier<ConditionalWriter> writer = createWriterSupplier(client, table);\n+\n+      // use require status passing all statuses. without the status column present this should\n+      // fail\n       assertThrows(IllegalStateException.class,\n-          () -> new FateMutatorImpl<>(context, table, fateId)\n+          () -> new FateMutatorImpl<>(context, table, fateId, writer)\n               .requireStatus(ReadOnlyFateStore.TStatus.values())\n               .putStatus(ReadOnlyFateStore.TStatus.NEW).mutate());\n       assertEquals(0, client.createScanner(table).stream().count());\n-      var status = new FateMutatorImpl<>(context, table, fateId)\n+      var status = new FateMutatorImpl<>(context, table, fateId, writer)\n           .requireStatus(ReadOnlyFateStore.TStatus.values())\n           .putStatus(ReadOnlyFateStore.TStatus.NEW).tryMutate();\n       assertEquals(REJECTED, status);\n       assertEquals(0, client.createScanner(table).stream().count());\n \n-      // use require status without passing any statuses to require that the status column is absent\n-      status = new FateMutatorImpl<>(context, table, fateId).requireStatus()\n+      // use require status without passing any statuses to require that the status column is\n+      // absent\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).requireStatus()\n           .putStatus(ReadOnlyFateStore.TStatus.NEW).tryMutate();\n       assertEquals(ACCEPTED, status);\n \n-      // try again with requiring an absent status column. this time it should fail because we just\n+      // try again with requiring an absent status column. this time it should fail because we\n+      // just\n       // put status NEW\n       assertThrows(IllegalStateException.class,\n-          () -> new FateMutatorImpl<>(context, table, fateId).requireStatus()\n+          () -> new FateMutatorImpl<>(context, table, fateId, writer).requireStatus()\n               .putStatus(ReadOnlyFateStore.TStatus.NEW).mutate(),\n           \""Expected to not be able to use requireStatus() without passing any statuses\"");\n-      status = new FateMutatorImpl<>(context, table, fateId).requireStatus()\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).requireStatus()\n           .putStatus(ReadOnlyFateStore.TStatus.NEW).tryMutate();\n       assertEquals(REJECTED, status,\n           \""Expected to not be able to use requireStatus() without passing any statuses\"");\n \n       // now use require same with the current status, NEW passed in\n-      status =\n-          new FateMutatorImpl<>(context, table, fateId).requireStatus(ReadOnlyFateStore.TStatus.NEW)\n-              .putStatus(ReadOnlyFateStore.TStatus.SUBMITTED).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer)\n+          .requireStatus(ReadOnlyFateStore.TStatus.NEW)\n+          .putStatus(ReadOnlyFateStore.TStatus.SUBMITTED).tryMutate();\n       assertEquals(ACCEPTED, status);\n \n       // use require same with an array of statuses, none of which are the current status\n       // (SUBMITTED)\n       assertThrows(IllegalStateException.class,\n-          () -> new FateMutatorImpl<>(context, table, fateId)\n+          () -> new FateMutatorImpl<>(context, table, fateId, writer)\n               .requireStatus(ReadOnlyFateStore.TStatus.NEW, ReadOnlyFateStore.TStatus.UNKNOWN)\n               .putStatus(ReadOnlyFateStore.TStatus.SUBMITTED).mutate(),\n           \""Expected to not be able to use requireStatus() with statuses that do not match the current status\"");\n-      status = new FateMutatorImpl<>(context, table, fateId)\n+      status = new FateMutatorImpl<>(context, table, fateId, writer)\n           .requireStatus(ReadOnlyFateStore.TStatus.NEW, ReadOnlyFateStore.TStatus.UNKNOWN)\n           .putStatus(ReadOnlyFateStore.TStatus.SUBMITTED).tryMutate();\n       assertEquals(REJECTED, status,\n           \""Expected to not be able to use requireStatus() with statuses that do not match the current status\"");\n \n-      // use require same with an array of statuses, one of which is the current status (SUBMITTED)\n-      status = new FateMutatorImpl<>(context, table, fateId)\n+      // use require same with an array of statuses, one of which is the current status\n+      // (SUBMITTED)\n+      status = new FateMutatorImpl<>(context, table, fateId, writer)\n           .requireStatus(ReadOnlyFateStore.TStatus.UNKNOWN, ReadOnlyFateStore.TStatus.SUBMITTED)\n           .putStatus(ReadOnlyFateStore.TStatus.IN_PROGRESS).tryMutate();\n       assertEquals(ACCEPTED, status);\n \n       // one more time check that we can use require same with the current status (IN_PROGRESS)\n-      status = new FateMutatorImpl<>(context, table, fateId)\n+      status = new FateMutatorImpl<>(context, table, fateId, writer)\n           .requireStatus(ReadOnlyFateStore.TStatus.IN_PROGRESS)\n           .putStatus(ReadOnlyFateStore.TStatus.FAILED_IN_PROGRESS).tryMutate();\n       assertEquals(ACCEPTED, status);\n-\n     }\n-\n   }\n \n   @Test\n@@ -183,29 +204,33 @@ public void testReservations() throws Exception {\n       var reservation = FateReservation.from(lockID, UUID.randomUUID());\n       var wrongReservation = FateReservation.from(lockID, UUID.randomUUID());\n \n+      Supplier<ConditionalWriter> writer = createWriterSupplier(client, table);\n+\n       // Ensure that reserving is the only thing we can do\n-      var status =\n-          new FateMutatorImpl<>(context, table, fateId).putUnreserveTx(reservation).tryMutate();\n+      var status = new FateMutatorImpl<>(context, table, fateId, writer).putUnreserveTx(reservation)\n+          .tryMutate();\n       assertEquals(REJECTED, status);\n-      status = new FateMutatorImpl<>(context, table, fateId).putReservedTx(reservation).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).putReservedTx(reservation)\n+          .tryMutate();\n       assertEquals(ACCEPTED, status);\n \n       // Should not be able to reserve when it is already reserved\n-      status =\n-          new FateMutatorImpl<>(context, table, fateId).putReservedTx(wrongReservation).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).putReservedTx(wrongReservation)\n+          .tryMutate();\n       assertEquals(REJECTED, status);\n-      status = new FateMutatorImpl<>(context, table, fateId).putReservedTx(reservation).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).putReservedTx(reservation)\n+          .tryMutate();\n       assertEquals(REJECTED, status);\n \n       // Should be able to unreserve\n-      status = new FateMutatorImpl<>(context, table, fateId).putUnreserveTx(wrongReservation)\n-          .tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer)\n+          .putUnreserveTx(wrongReservation).tryMutate();\n       assertEquals(REJECTED, status);\n-      status =\n-          new FateMutatorImpl<>(context, table, fateId).putUnreserveTx(reservation).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).putUnreserveTx(reservation)\n+          .tryMutate();\n       assertEquals(ACCEPTED, status);\n-      status =\n-          new FateMutatorImpl<>(context, table, fateId).putUnreserveTx(reservation).tryMutate();\n+      status = new FateMutatorImpl<>(context, table, fateId, writer).putUnreserveTx(reservation)\n+          .tryMutate();\n       assertEquals(REJECTED, status);\n     }\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java\nindex 9e9387d6a98..264096e06d7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT_SimpleSuite.java\n@@ -24,6 +24,7 @@\n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.test.fate.FateExecutionOrderITBase;\n \n@@ -32,11 +33,11 @@ public class UserFateExecutionOrderIT_SimpleSuite extends FateExecutionOrderITBa\n   public void executeTest(FateTestExecutor<FeoTestEnv> testMethod, int maxDeferred,\n       AbstractFateStore.FateIdGenerator fateIdGenerator) throws Exception {\n     var table = getUniqueNames(1)[0];\n-    try (ClientContext client =\n-        (ClientContext) Accumulo.newClient().from(getClientProps()).build()) {\n+    try (ClientContext client = (ClientContext) Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<FeoTestEnv> fs = new UserFateStore<>(client, table, createDummyLockID(), null,\n+            maxDeferred, fateIdGenerator)) {\n       createFateTable(client, table);\n-      testMethod.execute(new UserFateStore<>(client, table, createDummyLockID(), null, maxDeferred,\n-          fateIdGenerator), getCluster().getServerContext());\n+      testMethod.execute(fs, getCluster().getServerContext());\n       client.tableOperations().delete(table);\n     }\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java\nindex cfed04bea92..0719768683f 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT_SimpleSuite.java\n@@ -34,6 +34,7 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.fate.AbstractFateStore.FateIdGenerator;\n import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.core.fate.user.schema.FateSchema;\n@@ -65,11 +66,11 @@ public static void teardown() {\n   public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n       FateIdGenerator fateIdGenerator) throws Exception {\n     table = getUniqueNames(1)[0];\n-    try (ClientContext client =\n-        (ClientContext) Accumulo.newClient().from(getClientProps()).build()) {\n+    try (ClientContext client = (ClientContext) Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<TestEnv> fs = new UserFateStore<>(client, table, createDummyLockID(), null,\n+            maxDeferred, fateIdGenerator)) {\n       createFateTable(client, table);\n-      testMethod.execute(new UserFateStore<>(client, table, createDummyLockID(), null, maxDeferred,\n-          fateIdGenerator), getCluster().getServerContext());\n+      testMethod.execute(fs, getCluster().getServerContext());\n     }\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java\nindex fca55dc04c6..7d84a72f8af 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateOpsCommandsIT.java\n@@ -40,9 +40,11 @@ public class UserFateOpsCommandsIT extends FateOpsCommandsITBase {\n   public void executeTest(FateTestExecutor<LatchTestEnv> testMethod, int maxDeferred,\n       AbstractFateStore.FateIdGenerator fateIdGenerator) throws Exception {\n     var context = getCluster().getServerContext();\n-    // the test should not be reserving or checking reservations, so null lockID and isLockHeld\n-    testMethod.execute(new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null),\n-        context);\n+    try (FateStore<LatchTestEnv> fs =\n+        new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null)) {\n+      // the test should not be reserving or checking reservations, so null lockID and isLockHeld\n+      testMethod.execute(fs, context);\n+    }\n   }\n \n   /**\n@@ -61,8 +63,10 @@ public void stopManagerAndExecuteTest(FateTestExecutor<LatchTestEnv> testMethod)\n       ZooUtil.LockID lockID = testLock.getLockID();\n       Predicate<ZooUtil.LockID> isLockHeld =\n           lock -> ServiceLock.isLockHeld(context.getZooCache(), lock);\n-      testMethod.execute(\n-          new UserFateStore<>(context, SystemTables.FATE.tableName(), lockID, isLockHeld), context);\n+      try (FateStore<LatchTestEnv> fs =\n+          new UserFateStore<>(context, SystemTables.FATE.tableName(), lockID, isLockHeld)) {\n+        testMethod.execute(fs, context);\n+      }\n     } finally {\n       if (testLock != null) {\n         testLock.unlock();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java\nindex 5c3030a6dfb..5c4d05d5160 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT_SimpleSuite.java\n@@ -24,6 +24,7 @@\n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.fate.FatePoolsWatcherITBase;\n@@ -48,11 +49,11 @@ public static void teardown() {\n   public void executeTest(FateTestExecutor<PoolResizeTestEnv> testMethod, int maxDeferred,\n       AbstractFateStore.FateIdGenerator fateIdGenerator) throws Exception {\n     table = getUniqueNames(1)[0];\n-    try (ClientContext client =\n-        (ClientContext) Accumulo.newClient().from(getClientProps()).build()) {\n+    try (ClientContext client = (ClientContext) Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<PoolResizeTestEnv> fs = new UserFateStore<>(client, table, createDummyLockID(),\n+            null, maxDeferred, fateIdGenerator)) {\n       createFateTable(client, table);\n-      testMethod.execute(new UserFateStore<>(client, table, createDummyLockID(), null, maxDeferred,\n-          fateIdGenerator), getCluster().getServerContext());\n+      testMethod.execute(fs, getCluster().getServerContext());\n     }\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java\nindex 48dc90ffe13..971d336cfe0 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT_SimpleSuite.java\n@@ -57,6 +57,7 @@ public void beforeEachSetup() throws Exception {\n \n   @AfterEach\n   public void afterEachTeardown() {\n+    store.close();\n     client.close();\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java\nindex c1f902a5a70..5072f0176be 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT_SimpleSuite.java\n@@ -30,6 +30,7 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.fate.AbstractFateStore.FateIdGenerator;\n import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n@@ -54,11 +55,11 @@ public static void teardown() {\n   public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n       FateIdGenerator fateIdGenerator) throws Exception {\n     String table = getUniqueNames(1)[0] + \""fatestore\"";\n-    try (ClientContext client =\n-        (ClientContext) Accumulo.newClient().from(getClientProps()).build()) {\n+    try (ClientContext client = (ClientContext) Accumulo.newClient().from(getClientProps()).build();\n+        FateStore<TestEnv> fs = new UserFateStore<>(client, table, createDummyLockID(), null,\n+            maxDeferred, fateIdGenerator)) {\n       createFateTable(client, table);\n-      testMethod.execute(new UserFateStore<>(client, table, createDummyLockID(), null, maxDeferred,\n-          fateIdGenerator), getCluster().getServerContext());\n+      testMethod.execute(fs, getCluster().getServerContext());\n     }\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java b/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\nindex 66355ccd725..5b7993403b3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\n@@ -256,8 +256,8 @@ public void getFateStatus() {\n       try {\n \n         var zk = context.getZooSession();\n-        MetaFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n-        UserFateStore<String> readOnlyUFS =\n+        ReadOnlyFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n+        ReadOnlyFateStore<String> readOnlyUFS =\n             new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null);\n         var lockPath = context.getServerPaths().createTableLocksPath(tableId);\n         Map<FateInstanceType,ReadOnlyFateStore<String>> readOnlyFateStores =\n@@ -348,7 +348,7 @@ private boolean lookupFateInZookeeper(final String tableName) throws KeeperExcep\n       log.trace(\""tid: {}\"", tableId);\n \n       var zk = context.getZooSession();\n-      MetaFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n+      ReadOnlyFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n       var lockPath = context.getServerPaths().createTableLocksPath(tableId);\n       AdminUtil.FateStatus fateStatus =\n           admin.getStatus(readOnlyMFS, zk, lockPath, null, null, null);\n@@ -378,7 +378,7 @@ private boolean lookupFateInAccumulo(final String tableName) throws KeeperExcept\n \n       log.trace(\""tid: {}\"", tableId);\n \n-      UserFateStore<String> readOnlyUFS =\n+      ReadOnlyFateStore<String> readOnlyUFS =\n           new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null);\n       AdminUtil.FateStatus fateStatus = admin.getStatus(readOnlyUFS, null, null, null);\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java b/test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java\nindex 2618b6c2f89..9f7e2df223d 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/FunctionalTestUtils.java\n@@ -231,9 +231,9 @@ private static FateStatus getFateStatus(AccumuloCluster cluster) {\n       AdminUtil<String> admin = new AdminUtil<>();\n       ServerContext context = cluster.getServerContext();\n       var zk = context.getZooSession();\n-      MetaFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n-      UserFateStore<String> readOnlyUFS =\n+      ReadOnlyFateStore<String> readOnlyUFS =\n           new UserFateStore<>(context, SystemTables.FATE.tableName(), null, null);\n+      ReadOnlyFateStore<String> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n       Map<FateInstanceType,ReadOnlyFateStore<String>> readOnlyFateStores =\n           Map.of(FateInstanceType.META, readOnlyMFS, FateInstanceType.USER, readOnlyUFS);\n       var lockPath = context.getServerPaths().createTableLocksPath();\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5608"", ""pr_id"": 5608, ""issue_id"": 3689, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Convert multi-assignments to separate assignments\nMost of our code uses separate assignment statements, such as:\r\n\r\n```java\r\nString a = \""something\"";\r\nString b = \""something else\"";\r\n```\r\n\r\nHowever, some of our code uses multi-assignments, like:\r\n\r\n```java\r\nString a = \""something\"", b = \""something else\"";\r\n```\r\n\r\nThis is less than ideal style, and these should be changed to separate assignments wherever it makes sense to do so, throughout the code. There may be some places, such as in for loop initialization, where we want to keep the multi-assignment, but most places we don't.\r\n"", ""issue_word_count"": 89, ""test_files_count"": 45, ""non_test_files_count"": 49, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/security/tokens/CredentialProviderToken.java"", ""core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java"", ""core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedInputStream.java"", ""core/src/main/java/org/apache/accumulo/core/data/Key.java"", ""core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java"", ""core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java"", ""core/src/main/java/org/apache/accumulo/core/util/DurationFormat.java"", ""core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java"", ""core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java"", ""core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java"", ""core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java"", ""core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/RetryTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java"", ""core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java"", ""hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/RangeInputSplit.java"", ""hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java"", ""minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java"", ""minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java"", ""minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/ServerDirs.java"", ""server/base/src/main/java/org/apache/accumulo/server/constraints/BulkFileColData.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/UserImpersonation.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManager.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyDistributor.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/Admin.java"", ""server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CleanUp.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/PopulateMetadataTable.java"", ""server/monitor/src/main/java/org/apache/accumulo/monitor/rest/SummaryInformation.java"", ""server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/ServerStat.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java"", ""shell/src/main/java/org/apache/accumulo/shell/Shell.java"", ""shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/AddSplitsCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/CompactCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/DeleteCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/DeleteIterCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/DeleteShellIterCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ExtensionCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/GetSplitsCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/GrantCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListIterCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListShellIterCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/NamespacesCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/PingCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/RevokeCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/SetIterCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java"", ""test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java"", ""test/src/main/java/org/apache/accumulo/harness/TestingKdc.java"", ""test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java"", ""test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java"", ""test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java"", ""test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java"", ""test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/ImportExportIT.java"", ""test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java"", ""test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java"", ""test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java"", ""test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java"", ""test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java"", ""test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java"", ""test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java"", ""test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java"", ""core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java"", ""core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java"", ""core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java"", ""core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/RetryTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java"", ""core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java"", ""hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java"", ""minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java"", ""test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java"", ""test/src/main/java/org/apache/accumulo/harness/TestingKdc.java"", ""test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java"", ""test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java"", ""test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java"", ""test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java"", ""test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/ImportExportIT.java"", ""test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java"", ""test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java"", ""test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java"", ""test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java"", ""test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java"", ""test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java"", ""test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java"", ""test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java""], ""base_commit"": ""53be0aadb061c6b0d4e2683bdfe182ab8b366042"", ""head_commit"": ""cd5900c2bff8c8926026274b59abaeb6728f1c7d"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5608"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5608"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-03T19:18:03.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/security/tokens/CredentialProviderToken.java b/core/src/main/java/org/apache/accumulo/core/client/security/tokens/CredentialProviderToken.java\nindex 66af59395b7..ed42f18f8cb 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/security/tokens/CredentialProviderToken.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/security/tokens/CredentialProviderToken.java\n@@ -34,8 +34,8 @@\n  * An {@link AuthenticationToken} backed by a Hadoop CredentialProvider.\n  */\n public class CredentialProviderToken extends PasswordToken {\n-  public static final String NAME_PROPERTY = \""name\"",\n-      CREDENTIAL_PROVIDERS_PROPERTY = \""credentialProviders\"";\n+  public static final String NAME_PROPERTY = \""name\"";\n+  public static final String CREDENTIAL_PROVIDERS_PROPERTY = \""credentialProviders\"";\n \n   private String name;\n   private String credentialProviders;\n@@ -104,8 +104,8 @@ public String getCredentialProviders() {\n \n   @Override\n   public void init(Properties properties) {\n-    char[] nameCharArray = properties.get(NAME_PROPERTY),\n-        credentialProvidersCharArray = properties.get(CREDENTIAL_PROVIDERS_PROPERTY);\n+    char[] nameCharArray = properties.get(NAME_PROPERTY);\n+    char[] credentialProvidersCharArray = properties.get(CREDENTIAL_PROVIDERS_PROPERTY);\n     if (nameCharArray != null && credentialProvidersCharArray != null) {\n       try {\n         this.setWithCredentialProviders(new String(nameCharArray),\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java b/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\nindex 857f00b245f..576e57ebb46 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\n@@ -334,8 +334,10 @@ public boolean test(final String input) {\n \n   private static class Bounds implements Predicate<String> {\n \n-    private final long lowerBound, upperBound;\n-    private final boolean lowerInclusive, upperInclusive;\n+    private final long lowerBound;\n+    private final long upperBound;\n+    private final boolean lowerInclusive;\n+    private final boolean upperInclusive;\n \n     public Bounds(final long lowerBound, final long upperBound) {\n       this(lowerBound, true, upperBound, true);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedInputStream.java b/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedInputStream.java\nindex f3dcceac3f2..bd5095320e6 100644\n--- a/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedInputStream.java\n+++ b/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedInputStream.java\n@@ -31,7 +31,8 @@ public class BlockedInputStream extends InputStream {\n   byte[] array;\n   // ReadPos is where to start reading\n   // WritePos is the last position written to\n-  int readPos, writePos;\n+  int readPos;\n+  int writePos;\n   DataInputStream in;\n   int blockSize;\n   boolean finished = false;\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/data/Key.java b/core/src/main/java/org/apache/accumulo/core/data/Key.java\nindex 6344d4bb707..10359591d5d 100644\n--- a/core/src/main/java/org/apache/accumulo/core/data/Key.java\n+++ b/core/src/main/java/org/apache/accumulo/core/data/Key.java\n@@ -565,9 +565,20 @@ public Key(CharSequence row, CharSequence cf, CharSequence cq, ColumnVisibility\n    * @since 3.1.0\n    */\n   public Key(ByteSequence row, ByteSequence cf, ByteSequence cq, ByteSequence cv, long ts) {\n-    byte[] rowBytes, cfBytes, cqBytes, cvBytes;\n-    int rowOffset, cfOffset, cqOffset, cvOffset;\n-    int rowLen, cfLen, cqLen, cvLen;\n+    byte[] rowBytes;\n+    byte[] cfBytes;\n+    byte[] cqBytes;\n+    byte[] cvBytes;\n+\n+    int rowOffset;\n+    int cfOffset;\n+    int cqOffset;\n+    int cvOffset;\n+\n+    int rowLen;\n+    int cfLen;\n+    int cqLen;\n+    int cvLen;\n \n     if (row.isBackedByArray()) {\n       rowBytes = row.getBackingArray();\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java b/core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java\nindex f1846d02339..37dd5a7a7b0 100644\n--- a/core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java\n+++ b/core/src/main/java/org/apache/accumulo/core/file/rfile/RelativeKey.java\n@@ -172,7 +172,10 @@ public void readFields(DataInput in) throws IOException {\n       fieldsPrefixed = 0;\n     }\n \n-    final byte[] row, cf, cq, cv;\n+    final byte[] row;\n+    final byte[] cf;\n+    final byte[] cq;\n+    final byte[] cv;\n     final long ts;\n \n     row = getData(in, ROW_SAME, ROW_COMMON_PREFIX, () -> prevKey.getRowData());\n@@ -219,8 +222,15 @@ public static SkippR fastSkip(DataInput in, Key seekKey, ArrayByteSequence value\n       Key currKey, int entriesLeft) throws IOException {\n     // this method mostly avoids object allocation and only does compares when the row changes\n \n-    ArrayByteSequence row, cf, cq, cv;\n-    ArrayByteSequence prow, pcf, pcq, pcv;\n+    ArrayByteSequence row;\n+    ArrayByteSequence cf;\n+    ArrayByteSequence cq;\n+    ArrayByteSequence cv;\n+\n+    ArrayByteSequence prow;\n+    ArrayByteSequence pcf;\n+    ArrayByteSequence pcq;\n+    ArrayByteSequence pcv;\n \n     ByteSequence stopRow = seekKey.getRowData();\n     ByteSequence stopCF = seekKey.getColumnFamilyData();\n@@ -230,7 +240,9 @@ public static SkippR fastSkip(DataInput in, Key seekKey, ArrayByteSequence value\n     long pts = -1;\n     boolean pdel = false;\n \n-    int rowCmp = -1, cfCmp = -1, cqCmp = -1;\n+    int rowCmp = -1;\n+    int cfCmp = -1;\n+    int cqCmp = -1;\n \n     if (currKey != null) {\n \n@@ -397,7 +409,10 @@ public static SkippR fastSkip(DataInput in, Key seekKey, ArrayByteSequence value\n     }\n \n     if (count > 1) {\n-      ArrayByteSequence trow, tcf, tcq, tcv;\n+      ArrayByteSequence trow;\n+      ArrayByteSequence tcf;\n+      ArrayByteSequence tcq;\n+      ArrayByteSequence tcv;\n       long tts;\n \n       // when the current keys field is same as the last, then\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java b/core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java\nindex 44c58923c1e..e7792e3b8d4 100644\n--- a/core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java\n+++ b/core/src/main/java/org/apache/accumulo/core/rpc/ThriftUtil.java\n@@ -58,7 +58,8 @@ public class ThriftUtil {\n       new AccumuloTFramedTransportFactory(Integer.MAX_VALUE);\n   private static final Map<Integer,TTransportFactory> factoryCache = new HashMap<>();\n \n-  public static final String GSSAPI = \""GSSAPI\"", DIGEST_MD5 = \""DIGEST-MD5\"";\n+  public static final String GSSAPI = \""GSSAPI\"";\n+  public static final String DIGEST_MD5 = \""DIGEST-MD5\"";\n \n   private static final int RELOGIN_MAX_BACKOFF = 5000;\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/DurationFormat.java b/core/src/main/java/org/apache/accumulo/core/util/DurationFormat.java\nindex 16aff987c3c..2a914490b26 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/DurationFormat.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/DurationFormat.java\n@@ -23,7 +23,12 @@ public class DurationFormat {\n \n   public DurationFormat(long time, String space) {\n     String dash = \""-\"";\n-    long ms, sec, min, hr, day, yr;\n+    long ms;\n+    long sec;\n+    long min;\n+    long hr;\n+    long day;\n+    long yr;\n \n     if (time == 0) {\n       str = dash;\n\ndiff --git a/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/RangeInputSplit.java b/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/RangeInputSplit.java\nindex 6aebc89d53b..d6fc0ce8f1e 100644\n--- a/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/RangeInputSplit.java\n+++ b/hadoop-mapreduce/src/main/java/org/apache/accumulo/hadoopImpl/mapreduce/RangeInputSplit.java\n@@ -48,8 +48,11 @@\n public class RangeInputSplit extends InputSplit implements Writable {\n   private Range range;\n   private String[] locations;\n-  private String tableId, tableName;\n-  private Boolean offline, isolatedScan, localIterators;\n+  private String tableId;\n+  private String tableName;\n+  private Boolean offline;\n+  private Boolean isolatedScan;\n+  private Boolean localIterators;\n   private Set<IteratorSetting.Column> fetchedColumns;\n   private List<IteratorSetting> iterators;\n   private SamplerConfiguration samplerConfig;\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\nindex c9a62008f09..65090c21211 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\n@@ -60,7 +60,10 @@ public class StandaloneAccumuloCluster implements AccumuloCluster {\n           ServerType.GARBAGE_COLLECTOR, ServerType.MONITOR));\n \n   private final ClientInfo info;\n-  private String accumuloHome, clientAccumuloConfDir, serverAccumuloConfDir, hadoopConfDir;\n+  private String accumuloHome;\n+  private String clientAccumuloConfDir;\n+  private String serverAccumuloConfDir;\n+  private String hadoopConfDir;\n   private final Path tmp;\n   private final List<ClusterUser> users;\n   private String clientCmdPrefix;\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java\nindex 6a39e384ddf..5ba36cca8f1 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControl.java\n@@ -51,10 +51,12 @@\n public class StandaloneClusterControl implements ClusterControl {\n   private static final Logger log = LoggerFactory.getLogger(StandaloneClusterControl.class);\n \n-  private static final String ACCUMULO_SERVICE_SCRIPT = \""accumulo-service\"",\n-      ACCUMULO_SCRIPT = \""accumulo\"";\n-  private static final String MANAGER_HOSTS_FILE = \""managers\"", GC_HOSTS_FILE = \""gc\"",\n-      TSERVER_HOSTS_FILE = \""tservers\"", MONITOR_HOSTS_FILE = \""monitor\"";\n+  private static final String ACCUMULO_SERVICE_SCRIPT = \""accumulo-service\"";\n+  private static final String ACCUMULO_SCRIPT = \""accumulo\"";\n+  private static final String MANAGER_HOSTS_FILE = \""managers\"";\n+  private static final String GC_HOSTS_FILE = \""gc\"";\n+  private static final String TSERVER_HOSTS_FILE = \""tservers\"";\n+  private static final String MONITOR_HOSTS_FILE = \""monitor\"";\n \n   String accumuloHome;\n   String clientAccumuloConfDir;\n@@ -63,7 +65,8 @@ public class StandaloneClusterControl implements ClusterControl {\n   private String serverCmdPrefix;\n   protected RemoteShellOptions options;\n \n-  protected String accumuloServicePath, accumuloPath;\n+  protected String accumuloServicePath;\n+  protected String accumuloPath;\n \n   @SuppressFBWarnings(value = \""PATH_TRAVERSAL_IN\"",\n       justification = \""code runs in same security context as user who provided input file name\"")\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/ServerDirs.java b/server/base/src/main/java/org/apache/accumulo/server/ServerDirs.java\nindex 062198ba4b8..2eecf88c80a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/ServerDirs.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/ServerDirs.java\n@@ -163,7 +163,8 @@ private Map<Path,Path> loadVolumeReplacements() {\n             Property.INSTANCE_VOLUMES_REPLACEMENTS.getKey() + \"" contains malformed pair \"" + pair);\n       }\n \n-      Path p1, p2;\n+      Path p1;\n+      Path p2;\n       try {\n         // URI constructor handles hex escaping\n         p1 = new Path(new URI(VolumeUtil.removeTrailingSlash(uris[0].trim())));\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/constraints/BulkFileColData.java b/server/base/src/main/java/org/apache/accumulo/server/constraints/BulkFileColData.java\nindex 6533f8a19ae..ff160d0a7cc 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/constraints/BulkFileColData.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/constraints/BulkFileColData.java\n@@ -27,8 +27,10 @@\n  * Data needed to validate a BulkFileColumn update\n  */\n class BulkFileColData {\n-  private boolean isSplitMutation, isLocationMutation;\n-  private final Set<StoredTabletFile> dataFiles, loadedFiles;\n+  private boolean isSplitMutation;\n+  private boolean isLocationMutation;\n+  private final Set<StoredTabletFile> dataFiles;\n+  private final Set<StoredTabletFile> loadedFiles;\n   private final Set<String> tidsSeen;\n \n   BulkFileColData() {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java b/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\nindex d641c417d97..f76ae15fa93 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\n@@ -220,8 +220,8 @@ private void checkSASL(InitialConfiguration initConfig)\n       final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n       // We don't have any valid creds to talk to HDFS\n       if (!ugi.hasKerberosCredentials()) {\n-        final String accumuloKeytab = initConfig.get(Property.GENERAL_KERBEROS_KEYTAB),\n-            accumuloPrincipal = initConfig.get(Property.GENERAL_KERBEROS_PRINCIPAL);\n+        final String accumuloKeytab = initConfig.get(Property.GENERAL_KERBEROS_KEYTAB);\n+        final String accumuloPrincipal = initConfig.get(Property.GENERAL_KERBEROS_PRINCIPAL);\n \n         // Fail if the site configuration doesn't contain appropriate credentials\n         if (StringUtils.isBlank(accumuloKeytab) || StringUtils.isBlank(accumuloPrincipal)) {\n@@ -309,7 +309,8 @@ private String getInstanceNamePrefix() {\n   private String getInstanceNamePath(ZooReaderWriter zoo, Opts opts)\n       throws KeeperException, InterruptedException {\n     // set up the instance name\n-    String instanceName, instanceNamePath = null;\n+    String instanceName;\n+    String instanceNamePath = null;\n     boolean exists = true;\n     do {\n       if (opts.cliInstanceName == null) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\nindex fd37e171a0b..09f28e25043 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n@@ -479,7 +479,8 @@ private static ServerAddress createSaslThreadPoolServer(HostAndPort address, TPr\n \n     TServerSocket transport = new TServerSocket(args);\n \n-    String hostname, fqdn;\n+    String hostname;\n+    String fqdn;\n     try {\n       hostname = InetAddress.getByName(address.getHost()).getCanonicalHostName();\n       fqdn = InetAddress.getLocalHost().getCanonicalHostName();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/UserImpersonation.java b/server/base/src/main/java/org/apache/accumulo/server/security/UserImpersonation.java\nindex 7c866dab515..c6acbcc6ab5 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/UserImpersonation.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/UserImpersonation.java\n@@ -120,8 +120,10 @@ public void clear() {\n   }\n \n   public static class UsersWithHosts {\n-    private Set<String> users = new HashSet<>(), hosts = new HashSet<>();\n-    private boolean allUsers, allHosts;\n+    private Set<String> users = new HashSet<>();\n+    private Set<String> hosts = new HashSet<>();\n+    private boolean allUsers;\n+    private boolean allHosts;\n \n     public UsersWithHosts() {\n       allUsers = allHosts = false;\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManager.java b/server/base/src/main/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManager.java\nindex 73816d2bbf2..e3805bf20d0 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManager.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManager.java\n@@ -39,7 +39,8 @@ public class AuthenticationTokenKeyManager implements Runnable {\n   private final long keyUpdateInterval;\n   private final long tokenMaxLifetime;\n   private int idSeq = 0;\n-  private volatile boolean keepRunning = true, initialized = false;\n+  private volatile boolean keepRunning = true;\n+  private volatile boolean initialized = false;\n \n   /**\n    * Construct the key manager which will generate new AuthenticationKeys to generate and verify\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyDistributor.java b/server/base/src/main/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyDistributor.java\nindex 47ecefa9f88..253cc6d5a8a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyDistributor.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyDistributor.java\n@@ -69,7 +69,8 @@ public synchronized void initialize() throws KeeperException, InterruptedExcepti\n     if (zrw.exists(baseNode)) {\n       List<ACL> acls = zrw.getACL(baseNode);\n       if (acls.size() == 1) {\n-        ACL actualAcl = acls.get(0), expectedAcl = ZooUtil.PRIVATE.get(0);\n+        ACL actualAcl = acls.get(0);\n+        ACL expectedAcl = ZooUtil.PRIVATE.get(0);\n         Id actualId = actualAcl.getId();\n         // The expected outcome from ZooUtil.PRIVATE\n         if (actualAcl.getPerms() == expectedAcl.getPerms() && actualId.getScheme().equals(\""digest\"")\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\nindex b01ffcb82fb..54ba60ddd5b 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n@@ -757,7 +757,8 @@ static String qualifyWithZooKeeperSessionId(ClientContext context, ZooCache zooC\n       new MessageFormat(\""setauths -u {0} -s {1}\\n\"");\n \n   private DefaultConfiguration defaultConfig;\n-  private Map<String,String> siteConfig, systemConfig;\n+  private Map<String,String> siteConfig;\n+  private Map<String,String> systemConfig;\n   private List<String> localUsers;\n \n   public void printConfig(ClientContext context, DumpConfigCommand opts) throws Exception {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CleanUp.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CleanUp.java\nindex ce2a5f1ab37..6ddad0e44fd 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CleanUp.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CleanUp.java\n@@ -71,7 +71,10 @@ public long isReady(FateId fateId, Manager manager) throws Exception {\n       }\n     };\n \n-    long t1, t2, submitted = 0, total = 0;\n+    long t1;\n+    long t2;\n+    long submitted = 0;\n+    long total = 0;\n \n     try (\n         var tablets = ample.readTablets().forTable(tableId).overlapping(startRow, endRow)\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/PopulateMetadataTable.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/PopulateMetadataTable.java\nindex 0f1b65dc310..36944987cdd 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/PopulateMetadataTable.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/tableImport/PopulateMetadataTable.java\n@@ -78,7 +78,8 @@ static void readMappingFile(VolumeManager fs, ImportedTableInfo tableInfo, Strin\n     try (var fsDis = fs.open(new Path(importDir, IMPORT_MAPPINGS_FILE));\n         var isr = new InputStreamReader(fsDis, UTF_8);\n         BufferedReader in = new BufferedReader(isr)) {\n-      String line, prev;\n+      String line;\n+      String prev;\n       while ((line = in.readLine()) != null) {\n         String[] sa = line.split(\"":\"", 2);\n         prev = fileNameMappings.put(sa[0], importDir + \""/\"" + sa[1]);\n\ndiff --git a/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/SummaryInformation.java b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/SummaryInformation.java\nindex 79d59df597a..3880c2e79f4 100644\n--- a/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/SummaryInformation.java\n+++ b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/SummaryInformation.java\n@@ -42,7 +42,8 @@ public class SummaryInformation {\n   // Variable names become JSON keys\n   public List<TabletServer> servers = new ArrayList<>();\n \n-  public String managerGoalState, managerState;\n+  public String managerGoalState;\n+  public String managerState;\n \n   public BadTabletServers badTabletServers;\n   public ServersShuttingDown tabletServersShuttingDown;\n\ndiff --git a/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/ServerStat.java b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/ServerStat.java\nindex fc19a18b7af..7b4d4af22ba 100644\n--- a/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/ServerStat.java\n+++ b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/tservers/ServerStat.java\n@@ -29,7 +29,8 @@ public class ServerStat {\n   public int max;\n   public boolean adjustMax;\n   public float significance;\n-  public String description, name;\n+  public String description;\n+  public String name;\n   public boolean derived;\n \n   public ServerStat() {}\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java\nindex 9dd1229fc54..cae47debefe 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java\n@@ -583,7 +583,8 @@ boolean initiateMinorCompaction(long flushId, MinorCompactionReason mincReason)\n   private MinorCompactionTask createMinorCompactionTask(long flushId,\n       MinorCompactionReason mincReason) {\n     MinorCompactionTask mct;\n-    long t1, t2;\n+    long t1;\n+    long t2;\n \n     StringBuilder logMessage = null;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/Shell.java b/shell/src/main/java/org/apache/accumulo/shell/Shell.java\nindex 2c634f8d856..78947874331 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/Shell.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/Shell.java\n@@ -1107,7 +1107,8 @@ public void checkTableState() {\n \n   private void printConstraintViolationException(ConstraintViolationException cve) {\n     printException(cve, \""\"");\n-    int COL1 = 50, COL2 = 14;\n+    int COL1 = 50;\n+    int COL2 = 14;\n     int col3 = Math.max(1, Math.min(Integer.MAX_VALUE, terminal.getWidth() - COL1 - COL2 - 6));\n     logError(String.format(\""%\"" + COL1 + \""s-+-%\"" + COL2 + \""s-+-%\"" + col3 + \""s%n\"", repeat(\""-\"", COL1),\n         repeat(\""-\"", COL2), repeat(\""-\"", col3)));\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java b/shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java\nindex 31111cabf26..12c4c40d638 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/ShellCompletor.java\n@@ -61,7 +61,9 @@ public void complete(LineReader reader, ParsedLine line, List<Candidate> candida\n   }\n \n   private int _complete(String fullBuffer, int cursor, List<String> candidates) {\n-    boolean inTableFlag = false, inUserFlag = false, inNamespaceFlag = false;\n+    boolean inTableFlag = false;\n+    boolean inUserFlag = false;\n+    boolean inNamespaceFlag = false;\n     // Only want to grab the buffer up to the cursor because\n     // the user could be trying to tab complete in the middle\n     // of the line\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/AddSplitsCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/AddSplitsCommand.java\nindex 84119f3dad0..ca7e6a45536 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/AddSplitsCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/AddSplitsCommand.java\n@@ -31,7 +31,8 @@\n import org.apache.hadoop.io.Text;\n \n public class AddSplitsCommand extends Command {\n-  private Option optSplitsFile, base64Opt;\n+  private Option optSplitsFile;\n+  private Option base64Opt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/CompactCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/CompactCommand.java\nindex 9b09365a498..363475862ed 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/CompactCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/CompactCommand.java\n@@ -39,13 +39,30 @@\n import org.apache.commons.cli.Options;\n \n public class CompactCommand extends TableOperation {\n-  private Option noFlushOption, waitOpt, profileOpt, cancelOpt;\n+  private Option noFlushOption;\n+  private Option waitOpt;\n+  private Option profileOpt;\n+  private Option cancelOpt;\n \n   // file selection and file output options\n-  private Option enameOption, epathOption, sizeLtOption, sizeGtOption, minFilesOption,\n-      outBlockSizeOpt, outHdfsBlockSizeOpt, outIndexBlockSizeOpt, outCompressionOpt, outReplication,\n-      enoSampleOption, extraSummaryOption, enoSummaryOption, hintsOption, configurerOpt,\n-      configurerConfigOpt, selectorOpt, selectorConfigOpt;\n+  private Option enameOption;\n+  private Option epathOption;\n+  private Option sizeLtOption;\n+  private Option sizeGtOption;\n+  private Option minFilesOption;\n+  private Option outBlockSizeOpt;\n+  private Option outHdfsBlockSizeOpt;\n+  private Option outIndexBlockSizeOpt;\n+  private Option outCompressionOpt;\n+  private Option outReplication;\n+  private Option enoSampleOption;\n+  private Option extraSummaryOption;\n+  private Option enoSummaryOption;\n+  private Option hintsOption;\n+  private Option configurerOpt;\n+  private Option configurerConfigOpt;\n+  private Option selectorOpt;\n+  private Option selectorConfigOpt;\n \n   private CompactionConfig compactionConfig = null;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java\nindex 9be5d8fb5e0..3e4a5f6dd08 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ConfigCommand.java\n@@ -53,10 +53,18 @@\n import com.google.common.collect.ImmutableSortedMap;\n \n public class ConfigCommand extends Command {\n-  private Option tableOpt, deleteOpt, setOpt, forceOpt, filterOpt, filterWithValuesOpt,\n-      disablePaginationOpt, outputFileOpt, namespaceOpt;\n-\n-  private int COL1 = 10, COL2 = 7;\n+  private Option tableOpt;\n+  private Option deleteOpt;\n+  private Option setOpt;\n+  private Option forceOpt;\n+  private Option filterOpt;\n+  private Option filterWithValuesOpt;\n+  private Option disablePaginationOpt;\n+  private Option outputFileOpt;\n+  private Option namespaceOpt;\n+\n+  private int COL1 = 10;\n+  private int COL2 = 7;\n   private LineReader reader;\n \n   @Override\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java\nindex 9a768bf92c9..37e43fa8783 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/DUCommand.java\n@@ -50,7 +50,9 @@\n  */\n public class DUCommand extends Command {\n \n-  private Option optTablePattern, optHumanReadble, optNamespace;\n+  private Option optTablePattern;\n+  private Option optHumanReadble;\n+  private Option optNamespace;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteCommand.java\nindex 34917247580..e5d9b2a68db 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteCommand.java\n@@ -38,7 +38,8 @@\n import org.apache.hadoop.io.Text;\n \n public class DeleteCommand extends Command {\n-  private Option deleteOptAuths, timestampOpt;\n+  private Option deleteOptAuths;\n+  private Option timestampOpt;\n   private Option timeoutOption;\n \n   protected long getTimeout(final CommandLine cl) {\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteIterCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteIterCommand.java\nindex c90e1f45bf9..b4fff1e0498 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteIterCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteIterCommand.java\n@@ -29,7 +29,11 @@\n import org.apache.commons.cli.Options;\n \n public class DeleteIterCommand extends Command {\n-  private Option allScopeOpt, mincScopeOpt, majcScopeOpt, scanScopeOpt, nameOpt;\n+  private Option allScopeOpt;\n+  private Option mincScopeOpt;\n+  private Option majcScopeOpt;\n+  private Option scanScopeOpt;\n+  private Option nameOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteShellIterCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteShellIterCommand.java\nindex a6f6f0082d2..8965a4316d2 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteShellIterCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/DeleteShellIterCommand.java\n@@ -30,7 +30,9 @@\n import org.apache.commons.cli.Options;\n \n public class DeleteShellIterCommand extends Command {\n-  private Option nameOpt, allOpt, profileOpt;\n+  private Option nameOpt;\n+  private Option allOpt;\n+  private Option profileOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ExtensionCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ExtensionCommand.java\nindex b48403659d9..e0f9f7d23db 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ExtensionCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ExtensionCommand.java\n@@ -32,7 +32,9 @@\n \n public class ExtensionCommand extends Command {\n \n-  protected Option enable, disable, list;\n+  protected Option enable;\n+  protected Option disable;\n+  protected Option list;\n \n   private ServiceLoader<ShellExtension> extensions = null;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/GetSplitsCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/GetSplitsCommand.java\nindex 4a858d0aec1..e0bba10e552 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/GetSplitsCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/GetSplitsCommand.java\n@@ -47,7 +47,10 @@\n \n public class GetSplitsCommand extends Command {\n \n-  private Option outputFileOpt, maxSplitsOpt, base64Opt, verboseOpt;\n+  private Option outputFileOpt;\n+  private Option maxSplitsOpt;\n+  private Option base64Opt;\n+  private Option verboseOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/GrantCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/GrantCommand.java\nindex b7021387d6d..023041bde46 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/GrantCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/GrantCommand.java\n@@ -39,7 +39,8 @@ public class GrantCommand extends TableOperation {\n     disableUnflaggedTableOptions();\n   }\n \n-  private Option systemOpt, userOpt;\n+  private Option systemOpt;\n+  private Option userOpt;\n   private String user;\n   private String[] permission;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java\nindex 5b183b8f044..2b76d338bdb 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/InsertCommand.java\n@@ -47,7 +47,8 @@\n import org.apache.hadoop.io.Text;\n \n public class InsertCommand extends Command {\n-  private Option insertOptAuths, timestampOpt;\n+  private Option insertOptAuths;\n+  private Option timestampOpt;\n   private Option timeoutOption;\n   private Option durabilityOption;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java\nindex 2f6625bbe1a..117747c4958 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListBulkCommand.java\n@@ -30,7 +30,8 @@\n \n public class ListBulkCommand extends Command {\n \n-  private Option tserverOption, disablePaginationOpt;\n+  private Option tserverOption;\n+  private Option disablePaginationOpt;\n \n   @Override\n   public String description() {\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\nindex 2aec7dfce68..8f6f3607e62 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\n@@ -34,7 +34,11 @@\n \n public class ListCompactionsCommand extends Command {\n \n-  private Option serverOpt, tserverOption, rgOpt, disablePaginationOpt, filterOption;\n+  private Option serverOpt;\n+  private Option tserverOption;\n+  private Option rgOpt;\n+  private Option disablePaginationOpt;\n+  private Option filterOption;\n \n   @Override\n   public String description() {\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListIterCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListIterCommand.java\nindex 588064f7af1..3075551eb3c 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListIterCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListIterCommand.java\n@@ -35,7 +35,8 @@\n import org.apache.commons.cli.Options;\n \n public class ListIterCommand extends Command {\n-  private Option nameOpt, allScopesOpt;\n+  private Option nameOpt;\n+  private Option allScopesOpt;\n   private Map<IteratorScope,Option> scopeOpts;\n \n   @Override\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\nindex dbda36a8c0b..3daf90826a3 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n@@ -43,7 +43,10 @@\n \n public class ListScansCommand extends Command {\n \n-  private Option serverOpt, tserverOption, rgOpt, disablePaginationOpt;\n+  private Option serverOpt;\n+  private Option tserverOption;\n+  private Option rgOpt;\n+  private Option disablePaginationOpt;\n \n   @Override\n   public String description() {\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListShellIterCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListShellIterCommand.java\nindex 70f881ea138..42f08cc0128 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListShellIterCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListShellIterCommand.java\n@@ -31,7 +31,8 @@\n \n public class ListShellIterCommand extends Command {\n \n-  private Option nameOpt, profileOpt;\n+  private Option nameOpt;\n+  private Option profileOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java\nindex d6a74fce231..846b1f73d86 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/MergeCommand.java\n@@ -28,7 +28,10 @@\n import org.apache.hadoop.io.Text;\n \n public class MergeCommand extends Command {\n-  private Option verboseOpt, forceOpt, sizeOpt, allOpt;\n+  private Option verboseOpt;\n+  private Option forceOpt;\n+  private Option sizeOpt;\n+  private Option allOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/NamespacesCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/NamespacesCommand.java\nindex 2298b735d7b..e298886f9e1 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/NamespacesCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/NamespacesCommand.java\n@@ -35,7 +35,8 @@\n import com.google.common.collect.Iterators;\n \n public class NamespacesCommand extends Command {\n-  private Option disablePaginationOpt, namespaceIdOption;\n+  private Option disablePaginationOpt;\n+  private Option namespaceIdOption;\n \n   private static final String DEFAULT_NAMESPACE_DISPLAY_NAME = \""\\\""\\\""\"";\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/PingCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/PingCommand.java\nindex fb73bd34d79..a2f8a0b0c3e 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/PingCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/PingCommand.java\n@@ -31,7 +31,9 @@\n \n public class PingCommand extends Command {\n \n-  private Option serverOption, tserverOption, disablePaginationOpt;\n+  private Option serverOption;\n+  private Option tserverOption;\n+  private Option disablePaginationOpt;\n \n   @Override\n   public String description() {\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/RevokeCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/RevokeCommand.java\nindex 5fb2fe4d650..4743d26a780 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/RevokeCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/RevokeCommand.java\n@@ -39,7 +39,8 @@ public class RevokeCommand extends TableOperation {\n     disableUnflaggedTableOptions();\n   }\n \n-  private Option systemOpt, userOpt;\n+  private Option systemOpt;\n+  private Option userOpt;\n   private String user;\n   private String[] permission;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java\nindex 11570b1d908..dbcd19c1a14 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ScanCommand.java\n@@ -56,8 +56,13 @@\n \n public class ScanCommand extends Command {\n \n-  private Option scanOptAuths, scanOptRow, scanOptColumns, disablePaginationOpt, outputFileOpt,\n-      scanOptCf, scanOptCq;\n+  private Option scanOptAuths;\n+  private Option scanOptRow;\n+  private Option scanOptColumns;\n+  private Option disablePaginationOpt;\n+  private Option outputFileOpt;\n+  private Option scanOptCf;\n+  private Option scanOptCq;\n \n   protected Option showFewOpt;\n   protected Option timestampOpt;\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/SetIterCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/SetIterCommand.java\nindex 40816fa6519..686a1e3ff92 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/SetIterCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/SetIterCommand.java\n@@ -53,9 +53,18 @@\n \n public class SetIterCommand extends Command {\n \n-  private Option allScopeOpt, mincScopeOpt, majcScopeOpt, scanScopeOpt;\n-  Option profileOpt, priorityOpt, nameOpt;\n-  Option ageoffTypeOpt, regexTypeOpt, versionTypeOpt, reqvisTypeOpt, classnameTypeOpt;\n+  private Option allScopeOpt;\n+  private Option mincScopeOpt;\n+  private Option majcScopeOpt;\n+  private Option scanScopeOpt;\n+  Option profileOpt;\n+  Option priorityOpt;\n+  Option nameOpt;\n+  Option ageoffTypeOpt;\n+  Option regexTypeOpt;\n+  Option versionTypeOpt;\n+  Option reqvisTypeOpt;\n+  Option classnameTypeOpt;\n \n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java\nindex 06e3dcd19b2..7f567700662 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ShellPluginConfigurationCommand.java\n@@ -36,7 +36,9 @@\n import org.apache.commons.cli.ParseException;\n \n public abstract class ShellPluginConfigurationCommand extends Command {\n-  private Option removePluginOption, pluginClassOption, listPluginOption;\n+  private Option removePluginOption;\n+  private Option pluginClassOption;\n+  private Option listPluginOption;\n \n   private String pluginType;\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java b/shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java\nindex cf17dcb3c49..02641531b2c 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/TableOperation.java\n@@ -39,7 +39,9 @@\n \n public abstract class TableOperation extends Command {\n \n-  protected Option optTablePattern, optTableName, optNamespace;\n+  protected Option optTablePattern;\n+  protected Option optTableName;\n+  protected Option optNamespace;\n   private boolean force = true;\n   private boolean useCommandLine = true;\n \n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java b/core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java\nindex 2b72d3cac36..bc334c9da5a 100644\n--- a/core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/client/BatchWriterConfigTest.java\n@@ -180,7 +180,8 @@ public void testSerialize() throws IOException {\n \n   @Test\n   public void testDefaultEquality() {\n-    BatchWriterConfig cfg1 = new BatchWriterConfig(), cfg2 = new BatchWriterConfig();\n+    BatchWriterConfig cfg1 = new BatchWriterConfig();\n+    BatchWriterConfig cfg2 = new BatchWriterConfig();\n     assertEquals(cfg1, cfg2);\n     assertEquals(cfg1.hashCode(), cfg2.hashCode());\n     cfg2.setMaxMemory(1);\n@@ -193,7 +194,8 @@ public void testDefaultEquality() {\n \n   @Test\n   public void testManualEquality() {\n-    BatchWriterConfig cfg1 = new BatchWriterConfig(), cfg2 = new BatchWriterConfig();\n+    BatchWriterConfig cfg1 = new BatchWriterConfig();\n+    BatchWriterConfig cfg2 = new BatchWriterConfig();\n     cfg1.setMaxLatency(10, SECONDS);\n     cfg2.setMaxLatency(10000, MILLISECONDS);\n \n@@ -210,7 +212,9 @@ public void testManualEquality() {\n \n   @Test\n   public void testMerge() {\n-    BatchWriterConfig cfg1 = new BatchWriterConfig(), cfg2 = new BatchWriterConfig();\n+    BatchWriterConfig cfg1 = new BatchWriterConfig();\n+    BatchWriterConfig cfg2 = new BatchWriterConfig();\n+\n     cfg1.setMaxMemory(1234);\n     cfg2.setMaxMemory(5858);\n     cfg2.setDurability(Durability.LOG);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java b/core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java\nindex 5da5e3793ca..d67d5f62437 100644\n--- a/core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/client/admin/DelegationTokenConfigTest.java\n@@ -30,8 +30,8 @@ public class DelegationTokenConfigTest {\n \n   @Test\n   public void testTimeUnit() {\n-    DelegationTokenConfig config1 = new DelegationTokenConfig(),\n-        config2 = new DelegationTokenConfig();\n+    DelegationTokenConfig config1 = new DelegationTokenConfig();\n+    DelegationTokenConfig config2 = new DelegationTokenConfig();\n \n     config1.setTokenLifetime(1000, MILLISECONDS);\n     config2.setTokenLifetime(1, SECONDS);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java b/core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java\nindex 44fd5714898..858bb64771c 100644\n--- a/core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/clientImpl/ThriftTransportKeyTest.java\n@@ -98,11 +98,10 @@ public void testConnectionCaching() throws IOException, InterruptedException {\n     SaslConnectionParams saslParams2 =\n         user1.doAs((PrivilegedExceptionAction<SaslConnectionParams>) () -> createSaslParams(token));\n \n-    ThriftTransportKey ttk1 =\n-        new ThriftTransportKey(ThriftClientTypes.CLIENT, HostAndPort.fromParts(\""localhost\"", 9997),\n-            1L, null, saslParams1),\n-        ttk2 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n-            HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams2);\n+    ThriftTransportKey ttk1 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n+        HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams1);\n+    ThriftTransportKey ttk2 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n+        HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams2);\n \n     // Should equals() and hashCode() to make sure we don't throw away thrift cnxns\n     assertEquals(ttk1, ttk2);\n@@ -120,11 +119,10 @@ public void testSaslPrincipalIsSignificant() throws IOException, InterruptedExce\n     SaslConnectionParams saslParams2 =\n         user2.doAs((PrivilegedExceptionAction<SaslConnectionParams>) () -> createSaslParams(token));\n \n-    ThriftTransportKey ttk1 =\n-        new ThriftTransportKey(ThriftClientTypes.CLIENT, HostAndPort.fromParts(\""localhost\"", 9997),\n-            1L, null, saslParams1),\n-        ttk2 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n-            HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams2);\n+    ThriftTransportKey ttk1 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n+        HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams1);\n+    ThriftTransportKey ttk2 = new ThriftTransportKey(ThriftClientTypes.CLIENT,\n+        HostAndPort.fromParts(\""localhost\"", 9997), 1L, null, saslParams2);\n \n     assertNotEquals(ttk1, ttk2);\n     assertNotEquals(ttk1.hashCode(), ttk2.hashCode());\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java b/core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java\nindex 8ab5315b74e..6d3070f6534 100644\n--- a/core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/conf/HadoopCredentialProviderTest.java\n@@ -49,15 +49,16 @@ public class HadoopCredentialProviderTest {\n   private static final Configuration hadoopConf = new Configuration();\n   private static final Logger log = LoggerFactory.getLogger(HadoopCredentialProviderTest.class);\n \n-  private static final String populatedKeyStoreName = \""/accumulo.jceks\"",\n-      emptyKeyStoreName = \""/empty.jceks\"";\n-  private static File emptyKeyStore, populatedKeyStore;\n+  private static final String populatedKeyStoreName = \""/accumulo.jceks\"";\n+  private static final String emptyKeyStoreName = \""/empty.jceks\"";\n+  private static File emptyKeyStore;\n+  private static File populatedKeyStore;\n \n   @BeforeAll\n   public static void checkCredentialProviderAvailable() throws Exception {\n     URL populatedKeyStoreUrl =\n-        HadoopCredentialProviderTest.class.getResource(populatedKeyStoreName),\n-        emptyKeyStoreUrl = HadoopCredentialProviderTest.class.getResource(emptyKeyStoreName);\n+        HadoopCredentialProviderTest.class.getResource(populatedKeyStoreName);\n+    URL emptyKeyStoreUrl = HadoopCredentialProviderTest.class.getResource(emptyKeyStoreName);\n \n     assertNotNull(populatedKeyStoreUrl, \""Could not find \"" + populatedKeyStoreName);\n     assertNotNull(emptyKeyStoreUrl, \""Could not find \"" + emptyKeyStoreName);\n@@ -117,8 +118,8 @@ public void testEmptyKeyStoreParses() {\n \n   @Test\n   public void testEmptyAndPopulatedKeyStores() {\n-    String populatedAbsPath = getKeyStoreUrl(populatedKeyStore),\n-        emptyAbsPath = getKeyStoreUrl(emptyKeyStore);\n+    String populatedAbsPath = getKeyStoreUrl(populatedKeyStore);\n+    String emptyAbsPath = getKeyStoreUrl(emptyKeyStore);\n     Configuration conf = new Configuration();\n     HadoopCredentialProvider.setPath(conf, populatedAbsPath + \"",\"" + emptyAbsPath);\n     Map<String,String> expectations = new HashMap<>();\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\nindex 4fce2c26e2d..25dc7275e10 100644\n--- a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\n@@ -144,7 +144,8 @@ public void testPropertyValidation() {\n         continue;\n       }\n       PropertyType propertyType = property.getType();\n-      String invalidValue, validValue = property.getDefaultValue();\n+      String invalidValue;\n+      String validValue = property.getDefaultValue();\n       LOG.debug(\""Testing property: {} with type: {}\"", property.getKey(), propertyType);\n \n       switch (propertyType) {\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java b/core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java\nindex a563c6101b1..45d5bac1ed3 100644\n--- a/core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/data/ConditionalMutationTest.java\n@@ -49,7 +49,8 @@ public class ConditionalMutationTest {\n   private static final ColumnVisibility CVIS2 = new ColumnVisibility(\""B|C\"");\n   private static final long TIMESTAMP = 1234567890;\n \n-  private Condition c1, c2;\n+  private Condition c1;\n+  private Condition c2;\n   private ConditionalMutation cm;\n \n   @BeforeEach\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\nindex 8bcf1dac763..6fd69d89709 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\n@@ -248,8 +248,11 @@ public void test1() throws IOException {\n   public void testNullByteInKey() throws IOException {\n     TreeMap<Key,Value> tm = new TreeMap<>();\n \n-    String s1 = \""first\"", s2 = \""second\"";\n-    byte[] b1 = s1.getBytes(UTF_8), b2 = s2.getBytes(UTF_8), ball;\n+    String s1 = \""first\"";\n+    String s2 = \""second\"";\n+    byte[] b1 = s1.getBytes(UTF_8);\n+    byte[] b2 = s2.getBytes(UTF_8);\n+    byte[] ball;\n     ball = new byte[b1.length + b2.length + 1];\n     System.arraycopy(b1, 0, ball, 0, b1.length);\n     ball[b1.length] = (byte) 0;\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\nindex 4c106559ecd..bb3faf341fb 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\n@@ -182,7 +182,8 @@ public TreeMap<Key,Value> createKeyValues() {\n     List<Mutation> mutations = createMutations();\n     TreeMap<Key,Value> keyValues = new TreeMap<>();\n \n-    final Text cf = new Text(), cq = new Text();\n+    final Text cf = new Text();\n+    final Text cq = new Text();\n     for (Mutation m : mutations) {\n       final Text row = new Text(m.getRow());\n       for (ColumnUpdate update : m.getUpdates()) {\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java b/core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java\nindex f8d4d192356..fe4afe0e2be 100644\n--- a/core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/rpc/SaslClientDigestCallbackHandlerTest.java\n@@ -27,8 +27,9 @@ public class SaslClientDigestCallbackHandlerTest {\n   @Test\n   public void testEquality() {\n     SaslClientDigestCallbackHandler handler1 =\n-        new SaslClientDigestCallbackHandler(\""user\"", \""mypass\"".toCharArray()),\n-        handler2 = new SaslClientDigestCallbackHandler(\""user\"", \""mypass\"".toCharArray());\n+        new SaslClientDigestCallbackHandler(\""user\"", \""mypass\"".toCharArray());\n+    SaslClientDigestCallbackHandler handler2 =\n+        new SaslClientDigestCallbackHandler(\""user\"", \""mypass\"".toCharArray());\n     assertEquals(handler1, handler2);\n     assertEquals(handler1.hashCode(), handler2.hashCode());\n   }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java b/core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java\nindex 18ab400ac97..29acee35d8c 100644\n--- a/core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/spi/balancer/TableLoadBalancerTest.java\n@@ -135,8 +135,9 @@ public void test() {\n \n     replay(environment);\n \n-    String t1Id = TABLE_ID_MAP.get(\""t1\""), t2Id = TABLE_ID_MAP.get(\""t2\""),\n-        t3Id = TABLE_ID_MAP.get(\""t3\"");\n+    String t1Id = TABLE_ID_MAP.get(\""t1\"");\n+    String t2Id = TABLE_ID_MAP.get(\""t2\"");\n+    String t3Id = TABLE_ID_MAP.get(\""t3\"");\n     state.clear();\n     TabletServerId svr = mkts(\""10.0.0.1\"", 1234, \""0x01020304\"");\n     state.put(svr, status(t1Id, 10, t2Id, 10, t3Id, 10));\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/util/RetryTest.java b/core/src/test/java/org/apache/accumulo/core/util/RetryTest.java\nindex c05189a5149..24fe64f49a1 100644\n--- a/core/src/test/java/org/apache/accumulo/core/util/RetryTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/util/RetryTest.java\n@@ -131,11 +131,13 @@ public void testBackOffFactor() throws InterruptedException {\n     retry.setMaxRetries(MAX_RETRIES);\n     retry.setBackOffFactor(1.5);\n     retry.setStartWait(INITIAL_WAIT);\n-    Duration waitIncrement, currentWait = INITIAL_WAIT;\n+    Duration waitIncrement;\n+    Duration currentWait = INITIAL_WAIT;\n     retry.setWaitIncrement(WAIT_INC);\n     retry.setMaxWait(Duration.ofSeconds(128).multipliedBy(MAX_RETRIES));\n     retry.setDoTimeJitter(false);\n-    double backOfFactor = 1.5, originalBackoff = 1.5;\n+    double backOfFactor = 1.5;\n+    double originalBackoff = 1.5;\n \n     for (int i = 1; i <= MAX_RETRIES; i++) {\n       retry.sleep(currentWait);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java b/core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java\nindex 6bbf0182b6f..ab4d15b110e 100644\n--- a/core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/util/ValidatorTest.java\n@@ -29,7 +29,9 @@\n \n public class ValidatorTest {\n \n-  private Validator<String> v, v2, v3;\n+  private Validator<String> v;\n+  private Validator<String> v2;\n+  private Validator<String> v3;\n   private static final Pattern STARTSWITH_C = Pattern.compile(\""c.*\"");\n \n   @BeforeEach\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java b/core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java\nindex 5851df4ab71..83725509c82 100644\n--- a/core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/volume/VolumeImplTest.java\n@@ -42,7 +42,8 @@ public class VolumeImplTest {\n   @Test\n   public void testFileSystemInequivalence() {\n     Configuration hadoopConf = createMock(Configuration.class);\n-    FileSystem fs = createMock(FileSystem.class), other = createMock(FileSystem.class);\n+    FileSystem fs = createMock(FileSystem.class);\n+    FileSystem other = createMock(FileSystem.class);\n \n     String basePath = \""/accumulo\"";\n \n@@ -62,7 +63,8 @@ public void testFileSystemInequivalence() {\n   @Test\n   public void testFileSystemEquivalence() {\n     Configuration hadoopConf = createMock(Configuration.class);\n-    FileSystem fs = createMock(FileSystem.class), other = createMock(FileSystem.class);\n+    FileSystem fs = createMock(FileSystem.class);\n+    FileSystem other = createMock(FileSystem.class);\n     String basePath = \""/accumulo\"";\n \n     expect(fs.getConf()).andReturn(hadoopConf).anyTimes();\n\ndiff --git a/hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java b/hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java\nindex 93e3699b6d2..b3e476bf2c2 100644\n--- a/hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java\n+++ b/hadoop-mapreduce/src/test/java/org/apache/accumulo/hadoop/mapreduce/partition/RangePartitionerTest.java\n@@ -79,7 +79,8 @@ private void checkExpectedRangeBins(int numSubBins, String[] strings, int[] rang\n   private void checkExpectedBins(int numSubBins, String[] strings, int[] bins) throws IOException {\n     assertEquals(strings.length, bins.length);\n     for (int i = 0; i < strings.length; ++i) {\n-      int bin = bins[i], part =\n+      int bin = bins[i];\n+      int part =\n           prepPartitioner(numSubBins).findPartition(new Text(strings[i]), cutArray, numSubBins);\n       assertEquals(bin, part);\n     }\n\ndiff --git a/minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java b/minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java\nindex 33350213ba5..72e56dd64f2 100644\n--- a/minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java\n+++ b/minicluster/src/test/java/org/apache/accumulo/cluster/standalone/StandaloneClusterControlTest.java\n@@ -26,8 +26,9 @@ public class StandaloneClusterControlTest {\n \n   @Test\n   public void testPaths() {\n-    String accumuloHome = \""/usr/lib/accumulo\"", accumuloConfDir = \""/etc/accumulo/conf\"",\n-        accumuloServerConfDir = \""/etc/accumulo/conf/server\"";\n+    String accumuloHome = \""/usr/lib/accumulo\"";\n+    String accumuloConfDir = \""/etc/accumulo/conf\"";\n+    String accumuloServerConfDir = \""/etc/accumulo/conf/server\"";\n \n     StandaloneClusterControl control =\n         new StandaloneClusterControl(accumuloHome, accumuloConfDir, accumuloServerConfDir, \""\"", \""\"");\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java b/server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java\nindex 86583391169..a51894c382c 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/rpc/TCredentialsUpdatingInvocationHandlerTest.java\n@@ -165,7 +165,9 @@ public void testDisallowedImpersonationForMultipleUsers() {\n \n   @Test\n   public void testAllowedImpersonationFromSpecificHost() throws Exception {\n-    final String proxyServer = \""proxy\"", client = \""client\"", host = \""host.domain.com\"";\n+    final String proxyServer = \""proxy\"";\n+    final String client = \""client\"";\n+    final String host = \""host.domain.com\"";\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_USER_IMPERSONATION, proxyServer + \"":\"" + client);\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_HOST_IMPERSONATION, host);\n     proxy = new TCredentialsUpdatingInvocationHandler<>(new Object(), conf);\n@@ -178,7 +180,9 @@ public void testAllowedImpersonationFromSpecificHost() throws Exception {\n \n   @Test\n   public void testDisallowedImpersonationFromSpecificHost() {\n-    final String proxyServer = \""proxy\"", client = \""client\"", host = \""host.domain.com\"";\n+    final String proxyServer = \""proxy\"";\n+    final String client = \""client\"";\n+    final String host = \""host.domain.com\"";\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_USER_IMPERSONATION, proxyServer + \"":\"" + client);\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_HOST_IMPERSONATION, host);\n     proxy = new TCredentialsUpdatingInvocationHandler<>(new Object(), conf);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java\nindex 3f0859f57c8..d1b834b49d1 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/UserImpersonationTest.java\n@@ -108,7 +108,9 @@ public void testNoUsersByDefault() {\n \n   @Test\n   public void testSingleUserAndHost() {\n-    String server = \""server\"", host = \""single_host.domain.com\"", client = \""single_client\"";\n+    String server = \""server\"";\n+    String host = \""single_host.domain.com\"";\n+    String client = \""single_client\"";\n     setValidHosts(host);\n     setValidUsers(server, client);\n     UserImpersonation impersonation = new UserImpersonation(conf);\n@@ -131,7 +133,10 @@ public void testSingleUserAndHost() {\n \n   @Test\n   public void testMultipleExplicitUsers() {\n-    String server = \""server\"", client1 = \""client1\"", client2 = \""client2\"", client3 = \""client3\"";\n+    String server = \""server\"";\n+    String client1 = \""client1\"";\n+    String client2 = \""client2\"";\n+    String client3 = \""client3\"";\n     setValidHosts(\""*\"");\n     setValidUsers(server, Joiner.on(',').join(client1, client2, client3));\n     UserImpersonation impersonation = new UserImpersonation(conf);\n@@ -153,7 +158,10 @@ public void testMultipleExplicitUsers() {\n \n   @Test\n   public void testMultipleExplicitHosts() {\n-    String server = \""server\"", host1 = \""host1\"", host2 = \""host2\"", host3 = \""host3\"";\n+    String server = \""server\"";\n+    String host1 = \""host1\"";\n+    String host2 = \""host2\"";\n+    String host3 = \""host3\"";\n     setValidHosts(Joiner.on(',').join(host1, host2, host3));\n     setValidUsers(server, \""*\"");\n     UserImpersonation impersonation = new UserImpersonation(conf);\n@@ -175,8 +183,13 @@ public void testMultipleExplicitHosts() {\n \n   @Test\n   public void testMultipleExplicitUsersHosts() {\n-    String server = \""server\"", host1 = \""host1\"", host2 = \""host2\"", host3 = \""host3\"",\n-        client1 = \""client1\"", client2 = \""client2\"", client3 = \""client3\"";\n+    String server = \""server\"";\n+    String host1 = \""host1\"";\n+    String host2 = \""host2\"";\n+    String host3 = \""host3\"";\n+    String client1 = \""client1\"";\n+    String client2 = \""client2\"";\n+    String client3 = \""client3\"";\n     setValidHosts(Joiner.on(',').join(host1, host2, host3));\n     setValidUsers(server, Joiner.on(',').join(client1, client2, client3));\n     UserImpersonation impersonation = new UserImpersonation(conf);\n@@ -203,8 +216,14 @@ public void testMultipleExplicitUsersHosts() {\n \n   @Test\n   public void testMultipleAllowedImpersonators() {\n-    String server1 = \""server1\"", server2 = \""server2\"", host1 = \""host1\"", host2 = \""host2\"",\n-        host3 = \""host3\"", client1 = \""client1\"", client2 = \""client2\"", client3 = \""client3\"";\n+    String server1 = \""server1\"";\n+    String server2 = \""server2\"";\n+    String host1 = \""host1\"";\n+    String host2 = \""host2\"";\n+    String host3 = \""host3\"";\n+    String client1 = \""client1\"";\n+    String client2 = \""client2\"";\n+    String client3 = \""client3\"";\n     // server1 can impersonate client1 and client2 from host1 or host2\n     // server2 can impersonate only client3 from host3\n     setValidHosts(Joiner.on(',').join(host1, host2), host3);\n@@ -255,7 +274,8 @@ public void testMultipleAllowedImpersonators() {\n \n   @Test\n   public void testSingleUser() {\n-    final String server = \""server/hostname@EXAMPLE.COM\"", client = \""client@EXAMPLE.COM\"";\n+    final String server = \""server/hostname@EXAMPLE.COM\"";\n+    final String client = \""client@EXAMPLE.COM\"";\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_USER_IMPERSONATION, server + \"":\"" + client);\n     conf.set(Property.INSTANCE_RPC_SASL_ALLOWED_HOST_IMPERSONATION, \""*\"");\n     UserImpersonation impersonation = new UserImpersonation(conf);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java\nindex c8d9e175481..5ed864e3bc0 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationKeyTest.java\n@@ -56,7 +56,8 @@ public void testNullSecretKey() {\n   public void testAuthKey() {\n     SecretKey secretKey = keyGen.generateKey();\n     int keyId = 20;\n-    long creationDate = 38383838L, expirationDate = 83838383L;\n+    long creationDate = 38383838L;\n+    long expirationDate = 83838383L;\n     AuthenticationKey authKey =\n         new AuthenticationKey(keyId, creationDate, expirationDate, secretKey);\n     assertEquals(secretKey, authKey.getKey());\n@@ -80,7 +81,8 @@ public void testAuthKey() {\n   public void testWritable() throws IOException {\n     SecretKey secretKey = keyGen.generateKey();\n     int keyId = 20;\n-    long creationDate = 38383838L, expirationDate = 83838383L;\n+    long creationDate = 38383838L;\n+    long expirationDate = 83838383L;\n     AuthenticationKey authKey =\n         new AuthenticationKey(keyId, creationDate, expirationDate, secretKey);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java\nindex b9ec08086c2..7bbb1e120c7 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenKeyManagerTest.java\n@@ -187,10 +187,12 @@ public void testStopLoop() throws InterruptedException {\n   public void testExistingKeysAreAddedAtStartup() throws Exception {\n     long updateInterval = 0;\n     long tokenLifetime = 100_000L;\n-    SecretKey secretKey1 = keyGen.generateKey(), secretKey2 = keyGen.generateKey();\n+    SecretKey secretKey1 = keyGen.generateKey();\n+    SecretKey secretKey2 = keyGen.generateKey();\n \n-    AuthenticationKey authKey1 = new AuthenticationKey(1, 0, tokenLifetime, secretKey1),\n-        authKey2 = new AuthenticationKey(2, tokenLifetime, tokenLifetime * 2, secretKey2);\n+    AuthenticationKey authKey1 = new AuthenticationKey(1, 0, tokenLifetime, secretKey1);\n+    AuthenticationKey authKey2 =\n+        new AuthenticationKey(2, tokenLifetime, tokenLifetime * 2, secretKey2);\n     AuthenticationTokenKeyManager keyManager = new AuthenticationTokenKeyManager(secretManager,\n         zooDistributor, updateInterval, tokenLifetime);\n \n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java\nindex 7198e16883b..ca1c4d0c14b 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/AuthenticationTokenSecretManagerTest.java\n@@ -338,9 +338,9 @@ public void testManagerKeyExpiration() throws Exception {\n \n     // Make 2 keys, and add only one. The second has double the expiration of the first\n     AuthenticationKey authKey1 =\n-        new AuthenticationKey(1, then, then + tokenLifetime, keyGen.generateKey()),\n-        authKey2 = new AuthenticationKey(2, then + tokenLifetime, then + tokenLifetime * 2,\n-            keyGen.generateKey());\n+        new AuthenticationKey(1, then, then + tokenLifetime, keyGen.generateKey());\n+    AuthenticationKey authKey2 = new AuthenticationKey(2, then + tokenLifetime,\n+        then + tokenLifetime * 2, keyGen.generateKey());\n     secretManager.addKey(authKey1);\n \n     keyDistributor.remove(authKey1);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\nindex be333bd1d1a..1cad32921b4 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\n@@ -109,9 +109,11 @@ public void testBaseNodeCreated() throws Exception {\n   public void testBaseNodeCreatedWithChildren() throws Exception {\n     WatchedEvent event =\n         new WatchedEvent(EventType.NodeCreated, null, Constants.ZDELEGATION_TOKEN_KEYS);\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n-    byte[] serializedKey1 = serialize(key1), serializedKey2 = serialize(key2);\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    byte[] serializedKey1 = serialize(key1);\n+    byte[] serializedKey2 = serialize(key2);\n     List<String> children = Arrays.asList(\""1\"", \""2\"");\n \n     expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n@@ -132,9 +134,11 @@ public void testBaseNodeCreatedWithChildren() throws Exception {\n   public void testBaseNodeChildrenChanged() throws Exception {\n     WatchedEvent event =\n         new WatchedEvent(EventType.NodeChildrenChanged, null, Constants.ZDELEGATION_TOKEN_KEYS);\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n-    byte[] serializedKey1 = serialize(key1), serializedKey2 = serialize(key2);\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    byte[] serializedKey1 = serialize(key1);\n+    byte[] serializedKey2 = serialize(key2);\n     List<String> children = Arrays.asList(\""1\"", \""2\"");\n \n     expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n@@ -155,8 +159,9 @@ public void testBaseNodeChildrenChanged() throws Exception {\n   public void testBaseNodeDeleted() {\n     WatchedEvent event =\n         new WatchedEvent(EventType.NodeDeleted, null, Constants.ZDELEGATION_TOKEN_KEYS);\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n     secretManager.addKey(key1);\n     secretManager.addKey(key2);\n@@ -187,8 +192,9 @@ public void testBaseNodeDataChanged() {\n   public void testChildChanged() throws Exception {\n     WatchedEvent event =\n         new WatchedEvent(EventType.NodeCreated, null, Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"");\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     secretManager.addKey(key1);\n     assertEquals(1, secretManager.getKeys().size());\n     byte[] serializedKey2 = serialize(key2);\n@@ -208,8 +214,9 @@ public void testChildChanged() throws Exception {\n   public void testChildDeleted() {\n     WatchedEvent event =\n         new WatchedEvent(EventType.NodeDeleted, null, Constants.ZDELEGATION_TOKEN_KEYS + \""/1\"");\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     secretManager.addKey(key1);\n     secretManager.addKey(key2);\n     assertEquals(2, secretManager.getKeys().size());\n@@ -227,8 +234,10 @@ public void testChildDeleted() {\n   public void testChildChildrenChanged() {\n     WatchedEvent event = new WatchedEvent(EventType.NodeChildrenChanged, null,\n         Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"");\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+\n     secretManager.addKey(key1);\n     secretManager.addKey(key2);\n     assertEquals(2, secretManager.getKeys().size());\n@@ -259,8 +268,9 @@ public void testInitialUpdateNoNode() throws Exception {\n   @Test\n   public void testInitialUpdateWithKeys() throws Exception {\n     List<String> children = Arrays.asList(\""1\"", \""5\"");\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n     expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n     expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n@@ -294,8 +304,9 @@ private void lostZooKeeperBase(WatchedEvent disconnectEvent, WatchedEvent reconn\n       throws Exception {\n \n     List<String> children = Arrays.asList(\""1\"", \""5\"");\n-    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n-        key2 = new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n+    AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n+    AuthenticationKey key2 =\n+        new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n     expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n     expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java b/test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java\nindex 8f194971c3f..de1f612a8d6 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/MiniClusterHarness.java\n@@ -152,8 +152,8 @@ protected void configureForSsl(MiniAccumuloConfigImpl cfg, File folder) {\n     File rootKeystoreFile = sslDir.resolve(\""root-\"" + cfg.getInstanceName() + \"".jks\"").toFile();\n     File localKeystoreFile = sslDir.resolve(\""local-\"" + cfg.getInstanceName() + \"".jks\"").toFile();\n     File publicTruststoreFile = sslDir.resolve(\""public-\"" + cfg.getInstanceName() + \"".jks\"").toFile();\n-    final String rootKeystorePassword = \""root_keystore_password\"",\n-        truststorePassword = \""truststore_password\"";\n+    final String rootKeystorePassword = \""root_keystore_password\"";\n+    final String truststorePassword = \""truststore_password\"";\n     try {\n       new CertUtils(Property.RPC_SSL_KEYSTORE_TYPE.getDefaultValue(),\n           \""o=Apache Accumulo,cn=MiniAccumuloCluster\"", \""RSA\"", 4096, \""SHA512WITHRSA\"").createAll(\n\ndiff --git a/test/src/main/java/org/apache/accumulo/harness/TestingKdc.java b/test/src/main/java/org/apache/accumulo/harness/TestingKdc.java\nindex 045c21621e2..361d3bb7245 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/TestingKdc.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/TestingKdc.java\n@@ -46,10 +46,12 @@ public class TestingKdc {\n   public static final long MAX_TICKET_LIFETIME_MILLIS = 86400000; // one day\n \n   protected MiniKdc kdc = null;\n-  protected ClusterUser accumuloServerUser = null, accumuloAdmin = null;\n+  protected ClusterUser accumuloServerUser = null;\n+  protected ClusterUser accumuloAdmin = null;\n   protected List<ClusterUser> clientPrincipals = null;\n \n-  public final String ORG_NAME = \""EXAMPLE\"", ORG_DOMAIN = \""COM\"";\n+  public final String ORG_NAME = \""EXAMPLE\"";\n+  public final String ORG_DOMAIN = \""COM\"";\n \n   private String hostname;\n   private File keytabDir;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java b/test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java\nindex 74b45a0c7c8..69928bb8131 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/conf/AccumuloClusterPropertyConfiguration.java\n@@ -55,7 +55,8 @@ public abstract class AccumuloClusterPropertyConfiguration implements AccumuloCl\n   @SuppressFBWarnings(value = \""PATH_TRAVERSAL_IN\"", justification = \""path provided by test\"")\n   public static AccumuloClusterPropertyConfiguration get() {\n \n-    String clusterTypeValue = null, clientConf = null;\n+    String clusterTypeValue = null;\n+    String clientConf = null;\n     String propertyFile = System.getProperty(ACCUMULO_IT_PROPERTIES_FILE);\n \n     if (propertyFile != null) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java b/test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java\nindex 35ef803e566..c8a47dc939a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/BadDeleteMarkersCreatedIT.java\n@@ -78,7 +78,8 @@ public void getTimeoutFactor() {\n     timeoutFactor = Wait.getTimeoutFactor(e -> 1);\n   }\n \n-  private String gcCycleDelay, gcCycleStart;\n+  private String gcCycleDelay;\n+  private String gcCycleStart;\n \n   @BeforeEach\n   public void alterConfig() throws Exception {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java b/test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java\nindex 7e5bb5a60e3..0cf4f8658a7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/BatchWriterInTabletServerIT.java\n@@ -63,7 +63,8 @@ public boolean canRunTest(ClusterType type) {\n   @Test\n   public void testNormalWrite() throws Exception {\n     String[] uniqueNames = getUniqueNames(2);\n-    String t1 = uniqueNames[0], t2 = uniqueNames[1];\n+    String t1 = uniqueNames[0];\n+    String t2 = uniqueNames[1];\n     try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n       int numEntriesToWritePerEntry = 50;\n       IteratorSetting itset = BatchWriterIterator.iteratorSetting(6, 0, 15, 1000,\n@@ -83,7 +84,8 @@ public void testNormalWrite() throws Exception {\n   @Test\n   public void testClearLocatorAndSplitWrite() throws Exception {\n     String[] uniqueNames = getUniqueNames(2);\n-    String t1 = uniqueNames[0], t2 = uniqueNames[1];\n+    String t1 = uniqueNames[0];\n+    String t2 = uniqueNames[1];\n     try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n       int numEntriesToWritePerEntry = 50;\n       IteratorSetting itset = BatchWriterIterator.iteratorSetting(6, 0, 15, 1000,\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java b/test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java\nindex 901dba56dfb..3ae51fe40c3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java\n+++ b/test/src/main/java/org/apache/accumulo/test/BatchWriterIterator.java\n@@ -205,7 +205,9 @@ private void initBatchWriter() {\n   private void processNext() {\n     assert hasTop();\n     Key k = getTopKey();\n-    Text row = k.getRow(), cf = k.getColumnFamily(), cq = k.getColumnQualifier();\n+    Text row = k.getRow();\n+    Text cf = k.getColumnFamily();\n+    Text cq = k.getColumnQualifier();\n     Value v = super.getTopValue();\n     String failure = null;\n     try {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java b/test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java\nindex d2d5dd2cc79..11d0351d0e2 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ConditionalWriterIT.java\n@@ -1335,7 +1335,9 @@ public void testSecurity() throws Exception {\n       }\n \n       String[] tables = getUniqueNames(3);\n-      String table1 = tables[0], table2 = tables[1], table3 = tables[2];\n+      String table1 = tables[0];\n+      String table2 = tables[1];\n+      String table3 = tables[2];\n \n       // Create three tables\n       client.tableOperations().create(table1);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ImportExportIT.java b/test/src/main/java/org/apache/accumulo/test/ImportExportIT.java\nindex 21c0e6eac7e..90b884e249e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ImportExportIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ImportExportIT.java\n@@ -260,7 +260,8 @@ public void testExportImportOffline(boolean fenced) throws Exception {\n     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {\n \n       String[] tableNames = getUniqueNames(2);\n-      String srcTable = tableNames[0], destTable = tableNames[1];\n+      String srcTable = tableNames[0];\n+      String destTable = tableNames[1];\n       client.tableOperations().create(srcTable);\n \n       try (BatchWriter bw = client.createBatchWriter(srcTable)) {\n@@ -408,7 +409,8 @@ public void testImportedTableIsOnDemand() throws Exception {\n \n     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {\n       String[] tableNames = getUniqueNames(2);\n-      String srcTable = tableNames[0], destTable = tableNames[1];\n+      String srcTable = tableNames[0];\n+      String destTable = tableNames[1];\n \n       client.tableOperations().create(srcTable);\n       String srcTableId = client.tableOperations().tableIdMap().get(srcTable);\n@@ -602,13 +604,15 @@ public void importV2data() throws Exception {\n   private void verifyTableEquality(AccumuloClient client, String srcTable, String destTable,\n       int expected) throws Exception {\n     Iterator<Entry<Key,Value>> src =\n-        client.createScanner(srcTable, Authorizations.EMPTY).iterator(),\n-        dest = client.createScanner(destTable, Authorizations.EMPTY).iterator();\n+        client.createScanner(srcTable, Authorizations.EMPTY).iterator();\n+    Iterator<Entry<Key,Value>> dest =\n+        client.createScanner(destTable, Authorizations.EMPTY).iterator();\n     assertTrue(src.hasNext(), \""Could not read any data from source table\"");\n     assertTrue(dest.hasNext(), \""Could not read any data from destination table\"");\n     int entries = 0;\n     while (src.hasNext() && dest.hasNext()) {\n-      Entry<Key,Value> orig = src.next(), copy = dest.next();\n+      Entry<Key,Value> orig = src.next();\n+      Entry<Key,Value> copy = dest.next();\n       assertEquals(orig.getKey(), copy.getKey());\n       assertEquals(orig.getValue(), copy.getValue());\n       entries++;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java b/test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java\nindex 044d243edc6..7642140a4b6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/KeyValueEqualityIT.java\n@@ -48,7 +48,8 @@ public void testEquality() throws Exception {\n     try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {\n \n       final String[] tables = getUniqueNames(2);\n-      final String table1 = tables[0], table2 = tables[1];\n+      final String table1 = tables[0];\n+      final String table2 = tables[1];\n       final TableOperations tops = client.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n@@ -66,12 +67,13 @@ public void testEquality() throws Exception {\n         }\n       }\n \n-      Iterator<Entry<Key,Value>> t1 = client.createScanner(table1, Authorizations.EMPTY).iterator(),\n-          t2 = client.createScanner(table2, Authorizations.EMPTY).iterator();\n+      Iterator<Entry<Key,Value>> t1 = client.createScanner(table1, Authorizations.EMPTY).iterator();\n+      Iterator<Entry<Key,Value>> t2 = client.createScanner(table2, Authorizations.EMPTY).iterator();\n       while (t1.hasNext() && t2.hasNext()) {\n         // KeyValue, the implementation of Entry<Key,Value>, should support equality and hashCode\n         // properly\n-        Entry<Key,Value> e1 = t1.next(), e2 = t2.next();\n+        Entry<Key,Value> e1 = t1.next();\n+        Entry<Key,Value> e2 = t2.next();\n         assertEquals(e1, e2);\n         assertEquals(e1.hashCode(), e2.hashCode());\n       }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java\nindex d702811be32..bd1ba0e83cb 100644\n--- a/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT_SimpleSuite.java\n@@ -90,7 +90,8 @@ public void testTableRenameDataValidation() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(2);\n-      final String table1 = names[0], table2 = names[1];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n@@ -155,14 +156,17 @@ public void testTableRenameSameWriters() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(4);\n-      final String table1 = names[0], table2 = names[1];\n-      final String newTable1 = names[2], newTable2 = names[3];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n+      final String newTable1 = names[2];\n+      final String newTable2 = names[3];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n \n-      BatchWriter bw1 = mtbw.getBatchWriter(table1), bw2 = mtbw.getBatchWriter(table2);\n+      BatchWriter bw1 = mtbw.getBatchWriter(table1);\n+      BatchWriter bw2 = mtbw.getBatchWriter(table2);\n \n       Mutation m1 = new Mutation(\""foo\"");\n       m1.put(\""col1\"", \""\"", \""val1\"");\n@@ -212,14 +216,17 @@ public void testTableRenameNewWriters() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(4);\n-      final String table1 = names[0], table2 = names[1];\n-      final String newTable1 = names[2], newTable2 = names[3];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n+      final String newTable1 = names[2];\n+      final String newTable2 = names[3];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n \n-      BatchWriter bw1 = mtbw.getBatchWriter(table1), bw2 = mtbw.getBatchWriter(table2);\n+      BatchWriter bw1 = mtbw.getBatchWriter(table1);\n+      BatchWriter bw2 = mtbw.getBatchWriter(table2);\n \n       Mutation m1 = new Mutation(\""foo\"");\n       m1.put(\""col1\"", \""\"", \""val1\"");\n@@ -282,14 +289,17 @@ public void testTableRenameNewWritersNoCaching() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(4);\n-      final String table1 = names[0], table2 = names[1];\n-      final String newTable1 = names[2], newTable2 = names[3];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n+      final String newTable1 = names[2];\n+      final String newTable2 = names[3];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n \n-      BatchWriter bw1 = mtbw.getBatchWriter(table1), bw2 = mtbw.getBatchWriter(table2);\n+      BatchWriter bw1 = mtbw.getBatchWriter(table1);\n+      BatchWriter bw2 = mtbw.getBatchWriter(table2);\n \n       Mutation m1 = new Mutation(\""foo\"");\n       m1.put(\""col1\"", \""\"", \""val1\"");\n@@ -319,13 +329,15 @@ public void testTableDelete() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(2);\n-      final String table1 = names[0], table2 = names[1];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n \n-      BatchWriter bw1 = mtbw.getBatchWriter(table1), bw2 = mtbw.getBatchWriter(table2);\n+      BatchWriter bw1 = mtbw.getBatchWriter(table1);\n+      BatchWriter bw2 = mtbw.getBatchWriter(table2);\n \n       Mutation m1 = new Mutation(\""foo\"");\n       m1.put(\""col1\"", \""\"", \""val1\"");\n@@ -370,13 +382,15 @@ public void testOfflineTable() throws Exception {\n \n     try {\n       final String[] names = getUniqueNames(2);\n-      final String table1 = names[0], table2 = names[1];\n+      final String table1 = names[0];\n+      final String table2 = names[1];\n \n       TableOperations tops = accumuloClient.tableOperations();\n       tops.create(table1);\n       tops.create(table2);\n \n-      BatchWriter bw1 = mtbw.getBatchWriter(table1), bw2 = mtbw.getBatchWriter(table2);\n+      BatchWriter bw1 = mtbw.getBatchWriter(table1);\n+      BatchWriter bw2 = mtbw.getBatchWriter(table2);\n \n       Mutation m1 = new Mutation(\""foo\"");\n       m1.put(\""col1\"", \""\"", \""val1\"");\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java\nindex c3e3bbbdc30..75f2d315622 100644\n--- a/test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/NamespacesIT_SimpleSuite.java\n@@ -679,7 +679,10 @@ private void loginAs(ClusterUser user) throws IOException {\n    */\n   @Test\n   public void testPermissions() throws Exception {\n-    ClusterUser user1 = getUser(0), user2 = getUser(1), root = getAdminUser();\n+    ClusterUser user1 = getUser(0);\n+    ClusterUser user2 = getUser(1);\n+    ClusterUser root = getAdminUser();\n+\n     String u1 = user1.getPrincipal();\n     String u2 = user2.getPrincipal();\n     PasswordToken pass =\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java b/test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java\nindex 90f81ee33d5..ae78d12d62c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/TableConfigurationUpdateIT.java\n@@ -103,7 +103,8 @@ public static class TableConfRunner implements Callable<Exception> {\n     private static final Property prop = Property.TABLE_SPLIT_THRESHOLD;\n     private AccumuloConfiguration tableConf;\n     private CountDownLatch countDown;\n-    private int iterations, randMax;\n+    private int iterations;\n+    private int randMax;\n \n     public TableConfRunner(int randMax, int iterations, AccumuloConfiguration tableConf,\n         CountDownLatch countDown) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java b/test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java\nindex 5af6b261686..a31178ecae3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/TableOperationsIT.java\n@@ -331,7 +331,8 @@ public void createMergeClonedTable() throws Exception {\n       for (Entry<Key,Value> entry : s) {\n         final Key key = entry.getKey();\n         String row = key.getRow().toString();\n-        String cf = key.getColumnFamily().toString(), cq = key.getColumnQualifier().toString();\n+        String cf = key.getColumnFamily().toString();\n+        String cq = key.getColumnQualifier().toString();\n         String value = entry.getValue().toString();\n \n         if (rowCounts.containsKey(row)) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java b/test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java\nindex 7fea8091c64..91ec13c8e9b 100644\n--- a/test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java\n+++ b/test/src/main/java/org/apache/accumulo/test/TestBinaryRows.java\n@@ -96,8 +96,10 @@ public static class Opts extends ServerUtilOpts {\n \n   public static void runTest(AccumuloClient accumuloClient, Opts opts) throws Exception {\n \n-    final Text CF = new Text(\""cf\""), CQ = new Text(\""cq\"");\n-    final byte[] CF_BYTES = \""cf\"".getBytes(UTF_8), CQ_BYTES = \""cq\"".getBytes(UTF_8);\n+    final Text CF = new Text(\""cf\"");\n+    final Text CQ = new Text(\""cq\"");\n+    final byte[] CF_BYTES = \""cf\"".getBytes(UTF_8);\n+    final byte[] CQ_BYTES = \""cq\"".getBytes(UTF_8);\n     if (opts.mode.equals(\""ingest\"") || opts.mode.equals(\""delete\"")) {\n       try (BatchWriter bw = accumuloClient.createBatchWriter(opts.tableName)) {\n         boolean delete = opts.mode.equals(\""delete\"");\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java b/test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java\nindex 30e8eb24269..190f97ebfa7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/VolumeChooserIT.java\n@@ -81,7 +81,9 @@ static final String getPerTableProp(Scope scope) {\n   private static final Text EMPTY = new Text();\n   private static final Value EMPTY_VALUE = new Value();\n   private File volDirBase;\n-  private Path v1, v2, v3;\n+  private Path v1;\n+  private Path v2;\n+  private Path v3;\n   public static String[] alpha_rows =\n       \""a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z\"".split(\"",\"");\n   private String namespace1;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java b/test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java\nindex 84dc9cd7735..1a0276a9806 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/BalanceInPresenceOfOfflineTableIT.java\n@@ -84,7 +84,8 @@ public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoo\n \n   private static final int NUM_SPLITS = 200;\n \n-  private String UNUSED_TABLE, TEST_TABLE;\n+  private String UNUSED_TABLE;\n+  private String TEST_TABLE;\n \n   private AccumuloClient accumuloClient;\n \n@@ -189,7 +190,8 @@ public void test() throws Exception {\n             List.of(tabletsPerServer));\n         continue;\n       }\n-      long min = NumberUtils.min(tabletsPerServer), max = NumberUtils.max(tabletsPerServer);\n+      long min = NumberUtils.min(tabletsPerServer);\n+      long max = NumberUtils.max(tabletsPerServer);\n       log.debug(\""Min={}, Max={}\"", min, max);\n       if ((min / ((double) max)) < 0.5) {\n         log.debug(\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java b/test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java\nindex 40e17a9e614..d3666eba1d7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/BloomFilterIT.java\n@@ -192,7 +192,9 @@ private long query(AccumuloClient c, String table, int depth, long start, long e\n     HashSet<Long> expected = new HashSet<>();\n     List<Range> ranges = new ArrayList<>(num);\n     Text key = new Text();\n-    Text row = new Text(\""row\""), cq = new Text(\""cq\""), cf = new Text(\""cf\"");\n+    Text row = new Text(\""row\"");\n+    Text cq = new Text(\""cq\"");\n+    Text cf = new Text(\""cf\"");\n \n     for (int i = 0; i < num; ++i) {\n       Long k = ((RANDOM.get().nextLong() & 0x7fffffffffffffffL) % (end - start)) + start;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java\nindex eaf94829978..dcabbcb9472 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT_SimpleSuite.java\n@@ -167,7 +167,8 @@ private void checkMetadata(String table, AccumuloClient client) throws Exception\n       s.setRange(Range.prefix(tableId));\n \n       Key k;\n-      Text cf = new Text(), cq = new Text();\n+      Text cf = new Text();\n+      Text cq = new Text();\n       int itemsInspected = 0;\n       var pattern = Pattern.compile(\""[tc]-[0-9a-z]+\"");\n       for (Entry<Key,Value> entry : s) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java b/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\nindex 05dbc77404b..8d2c5217256 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\n@@ -99,8 +99,8 @@ protected static void configureForSsl(MiniAccumuloConfigImpl cfg, File sslDir) {\n     File localKeystoreFile = sslDirPath.resolve(\""local-\"" + cfg.getInstanceName() + \"".jks\"").toFile();\n     File publicTruststoreFile =\n         sslDirPath.resolve(\""public-\"" + cfg.getInstanceName() + \"".jks\"").toFile();\n-    final String rootKeystorePassword = \""root_keystore_password\"",\n-        truststorePassword = \""truststore_password\"";\n+    final String rootKeystorePassword = \""root_keystore_password\"";\n+    final String truststorePassword = \""truststore_password\"";\n     try {\n       String hostname = InetAddress.getLocalHost().getHostName();\n       new CertUtils(Property.RPC_SSL_KEYSTORE_TYPE.getDefaultValue(),\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java b/test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java\nindex 06335da1777..2ef8a8b41cc 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/KerberosIT.java\n@@ -395,7 +395,8 @@ public void testDelegationToken() throws Exception {\n         rootUser.getPrincipal(), rootUser.getKeytab().getAbsolutePath());\n     log.info(\""Logged in as {}\"", rootUser.getPrincipal());\n \n-    final int numRows = 100, numColumns = 10;\n+    final int numRows = 100;\n+    final int numColumns = 10;\n \n     // As the \""root\"" user, open up the connection and get a delegation token\n     final AuthenticationToken delegationToken =\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java b/test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java\nindex 7398126db1e..1009f874ca9 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/PermissionsIT.java\n@@ -95,7 +95,8 @@ private void loginAs(ClusterUser user) throws IOException {\n \n   @Test\n   public void systemPermissionsTest() throws Exception {\n-    ClusterUser testUser = getUser(0), rootUser = getAdminUser();\n+    ClusterUser testUser = getUser(0);\n+    ClusterUser rootUser = getAdminUser();\n \n     // verify that the test is being run by root\n     try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n@@ -149,7 +150,10 @@ static Map<String,String> map(Iterable<Entry<String,String>> i) {\n   private void testMissingSystemPermission(String tableNamePrefix, AccumuloClient root_client,\n       ClusterUser rootUser, AccumuloClient test_user_client, ClusterUser testUser,\n       SystemPermission perm) throws Exception {\n-    String tableName, user, password = \""password\"", namespace;\n+    String tableName;\n+    String user;\n+    String password = \""password\"";\n+    String namespace;\n     boolean passwordBased = testUser.getPassword() != null;\n     log.debug(\""Confirming that the lack of the {} permission properly restricts the user\"", perm);\n \n@@ -459,7 +463,10 @@ private void testMissingSystemPermission(String tableNamePrefix, AccumuloClient\n   private void testGrantedSystemPermission(String tableNamePrefix, AccumuloClient root_client,\n       ClusterUser rootUser, AccumuloClient test_user_client, ClusterUser testUser,\n       SystemPermission perm) throws Exception {\n-    String tableName, user, password = \""password\"", namespace;\n+    String tableName;\n+    String user;\n+    String password = \""password\"";\n+    String namespace;\n     boolean passwordBased = testUser.getPassword() != null;\n     log.debug(\""Confirming that the presence of the {} permission properly permits the user\"", perm);\n \n@@ -676,7 +683,8 @@ private void verifyHasNoSystemPermissions(AccumuloClient root_client, String use\n   @Test\n   public void tablePermissionTest() throws Exception {\n     // create the test user\n-    ClusterUser testUser = getUser(0), rootUser = getAdminUser();\n+    ClusterUser testUser = getUser(0);\n+    ClusterUser rootUser = getAdminUser();\n \n     String principal = testUser.getPrincipal();\n     AuthenticationToken token = testUser.getToken();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java b/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\nindex 530919f0106..f2e064877bc 100644\n--- a/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\n+++ b/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\n@@ -251,7 +251,8 @@ private abstract static class Test implements Runnable {\n     private int count;\n     private long t1;\n     private long t2;\n-    private CountDownLatch startCdl, finishCdl;\n+    private CountDownLatch startCdl;\n+    private CountDownLatch finishCdl;\n     private KeyExtent ke;\n \n     Test(KeyExtent ke) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java b/test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java\nindex 82c7576ca0d..90f15b42001 100644\n--- a/test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/server/security/SystemCredentialsIT.java\n@@ -41,7 +41,8 @@\n \n public class SystemCredentialsIT extends ConfigurableMacBase {\n \n-  private static final int SCAN_FAILED = 7, AUTHENICATION_FAILED = 8;\n+  private static final int SCAN_FAILED = 7;\n+  private static final int AUTHENICATION_FAILED = 8;\n \n   @Override\n   protected Duration defaultTimeout() {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java b/test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java\nindex 1152f6daea1..731b6581abd 100644\n--- a/test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java\n+++ b/test/src/main/java/org/apache/accumulo/test/shell/ShellIT_SimpleSuite.java\n@@ -500,7 +500,8 @@ void configTest() throws IOException {\n \n     for (Property property : Property.values()) {\n       PropertyType propertyType = property.getType();\n-      String invalidValue, validValue = property.getDefaultValue();\n+      String invalidValue;\n+      String validValue = property.getDefaultValue();\n \n       // Skip test if we can't set this property via shell\n       if (!Property.isValidZooPropertyKey(property.getKey())) {\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5576"", ""pr_id"": 5576, ""issue_id"": 5546, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Classify more server threads as critical\n**Describe the bug**\n\nServer processes like the manager and tserver create some threads which must be running for the process to be healthy.  Currently should these threads exit w/ an exception the process would not be healthy.\n\n**Expected behavior**\n\nAll threads created by Accumulo  servers processes that are always expected to be running should use the new `Threads.createCriticalThread` method add in #5543.  Looked at the current usage of `Threads.createThread.*` methods and saw a few candidates to use the new mthod. \n"", ""issue_word_count"": 88, ""test_files_count"": 3, ""non_test_files_count"": 17, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java"", ""core/src/main/java/org/apache/accumulo/core/util/Halt.java"", ""core/src/main/java/org/apache/accumulo/core/util/threads/NamedThreadFactory.java"", ""core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java"", ""server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/SecurityUtil.java"", ""server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionManager.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/log/DfsLogger.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java"", ""test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java""], ""pr_changed_test_files"": [""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java"", ""test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java""], ""base_commit"": ""97e10e61921f9a300e86ce78146074dad38d8c6c"", ""head_commit"": ""d80992f5cb5973f81ddb31a05b05b31d0b1f8bd3"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5576"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5576"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-02T18:49:41.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java\nindex 6f621255b4e..ceb35c5471a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java\n@@ -73,7 +73,7 @@ public class ThriftTransportPool {\n \n   private ThriftTransportPool(LongSupplier maxAgeMillis) {\n     this.maxAgeMillis = maxAgeMillis;\n-    this.checkThread = Threads.createThread(\""Thrift Connection Pool Checker\"", () -> {\n+    this.checkThread = Threads.createNonCriticalThread(\""Thrift Connection Pool Checker\"", () -> {\n       try {\n         final long minNanos = MILLISECONDS.toNanos(250);\n         final long maxNanos = MINUTES.toNanos(1);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/Halt.java b/core/src/main/java/org/apache/accumulo/core/util/Halt.java\nindex 1a9eb73fbff..4381ed9f52e 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/Halt.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/Halt.java\n@@ -45,7 +45,7 @@ public static void halt(final int status, final String msg, final Throwable exce\n     try {\n \n       // give ourselves a little time to try and do something\n-      Threads.createThread(\""Halt Thread\"", () -> {\n+      Threads.createNonCriticalThread(\""Halt Thread\"", () -> {\n         sleepUninterruptibly(100, MILLISECONDS);\n         Runtime.getRuntime().halt(status);\n       }).start();\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/threads/NamedThreadFactory.java b/core/src/main/java/org/apache/accumulo/core/util/threads/NamedThreadFactory.java\nindex fd632628bf7..abd453cb61a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/threads/NamedThreadFactory.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/threads/NamedThreadFactory.java\n@@ -55,6 +55,6 @@ public Thread newThread(Runnable r) {\n       threadName =\n           String.format(FORMAT, name, r.getClass().getSimpleName(), threadNum.getAndIncrement());\n     }\n-    return Threads.createThread(threadName, priority, r, handler);\n+    return Threads.createNonCriticalThread(threadName, priority, r, handler);\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java b/core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java\nindex fe962ee9017..611f7a67551 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/threads/Threads.java\n@@ -51,15 +51,15 @@ public static Runnable createNamedRunnable(String name, Runnable r) {\n     return new NamedRunnable(name, r);\n   }\n \n-  public static Thread createThread(String name, Runnable r) {\n-    return createThread(name, OptionalInt.empty(), r, UEH);\n+  public static Thread createNonCriticalThread(String name, Runnable r) {\n+    return createNonCriticalThread(name, OptionalInt.empty(), r, UEH);\n   }\n \n-  public static Thread createThread(String name, OptionalInt priority, Runnable r) {\n-    return createThread(name, priority, r, UEH);\n+  public static Thread createNonCriticalThread(String name, OptionalInt priority, Runnable r) {\n+    return createNonCriticalThread(name, priority, r, UEH);\n   }\n \n-  public static Thread createThread(String name, OptionalInt priority, Runnable r,\n+  public static Thread createNonCriticalThread(String name, OptionalInt priority, Runnable r,\n       UncaughtExceptionHandler ueh) {\n     Thread thread = new AccumuloDaemonThread(TraceUtil.wrap(r), name, ueh);\n     priority.ifPresent(thread::setPriority);\n@@ -67,6 +67,10 @@ public static Thread createThread(String name, OptionalInt priority, Runnable r,\n   }\n \n   public static Thread createCriticalThread(String name, Runnable r) {\n+    return createCriticalThread(name, OptionalInt.empty(), r);\n+  }\n+\n+  public static Thread createCriticalThread(String name, OptionalInt priority, Runnable r) {\n     Runnable wrapped = () -> {\n       try {\n         r.run();\n@@ -77,6 +81,6 @@ public static Thread createCriticalThread(String name, Runnable r) {\n       }\n     };\n \n-    return createThread(name, wrapped);\n+    return createNonCriticalThread(name, priority, wrapped);\n   }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java b/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\nindex 4bee11be067..37cb9ee381b 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\n@@ -243,7 +243,7 @@ public void startServiceLockVerificationThread() {\n     final long interval =\n         getConfiguration().getTimeInMillis(Property.GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL);\n     if (interval > 0) {\n-      verificationThread = Threads.createThread(\""service-lock-verification-thread\"",\n+      verificationThread = Threads.createCriticalThread(\""service-lock-verification-thread\"",\n           OptionalInt.of(Thread.NORM_PRIORITY + 1), () -> {\n             while (serverThread.isAlive()) {\n               ServiceLock lock = getLock();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java b/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\nindex 5ccc3b9335f..917424c6127 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\n@@ -91,8 +91,8 @@ public ServerConfigurationFactory(ServerContext context, SiteConfiguration siteC\n         Caffeine.newBuilder().expireAfterAccess(CACHE_EXPIRATION_HRS, TimeUnit.HOURS).build();\n \n     refresher = new ConfigRefreshRunner();\n-    Runtime.getRuntime()\n-        .addShutdownHook(Threads.createThread(\""config-refresh-shutdownHook\"", refresher::shutdown));\n+    Runtime.getRuntime().addShutdownHook(\n+        Threads.createNonCriticalThread(\""config-refresh-shutdownHook\"", refresher::shutdown));\n   }\n \n   public ServerContext getServerContext() {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\nindex 332e286a3fd..ea2572ffe7f 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n@@ -47,7 +47,6 @@\n import org.apache.accumulo.core.rpc.SslConnectionParams;\n import org.apache.accumulo.core.rpc.ThriftUtil;\n import org.apache.accumulo.core.rpc.UGIAssumingTransportFactory;\n-import org.apache.accumulo.core.util.Halt;\n import org.apache.accumulo.core.util.HostAndPort;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.core.util.UtilWaitThread;\n@@ -660,13 +659,7 @@ private static ServerAddress startTServer(ThriftServerType serverType, TimedProc\n \n     final TServer finalServer = serverAddress.server;\n \n-    Threads.createThread(threadName, () -> {\n-      try {\n-        finalServer.serve();\n-      } catch (Error e) {\n-        Halt.halt(1, \""Unexpected error in TThreadPoolServer \"" + e + \"", halting.\"", e);\n-      }\n-    }).start();\n+    Threads.createCriticalThread(threadName, finalServer::serve).start();\n \n     while (!finalServer.isServing()) {\n       // Wait for the thread to start and for the TServer to start\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/SecurityUtil.java b/server/base/src/main/java/org/apache/accumulo/server/security/SecurityUtil.java\nindex 7d5c7f69954..311665a67fd 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/SecurityUtil.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/SecurityUtil.java\n@@ -63,7 +63,7 @@ public static void serverLogin(AccumuloConfiguration acuConf, String keyTab, Str\n \n     if (login(principal, keyTab)) {\n       try {\n-        startTicketRenewalThread(acuConf, UserGroupInformation.getCurrentUser(),\n+        startTicketRenewalThread(UserGroupInformation.getCurrentUser(),\n             acuConf.getTimeInMillis(Property.GENERAL_KERBEROS_RENEWAL_PERIOD));\n         return;\n       } catch (IOException e) {\n@@ -115,13 +115,11 @@ public static String getServerPrincipal(String configuredPrincipal) {\n   /**\n    * Start a thread that periodically attempts to renew the current Kerberos user's ticket.\n    *\n-   * @param conf Accumulo configuration\n    * @param ugi The current Kerberos user.\n    * @param renewalPeriod The amount of time between attempting renewals.\n    */\n-  static void startTicketRenewalThread(AccumuloConfiguration conf, final UserGroupInformation ugi,\n-      final long renewalPeriod) {\n-    Threads.createThread(\""Kerberos Ticket Renewal\"", () -> {\n+  static void startTicketRenewalThread(final UserGroupInformation ugi, final long renewalPeriod) {\n+    Threads.createCriticalThread(\""Kerberos Ticket Renewal\"", () -> {\n       while (true) {\n         try {\n           renewalLog.debug(\""Invoking renewal attempt for Kerberos ticket\"");\n\ndiff --git a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\nindex 33ec336f4a4..46e282bc9b5 100644\n--- a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n+++ b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n@@ -738,8 +738,8 @@ public void run() {\n           final FileCompactorRunnable fcr =\n               createCompactionJob(job, totalInputEntries, totalInputBytes, started, stopped, err);\n \n-          final Thread compactionThread =\n-              Threads.createThread(\""Compaction job for tablet \"" + job.getExtent().toString(), fcr);\n+          final Thread compactionThread = Threads.createNonCriticalThread(\n+              \""Compaction job for tablet \"" + job.getExtent().toString(), fcr);\n \n           JOB_HOLDER.set(job, compactionThread, fcr.getFileCompactor());\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex 6a7ecb86fdc..0937089d965 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -1268,14 +1268,14 @@ public void run() {\n \n     context.getTableManager().addObserver(this);\n \n-    Thread statusThread = Threads.createThread(\""Status Thread\"", new StatusThread());\n+    Thread statusThread = Threads.createCriticalThread(\""Status Thread\"", new StatusThread());\n     statusThread.start();\n \n-    Threads.createThread(\""Migration Cleanup Thread\"", new MigrationCleanupThread()).start();\n+    Threads.createCriticalThread(\""Migration Cleanup Thread\"", new MigrationCleanupThread()).start();\n \n     tserverSet.startListeningForTabletServerChanges();\n \n-    Threads.createThread(\""ScanServer Cleanup Thread\"", new ScanServerZKCleaner()).start();\n+    Threads.createCriticalThread(\""ScanServer Cleanup Thread\"", new ScanServerZKCleaner()).start();\n \n     try {\n       blockForTservers();\n@@ -1381,8 +1381,8 @@ boolean canSuspendTablets() {\n       } catch (KeeperException | InterruptedException e) {\n         throw new IllegalStateException(\""Exception setting up delegation-token key manager\"", e);\n       }\n-      authenticationTokenKeyManagerThread =\n-          Threads.createThread(\""Delegation Token Key Manager\"", authenticationTokenKeyManager);\n+      authenticationTokenKeyManagerThread = Threads\n+          .createCriticalThread(\""Delegation Token Key Manager\"", authenticationTokenKeyManager);\n       authenticationTokenKeyManagerThread.start();\n       boolean logged = false;\n       while (!authenticationTokenKeyManager.isInitialized()) {\n@@ -1620,13 +1620,13 @@ private TServer setupReplication()\n \n     log.info(\""Started replication coordinator service at \"" + replAddress.address);\n     // Start the daemon to scan the replication table and make units of work\n-    replicationWorkThread = Threads.createThread(\""Replication Driver\"",\n+    replicationWorkThread = Threads.createNonCriticalThread(\""Replication Driver\"",\n         new org.apache.accumulo.manager.replication.ReplicationDriver(this));\n     replicationWorkThread.start();\n \n     // Start the daemon to assign work to tservers to replicate to our peers\n     var wd = new org.apache.accumulo.manager.replication.WorkDriver(this);\n-    replicationAssignerThread = Threads.createThread(wd.getName(), wd);\n+    replicationAssignerThread = Threads.createNonCriticalThread(wd.getName(), wd);\n     replicationAssignerThread.start();\n \n     // Advertise that port we used so peers don't have to be told what it is\n\ndiff --git a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\nindex fdd077713d8..defa59dacd4 100644\n--- a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n+++ b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n@@ -514,7 +514,7 @@ public void run() {\n     }\n \n     // need to regularly fetch data so plot data is updated\n-    Threads.createThread(\""Data fetcher\"", () -> {\n+    Threads.createCriticalThread(\""Data fetcher\"", () -> {\n       while (true) {\n         try {\n           fetchData();\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java\nindex 8b4f117a213..e6154e1392e 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/AssignmentHandler.java\n@@ -232,7 +232,7 @@ public void run() {\n               AssignmentHandler handler = new AssignmentHandler(server, extent, retryAttempt + 1);\n               if (extent.isMeta()) {\n                 if (extent.isRootTablet()) {\n-                  Threads.createThread(\""Root tablet assignment retry\"", handler).start();\n+                  Threads.createNonCriticalThread(\""Root tablet assignment retry\"", handler).start();\n                 } else {\n                   server.resourceManager.addMetaDataAssignment(extent, log, handler);\n                 }\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java\nindex deb5b8df2cd..97dad09224b 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletClientHandler.java\n@@ -1244,11 +1244,10 @@ public void loadTablet(TInfo tinfo, TCredentials credentials, String lock,\n     TabletLogger.loading(extent, server.getTabletSession());\n \n     final AssignmentHandler ah = new AssignmentHandler(server, extent);\n-    // final Runnable ah = new LoggingRunnable(log, );\n     // Root tablet assignment must take place immediately\n \n     if (extent.isRootTablet()) {\n-      Threads.createThread(\""Root Tablet Assignment\"", () -> {\n+      Threads.createNonCriticalThread(\""Root Tablet Assignment\"", () -> {\n         ah.run();\n         if (server.getOnlineTablets().containsKey(extent)) {\n           log.info(\""Root tablet loaded: {}\"", extent);\n@@ -1256,12 +1255,10 @@ public void loadTablet(TInfo tinfo, TCredentials credentials, String lock,\n           log.info(\""Root tablet failed to load\"");\n         }\n       }).start();\n+    } else if (extent.isMeta()) {\n+      server.resourceManager.addMetaDataAssignment(extent, log, ah);\n     } else {\n-      if (extent.isMeta()) {\n-        server.resourceManager.addMetaDataAssignment(extent, log, ah);\n-      } else {\n-        server.resourceManager.addAssignment(extent, log, ah);\n-      }\n+      server.resourceManager.addAssignment(extent, log, ah);\n     }\n   }\n \n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\nindex bee674f795b..8747bcac272 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n@@ -1118,7 +1118,7 @@ private static void checkWalCanSync(ServerContext context) {\n \n   private void config() {\n     log.info(\""Tablet server starting on {}\"", getHostname());\n-    Threads.createThread(\""Split/MajC initiator\"", new MajorCompactor(context)).start();\n+    Threads.createCriticalThread(\""Split/MajC initiator\"", new MajorCompactor(context)).start();\n \n     clientAddress = HostAndPort.fromParts(getHostname(), 0);\n \n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java\nindex 88767f03459..1fdfcffda4a 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServerResourceManager.java\n@@ -480,10 +480,10 @@ private class MemoryManagementFramework {\n       tabletReports = Collections.synchronizedMap(new HashMap<>());\n       memUsageReports = new LinkedBlockingQueue<>();\n       maxMem = context.getConfiguration().getAsBytes(Property.TSERV_MAXMEM);\n-      memoryGuardThread = Threads.createThread(\""Accumulo Memory Guard\"",\n+      memoryGuardThread = Threads.createCriticalThread(\""Accumulo Memory Guard\"",\n           OptionalInt.of(Thread.NORM_PRIORITY + 1), this::processTabletMemStats);\n       minorCompactionInitiatorThread =\n-          Threads.createThread(\""Accumulo Minor Compaction Initiator\"", this::manageMemory);\n+          Threads.createCriticalThread(\""Accumulo Minor Compaction Initiator\"", this::manageMemory);\n     }\n \n     void startThreads() {\n@@ -579,8 +579,6 @@ private void manageMemory() {\n                 }\n               }\n             }\n-\n-            // log.debug(\""mma.tabletsToMinorCompact = \""+mma.tabletsToMinorCompact);\n           }\n         } catch (Exception t) {\n           log.error(\""Minor compactions for memory management failed\"", t);\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionManager.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionManager.java\nindex 5b890f83870..5d201e04fa9 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionManager.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionManager.java\n@@ -315,7 +315,7 @@ private synchronized void checkForConfigChanges(boolean force) {\n \n   public void start() {\n     log.debug(\""Started compaction manager\"");\n-    Threads.createThread(\""Compaction Manager\"", () -> mainLoop()).start();\n+    Threads.createCriticalThread(\""Compaction Manager\"", () -> mainLoop()).start();\n   }\n \n   public CompactionServices getServices() {\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/log/DfsLogger.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/log/DfsLogger.java\nindex 1f56568486c..96f85f9ba53 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/log/DfsLogger.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/log/DfsLogger.java\n@@ -475,7 +475,8 @@ public synchronized void open(String address) throws IOException {\n       throw new IOException(ex);\n     }\n \n-    syncThread = Threads.createThread(\""Accumulo WALog thread \"" + this, new LogSyncingTask());\n+    syncThread =\n+        Threads.createCriticalThread(\""Accumulo WALog thread \"" + this, new LogSyncingTask());\n     syncThread.start();\n     op.await();\n     log.debug(\""Got new write-ahead log: {}\"", this);\n"", ""test_patch"": ""diff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java\nindex 459fc8b2d48..646196285a7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionMetricsIT.java\n@@ -116,7 +116,7 @@ public void testMetrics() throws Exception {\n       final LinkedBlockingQueue<Metric> queueMetrics = new LinkedBlockingQueue<>();\n       final AtomicBoolean shutdownTailer = new AtomicBoolean(false);\n \n-      Thread thread = Threads.createThread(\""metric-tailer\"", () -> {\n+      Thread thread = Threads.createNonCriticalThread(\""metric-tailer\"", () -> {\n         while (!shutdownTailer.get()) {\n           List<String> statsDMetrics = sink.getLines();\n           for (String s : statsDMetrics) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java\nindex 38105d5b4ba..7775d303f03 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionProgressIT.java\n@@ -270,7 +270,7 @@ public void testProgressViaMetrics() throws Exception {\n    */\n   private static Thread getMetricsCheckerThread(AtomicLong totalEntriesRead,\n       AtomicLong totalEntriesWritten) {\n-    return Threads.createThread(\""metric-tailer\"", () -> {\n+    return Threads.createNonCriticalThread(\""metric-tailer\"", () -> {\n       log.info(\""Starting metric tailer\"");\n \n       sink.getLines().clear();\n@@ -408,7 +408,7 @@ private String getDir(AccumuloClient client, String tableName) {\n   }\n \n   public Thread startChecker() {\n-    return Threads.createThread(\""RC checker\"", () -> {\n+    return Threads.createNonCriticalThread(\""RC checker\"", () -> {\n       try {\n         while (!stopCheckerThread.get()) {\n           checkRunning();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java b/test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java\nindex b444a273ce2..9d64eea5729 100644\n--- a/test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java\n+++ b/test/src/main/java/org/apache/accumulo/test/metrics/TestStatsDSink.java\n@@ -103,7 +103,7 @@ public static Metric parseStatsDMetric(String line) {\n   public TestStatsDSink() throws SocketException {\n     sock = new DatagramSocket();\n     int len = sock.getReceiveBufferSize();\n-    Threads.createThread(\""test-server-thread\"", () -> {\n+    Threads.createNonCriticalThread(\""test-server-thread\"", () -> {\n       while (!sock.isClosed()) {\n         byte[] buf = new byte[len];\n         DatagramPacket packet = new DatagramPacket(buf, len);\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5536"", ""pr_id"": 5536, ""issue_id"": 5529, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Shared mini cluster across multiple ITs\n**Is your feature request related to a problem? Please describe.**\nThe number of ITs which extend `SharedMiniClusterBase` has more than doubled from `2.1` to `main` (40 to 95). If the shared mini cluster ITs share one cluster between them all, this would reduce Jenkins and Maven build times. On Jenkins, starting the cluster takes about 10 seconds and also a couple seconds to stop the cluster. This change would currently be around a 20 minute speedup, and more as more tests are added. The improved maven build times would also be nice for testing locally.\n\n**Describe the solution you'd like**\nReplace as many `SharedMiniClusterBase` tests as possible to instead share a cluster between them.\n\n**Additional context**\nThis had been discussed before, but I don't remember who first suggested it."", ""issue_word_count"": 138, ""test_files_count"": 38, ""non_test_files_count"": 2, ""pr_changed_files"": [""pom.xml"", ""src/build/ci/find-unapproved-junit.sh"", ""test/pom.xml"", ""test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java"", ""test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java"", ""test/src/main/java/org/apache/accumulo/suites/SimpleSharedMacTestSuite.java"", ""test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java"", ""test/src/main/java/org/apache/accumulo/test/CloneIT.java"", ""test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java"", ""test/src/main/java/org/apache/accumulo/test/CreateTableIT.java"", ""test/src/main/java/org/apache/accumulo/test/GCRunIT.java"", ""test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/NamespacesIT.java"", ""test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java"", ""test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java"", ""test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java"", ""test/src/main/java/org/apache/accumulo/test/SampleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java"", ""test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java"", ""test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java"", ""test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java""], ""pr_changed_test_files"": [""test/pom.xml"", ""test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java"", ""test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java"", ""test/src/main/java/org/apache/accumulo/suites/SimpleSharedMacTestSuite.java"", ""test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java"", ""test/src/main/java/org/apache/accumulo/test/CloneIT.java"", ""test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java"", ""test/src/main/java/org/apache/accumulo/test/CreateTableIT.java"", ""test/src/main/java/org/apache/accumulo/test/GCRunIT.java"", ""test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/NamespacesIT.java"", ""test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java"", ""test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java"", ""test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java"", ""test/src/main/java/org/apache/accumulo/test/SampleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java"", ""test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java"", ""test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java"", ""test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java""], ""base_commit"": ""5b330d48bda767e788037458b1a9e3ed8c4d5712"", ""head_commit"": ""6a1d09e800dcf50c2e4a4b0dedfb76e40bd38d11"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5536"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5536"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-15T11:47:48.000Z"", ""patch"": ""diff --git a/pom.xml b/pom.xml\nindex 58931ff164c..eb35b6b88ea 100644\n--- a/pom.xml\n+++ b/pom.xml\n@@ -945,6 +945,7 @@\n                 <!-- ignore log4j dep used for annotations and needed for compile time linting -->\n                 <unused>biz.aQute.bnd:biz.aQute.bnd.annotation:jar:*</unused>\n                 <unused>org.junit.jupiter:junit-jupiter-engine:jar:*</unused>\n+                <unused>org.junit.platform:junit-platform-suite-engine:jar:*</unused>\n               </ignoredUnusedDeclaredDependencies>\n             </configuration>\n           </execution>\n@@ -1152,12 +1153,29 @@\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-failsafe-plugin</artifactId>\n         <executions>\n+          <execution>\n+            <id>run-integration-test-suite</id>\n+            <goals>\n+              <goal>integration-test</goal>\n+              <goal>verify</goal>\n+            </goals>\n+            <configuration>\n+              <test>org.apache.accumulo.suites.SimpleSharedMacTestSuite</test>\n+            </configuration>\n+          </execution>\n           <execution>\n             <id>run-integration-tests</id>\n             <goals>\n               <goal>integration-test</goal>\n               <goal>verify</goal>\n             </goals>\n+            <configuration>\n+              <!--\n+                exclude tests that were run as part of the test suite and\n+                the test suite itself\n+              -->\n+              <excludedGroups>SimpleMiniClusterSuite</excludedGroups>\n+            </configuration>\n           </execution>\n         </executions>\n       </plugin>\n\ndiff --git a/src/build/ci/find-unapproved-junit.sh b/src/build/ci/find-unapproved-junit.sh\nindex da8ac47cb07..8802208e8eb 100755\n--- a/src/build/ci/find-unapproved-junit.sh\n+++ b/src/build/ci/find-unapproved-junit.sh\n@@ -35,7 +35,7 @@ function findalljunitproblems() {\n     opts='-PRlH'\n   fi\n   # find any new classes using something other than the jupiter API, except those allowed\n-  grep \""$opts\"" --include='*.java' 'org[.]junit[.](?!jupiter)' | grep -Pv \""^(${ALLOWED_PIPE_SEP//./[.]})\\$\""\n+  grep \""$opts\"" --include='*.java' 'org[.]junit[.](?!(jupiter|platform[.]suite)' | grep -Pv \""^(${ALLOWED_PIPE_SEP//./[.]})\\$\""\n   if ((${#ALLOWED[@]} != 0)); then\n     # find any uses of the jupiter API in the allowed vintage classes\n     grep \""$opts\"" 'org[.]junit[.]jupiter' \""${ALLOWED[@]}\""\n"", ""test_patch"": ""diff --git a/test/pom.xml b/test/pom.xml\nindex a02d7fadb99..41ba33ab3b7 100644\n--- a/test/pom.xml\n+++ b/test/pom.xml\n@@ -198,6 +198,14 @@\n       <groupId>org.junit.jupiter</groupId>\n       <artifactId>junit-jupiter-params</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.junit.platform</groupId>\n+      <artifactId>junit-platform-suite-api</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.junit.platform</groupId>\n+      <artifactId>junit-platform-suite-engine</artifactId>\n+    </dependency>\n     <dependency>\n       <groupId>org.slf4j</groupId>\n       <artifactId>slf4j-api</artifactId>\n\ndiff --git a/test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java b/test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java\nindex c218f41c205..41c49a9cbef 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/AccumuloITBase.java\n@@ -50,6 +50,7 @@ public class AccumuloITBase extends WithTestNames {\n   public static final String SUNNY_DAY = \""SunnyDay\"";\n   public static final String MINI_CLUSTER_ONLY = \""MiniClusterOnly\"";\n   public static final String ZOOKEEPER_TESTING_SERVER = \""ZooKeeperTestingServer\"";\n+  public static final String SIMPLE_MINI_CLUSTER_SUITE = \""SimpleMiniClusterSuite\"";\n \n   protected <T> T getOnlyElement(Collection<T> c) {\n     return c.stream().collect(onlyElement());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java b/test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java\nindex 360ecbc931a..68ab06201ef 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/SharedMiniClusterBase.java\n@@ -28,16 +28,25 @@\n import java.nio.file.Path;\n import java.util.Optional;\n import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Function;\n import java.util.function.Predicate;\n import java.util.stream.Stream;\n \n import org.apache.accumulo.cluster.ClusterUser;\n import org.apache.accumulo.cluster.ClusterUsers;\n+import org.apache.accumulo.core.client.Accumulo;\n+import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.client.AccumuloSecurityException;\n+import org.apache.accumulo.core.client.NamespaceNotEmptyException;\n+import org.apache.accumulo.core.client.NamespaceNotFoundException;\n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;\n import org.apache.accumulo.core.client.security.tokens.KerberosToken;\n import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n import org.apache.accumulo.core.clientImpl.ClientInfo;\n+import org.apache.accumulo.core.clientImpl.Namespace;\n import org.apache.accumulo.core.conf.ClientProperty;\n import org.apache.accumulo.miniclusterImpl.MiniAccumuloClusterImpl;\n import org.apache.hadoop.conf.Configuration;\n@@ -67,6 +76,7 @@\n public abstract class SharedMiniClusterBase extends AccumuloITBase implements ClusterUsers {\n   private static final Logger log = LoggerFactory.getLogger(SharedMiniClusterBase.class);\n   public static final String TRUE = Boolean.toString(true);\n+  protected static final AtomicBoolean STOP_DISABLED = new AtomicBoolean(false);\n \n   private static String rootPassword;\n   private static AuthenticationToken token;\n@@ -86,8 +96,12 @@ public static void startMiniCluster() throws Exception {\n    *\n    * @param miniClusterCallback A callback to configure the minicluster before it is started.\n    */\n-  public static void startMiniClusterWithConfig(\n+  public static synchronized void startMiniClusterWithConfig(\n       MiniClusterConfigurationCallback miniClusterCallback) throws Exception {\n+    if (cluster != null) {\n+      return;\n+    }\n+\n     File baseDir = Path.of(System.getProperty(\""user.dir\"") + \""/target/mini-tests\"").toFile();\n     assertTrue(baseDir.mkdirs() || baseDir.isDirectory());\n \n@@ -132,10 +146,48 @@ private static String getTestClassName() {\n   /**\n    * Stops the MiniAccumuloCluster and related services if they are running.\n    */\n-  public static void stopMiniCluster() {\n+  public static synchronized void stopMiniCluster() {\n+    if (STOP_DISABLED.get()) {\n+      // If stop is disabled, then we are likely running a\n+      // test class that is part of a larger suite. We don't\n+      // want to shut down the cluster, but we should clean\n+      // up any tables that were created, but not deleted,\n+      // by the test class. This will prevent issues with\n+      // subsequent tests that count objects or initiate\n+      // compactions and wait for them, but some other table\n+      // from a prior test is compacting.\n+      try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {\n+        for (String tableName : client.tableOperations().list()) {\n+          if (!tableName.startsWith(Namespace.ACCUMULO.name() + \"".\"")) {\n+            try {\n+              client.tableOperations().delete(tableName);\n+            } catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {\n+              log.error(\""Error deleting table {}\"", tableName, e);\n+            }\n+          }\n+        }\n+        try {\n+          for (String namespaceName : client.namespaceOperations().list()) {\n+            if (!namespaceName.equals(Namespace.ACCUMULO.name())\n+                && !namespaceName.equals(Namespace.DEFAULT.name())) {\n+              try {\n+                client.namespaceOperations().delete(namespaceName);\n+              } catch (AccumuloException | AccumuloSecurityException | NamespaceNotFoundException\n+                  | NamespaceNotEmptyException e) {\n+                log.error(\""Error deleting namespace {}\"", namespaceName, e);\n+              }\n+            }\n+          }\n+        } catch (AccumuloSecurityException | AccumuloException e) {\n+          log.error(\""Error listing namespaces\"", e);\n+        }\n+      }\n+      return;\n+    }\n     if (cluster != null) {\n       try {\n         cluster.stop();\n+        cluster = null;\n       } catch (Exception e) {\n         log.error(\""Failed to stop minicluster\"", e);\n       }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/suites/SimpleSharedMacTestSuite.java b/test/src/main/java/org/apache/accumulo/suites/SimpleSharedMacTestSuite.java\nnew file mode 100644\nindex 00000000000..fdd36fd9989\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/suites/SimpleSharedMacTestSuite.java\n@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.suites;\n+\n+import static org.apache.accumulo.harness.AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE;\n+\n+import org.apache.accumulo.harness.SharedMiniClusterBase;\n+import org.junit.platform.suite.api.AfterSuite;\n+import org.junit.platform.suite.api.BeforeSuite;\n+import org.junit.platform.suite.api.IncludeClassNamePatterns;\n+import org.junit.platform.suite.api.IncludeTags;\n+import org.junit.platform.suite.api.SelectPackages;\n+import org.junit.platform.suite.api.Suite;\n+\n+@Suite\n+@SelectPackages(\""org.apache.accumulo.test\"") // look in this package and subpackages\n+@IncludeTags(SIMPLE_MINI_CLUSTER_SUITE) // for tests with this tag\n+@IncludeClassNamePatterns(\"".*IT\"") // need to override the default pattern \"".*Test\""\n+public class SimpleSharedMacTestSuite extends SharedMiniClusterBase {\n+\n+  @BeforeSuite\n+  public static void beforeAllTests() throws Exception {\n+    SharedMiniClusterBase.startMiniCluster();\n+\n+    // Disable tests that are run as part of this suite\n+    // from stopping MiniAccumuloCluster in there JUnit\n+    // lifecycle methods (e.g. AfterEach, AfterAll)\n+    SharedMiniClusterBase.STOP_DISABLED.set(true);\n+  }\n+\n+  @AfterSuite\n+  public static void afterAllTests() throws Exception {\n+    SharedMiniClusterBase.STOP_DISABLED.set(false);\n+    SharedMiniClusterBase.stopMiniCluster();\n+  }\n+}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java b/test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java\nindex 13b0cbf4109..92b749ba80a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/AdminCheckIT.java\n@@ -40,6 +40,7 @@\n import org.apache.accumulo.core.client.AccumuloClient;\n import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n@@ -52,11 +53,13 @@\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n import com.beust.jcommander.JCommander;\n import com.google.common.collect.Sets;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class AdminCheckIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/CloneIT.java b/test/src/main/java/org/apache/accumulo/test/CloneIT.java\nindex d00d77bd2f6..e8934b58622 100644\n--- a/test/src/main/java/org/apache/accumulo/test/CloneIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/CloneIT.java\n@@ -44,12 +44,14 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n import org.apache.accumulo.core.metadata.schema.TabletDeletedException;\n import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.util.MetadataTableUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.ExtensionContext;\n import org.junit.jupiter.params.ParameterizedTest;\n@@ -57,6 +59,7 @@\n import org.junit.jupiter.params.provider.ArgumentsProvider;\n import org.junit.jupiter.params.provider.ArgumentsSource;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class CloneIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java b/test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java\nindex 51557edac8a..9595fd9667d 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ComprehensiveIT.java\n@@ -22,12 +22,14 @@\n \n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.Tag;\n \n @Tag(SUNNY_DAY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ComprehensiveIT extends ComprehensiveBaseIT {\n   @BeforeAll\n   public static void setup() throws Exception {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/CreateTableIT.java b/test/src/main/java/org/apache/accumulo/test/CreateTableIT.java\nindex 8c3fd77d7f5..f9ae5c529dc 100644\n--- a/test/src/main/java/org/apache/accumulo/test/CreateTableIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/CreateTableIT.java\n@@ -31,13 +31,16 @@\n import org.apache.accumulo.core.client.TableExistsException;\n import org.apache.accumulo.core.metadata.SystemTables;\n import org.apache.accumulo.core.util.Timer;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class CreateTableIT extends SharedMiniClusterBase {\n   private static final Logger log = LoggerFactory.getLogger(CreateTableIT.class);\n   public static final int NUM_TABLES = 500;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/GCRunIT.java b/test/src/main/java/org/apache/accumulo/test/GCRunIT.java\nindex 599811c95ce..d3b49cc2fdc 100644\n--- a/test/src/main/java/org/apache/accumulo/test/GCRunIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/GCRunIT.java\n@@ -45,6 +45,7 @@\n import org.apache.accumulo.core.security.ColumnVisibility;\n import org.apache.accumulo.core.security.TablePermission;\n import org.apache.accumulo.gc.GCRun;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.minicluster.ServerType;\n import org.apache.hadoop.io.Text;\n@@ -57,6 +58,7 @@\n import org.slf4j.LoggerFactory;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class GCRunIT extends SharedMiniClusterBase {\n \n   public static final Logger log = LoggerFactory.getLogger(GCRunIT.class);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java b/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java\nindex 4a483eca736..7411c692406 100644\n--- a/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/MultiTableBatchWriterIT.java\n@@ -41,15 +41,18 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n import com.google.common.collect.Maps;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class MultiTableBatchWriterIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/NamespacesIT.java b/test/src/main/java/org/apache/accumulo/test/NamespacesIT.java\nindex b9053adc43f..81aca7658ea 100644\n--- a/test/src/main/java/org/apache/accumulo/test/NamespacesIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/NamespacesIT.java\n@@ -79,6 +79,7 @@\n import org.apache.accumulo.core.security.SystemPermission;\n import org.apache.accumulo.core.security.TablePermission;\n import org.apache.accumulo.core.util.tables.TableNameUtil;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.constraints.NumericValueConstraint;\n import org.apache.commons.lang3.StringUtils;\n@@ -95,6 +96,7 @@\n  * Test different namespace permissions\n  */\n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class NamespacesIT extends SharedMiniClusterBase {\n \n   private AccumuloClient c;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java b/test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java\nindex 999c1f0c9a0..09d30333e6d 100644\n--- a/test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/NewTableConfigurationIT.java\n@@ -48,12 +48,15 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class NewTableConfigurationIT extends SharedMiniClusterBase {\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java b/test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java\nindex 05c1ba88711..0c9e27d2c38 100644\n--- a/test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/OrIteratorIT.java\n@@ -46,12 +46,15 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.iterators.OrIterator;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class OrIteratorIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java b/test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java\nindex 641899d6742..b5d35c9ea30 100644\n--- a/test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/PrintInfoIT.java\n@@ -41,14 +41,17 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.file.rfile.PrintInfo;\n import org.apache.accumulo.core.security.ColumnVisibility;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.miniclusterImpl.MiniAccumuloClusterImpl;\n import org.apache.commons.io.FileUtils;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.io.TempDir;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class PrintInfoIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/SampleIT.java b/test/src/main/java/org/apache/accumulo/test/SampleIT.java\nindex 928ad740f9f..3c55b361aa9 100644\n--- a/test/src/main/java/org/apache/accumulo/test/SampleIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/SampleIT.java\n@@ -66,15 +66,18 @@\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n import org.apache.accumulo.core.iterators.WrappingIterator;\n import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.util.FileMetadataUtil;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n import com.google.common.collect.Iterables;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class SampleIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java b/test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java\nindex 7e06a43872b..28628a7e3e3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ScanServerMetadataEntriesCleanIT.java\n@@ -28,6 +28,7 @@\n import java.util.stream.Stream;\n \n import org.apache.accumulo.core.metadata.ScanServerRefTabletFile;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.util.ScanServerMetadataEntries;\n@@ -39,6 +40,7 @@\n import com.google.common.net.HostAndPort;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ScanServerMetadataEntriesCleanIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java b/test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java\nindex d5e93c515cd..547642c6cf6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ZooKeeperPropertiesIT.java\n@@ -36,6 +36,7 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.store.NamespacePropKey;\n@@ -43,9 +44,11 @@\n import org.apache.accumulo.server.util.PropUtil;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.Timeout;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ZooKeeperPropertiesIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\nindex 45593012357..b6baba3b58f 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\n@@ -38,17 +38,20 @@\n import org.apache.accumulo.core.metadata.schema.Ample.DataLevel;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n import org.apache.accumulo.core.metadata.schema.TabletOperationType;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.ample.metadata.TestAmple;\n import org.apache.accumulo.test.ample.metadata.TestAmple.TestServerAmpleImpl;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.ValueSource;\n \n import com.google.common.base.Preconditions;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class TestAmpleIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java b/test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java\nindex abc9242b586..8370de1b33e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/usage/TabletFileUpdateIT.java\n@@ -40,6 +40,7 @@\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.MetadataTime;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.Location;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.tablets.TabletTime;\n import org.apache.accumulo.test.ample.metadata.TestAmple;\n@@ -47,12 +48,14 @@\n import org.apache.hadoop.fs.Path;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n /**\n  * Tests tablet usage of ample to add a new minor compacted file to tablet. This tests edge cases,\n  * the normal cases are well tested by many other ITs from simply running Accumulo.\n  */\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class TabletFileUpdateIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java b/test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java\nindex d7720b9f141..5bb1303e1f6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/conf/PropStoreConfigIT.java\n@@ -48,6 +48,7 @@\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n import org.apache.accumulo.core.rpc.clients.ThriftClientTypes;\n import org.apache.accumulo.core.trace.TraceUtil;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.store.NamespacePropKey;\n@@ -63,6 +64,7 @@\n import org.slf4j.LoggerFactory;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class PropStoreConfigIT extends SharedMiniClusterBase {\n \n   private static final Logger log = LoggerFactory.getLogger(PropStoreConfigIT.class);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java b/test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java\nindex dcd57ec5e78..e9d109f1f93 100644\n--- a/test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/conf/util/ZooPropEditorIT.java\n@@ -24,6 +24,7 @@\n \n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.conf.util.ZooPropEditor;\n import org.apache.accumulo.test.util.Wait;\n@@ -35,6 +36,7 @@\n import org.slf4j.LoggerFactory;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ZooPropEditorIT extends SharedMiniClusterBase {\n \n   private static final Logger LOG = LoggerFactory.getLogger(ZooPropEditorIT.class);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\nindex da3b76a242b..ddf484c271e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n@@ -55,16 +55,19 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.Repo;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n import com.google.common.collect.Iterators;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public abstract class FateExecutionOrderIT extends SharedMiniClusterBase\n     implements FateTestRunner<FateExecutionOrderIT.FeoTestEnv> {\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\nindex 59548cc7384..591e3cd2aa7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n@@ -74,6 +74,7 @@\n import org.apache.accumulo.core.tabletserver.log.LogEntry;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.core.util.time.SteadyTime;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.manager.Manager;\n import org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason;\n@@ -97,6 +98,7 @@\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.EnumSource;\n@@ -104,6 +106,7 @@\n \n import com.google.common.collect.Sets;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ManagerRepoIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java\nindex 1d80a5fb9b6..bdfb5ac2a06 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/FateMutatorImplIT.java\n@@ -38,14 +38,17 @@\n import org.apache.accumulo.core.fate.user.FateMutator;\n import org.apache.accumulo.core.fate.user.FateMutatorImpl;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.fate.FateIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class FateMutatorImplIT extends SharedMiniClusterBase {\n \n   Logger log = LoggerFactory.getLogger(FateMutatorImplIT.class);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\nindex 30d45fedfb9..c53a6ff91da 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n@@ -40,13 +40,16 @@\n import org.apache.accumulo.core.iterators.user.VersioningIterator;\n import org.apache.accumulo.core.metadata.SystemTables;\n import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.fate.FateIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class UserFateIT extends FateIT {\n \n   private String table;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\nindex 67f5cfb1b07..8fdb3e6e8a5 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\n@@ -25,11 +25,14 @@\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.fate.FatePoolsWatcherIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class UserFatePoolsWatcherIT extends FatePoolsWatcherIT {\n \n   private String table;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\nindex d2d8c3ec125..a591c95cddd 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\n@@ -24,13 +24,16 @@\n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.fate.FateStatusEnforcementIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class UserFateStatusEnforcementIT extends FateStatusEnforcementIT {\n   private ClientContext client;\n   private String table;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\nindex a112445d993..590284c62d9 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\n@@ -32,12 +32,15 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.fate.FateStoreIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class UserFateStoreFateIT extends FateStoreIT {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\nindex dff619c01f6..2c87e811ea3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\n@@ -30,13 +30,16 @@\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.fate.MultipleStoresIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class UserMultipleStoresIT extends MultipleStoresIT {\n   private ClientContext client;\n   private String tableName;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\nindex 5a72053d93c..93109f06339 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n@@ -55,14 +55,17 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n import com.google.common.collect.ImmutableMap;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class AddSplitIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java b/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java\nindex f0e71d6424a..a0a4c7b7d18 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/CloneTestIT.java\n@@ -67,6 +67,7 @@\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.security.NamespacePermission;\n import org.apache.accumulo.core.security.TablePermission;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.miniclusterImpl.MiniAccumuloClusterImpl;\n import org.apache.hadoop.fs.FileStatus;\n@@ -75,8 +76,10 @@\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class CloneTestIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java b/test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java\nindex 6060f1a62cd..4b5286cb8fa 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/DeletedTablesDontFlushIT.java\n@@ -28,12 +28,15 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n // ACCUMULO-2880\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class DeletedTablesDontFlushIT extends SharedMiniClusterBase {\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java b/test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java\nindex e7a9d415037..149e89b654e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/FileNormalizationIT.java\n@@ -40,12 +40,14 @@\n import org.apache.accumulo.core.metadata.SystemTables;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n import org.apache.accumulo.core.security.TablePermission;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.test.TestIngest;\n import org.apache.accumulo.test.VerifyIngest;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -56,6 +58,7 @@\n  * to update the metadata table then the key will not match. The mismatch could result in duplicate\n  * entries.\n  */\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class FileNormalizationIT extends SharedMiniClusterBase {\n \n   private static final Logger log = LoggerFactory.getLogger(FileNormalizationIT.class);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java b/test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java\nindex 9721c82a987..16be75fa5b5 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/FindCompactionTmpFilesIT.java\n@@ -33,6 +33,7 @@\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.tablets.TabletNameGenerator;\n@@ -42,8 +43,10 @@\n import org.apache.hadoop.fs.Path;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class FindCompactionTmpFilesIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java b/test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java\nindex a637c33efc0..86efc0667be 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/MergeTabletsIT.java\n@@ -18,10 +18,13 @@\n  */\n package org.apache.accumulo.test.functional;\n \n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class MergeTabletsIT extends MergeTabletsBaseIT {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java b/test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java\nindex e5206838435..b00635f13d6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/shell/ConfigSetIT.java\n@@ -26,6 +26,7 @@\n import org.apache.accumulo.core.client.AccumuloClient;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n@@ -35,6 +36,7 @@\n import org.slf4j.LoggerFactory;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ConfigSetIT extends SharedMiniClusterBase {\n   @BeforeAll\n   public static void setup() throws Exception {\n@@ -49,7 +51,6 @@ public static void teardown() {\n   private static final Logger log = LoggerFactory.getLogger(ConfigSetIT.class);\n \n   @Test\n-  @SuppressWarnings(\""deprecation\"")\n   public void setInvalidJson() throws Exception {\n     log.debug(\""Starting setInvalidJson test ------------------\"");\n \n@@ -64,6 +65,7 @@ public void setInvalidJson() throws Exception {\n           validJson);\n       assertThrows(AccumuloException.class, () -> client.instanceOperations()\n           .setProperty(MONITOR_RESOURCES_EXTERNAL.getKey(), invalidJson));\n+      client.instanceOperations().removeProperty(COMPACTION_SERVICE_DEFAULT_GROUPS.getKey());\n \n     }\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java b/test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java\nindex 8dd631adea0..2969b4a1e95 100644\n--- a/test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/shell/ShellAuthenticatorIT.java\n@@ -24,6 +24,7 @@\n import java.io.IOException;\n import java.util.TimeZone;\n \n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.shell.Shell;\n import org.apache.accumulo.test.shell.ShellIT.StringInputStream;\n@@ -37,8 +38,10 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n import org.junit.jupiter.api.Test;\n \n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ShellAuthenticatorIT extends SharedMiniClusterBase {\n \n   @BeforeAll\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java b/test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java\nindex ac83451c7f3..84bcc72cd25 100644\n--- a/test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/shell/ShellIT.java\n@@ -36,6 +36,7 @@\n \n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.conf.PropertyType;\n+import org.apache.accumulo.harness.AccumuloITBase;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.shell.Shell;\n import org.jline.reader.LineReader;\n@@ -53,6 +54,7 @@\n import org.slf4j.LoggerFactory;\n \n @Tag(MINI_CLUSTER_ONLY)\n+@Tag(AccumuloITBase.SIMPLE_MINI_CLUSTER_SUITE)\n public class ShellIT extends SharedMiniClusterBase {\n \n   @Override\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5509"", ""pr_id"": 5509, ""issue_id"": 5501, ""repo"": ""apache/accumulo"", ""problem_statement"": ""ReadyMonitor thread failure on ZooKeeper restart\n**Describe the bug**\nI'm seeing the following stack trace when running `CompactionIT` in `main`. I don't see this stack trace when running the same IT in `2.1`.\n\n```\n2025-04-24T12:46:19,860 71 [threads.AccumuloUncaughtExceptionHandler] ERROR: Caught an Exception in Thread[Thrift Connection Pool Checker,5,main]. Thread is dead.\njava.lang.IllegalStateException: prop-store failed to be ready within 26250 ms\n\tat org.apache.accumulo.server.conf.store.impl.ReadyMonitor.isReady(ReadyMonitor.java:86) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.store.impl.ZooPropStore.checkZkConnection(ZooPropStore.java:343) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.store.impl.ZooPropStore.get(ZooPropStore.java:149) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.util.PropSnapshot.updateSnapshot(PropSnapshot.java:104) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.util.PropSnapshot.getVersionedProperties(PropSnapshot.java:69) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.ZooBasedConfiguration.getDataVersion(ZooBasedConfiguration.java:72) ~[classes/:?]\n\tat org.apache.accumulo.server.conf.ZooBasedConfiguration.getUpdateCount(ZooBasedConfiguration.java:89) ~[classes/:?]\n\tat org.apache.accumulo.core.conf.AccumuloConfiguration$DeriverImpl.derive(AccumuloConfiguration.java:500) ~[classes/:?]\n\tat org.apache.accumulo.core.conf.AccumuloConfiguration.getDuration(AccumuloConfiguration.java:289) ~[classes/:?]\n\tat org.apache.accumulo.core.conf.AccumuloConfiguration.getTimeInMillis(AccumuloConfiguration.java:301) ~[classes/:?]\n\tat org.apache.accumulo.core.clientImpl.ClientContext.lambda$3(ClientContext.java:259) ~[classes/:?]\n\tat com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:315) ~[guava-33.4.6-jre.jar:?]\n\tat org.apache.accumulo.core.clientImpl.ClientContext.getClientTimeoutInMillis(ClientContext.java:373) ~[classes/:?]\n \tat org.apache.accumulo.server.ServerContext.getTransportPoolMaxAgeMillis(ServerContext.java:437) ~[classes/:?]\n\tat org.apache.accumulo.core.clientImpl.ClientContext.lambda$11(ClientContext.java:1039) ~[classes/:?]\n\tat org.apache.accumulo.core.clientImpl.ThriftTransportPool.lambda$0(ThriftTransportPool.java:84) ~[classes/:?]\n\tat org.apache.accumulo.core.trace.TraceWrappedRunnable.run(TraceWrappedRunnable.java:52) ~[classes/:?]\n\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n```\n\n\n**Versions (OS, Maven, Java, and others, as appropriate):**\n - Affected version(s) of this project: 4.0\n\n**To Reproduce**\nRun `CompactionIT` in `main`. I'm seeing this stack trace multiple times.\n\n**Expected behavior**\nShould not see the stack trace from the thread dying.\n\n**Additional context**\nI **think** this may be related to the recent `ZooSession` changes in `main`, but not entirely sure.\n"", ""issue_word_count"": 363, ""test_files_count"": 3, ""non_test_files_count"": 4, ""pr_changed_files"": [""minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java"", ""minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java"", ""minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/ServerContext.java"", ""test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java"", ""test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java""], ""pr_changed_test_files"": [""test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java"", ""test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java""], ""base_commit"": ""802c70021a9e7234b4bb1ff6adf4db30a7e8ce7d"", ""head_commit"": ""b5a3214627b9333805c14a298fbe98fe29738641"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5509"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5509"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-15T10:55:47.000Z"", ""patch"": ""diff --git a/minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java b/minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java\nindex 39e79a50c3e..ee8d2cbc644 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/cluster/AccumuloCluster.java\n@@ -36,6 +36,10 @@\n  */\n public interface AccumuloCluster {\n \n+  enum State {\n+    STARTED, STOPPED, TERMINATED;\n+  }\n+\n   /**\n    * @return Accumulo instance name\n    */\n@@ -47,6 +51,9 @@ public interface AccumuloCluster {\n   String getZooKeepers();\n \n   /**\n+   * An AccumuloCluster can be started and stopped. Callers that keep a reference to the\n+   * ServerContext may experience errors when trying to use it while the cluster is stopped.\n+   *\n    * @return ServerContext\n    */\n   ServerContext getServerContext();\n@@ -96,6 +103,11 @@ public interface AccumuloCluster {\n    */\n   void stop() throws Exception;\n \n+  /**\n+   * Cleans up the AccumuloCluster, stopping it if necessary. Cannot be restarted after this call.\n+   */\n+  void terminate() throws Exception;\n+\n   /**\n    * @return the {@link FileSystem} in use by this cluster\n    */\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\nindex d60f8f5dc03..c9a62008f09 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/cluster/standalone/StandaloneAccumuloCluster.java\n@@ -26,6 +26,7 @@\n import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Supplier;\n \n import org.apache.accumulo.cluster.AccumuloCluster;\n@@ -44,6 +45,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n+import com.google.common.base.Preconditions;\n import com.google.common.base.Suppliers;\n \n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n@@ -65,6 +67,8 @@ public class StandaloneAccumuloCluster implements AccumuloCluster {\n   private String serverCmdPrefix;\n   private final SiteConfiguration siteConfig;\n   private final Supplier<ServerContext> contextSupplier;\n+  private volatile State clusterState = State.STOPPED;\n+  private final AtomicBoolean serverContextCreated = new AtomicBoolean(false);\n \n   @SuppressFBWarnings(value = \""PATH_TRAVERSAL_IN\"",\n       justification = \""code runs in same security context as user who provided input file name\"")\n@@ -135,6 +139,7 @@ public String getZooKeepers() {\n \n   @Override\n   public ServerContext getServerContext() {\n+    serverContextCreated.set(true);\n     return contextSupplier.get();\n   }\n \n@@ -156,6 +161,8 @@ public StandaloneClusterControl getClusterControl() {\n \n   @Override\n   public void start() throws IOException {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot start a cluster that is terminated.\"");\n     StandaloneClusterControl control = getClusterControl();\n \n     // TODO We can check the hosts files, but that requires us to be on a host with the\n@@ -166,10 +173,13 @@ public void start() throws IOException {\n     for (ServerType type : ALL_SERVER_TYPES) {\n       control.startAllServers(type);\n     }\n+    clusterState = State.STARTED;\n   }\n \n   @Override\n   public void stop() throws IOException {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot stop a cluster that is terminated.\"");\n     StandaloneClusterControl control = getClusterControl();\n \n     // TODO We can check the hosts files, but that requires us to be on a host with the\n@@ -178,6 +188,22 @@ public void stop() throws IOException {\n     for (ServerType type : ALL_SERVER_TYPES) {\n       control.stopAllServers(type);\n     }\n+    clusterState = State.STOPPED;\n+  }\n+\n+  @Override\n+  public void terminate() throws Exception {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot stop a cluster that is terminated.\"");\n+\n+    if (clusterState != State.STOPPED) {\n+      stop();\n+    }\n+\n+    if (serverContextCreated.get()) {\n+      getServerContext().close();\n+    }\n+    clusterState = State.TERMINATED;\n   }\n \n   public Configuration getHadoopConfiguration() {\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\nindex 1301095e6eb..c2dc708cdc6 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\n@@ -123,6 +123,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n import com.google.common.base.Suppliers;\n \n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n@@ -144,16 +145,18 @@ public class MiniAccumuloClusterImpl implements AccumuloCluster {\n   private final MiniAccumuloConfigImpl config;\n   private final Supplier<Properties> clientProperties;\n   private final SiteConfiguration siteConfig;\n-  private final Supplier<ServerContext> context;\n   private final AtomicReference<MiniDFSCluster> miniDFS = new AtomicReference<>();\n   private final List<Process> cleanup = new ArrayList<>();\n   private final MiniAccumuloClusterControl clusterControl;\n+  private final Supplier<ServerContext> context;\n+  private final AtomicBoolean serverContextCreated = new AtomicBoolean(false);\n \n   private boolean initialized = false;\n   private volatile ExecutorService executor;\n   private ServiceLock miniLock;\n   private ZooSession miniLockZk;\n   private AccumuloClient client;\n+  private volatile State clusterState = State.STOPPED;\n \n   /**\n    *\n@@ -503,6 +506,9 @@ private void writeConfigProperties(File file, Map<String,String> settings) throw\n       justification = \""insecure socket used for reservation\"")\n   @Override\n   public synchronized void start() throws IOException, InterruptedException {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot start a cluster that is terminated.\"");\n+\n     if (config.useMiniDFS() && miniDFS.get() == null) {\n       throw new IllegalStateException(\""Cannot restart mini when using miniDFS\"");\n     }\n@@ -524,11 +530,14 @@ public synchronized void start() throws IOException, InterruptedException {\n       if (!initialized) {\n         Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n           try {\n-            MiniAccumuloClusterImpl.this.stop();\n-          } catch (IOException e) {\n-            log.error(\""IOException while attempting to stop the MiniAccumuloCluster.\"", e);\n+            if (clusterState != State.TERMINATED) {\n+              MiniAccumuloClusterImpl.this.stop();\n+              MiniAccumuloClusterImpl.this.terminate();\n+            }\n           } catch (InterruptedException e) {\n             log.error(\""The stopping of MiniAccumuloCluster was interrupted.\"", e);\n+          } catch (Exception e) {\n+            log.error(\""Exception while attempting to stop the MiniAccumuloCluster.\"", e);\n           }\n         }));\n       }\n@@ -716,6 +725,7 @@ public void failedToAcquireLock(Exception e) {\n     verifyUp((ClientContext) client, iid);\n \n     printProcessSummary();\n+    clusterState = State.STARTED;\n \n   }\n \n@@ -925,6 +935,7 @@ public String getZooKeepers() {\n \n   @Override\n   public ServerContext getServerContext() {\n+    serverContextCreated.set(true);\n     return context.get();\n   }\n \n@@ -935,6 +946,9 @@ public ServerContext getServerContext() {\n    */\n   @Override\n   public synchronized void stop() throws IOException, InterruptedException {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot stop a cluster that is terminated.\"");\n+\n     if (executor == null) {\n       // keep repeated calls to stop() from failing\n       return;\n@@ -1015,6 +1029,22 @@ public synchronized void stop() throws IOException, InterruptedException {\n       p.waitFor();\n     }\n     miniDFS.set(null);\n+    clusterState = State.STOPPED;\n+  }\n+\n+  @Override\n+  public synchronized void terminate() throws Exception {\n+    Preconditions.checkState(clusterState != State.TERMINATED,\n+        \""Cannot stop a cluster that is terminated.\"");\n+\n+    if (clusterState != State.STOPPED) {\n+      stop();\n+    }\n+\n+    if (serverContextCreated.get()) {\n+      getServerContext().close();\n+    }\n+    clusterState = State.TERMINATED;\n   }\n \n   /**\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\nindex b64fac88846..03548c888b4 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\n@@ -37,6 +37,7 @@\n import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.function.Supplier;\n \n@@ -104,6 +105,9 @@ public class ServerContext extends ClientContext {\n   private final AtomicReference<ServiceLock> serverLock = new AtomicReference<>();\n   private final Supplier<MetricsInfo> metricsInfoSupplier;\n \n+  private final AtomicBoolean metricsInfoCreated = new AtomicBoolean(false);\n+  private final AtomicBoolean sharedSchedExecutorCreated = new AtomicBoolean(false);\n+\n   public ServerContext(SiteConfiguration siteConfig) {\n     this(ServerInfo.fromServerConfig(siteConfig));\n   }\n@@ -424,6 +428,7 @@ private void monitorSwappiness() {\n   }\n \n   public ScheduledThreadPoolExecutor getScheduledExecutor() {\n+    sharedSchedExecutorCreated.set(true);\n     return sharedScheduledThreadPool.get();\n   }\n \n@@ -460,12 +465,18 @@ public void clearServiceLock() {\n   }\n \n   public MetricsInfo getMetricsInfo() {\n+    metricsInfoCreated.set(true);\n     return metricsInfoSupplier.get();\n   }\n \n   @Override\n   public void close() {\n-    getMetricsInfo().close();\n+    if (metricsInfoCreated.get()) {\n+      getMetricsInfo().close();\n+    }\n+    if (sharedSchedExecutorCreated.get()) {\n+      getScheduledExecutor().shutdownNow();\n+    }\n     super.close();\n   }\n }\n"", ""test_patch"": ""diff --git a/test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java b/test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java\nindex 95d62e61f5e..ad3d0551eb5 100644\n--- a/test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java\n+++ b/test/src/main/java/org/apache/accumulo/harness/AccumuloClusterHarness.java\n@@ -205,6 +205,7 @@ public void teardownCluster() throws Exception {\n     if (cluster != null) {\n       if (type.isDynamic()) {\n         cluster.stop();\n+        cluster.terminate();\n       } else {\n         log.info(\""Removing tables which appear to be from the current test\"");\n         cleanupTables();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java b/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\nindex 231d2256e20..05dbc77404b 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/ConfigurableMacBase.java\n@@ -190,6 +190,7 @@ public void tearDown() {\n     if (cluster != null) {\n       try {\n         cluster.stop();\n+        cluster.terminate();\n       } catch (Exception e) {\n         // ignored\n       }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java b/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java\nindex 14d3a5c4773..a4f64088266 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java\n@@ -132,7 +132,6 @@ public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoo\n   @AfterEach\n   public void afterTest() throws Exception {\n     getCluster().getClusterControl().stopAllServers(ServerType.TABLET_SERVER);\n-    super.teardownCluster();\n     USE_VERIFICATION_THREAD.set(!USE_VERIFICATION_THREAD.get());\n   }\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5490"", ""pr_id"": 5490, ""issue_id"": 4810, ""repo"": ""apache/accumulo"", ""problem_statement"": ""RFileScanner's IteratorEnvironment throws UnsupportedOperationException for getConfig() and getServiceEnv()\n**Describe the bug**\r\nWhen attaching an iterator to an RFileScanner which uses either the iterator environments config or ServiceEnvironment, the iterators will throw UnsupportedOperationExceptions \r\n\r\n**Versions (OS, Maven, Java, and others, as appropriate):**\r\nAccumulo 2.x\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior (or a link to an example repository that reproduces the problem):\r\n1. Create an RFileScanner\r\n2. Attach an Iterator which in the init method calls env.getConfig() or env.getServiceEnv()\r\n\r\n**Expected behavior**\r\nThe config for the RFileScanner should be built from any table properties passed into the builder's withTableProperties() method. These properties can also be returned for any env.getServiceEnv().getConfiguration() or env.getServiceEnv().getConfiguration(TableId) methods."", ""issue_word_count"": 119, ""test_files_count"": 22, ""non_test_files_count"": 6, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java"", ""core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java"", ""core/src/main/java/org/apache/accumulo/core/iterators/IteratorEnvironment.java"", ""core/src/main/java/org/apache/accumulo/core/iteratorsImpl/ClientIteratorEnvironment.java"", ""core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/DefaultIteratorEnvironment.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java"", ""iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java"", ""iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/environments/SimpleIteratorEnvironment.java"", ""server/base/src/main/java/org/apache/accumulo/server/iterators/TabletIteratorEnvironment.java"", ""server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java"", ""server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java"", ""test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java"", ""test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/DefaultIteratorEnvironment.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java"", ""core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java"", ""core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java"", ""iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java"", ""iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/environments/SimpleIteratorEnvironment.java"", ""server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java"", ""server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java"", ""test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java"", ""test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java""], ""base_commit"": ""c2df3baeae3af49a917cdadf49c6d83f87269529"", ""head_commit"": ""aa7e54829ecf229d6a426f8f9c46145f37a2945a"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5490"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5490"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-24T17:41:33.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java b/core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java\nindex 217a3bb31cd..8a646219511 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/ClientSideIteratorScanner.java\n@@ -34,7 +34,6 @@\n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n-import org.apache.accumulo.core.clientImpl.ClientServiceEnvironmentImpl;\n import org.apache.accumulo.core.clientImpl.ScannerImpl;\n import org.apache.accumulo.core.clientImpl.ScannerOptions;\n import org.apache.accumulo.core.data.ArrayByteSequence;\n@@ -49,10 +48,10 @@\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.IteratorBuilder;\n import org.apache.accumulo.core.iteratorsImpl.IteratorConfigUtil;\n import org.apache.accumulo.core.security.Authorizations;\n-import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -87,70 +86,6 @@ public class ClientSideIteratorScanner extends ScannerOptions implements Scanner\n   private final Supplier<ClientContext> context;\n   private final Supplier<TableId> tableId;\n \n-  private class ClientSideIteratorEnvironment implements IteratorEnvironment {\n-\n-    private final SamplerConfiguration samplerConfig;\n-    private final boolean sampleEnabled;\n-\n-    ClientSideIteratorEnvironment(boolean sampleEnabled, SamplerConfiguration samplerConfig) {\n-      this.sampleEnabled = sampleEnabled;\n-      this.samplerConfig = samplerConfig;\n-    }\n-\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return IteratorScope.scan;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      // The javadocs state this method will throw an ISE when scope is not majc\n-      throw new IllegalStateException(\n-          \""Asked about major compaction type when scope is \"" + getIteratorScope());\n-    }\n-\n-    @Override\n-    public boolean isUserCompaction() {\n-      return false;\n-    }\n-\n-    @Override\n-    public Authorizations getAuthorizations() {\n-      return ClientSideIteratorScanner.this.getAuthorizations();\n-    }\n-\n-    @Override\n-    public IteratorEnvironment cloneWithSamplingEnabled() {\n-      return new ClientSideIteratorEnvironment(true, samplerConfig);\n-    }\n-\n-    @Override\n-    public boolean isSamplingEnabled() {\n-      return sampleEnabled;\n-    }\n-\n-    @Override\n-    public SamplerConfiguration getSamplerConfiguration() {\n-      return samplerConfig;\n-    }\n-\n-    @Deprecated(since = \""2.1.0\"")\n-    @Override\n-    public ServiceEnvironment getServiceEnv() {\n-      return new ClientServiceEnvironmentImpl(context.get());\n-    }\n-\n-    @Override\n-    public PluginEnvironment getPluginEnv() {\n-      return new ClientServiceEnvironmentImpl(context.get());\n-    }\n-\n-    @Override\n-    public TableId getTableId() {\n-      return tableId.get();\n-    }\n-  }\n-\n   /**\n    * A class that wraps a Scanner in a SortedKeyValueIterator so that other accumulo iterators can\n    * use it as a source.\n@@ -295,9 +230,14 @@ public Iterator<Entry<Key,Value>> iterator() {\n \n     SortedKeyValueIterator<Key,Value> skvi;\n     try {\n-      IteratorEnvironment iterEnv = new ClientSideIteratorEnvironment(\n-          getSamplerConfiguration() != null, getIteratorSamplerConfigurationInternal());\n-\n+      ClientIteratorEnvironment.Builder builder = new ClientIteratorEnvironment.Builder()\n+          .withClient(context.get()).withAuthorizations(getAuthorizations())\n+          .withScope(IteratorScope.scan).withTableId(tableId.get())\n+          .withSamplerConfiguration(getIteratorSamplerConfigurationInternal());\n+      if (getSamplerConfiguration() != null) {\n+        builder.withSamplingEnabled();\n+      }\n+      IteratorEnvironment iterEnv = builder.build();\n       IteratorBuilder ib =\n           IteratorBuilder.builder(tm.values()).opts(serverSideIteratorOptions).env(iterEnv).build();\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java b/core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java\nindex be49a9c68ac..38172594afb 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/rfile/RFileScanner.java\n@@ -30,8 +30,9 @@\n \n import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.Scanner;\n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.rfile.RFileScannerBuilder.InputArgs;\n-import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n+import org.apache.accumulo.core.clientImpl.ClientServiceEnvironmentImpl;\n import org.apache.accumulo.core.clientImpl.ScannerOptions;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.ConfigurationCopy;\n@@ -42,6 +43,7 @@\n import org.apache.accumulo.core.data.Column;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n+import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.file.blockfile.cache.impl.BlockCacheConfiguration;\n import org.apache.accumulo.core.file.blockfile.cache.impl.BlockCacheManagerFactory;\n@@ -55,6 +57,7 @@\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.IteratorBuilder;\n import org.apache.accumulo.core.iteratorsImpl.IteratorConfigUtil;\n import org.apache.accumulo.core.iteratorsImpl.system.MultiIterator;\n@@ -66,6 +69,7 @@\n import org.apache.accumulo.core.spi.cache.CacheType;\n import org.apache.accumulo.core.spi.crypto.CryptoEnvironment;\n import org.apache.accumulo.core.spi.crypto.CryptoService;\n+import org.apache.accumulo.core.util.ConfigurationImpl;\n import org.apache.accumulo.core.util.LocalityGroupUtil;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.io.Text;\n@@ -74,8 +78,46 @@\n \n class RFileScanner extends ScannerOptions implements Scanner {\n \n+  private static class RFileScannerEnvironmentImpl extends ClientServiceEnvironmentImpl {\n+\n+    private final Configuration conf;\n+    private final Configuration tableConf;\n+\n+    public RFileScannerEnvironmentImpl(Opts opts) {\n+      super(null);\n+      conf = new ConfigurationImpl(new ConfigurationCopy(DefaultConfiguration.getInstance()));\n+      ConfigurationCopy tableCC = new ConfigurationCopy(DefaultConfiguration.getInstance());\n+      if (opts.tableConfig != null) {\n+        opts.tableConfig.forEach(tableCC::set);\n+      }\n+      tableConf = new ConfigurationImpl(tableCC);\n+    }\n+\n+    @Override\n+    public String getTableName(TableId tableId) throws TableNotFoundException {\n+      Preconditions.checkArgument(tableId == TABLE_ID, \""Expected \"" + TABLE_ID + \"" obtained\""\n+          + \"" from IteratorEnvironment.getTableId(), but got: \"" + tableId);\n+      return TABLE_NAME;\n+    }\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+      return conf;\n+    }\n+\n+    @Override\n+    public Configuration getConfiguration(TableId tableId) {\n+      Preconditions.checkArgument(tableId == TABLE_ID, \""Expected \"" + TABLE_ID + \"" obtained\""\n+          + \"" from IteratorEnvironment.getTableId(), but got: \"" + tableId);\n+      return tableConf;\n+    }\n+\n+  }\n+\n   private static final byte[] EMPTY_BYTES = new byte[0];\n   private static final Range EMPTY_RANGE = new Range();\n+  private static final String TABLE_NAME = \""rfileScanner\"";\n+  private static final TableId TABLE_ID = TableId.of(TABLE_NAME);\n \n   private Range range;\n   private BlockCacheManager blockCacheManager = null;\n@@ -225,33 +267,6 @@ public void updateScanIteratorOption(String iteratorName, String key, String val\n     super.updateScanIteratorOption(iteratorName, key, value);\n   }\n \n-  private class IterEnv implements IteratorEnvironment {\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return IteratorScope.scan;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      return false;\n-    }\n-\n-    @Override\n-    public Authorizations getAuthorizations() {\n-      return opts.auths;\n-    }\n-\n-    @Override\n-    public boolean isSamplingEnabled() {\n-      return RFileScanner.this.getSamplerConfiguration() != null;\n-    }\n-\n-    @Override\n-    public SamplerConfiguration getSamplerConfiguration() {\n-      return RFileScanner.this.getSamplerConfiguration();\n-    }\n-  }\n-\n   @Override\n   public Iterator<Entry<Key,Value>> iterator() {\n     try {\n@@ -292,15 +307,23 @@ public Iterator<Entry<Key,Value>> iterator() {\n             EMPTY_BYTES, tableConf);\n       }\n \n+      ClientIteratorEnvironment.Builder iterEnvBuilder = new ClientIteratorEnvironment.Builder()\n+          .withEnvironment(new RFileScannerEnvironmentImpl(opts)).withAuthorizations(opts.auths)\n+          .withScope(IteratorScope.scan).withTableId(TABLE_ID);\n+      if (getSamplerConfiguration() != null) {\n+        iterEnvBuilder.withSamplerConfiguration(getSamplerConfiguration());\n+        iterEnvBuilder.withSamplingEnabled();\n+      }\n+      IteratorEnvironment iterEnv = iterEnvBuilder.build();\n       try {\n         if (opts.tableConfig != null && !opts.tableConfig.isEmpty()) {\n           var ibEnv = IteratorConfigUtil.loadIterConf(IteratorScope.scan, serverSideIteratorList,\n               serverSideIteratorOptions, tableConf);\n-          var iteratorBuilder = ibEnv.env(new IterEnv()).build();\n+          var iteratorBuilder = ibEnv.env(iterEnv).build();\n           iterator = IteratorConfigUtil.loadIterators(iterator, iteratorBuilder);\n         } else {\n           var iteratorBuilder = IteratorBuilder.builder(serverSideIteratorList)\n-              .opts(serverSideIteratorOptions).env(new IterEnv()).build();\n+              .opts(serverSideIteratorOptions).env(iterEnv).build();\n           iterator = IteratorConfigUtil.loadIterators(iterator, iteratorBuilder);\n         }\n       } catch (IOException e) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java\nindex a03cc811ab8..d64e0181487 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/OfflineIterator.java\n@@ -34,7 +34,6 @@\n \n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n-import org.apache.accumulo.core.client.PluginEnvironment;\n import org.apache.accumulo.core.client.SampleNotPresentException;\n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n@@ -54,6 +53,7 @@\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.IteratorConfigUtil;\n import org.apache.accumulo.core.iteratorsImpl.system.MultiIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SystemIteratorUtil;\n@@ -66,7 +66,6 @@\n import org.apache.accumulo.core.sample.impl.SamplerConfigurationImpl;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.security.ColumnVisibility;\n-import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n import org.apache.accumulo.core.util.LocalityGroupUtil;\n import org.apache.accumulo.core.volume.VolumeConfiguration;\n import org.apache.hadoop.conf.Configuration;\n@@ -75,105 +74,6 @@\n \n class OfflineIterator implements Iterator<Entry<Key,Value>> {\n \n-  static class OfflineIteratorEnvironment implements IteratorEnvironment {\n-\n-    private final Authorizations authorizations;\n-    private final AccumuloConfiguration conf;\n-    private final boolean useSample;\n-    private final SamplerConfiguration sampleConf;\n-    private final ClientContext context;\n-    private final TableId tableId;\n-\n-    public OfflineIteratorEnvironment(ClientContext context, TableId tableId, Authorizations auths,\n-        AccumuloConfiguration acuTableConf, boolean useSample, SamplerConfiguration samplerConf) {\n-      this.context = context;\n-      this.tableId = tableId;\n-      this.authorizations = auths;\n-      this.conf = acuTableConf;\n-      this.useSample = useSample;\n-      this.sampleConf = samplerConf;\n-    }\n-\n-    @Deprecated(since = \""2.0.0\"")\n-    @Override\n-    public AccumuloConfiguration getConfig() {\n-      return conf;\n-    }\n-\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return IteratorScope.scan;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      return false;\n-    }\n-\n-    @Override\n-    public boolean isUserCompaction() {\n-      return false;\n-    }\n-\n-    private final ArrayList<SortedKeyValueIterator<Key,Value>> topLevelIterators =\n-        new ArrayList<>();\n-\n-    @Deprecated(since = \""2.0.0\"")\n-    @Override\n-    public void registerSideChannel(SortedKeyValueIterator<Key,Value> iter) {\n-      topLevelIterators.add(iter);\n-    }\n-\n-    @Override\n-    public Authorizations getAuthorizations() {\n-      return authorizations;\n-    }\n-\n-    SortedKeyValueIterator<Key,Value> getTopLevelIterator(SortedKeyValueIterator<Key,Value> iter) {\n-      if (topLevelIterators.isEmpty()) {\n-        return iter;\n-      }\n-      ArrayList<SortedKeyValueIterator<Key,Value>> allIters = new ArrayList<>(topLevelIterators);\n-      allIters.add(iter);\n-      return new MultiIterator(allIters, false);\n-    }\n-\n-    @Override\n-    public boolean isSamplingEnabled() {\n-      return useSample;\n-    }\n-\n-    @Override\n-    public SamplerConfiguration getSamplerConfiguration() {\n-      return sampleConf;\n-    }\n-\n-    @Override\n-    public IteratorEnvironment cloneWithSamplingEnabled() {\n-      if (sampleConf == null) {\n-        throw new SampleNotPresentException();\n-      }\n-      return new OfflineIteratorEnvironment(context, tableId, authorizations, conf, true,\n-          sampleConf);\n-    }\n-\n-    @Deprecated(since = \""2.1.0\"")\n-    @Override\n-    public ServiceEnvironment getServiceEnv() {\n-      return new ClientServiceEnvironmentImpl(context);\n-    }\n-\n-    @Override\n-    public PluginEnvironment getPluginEnv() {\n-      return new ClientServiceEnvironmentImpl(context);\n-    }\n-\n-    @Override\n-    public TableId getTableId() {\n-      return tableId;\n-    }\n-  }\n-\n   private SortedKeyValueIterator<Key,Value> iter;\n   private Range range;\n   private KeyExtent currentExtent;\n@@ -345,9 +245,13 @@ private SortedKeyValueIterator<Key,Value> createIterator(KeyExtent extent,\n \n     MultiIterator multiIter = new MultiIterator(readers, extent);\n \n-    OfflineIteratorEnvironment iterEnv =\n-        new OfflineIteratorEnvironment(context, tableId, authorizations, tableCC, false,\n-            samplerConfImpl == null ? null : samplerConfImpl.toSamplerConfiguration());\n+    ClientIteratorEnvironment.Builder iterEnvBuilder =\n+        new ClientIteratorEnvironment.Builder().withAuthorizations(authorizations)\n+            .withScope(IteratorScope.scan).withTableId(tableId).withClient(context);\n+    if (samplerConfImpl != null) {\n+      iterEnvBuilder.withSamplerConfiguration(samplerConfImpl.toSamplerConfiguration());\n+    }\n+    IteratorEnvironment iterEnv = iterEnvBuilder.build();\n \n     byte[] defaultSecurityLabel;\n     ColumnVisibility cv =\n@@ -360,8 +264,7 @@ private SortedKeyValueIterator<Key,Value> createIterator(KeyExtent extent,\n     var iteratorBuilderEnv = IteratorConfigUtil.loadIterConf(IteratorScope.scan,\n         options.serverSideIteratorList, options.serverSideIteratorOptions, tableCC);\n     var iteratorBuilder = iteratorBuilderEnv.env(iterEnv).build();\n-    return iterEnv\n-        .getTopLevelIterator(IteratorConfigUtil.loadIterators(visFilter, iteratorBuilder));\n+    return IteratorConfigUtil.loadIterators(visFilter, iteratorBuilder);\n   }\n \n   @Override\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/iterators/IteratorEnvironment.java b/core/src/main/java/org/apache/accumulo/core/iterators/IteratorEnvironment.java\nindex 372a0e49a30..1646a196388 100644\n--- a/core/src/main/java/org/apache/accumulo/core/iterators/IteratorEnvironment.java\n+++ b/core/src/main/java/org/apache/accumulo/core/iterators/IteratorEnvironment.java\n@@ -24,6 +24,7 @@\n import org.apache.accumulo.core.client.SampleNotPresentException;\n import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n+import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n@@ -37,10 +38,7 @@ public interface IteratorEnvironment {\n    * @deprecated since 2.0.0. This is a legacy method used for internal backwards compatibility.\n    */\n   @Deprecated(since = \""2.0.0\"")\n-  default SortedKeyValueIterator<Key,Value> reserveMapFileReader(String mapFileName)\n-      throws IOException {\n-    throw new UnsupportedOperationException();\n-  }\n+  SortedKeyValueIterator<Key,Value> reserveMapFileReader(String mapFileName) throws IOException;\n \n   /**\n    * @deprecated since 2.0.0. This method was using an unstable non public type. Use\n@@ -48,40 +46,32 @@ default SortedKeyValueIterator<Key,Value> reserveMapFileReader(String mapFileNam\n    */\n   @Deprecated(since = \""2.0.0\"")\n   default AccumuloConfiguration getConfig() {\n-    throw new UnsupportedOperationException();\n+    return new ConfigurationCopy(getPluginEnv().getConfiguration());\n   }\n \n   /**\n    * Return the executed scope of the Iterator. Value will be one of the following:\n    * {@link IteratorScope#scan}, {@link IteratorScope#minc}, {@link IteratorScope#majc}\n    */\n-  default IteratorScope getIteratorScope() {\n-    throw new UnsupportedOperationException();\n-  }\n+  IteratorScope getIteratorScope();\n \n   /**\n    * Return true if the compaction is a full major compaction. Will throw IllegalStateException if\n    * {@link #getIteratorScope()} != {@link IteratorScope#majc}.\n    */\n-  default boolean isFullMajorCompaction() {\n-    throw new UnsupportedOperationException();\n-  }\n+  boolean isFullMajorCompaction();\n \n   /**\n    * @deprecated since 2.0.0. This was an experimental feature and was never tested or documented.\n    */\n   @Deprecated(since = \""2.0.0\"")\n-  default void registerSideChannel(SortedKeyValueIterator<Key,Value> iter) {\n-    throw new UnsupportedOperationException();\n-  }\n+  void registerSideChannel(SortedKeyValueIterator<Key,Value> iter);\n \n   /**\n    * Return the Scan Authorizations used in this Iterator. Will throw UnsupportedOperationException\n    * if {@link #getIteratorScope()} != {@link IteratorScope#scan}.\n    */\n-  default Authorizations getAuthorizations() {\n-    throw new UnsupportedOperationException();\n-  }\n+  Authorizations getAuthorizations();\n \n   /**\n    * Returns a new iterator environment object that can be used to create deep copies over sample\n@@ -113,9 +103,7 @@ default Authorizations getAuthorizations() {\n    * @throws SampleNotPresentException when sampling is not configured for table.\n    * @since 1.8.0\n    */\n-  default IteratorEnvironment cloneWithSamplingEnabled() {\n-    throw new UnsupportedOperationException();\n-  }\n+  IteratorEnvironment cloneWithSamplingEnabled();\n \n   /**\n    * There are at least two conditions under which sampling will be enabled for an environment. One\n@@ -126,27 +114,21 @@ default IteratorEnvironment cloneWithSamplingEnabled() {\n    * @return true if sampling is enabled for this environment.\n    * @since 1.8.0\n    */\n-  default boolean isSamplingEnabled() {\n-    throw new UnsupportedOperationException();\n-  }\n+  boolean isSamplingEnabled();\n \n   /**\n    *\n    * @return sampling configuration is sampling is enabled for environment, otherwise returns null.\n    * @since 1.8.0\n    */\n-  default SamplerConfiguration getSamplerConfiguration() {\n-    throw new UnsupportedOperationException();\n-  }\n+  SamplerConfiguration getSamplerConfiguration();\n \n   /**\n    * True if compaction was user initiated.\n    *\n    * @since 2.0.0\n    */\n-  default boolean isUserCompaction() {\n-    throw new UnsupportedOperationException();\n-  }\n+  boolean isUserCompaction();\n \n   /**\n    * Returns an object containing information about the server where this iterator was run. To\n@@ -161,9 +143,7 @@ default boolean isUserCompaction() {\n    *             {@link #getPluginEnv()} instead because it has better stability guarantees.\n    */\n   @Deprecated(since = \""2.1.0\"")\n-  default ServiceEnvironment getServiceEnv() {\n-    throw new UnsupportedOperationException();\n-  }\n+  ServiceEnvironment getServiceEnv();\n \n   /**\n    * Returns an object containing information about the server where this iterator was run. To\n@@ -175,16 +155,12 @@ default ServiceEnvironment getServiceEnv() {\n    *\n    * @since 2.1.0\n    */\n-  default PluginEnvironment getPluginEnv() {\n-    return getServiceEnv();\n-  }\n+  PluginEnvironment getPluginEnv();\n \n   /**\n    * Return the table Id associated with this iterator.\n    *\n    * @since 2.0.0\n    */\n-  default TableId getTableId() {\n-    throw new UnsupportedOperationException();\n-  }\n+  TableId getTableId();\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/iteratorsImpl/ClientIteratorEnvironment.java b/core/src/main/java/org/apache/accumulo/core/iteratorsImpl/ClientIteratorEnvironment.java\nnew file mode 100644\nindex 00000000000..41a514f19aa\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/iteratorsImpl/ClientIteratorEnvironment.java\n@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.iteratorsImpl;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.PluginEnvironment;\n+import org.apache.accumulo.core.client.SampleNotPresentException;\n+import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n+import org.apache.accumulo.core.clientImpl.ClientContext;\n+import org.apache.accumulo.core.clientImpl.ClientServiceEnvironmentImpl;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.iterators.IteratorEnvironment;\n+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n+\n+public class ClientIteratorEnvironment implements IteratorEnvironment {\n+\n+  public static class Builder {\n+\n+    private Optional<IteratorScope> scope = Optional.empty();\n+    private boolean isFullMajorCompaction = false;\n+    private Optional<Authorizations> auths = Optional.empty();\n+    private boolean isUserCompaction = false;\n+    private Optional<TableId> tableId = Optional.empty();\n+    private Optional<SamplerConfiguration> samplerConfig = Optional.empty();\n+    private boolean samplingEnabled = false;\n+    protected Optional<ClientServiceEnvironmentImpl> env = Optional.empty();\n+\n+    public Builder withScope(IteratorScope scope) {\n+      checkState(this.scope.isEmpty(), \""Scope has already been set\"");\n+      this.scope = Optional.of(scope);\n+      return this;\n+    }\n+\n+    public Builder isFullMajorCompaction() {\n+      this.isFullMajorCompaction = true;\n+      return this;\n+    }\n+\n+    public Builder withAuthorizations(Authorizations auths) {\n+      checkState(this.auths.isEmpty(), \""Authorizations have already been set\"");\n+      this.auths = Optional.of(auths);\n+      return this;\n+    }\n+\n+    public Builder isUserCompaction() {\n+      this.isUserCompaction = true;\n+      return this;\n+    }\n+\n+    public Builder withTableId(TableId tableId) {\n+      checkState(this.tableId.isEmpty(), \""TableId has already been set\"");\n+      this.tableId = Optional.of(tableId);\n+      return this;\n+    }\n+\n+    public Builder withSamplingEnabled() {\n+      this.samplingEnabled = true;\n+      return this;\n+    }\n+\n+    public Builder withSamplerConfiguration(SamplerConfiguration sc) {\n+      checkState(this.samplerConfig.isEmpty(), \""SamplerConfiguration has already been set\"");\n+      this.samplerConfig = Optional.ofNullable(sc);\n+      return this;\n+    }\n+\n+    public ClientIteratorEnvironment.Builder withEnvironment(ClientServiceEnvironmentImpl env) {\n+      checkState(this.env.isEmpty(), \""ClientServiceEnvironmentImpl has already been set\"");\n+      this.env = Optional.of(env);\n+      return this;\n+    }\n+\n+    public Builder withClient(AccumuloClient client) {\n+      checkState(this.env.isEmpty(), \""ClientServiceEnvironmentImpl has already been set\"");\n+      this.env = Optional.of(new ClientServiceEnvironmentImpl((ClientContext) client));\n+      return this;\n+    }\n+\n+    public ClientIteratorEnvironment build() {\n+      return new ClientIteratorEnvironment(this);\n+    }\n+\n+  }\n+\n+  public static final IteratorEnvironment DEFAULT = new Builder().build();\n+\n+  private final Optional<IteratorScope> scope;\n+  private final boolean isFullMajorCompaction;\n+  private final Optional<Authorizations> auths;\n+  private final boolean isUserCompaction;\n+  private final Optional<TableId> tableId;\n+  private final Optional<SamplerConfiguration> samplerConfig;\n+  private final boolean samplingEnabled;\n+  private final Optional<ClientServiceEnvironmentImpl> env;\n+\n+  private ClientIteratorEnvironment(Builder builder) {\n+    this.scope = builder.scope;\n+    this.isFullMajorCompaction = builder.isFullMajorCompaction;\n+    this.auths = builder.auths;\n+    this.isUserCompaction = builder.isUserCompaction;\n+    this.tableId = builder.tableId;\n+    this.samplerConfig = builder.samplerConfig;\n+    this.env = builder.env;\n+    this.samplingEnabled = builder.samplingEnabled;\n+  }\n+\n+  /**\n+   * Copy constructor used for enabling sample. Only called from {@link cloneWithSamplingEnabled}.\n+   */\n+  private ClientIteratorEnvironment(ClientIteratorEnvironment copy) {\n+    this.scope = copy.scope;\n+    this.isFullMajorCompaction = copy.isFullMajorCompaction;\n+    this.auths = copy.auths;\n+    this.isUserCompaction = copy.isUserCompaction;\n+    this.tableId = copy.tableId;\n+    this.samplerConfig = copy.samplerConfig;\n+    this.env = copy.env;\n+    this.samplingEnabled = true;\n+  }\n+\n+  @Override\n+  @Deprecated(since = \""2.0.0\"")\n+  public SortedKeyValueIterator<Key,Value> reserveMapFileReader(String mapFileName)\n+      throws IOException {\n+    throw new UnsupportedOperationException(\""Feature not supported\"");\n+  }\n+\n+  @Override\n+  public IteratorScope getIteratorScope() {\n+    return scope.orElseThrow();\n+  }\n+\n+  @Override\n+  public boolean isFullMajorCompaction() {\n+    if (getIteratorScope() != IteratorScope.majc) {\n+      throw new IllegalStateException(\""Iterator scope is not majc\"");\n+    }\n+    return isFullMajorCompaction;\n+  }\n+\n+  @Override\n+  @Deprecated(since = \""2.0.0\"")\n+  public void registerSideChannel(SortedKeyValueIterator<Key,Value> iter) {\n+    throw new UnsupportedOperationException(\""Feature not supported\"");\n+  }\n+\n+  @Override\n+  public Authorizations getAuthorizations() {\n+    if (getIteratorScope() != IteratorScope.scan) {\n+      throw new IllegalStateException(\""Iterator scope is not scan\"");\n+    }\n+    return auths.orElseThrow();\n+  }\n+\n+  @Override\n+  public IteratorEnvironment cloneWithSamplingEnabled() {\n+    if (samplerConfig.isEmpty()) {\n+      throw new SampleNotPresentException();\n+    }\n+    return new ClientIteratorEnvironment(this);\n+  }\n+\n+  @Override\n+  public boolean isSamplingEnabled() {\n+    return this.samplingEnabled;\n+  }\n+\n+  @Override\n+  public SamplerConfiguration getSamplerConfiguration() {\n+    if (!isSamplingEnabled()) {\n+      return null;\n+    }\n+    return samplerConfig.orElseThrow();\n+  }\n+\n+  @Override\n+  public boolean isUserCompaction() {\n+    if (getIteratorScope() == IteratorScope.scan) {\n+      throw new IllegalStateException(\n+          \""scan iterator scope is incompatible with a possible user compaction\"");\n+    }\n+    return this.isUserCompaction;\n+  }\n+\n+  @Override\n+  @Deprecated(since = \""2.1.0\"")\n+  public ServiceEnvironment getServiceEnv() {\n+    return env.orElseThrow();\n+  }\n+\n+  @Override\n+  public PluginEnvironment getPluginEnv() {\n+    return env.orElseThrow();\n+  }\n+\n+  @Override\n+  public TableId getTableId() {\n+    return this.tableId.orElseThrow();\n+  }\n+\n+}\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/iterators/TabletIteratorEnvironment.java b/server/base/src/main/java/org/apache/accumulo/server/iterators/TabletIteratorEnvironment.java\nindex 63970b2943a..a1b657b5628 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/iterators/TabletIteratorEnvironment.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/iterators/TabletIteratorEnvironment.java\n@@ -23,6 +23,7 @@\n import java.util.Collections;\n import java.util.Map;\n \n+import org.apache.accumulo.core.client.PluginEnvironment;\n import org.apache.accumulo.core.client.SampleNotPresentException;\n import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n@@ -79,6 +80,30 @@ public TabletIteratorEnvironment(ServerContext context, IteratorScope scope,\n     this.topLevelIterators = new ArrayList<>();\n   }\n \n+  public TabletIteratorEnvironment(ServerContext context, IteratorScope scope,\n+      AccumuloConfiguration tableConfig, TableId tableId, SamplerConfigurationImpl samplerConfig) {\n+    if (scope == IteratorScope.majc) {\n+      throw new IllegalArgumentException(\""must set if compaction is full\"");\n+    }\n+\n+    this.context = context;\n+    this.serviceEnvironment = new ServiceEnvironmentImpl(context);\n+    this.scope = scope;\n+    this.trm = null;\n+    this.tableConfig = tableConfig;\n+    this.tableId = tableId;\n+    this.fullMajorCompaction = false;\n+    this.userCompaction = false;\n+    this.authorizations = Authorizations.EMPTY;\n+    if (samplerConfig != null) {\n+      enableSampleForDeepCopy = true;\n+      this.samplerConfig = samplerConfig.toSamplerConfiguration();\n+    } else {\n+      enableSampleForDeepCopy = false;\n+    }\n+    this.topLevelIterators = new ArrayList<>();\n+  }\n+\n   public TabletIteratorEnvironment(ServerContext context, IteratorScope scope,\n       AccumuloConfiguration tableConfig, TableId tableId, ScanFileManager trm,\n       Map<TabletFile,DataFileValue> files, Authorizations authorizations,\n@@ -236,4 +261,9 @@ public ServiceEnvironment getServiceEnv() {\n   public TableId getTableId() {\n     return tableId;\n   }\n+\n+  @Override\n+  public PluginEnvironment getPluginEnv() {\n+    return serviceEnvironment;\n+  }\n }\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java b/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\nindex 4b0c533dd1d..0bb58e6bb55 100644\n--- a/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\n@@ -49,7 +49,6 @@\n \n import org.apache.accumulo.core.client.sample.RowSampler;\n import org.apache.accumulo.core.client.sample.Sampler;\n-import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n@@ -74,6 +73,7 @@\n import org.apache.accumulo.core.file.rfile.bcfile.BCFile;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.metadata.MetadataTable;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n@@ -107,26 +107,6 @@\n public class RFileTest {\n \n   private static final SecureRandom random = new SecureRandom();\n-\n-  public static class SampleIE implements IteratorEnvironment {\n-\n-    private SamplerConfiguration samplerConfig;\n-\n-    SampleIE(SamplerConfiguration config) {\n-      this.samplerConfig = config;\n-    }\n-\n-    @Override\n-    public boolean isSamplingEnabled() {\n-      return samplerConfig != null;\n-    }\n-\n-    @Override\n-    public SamplerConfiguration getSamplerConfiguration() {\n-      return samplerConfig;\n-    }\n-  }\n-\n   private static final Collection<ByteSequence> EMPTY_COL_FAMS = new ArrayList<>();\n   private static final Configuration hadoopConf = new Configuration();\n \n@@ -2068,15 +2048,16 @@ public void testSample() throws IOException {\n \n         trf.openReader();\n \n-        FileSKVIterator sample =\n-            trf.reader.getSample(SamplerConfigurationImpl.newSamplerConfig(sampleConf));\n+        SamplerConfigurationImpl sc = SamplerConfigurationImpl.newSamplerConfig(sampleConf);\n+\n+        FileSKVIterator sample = trf.reader.getSample(sc);\n \n         checkSample(sample, sampleData);\n \n         assertEquals(expectedDataHash, hash(trf.reader));\n \n-        SampleIE ie = new SampleIE(\n-            SamplerConfigurationImpl.newSamplerConfig(sampleConf).toSamplerConfiguration());\n+        IteratorEnvironment ie = new ClientIteratorEnvironment.Builder()\n+            .withSamplerConfiguration(sc.toSamplerConfiguration()).withSamplingEnabled().build();\n \n         for (int i = 0; i < 3; i++) {\n           // test opening and closing deep copies a few times.\n@@ -2086,8 +2067,10 @@ public void testSample() throws IOException {\n           SortedKeyValueIterator<Key,Value> sampleDC1 = sample.deepCopy(ie);\n           SortedKeyValueIterator<Key,Value> sampleDC2 = sample.deepCopy(ie);\n           SortedKeyValueIterator<Key,Value> sampleDC3 = trf.reader.deepCopy(ie);\n-          SortedKeyValueIterator<Key,Value> allDC1 = sampleDC1.deepCopy(new SampleIE(null));\n-          SortedKeyValueIterator<Key,Value> allDC2 = sample.deepCopy(new SampleIE(null));\n+          SortedKeyValueIterator<Key,Value> allDC1 =\n+              sampleDC1.deepCopy(ClientIteratorEnvironment.DEFAULT);\n+          SortedKeyValueIterator<Key,Value> allDC2 =\n+              sample.deepCopy(ClientIteratorEnvironment.DEFAULT);\n \n           assertEquals(expectedDataHash, hash(allDC1));\n           assertEquals(expectedDataHash, hash(allDC2));\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/DefaultIteratorEnvironment.java b/core/src/test/java/org/apache/accumulo/core/iterators/DefaultIteratorEnvironment.java\ndeleted file mode 100644\nindex 802dfe849cc..00000000000\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/DefaultIteratorEnvironment.java\n+++ /dev/null\n@@ -1,56 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.core.iterators;\n-\n-import java.io.IOException;\n-\n-import org.apache.accumulo.core.conf.AccumuloConfiguration;\n-import org.apache.accumulo.core.conf.DefaultConfiguration;\n-import org.apache.accumulo.core.data.Key;\n-import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iteratorsImpl.system.MapFileIterator;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-\n-public class DefaultIteratorEnvironment implements IteratorEnvironment {\n-\n-  AccumuloConfiguration conf;\n-  Configuration hadoopConf = new Configuration();\n-\n-  public DefaultIteratorEnvironment(AccumuloConfiguration conf) {\n-    this.conf = conf;\n-  }\n-\n-  public DefaultIteratorEnvironment() {\n-    this.conf = DefaultConfiguration.getInstance();\n-  }\n-\n-  @Deprecated(since = \""2.0.0\"")\n-  @Override\n-  public SortedKeyValueIterator<Key,Value> reserveMapFileReader(String mapFileName)\n-      throws IOException {\n-    FileSystem fs = FileSystem.get(hadoopConf);\n-    return new MapFileIterator(fs, mapFileName, hadoopConf);\n-  }\n-\n-  @Override\n-  public boolean isSamplingEnabled() {\n-    return false;\n-  }\n-}\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java\nindex 5a7f6dbcc07..06e8311677a 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/FirstEntryInRowIteratorTest.java\n@@ -30,6 +30,7 @@\n import org.apache.accumulo.core.data.PartialKey;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.CountingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.junit.jupiter.api.Test;\n@@ -41,9 +42,8 @@ private static long process(TreeMap<Key,Value> sourceMap, TreeMap<Key,Value> res\n     SortedMapIterator source = new SortedMapIterator(sourceMap);\n     CountingIterator counter = new CountingIterator(source);\n     FirstEntryInRowIterator feiri = new FirstEntryInRowIterator();\n-    IteratorEnvironment env = new DefaultIteratorEnvironment();\n \n-    feiri.init(counter, iteratorSetting.getOptions(), env);\n+    feiri.init(counter, iteratorSetting.getOptions(), ClientIteratorEnvironment.DEFAULT);\n \n     feiri.seek(range, Set.of(), false);\n     while (feiri.hasTop()) {\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java\nindex c35cf8f1c6a..3dab1bf667e 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/SortedMapIteratorTest.java\n@@ -25,6 +25,7 @@\n import org.apache.accumulo.core.client.SampleNotPresentException;\n import org.apache.accumulo.core.client.sample.RowSampler;\n import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.junit.jupiter.api.Test;\n \n@@ -33,16 +34,9 @@ public class SortedMapIteratorTest {\n   @Test\n   public void testSampleNotPresent() {\n     SortedMapIterator smi = new SortedMapIterator(new TreeMap<>());\n-    assertThrows(SampleNotPresentException.class, () -> smi.deepCopy(new IteratorEnvironment() {\n-      @Override\n-      public boolean isSamplingEnabled() {\n-        return true;\n-      }\n-\n-      @Override\n-      public SamplerConfiguration getSamplerConfiguration() {\n-        return new SamplerConfiguration(RowSampler.class.getName());\n-      }\n-    }));\n+    assertThrows(SampleNotPresentException.class,\n+        () -> smi.deepCopy(new ClientIteratorEnvironment.Builder()\n+            .withSamplerConfiguration(new SamplerConfiguration(RowSampler.class.getName()))\n+            .withSamplingEnabled().build()));\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java\nindex 1fe7f3ac524..0a0716006e9 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/ColumnSliceFilterTest.java\n@@ -34,8 +34,8 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.BeforeEach;\n@@ -72,7 +72,7 @@ private static Key newKey(String row, String cf, String cq) {\n   @BeforeEach\n   public void setUp() {\n     columnSliceFilter.describeOptions();\n-    iteratorEnvironment = new DefaultIteratorEnvironment();\n+    iteratorEnvironment = ClientIteratorEnvironment.DEFAULT;\n     is = new IteratorSetting(1, ColumnSliceFilter.class);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java\nindex df757966903..04c64acf6dc 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/CombinerTest.java\n@@ -40,7 +40,6 @@\n import org.apache.accumulo.core.iterators.Combiner;\n import org.apache.accumulo.core.iterators.Combiner.ValueIterator;\n import org.apache.accumulo.core.iterators.CombinerTestUtil;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.LongCombiner;\n@@ -50,6 +49,7 @@\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n import org.apache.accumulo.core.iterators.TypedValueCombiner;\n import org.apache.accumulo.core.iterators.ValueFormatException;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.MultiIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n@@ -59,29 +59,8 @@ public class CombinerTest {\n \n   private static final Collection<ByteSequence> EMPTY_COL_FAMS = new ArrayList<>();\n \n-  static class CombinerIteratorEnvironment extends DefaultIteratorEnvironment {\n-\n-    private IteratorScope scope;\n-    private boolean isFullMajc;\n-\n-    CombinerIteratorEnvironment(IteratorScope scope, boolean isFullMajc) {\n-      this.scope = scope;\n-      this.isFullMajc = isFullMajc;\n-    }\n-\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return scope;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      return isFullMajc;\n-    }\n-  }\n-\n   static final IteratorEnvironment SCAN_IE =\n-      new CombinerIteratorEnvironment(IteratorScope.scan, false);\n+      new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build();\n \n   static Key newKey(int row, int colf, int colq, long ts, boolean deleted) {\n     Key k = newKey(row, colf, colq, ts);\n@@ -884,8 +863,10 @@ public void testDeleteHandling() throws Exception {\n \n     TreeMap<Key,Value> input = new TreeMap<>();\n \n-    IteratorEnvironment paritalMajcIe = new CombinerIteratorEnvironment(IteratorScope.majc, false);\n-    IteratorEnvironment fullMajcIe = new CombinerIteratorEnvironment(IteratorScope.majc, true);\n+    IteratorEnvironment paritalMajcIe =\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.majc).build();\n+    IteratorEnvironment fullMajcIe = new ClientIteratorEnvironment.Builder()\n+        .withScope(IteratorScope.majc).isFullMajorCompaction().build();\n \n     // keys that aggregate\n     newKeyValue(input, 1, 1, 1, 1, false, 4L, encoder);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java\nindex 996b3df97a5..c1dd3466769 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/FilterTest.java\n@@ -40,9 +40,9 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.Filter;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnQualifierFilter;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.VisibilityFilter;\n@@ -230,19 +230,19 @@ public void test2a() throws IOException {\n \n     ColumnAgeOffFilter a = new ColumnAgeOffFilter();\n     assertTrue(a.validateOptions(is.getOptions()));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(902, size(a));\n \n     ColumnAgeOffFilter.addTTL(is, new IteratorSetting.Column(\""a\"", \""b\""), 101L);\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(102, size(a));\n \n     ColumnAgeOffFilter.removeTTL(is, new IteratorSetting.Column(\""a\"", \""b\""));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a = (ColumnAgeOffFilter) a.deepCopy(null);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n@@ -271,19 +271,19 @@ public void test2aNegate() throws IOException {\n \n     ColumnAgeOffFilter a = new ColumnAgeOffFilter();\n     assertTrue(a.validateOptions(is.getOptions()));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(98, size(a));\n \n     ColumnAgeOffFilter.addTTL(is, new IteratorSetting.Column(\""a\"", \""b\""), 101L);\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(898, size(a));\n \n     ColumnAgeOffFilter.removeTTL(is, new IteratorSetting.Column(\""a\"", \""b\""));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a = (ColumnAgeOffFilter) a.deepCopy(null);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n@@ -312,19 +312,19 @@ public void test2b() throws IOException {\n \n     ColumnAgeOffFilter a = new ColumnAgeOffFilter();\n     assertTrue(a.validateOptions(is.getOptions()));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(902, size(a));\n \n     ColumnAgeOffFilter.addTTL(is, new IteratorSetting.Column(\""negate\"", \""b\""), 101L);\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n     assertEquals(102, size(a));\n \n     ColumnAgeOffFilter.removeTTL(is, new IteratorSetting.Column(\""negate\"", \""b\""));\n-    a.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    a.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     a = (ColumnAgeOffFilter) a.deepCopy(null);\n     a.overrideCurrentTime(ts);\n     a.seek(new Range(), EMPTY_COL_FAMS, false);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java\nindex fdb34e1fa36..b9ee9c6a3a9 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/IndexedDocIteratorTest.java\n@@ -38,9 +38,9 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.file.rfile.RFileTest;\n import org.apache.accumulo.core.file.rfile.RFileTest.TestRFile;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.MultiIterator;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.Test;\n@@ -51,7 +51,7 @@ public class IndexedDocIteratorTest {\n   private static final Collection<ByteSequence> EMPTY_COL_FAMS = new ArrayList<>();\n   private static final byte[] nullByte = {0};\n \n-  private static IteratorEnvironment env = new DefaultIteratorEnvironment();\n+  private static IteratorEnvironment env = ClientIteratorEnvironment.DEFAULT;\n \n   Text[] columnFamilies;\n   Text[] otherColumnFamilies;\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java\nindex 2c3710d9579..7106db27706 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/IntersectingIteratorTest.java\n@@ -33,9 +33,9 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.MultiIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n@@ -45,7 +45,7 @@ public class IntersectingIteratorTest {\n \n   private static final SecureRandom random = new SecureRandom();\n   private static final Collection<ByteSequence> EMPTY_COL_FAMS = new ArrayList<>();\n-  private static IteratorEnvironment env = new DefaultIteratorEnvironment();\n+  private static IteratorEnvironment env = ClientIteratorEnvironment.DEFAULT;\n \n   HashSet<Text> docs = new HashSet<>();\n   Text[] columnFamilies;\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java\nindex b0a83759348..fd6c7c6b9b5 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/LargeRowFilterTest.java\n@@ -34,6 +34,7 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.junit.jupiter.api.Test;\n@@ -67,7 +68,7 @@ private LargeRowFilter setupIterator(TreeMap<Key,Value> testData, int maxColumns\n     IteratorSetting is = new IteratorSetting(1, LargeRowFilter.class);\n     LargeRowFilter.setMaxColumns(is, maxColumns);\n     lrfi.init(new ColumnFamilySkippingIterator(smi), is.getOptions(),\n-        new RowDeletingIteratorTest.TestIE(scope, false));\n+        new ClientIteratorEnvironment.Builder().withScope(scope).build());\n     return lrfi;\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\nindex 3a558374b4a..8bcf1dac763 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RegExFilterTest.java\n@@ -33,7 +33,7 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.Test;\n@@ -67,7 +67,7 @@ public void test1() throws IOException {\n     RegExFilter.setRegexs(is, \"".*2\"", null, null, null, false);\n \n     assertTrue(rei.validateOptions(is.getOptions()));\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -82,7 +82,7 @@ public void test1() throws IOException {\n     RegExFilter.setRegexs(is, null, null, null, \""amst\"", false, true); // Should only match hamster\n \n     rei.validateOptions(is.getOptions());\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -95,7 +95,7 @@ public void test1() throws IOException {\n \n     RegExFilter.setRegexs(is, null, \""ya.*\"", null, null, false);\n     assertTrue(rei.validateOptions(is.getOptions()));\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -108,7 +108,7 @@ public void test1() throws IOException {\n \n     RegExFilter.setRegexs(is, null, null, \"".*01\"", null, false);\n     assertTrue(rei.validateOptions(is.getOptions()));\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -121,7 +121,7 @@ public void test1() throws IOException {\n \n     RegExFilter.setRegexs(is, null, null, null, \"".*at\"", false);\n     assertTrue(rei.validateOptions(is.getOptions()));\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -133,7 +133,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, null, null, \"".*ap\"", false);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertFalse(rei.hasTop());\n@@ -142,7 +142,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, \""ya.*\"", null, \"".*at\"", false);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -154,7 +154,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, \""ya.*\"", null, \"".*ap\"", false);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertFalse(rei.hasTop());\n@@ -163,7 +163,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, \""boo1\"", null, null, null, false);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -177,7 +177,7 @@ public void test1() throws IOException {\n     // -----------------------------------------------------\n     is.clearOptions();\n \n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -195,7 +195,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, \""hamster\"", null, \""hamster\"", \""hamster\"", true);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -207,7 +207,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, \""ya.*\"", \""hamster\"", null, true);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n@@ -218,9 +218,9 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, \""ya.*\"", \""hamster\"", null, true);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n-    rei.deepCopy(new DefaultIteratorEnvironment());\n+    rei.deepCopy(ClientIteratorEnvironment.DEFAULT);\n \n     // -----------------------------------------------------\n     String multiByteText = new String(\""\\u6d67\\u6F68\\u7067\"");\n@@ -234,7 +234,7 @@ public void test1() throws IOException {\n     is.clearOptions();\n \n     RegExFilter.setRegexs(is, null, null, null, multiByteRegex, true);\n-    rei.init(new SortedMapIterator(tm), is.getOptions(), new DefaultIteratorEnvironment());\n+    rei.init(new SortedMapIterator(tm), is.getOptions(), ClientIteratorEnvironment.DEFAULT);\n     rei.seek(new Range(), EMPTY_COL_FAMS, false);\n \n     assertTrue(rei.hasTop());\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java\nindex 39f0547cff8..0a8ee00998a 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowDeletingIteratorTest.java\n@@ -32,8 +32,8 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n@@ -41,27 +41,6 @@\n \n public class RowDeletingIteratorTest {\n \n-  public static class TestIE implements IteratorEnvironment {\n-\n-    private IteratorScope scope;\n-    private boolean fmc;\n-\n-    public TestIE(IteratorScope scope, boolean fmc) {\n-      this.scope = scope;\n-      this.fmc = fmc;\n-    }\n-\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return scope;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      return fmc;\n-    }\n-  }\n-\n   Key newKey(String row, String cf, String cq, long time) {\n     return new Key(new Text(row), new Text(cf), new Text(cq), time);\n   }\n@@ -91,7 +70,8 @@ public void test1() throws Exception {\n     put(tm1, \""r2\"", \""cf1\"", \""cq1\"", 5, \""v1\"");\n \n     RowDeletingIterator rdi = new RowDeletingIterator();\n-    rdi.init(new SortedMapIterator(tm1), null, new TestIE(IteratorScope.scan, false));\n+    rdi.init(new SortedMapIterator(tm1), null,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n \n     rdi.seek(new Range(), new ArrayList<>(), false);\n     testAssertions(rdi, \""r2\"", \""cf1\"", \""cq1\"", 5, \""v1\"");\n@@ -133,7 +113,8 @@ public void test2() throws Exception {\n     put(tm1, \""r2\"", \""cf1\"", \""cq1\"", 5, \""v1\"");\n \n     RowDeletingIterator rdi = new RowDeletingIterator();\n-    rdi.init(new SortedMapIterator(tm1), null, new TestIE(IteratorScope.scan, false));\n+    rdi.init(new SortedMapIterator(tm1), null,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n \n     rdi.seek(new Range(), new ArrayList<>(), false);\n     testAssertions(rdi, \""r1\"", \""cf1\"", \""cq3\"", 15, \""v1\"");\n@@ -175,7 +156,7 @@ public void test3() throws Exception {\n \n     RowDeletingIterator rdi = new RowDeletingIterator();\n     rdi.init(new ColumnFamilySkippingIterator(new SortedMapIterator(tm1)), null,\n-        new TestIE(IteratorScope.scan, false));\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n \n     HashSet<ByteSequence> cols = new HashSet<>();\n     cols.add(new ArrayByteSequence(\""cf1\"".getBytes(UTF_8)));\n@@ -206,7 +187,8 @@ public void test4() throws Exception {\n     put(tm1, \""r2\"", \""cf1\"", \""cq1\"", 5, \""v1\"");\n \n     RowDeletingIterator rdi = new RowDeletingIterator();\n-    rdi.init(new SortedMapIterator(tm1), null, new TestIE(IteratorScope.minc, false));\n+    rdi.init(new SortedMapIterator(tm1), null,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.minc).build());\n \n     rdi.seek(new Range(), new ArrayList<>(), false);\n     testAssertions(rdi, \""r1\"", \""\"", \""\"", 10, RowDeletingIterator.DELETE_ROW_VALUE.toString());\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java\nindex 91908a7764f..63da92a45c3 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowEncodingIteratorTest.java\n@@ -38,26 +38,14 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n-import org.apache.accumulo.core.iterators.IteratorUtil;\n+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.Test;\n \n public class RowEncodingIteratorTest {\n \n-  private static final class DummyIteratorEnv implements IteratorEnvironment {\n-    @Override\n-    public IteratorUtil.IteratorScope getIteratorScope() {\n-      return IteratorUtil.IteratorScope.scan;\n-    }\n-\n-    @Override\n-    public boolean isFullMajorCompaction() {\n-      return false;\n-    }\n-  }\n-\n   private static final class RowEncodingIteratorImpl extends RowEncodingIterator {\n \n     public static SortedMap<Key,Value> decodeRow(Value rowValue) throws IOException {\n@@ -139,7 +127,8 @@ public void testEncodeAll() throws IOException {\n     RowEncodingIteratorImpl iter = new RowEncodingIteratorImpl();\n     Map<String,String> bigBufferOpts = new HashMap<>();\n     bigBufferOpts.put(RowEncodingIterator.MAX_BUFFER_SIZE_OPT, \""3K\"");\n-    iter.init(src, bigBufferOpts, new DummyIteratorEnv());\n+    iter.init(src, bigBufferOpts,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n     iter.seek(range, new ArrayList<>(), false);\n \n     assertTrue(iter.hasTop());\n@@ -173,7 +162,8 @@ public void testEncodeSome() throws IOException {\n     RowEncodingIteratorImpl iter = new RowEncodingIteratorImpl();\n     Map<String,String> bigBufferOpts = new HashMap<>();\n     bigBufferOpts.put(RowEncodingIterator.MAX_BUFFER_SIZE_OPT, \""1K\"");\n-    iter.init(src, bigBufferOpts, new DummyIteratorEnv());\n+    iter.init(src, bigBufferOpts,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n     assertThrows(IllegalArgumentException.class, () -> iter.seek(range, new ArrayList<>(), false));\n     // IllegalArgumentException should be thrown as we can't fit the whole row into its buffer\n   }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\nindex a18583efb43..4c106559ecd 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/RowFilterTest.java\n@@ -39,9 +39,9 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.hadoop.io.Text;\n@@ -205,7 +205,7 @@ public void test1() throws Exception {\n         new ColumnFamilySkippingIterator(new SortedMapIterator(createKeyValues()));\n \n     RowFilter filter = new SummingRowFilter();\n-    filter.init(source, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter.init(source, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     filter.seek(new Range(), Collections.emptySet(), false);\n \n@@ -235,10 +235,10 @@ public void testChainedRowFilters() throws Exception {\n     SortedMapIterator source = new SortedMapIterator(createKeyValues());\n \n     RowFilter filter0 = new TrueFilter();\n-    filter0.init(source, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter0.init(source, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     RowFilter filter = new TrueFilter();\n-    filter.init(filter0, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter.init(filter0, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     filter.seek(new Range(), Collections.emptySet(), false);\n \n@@ -251,10 +251,10 @@ public void testFilterConjunction() throws Exception {\n     SortedMapIterator source = new SortedMapIterator(createKeyValues());\n \n     RowFilter filter0 = new RowZeroOrOneFilter();\n-    filter0.init(source, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter0.init(source, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     RowFilter filter = new RowOneOrTwoFilter();\n-    filter.init(filter0, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter.init(filter0, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     filter.seek(new Range(), Collections.emptySet(), false);\n \n@@ -266,7 +266,7 @@ public void deepCopyCopiesTheSource() throws Exception {\n     SortedMapIterator source = new SortedMapIterator(createKeyValues());\n \n     RowFilter filter = new RowZeroOrOneFilter();\n-    filter.init(source, Collections.emptyMap(), new DefaultIteratorEnvironment());\n+    filter.init(source, Collections.emptyMap(), ClientIteratorEnvironment.DEFAULT);\n \n     filter.seek(new Range(), Collections.emptySet(), false);\n \n@@ -286,7 +286,7 @@ public void deepCopyCopiesTheSource() throws Exception {\n     }\n \n     // Make a copy of the original RowFilter\n-    RowFilter copy = (RowFilter) filter.deepCopy(new DefaultIteratorEnvironment());\n+    RowFilter copy = (RowFilter) filter.deepCopy(ClientIteratorEnvironment.DEFAULT);\n \n     // Because it's a copy, we should be able to safely seek this one without affecting the original\n     copy.seek(new Range(), Collections.emptySet(), false);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java\nindex 6798b6fecfb..a9c9c92c938 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iterators/user/TransformingIteratorTest.java\n@@ -47,6 +47,7 @@\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n import org.apache.accumulo.core.iterators.WrappingIterator;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.SortedMapIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.VisibilityFilter;\n@@ -619,7 +620,7 @@ public static class ColFamReversingCompactionKeyTransformingIterator\n     @Override\n     public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,\n         IteratorEnvironment env) throws IOException {\n-      env = new MajCIteratorEnvironmentAdapter();\n+      env = new ClientIteratorEnvironment.Builder().withScope(IteratorScope.majc).build();\n       super.init(source, options, env);\n     }\n   }\n@@ -665,7 +666,7 @@ public static class IllegalVisCompactionKeyTransformingIterator\n     @Override\n     public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,\n         IteratorEnvironment env) throws IOException {\n-      env = new MajCIteratorEnvironmentAdapter();\n+      env = new ClientIteratorEnvironment.Builder().withScope(IteratorScope.majc).build();\n       super.init(source, options, env);\n     }\n   }\n@@ -695,7 +696,7 @@ public static class BadVisCompactionKeyTransformingIterator\n     @Override\n     public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,\n         IteratorEnvironment env) throws IOException {\n-      env = new MajCIteratorEnvironmentAdapter();\n+      env = new ClientIteratorEnvironment.Builder().withScope(IteratorScope.majc).build();\n       super.init(source, options, env);\n     }\n   }\n@@ -742,10 +743,4 @@ private void loadTop() {\n     }\n   }\n \n-  private static class MajCIteratorEnvironmentAdapter implements IteratorEnvironment {\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return IteratorScope.majc;\n-    }\n-  }\n }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java b/core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java\nindex de5794607c4..251a3441b36 100644\n--- a/core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/iteratorsImpl/IteratorConfigUtilTest.java\n@@ -39,7 +39,6 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.dataImpl.thrift.IterInfo;\n-import org.apache.accumulo.core.iterators.DefaultIteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n@@ -139,8 +138,7 @@ public Value getTopValue() {\n   private SortedKeyValueIterator<Key,Value> createIter(IteratorScope scope,\n       SortedMapIterator source, AccumuloConfiguration conf) throws IOException {\n     var ibEnv = IteratorConfigUtil.loadIterConf(scope, EMPTY_ITERS, new HashMap<>(), conf);\n-    var iteratorBuilder =\n-        ibEnv.env(new DefaultIteratorEnvironment(conf)).useClassLoader(null).build();\n+    var iteratorBuilder = ibEnv.env(ClientIteratorEnvironment.DEFAULT).useClassLoader(null).build();\n     return IteratorConfigUtil.loadIterators(source, iteratorBuilder);\n   }\n \n\ndiff --git a/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java b/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java\nindex 408f9242b43..4794c4f713f 100644\n--- a/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java\n+++ b/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/IteratorTestInput.java\n@@ -33,7 +33,7 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n-import org.apache.accumulo.iteratortest.environments.SimpleIteratorEnvironment;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n \n /**\n  * The necessary user-input to invoke a test on a {@link SortedKeyValueIterator}.\n@@ -49,7 +49,7 @@ public class IteratorTestInput {\n   private final IteratorEnvironment iteratorEnvironment;\n \n   /**\n-   * Construct an instance of the test input, using {@link SimpleIteratorEnvironment}.\n+   * Construct an instance of the test input, using {@link ClientIteratorEnvironment}.\n    *\n    * @param iteratorClass The class for the iterator to test.\n    * @param iteratorOptions Options, if any, to provide to the iterator ({@link IteratorSetting}'s\n@@ -61,7 +61,7 @@ public class IteratorTestInput {\n   public IteratorTestInput(Class<? extends SortedKeyValueIterator<Key,Value>> iteratorClass,\n       Map<String,String> iteratorOptions, Range range, SortedMap<Key,Value> input) {\n     this(iteratorClass, iteratorOptions, range, input, Collections.emptySet(), false,\n-        new SimpleIteratorEnvironment());\n+        ClientIteratorEnvironment.DEFAULT);\n   }\n \n   /**\n@@ -93,7 +93,7 @@ public IteratorTestInput(Class<? extends SortedKeyValueIterator<Key,Value>> iter\n    * @param families Column families passed to {@link SortedKeyValueIterator#seek}.\n    * @param inclusive Whether the families are inclusive or exclusive.\n    * @param iterEnv An optional provided {@link IteratorEnvironment}.\n-   *        {@link SimpleIteratorEnvironment} will be used if null.\n+   *        {@link ClientIteratorEnvironment} will be used if null.\n    */\n   public IteratorTestInput(Class<? extends SortedKeyValueIterator<Key,Value>> iteratorClass,\n       Map<String,String> iteratorOptions, Range range, SortedMap<Key,Value> input,\n@@ -108,7 +108,7 @@ public IteratorTestInput(Class<? extends SortedKeyValueIterator<Key,Value>> iter\n     this.input = Collections.unmodifiableSortedMap(requireNonNull(input));\n     this.families = Collections.unmodifiableCollection(requireNonNull(families));\n     this.inclusive = inclusive;\n-    this.iteratorEnvironment = iterEnv == null ? new SimpleIteratorEnvironment() : iterEnv;\n+    this.iteratorEnvironment = iterEnv == null ? ClientIteratorEnvironment.DEFAULT : iterEnv;\n   }\n \n   public Class<? extends SortedKeyValueIterator<Key,Value>> getIteratorClass() {\n\ndiff --git a/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/environments/SimpleIteratorEnvironment.java b/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/environments/SimpleIteratorEnvironment.java\ndeleted file mode 100644\nindex 0c89197aed8..00000000000\n--- a/iterator-test-harness/src/main/java/org/apache/accumulo/iteratortest/environments/SimpleIteratorEnvironment.java\n+++ /dev/null\n@@ -1,33 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.iteratortest.environments;\n-\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n-\n-/**\n- * A simple implementation of {@link IteratorEnvironment} which is unimplemented.\n- */\n-public class SimpleIteratorEnvironment implements IteratorEnvironment {\n-\n-  @Override\n-  public boolean isSamplingEnabled() {\n-    return false;\n-  }\n-\n-}\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java b/server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java\nindex 974617d5e84..faa24bdf736 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/replication/StatusCombinerTest.java\n@@ -34,8 +34,8 @@\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.iterators.Combiner;\n import org.apache.accumulo.core.iterators.DevNull;\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.core.replication.ReplicationSchema.StatusSection;\n import org.apache.accumulo.server.replication.proto.Replication.Status;\n import org.junit.jupiter.api.BeforeEach;\n@@ -48,13 +48,6 @@ public class StatusCombinerTest {\n   private Key key;\n   private Status.Builder builder;\n \n-  private static class TestIE implements IteratorEnvironment {\n-    @Override\n-    public IteratorScope getIteratorScope() {\n-      return IteratorScope.scan;\n-    }\n-  }\n-\n   @BeforeEach\n   public void initCombiner() throws IOException {\n     key = new Key();\n@@ -62,7 +55,8 @@ public void initCombiner() throws IOException {\n     builder = Status.newBuilder();\n     IteratorSetting cfg = new IteratorSetting(50, StatusCombiner.class);\n     Combiner.setColumns(cfg, Collections.singletonList(new Column(StatusSection.NAME)));\n-    combiner.init(new DevNull(), cfg.getOptions(), new TestIE());\n+    combiner.init(new DevNull(), cfg.getOptions(),\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.scan).build());\n   }\n \n   @Test\n\ndiff --git a/server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java b/server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java\nindex eefa5c1252b..5bd26d5b74f 100644\n--- a/server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java\n+++ b/server/tserver/src/test/java/org/apache/accumulo/tserver/InMemoryMapTest.java\n@@ -38,7 +38,6 @@\n import org.apache.accumulo.core.client.SampleNotPresentException;\n import org.apache.accumulo.core.client.sample.RowSampler;\n import org.apache.accumulo.core.client.sample.Sampler;\n-import org.apache.accumulo.core.client.sample.SamplerConfiguration;\n import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.Property;\n@@ -49,7 +48,7 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n+import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.ColumnFamilySkippingIterator;\n import org.apache.accumulo.core.iteratorsImpl.system.IterationInterruptedException;\n@@ -59,6 +58,7 @@\n import org.apache.accumulo.core.util.LocalityGroupUtil;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.TableConfiguration;\n+import org.apache.accumulo.server.iterators.TabletIteratorEnvironment;\n import org.apache.accumulo.tserver.InMemoryMap.MemoryIterator;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.Text;\n@@ -71,29 +71,6 @@\n @SuppressFBWarnings(value = \""PATH_TRAVERSAL_IN\"", justification = \""paths not set by user input\"")\n public class InMemoryMapTest extends WithTestNames {\n \n-  private static class SampleIE implements IteratorEnvironment {\n-\n-    private final SamplerConfiguration sampleConfig;\n-\n-    public SampleIE() {\n-      this.sampleConfig = null;\n-    }\n-\n-    public SampleIE(SamplerConfigurationImpl sampleConfig) {\n-      this.sampleConfig = sampleConfig.toSamplerConfiguration();\n-    }\n-\n-    @Override\n-    public boolean isSamplingEnabled() {\n-      return sampleConfig != null;\n-    }\n-\n-    @Override\n-    public SamplerConfiguration getSamplerConfiguration() {\n-      return sampleConfig;\n-    }\n-  }\n-\n   public static ServerContext getServerContext() {\n     Configuration hadoopConf = new Configuration();\n     ServerContext context = EasyMock.createMock(ServerContext.class);\n@@ -319,7 +296,9 @@ public void test6() throws Exception {\n \n     mutate(imm, \""r1\"", \""foo:cq5\"", 3, \""bar5\"");\n \n-    SortedKeyValueIterator<Key,Value> dc = ski1.deepCopy(new SampleIE());\n+    SortedKeyValueIterator<Key,Value> dc =\n+        ski1.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+            getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n \n     ski1.seek(new Range(newKey(\""r1\"", \""foo:cq1\"", 3), null), Set.of(), false);\n     testAndCallNext(ski1, \""r1\"", \""foo:cq1\"", 3, \""bar1\"");\n@@ -371,8 +350,9 @@ private void deepCopyAndDelete(int interleaving, boolean interrupt) throws Excep\n       }\n     }\n \n-    SortedKeyValueIterator<Key,Value> dc = ski1.deepCopy(new SampleIE());\n-\n+    SortedKeyValueIterator<Key,Value> dc =\n+        ski1.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+            getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n     if (interleaving == 2) {\n       imm.delete(0);\n       if (interrupt) {\n@@ -523,7 +503,9 @@ public void testLocalityGroups() throws Exception {\n     MemoryIterator iter1 = imm.skvIterator(null);\n \n     seekLocalityGroups(iter1);\n-    SortedKeyValueIterator<Key,Value> dc1 = iter1.deepCopy(new SampleIE());\n+    SortedKeyValueIterator<Key,Value> dc1 =\n+        iter1.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+            getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n     seekLocalityGroups(dc1);\n \n     assertEquals(10, imm.getNumEntries());\n@@ -576,12 +558,27 @@ public void testSample() throws Exception {\n \n       MemoryIterator iter1 = imm.skvIterator(sampleConfig);\n       MemoryIterator iter2 = imm.skvIterator(null);\n-      SortedKeyValueIterator<Key,Value> iter0dc1 = iter0.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter0dc2 = iter0.deepCopy(new SampleIE(sampleConfig));\n-      SortedKeyValueIterator<Key,Value> iter1dc1 = iter1.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter1dc2 = iter1.deepCopy(new SampleIE(sampleConfig));\n-      SortedKeyValueIterator<Key,Value> iter2dc1 = iter2.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter2dc2 = iter2.deepCopy(new SampleIE(sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter0dc1 =\n+          iter0.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter0dc2 = iter0.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter1dc1 =\n+          iter1.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter1dc2 = iter1.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter2dc1 =\n+          iter2.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter2dc2 = iter2.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n \n       assertEquals(expectedNone, readAll(iter0));\n       assertEquals(expectedNone, readAll(iter0dc1));\n@@ -605,12 +602,27 @@ public void testSample() throws Exception {\n       assertEquals(expectedSample, readAll(iter1dc2));\n       assertEquals(expectedSample, readAll(iter2dc2));\n \n-      SortedKeyValueIterator<Key,Value> iter0dc3 = iter0.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter0dc4 = iter0.deepCopy(new SampleIE(sampleConfig));\n-      SortedKeyValueIterator<Key,Value> iter1dc3 = iter1.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter1dc4 = iter1.deepCopy(new SampleIE(sampleConfig));\n-      SortedKeyValueIterator<Key,Value> iter2dc3 = iter2.deepCopy(new SampleIE());\n-      SortedKeyValueIterator<Key,Value> iter2dc4 = iter2.deepCopy(new SampleIE(sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter0dc3 =\n+          iter0.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter0dc4 = iter0.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter1dc3 =\n+          iter1.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter1dc4 = iter1.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n+      SortedKeyValueIterator<Key,Value> iter2dc3 =\n+          iter2.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+              getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\"")));\n+      SortedKeyValueIterator<Key,\n+          Value> iter2dc4 = iter2.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+              IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+              TableId.of(\""foo\""), sampleConfig));\n \n       assertEquals(expectedNone, readAll(iter0dc3));\n       assertEquals(expectedNone, readAll(iter0dc4));\n@@ -667,7 +679,9 @@ private void runInterruptSampleTest(boolean deepCopy, boolean delete, boolean dc\n     }\n \n     if (deepCopy) {\n-      iter = iter.deepCopy(new SampleIE(sampleConfig1));\n+      iter = iter.deepCopy(new TabletIteratorEnvironment(getServerContext(), IteratorScope.scan,\n+          getServerContext().getTableConfiguration(TableId.of(\""foo\"")), TableId.of(\""foo\""),\n+          sampleConfig1));\n     }\n \n     if (delete && dcAfterDelete) {\n@@ -768,7 +782,10 @@ public void testDeferredSamplerCreation() throws Exception {\n     iter.seek(new Range(), Set.of(), false);\n     assertEquals(expectedSample, readAll(iter));\n \n-    SortedKeyValueIterator<Key,Value> dc = iter.deepCopy(new SampleIE(sampleConfig2));\n+    SortedKeyValueIterator<Key,\n+        Value> dc = iter.deepCopy(new TabletIteratorEnvironment(getServerContext(),\n+            IteratorScope.scan, getServerContext().getTableConfiguration(TableId.of(\""foo\"")),\n+            TableId.of(\""foo\""), sampleConfig2));\n     dc.seek(new Range(), Set.of(), false);\n     assertEquals(expectedSample, readAll(dc));\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java b/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\nindex 63ec7fcf113..e34747e3aea 100644\n--- a/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\n+++ b/test/src/main/java/org/apache/accumulo/test/performance/scan/CollectTabletStats.java\n@@ -52,7 +52,6 @@\n import org.apache.accumulo.core.dataImpl.thrift.IterInfo;\n import org.apache.accumulo.core.file.FileOperations;\n import org.apache.accumulo.core.file.FileSKVIterator;\n-import org.apache.accumulo.core.iterators.IteratorEnvironment;\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n import org.apache.accumulo.core.iteratorsImpl.IteratorConfigUtil;\n@@ -73,6 +72,7 @@\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n import org.apache.accumulo.server.conf.TableConfiguration;\n import org.apache.accumulo.server.fs.VolumeManager;\n+import org.apache.accumulo.server.iterators.TabletIteratorEnvironment;\n import org.apache.accumulo.server.util.MetadataTableUtil;\n import org.apache.hadoop.fs.BlockLocation;\n import org.apache.hadoop.fs.FileStatus;\n@@ -103,8 +103,6 @@ static class CollectOptions extends ServerUtilOpts {\n     String columns;\n   }\n \n-  static class TestEnvironment implements IteratorEnvironment {}\n-\n   public static void main(String[] args) throws Exception {\n \n     final CollectOptions opts = new CollectOptions();\n@@ -429,8 +427,8 @@ private static void reportHdfsBlockLocations(ServerContext context, List<TabletF\n   private static SortedKeyValueIterator<Key,Value> createScanIterator(KeyExtent ke,\n       Collection<SortedKeyValueIterator<Key,Value>> mapfiles, Authorizations authorizations,\n       byte[] defaultLabels, HashSet<Column> columnSet, List<IterInfo> ssiList,\n-      Map<String,Map<String,String>> ssio, boolean useTableIterators, TableConfiguration conf)\n-      throws IOException {\n+      Map<String,Map<String,String>> ssio, boolean useTableIterators, TableConfiguration conf,\n+      ServerContext context) throws IOException {\n \n     SortedMapIterator smi = new SortedMapIterator(new TreeMap<>());\n \n@@ -449,7 +447,9 @@ private static SortedKeyValueIterator<Key,Value> createScanIterator(KeyExtent ke\n \n     if (useTableIterators) {\n       var ibEnv = IteratorConfigUtil.loadIterConf(IteratorScope.scan, ssiList, ssio, conf);\n-      var iteratorBuilder = ibEnv.env(new TestEnvironment()).useClassLoader(\""test\"").build();\n+      TabletIteratorEnvironment iterEnv =\n+          new TabletIteratorEnvironment(context, IteratorScope.scan, conf, ke.tableId());\n+      var iteratorBuilder = ibEnv.env(iterEnv).useClassLoader(\""test\"").build();\n       return IteratorConfigUtil.loadIterators(visFilter, iteratorBuilder);\n     }\n     return visFilter;\n@@ -506,7 +506,7 @@ private static int readFilesUsingIterStack(VolumeManager fs, ServerContext conte\n     Map<String,Map<String,String>> emptySsio = Collections.emptyMap();\n     TableConfiguration tconf = context.getTableConfiguration(ke.tableId());\n     reader = createScanIterator(ke, readers, auths, new byte[] {}, new HashSet<>(), emptyIterinfo,\n-        emptySsio, useTableIterators, tconf);\n+        emptySsio, useTableIterators, tconf, context);\n \n     HashSet<ByteSequence> columnSet = createColumnBSS(columns);\n \n\ndiff --git a/test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java b/test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java\nindex 7d8b21519f0..0bbb6a85139 100644\n--- a/test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java\n+++ b/test/src/test/java/org/apache/accumulo/test/iterator/SummingCombinerTest.java\n@@ -32,11 +32,11 @@\n import org.apache.accumulo.core.iterators.IteratorUtil.IteratorScope;\n import org.apache.accumulo.core.iterators.LongCombiner;\n import org.apache.accumulo.core.iterators.user.SummingCombiner;\n+import org.apache.accumulo.core.iteratorsImpl.ClientIteratorEnvironment;\n import org.apache.accumulo.iteratortest.IteratorTestBase;\n import org.apache.accumulo.iteratortest.IteratorTestInput;\n import org.apache.accumulo.iteratortest.IteratorTestOutput;\n import org.apache.accumulo.iteratortest.IteratorTestParameters;\n-import org.apache.accumulo.iteratortest.environments.SimpleIteratorEnvironment;\n \n /**\n  * Iterator test harness tests for SummingCombiner\n@@ -48,14 +48,8 @@ public class SummingCombinerTest extends IteratorTestBase {\n \n   @Override\n   protected Stream<IteratorTestParameters> parameters() {\n-    var env = new SimpleIteratorEnvironment() {\n-      @Override\n-      public IteratorScope getIteratorScope() {\n-        return IteratorScope.majc;\n-      }\n-    };\n-    var input =\n-        new IteratorTestInput(SummingCombiner.class, createOpts(), new Range(), INPUT_DATA, env);\n+    var input = new IteratorTestInput(SummingCombiner.class, createOpts(), new Range(), INPUT_DATA,\n+        new ClientIteratorEnvironment.Builder().withScope(IteratorScope.majc).build());\n     var expectedOutput = new IteratorTestOutput(OUTPUT_DATA);\n     return builtinTestCases().map(test -> test.toParameters(input, expectedOutput));\n   }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5419"", ""pr_id"": 5419, ""issue_id"": 5411, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Wrong error being logged during upgrade\nDuring the upgrade process the HighlyAvailableServiceInvocationHandler blocks all incoming Thrift requests and raises a ThriftNotActiveServiceException (see below).\n\nhttps://github.com/apache/accumulo/blob/4cba6dee3e89960bbaac2f9c86eb34a3a0426779/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java#L51-L56\n\nHowever, what I'm seeing in the log looks like the following (note the null message):\n\n```\n2025-03-17T18:26:00,241 [thrift.ProcessFunction] ERROR: Internal error processing reportTabletStatus\norg.apache.accumulo.core.clientImpl.thrift.ThriftNotActiveServiceException: null\n        at org.apache.accumulo.server.rpc.HighlyAvailableServiceInvocationHandler.invoke(HighlyAvailableServiceInvocationHandler.java:54) ~[accumulo-server-base-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\n        at jdk.proxy2/jdk.proxy2.$Proxy33.reportTabletStatus(Unknown Source) ~[?:?]\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[?:?]\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n        at java.base/java.lang.reflect.Method.invoke(Method.java:569) ~[?:?]\n        at org.apache.accumulo.core.trace.TraceUtil.lambda$wrapService$0(TraceUtil.java:203) ~[accumulo-core-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\n        at jdk.proxy2/jdk.proxy2.$Proxy37.reportTabletStatus(Unknown Source) ~[?:?]\n        at org.apache.accumulo.core.manager.thrift.ManagerClientService$Processor$reportTabletStatus.getResult(ManagerClientService.java:2565) ~[accumulo-core-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\n        at org.apache.accumulo.core.manager.thrift.ManagerClientService$Processor$reportTabletStatus.getResult(ManagerClientService.java:2543) ~[accumulo-core-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:40) ~[libthrift-0.17.0.jar:0.17.0]\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:40) ~[libthrift-0.17.0.jar:0.17.0]\n        at org.apache.thrift.TMultiplexedProcessor.process(TMultiplexedProcessor.java:147) ~[libthrift-0.17.0.jar:0.17.0]\n        at org.apache.accumulo.server.rpc.TimedProcessor.process(TimedProcessor.java:50) ~[accumulo-server-base-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:492) ~[libthrift-0.17.0.jar:0.17.0]\n\n```\n\nFrom what I can tell the logging is coming from the Thrift ProcessFunction class at:\n\nhttps://github.com/apache/thrift/blob/60655d2de79e973b89fab52af82f9628d4843b0f/lib/java/src/main/java/org/apache/thrift/ProcessFunction.java#L48-L49\n\n`ex` being passed to `LOGGER` is ThriftNotActiveServiceException and the message is null because ThriftNotActiveServiceException is not passing a message using a TException constructor. I'm thinking that all of our Thrift generated Thrift exceptions have this issue."", ""issue_word_count"": 403, ""test_files_count"": 3, ""non_test_files_count"": 11, ""pr_changed_files"": [""server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceWrapper.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/ServerAddress.java"", ""server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java"", ""server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java"", ""server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java"", ""server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/ScanServer.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java"", ""test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java""], ""pr_changed_test_files"": [""server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java"", ""test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java""], ""base_commit"": ""cc9aff0bbc563afb74a232e36f6473e9f9bae911"", ""head_commit"": ""97f0b10c247f3c525b4e7d6ec582c162defaef6c"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5419"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5419"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-21T12:06:52.000Z"", ""patch"": ""diff --git a/server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java b/server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java\ndeleted file mode 100644\nindex 4d529369a0a..00000000000\n--- a/server/base/src/main/java/org/apache/accumulo/server/HighlyAvailableService.java\n+++ /dev/null\n@@ -1,51 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.server;\n-\n-/**\n- * This interface allows service implementations which support running multiple instances\n- * concurrently with only one active instance to report whether or not they are the active service.\n- */\n-public interface HighlyAvailableService {\n-\n-  /**\n-   * Is this service instance currently the active instance for the Accumulo cluster.\n-   *\n-   * @return True if the service is the active service, false otherwise.\n-   */\n-  boolean isActiveService();\n-\n-  /**\n-   * Is this service instance currently in the process of upgrading.\n-   *\n-   * @return True if the service is upgrading, false otherwise.\n-   */\n-  default boolean isUpgrading() {\n-    return false;\n-  }\n-\n-  /**\n-   * Get the name of the service\n-   *\n-   * @return service name\n-   */\n-  default String getServiceName() {\n-    return this.getClass().getSimpleName();\n-  }\n-}\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java\ndeleted file mode 100644\nindex a0e8a9453af..00000000000\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceInvocationHandler.java\n+++ /dev/null\n@@ -1,71 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.server.rpc;\n-\n-import java.lang.reflect.InvocationHandler;\n-import java.lang.reflect.InvocationTargetException;\n-import java.lang.reflect.Method;\n-import java.util.Objects;\n-\n-import org.apache.accumulo.core.clientImpl.thrift.ThriftNotActiveServiceException;\n-import org.apache.accumulo.server.HighlyAvailableService;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * An {@link InvocationHandler} which checks to see if a {@link HighlyAvailableService} is the\n- * current active instance of that service, throwing {@link ThriftNotActiveServiceException} when it\n- * is not the current active instance.\n- */\n-public class HighlyAvailableServiceInvocationHandler<I> implements InvocationHandler {\n-  private static final Logger LOG =\n-      LoggerFactory.getLogger(HighlyAvailableServiceInvocationHandler.class);\n-\n-  private final I instance;\n-  private final HighlyAvailableService service;\n-\n-  public HighlyAvailableServiceInvocationHandler(I instance, HighlyAvailableService service) {\n-    this.instance = Objects.requireNonNull(instance);\n-    this.service = Objects.requireNonNull(service);\n-  }\n-\n-  @Override\n-  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n-\n-    // If the service is upgrading, throw an exception\n-    if (service.isUpgrading()) {\n-      LOG.trace(\""Service can not be accessed while it is upgrading.\"");\n-      throw new ThriftNotActiveServiceException(service.getServiceName(),\n-          \""Service can not be accessed while it is upgrading\"");\n-    }\n-\n-    // If the service is not active, throw an exception\n-    if (!service.isActiveService()) {\n-      LOG.trace(\""Denying access to RPC service as this instance is not the active instance.\"");\n-      throw new ThriftNotActiveServiceException(service.getServiceName(),\n-          \""Denying access to RPC service as this instance is not the active instance\"");\n-    }\n-    try {\n-      // Otherwise, call the real method\n-      return method.invoke(instance, args);\n-    } catch (InvocationTargetException ex) {\n-      throw ex.getCause();\n-    }\n-  }\n-}\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceWrapper.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceWrapper.java\ndeleted file mode 100644\nindex 307b2dfa36f..00000000000\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/HighlyAvailableServiceWrapper.java\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.server.rpc;\n-\n-import java.lang.reflect.InvocationHandler;\n-import java.lang.reflect.Proxy;\n-\n-import org.apache.accumulo.core.util.ClassUtil;\n-import org.apache.accumulo.server.HighlyAvailableService;\n-\n-/**\n- * A class to wrap invocations to the Thrift handler to prevent these invocations from succeeding\n- * when the Accumulo service that this Thrift service is for has not yet obtained its ZooKeeper\n- * lock.\n- *\n- * @since 2.0\n- */\n-public class HighlyAvailableServiceWrapper {\n-\n-  private static final HighlyAvailableServiceWrapper INSTANCE = new HighlyAvailableServiceWrapper();\n-\n-  // Not for public use.\n-  private HighlyAvailableServiceWrapper() {}\n-\n-  public static <I> I service(final I instance, HighlyAvailableService service) {\n-    InvocationHandler handler = INSTANCE.getInvocationHandler(instance, service);\n-\n-    @SuppressWarnings(\""unchecked\"")\n-    I proxiedInstance = (I) Proxy.newProxyInstance(instance.getClass().getClassLoader(),\n-        ClassUtil.getInterfaces(instance.getClass()).toArray(new Class<?>[0]), handler);\n-    return proxiedInstance;\n-  }\n-\n-  protected <T> HighlyAvailableServiceInvocationHandler<T> getInvocationHandler(final T instance,\n-      final HighlyAvailableService service) {\n-    return new HighlyAvailableServiceInvocationHandler<>(instance, service);\n-  }\n-}\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/ServerAddress.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/ServerAddress.java\nindex 0e033b9056c..3924bea1156 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/ServerAddress.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/rpc/ServerAddress.java\n@@ -18,8 +18,12 @@\n  */\n package org.apache.accumulo.server.rpc;\n \n+import org.apache.accumulo.core.util.Halt;\n+import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.core.util.threads.Threads;\n import org.apache.thrift.server.TServer;\n \n+import com.google.common.base.Preconditions;\n import com.google.common.net.HostAndPort;\n \n /**\n@@ -41,4 +45,22 @@ public TServer getServer() {\n   public HostAndPort getAddress() {\n     return address;\n   }\n+\n+  public void startThriftServer(String threadName) {\n+    Threads.createThread(threadName, () -> {\n+      try {\n+        server.serve();\n+      } catch (Error e) {\n+        Halt.halt(\""Unexpected error in TThreadPoolServer \"" + e + \"", halting.\"", 1);\n+      }\n+    }).start();\n+\n+    while (!server.isServing()) {\n+      // Wait for the thread to start and for the TServer to start\n+      // serving events\n+      UtilWaitThread.sleep(10);\n+      Preconditions.checkState(!server.getShouldStop());\n+    }\n+\n+  }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\nindex a1318c4beba..fd37e171a0b 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/rpc/TServerUtils.java\n@@ -47,18 +47,14 @@\n import org.apache.accumulo.core.rpc.SslConnectionParams;\n import org.apache.accumulo.core.rpc.ThriftUtil;\n import org.apache.accumulo.core.rpc.UGIAssumingTransportFactory;\n-import org.apache.accumulo.core.util.Halt;\n import org.apache.accumulo.core.util.Pair;\n-import org.apache.accumulo.core.util.UtilWaitThread;\n import org.apache.accumulo.core.util.threads.ThreadPools;\n-import org.apache.accumulo.core.util.threads.Threads;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.hadoop.security.SaslRpcServer;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.thrift.TProcessor;\n import org.apache.thrift.TProcessorFactory;\n import org.apache.thrift.protocol.TProtocolFactory;\n-import org.apache.thrift.server.TServer;\n import org.apache.thrift.server.TThreadPoolServer;\n import org.apache.thrift.server.TThreadedSelectorServer;\n import org.apache.thrift.transport.TNonblockingServerSocket;\n@@ -73,7 +69,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import com.google.common.base.Preconditions;\n import com.google.common.net.HostAndPort;\n import com.google.common.primitives.Ints;\n \n@@ -114,13 +109,14 @@ static Map<Integer,Property> getReservedPorts(AccumuloConfiguration config,\n   }\n \n   /**\n-   * Start a server, at the given port, or higher, if that port is not available.\n+   * Create a ServerAddress, at the given port, or higher, if that port is not available. Callers\n+   * must start the ThriftServer after calling this method using\n+   * {@code ServerAddress#startThriftServer(String)}\n    *\n    * @param context RPC configuration\n    * @param portHintProperty the port to attempt to open, can be zero, meaning \""any available port\""\n    * @param processor the service to be started\n    * @param serverName the name of the class that is providing the service\n-   * @param threadName name this service's thread for better debugging\n    * @param portSearchProperty A boolean Property to control if port-search should be used, or null\n    *        to disable\n    * @param minThreadProperty A Property to control the minimum number of threads in the pool\n@@ -129,8 +125,8 @@ static Map<Integer,Property> getReservedPorts(AccumuloConfiguration config,\n    * @return the server object created, and the port actually used\n    * @throws UnknownHostException when we don't know our own address\n    */\n-  public static ServerAddress startServer(ServerContext context, String hostname,\n-      Property portHintProperty, TProcessor processor, String serverName, String threadName,\n+  public static ServerAddress createThriftServer(ServerContext context, String hostname,\n+      Property portHintProperty, TProcessor processor, String serverName,\n       Property portSearchProperty, Property minThreadProperty, Property threadTimeOutProperty,\n       Property timeBetweenThreadChecksProperty) throws UnknownHostException {\n     final AccumuloConfiguration config = context.getConfiguration();\n@@ -174,8 +170,8 @@ public static ServerAddress startServer(ServerContext context, String hostname,\n \n     HostAndPort[] addresses = getHostAndPorts(hostname, portHint);\n     try {\n-      return TServerUtils.startTServer(serverType, timedProcessor, serverName, threadName,\n-          minThreads, threadTimeOut, config, timeBetweenThreadChecks, maxMessageSize,\n+      return TServerUtils.createThriftServer(serverType, timedProcessor, serverName, minThreads,\n+          threadTimeOut, config, timeBetweenThreadChecks, maxMessageSize,\n           context.getServerSslParams(), context.getSaslParams(), context.getClientTimeoutInMillis(),\n           backlog, portSearch, addresses);\n     } catch (TTransportException e) {\n@@ -199,12 +195,12 @@ public static ServerAddress startServer(ServerContext context, String hostname,\n           }\n           try {\n             HostAndPort addr = HostAndPort.fromParts(hostname, port);\n-            return TServerUtils.startTServer(serverType, timedProcessor, serverName, threadName,\n+            return TServerUtils.createThriftServer(serverType, timedProcessor, serverName,\n                 minThreads, threadTimeOut, config, timeBetweenThreadChecks, maxMessageSize,\n                 context.getServerSslParams(), context.getSaslParams(),\n                 context.getClientTimeoutInMillis(), backlog, portSearch, addr);\n           } catch (TTransportException tte) {\n-            log.info(\""Unable to use port {}, retrying. (Thread Name = {})\"", port, threadName);\n+            log.info(\""Unable to use port {}, retrying.\"", port);\n           }\n         }\n         log.error(\""Unable to start TServer\"", e);\n@@ -562,9 +558,14 @@ private static ServerAddress createSaslThreadPoolServer(HostAndPort address, TPr\n     return new ServerAddress(server, address);\n   }\n \n-  public static ServerAddress startTServer(final AccumuloConfiguration conf,\n-      ThriftServerType serverType, TProcessor processor, String serverName, String threadName,\n-      int numThreads, long threadTimeOut, long timeBetweenThreadChecks, long maxMessageSize,\n+  /**\n+   * Create a ServerAddress, at the given port, or higher, if that port is not available. Callers\n+   * must start the ThriftServer after calling this method using\n+   * {@code ServerAddress#startThriftServer(String)}\n+   */\n+  public static ServerAddress createThriftServer(final AccumuloConfiguration conf,\n+      ThriftServerType serverType, TProcessor processor, String serverName, int numThreads,\n+      long threadTimeOut, long timeBetweenThreadChecks, long maxMessageSize,\n       SslConnectionParams sslParams, SaslServerConnectionParams saslParams,\n       long serverSocketTimeout, int backlog, MetricsInfo metricsInfo, boolean portSearch,\n       HostAndPort... addresses) {\n@@ -574,9 +575,9 @@ public static ServerAddress startTServer(final AccumuloConfiguration conf,\n     }\n \n     try {\n-      return startTServer(serverType, new TimedProcessor(processor, metricsInfo), serverName,\n-          threadName, numThreads, threadTimeOut, conf, timeBetweenThreadChecks, maxMessageSize,\n-          sslParams, saslParams, serverSocketTimeout, backlog, portSearch, addresses);\n+      return createThriftServer(serverType, new TimedProcessor(processor, metricsInfo), serverName,\n+          numThreads, threadTimeOut, conf, timeBetweenThreadChecks, maxMessageSize, sslParams,\n+          saslParams, serverSocketTimeout, backlog, portSearch, addresses);\n     } catch (TTransportException e) {\n       throw new IllegalStateException(e);\n     }\n@@ -589,8 +590,8 @@ public static ServerAddress startTServer(final AccumuloConfiguration conf,\n    * @return A ServerAddress encapsulating the Thrift server created and the host/port which it is\n    *         bound to.\n    */\n-  private static ServerAddress startTServer(ThriftServerType serverType, TimedProcessor processor,\n-      String serverName, String threadName, int numThreads, long threadTimeOut,\n+  private static ServerAddress createThriftServer(ThriftServerType serverType,\n+      TimedProcessor processor, String serverName, int numThreads, long threadTimeOut,\n       final AccumuloConfiguration conf, long timeBetweenThreadChecks, long maxMessageSize,\n       SslConnectionParams sslParams, SaslServerConnectionParams saslParams,\n       long serverSocketTimeout, int backlog, boolean portSearch, HostAndPort... addresses)\n@@ -653,28 +654,11 @@ private static ServerAddress startTServer(ThriftServerType serverType, TimedProc\n           \""Unable to create server on addresses: \"" + Arrays.toString(addresses));\n     }\n \n-    final TServer finalServer = serverAddress.server;\n-\n-    Threads.createThread(threadName, () -> {\n-      try {\n-        finalServer.serve();\n-      } catch (Error e) {\n-        Halt.halt(\""Unexpected error in TThreadPoolServer \"" + e + \"", halting.\"", 1);\n-      }\n-    }).start();\n-\n-    while (!finalServer.isServing()) {\n-      // Wait for the thread to start and for the TServer to start\n-      // serving events\n-      UtilWaitThread.sleep(10);\n-      Preconditions.checkState(!finalServer.getShouldStop());\n-    }\n-\n     // check for the special \""bind to everything address\""\n     if (serverAddress.address.getHost().equals(\""0.0.0.0\"")) {\n       // can't get the address from the bind, so we'll do our best to invent our hostname\n       try {\n-        serverAddress = new ServerAddress(finalServer, HostAndPort\n+        serverAddress = new ServerAddress(serverAddress.server, HostAndPort\n             .fromParts(InetAddress.getLocalHost().getHostName(), serverAddress.address.getPort()));\n       } catch (UnknownHostException e) {\n         throw new TTransportException(e);\n\ndiff --git a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\nindex 72145ab32d0..2bfc6b72769 100644\n--- a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n+++ b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n@@ -322,10 +322,11 @@ protected ServerAddress startCompactorClientService() throws UnknownHostExceptio\n     ClientServiceHandler clientHandler = new ClientServiceHandler(getContext());\n     var processor = ThriftProcessorTypes.getCompactorTProcessor(this, clientHandler,\n         getCompactorThriftHandlerInterface(), getContext());\n-    ServerAddress sp = TServerUtils.startServer(getContext(), getHostname(),\n+    ServerAddress sp = TServerUtils.createThriftServer(getContext(), getHostname(),\n         Property.COMPACTOR_CLIENTPORT, processor, this.getClass().getSimpleName(),\n-        \""Thrift Client Server\"", Property.COMPACTOR_PORTSEARCH, Property.COMPACTOR_MINTHREADS,\n+        Property.COMPACTOR_PORTSEARCH, Property.COMPACTOR_MINTHREADS,\n         Property.COMPACTOR_MINTHREADS_TIMEOUT, Property.COMPACTOR_THREADCHECK);\n+    sp.startThriftServer(\""Thrift Client Server\"");\n     setHostname(sp.address);\n     LOG.info(\""address = {}\"", sp.address);\n     return sp;\n\ndiff --git a/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java b/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\nindex 2d7afe261d9..46e4fe8c8db 100644\n--- a/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\n+++ b/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\n@@ -429,12 +429,13 @@ private HostAndPort startStatsService() {\n     IntStream port = getConfiguration().getPortStream(Property.GC_PORT);\n     HostAndPort[] addresses = TServerUtils.getHostAndPorts(getHostname(), port);\n     long maxMessageSize = getConfiguration().getAsBytes(Property.RPC_MAX_MESSAGE_SIZE);\n-    ServerAddress server = TServerUtils.startTServer(getConfiguration(),\n-        getContext().getThriftServerType(), processor, this.getClass().getSimpleName(),\n-        \""GC Monitor Service\"", 2, ThreadPools.DEFAULT_TIMEOUT_MILLISECS, 1000, maxMessageSize,\n-        getContext().getServerSslParams(), getContext().getSaslParams(), 0,\n-        getConfiguration().getCount(Property.RPC_BACKLOG), getContext().getMetricsInfo(), false,\n-        addresses);\n+    ServerAddress server =\n+        TServerUtils.createThriftServer(getConfiguration(), getContext().getThriftServerType(),\n+            processor, this.getClass().getSimpleName(), 2, ThreadPools.DEFAULT_TIMEOUT_MILLISECS,\n+            1000, maxMessageSize, getContext().getServerSslParams(), getContext().getSaslParams(),\n+            0, getConfiguration().getCount(Property.RPC_BACKLOG), getContext().getMetricsInfo(),\n+            false, addresses);\n+    server.startThriftServer(\""GC Monitor Service\"");\n     setHostname(server.address);\n     log.debug(\""Starting garbage collector listening on \"" + server.address);\n     return server.address;\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex edeef321a6b..5da833ee845 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -52,7 +52,6 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Future;\n import java.util.concurrent.ThreadPoolExecutor;\n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.function.Function;\n@@ -97,7 +96,6 @@\n import org.apache.accumulo.core.manager.balancer.TabletServerIdImpl;\n import org.apache.accumulo.core.manager.state.tables.TableState;\n import org.apache.accumulo.core.manager.thrift.BulkImportState;\n-import org.apache.accumulo.core.manager.thrift.ManagerClientService;\n import org.apache.accumulo.core.manager.thrift.ManagerGoalState;\n import org.apache.accumulo.core.manager.thrift.ManagerMonitorInfo;\n import org.apache.accumulo.core.manager.thrift.ManagerState;\n@@ -133,7 +131,6 @@\n import org.apache.accumulo.manager.upgrade.UpgradeCoordinator;\n import org.apache.accumulo.manager.upgrade.UpgradeCoordinator.UpgradeStatus;\n import org.apache.accumulo.server.AbstractServer;\n-import org.apache.accumulo.server.HighlyAvailableService;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.compaction.CompactionConfigStorage;\n import org.apache.accumulo.server.fs.VolumeManager;\n@@ -145,7 +142,6 @@\n import org.apache.accumulo.server.manager.state.TabletServerState;\n import org.apache.accumulo.server.manager.state.TabletStateStore;\n import org.apache.accumulo.server.manager.state.UnassignedTablet;\n-import org.apache.accumulo.server.rpc.HighlyAvailableServiceWrapper;\n import org.apache.accumulo.server.rpc.ServerAddress;\n import org.apache.accumulo.server.rpc.TServerUtils;\n import org.apache.accumulo.server.rpc.ThriftProcessorTypes;\n@@ -183,8 +179,7 @@\n  * <p>\n  * The manager will also coordinate log recoveries and reports general status.\n  */\n-public class Manager extends AbstractServer\n-    implements LiveTServerSet.Listener, TableObserver, HighlyAvailableService {\n+public class Manager extends AbstractServer implements LiveTServerSet.Listener, TableObserver {\n \n   static final Logger log = LoggerFactory.getLogger(Manager.class);\n \n@@ -236,8 +231,6 @@ public class Manager extends AbstractServer\n \n   final ServerBulkImportStatus bulkImportStatus = new ServerBulkImportStatus();\n \n-  private final AtomicBoolean managerInitialized = new AtomicBoolean(false);\n-\n   private final long timeToCacheRecoveryWalExistence;\n   private ExecutorService tableInformationStatusPool = null;\n   private ThreadPoolExecutor tabletRefreshThreadPool;\n@@ -1146,26 +1139,24 @@ public void run() {\n     managerClientHandler = new ManagerClientServiceHandler(this);\n     compactionCoordinator = new CompactionCoordinator(context, security, fateRefs, this);\n \n-    // Start the Manager's Client service\n-    // Ensure that calls before the manager gets the lock fail\n-    ManagerClientService.Iface haProxy =\n-        HighlyAvailableServiceWrapper.service(managerClientHandler, this);\n-\n     ServerAddress sa;\n     var processor = ThriftProcessorTypes.getManagerTProcessor(this, fateServiceHandler,\n-        compactionCoordinator.getThriftService(), haProxy, getContext());\n+        compactionCoordinator.getThriftService(), managerClientHandler, getContext());\n \n     try {\n-      sa = TServerUtils.startServer(context, getHostname(), Property.MANAGER_CLIENTPORT, processor,\n-          \""Manager\"", \""Manager Client Service Handler\"", null, Property.MANAGER_MINTHREADS,\n+      sa = TServerUtils.createThriftServer(context, getHostname(), Property.MANAGER_CLIENTPORT,\n+          processor, \""Manager\"", null, Property.MANAGER_MINTHREADS,\n           Property.MANAGER_MINTHREADS_TIMEOUT, Property.MANAGER_THREADCHECK);\n     } catch (UnknownHostException e) {\n       throw new IllegalStateException(\""Unable to start server on host \"" + getHostname(), e);\n     }\n-    clientService = sa.server;\n-    log.info(\""Started Manager client service at {}\"", sa.address);\n \n-    // block until we can obtain the ZK lock for the manager\n+    // block until we can obtain the ZK lock for the manager. Create the\n+    // initial lock using ThriftService.NONE. This will allow the lock\n+    // allocation to occur, but prevent any services from getting the\n+    // Manager address for the COORDINATOR, FATE, and MANAGER services.\n+    // The lock data is replaced below and the manager address is exposed\n+    // for each of these services.\n     ServiceLockData sld;\n     try {\n       sld = getManagerLock(context.getServerPaths().createManagerPath());\n@@ -1402,7 +1393,16 @@ boolean canSuspendTablets() {\n       log.info(\""AuthenticationTokenSecretManager is initialized\"");\n     }\n \n-    UUID uuid = sld.getServerUUID(ThriftService.MANAGER);\n+    // Now that the Manager is up, start the ThriftServer\n+    sa.startThriftServer(\""Manager Client Service Handler\"");\n+    clientService = sa.server;\n+    log.info(\""Started Manager client service at {}\"", sa.address);\n+\n+    // Replace the ServiceLockData information in the Manager lock node in ZooKeeper.\n+    // This advertises the address that clients can use to connect to the Manager\n+    // for the Coordinator, Fate, and Manager services. Do **not** do this until\n+    // after the upgrade process is finished and the dependent services are started.\n+    UUID uuid = sld.getServerUUID(ThriftService.NONE);\n     ServiceDescriptors descriptors = new ServiceDescriptors();\n     for (ThriftService svc : new ThriftService[] {ThriftService.MANAGER, ThriftService.COORDINATOR,\n         ThriftService.FATE}) {\n@@ -1418,13 +1418,6 @@ boolean canSuspendTablets() {\n       throw new IllegalStateException(\""Exception updating manager lock\"", e);\n     }\n \n-    while (!clientService.isServing()) {\n-      sleepUninterruptibly(100, MILLISECONDS);\n-    }\n-\n-    // The manager is fully initialized. Clients are allowed to connect now.\n-    managerInitialized.set(true);\n-\n     while (!isShutdownRequested() && clientService.isServing()) {\n       if (Thread.currentThread().isInterrupted()) {\n         log.info(\""Server process thread has been interrupted, shutting down\"");\n@@ -1607,7 +1600,7 @@ private ServiceLockData getManagerLock(final ServiceLockPath zManagerLoc)\n     UUID zooLockUUID = UUID.randomUUID();\n \n     ServiceDescriptors descriptors = new ServiceDescriptors();\n-    descriptors.addService(new ServiceDescriptor(zooLockUUID, ThriftService.MANAGER,\n+    descriptors.addService(new ServiceDescriptor(zooLockUUID, ThriftService.NONE,\n         managerClientAddress, this.getResourceGroup()));\n     ServiceLockData sld = new ServiceLockData(descriptors);\n     managerLock = new ServiceLock(zooKeeper, zManagerLoc, zooLockUUID);\n@@ -1871,12 +1864,6 @@ public SteadyTime getSteadyTime() {\n     return timeKeeper.getTime();\n   }\n \n-  @Override\n-  public boolean isActiveService() {\n-    return managerInitialized.get();\n-  }\n-\n-  @Override\n   public boolean isUpgrading() {\n     return upgradeCoordinator.getStatus() != UpgradeCoordinator.UpgradeStatus.COMPLETE;\n   }\n\ndiff --git a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\nindex d0ec4b13dc0..51ea5edc4cb 100644\n--- a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n+++ b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n@@ -82,7 +82,6 @@\n import org.apache.accumulo.monitor.rest.compactions.external.RunningCompactions;\n import org.apache.accumulo.monitor.rest.compactions.external.RunningCompactorDetails;\n import org.apache.accumulo.server.AbstractServer;\n-import org.apache.accumulo.server.HighlyAvailableService;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.util.TableInfoUtil;\n import org.apache.thrift.transport.TTransportException;\n@@ -109,7 +108,7 @@\n /**\n  * Serve manager statistics with an embedded web server.\n  */\n-public class Monitor extends AbstractServer implements HighlyAvailableService, Connection.Listener {\n+public class Monitor extends AbstractServer implements Connection.Listener {\n \n   private static final Logger log = LoggerFactory.getLogger(Monitor.class);\n   private static final int REFRESH_TIME = 5;\n@@ -140,7 +139,6 @@ public static void main(String[] args) throws Exception {\n   private long totalHoldTime = 0;\n   private long totalLookups = 0;\n   private int totalTables = 0;\n-  private final AtomicBoolean monitorInitialized = new AtomicBoolean(false);\n \n   private EventCounter lookupRateTracker = new EventCounter();\n   private EventCounter indexCacheHitTracker = new EventCounter();\n@@ -430,8 +428,6 @@ public void run() {\n     }).start();\n     Threads.createThread(\""Metric Fetcher Thread\"", fetcher).start();\n \n-    monitorInitialized.set(true);\n-\n     while (!isShutdownRequested()) {\n       if (Thread.currentThread().isInterrupted()) {\n         log.info(\""Server process thread has been interrupted, shutting down\"");\n@@ -825,11 +821,6 @@ public double getLookupRate() {\n     return lookupRateTracker.calculateRate();\n   }\n \n-  @Override\n-  public boolean isActiveService() {\n-    return monitorInitialized.get();\n-  }\n-\n   public Optional<HostAndPort> getCoordinatorHost() {\n     return coordinatorHost;\n   }\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/ScanServer.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/ScanServer.java\nindex 535d100dfeb..595df77ba44 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/ScanServer.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/ScanServer.java\n@@ -304,11 +304,11 @@ protected ServerAddress startScanServerClientService() throws UnknownHostExcepti\n     TProcessor processor =\n         ThriftProcessorTypes.getScanServerTProcessor(this, clientHandler, this, getContext());\n \n-    ServerAddress sp = TServerUtils.startServer(getContext(), getHostname(),\n+    ServerAddress sp = TServerUtils.createThriftServer(getContext(), getHostname(),\n         Property.SSERV_CLIENTPORT, processor, this.getClass().getSimpleName(),\n-        \""Thrift Client Server\"", Property.SSERV_PORTSEARCH, Property.SSERV_MINTHREADS,\n-        Property.SSERV_MINTHREADS_TIMEOUT, Property.SSERV_THREADCHECK);\n-\n+        Property.SSERV_PORTSEARCH, Property.SSERV_MINTHREADS, Property.SSERV_MINTHREADS_TIMEOUT,\n+        Property.SSERV_THREADCHECK);\n+    sp.startThriftServer(\""Thrift Client Server\"");\n     setHostname(sp.address);\n     LOG.info(\""address = {}\"", sp.address);\n     return sp;\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\nindex 5a71c6cad26..93a53b83d3a 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n@@ -413,10 +413,11 @@ AutoCloseable acquireRecoveryMemory(TabletMetadata tabletMetadata) {\n \n   private HostAndPort startServer(String address, TProcessor processor)\n       throws UnknownHostException {\n-    ServerAddress sp = TServerUtils.startServer(getContext(), address, Property.TSERV_CLIENTPORT,\n-        processor, this.getClass().getSimpleName(), \""Thrift Client Server\"",\n-        Property.TSERV_PORTSEARCH, Property.TSERV_MINTHREADS, Property.TSERV_MINTHREADS_TIMEOUT,\n-        Property.TSERV_THREADCHECK);\n+    ServerAddress sp =\n+        TServerUtils.createThriftServer(getContext(), address, Property.TSERV_CLIENTPORT, processor,\n+            this.getClass().getSimpleName(), Property.TSERV_PORTSEARCH, Property.TSERV_MINTHREADS,\n+            Property.TSERV_MINTHREADS_TIMEOUT, Property.TSERV_THREADCHECK);\n+    sp.startThriftServer(\""Thrift Client Server\"");\n     this.server = sp.server;\n     return sp.address;\n   }\n"", ""test_patch"": ""diff --git a/server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java b/server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java\nindex 6bf9218e600..c4eed579b30 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/rpc/TServerUtilsTest.java\n@@ -301,9 +301,10 @@ private ServerAddress startServer() throws Exception {\n     // misconfiguration)\n     String hostname = \""localhost\"";\n \n-    return TServerUtils.startServer(context, hostname, Property.TSERV_CLIENTPORT, processor,\n-        \""TServerUtilsTest\"", \""TServerUtilsTestThread\"", Property.TSERV_PORTSEARCH,\n-        Property.TSERV_MINTHREADS, Property.TSERV_MINTHREADS_TIMEOUT, Property.TSERV_THREADCHECK);\n-\n+    ServerAddress sa = TServerUtils.createThriftServer(context, hostname, Property.TSERV_CLIENTPORT,\n+        processor, \""TServerUtilsTest\"", Property.TSERV_PORTSEARCH, Property.TSERV_MINTHREADS,\n+        Property.TSERV_MINTHREADS_TIMEOUT, Property.TSERV_THREADCHECK);\n+    sa.startThriftServer(\""TServerUtilsTestThread\"");\n+    return sa;\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java b/test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java\nindex 2155fc93e03..71076c8764a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/ZombieTServer.java\n@@ -123,12 +123,12 @@ public static void main(String[] args) throws Exception {\n         ThriftProcessorTypes.TABLET_SCAN.getTProcessor(TabletScanClientService.Processor.class,\n             TabletScanClientService.Iface.class, tch, context));\n \n-    ServerAddress serverPort = TServerUtils.startTServer(context.getConfiguration(),\n-        ThriftServerType.CUSTOM_HS_HA, muxProcessor, \""ZombieTServer\"", \""walking dead\"", 2,\n+    ServerAddress serverPort = TServerUtils.createThriftServer(context.getConfiguration(),\n+        ThriftServerType.CUSTOM_HS_HA, muxProcessor, \""ZombieTServer\"", 2,\n         ThreadPools.DEFAULT_TIMEOUT_MILLISECS, 1000, 10 * 1024 * 1024, null, null, -1,\n         context.getConfiguration().getCount(Property.RPC_BACKLOG), context.getMetricsInfo(), false,\n         HostAndPort.fromParts(\""0.0.0.0\"", port));\n-\n+    serverPort.startThriftServer(\""walking dead\"");\n     String addressString = serverPort.address.toString();\n \n     var zLockPath = context.getServerPaths()\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java b/test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java\nindex 6335a8dd620..7a09fc5e856 100644\n--- a/test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java\n+++ b/test/src/main/java/org/apache/accumulo/test/performance/NullTserver.java\n@@ -79,6 +79,7 @@\n import org.apache.accumulo.server.client.ClientServiceHandler;\n import org.apache.accumulo.server.manager.state.Assignment;\n import org.apache.accumulo.server.manager.state.TabletStateStore;\n+import org.apache.accumulo.server.rpc.ServerAddress;\n import org.apache.accumulo.server.rpc.TServerUtils;\n import org.apache.accumulo.server.rpc.ThriftProcessorTypes;\n import org.apache.accumulo.server.rpc.ThriftServerType;\n@@ -313,10 +314,12 @@ public static void main(String[] args) throws Exception {\n             TabletManagementClientService.Processor.class,\n             TabletManagementClientService.Iface.class, tch, context));\n \n-    TServerUtils.startTServer(context.getConfiguration(), ThriftServerType.CUSTOM_HS_HA,\n-        muxProcessor, \""NullTServer\"", \""null tserver\"", 2, ThreadPools.DEFAULT_TIMEOUT_MILLISECS, 1000,\n-        10 * 1024 * 1024, null, null, -1, context.getConfiguration().getCount(Property.RPC_BACKLOG),\n-        context.getMetricsInfo(), false, HostAndPort.fromParts(\""0.0.0.0\"", opts.port));\n+    ServerAddress sa = TServerUtils.createThriftServer(context.getConfiguration(),\n+        ThriftServerType.CUSTOM_HS_HA, muxProcessor, \""NullTServer\"", 2,\n+        ThreadPools.DEFAULT_TIMEOUT_MILLISECS, 1000, 10 * 1024 * 1024, null, null, -1,\n+        context.getConfiguration().getCount(Property.RPC_BACKLOG), context.getMetricsInfo(), false,\n+        HostAndPort.fromParts(\""0.0.0.0\"", opts.port));\n+    sa.startThriftServer(\""null tserver\"");\n \n     AccumuloLockWatcher miniLockWatcher = new AccumuloLockWatcher() {\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5416"", ""pr_id"": 5416, ""issue_id"": 5010, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Replace in memory migrating set in manager with metadata table column\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThe Manager maintains a set of migrating tablets in memory and this in memory set tightly couples the tablet group watcher and balancer code.  When there is a problem it would be useful to see the contents of this set, logging provides perdiodic information on this but its cumbersome.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nRemove the in memory set and replace it with a column in the metadata table that is set in the source tablet and has a value of the destination tablet.  Hopefully this will avoid the need for the balancer code to pass this set to the tablet group watcher code which in turn passes the set to iterators for filtering.  The filtering iterators could see the column in the metadata table and would no longer need to pass this potentially large set around.\r\n\r\nThis change should also make it easy to see the contents of the set at any time via a metadata table scan.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nThe in memory set could be kept and one manager process could request it from another via RPC in order to  support multiple managers.  Placing the set in the metadata table has benefits for filtering and observe-ability that the RPC would not have.  Also the metadata approach may be simple to keep consistent. \r\n\r\n**Additional context**\r\n\r\nThis change is probably a pre-requisite change to multiple managers because it removes an in memory dependencies between different functional components in the manager and moves those from memory to the metadata table.  This makes it possible to run those different functional components in different processes."", ""issue_word_count"": 289, ""test_files_count"": 7, ""non_test_files_count"": 17, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java"", ""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletGoalState.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementIterator.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementParameters.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/CurrentLocationNotEqualToIterator.java"", ""server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/delete/CleanUp.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java"", ""server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java""], ""base_commit"": ""2f37ef5db46065394bb1466f85cf068bcde19cfe"", ""head_commit"": ""7bca40c01cae76c665b68ce8aa8faa174b808b80"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5416"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5416"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-30T16:43:35.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\nindex 9483ad623ac..544a44ead03 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\n@@ -401,6 +401,10 @@ interface TabletUpdates<T> {\n      * false.\n      */\n     T automaticallyPutServerLock(boolean b);\n+\n+    T putMigration(TServerInstance tserver);\n+\n+    T deleteMigration();\n   }\n \n   interface TabletMutator extends TabletUpdates<TabletMutator> {\n@@ -500,6 +504,8 @@ interface ConditionalTabletMutator extends TabletUpdates<ConditionalTabletMutato\n      */\n     ConditionalTabletMutator requireLocation(Location location);\n \n+    ConditionalTabletMutator requireCurrentLocationNotEqualTo(TServerInstance tsi);\n+\n     /**\n      * Requires the tablet to have the specified tablet availability before any changes are made.\n      */\n@@ -540,6 +546,11 @@ ConditionalTabletMutator requireSame(TabletMetadata tabletMetadata, ColumnType t\n      */\n     ConditionalTabletMutator requireCheckSuccess(TabletMetadataCheck check);\n \n+    /**\n+     * Requires that a tablet be migrating to the given tserver\n+     */\n+    ConditionalTabletMutator requireMigration(TServerInstance tserver);\n+\n     /**\n      * <p>\n      * Ample provides the following features on top of the conditional writer to help automate\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\nindex 6a003c22fd3..37fb46cf0e3 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\n@@ -309,6 +309,14 @@ public static void validateDirCol(String dirName) {\n        */\n       public static final String SELECTED_QUAL = \""selected\"";\n       public static final ColumnFQ SELECTED_COLUMN = new ColumnFQ(NAME, new Text(SELECTED_QUAL));\n+\n+      /**\n+       * This column is used to indicate that a tablet is in the process of being migrated from one\n+       * tablet server to another. The destination of the migration is the value. The tserver being\n+       * migrated is the row this is set on.\n+       */\n+      public static final String MIGRATION_QUAL = \""migration\"";\n+      public static final ColumnFQ MIGRATION_COLUMN = new ColumnFQ(NAME, new Text(MIGRATION_QUAL));\n     }\n \n     /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\nindex bb65372f582..8d0f84a60d8 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n@@ -25,6 +25,8 @@\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_NONCE_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_NONCE_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.MIGRATION_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.MIGRATION_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_COLUMN;\n@@ -142,6 +144,7 @@ public class TabletMetadata {\n   private final UnSplittableMetadata unSplittableMetadata;\n   private final TabletMergeabilityMetadata mergeability;\n   private final Supplier<Long> fileSize;\n+  private final TServerInstance migration;\n \n   private TabletMetadata(Builder tmBuilder) {\n     this.tableId = tmBuilder.tableId;\n@@ -185,6 +188,7 @@ private TabletMetadata(Builder tmBuilder) {\n     });\n     this.extent =\n         Suppliers.memoize(() -> new KeyExtent(getTableId(), getEndRow(), getPrevEndRow()));\n+    this.migration = tmBuilder.migration;\n   }\n \n   public static TabletMetadataBuilder builder(KeyExtent extent) {\n@@ -218,7 +222,8 @@ public enum ColumnType {\n     COMPACTED,\n     USER_COMPACTION_REQUESTED,\n     UNSPLITTABLE,\n-    MERGEABILITY;\n+    MERGEABILITY,\n+    MIGRATION;\n \n     public static final Map<ColumnType,Set<Text>> COLUMNS_TO_FAMILIES;\n     public static final Map<ColumnType,ColumnFQ> COLUMNS_TO_QUALIFIERS;\n@@ -238,6 +243,7 @@ public enum ColumnType {\n           case OPID:\n           case SELECTED:\n           case FLUSH_NONCE:\n+          case MIGRATION:\n             colsToFamilies.put(column, Set.of(ServerColumnFamily.NAME));\n             break;\n           case FILES:\n@@ -340,6 +346,9 @@ public enum ColumnType {\n           case UNSPLITTABLE:\n             colsToQualifiers.put(column, UNSPLITTABLE_COLUMN);\n             break;\n+          case MIGRATION:\n+            colsToQualifiers.put(column, MIGRATION_COLUMN);\n+            break;\n           default:\n             throw new IllegalArgumentException(\""Unknown col type \"" + column);\n         }\n@@ -608,6 +617,11 @@ public TabletMergeabilityMetadata getTabletMergeability() {\n     return mergeability;\n   }\n \n+  public TServerInstance getMigration() {\n+    ensureFetched(ColumnType.MIGRATION);\n+    return migration;\n+  }\n+\n   @Override\n   public String toString() {\n     return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE).append(\""tableId\"", tableId)\n@@ -623,7 +637,7 @@ public String toString() {\n         .append(\""futureAndCurrentLocationSet\"", futureAndCurrentLocationSet)\n         .append(\""userCompactionsRequested\"", userCompactionsRequested)\n         .append(\""unSplittableMetadata\"", unSplittableMetadata).append(\""mergeability\"", mergeability)\n-        .toString();\n+        .append(\""migration\"", migration).toString();\n   }\n \n   public List<Entry<Key,Value>> getKeyValues() {\n@@ -726,6 +740,8 @@ public static <E extends Entry<Key,Value>> TabletMetadata convertRow(Iterator<E>\n             case SELECTED_QUAL:\n               tmBuilder.selectedFiles(SelectedFiles.from(val));\n               break;\n+            case MIGRATION_QUAL:\n+              tmBuilder.migration(new TServerInstance(val));\n           }\n           break;\n         case DataFileColumnFamily.STR_NAME:\n@@ -863,6 +879,7 @@ static class Builder {\n     private final ImmutableSet.Builder<FateId> userCompactionsRequested = ImmutableSet.builder();\n     private UnSplittableMetadata unSplittableMetadata;\n     private TabletMergeabilityMetadata mergeability = TabletMergeabilityMetadata.never();\n+    private TServerInstance migration;\n \n     void table(TableId tableId) {\n       this.tableId = tableId;\n@@ -908,6 +925,10 @@ void selectedFiles(SelectedFiles selectedFiles) {\n       this.selectedFiles = selectedFiles;\n     }\n \n+    void migration(TServerInstance tserver) {\n+      this.migration = tserver;\n+    }\n+\n     void location(String val, String qual, LocationType lt, boolean suppressError) {\n       if (location != null) {\n         if (!suppressError) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\nindex 8ca33d9eb79..c40789d9a25 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\n@@ -32,6 +32,7 @@\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOGS;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGED;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MIGRATION;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.OPID;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.SCANS;\n@@ -326,6 +327,18 @@ public TabletMetadataBuilder automaticallyPutServerLock(boolean b) {\n     throw new UnsupportedOperationException();\n   }\n \n+  @Override\n+  public TabletMetadataBuilder putMigration(TServerInstance tserver) {\n+    fetched.add(MIGRATION);\n+    internalBuilder.putMigration(tserver);\n+    return this;\n+  }\n+\n+  @Override\n+  public TabletMetadataBuilder deleteMigration() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n   /**\n    * @param extraFetched Anything that was put on the builder will automatically be added to the\n    *        fetched set. However, for the case where something was not put and it needs to be\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\nindex 94acb32447f..eec27ec3c9d 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n@@ -395,6 +395,18 @@ public T putTabletMergeability(TabletMergeabilityMetadata tabletMergeability) {\n     return getThis();\n   }\n \n+  @Override\n+  public T putMigration(TServerInstance tserver) {\n+    ServerColumnFamily.MIGRATION_COLUMN.put(mutation, new Value(tserver.getHostPortSession()));\n+    return getThis();\n+  }\n+\n+  @Override\n+  public T deleteMigration() {\n+    ServerColumnFamily.MIGRATION_COLUMN.putDelete(mutation);\n+    return getThis();\n+  }\n+\n   public void setCloseAfterMutate(AutoCloseable closeable) {\n     this.closeAfterMutate = closeable;\n   }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\nindex c7669168522..7f8729f2309 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n@@ -39,6 +39,7 @@\n import org.apache.accumulo.core.metadata.StoredTabletFile;\n import org.apache.accumulo.core.metadata.SuspendingTServer;\n import org.apache.accumulo.core.metadata.SystemTables;\n+import org.apache.accumulo.core.metadata.TServerInstance;\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.BulkFileColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ChoppedColumnFamily;\n@@ -94,6 +95,7 @@ public class MetadataConstraints implements Constraint {\n           ServerColumnFamily.FLUSH_COLUMN,\n           ServerColumnFamily.FLUSH_NONCE_COLUMN,\n           ServerColumnFamily.OPID_COLUMN,\n+          ServerColumnFamily.MIGRATION_COLUMN,\n           TabletColumnFamily.AVAILABILITY_COLUMN,\n           TabletColumnFamily.REQUESTED_COLUMN,\n           ServerColumnFamily.SELECTED_COLUMN,\n@@ -301,6 +303,8 @@ public String getViolationDescription(short violationCode) {\n         return \""Malformed availability value\"";\n       case 4006:\n         return \""Malformed mergeability value\"";\n+      case 4007:\n+        return \""Malformed migration value\"";\n \n     }\n     return null;\n@@ -440,6 +444,12 @@ private void validateServerFamily(ArrayList<Short> violations, ColumnUpdate colu\n           addViolation(violations, 4001);\n         }\n         break;\n+      case ServerColumnFamily.MIGRATION_QUAL:\n+        try {\n+          new TServerInstance(new String(columnUpdate.getValue(), UTF_8));\n+        } catch (Exception e) {\n+          addViolation(violations, 4007);\n+        }\n     }\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java b/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\nindex 97fe2bf8394..fda21912c36 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\n@@ -221,7 +221,7 @@ public synchronized void startListeningForTabletServerChanges() {\n     scanServers();\n \n     ThreadPools.watchCriticalScheduledTask(this.context.getScheduledExecutor()\n-        .scheduleWithFixedDelay(this::scanServers, 0, 5000, TimeUnit.MILLISECONDS));\n+        .scheduleWithFixedDelay(this::scanServers, 5000, 5000, TimeUnit.MILLISECONDS));\n   }\n \n   public void tabletServerShuttingDown(String server) {\n@@ -250,7 +250,6 @@ public synchronized void scanServers() {\n         checkServer(updates, doomed, tserverPath);\n       }\n \n-      // log.debug(\""Current: \"" + current.keySet());\n       this.cback.update(this, doomed, updates);\n     } catch (Exception ex) {\n       log.error(\""{}\"", ex.getMessage(), ex);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletGoalState.java b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletGoalState.java\nindex 8c6452e21bb..92b5da49058 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletGoalState.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletGoalState.java\n@@ -21,7 +21,6 @@\n import java.util.function.Supplier;\n \n import org.apache.accumulo.core.data.TabletId;\n-import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.dataImpl.TabletIdImpl;\n import org.apache.accumulo.core.manager.balancer.TabletServerIdImpl;\n import org.apache.accumulo.core.metadata.TServerInstance;\n@@ -68,7 +67,6 @@ public static TabletGoalState compute(TabletMetadata tm, TabletState currentStat\n       return trace(HOSTED, tm, \""tablet is in assigned state\"");\n     }\n \n-    KeyExtent extent = tm.getExtent();\n     // Shutting down?\n     TabletGoalState systemGoalState = getSystemGoalState(tm, params);\n \n@@ -113,7 +111,7 @@ public static TabletGoalState compute(TabletMetadata tm, TabletState currentStat\n         }\n       }\n \n-      TServerInstance dest = params.getMigrations().get(extent);\n+      TServerInstance dest = tm.getMigration();\n       if (dest != null && tm.hasCurrent() && !dest.equals(tm.getLocation().getServerInstance())) {\n         return trace(UNASSIGNED, tm, () -> \""tablet has a migration to \"" + dest);\n       }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementIterator.java b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementIterator.java\nindex e5c7bd32b44..fa36ae197cb 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementIterator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementIterator.java\n@@ -126,11 +126,11 @@ private boolean shouldReturnDueToLocation(final TabletMetadata tm) {\n       return true;\n     }\n \n-    if (tabletMgmtParams.getMigrations().containsKey(tm.getExtent())) {\n+    if (tm.getMigration() != null) {\n       // Ideally only the state and goalState would need to be used to determine if a tablet should\n-      // be returned. However, the Manager/TGW currently needs everything in the migrating set\n-      // returned so it can update in memory maps it has. If this were improved then this case would\n-      // not be needed.\n+      // be returned. However, the Manager/TGW currently needs everything currently migrating\n+      // returned so it can update the migrations. If this were improved then this case would not\n+      // be needed.\n       return true;\n     }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementParameters.java b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementParameters.java\nindex 19ad33d6af6..8a219ad3141 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementParameters.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/state/TabletManagementParameters.java\n@@ -37,7 +37,6 @@\n \n import org.apache.accumulo.core.data.AbstractId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.manager.thrift.ManagerState;\n import org.apache.accumulo.core.metadata.TServerInstance;\n@@ -63,7 +62,6 @@ public class TabletManagementParameters {\n   private final Map<Ample.DataLevel,Boolean> parentUpgradeMap;\n   private final Set<TableId> onlineTables;\n   private final Set<TServerInstance> serversToShutdown;\n-  private final Map<KeyExtent,TServerInstance> migrations;\n \n   private final Ample.DataLevel level;\n \n@@ -78,9 +76,9 @@ public class TabletManagementParameters {\n   public TabletManagementParameters(ManagerState managerState,\n       Map<Ample.DataLevel,Boolean> parentUpgradeMap, Set<TableId> onlineTables,\n       LiveTServerSet.LiveTServersSnapshot liveTServersSnapshot,\n-      Set<TServerInstance> serversToShutdown, Map<KeyExtent,TServerInstance> migrations,\n-      Ample.DataLevel level, Map<FateId,Map<String,String>> compactionHints,\n-      boolean canSuspendTablets, Map<Path,Path> volumeReplacements, SteadyTime steadyTime) {\n+      Set<TServerInstance> serversToShutdown, Ample.DataLevel level,\n+      Map<FateId,Map<String,String>> compactionHints, boolean canSuspendTablets,\n+      Map<Path,Path> volumeReplacements, SteadyTime steadyTime) {\n     this.managerState = managerState;\n     this.parentUpgradeMap = Map.copyOf(parentUpgradeMap);\n     // TODO could filter by level\n@@ -88,8 +86,6 @@ public TabletManagementParameters(ManagerState managerState,\n     // This is already immutable, so no need to copy\n     this.onlineTservers = liveTServersSnapshot.getTservers();\n     this.serversToShutdown = Set.copyOf(serversToShutdown);\n-    // TODO could filter by level\n-    this.migrations = Map.copyOf(migrations);\n     this.level = level;\n     // This is already immutable, so no need to copy\n     this.tserverGroups = liveTServersSnapshot.getTserverGroups();\n@@ -113,9 +109,6 @@ private TabletManagementParameters(JsonData jdata) {\n         jdata.onlineTservers.stream().map(TServerInstance::new).collect(toUnmodifiableSet());\n     this.serversToShutdown =\n         jdata.serversToShutdown.stream().map(TServerInstance::new).collect(toUnmodifiableSet());\n-    this.migrations = jdata.migrations.entrySet().stream()\n-        .collect(toUnmodifiableMap(entry -> KeyExtent.fromBase64(entry.getKey()),\n-            entry -> new TServerInstance(entry.getValue())));\n     this.level = jdata.level;\n     this.compactionHints = makeImmutable(jdata.compactionHints.entrySet().stream()\n         .collect(Collectors.toMap(entry -> FateId.from(entry.getKey()), Map.Entry::getValue)));\n@@ -158,10 +151,6 @@ public boolean isTableOnline(TableId tableId) {\n     return onlineTables.contains(tableId);\n   }\n \n-  public Map<KeyExtent,TServerInstance> getMigrations() {\n-    return migrations;\n-  }\n-\n   public Ample.DataLevel getLevel() {\n     return level;\n   }\n@@ -208,7 +197,6 @@ private static class JsonData {\n     Collection<String> onlineTables;\n     Collection<String> onlineTservers;\n     Collection<String> serversToShutdown;\n-    Map<String,String> migrations;\n \n     Ample.DataLevel level;\n \n@@ -232,9 +220,6 @@ private JsonData() {}\n           .collect(toList());\n       serversToShutdown = params.serversToShutdown.stream().map(TServerInstance::getHostPortSession)\n           .collect(toList());\n-      migrations =\n-          params.migrations.entrySet().stream().collect(toMap(entry -> entry.getKey().toBase64(),\n-              entry -> entry.getValue().getHostPortSession()));\n       level = params.level;\n       tserverGroups = params.getGroupedTServers().entrySet().stream()\n           .collect(toMap(Map.Entry::getKey, entry -> entry.getValue().stream()\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\nindex 24aaedccfe7..50acebc727d 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\n@@ -21,6 +21,7 @@\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.MIGRATION_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_COLUMN;\n@@ -48,6 +49,7 @@\n import org.apache.accumulo.core.lock.ServiceLock;\n import org.apache.accumulo.core.metadata.ReferencedTabletFile;\n import org.apache.accumulo.core.metadata.StoredTabletFile;\n+import org.apache.accumulo.core.metadata.TServerInstance;\n import org.apache.accumulo.core.metadata.schema.Ample;\n import org.apache.accumulo.core.metadata.schema.Ample.ConditionalTabletMutator;\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n@@ -68,6 +70,7 @@\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.metadata.iterators.ColumnFamilySizeLimitIterator;\n+import org.apache.accumulo.server.metadata.iterators.CurrentLocationNotEqualToIterator;\n import org.apache.accumulo.server.metadata.iterators.PresentIterator;\n import org.apache.accumulo.server.metadata.iterators.SetEncodingIterator;\n import org.apache.accumulo.server.metadata.iterators.TabletExistsIterator;\n@@ -165,6 +168,14 @@ public Ample.ConditionalTabletMutator requireLocation(Location location) {\n     return this;\n   }\n \n+  @Override\n+  public Ample.ConditionalTabletMutator requireCurrentLocationNotEqualTo(TServerInstance tsi) {\n+    Preconditions.checkState(updatesEnabled, \""Cannot make updates after calling mutate.\"");\n+    Condition c = CurrentLocationNotEqualToIterator.createCondition(tsi);\n+    mutation.addCondition(c);\n+    return this;\n+  }\n+\n   @Override\n   public Ample.ConditionalTabletMutator\n       requireTabletAvailability(TabletAvailability tabletAvailability) {\n@@ -378,6 +389,16 @@ public ConditionalTabletMutator requireCheckSuccess(TabletMetadataCheck check) {\n     return this;\n   }\n \n+  @Override\n+  public ConditionalTabletMutator requireMigration(TServerInstance tserver) {\n+    Preconditions.checkState(updatesEnabled, \""Cannot make updates after calling mutate.\"");\n+    Condition condition =\n+        new Condition(MIGRATION_COLUMN.getColumnFamily(), MIGRATION_COLUMN.getColumnQualifier())\n+            .setValue(tserver.getHostPortSession());\n+    mutation.addCondition(condition);\n+    return this;\n+  }\n+\n   @Override\n   public void submit(Ample.RejectionHandler rejectionCheck) {\n     Preconditions.checkState(updatesEnabled, \""Cannot make updates after calling mutate.\"");\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/CurrentLocationNotEqualToIterator.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/CurrentLocationNotEqualToIterator.java\nnew file mode 100644\nindex 00000000000..3ade6465cec\n--- /dev/null\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/CurrentLocationNotEqualToIterator.java\n@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.server.metadata.iterators;\n+\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.CurrentLocationColumnFamily;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.accumulo.core.client.IteratorSetting;\n+import org.apache.accumulo.core.data.Condition;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.iterators.IteratorEnvironment;\n+import org.apache.accumulo.core.iterators.SortedKeyValueIterator;\n+import org.apache.accumulo.core.metadata.TServerInstance;\n+import org.apache.accumulo.server.metadata.ConditionalTabletMutatorImpl;\n+import org.apache.hadoop.io.Text;\n+\n+public class CurrentLocationNotEqualToIterator extends ColumnFamilyTransformationIterator {\n+  private static final String TSERVER_INSTANCE_OPTION = \""tsi_option\"";\n+  private static final String NOT_EQUAL = \""0\"";\n+  private static final String EQUAL = \""1\"";\n+  private TServerInstance tsi;\n+\n+  @Override\n+  public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> options,\n+      IteratorEnvironment env) throws IOException {\n+    super.init(source, options, env);\n+    tsi = new TServerInstance(options.get(TSERVER_INSTANCE_OPTION));\n+  }\n+\n+  @Override\n+  protected Value transform(SortedKeyValueIterator<Key,Value> source) throws IOException {\n+    TServerInstance tsiSeen;\n+    while (source.hasTop()) {\n+      Value address = source.getTopValue();\n+      Text session = source.getTopKey().getColumnQualifier();\n+      tsiSeen = new TServerInstance(address, session);\n+      if (tsiSeen.equals(tsi)) {\n+        return new Value(EQUAL);\n+      }\n+      source.next();\n+    }\n+\n+    return new Value(NOT_EQUAL);\n+  }\n+\n+  /**\n+   * Create a condition that fails if the {@link CurrentLocationColumnFamily} has an entry which is\n+   * equal to the given {@link TServerInstance}, passing otherwise\n+   */\n+  public static Condition createCondition(TServerInstance tsi) {\n+    IteratorSetting is = new IteratorSetting(ConditionalTabletMutatorImpl.INITIAL_ITERATOR_PRIO,\n+        CurrentLocationNotEqualToIterator.class);\n+    is.addOption(TSERVER_INSTANCE_OPTION, tsi.getHostPortSession());\n+    return new Condition(CurrentLocationColumnFamily.NAME, EMPTY).setValue(NOT_EQUAL)\n+        .setIterators(is);\n+  }\n+}\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex c23605724f4..7a18d3bb826 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -63,7 +63,6 @@\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n import org.apache.accumulo.core.client.admin.servers.ServerId.Type;\n-import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n import org.apache.accumulo.core.clientImpl.thrift.ThriftTableOperationException;\n@@ -197,8 +196,6 @@ public class Manager extends AbstractServer implements LiveTServerSet.Listener {\n   final Map<TServerInstance,AtomicInteger> badServers =\n       Collections.synchronizedMap(new HashMap<>());\n   final Set<TServerInstance> serversToShutdown = Collections.synchronizedSet(new HashSet<>());\n-  final SortedMap<KeyExtent,TServerInstance> migrations =\n-      Collections.synchronizedSortedMap(new TreeMap<>());\n   final EventCoordinator nextEvent = new EventCoordinator();\n   RecoveryManager recoveryManager = null;\n   private final ManagerTime timeKeeper;\n@@ -518,12 +515,6 @@ ManagerGoalState getManagerGoalState() {\n     }\n   }\n \n-  public void clearMigrations(TableId tableId) {\n-    synchronized (migrations) {\n-      migrations.keySet().removeIf(extent -> extent.tableId().equals(tableId));\n-    }\n-  }\n-\n   private Splitter splitter;\n \n   public Splitter getSplitter() {\n@@ -557,67 +548,46 @@ private class MigrationCleanupThread implements Runnable {\n     @Override\n     public void run() {\n       while (stillManager()) {\n-        if (!migrations.isEmpty()) {\n-          try {\n-            cleanupOfflineMigrations();\n-            cleanupNonexistentMigrations(getContext());\n-          } catch (Exception ex) {\n-            log.error(\""Error cleaning up migrations\"", ex);\n+        try {\n+          // - Remove any migrations for tablets of offline tables, as the migration can never\n+          // succeed because no tablet server will load the tablet\n+          // - Remove any migrations to tablet servers that are not live\n+          // - Remove any migrations where the tablets current location equals the migration\n+          // (the migration has completed)\n+          var ample = getContext().getAmple();\n+          for (DataLevel dl : DataLevel.values()) {\n+            // prev row needed for the extent\n+            try (\n+                var tabletsMetadata =\n+                    ample.readTablets().forLevel(dl)\n+                        .fetch(TabletMetadata.ColumnType.PREV_ROW,\n+                            TabletMetadata.ColumnType.MIGRATION)\n+                        .build();\n+                var tabletsMutator = ample.conditionallyMutateTablets(result -> {})) {\n+              for (var tabletMetadata : tabletsMetadata) {\n+                var migration = tabletMetadata.getMigration();\n+                if (migration != null && shouldCleanupMigration(tabletMetadata)) {\n+                  tabletsMutator.mutateTablet(tabletMetadata.getExtent()).requireAbsentOperation()\n+                      .requireMigration(migration).deleteMigration().submit(tm -> false);\n+                }\n+              }\n+            }\n           }\n+        } catch (Exception ex) {\n+          log.error(\""Error cleaning up migrations\"", ex);\n         }\n         sleepUninterruptibly(CLEANUP_INTERVAL_MINUTES, MINUTES);\n       }\n     }\n+  }\n \n-    /**\n-     * If a migrating tablet splits, and the tablet dies before sending the manager a message, the\n-     * migration will refer to a non-existing tablet, so it can never complete. Periodically scan\n-     * the metadata table and remove any migrating tablets that no longer exist.\n-     */\n-    private void cleanupNonexistentMigrations(final ClientContext clientContext) {\n-\n-      Map<DataLevel,Set<KeyExtent>> notSeen;\n-\n-      synchronized (migrations) {\n-        notSeen = partitionMigrations(migrations.keySet());\n-      }\n-\n-      // for each level find the set of migrating tablets that do not exists in metadata store\n-      for (DataLevel dataLevel : DataLevel.values()) {\n-        var notSeenForLevel = notSeen.getOrDefault(dataLevel, Set.of());\n-        if (notSeenForLevel.isEmpty() || dataLevel == DataLevel.ROOT) {\n-          // No need to scan this level if there are no migrations. The root tablet is always\n-          // expected to exists, so no need to read its metadata.\n-          continue;\n-        }\n-\n-        try (var tablets = clientContext.getAmple().readTablets().forLevel(dataLevel)\n-            .fetch(TabletMetadata.ColumnType.PREV_ROW).build()) {\n-          // A goal of this code is to avoid reading all extents in the metadata table into memory\n-          // when finding extents that exists in the migrating set and not in the metadata table.\n-          tablets.forEach(tabletMeta -> notSeenForLevel.remove(tabletMeta.getExtent()));\n-        }\n-\n-        // remove any tablets that previously existed in migrations for this level but were not seen\n-        // in the metadata table for the level\n-        migrations.keySet().removeAll(notSeenForLevel);\n-      }\n-    }\n-\n-    /**\n-     * If migrating a tablet for a table that is offline, the migration can never succeed because no\n-     * tablet server will load the tablet. check for offline tables and remove their migrations.\n-     */\n-    private void cleanupOfflineMigrations() {\n-      ServerContext context = getContext();\n-      TableManager manager = context.getTableManager();\n-      for (TableId tableId : context.createTableIdToQualifiedNameMap().keySet()) {\n-        TableState state = manager.getTableState(tableId);\n-        if (state == TableState.OFFLINE) {\n-          clearMigrations(tableId);\n-        }\n-      }\n-    }\n+  private boolean shouldCleanupMigration(TabletMetadata tabletMetadata) {\n+    var tableState = getContext().getTableManager().getTableState(tabletMetadata.getTableId());\n+    var migration = tabletMetadata.getMigration();\n+    Preconditions.checkState(migration != null,\n+        \""This method should only be called if there is a migration\"");\n+    return tableState == TableState.OFFLINE || !onlineTabletServers().contains(migration)\n+        || tabletMetadata.getLocation().getServerInstance().equals(migration);\n   }\n \n   private class ScanServerZKCleaner implements Runnable {\n@@ -666,16 +636,20 @@ public void run() {\n    * balanceTablets() balances tables by DataLevel. Return the current set of migrations partitioned\n    * by DataLevel\n    */\n-  private static Map<DataLevel,Set<KeyExtent>>\n-      partitionMigrations(final Set<KeyExtent> migrations) {\n+  private Map<DataLevel,Set<KeyExtent>> partitionMigrations() {\n     final Map<DataLevel,Set<KeyExtent>> partitionedMigrations = new EnumMap<>(DataLevel.class);\n-    // populate to prevent NPE\n     for (DataLevel dl : DataLevel.values()) {\n-      partitionedMigrations.put(dl, new HashSet<>());\n+      Set<KeyExtent> extents = new HashSet<>();\n+      // prev row needed for the extent\n+      try (var tabletsMetadata = getContext().getAmple().readTablets().forLevel(dl)\n+          .fetch(TabletMetadata.ColumnType.PREV_ROW, TabletMetadata.ColumnType.MIGRATION).build()) {\n+        // filter out migrations that are awaiting cleanup\n+        tabletsMetadata.stream()\n+            .filter(tm -> tm.getMigration() != null && !shouldCleanupMigration(tm))\n+            .forEach(tm -> extents.add(tm.getExtent()));\n+      }\n+      partitionedMigrations.put(dl, extents);\n     }\n-    migrations.forEach(ke -> {\n-      partitionedMigrations.get(DataLevel.of(ke.tableId())).add(ke);\n-    });\n     return partitionedMigrations;\n   }\n \n@@ -925,8 +899,7 @@ private long balanceTablets() {\n       BalanceParamsImpl params = null;\n       long wait = 0;\n       long totalMigrationsOut = 0;\n-      final Map<DataLevel,Set<KeyExtent>> partitionedMigrations =\n-          partitionMigrations(migrationsSnapshot().keySet());\n+      final Map<DataLevel,Set<KeyExtent>> partitionedMigrations = partitionMigrations();\n       int levelsCompleted = 0;\n \n       for (DataLevel dl : DataLevel.values()) {\n@@ -965,31 +938,38 @@ private long balanceTablets() {\n             tserverStatusForLevel, partitionedMigrations.get(dl), dl, getTablesForLevel(dl));\n         wait = Math.max(tabletBalancer.balance(params), wait);\n         long migrationsOutForLevel = 0;\n-        for (TabletMigration m : checkMigrationSanity(statusForBalancerLevel.keySet(),\n-            params.migrationsOut(), dl)) {\n-          final KeyExtent ke = KeyExtent.fromTabletId(m.getTablet());\n-          if (partitionedMigrations.get(dl).contains(ke)) {\n-            log.warn(\""balancer requested migration more than once, skipping {}\"", m);\n-            continue;\n+        try (\n+            var tabletsMutator = getContext().getAmple().conditionallyMutateTablets(result -> {})) {\n+          for (TabletMigration m : checkMigrationSanity(statusForBalancerLevel.keySet(),\n+              params.migrationsOut(), dl)) {\n+            final KeyExtent ke = KeyExtent.fromTabletId(m.getTablet());\n+            if (partitionedMigrations.get(dl).contains(ke)) {\n+              log.warn(\""balancer requested migration more than once, skipping {}\"", m);\n+              continue;\n+            }\n+            migrationsOutForLevel++;\n+            var migration = TabletServerIdImpl.toThrift(m.getNewTabletServer());\n+            tabletsMutator.mutateTablet(ke).requireAbsentOperation()\n+                .requireCurrentLocationNotEqualTo(migration).putMigration(migration)\n+                .submit(tm -> false);\n+            log.debug(\""migration {}\"", m);\n           }\n-          migrationsOutForLevel++;\n-          migrations.put(ke, TabletServerIdImpl.toThrift(m.getNewTabletServer()));\n-          log.debug(\""migration {}\"", m);\n         }\n         totalMigrationsOut += migrationsOutForLevel;\n \n         // increment this at end of loop to signal complete run w/o any continue\n         levelsCompleted++;\n       }\n-      balancerMetrics.assignMigratingCount(migrations::size);\n+      final long totalMigrations =\n+          totalMigrationsOut + partitionedMigrations.values().stream().mapToLong(Set::size).sum();\n+      balancerMetrics.assignMigratingCount(() -> totalMigrations);\n \n       if (totalMigrationsOut == 0 && levelsCompleted == DataLevel.values().length) {\n         synchronized (balancedNotifier) {\n           balancedNotifier.notifyAll();\n         }\n       } else if (totalMigrationsOut > 0) {\n-        nextEvent.event(\""Migrating %d more tablets, %d total\"", totalMigrationsOut,\n-            migrations.size());\n+        nextEvent.event(\""Migrating %d more tablets, %d total\"", totalMigrationsOut, totalMigrations);\n       }\n       return wait;\n     }\n@@ -1172,9 +1152,6 @@ public void run() {\n       TableState state = getTableManager().getTableState(tableId);\n       log.debug(\""Table state transition to {} @ {}\"", state, event);\n       nextEvent.event(tableId, \""Table state in zookeeper changed for %s to %s\"", tableId, state);\n-      if (state == TableState.OFFLINE) {\n-        clearMigrations(tableId);\n-      }\n     }));\n \n     tableInformationStatusPool = ThreadPools.getServerThreadPools()\n@@ -1670,17 +1647,6 @@ public void update(LiveTServerSet current, Set<TServerInstance> deleted,\n       synchronized (serversToShutdown) {\n         cleanListByHostAndPort(serversToShutdown, deleted, added);\n       }\n-\n-      synchronized (migrations) {\n-        Iterator<Entry<KeyExtent,TServerInstance>> iter = migrations.entrySet().iterator();\n-        while (iter.hasNext()) {\n-          Entry<KeyExtent,TServerInstance> entry = iter.next();\n-          if (deleted.contains(entry.getValue())) {\n-            log.info(\""Canceling migration of {} to {}\"", entry.getKey(), entry.getValue());\n-            iter.remove();\n-          }\n-        }\n-      }\n       nextEvent.event(\""There are now %d tablet servers\"", current.size());\n     }\n \n@@ -1776,7 +1742,7 @@ public void waitForBalance() {\n         } catch (InterruptedException e) {\n           log.debug(e.toString(), e);\n         }\n-      } while (displayUnassigned() > 0 || !migrations.isEmpty()\n+      } while (displayUnassigned() > 0 || numMigrations() > 0\n           || eventCounter != nextEvent.waitForEvents(0, 0));\n     }\n   }\n@@ -1822,12 +1788,6 @@ public boolean delegationTokensAvailable() {\n     return delegationTokensAvailable;\n   }\n \n-  public Map<KeyExtent,TServerInstance> migrationsSnapshot() {\n-    synchronized (migrations) {\n-      return Map.copyOf(migrations);\n-    }\n-  }\n-\n   public Set<TServerInstance> shutdownServers() {\n     synchronized (serversToShutdown) {\n       return Set.copyOf(serversToShutdown);\n@@ -1909,4 +1869,17 @@ private Map<FateInstanceType,Fate<Manager>> getFateRefs() {\n   public ServiceLock getLock() {\n     return managerLock;\n   }\n+\n+  private long numMigrations() {\n+    long count = 0;\n+    for (DataLevel dl : DataLevel.values()) {\n+      // prev row needed for the extent\n+      try (var tabletsMetadata = getContext().getAmple().readTablets().forLevel(dl)\n+          .fetch(TabletMetadata.ColumnType.PREV_ROW, TabletMetadata.ColumnType.MIGRATION).build()) {\n+        count += tabletsMetadata.stream()\n+            .filter(tabletMetadata -> tabletMetadata.getMigration() != null).count();\n+      }\n+    }\n+    return count;\n+  }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java b/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\nindex fe39e603818..46af91364f3 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\n@@ -22,6 +22,7 @@\n import static java.lang.Math.min;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOGS;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MIGRATION;\n \n import java.io.IOException;\n import java.time.Duration;\n@@ -59,7 +60,6 @@\n import org.apache.accumulo.core.logging.TabletLogger;\n import org.apache.accumulo.core.manager.state.TabletManagement;\n import org.apache.accumulo.core.manager.state.TabletManagement.ManagementAction;\n-import org.apache.accumulo.core.manager.state.tables.TableState;\n import org.apache.accumulo.core.manager.thrift.ManagerGoalState;\n import org.apache.accumulo.core.manager.thrift.ManagerState;\n import org.apache.accumulo.core.manager.thrift.TabletServerStatus;\n@@ -397,13 +397,12 @@ public void hostOndemand(Collection<KeyExtent> extents) {\n \n     var tServersSnapshot = manager.tserversSnapshot();\n \n-    var tabletMgmtParams =\n-        new TabletManagementParameters(manager.getManagerState(), parentLevelUpgrade,\n-            manager.onlineTables(), tServersSnapshot, shutdownServers, manager.migrationsSnapshot(),\n-            store.getLevel(), manager.getCompactionHints(store.getLevel()), canSuspendTablets(),\n-            lookForTabletsNeedingVolReplacement ? manager.getContext().getVolumeReplacements()\n-                : Map.of(),\n-            manager.getSteadyTime());\n+    var tabletMgmtParams = new TabletManagementParameters(manager.getManagerState(),\n+        parentLevelUpgrade, manager.onlineTables(), tServersSnapshot, shutdownServers,\n+        store.getLevel(), manager.getCompactionHints(store.getLevel()), canSuspendTablets(),\n+        lookForTabletsNeedingVolReplacement ? manager.getContext().getVolumeReplacements()\n+            : Map.of(),\n+        manager.getSteadyTime());\n \n     if (LOG.isTraceEnabled()) {\n       // Log the json that will be passed to iterators to make tablet filtering decisions.\n@@ -642,11 +641,6 @@ private TableMgmtStats manageTablets(Iterator<TabletManagement> iter,\n             continue;\n           }\n           switch (state) {\n-            case HOSTED:\n-              if (location.getServerInstance().equals(manager.migrations.get(tm.getExtent()))) {\n-                manager.migrations.remove(tm.getExtent());\n-              }\n-              break;\n             case ASSIGNED_TO_DEAD_SERVER:\n               hostDeadTablet(tLists, tm, location);\n               break;\n@@ -662,7 +656,7 @@ private TableMgmtStats manageTablets(Iterator<TabletManagement> iter,\n               tLists.assigned.add(new Assignment(tm.getExtent(),\n                   future != null ? future.getServerInstance() : null, tm.getLast()));\n               break;\n-            default:\n+            case HOSTED:\n               break;\n           }\n         } else {\n@@ -670,10 +664,6 @@ private TableMgmtStats manageTablets(Iterator<TabletManagement> iter,\n             case SUSPENDED:\n               // Request a move to UNASSIGNED, so as to allow balancing to continue.\n               tLists.suspendedToGoneServers.add(tm);\n-              cancelOfflineTableMigrations(tm.getExtent());\n-              break;\n-            case UNASSIGNED:\n-              cancelOfflineTableMigrations(tm.getExtent());\n               break;\n             case ASSIGNED_TO_DEAD_SERVER:\n               unassignDeadTablet(tLists, tm);\n@@ -693,6 +683,7 @@ private TableMgmtStats manageTablets(Iterator<TabletManagement> iter,\n               }\n               break;\n             case ASSIGNED:\n+            case UNASSIGNED:\n               break;\n           }\n         }\n@@ -834,14 +825,13 @@ private void unassignDeadTablet(TabletLists tLists, TabletMetadata tm) throws Wa\n   private void hostUnassignedTablet(TabletLists tLists, KeyExtent tablet,\n       UnassignedTablet unassignedTablet) {\n     // maybe it's a finishing migration\n-    TServerInstance dest = manager.migrations.get(tablet);\n+    TServerInstance dest =\n+        manager.getContext().getAmple().readTablet(tablet, MIGRATION).getMigration();\n     if (dest != null) {\n       // if destination is still good, assign it\n       if (tLists.destinations.containsKey(dest)) {\n         tLists.assignments.add(new Assignment(tablet, dest, unassignedTablet.getLastLocation()));\n       } else {\n-        // get rid of this migration\n-        manager.migrations.remove(tablet);\n         tLists.unassigned.put(tablet, unassignedTablet);\n       }\n     } else {\n@@ -879,23 +869,12 @@ private void hostSuspendedTablet(TabletLists tLists, TabletMetadata tm, Location\n   private void hostDeadTablet(TabletLists tLists, TabletMetadata tm, Location location)\n       throws WalMarkerException {\n     tLists.assignedToDeadServers.add(tm);\n-    if (location.getServerInstance().equals(manager.migrations.get(tm.getExtent()))) {\n-      manager.migrations.remove(tm.getExtent());\n-    }\n     TServerInstance tserver = tm.getLocation().getServerInstance();\n     if (!tLists.logsForDeadServers.containsKey(tserver)) {\n       tLists.logsForDeadServers.put(tserver, walStateManager.getWalsInUse(tserver));\n     }\n   }\n \n-  private void cancelOfflineTableMigrations(KeyExtent extent) {\n-    TServerInstance dest = manager.migrations.get(extent);\n-    TableState tableState = manager.getTableManager().getTableState(extent.tableId());\n-    if (dest != null && tableState == TableState.OFFLINE) {\n-      manager.migrations.remove(extent);\n-    }\n-  }\n-\n   /**\n    * Read tablet metadata entries for tablet that have multiple locations. Not using Ample because\n    * it throws an exception when tablets have multiple locations.\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/delete/CleanUp.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/delete/CleanUp.java\nindex 2e6569a190c..5d7622be880 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/delete/CleanUp.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/delete/CleanUp.java\n@@ -65,9 +65,6 @@ public CleanUp(TableId tableId, NamespaceId namespaceId) {\n \n   @Override\n   public Repo<Manager> call(FateId fateId, Manager manager) {\n-\n-    manager.clearMigrations(tableId);\n-\n     int refCount = 0;\n \n     try {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\nindex 5b490ab567e..f444e6baae8 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\n@@ -213,6 +213,11 @@ public Repo<Manager> call(FateId fateId, Manager manager) throws Exception {\n           tabletMutator.deleteUnSplittable();\n         }\n \n+        if (lastTabletMeta.getMigration() != null) {\n+          // This is no longer the same tablet, so delete the migration\n+          tabletMutator.deleteMigration();\n+        }\n+\n         // Set merged marker on the last tablet when we are finished\n         // so we know that we already updated metadata if the process restarts\n         tabletMutator.setMerged();\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\nindex 5a6e835119a..85b57e6b8ce 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\n@@ -314,6 +314,14 @@ private void updateExistingTablet(FateId fateId, Manager manager, TabletMetadata\n         log.debug(\""{} deleting unsplittable metadata from {} because of split\"", fateId, newExtent);\n       }\n \n+      var migration = tabletMetadata.getMigration();\n+      if (migration != null) {\n+        // This is no longer the same tablet, so delete the migration\n+        mutator.deleteMigration();\n+        log.debug(\""{} deleting migration {} metadata from {} because of split\"", fateId, migration,\n+            newExtent);\n+      }\n+\n       // if the tablet no longer exists (because changed prev end row, then the update was\n       // successful.\n       mutator.submit(Ample.RejectionHandler.acceptAbsentTablet());\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\nindex 1381f44f334..c974493f6d2 100644\n--- a/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\n@@ -25,6 +25,7 @@\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.DIRECTORY_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_NONCE_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.MIGRATION_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SuspendLocationColumn.SUSPEND_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.AVAILABILITY;\n@@ -166,6 +167,9 @@ public void testAllColumns() {\n             CompactorGroupId.of(\""Q1\""), true, FateId.from(FateInstanceType.USER, UUID.randomUUID()));\n     mutation.put(ExternalCompactionColumnFamily.STR_NAME, ecid.canonical(), ecMeta.toJson());\n \n+    TServerInstance tsi = new TServerInstance(\""localhost:9997\"", 5000L);\n+    MIGRATION_COLUMN.put(mutation, new Value(tsi.getHostPortSession()));\n+\n     SortedMap<Key,Value> rowMap = toRowMap(mutation);\n \n     TabletMetadata tm = TabletMetadata.convertRow(rowMap.entrySet().iterator(),\n@@ -203,6 +207,7 @@ public void testAllColumns() {\n     assertEquals(unsplittableMeta, tm.getUnSplittable());\n     assertEquals(ecMeta.toJson(), tm.getExternalCompactions().get(ecid).toJson());\n     assertEquals(10, tm.getFlushNonce().getAsLong());\n+    assertEquals(tsi, tm.getMigration());\n   }\n \n   @Test\n@@ -639,6 +644,8 @@ public void testBuilder() {\n     FateId compactFateId1 = FateId.from(type, UUID.randomUUID());\n     FateId compactFateId2 = FateId.from(type, UUID.randomUUID());\n \n+    TServerInstance migration = new TServerInstance(\""localhost:9999\"", 1000L);\n+\n     TabletMetadata tm = TabletMetadata.builder(extent)\n         .putTabletAvailability(TabletAvailability.UNHOSTED).putLocation(Location.future(ser1))\n         .putFile(sf1, dfv1).putFile(sf2, dfv2).putBulkFile(rf1, loadedFateId1)\n@@ -646,6 +653,7 @@ public void testBuilder() {\n         .putCompacted(compactFateId1).putCompacted(compactFateId2).putCloned()\n         .putTabletMergeability(\n             TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS)))\n+        .putMigration(migration)\n         .build(ECOMP, HOSTING_REQUESTED, MERGED, USER_COMPACTION_REQUESTED, UNSPLITTABLE);\n \n     assertEquals(extent, tm.getExtent());\n@@ -667,6 +675,7 @@ public void testBuilder() {\n     assertEquals(\""OK\"", tm.getCloned());\n     assertEquals(TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS)),\n         tm.getTabletMergeability());\n+    assertEquals(migration, tm.getMigration());\n     assertThrows(IllegalStateException.class, tm::getOperationId);\n     assertThrows(IllegalStateException.class, tm::getSuspend);\n     assertThrows(IllegalStateException.class, tm::getTime);\n@@ -694,6 +703,7 @@ public void testBuilder() {\n     assertThrows(IllegalStateException.class, tm2::getUserCompactionsRequested);\n     assertThrows(IllegalStateException.class, tm2::getUnSplittable);\n     assertThrows(IllegalStateException.class, tm2::getTabletAvailability);\n+    assertThrows(IllegalStateException.class, tm2::getMigration);\n \n     var ecid1 = ExternalCompactionId.generate(UUID.randomUUID());\n     CompactionMetadata ecm =\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java b/server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java\nindex 56f80eeae9c..775331960a3 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/state/AbstractTabletStateStore.java\n@@ -87,7 +87,7 @@ public Set<KeyExtent> setFutureLocations(Collection<Assignment> assignments)\n       for (Assignment assignment : assignments) {\n         tabletsMutator.mutateTablet(assignment.tablet).requireAbsentOperation()\n             .requireAbsentLocation().deleteSuspension()\n-            .putLocation(TabletMetadata.Location.future(assignment.server))\n+            .putLocation(TabletMetadata.Location.future(assignment.server)).deleteMigration()\n             .submit(tabletMetadata -> {\n               Preconditions.checkArgument(tabletMetadata.getExtent().equals(assignment.tablet));\n               return tabletMetadata.getLocation() != null && tabletMetadata.getLocation()\n@@ -160,6 +160,12 @@ private void unassign(Collection<TabletMetadata> tablets,\n           tabletMutator.deleteLocation(tm.getLocation());\n         }\n \n+        if (tm.getMigration() != null) {\n+          // Since the tablet server died, any migration plans may no longer be valid, so remove\n+          // any migrations that may exist\n+          tabletMutator.deleteMigration();\n+        }\n+\n         processSuspension(tabletMutator, tm, suspensionTimestamp);\n \n         tabletMutator.submit(tabletMetadata -> tabletMetadata.getLocation() == null);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java b/server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java\nindex fc9fe4e9d5f..71b7e94fccb 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/manager/state/TabletManagementParametersTest.java\n@@ -26,7 +26,6 @@\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.manager.thrift.ManagerState;\n import org.apache.accumulo.core.metadata.TServerInstance;\n@@ -51,7 +50,6 @@ public void testDeSer() {\n         new LiveTServerSet.LiveTServersSnapshot(tservers,\n             Map.of(Constants.DEFAULT_RESOURCE_GROUP_NAME, tservers));\n     final Set<TServerInstance> serversToShutdown = Set.of();\n-    final Map<KeyExtent,TServerInstance> migrations = Map.of();\n     final Ample.DataLevel dataLevel = Ample.DataLevel.USER;\n     final Map<FateId,Map<String,String>> compactionHints = Map.of();\n     final boolean canSuspendTablets = true;\n@@ -60,7 +58,7 @@ public void testDeSer() {\n     final SteadyTime steadyTime = SteadyTime.from(100_000, TimeUnit.NANOSECONDS);\n \n     final TabletManagementParameters tmp = new TabletManagementParameters(managerState,\n-        parentUpgradeMap, onlineTables, serverSnapshot, serversToShutdown, migrations, dataLevel,\n+        parentUpgradeMap, onlineTables, serverSnapshot, serversToShutdown, dataLevel,\n         compactionHints, canSuspendTablets, replacements, steadyTime);\n \n     String jsonString = tmp.serialize();\n@@ -71,7 +69,6 @@ public void testDeSer() {\n     assertEquals(onlineTables, tmp2.getOnlineTables());\n     assertEquals(tservers, tmp2.getOnlineTsevers());\n     assertEquals(serversToShutdown, tmp2.getServersToShutdown());\n-    assertEquals(migrations, tmp2.getMigrations());\n     assertEquals(dataLevel, tmp2.getLevel());\n     assertEquals(compactionHints, tmp2.getCompactionHints());\n     assertEquals(canSuspendTablets, tmp2.canSuspendTablets());\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\nindex 08d951e21f7..9e3037dbe01 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\n@@ -34,6 +34,7 @@\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOGS;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGED;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MIGRATION;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.OPID;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.SCANS;\n@@ -109,10 +110,10 @@ public class MergeTabletsTest {\n    * implications for merging tablets. For a column to be in this set it means an Accumulo developer\n    * has determined that merge code can handle that column OR has opened an issue about handling it.\n    */\n-  private static final Set<TabletMetadata.ColumnType> COLUMNS_HANDLED_BY_MERGE =\n-      EnumSet.of(TIME, LOGS, FILES, PREV_ROW, OPID, LOCATION, ECOMP, SELECTED, LOADED,\n-          USER_COMPACTION_REQUESTED, MERGED, LAST, SCANS, DIR, CLONED, FLUSH_ID, FLUSH_NONCE,\n-          SUSPEND, AVAILABILITY, HOSTING_REQUESTED, COMPACTED, UNSPLITTABLE, MERGEABILITY);\n+  private static final Set<TabletMetadata.ColumnType> COLUMNS_HANDLED_BY_MERGE = EnumSet.of(TIME,\n+      LOGS, FILES, PREV_ROW, OPID, LOCATION, ECOMP, SELECTED, LOADED, USER_COMPACTION_REQUESTED,\n+      MERGED, LAST, SCANS, DIR, CLONED, FLUSH_ID, FLUSH_NONCE, SUSPEND, AVAILABILITY,\n+      HOSTING_REQUESTED, COMPACTED, UNSPLITTABLE, MERGEABILITY, MIGRATION);\n \n   /**\n    * The purpose of this test is to catch new tablet metadata columns that were added w/o\n@@ -153,6 +154,7 @@ public void testManyColumns() throws Exception {\n     var lastLocation = TabletMetadata.Location.last(\""1.2.3.4:1234\"", \""123456789\"");\n     var suspendingTServer = SuspendingTServer.fromValue(new Value(\""1.2.3.4:5|56\""));\n     var mergeability = TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS));\n+    var migration = new TServerInstance(\""localhost:1234\"", 56L);\n \n     var tablet1 =\n         TabletMetadata.builder(ke1).putOperation(opid).putDirName(\""td1\"").putFile(file3, dfv3)\n@@ -199,6 +201,7 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(lastTabletMeta.getLast()).andReturn(lastLocation).atLeastOnce();\n     EasyMock.expect(lastTabletMeta.getUnSplittable()).andReturn(unsplittableMeta).atLeastOnce();\n     EasyMock.expect(lastTabletMeta.getTabletMergeability()).andReturn(mergeability).atLeastOnce();\n+    EasyMock.expect(lastTabletMeta.getMigration()).andReturn(migration).atLeastOnce();\n \n     EasyMock.replay(lastTabletMeta, compactions);\n \n@@ -238,6 +241,7 @@ public void testManyColumns() throws Exception {\n           .expect(tabletMutator.putTabletMergeability(\n               TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS))))\n           .andReturn(tabletMutator).once();\n+      EasyMock.expect(tabletMutator.deleteMigration()).andReturn(tabletMutator);\n \n     });\n \n@@ -386,17 +390,17 @@ public void testTime() throws Exception {\n           .putTime(MetadataTime.parse(times[0])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE, MERGEABILITY);\n+              UNSPLITTABLE, MERGEABILITY, MIGRATION);\n       var tablet2 = TabletMetadata.builder(ke2).putOperation(opid).putDirName(\""td2\"")\n           .putTime(MetadataTime.parse(times[1])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE, MERGEABILITY);\n+              UNSPLITTABLE, MERGEABILITY, MIGRATION);\n       var tablet3 = TabletMetadata.builder(ke3).putOperation(opid).putDirName(\""td3\"")\n           .putTime(MetadataTime.parse(times[2])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE, MERGEABILITY);\n+              UNSPLITTABLE, MERGEABILITY, MIGRATION);\n \n       testMerge(List.of(tablet1, tablet2, tablet3), tableId, null, null, tabletMutator -> {\n         EasyMock.expect(tabletMutator.putTime(MetadataTime.parse(\""L30\""))).andReturn(tabletMutator)\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\nindex 66f8665fbe6..b4310a5eb58 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\n@@ -98,7 +98,7 @@ Splitter.FileInfo newFileInfo(String start, String end) {\n       ColumnType.USER_COMPACTION_REQUESTED, ColumnType.MERGED, ColumnType.LAST, ColumnType.SCANS,\n       ColumnType.DIR, ColumnType.CLONED, ColumnType.FLUSH_ID, ColumnType.FLUSH_NONCE,\n       ColumnType.SUSPEND, ColumnType.AVAILABILITY, ColumnType.HOSTING_REQUESTED,\n-      ColumnType.COMPACTED, ColumnType.UNSPLITTABLE, ColumnType.MERGEABILITY);\n+      ColumnType.COMPACTED, ColumnType.UNSPLITTABLE, ColumnType.MERGEABILITY, ColumnType.MIGRATION);\n \n   /**\n    * The purpose of this test is to catch new tablet metadata columns that were added w/o\n@@ -225,6 +225,7 @@ public void testManyColumns() throws Exception {\n     var availability = TabletAvailability.HOSTED;\n     var lastLocation = TabletMetadata.Location.last(\""1.2.3.4:1234\"", \""123456789\"");\n     var suspendingTServer = SuspendingTServer.fromValue(new Value(\""1.2.3.4:5|56\""));\n+    var migration = new TServerInstance(\""localhost:1234\"", 56L);\n \n     String dir1 = \""dir1\"";\n     String dir2 = \""dir2\"";\n@@ -280,6 +281,7 @@ public void testManyColumns() throws Exception {\n     UnSplittableMetadata usm =\n         UnSplittableMetadata.toUnSplittable(origExtent, 1000, 1001, 1002, tabletFiles.keySet());\n     EasyMock.expect(tabletMeta.getUnSplittable()).andReturn(usm).atLeastOnce();\n+    EasyMock.expect(tabletMeta.getMigration()).andReturn(migration).atLeastOnce();\n \n     EasyMock.expect(ample.readTablet(origExtent)).andReturn(tabletMeta);\n \n@@ -370,6 +372,7 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(tablet3Mutator.deleteSuspension()).andReturn(tablet3Mutator);\n     EasyMock.expect(tablet3Mutator.deleteLocation(lastLocation)).andReturn(tablet3Mutator);\n     EasyMock.expect(tablet3Mutator.deleteUnSplittable()).andReturn(tablet3Mutator);\n+    EasyMock.expect(tablet3Mutator.deleteMigration()).andReturn(tablet3Mutator);\n     tablet3Mutator.submit(EasyMock.anyObject());\n     EasyMock.expectLastCall().once();\n     EasyMock.expect(tabletsMutator.mutateTablet(origExtent)).andReturn(tablet3Mutator);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java b/test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java\nindex c25dd9d196c..ee6771cc90f 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/AmpleConditionalWriterIT.java\n@@ -1906,4 +1906,38 @@ public void testMetadataCheck() {\n     }\n     assertEquals(Set.of(stf1, stf3), context.getAmple().readTablet(e1).getFiles());\n   }\n+\n+  @Test\n+  public void testRequireMigration() {\n+    var context = getCluster().getServerContext();\n+    var tsi = new TServerInstance(\""localhost:1234\"", 56L);\n+    var otherTsi = new TServerInstance(\""localhost:9876\"", 54L);\n+\n+    try (var ctmi = new ConditionalTabletsMutatorImpl(context)) {\n+      ctmi.mutateTablet(e1).requireAbsentOperation().requireMigration(tsi).deleteMigration()\n+          .submit(tm -> false);\n+      assertEquals(Status.REJECTED, ctmi.process().get(e1).getStatus());\n+    }\n+    assertNull(context.getAmple().readTablet(e1).getMigration());\n+\n+    try (var ctmi = new ConditionalTabletsMutatorImpl(context)) {\n+      ctmi.mutateTablet(e1).requireAbsentOperation().putMigration(tsi).submit(tm -> false);\n+      assertEquals(Status.ACCEPTED, ctmi.process().get(e1).getStatus());\n+    }\n+    assertEquals(tsi, context.getAmple().readTablet(e1).getMigration());\n+\n+    try (var ctmi = new ConditionalTabletsMutatorImpl(context)) {\n+      ctmi.mutateTablet(e1).requireAbsentOperation().requireMigration(otherTsi).deleteMigration()\n+          .submit(tm -> false);\n+      assertEquals(Status.REJECTED, ctmi.process().get(e1).getStatus());\n+    }\n+    assertEquals(tsi, context.getAmple().readTablet(e1).getMigration());\n+\n+    try (var ctmi = new ConditionalTabletsMutatorImpl(context)) {\n+      ctmi.mutateTablet(e1).requireAbsentOperation().requireMigration(tsi).deleteMigration()\n+          .submit(tm -> false);\n+      assertEquals(Status.ACCEPTED, ctmi.process().get(e1).getStatus());\n+    }\n+    assertNull(context.getAmple().readTablet(e1).getMigration());\n+  }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java b/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java\nindex 5edf7329f62..d347d51c42e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/TabletManagementIteratorIT.java\n@@ -592,7 +592,7 @@ private static TabletManagementParameters createParameters(AccumuloClient client\n         onlineTables,\n         new LiveTServerSet.LiveTServersSnapshot(tservers,\n             Map.of(Constants.DEFAULT_RESOURCE_GROUP_NAME, tservers)),\n-        Set.of(), Map.of(), Ample.DataLevel.USER, Map.of(), true, replacements,\n+        Set.of(), Ample.DataLevel.USER, Map.of(), true, replacements,\n         SteadyTime.from(10000, TimeUnit.NANOSECONDS));\n   }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5404"", ""pr_id"": 5404, ""issue_id"": 5160, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Improve performance of seeding split fate operations\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThe changes in #5122 improved the performance of seeding fate split operations.  However there is still further room for improvement.  [This code](https://github.com/apache/accumulo/blob/6dc52bc4ee703cac23921dc61705c45b6734f93c/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java#L48) will seed a single split operation which will write a single conditional mutation.  Writing single conditional mutations at a time requires waiting on them to committed in the write ahead log.  Could get much better performance by writing many conditional mutaitons at once as this would allow many to be written to the write ahead log and then wait for all of them to be committed.\r\n\r\n**Describe the solution you'd like**\r\n\r\nThe code for seeding split fate operations is restructured so that is takes a batch of tablets that need split operations seeded and writes the entire batch to a conditional writer.   This would avoid the single writes.  The current code avoids two problems and we would want to continue avoiding these when restructuring the code.\r\n\r\nThe first problem the current code avoids is attempting to seed the same tablet for split when its currently in the processes of seeding.   This can happen when there is a situation like 100 tablets that need to split and tablet group watcher keeps finding these and queueing them up for seeding over and over.  This will not cause problems with correctness, it just waste resources.\r\n\r\nThe second problem the current code avoids is making the tablet group watcher wait on seeding split operations.  If there is a problem writing to the fate table to seed split, this will not block the tablet group watcher.  Instead it eventually starts dropping/ignoring request to seed split operations.\r\n"", ""issue_word_count"": 302, ""test_files_count"": 3, ""non_test_files_count"": 8, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/split/Splitter.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java""], ""base_commit"": ""ba68de18d1b0721e47f51b1931d765e6af892037"", ""head_commit"": ""e7f48aa83e0afdc05d37e1b83911d7232395d59d"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5404"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5404"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-21T13:18:40.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 9687d2b7f60..62ae03cf69f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -466,10 +466,6 @@ public enum Property {\n           + \""indefinitely. Default is 0 to block indefinitely. Only valid when tserver available \""\n           + \""threshold is set greater than 0.\"",\n       \""1.10.0\""),\n-  MANAGER_SPLIT_WORKER_THREADS(\""manager.split.seed.threadpool.size\"", \""8\"", PropertyType.COUNT,\n-      \""The number of threads used to seed fate split task, the actual split work is done by fate\""\n-          + \"" threads.\"",\n-      \""4.0.0\""),\n   MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE(\""manager.compaction.major.service.queue.size\"",\n       \""1M\"", PropertyType.MEMORY,\n       \""The data size of each resource groups compaction job priority queue.  The memory size of \""\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 93f42181198..60d3f427c06 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -55,6 +55,7 @@\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n+import org.apache.accumulo.core.fate.FateStore.Seeder;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.logging.FateLogger;\n import org.apache.accumulo.core.manager.thrift.TFateOperation;\n@@ -539,6 +540,10 @@ public FateId startTransaction() {\n     return store.create();\n   }\n \n+  public Seeder<T> beginSeeding() {\n+    return store.beginSeeding();\n+  }\n+\n   public void seedTransaction(FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n     try (var seeder = store.beginSeeding()) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\nindex 25fc8fdd476..0280dbf7498 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.accumulo.core.fate.user;\n \n+import org.apache.accumulo.core.data.ConditionalMutation;\n import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.core.fate.FateStore;\n@@ -101,4 +102,6 @@ enum Status {\n \n   Status tryMutate();\n \n+  ConditionalMutation getMutation();\n+\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\nindex b742361ccfb..bb33f6ea818 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n@@ -260,4 +260,9 @@ public Status tryMutate() {\n       throw new RuntimeException(e);\n     }\n   }\n+\n+  @Override\n+  public ConditionalMutation getMutation() {\n+    return mutation;\n+  }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex edeef321a6b..6b8c6931e3a 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -1346,7 +1346,7 @@ boolean canSuspendTablets() {\n     // Don't call start the CompactionCoordinator until we have tservers and upgrade is complete.\n     compactionCoordinator.start();\n \n-    this.splitter = new Splitter(context);\n+    this.splitter = new Splitter(this);\n     this.splitter.start();\n \n     try {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java b/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\nindex e29413e82aa..fe39e603818 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/TabletGroupWatcher.java\n@@ -75,7 +75,6 @@\n import org.apache.accumulo.core.util.threads.Threads;\n import org.apache.accumulo.core.util.threads.Threads.AccumuloDaemonThread;\n import org.apache.accumulo.manager.metrics.ManagerMetrics;\n-import org.apache.accumulo.manager.split.SeedSplitTask;\n import org.apache.accumulo.manager.state.TableCounts;\n import org.apache.accumulo.manager.state.TableStats;\n import org.apache.accumulo.manager.upgrade.UpgradeCoordinator;\n@@ -607,7 +606,7 @@ private TableMgmtStats manageTablets(Iterator<TabletManagement> iter,\n       final boolean needsSplit = actions.contains(ManagementAction.NEEDS_SPLITTING);\n       if (needsSplit) {\n         LOG.debug(\""{} may need splitting.\"", tm.getExtent());\n-        manager.getSplitter().initiateSplit(new SeedSplitTask(manager, tm.getExtent()));\n+        manager.getSplitter().initiateSplit(tm.getExtent());\n       }\n \n       if (actions.contains(ManagementAction.NEEDS_COMPACTING) && compactionGenerator != null) {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java b/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java\ndeleted file mode 100644\nindex 8270bc423f3..00000000000\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java\n+++ /dev/null\n@@ -1,55 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.manager.split;\n-\n-import org.apache.accumulo.core.dataImpl.KeyExtent;\n-import org.apache.accumulo.core.fate.Fate;\n-import org.apache.accumulo.core.fate.FateInstanceType;\n-import org.apache.accumulo.core.fate.FateKey;\n-import org.apache.accumulo.manager.Manager;\n-import org.apache.accumulo.manager.tableOps.split.FindSplits;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-public class SeedSplitTask implements Runnable {\n-\n-  private static final Logger log = LoggerFactory.getLogger(SeedSplitTask.class);\n-  private final Manager manager;\n-  private final KeyExtent extent;\n-\n-  public SeedSplitTask(Manager manager, KeyExtent extent) {\n-    this.manager = manager;\n-    this.extent = extent;\n-  }\n-\n-  @Override\n-  public void run() {\n-    try {\n-      var fateInstanceType = FateInstanceType.fromTableId((extent.tableId()));\n-      manager.fate(fateInstanceType).seedTransaction(Fate.FateOperation.SYSTEM_SPLIT,\n-          FateKey.forSplit(extent), new FindSplits(extent), true);\n-    } catch (Exception e) {\n-      log.error(\""Failed to split {}\"", extent, e);\n-    }\n-  }\n-\n-  public KeyExtent getExtent() {\n-    return extent;\n-  }\n-}\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/split/Splitter.java b/server/manager/src/main/java/org/apache/accumulo/manager/split/Splitter.java\nindex 85b841d1cf4..d88e52ed66b 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/split/Splitter.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/split/Splitter.java\n@@ -18,22 +18,28 @@\n  */\n package org.apache.accumulo.manager.split;\n \n+import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;\n+\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.RejectedExecutionException;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n \n-import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.dataImpl.KeyExtent;\n+import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.FateInstanceType;\n+import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.core.file.FileOperations;\n import org.apache.accumulo.core.file.FileSKVIterator;\n import org.apache.accumulo.core.metadata.TabletFile;\n import org.apache.accumulo.core.util.cache.Caches.CacheName;\n+import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.tableOps.split.FindSplits;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.TableConfiguration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -49,9 +55,64 @@ public class Splitter {\n \n   private static final Logger LOG = LoggerFactory.getLogger(Splitter.class);\n \n+  private final Manager manager;\n   private final ThreadPoolExecutor splitExecutor;\n   // tracks which tablets are queued in splitExecutor\n-  private final Set<Text> queuedTablets = ConcurrentHashMap.newKeySet();\n+  private final Map<Text,KeyExtent> queuedTablets = new ConcurrentHashMap<>();\n+\n+  class SplitWorker implements Runnable {\n+\n+    @Override\n+    public void run() {\n+      try {\n+        while (manager.stillManager()) {\n+          if (queuedTablets.isEmpty()) {\n+            sleepUninterruptibly(10, TimeUnit.MILLISECONDS);\n+            continue;\n+          }\n+\n+          final Map<Text,KeyExtent> userSplits = new HashMap<>();\n+          final Map<Text,KeyExtent> metaSplits = new HashMap<>();\n+\n+          // Go through all the queued up splits and partition\n+          // into the different store types to be submitted.\n+          queuedTablets.forEach((metaRow, extent) -> {\n+            switch (FateInstanceType.fromTableId((extent.tableId()))) {\n+              case USER:\n+                userSplits.put(metaRow, extent);\n+                break;\n+              case META:\n+                metaSplits.put(metaRow, extent);\n+                break;\n+              default:\n+                throw new IllegalStateException(\""Unexpected FateInstanceType\"");\n+            }\n+          });\n+\n+          // see the user and then meta splits\n+          // The meta plits (zk) will be processed one at a time but there will not be\n+          // many of those splits. The user splits are processed as a batch.\n+          seedSplits(FateInstanceType.USER, userSplits);\n+          seedSplits(FateInstanceType.META, metaSplits);\n+        }\n+      } catch (Exception e) {\n+        LOG.error(\""Failed to split\"", e);\n+      }\n+    }\n+  }\n+\n+  private void seedSplits(FateInstanceType instanceType, Map<Text,KeyExtent> splits) {\n+    if (!splits.isEmpty()) {\n+      try (var seeder = manager.fate(instanceType).beginSeeding()) {\n+        for (KeyExtent extent : splits.values()) {\n+          var unused = seeder.attemptToSeedTransaction(Fate.FateOperation.SYSTEM_SPLIT,\n+              FateKey.forSplit(extent), new FindSplits(extent), true);\n+        }\n+      } finally {\n+        queuedTablets.keySet().removeAll(splits.keySet());\n+      }\n+    }\n+  }\n \n   public static class FileInfo {\n     final Text firstRow;\n@@ -151,12 +212,12 @@ public int hashCode() {\n \n   final LoadingCache<CacheKey,FileInfo> splitFileCache;\n \n-  public Splitter(ServerContext context) {\n-    int numThreads = context.getConfiguration().getCount(Property.MANAGER_SPLIT_WORKER_THREADS);\n+  public Splitter(Manager manager) {\n+    this.manager = manager;\n+    ServerContext context = manager.getContext();\n \n-    this.splitExecutor = context.threadPools().getPoolBuilder(\""split_seeder\"")\n-        .numCoreThreads(numThreads).numMaxThreads(numThreads).withTimeOut(0L, TimeUnit.MILLISECONDS)\n-        .enableThreadPoolMetrics().build();\n+    this.splitExecutor = context.threadPools().getPoolBuilder(\""split_seeder\"").numCoreThreads(1)\n+        .numMaxThreads(1).withTimeOut(0L, TimeUnit.MILLISECONDS).enableThreadPoolMetrics().build();\n \n     Weigher<CacheKey,\n         FileInfo> weigher = (key, info) -> key.tableId.canonical().length()\n@@ -175,7 +236,9 @@ public Splitter(ServerContext context) {\n \n   }\n \n-  public synchronized void start() {}\n+  public synchronized void start() {\n+    splitExecutor.execute(new SplitWorker());\n+  }\n \n   public synchronized void stop() {\n     splitExecutor.shutdownNow();\n@@ -185,29 +248,14 @@ public FileInfo getCachedFileInfo(TableId tableId, TabletFile tabletFile) {\n     return splitFileCache.get(new CacheKey(tableId, tabletFile));\n   }\n \n-  public void initiateSplit(SeedSplitTask seedSplitTask) {\n+  public void initiateSplit(KeyExtent extent) {\n     // Want to avoid queuing the same tablet multiple times, it would not cause bugs but would waste\n     // work. Use the metadata row to identify a tablet because the KeyExtent also includes the prev\n     // end row which may change when splits happen. The metaRow is conceptually tableId+endRow and\n     // that does not change for a split.\n-    Text metaRow = seedSplitTask.getExtent().toMetaRow();\n+    Text metaRow = extent.toMetaRow();\n     int qsize = queuedTablets.size();\n-    if (qsize < 10_000 && queuedTablets.add(metaRow)) {\n-      Runnable taskWrapper = () -> {\n-        try {\n-          seedSplitTask.run();\n-        } finally {\n-          queuedTablets.remove(metaRow);\n-        }\n-      };\n-\n-      try {\n-        splitExecutor.execute(taskWrapper);\n-      } catch (RejectedExecutionException rje) {\n-        queuedTablets.remove(metaRow);\n-        throw rje;\n-      }\n-    } else {\n+    if (qsize >= 10_000 || queuedTablets.putIfAbsent(metaRow, extent) != null) {\n       LOG.trace(\""Did not add {} to split queue {}\"", metaRow, qsize);\n     }\n   }\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex c7ec3b4e4cd..3f5a8ec0402 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -56,10 +56,6 @@ interface Seeder<T> extends AutoCloseable {\n      * Attempts to seed a transaction with the given repo if it does not exist. A fateId will be\n      * derived from the fateKey. If seeded, sets the following data for the fateId in the store.\n      *\n-     * TODO: Support completing futures later in close method The current version will always return\n-     * with a CompleteableFuture that is already completed. Future version will process will\n-     * complete in the close() method for the User store.\n-     *\n      * <ul>\n      * <li>Set the fate op</li>\n      * <li>Set the status to SUBMITTED</li>\n@@ -76,15 +72,12 @@ interface Seeder<T> extends AutoCloseable {\n     CompletableFuture<Optional<FateId>> attemptToSeedTransaction(Fate.FateOperation fateOp,\n         FateKey fateKey, Repo<T> repo, boolean autoCleanUp);\n \n-    // TODO: Right now all implementations do nothing\n-    // Eventually this would check the status of all added conditional mutations,\n-    // retry unknown, and then close the conditional writer.\n     @Override\n     void close();\n   }\n \n-  // Creates a conditional writer for the user fate store. For Zookeeper all this code will probably\n-  // do the same thing its currently doing as zookeeper does not support multi-node operations.\n+  // Creates a conditional writer for the user fate store. For Zookeeper this will be a no-op\n+  // because currently zookeeper does not support multi-node operations.\n   Seeder<T> beginSeeding();\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex 466c771d1e7..2a6efbed0dc 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -21,19 +21,26 @@\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n import java.util.Map.Entry;\n import java.util.Objects;\n import java.util.Optional;\n import java.util.SortedMap;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Function;\n import java.util.function.Predicate;\n import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n+import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.client.AccumuloSecurityException;\n+import org.apache.accumulo.core.client.ConditionalWriter;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n@@ -47,9 +54,11 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n+import org.apache.accumulo.core.fate.FateKey.FateKeyType;\n import org.apache.accumulo.core.fate.ReadOnlyRepo;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.StackOverflowException;\n+import org.apache.accumulo.core.fate.user.FateMutator.Status;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.RepoColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxInfoColumnFamily;\n@@ -57,6 +66,7 @@\n import org.apache.accumulo.core.iterators.user.WholeRowIterator;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.util.ColumnFQ;\n+import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.core.util.UtilWaitThread;\n import org.apache.hadoop.io.Text;\n import org.slf4j.Logger;\n@@ -136,36 +146,14 @@ public FateId getFateId() {\n \n   @Override\n   public Seeder<T> beginSeeding() {\n-    // TODO: For now can handle seeding 1 transaction at a time so just process\n-    // everything in attemptToSeedTransaction\n-    // Part 2 of the changes in #5160 will allow multiple seeding attempts to be combined\n-    // into one conditional mutation and we will need to track the pending operations\n-    // and futures in a map\n-    return new Seeder<T>() {\n-      @Override\n-      public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n-          FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n-        return CompletableFuture\n-            .completedFuture(seedTransaction(fateOp, fateKey, repo, autoCleanUp));\n-      }\n-\n-      @Override\n-      public void close() {\n-        // TODO: This will be used in Part 2 of #5160\n-      }\n-    };\n+    return new BatchSeeder();\n   }\n \n-  private Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n-      boolean autoCleanUp) {\n-    final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n+  private FateMutator<T> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, FateId fateId,\n+      Repo<T> repo, boolean autoCleanUp) {\n     Supplier<FateMutator<T>> mutatorFactory = () -> newMutator(fateId).requireAbsent()\n         .putKey(fateKey).putCreateTime(System.currentTimeMillis());\n-    if (seedTransaction(mutatorFactory, fateKey + \"" \"" + fateId, fateOp, repo, autoCleanUp)) {\n-      return Optional.of(fateId);\n-    } else {\n-      return Optional.empty();\n-    }\n+    return buildMutator(mutatorFactory, fateOp, repo, autoCleanUp);\n   }\n \n   @Override\n@@ -176,16 +164,22 @@ public boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<T>\n     return seedTransaction(mutatorFactory, fateId.canonical(), fateOp, repo, autoCleanUp);\n   }\n \n+  private FateMutator<T> buildMutator(Supplier<FateMutator<T>> mutatorFactory,\n+      Fate.FateOperation fateOp, Repo<T> repo, boolean autoCleanUp) {\n+    var mutator = mutatorFactory.get();\n+    mutator =\n+        mutator.putFateOp(serializeTxInfo(fateOp)).putRepo(1, repo).putStatus(TStatus.SUBMITTED);\n+    if (autoCleanUp) {\n+      mutator = mutator.putAutoClean(serializeTxInfo(autoCleanUp));\n+    }\n+    return mutator;\n+  }\n+\n   private boolean seedTransaction(Supplier<FateMutator<T>> mutatorFactory, String logId,\n       Fate.FateOperation fateOp, Repo<T> repo, boolean autoCleanUp) {\n+    var mutator = buildMutator(mutatorFactory, fateOp, repo, autoCleanUp);\n     int maxAttempts = 5;\n     for (int attempt = 0; attempt < maxAttempts; attempt++) {\n-      var mutator = mutatorFactory.get();\n-      mutator =\n-          mutator.putFateOp(serializeTxInfo(fateOp)).putRepo(1, repo).putStatus(TStatus.SUBMITTED);\n-      if (autoCleanUp) {\n-        mutator = mutator.putAutoClean(serializeTxInfo(autoCleanUp));\n-      }\n       var status = mutator.tryMutate();\n       if (status == FateMutator.Status.ACCEPTED) {\n         // signal to the super class that a new fate transaction was seeded and is ready to run\n@@ -393,6 +387,113 @@ public FateInstanceType type() {\n     return fateInstanceType;\n   }\n \n+  private class BatchSeeder implements Seeder<T> {\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+\n+    private final Map<FateId,Pair<FateMutator<T>,CompletableFuture<Optional<FateId>>>> pending =\n+        new HashMap<>();\n+\n+    @Override\n+    public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n+        FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+      Preconditions.checkState(!closed.get(), \""Can't attempt to seed with a closed seeder.\"");\n+\n+      final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n+      // If not already submitted, add to the pending list and return the future\n+      // or the existing future if duplicate. The pending map will store the mutator\n+      // to be processed on close in a one batch.\n+      return pending.computeIfAbsent(fateId, id -> {\n+        FateMutator<T> mutator = seedTransaction(fateOp, fateKey, fateId, repo, autoCleanUp);\n+        CompletableFuture<Optional<FateId>> future = new CompletableFuture<>();\n+        return new Pair<>(mutator, future);\n+      }).getSecond();\n+    }\n+\n+    @Override\n+    public void close() {\n+      closed.set(true);\n+\n+      int maxAttempts = 5;\n+\n+      // This loop will submit all the pending mutations as one batch\n+      // to a conditional writer and any known results will be removed\n+      // from the pending map. Unknown results will be re-attempted up\n+      // to the maxAttempts count\n+      for (int attempt = 0; attempt < maxAttempts && !pending.isEmpty(); attempt++) {\n+        var currentResults = tryMutateBatch();\n+        for (Entry<FateId,ConditionalWriter.Status> result : currentResults.entrySet()) {\n+          var fateId = result.getKey();\n+          var status = result.getValue();\n+          var future = pending.get(fateId).getSecond();\n+          switch (result.getValue()) {\n+            case ACCEPTED:\n+              seededTx();\n+              log.trace(\""Attempt to seed {} returned {}\"", fateId.canonical(), status);\n+              // Complete the future with the fatId and remove from pending\n+              future.complete(Optional.of(fateId));\n+              pending.remove(fateId);\n+              break;\n+            case REJECTED:\n+              log.debug(\""Attempt to seed {} returned {}\"", fateId.canonical(), status);\n+              // Rejected so complete with an empty optional and remove from pending\n+              future.complete(Optional.empty());\n+              pending.remove(fateId);\n+              break;\n+            case UNKNOWN:\n+              log.debug(\""Attempt to seed {} returned {} status, retrying\"", fateId.canonical(),\n+                  status);\n+              // unknown, so don't remove from map so that we try again if still under\n+              // max attempts\n+              break;\n+            default:\n+              // do not expect other statuses\n+              throw new IllegalStateException(\""Unhandled status for mutation \"" + status);\n+          }\n+        }\n+\n+        if (!pending.isEmpty()) {\n+          // At this point can not reliably determine if the unknown pending mutations were\n+          // successful or not because no reservation was acquired. For example since no\n+          // reservation was acquired it is possible that seeding was a success and something\n+          // immediately picked it up and started operating on it and changing it.\n+          // If scanning after that point can not conclude success or failure. Another situation\n+          // is that maybe the fateId already existed in a seeded form prior to getting this\n+          // unknown.\n+          UtilWaitThread.sleep(250);\n+        }\n+      }\n+\n+      // Any remaining will be UNKNOWN status, so complete the futures with an optional empty\n+      pending.forEach((fateId, pair) -> {\n+        pair.getSecond().complete(Optional.empty());\n+        log.warn(\""Repeatedly received unknown status when attempting to seed {}\"",\n+            fateId.canonical());\n+      });\n+    }\n+\n+    // Submit all the pending mutations to a single conditional writer\n+    // as one batch and return the results for each mutation\n+    private Map<FateId,ConditionalWriter.Status> tryMutateBatch() {\n+      if (pending.isEmpty()) {\n+        return Map.of();\n+      }\n+\n+      final Map<FateId,ConditionalWriter.Status> resultsMap = new HashMap<>();\n+      try (ConditionalWriter writer = context.createConditionalWriter(tableName)) {\n+        Iterator<ConditionalWriter.Result> results = writer\n+            .write(pending.values().stream().map(pair -> pair.getFirst().getMutation()).iterator());\n+        while (results.hasNext()) {\n+          var result = results.next();\n+          var row = new Text(result.getMutation().getRow());\n+          resultsMap.put(FateId.from(FateInstanceType.USER, row.toString()), result.getStatus());\n+        }\n+      } catch (AccumuloException | AccumuloSecurityException | TableNotFoundException e) {\n+        throw new IllegalStateException(e);\n+      }\n+      return resultsMap;\n+    }\n+  }\n+\n   private class FateTxStoreImpl extends AbstractFateTxStoreImpl {\n \n     private FateTxStoreImpl(FateId fateId) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\nindex 61ea073a6f2..5b12b0f3cdd 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\n@@ -77,9 +77,6 @@ public static void createFateTable(ClientContext client, String table) throws Ex\n     assertEquals(fateTableProps, testFateTableProps);\n   }\n \n-  // For now just process one at a time as the current impl completes\n-  // each seed transaction individually. In future versions we can test\n-  // batching multiple seeding atempts together.\n   public static <T> Optional<FateId> seedTransaction(FateStore<T> store, Fate.FateOperation fateOp,\n       FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n     CompletableFuture<Optional<FateId>> fateIdFuture;\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5400"", ""pr_id"": 5400, ""issue_id"": 5386, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Make Cipher and OutputStream 1:1 in AESCryptoService\n**Describe the bug**\n\nAESCryptoService uses the same Cipher object for multiple output streams.  Cipher is a stateful object so it will break if the output streams use overlaps.  This overlap could be caused by multiple threads or a single thread interleaving writes to multiple streams. \n\n**Expected behavior**\n\nMultiple output streams can be created and written concurrently OR if this is not supported the code should defend against this.  Test should be added for either case.\n\n**Additional context**\n\nThis was fixed for reading in #5384.\n"", ""issue_word_count"": 92, ""test_files_count"": 2, ""non_test_files_count"": 4, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedOutputStream.java"", ""core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java"", ""core/src/main/java/org/apache/accumulo/core/spi/crypto/CryptoService.java"", ""core/src/main/java/org/apache/accumulo/core/spi/crypto/FileEncrypter.java"", ""core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java"", ""core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java"", ""core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java""], ""base_commit"": ""29506999476e99ffc15019469c35705ced0d3278"", ""head_commit"": ""352a0966d419365f7af73f980d755fbfa7e499f9"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5400"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5400"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-13T17:11:02.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedOutputStream.java b/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedOutputStream.java\nindex 05de9f7373a..c4ab873c131 100644\n--- a/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedOutputStream.java\n+++ b/core/src/main/java/org/apache/accumulo/core/crypto/streams/BlockedOutputStream.java\n@@ -22,6 +22,7 @@\n import java.io.IOException;\n import java.io.OutputStream;\n import java.nio.ByteBuffer;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n /**\n  * Buffers all input in a growing buffer until flush() is called. Then entire buffer is written,\n@@ -32,8 +33,10 @@ public class BlockedOutputStream extends OutputStream {\n   int blockSize;\n   DataOutputStream out;\n   ByteBuffer bb;\n+  private final AtomicBoolean openTracker;\n \n-  public BlockedOutputStream(OutputStream out, int blockSize, int bufferSize) {\n+  public BlockedOutputStream(OutputStream out, int blockSize, int bufferSize,\n+      AtomicBoolean openTracker) {\n     if (bufferSize <= 0) {\n       throw new IllegalArgumentException(\""bufferSize must be greater than 0.\"");\n     }\n@@ -50,6 +53,7 @@ public BlockedOutputStream(OutputStream out, int blockSize, int bufferSize) {\n     // some buffer space + bytes to make the buffer evened up with the cipher block size - 4 bytes\n     // for the size int\n     bb = ByteBuffer.allocate(bufferSize + remainder - 4);\n+    this.openTracker = openTracker;\n   }\n \n   @Override\n@@ -113,5 +117,6 @@ public void write(byte[] b) throws IOException {\n   public void close() throws IOException {\n     flush();\n     out.close();\n+    openTracker.compareAndSet(true, false);\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java b/core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java\nindex 85895053b36..555103fd983 100644\n--- a/core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java\n+++ b/core/src/main/java/org/apache/accumulo/core/spi/crypto/AESCryptoService.java\n@@ -40,6 +40,7 @@\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import javax.crypto.Cipher;\n import javax.crypto.CipherInputStream;\n@@ -351,6 +352,7 @@ public class AESGCMFileEncrypter implements FileEncrypter {\n       private final byte[] initVector = new byte[GCM_IV_LENGTH_IN_BYTES];\n       private final Cipher cipher;\n       private final byte[] decryptionParameters;\n+      private final AtomicBoolean openTracker = new AtomicBoolean();\n \n       AESGCMFileEncrypter() {\n         try {\n@@ -371,6 +373,10 @@ public OutputStream encryptStream(OutputStream outputStream) throws CryptoExcept\n           throw new CryptoException(\n               \""Key/IV reuse is forbidden in AESGCMCryptoModule. Too many RBlocks.\"");\n         }\n+        if (!openTracker.compareAndSet(false, true)) {\n+          throw new CryptoException(\""Attempted to obtain new stream without closing previous one.\"");\n+        }\n+\n         incrementIV(initVector, initVector.length - 1);\n         if (Arrays.equals(initVector, firstInitVector)) {\n           ivReused = true; // This will allow us to write the final block, since the\n@@ -397,7 +403,7 @@ public OutputStream encryptStream(OutputStream outputStream) throws CryptoExcept\n         // Without this, when the crypto stream is closed (in order to flush its last bytes)\n         // the underlying RFile stream will *also* be closed, and that's undesirable as the\n         // cipher stream is closed for every block written.\n-        return new BlockedOutputStream(cos, cipher.getBlockSize(), 1024);\n+        return new BlockedOutputStream(cos, cipher.getBlockSize(), 1024, openTracker);\n       }\n \n       /**\n@@ -493,6 +499,7 @@ public class AESCBCFileEncrypter implements FileEncrypter {\n       private final Key fek;\n       private final byte[] initVector = new byte[IV_LENGTH_IN_BYTES];\n       private final byte[] decryptionParameters;\n+      private final AtomicBoolean openTracker = new AtomicBoolean();\n \n       AESCBCFileEncrypter() {\n         try {\n@@ -507,6 +514,10 @@ public class AESCBCFileEncrypter implements FileEncrypter {\n \n       @Override\n       public OutputStream encryptStream(OutputStream outputStream) throws CryptoException {\n+        if (!openTracker.compareAndSet(false, true)) {\n+          throw new CryptoException(\""Attempted to obtain new stream without closing previous one.\"");\n+        }\n+\n         random.nextBytes(initVector);\n         try {\n           outputStream.write(initVector);\n@@ -521,7 +532,7 @@ public OutputStream encryptStream(OutputStream outputStream) throws CryptoExcept\n         }\n \n         CipherOutputStream cos = new CipherOutputStream(outputStream, cipher);\n-        return new BlockedOutputStream(cos, cipher.getBlockSize(), 1024);\n+        return new BlockedOutputStream(cos, cipher.getBlockSize(), 1024, openTracker);\n       }\n \n       @Override\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/spi/crypto/CryptoService.java b/core/src/main/java/org/apache/accumulo/core/spi/crypto/CryptoService.java\nindex a2055f1a891..dfdf63013bc 100644\n--- a/core/src/main/java/org/apache/accumulo/core/spi/crypto/CryptoService.java\n+++ b/core/src/main/java/org/apache/accumulo/core/spi/crypto/CryptoService.java\n@@ -37,7 +37,8 @@ public interface CryptoService {\n \n   /**\n    * Initialize the FileEncrypter for the environment and return. This will get called once per\n-   * R-File or Write Ahead Log. FileEncrypter implementation must be thread safe.\n+   * R-File or Write Ahead Log. FileEncrypter implementation is not expected be called by multiple\n+   * threads.\n    */\n   FileEncrypter getFileEncrypter(CryptoEnvironment environment);\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/spi/crypto/FileEncrypter.java b/core/src/main/java/org/apache/accumulo/core/spi/crypto/FileEncrypter.java\nindex ab0192c2d0f..47e7d8c073a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/spi/crypto/FileEncrypter.java\n+++ b/core/src/main/java/org/apache/accumulo/core/spi/crypto/FileEncrypter.java\n@@ -21,13 +21,14 @@\n import java.io.OutputStream;\n \n /**\n- * Class implementation that will encrypt a file. Make sure implementation is thread safe.\n+ * Class implementation that will encrypt a file.\n  *\n  * @since 2.0\n  */\n public interface FileEncrypter {\n   /**\n-   * Encrypt the OutputStream.\n+   * Encrypt the OutputStream. Only one OutputStream is expected to be active at a time. Before a\n+   * new encryption OutputStream is created the previous one is expected to be closed.\n    */\n   OutputStream encryptStream(OutputStream outputStream) throws CryptoService.CryptoException;\n \n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java b/core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java\nindex 6c49fde2a21..584db67a9ea 100644\n--- a/core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/crypto/BlockedIOStreamTest.java\n@@ -28,6 +28,7 @@\n import java.io.IOException;\n import java.security.SecureRandom;\n import java.util.Arrays;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.accumulo.core.crypto.streams.BlockedInputStream;\n import org.apache.accumulo.core.crypto.streams.BlockedOutputStream;\n@@ -44,7 +45,8 @@ public void testLargeBlockIO() throws IOException {\n \n   private void writeRead(int blockSize, int expectedSize) throws IOException {\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    BlockedOutputStream blockOut = new BlockedOutputStream(baos, blockSize, 1);\n+    BlockedOutputStream blockOut =\n+        new BlockedOutputStream(baos, blockSize, 1, new AtomicBoolean(true));\n \n     String contentString = \""My Blocked Content String\"";\n     byte[] content = contentString.getBytes(UTF_8);\n@@ -86,7 +88,7 @@ public void testSmallBufferBlockedIO() throws IOException {\n   public void testSpillingOverOutputStream() throws IOException {\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n     // buffer will be size 12\n-    BlockedOutputStream blockOut = new BlockedOutputStream(baos, 16, 16);\n+    BlockedOutputStream blockOut = new BlockedOutputStream(baos, 16, 16, new AtomicBoolean(true));\n \n     byte[] undersized = new byte[11];\n     byte[] perfectSized = new byte[12];\n@@ -129,7 +131,8 @@ public void testGiantWrite() throws IOException {\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n     int blockSize = 16;\n     // buffer will be size 12\n-    BlockedOutputStream blockOut = new BlockedOutputStream(baos, blockSize, blockSize);\n+    BlockedOutputStream blockOut =\n+        new BlockedOutputStream(baos, blockSize, blockSize, new AtomicBoolean(true));\n \n     int size = 1024 * 1024 * 128;\n     byte[] giant = new byte[size];\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java b/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\nindex 45f2ab22e5d..4a93b36043b 100644\n--- a/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\n@@ -562,6 +562,30 @@ private void testMultipleThreads(Scope scope) throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testOverlappingWrites() throws Exception {\n+    testOverlappingWrites(WAL);\n+    testOverlappingWrites(TABLE);\n+  }\n+\n+  private void testOverlappingWrites(Scope scope) throws Exception {\n+    AESCryptoService cs = new AESCryptoService();\n+    cs.init(getAllCryptoProperties(ConfigMode.CRYPTO_TABLE_ON));\n+    CryptoEnvironment encEnv = new CryptoEnvironmentImpl(scope, null, null);\n+    FileEncrypter encrypter = cs.getFileEncrypter(encEnv);\n+\n+    ByteArrayOutputStream out1 = new ByteArrayOutputStream();\n+    var es1 = encrypter.encryptStream(out1);\n+\n+    // try to create a new encryption stream w/o closing the previous one\n+    ByteArrayOutputStream out2 = new ByteArrayOutputStream();\n+    var ce = assertThrows(CryptoException.class, () -> encrypter.encryptStream(out2));\n+    assertTrue(ce.getMessage().contains(\""closing previous\""));\n+\n+    es1.close();\n+    var es2 = encrypter.encryptStream(out2);\n+  }\n+\n   private ArrayList<Key> testData() {\n     ArrayList<Key> keys = new ArrayList<>();\n     keys.add(new Key(\""a\"", \""cf\"", \""cq\""));\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5374"", ""pr_id"": 5374, ""issue_id"": 5160, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Improve performance of seeding split fate operations\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThe changes in #5122 improved the performance of seeding fate split operations.  However there is still further room for improvement.  [This code](https://github.com/apache/accumulo/blob/6dc52bc4ee703cac23921dc61705c45b6734f93c/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java#L48) will seed a single split operation which will write a single conditional mutation.  Writing single conditional mutations at a time requires waiting on them to committed in the write ahead log.  Could get much better performance by writing many conditional mutaitons at once as this would allow many to be written to the write ahead log and then wait for all of them to be committed.\r\n\r\n**Describe the solution you'd like**\r\n\r\nThe code for seeding split fate operations is restructured so that is takes a batch of tablets that need split operations seeded and writes the entire batch to a conditional writer.   This would avoid the single writes.  The current code avoids two problems and we would want to continue avoiding these when restructuring the code.\r\n\r\nThe first problem the current code avoids is attempting to seed the same tablet for split when its currently in the processes of seeding.   This can happen when there is a situation like 100 tablets that need to split and tablet group watcher keeps finding these and queueing them up for seeding over and over.  This will not cause problems with correctness, it just waste resources.\r\n\r\nThe second problem the current code avoids is making the tablet group watcher wait on seeding split operations.  If there is a problem writing to the fate table to seed split, this will not block the tablet group watcher.  Instead it eventually starts dropping/ignoring request to seed split operations.\r\n"", ""issue_word_count"": 302, ""test_files_count"": 7, ""non_test_files_count"": 2, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java""], ""base_commit"": ""b694d6cbf7c4a9b8f02119fb46434dc63e657021"", ""head_commit"": ""ac95d604bb285b73c0fb3ba73cbe08321f7933c0"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5374"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5374"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-07T17:32:25.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex f041c07313d..dd521e3a0c7 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -540,7 +540,9 @@ public FateId startTransaction() {\n \n   public void seedTransaction(FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n-    store.seedTransaction(fateOp, fateKey, repo, autoCleanUp);\n+    try (var seeder = store.beginSeeding()) {\n+      var unused = seeder.attemptToSeedTransaction(fateOp, fateKey, repo, autoCleanUp);\n+    }\n   }\n \n   // start work in the transaction.. it is safe to call this\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\nindex be4b2aec298..854452ce642 100644\n--- a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n+++ b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n@@ -21,18 +21,22 @@\n import java.io.Serializable;\n import java.util.EnumSet;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Consumer;\n import java.util.function.Function;\n import java.util.stream.Stream;\n \n import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.Fate.FateOperation;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n+import org.apache.accumulo.core.fate.FateStore.Seeder;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.StackOverflowException;\n@@ -150,19 +154,8 @@ public FateId create() {\n       }\n \n       @Override\n-      public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey,\n-          Repo<T> repo, boolean autoCleanUp) {\n-        var optional = store.seedTransaction(fateOp, fateKey, repo, autoCleanUp);\n-        if (storeLog.isTraceEnabled()) {\n-          optional.ifPresentOrElse(fateId -> {\n-            storeLog.trace(\""{} seeded {} {} {}\"", fateId, fateKey, toLogString.apply(repo),\n-                autoCleanUp);\n-          }, () -> {\n-            storeLog.trace(\""Possibly unable to seed {} {} {}\"", fateKey, toLogString.apply(repo),\n-                autoCleanUp);\n-          });\n-        }\n-        return optional;\n+      public Seeder<T> beginSeeding() {\n+        return new SeederLogger<>(store, toLogString);\n       }\n \n       @Override\n@@ -202,4 +195,41 @@ public void deleteDeadReservations() {\n       }\n     };\n   }\n+\n+  public static class SeederLogger<T> implements Seeder<T> {\n+    private final FateStore<T> store;\n+    private final Seeder<T> seeder;\n+    private final Function<Repo<T>,String> toLogString;\n+\n+    public SeederLogger(FateStore<T> store, Function<Repo<T>,String> toLogString) {\n+      this.store = Objects.requireNonNull(store);\n+      this.seeder = store.beginSeeding();\n+      this.toLogString = Objects.requireNonNull(toLogString);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n+        FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+      var future = this.seeder.attemptToSeedTransaction(fateOp, fateKey, repo, autoCleanUp);\n+      return future.whenComplete((optional, throwable) -> {\n+        if (storeLog.isTraceEnabled()) {\n+          optional.ifPresentOrElse(fateId -> {\n+            storeLog.trace(\""{} seeded {} {} {}\"", fateId, fateKey, toLogString.apply(repo),\n+                autoCleanUp);\n+          }, () -> {\n+            storeLog.trace(\""Possibly unable to seed {} {} {}\"", fateKey, toLogString.apply(repo),\n+                autoCleanUp);\n+          });\n+        }\n+      });\n+    }\n+\n+    @Override\n+    public void close() {\n+      seeder.close();\n+      if (storeLog.isTraceEnabled()) {\n+        storeLog.trace(\""attempted to close seeder for {}\"", store.type());\n+      }\n+    }\n+  }\n }\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex ef3ea18e5af..6854aec7136 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -27,7 +27,9 @@\n import java.util.Arrays;\n import java.util.Objects;\n import java.util.Optional;\n+import java.util.Set;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n \n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.hadoop.io.DataInputBuffer;\n@@ -49,25 +51,42 @@ public interface FateStore<T> extends ReadOnlyFateStore<T> {\n    */\n   FateId create();\n \n-  /**\n-   * Seeds a transaction with the given repo if it does not exist. A fateId will be derived from the\n-   * fateKey. If seeded, sets the following data for the fateId in the store.\n-   *\n-   * <ul>\n-   * <li>Set the fate op</li>\n-   * <li>Set the status to SUBMITTED</li>\n-   * <li>Set the fate key</li>\n-   * <li>Sets autocleanup only if true</li>\n-   * <li>Sets the creation time</li>\n-   * </ul>\n-   *\n-   * @return The return type is only intended for testing it may not be correct in the face of\n-   *         failures. When there are no failures returns optional w/ the fate id set if seeded and\n-   *         empty optional otherwise. If there was a failure this could return an empty optional\n-   *         when it actually succeeded.\n-   */\n-  Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n-      boolean autoCleanUp);\n+  interface Seeder<T> extends AutoCloseable {\n+\n+    /**\n+     * Attempts to seed a transaction with the given repo if it does not exist. A fateId will be\n+     * derived from the fateKey. If seeded, sets the following data for the fateId in the store.\n+     *\n+     * TODO: Support completing futures later in close method The current version will always return\n+     * with a CompleteableFuture that is already completed. Future version will process will\n+     * complete in the close() method for the User store.\n+     *\n+     * <ul>\n+     * <li>Set the fate op</li>\n+     * <li>Set the status to SUBMITTED</li>\n+     * <li>Set the fate key</li>\n+     * <li>Sets autocleanup only if true</li>\n+     * <li>Sets the creation time</li>\n+     * </ul>\n+     *\n+     * @return The return type is only intended for testing it may not be correct in the face of\n+     *         failures. When there are no failures returns optional w/ the fate id set if seeded\n+     *         and empty optional otherwise. If there was a failure this could return an empty\n+     *         optional when it actually succeeded.\n+     */\n+    CompletableFuture<Optional<FateId>> attemptToSeedTransaction(Fate.FateOperation fateOp,\n+        FateKey fateKey, Repo<T> repo, boolean autoCleanUp);\n+\n+    // TODO: Right now all implementations do nothing\n+    // Eventually this would check the status of all added conditional mutations,\n+    // retry unknown, and then close the conditional writer.\n+    @Override\n+    void close();\n+  }\n+\n+  // Creates a conditional writer for the user fate store. For Zookeeper all this code will probably\n+  // do the same thing its currently doing as zookeeper does not support multi-node operations.\n+  Seeder<T> beginSeeding();\n \n   /**\n    * Seeds a transaction with the given repo if its current status is NEW and it is currently\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex e444d2c0023..466c771d1e7 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -27,6 +27,7 @@\n import java.util.Optional;\n import java.util.SortedMap;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n import java.util.function.Function;\n import java.util.function.Predicate;\n import java.util.function.Supplier;\n@@ -41,6 +42,7 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.Fate.FateOperation;\n import org.apache.accumulo.core.fate.Fate.TxInfo;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n@@ -133,7 +135,28 @@ public FateId getFateId() {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n+  public Seeder<T> beginSeeding() {\n+    // TODO: For now can handle seeding 1 transaction at a time so just process\n+    // everything in attemptToSeedTransaction\n+    // Part 2 of the changes in #5160 will allow multiple seeding attempts to be combined\n+    // into one conditional mutation and we will need to track the pending operations\n+    // and futures in a map\n+    return new Seeder<T>() {\n+      @Override\n+      public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n+          FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+        return CompletableFuture\n+            .completedFuture(seedTransaction(fateOp, fateKey, repo, autoCleanUp));\n+      }\n+\n+      @Override\n+      public void close() {\n+        // TODO: This will be used in Part 2 of #5160\n+      }\n+    };\n+  }\n+\n+  private Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n     final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n     Supplier<FateMutator<T>> mutatorFactory = () -> newMutator(fateId).requireAbsent()\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex 35fdf07c317..693fdb2437b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -40,6 +40,7 @@\n import java.util.Optional;\n import java.util.Set;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n import java.util.function.Predicate;\n import java.util.function.Supplier;\n import java.util.function.UnaryOperator;\n@@ -48,6 +49,7 @@\n import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.Fate.FateOperation;\n import org.apache.accumulo.core.fate.Fate.TxInfo;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n@@ -181,16 +183,26 @@ private Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n-      boolean autoCleanUp) {\n-    return createAndReserve(fateKey).map(txStore -> {\n-      try {\n-        seedTransaction(fateOp, repo, autoCleanUp, txStore);\n-        return txStore.getID();\n-      } finally {\n-        txStore.unreserve(Duration.ZERO);\n+  public Seeder<T> beginSeeding() {\n+    return new Seeder<T>() {\n+      @Override\n+      public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n+          FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+        return CompletableFuture.completedFuture(createAndReserve(fateKey).map(txStore -> {\n+          try {\n+            seedTransaction(fateOp, repo, autoCleanUp, txStore);\n+            return txStore.getID();\n+          } finally {\n+            txStore.unreserve(Duration.ZERO);\n+          }\n+        }));\n+      }\n+\n+      @Override\n+      public void close() {\n+        // Nothing to do for Meta fate store\n       }\n-    });\n+    };\n   }\n \n   @Override\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\nindex 7e339a2e813..c9288b90890 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n@@ -29,10 +29,12 @@\n import java.util.Optional;\n import java.util.Set;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Consumer;\n import java.util.stream.Stream;\n \n+import org.apache.accumulo.core.fate.Fate.FateOperation;\n import org.apache.accumulo.core.util.Pair;\n \n /**\n@@ -53,9 +55,17 @@ public FateId create() {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey,\n-      Repo<String> repo, boolean autoCleanUp) {\n-    return Optional.empty();\n+  public Seeder<String> beginSeeding() {\n+    return new Seeder<>() {\n+      @Override\n+      public CompletableFuture<Optional<FateId>> attemptToSeedTransaction(FateOperation fateOp,\n+          FateKey fateKey, Repo<String> repo, boolean autoCleanUp) {\n+        return CompletableFuture.completedFuture(Optional.empty());\n+      }\n+\n+      @Override\n+      public void close() {}\n+    };\n   }\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\nindex fc1020bcfad..edc14c42c2f 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n@@ -32,6 +32,7 @@\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.row;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.verify;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.writeData;\n+import static org.apache.accumulo.test.fate.FateStoreUtil.seedTransaction;\n import static org.apache.accumulo.test.util.FileMetadataUtil.countFencedFiles;\n import static org.apache.accumulo.test.util.FileMetadataUtil.splitFilesIntoRanges;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n@@ -381,7 +382,7 @@ private FateId createCompactionCommitAndDeadMetadata(AccumuloClient c,\n     // should never run. Its purpose is to prevent the dead compaction detector\n     // from deleting the id.\n     Repo<Manager> repo = new FakeRepo();\n-    var fateId = fateStore.seedTransaction(Fate.FateOperation.COMMIT_COMPACTION,\n+    var fateId = seedTransaction(fateStore, Fate.FateOperation.COMMIT_COMPACTION,\n         FateKey.forCompactionCommit(allCids.get(tableId).get(0)), repo, true).orElseThrow();\n \n     // Read the tablet metadata\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\nindex bdeb5d6695d..27de329921e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n@@ -20,6 +20,7 @@\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n+import static org.apache.accumulo.test.fate.FateStoreUtil.seedTransaction;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertInstanceOf;\n@@ -299,8 +300,10 @@ protected void testCreateWithKey(FateStore<TestEnv> store, ServerContext sctx) {\n     FateKey fateKey2 =\n         FateKey.forCompactionCommit(ExternalCompactionId.generate(UUID.randomUUID()));\n \n-    var fateId1 = store.seedTransaction(TEST_FATE_OP, fateKey1, new TestRepo(), true).orElseThrow();\n-    var fateId2 = store.seedTransaction(TEST_FATE_OP, fateKey2, new TestRepo(), true).orElseThrow();\n+    var fateId1 =\n+        seedTransaction(store, TEST_FATE_OP, fateKey1, new TestRepo(), true).orElseThrow();\n+    var fateId2 =\n+        seedTransaction(store, TEST_FATE_OP, fateKey2, new TestRepo(), true).orElseThrow();\n \n     assertNotEquals(fateId1, fateId2);\n \n@@ -334,10 +337,10 @@ protected void testCreateWithKeyDuplicate(FateStore<TestEnv> store, ServerContex\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n \n     FateKey fateKey = FateKey.forSplit(ke);\n-    var fateId = store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n+    var fateId = seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n \n     // second call is empty\n-    assertTrue(store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n+    assertTrue(seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n     assertFalse(store.seedTransaction(TEST_FATE_OP, fateId, new TestRepo(), true));\n \n     var txStore = store.reserve(fateId);\n@@ -363,7 +366,7 @@ protected void testCreateWithKeyInProgress(FateStore<TestEnv> store, ServerConte\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n     FateKey fateKey = FateKey.forSplit(ke);\n \n-    var fateId = store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n+    var fateId = seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n \n     var txStore = store.reserve(fateId);\n     try {\n@@ -372,7 +375,7 @@ protected void testCreateWithKeyInProgress(FateStore<TestEnv> store, ServerConte\n \n       // We have an existing transaction with the same key in progress\n       // so should return an empty Optional\n-      assertTrue(store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n+      assertTrue(seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n       assertEquals(TStatus.IN_PROGRESS, txStore.getStatus());\n     } finally {\n       txStore.setStatus(TStatus.SUCCESSFUL);\n@@ -385,7 +388,7 @@ protected void testCreateWithKeyInProgress(FateStore<TestEnv> store, ServerConte\n     try {\n       // After deletion, make sure we can create again with the same key\n       var fateId2 =\n-          store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n+          seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n       txStore = store.reserve(fateId);\n       assertEquals(fateId, fateId2);\n       assertTrue(txStore.timeCreated() > 0);\n@@ -426,10 +429,11 @@ protected void testCreateWithKeyCollision(FateStore<TestEnv> store, ServerContex\n     FateKey fateKey1 = FateKey.forSplit(ke1);\n     FateKey fateKey2 = FateKey.forSplit(ke2);\n \n-    var fateId1 = store.seedTransaction(TEST_FATE_OP, fateKey1, new TestRepo(), true).orElseThrow();\n+    var fateId1 =\n+        seedTransaction(store, TEST_FATE_OP, fateKey1, new TestRepo(), true).orElseThrow();\n     var txStore = store.reserve(fateId1);\n     try {\n-      assertTrue(store.seedTransaction(TEST_FATE_OP, fateKey2, new TestRepo(), true).isEmpty());\n+      assertTrue(seedTransaction(store, TEST_FATE_OP, fateKey2, new TestRepo(), true).isEmpty());\n       assertEquals(fateKey1, txStore.getKey().orElseThrow());\n     } finally {\n       txStore.delete();\n@@ -449,14 +453,14 @@ protected void testCollisionWithRandomFateId(FateStore<TestEnv> store, ServerCon\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n \n     FateKey fateKey = FateKey.forSplit(ke);\n-    var fateId = store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n+    var fateId = seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n \n     // After seeding a fate transaction using a key we can simulate a collision with\n     // a random FateId by deleting the key out of Fate and calling seed again to\n     // verify it detects the key is missing. Then we can continue and see if we can still use\n     // the existing transaction.\n     deleteKey(fateId, sctx);\n-    assertTrue(store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n+    assertTrue(seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).isEmpty());\n \n     var txStore = store.reserve(fateId);\n     // We should still be able to use the existing transaction\n@@ -603,7 +607,7 @@ protected void testConcurrent(FateStore<TestEnv> store, ServerContext sctx) thro\n       List<Future<Optional<FateId>>> futures = new ArrayList<>(10);\n       for (int i = 0; i < 10; i++) {\n         futures.add(executor\n-            .submit(() -> store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true)));\n+            .submit(() -> seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true)));\n       }\n \n       int idsSeen = 0;\n@@ -685,7 +689,8 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n \n     Map<FateKey,FateId> fateKeyIds = new HashMap<>();\n     for (FateKey fateKey : List.of(fateKey1, fateKey2, fateKey3, fateKey4)) {\n-      var fateId = store.seedTransaction(TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n+      var fateId =\n+          seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n       fateKeyIds.put(fateKey, fateId);\n     }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\nindex 6c7b482d884..0e4c1800bfc 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\n@@ -22,7 +22,9 @@\n import static org.junit.jupiter.api.Assertions.assertEquals;\n \n import java.io.File;\n+import java.util.Optional;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n@@ -31,6 +33,10 @@\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateKey;\n+import org.apache.accumulo.core.fate.FateStore;\n+import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.metadata.AccumuloTable;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.test.zookeeper.ZooKeeperTestingServer;\n@@ -72,6 +78,22 @@ public static void createFateTable(ClientContext client, String table) throws Ex\n     assertEquals(fateTableProps, testFateTableProps);\n   }\n \n+  // For now just process one at a time as the current impl completes\n+  // each seed transaction individually. In future versions we can test\n+  // batching multiple seeding atempts together.\n+  public static <T> Optional<FateId> seedTransaction(FateStore<T> store, Fate.FateOperation fateOp,\n+      FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+    CompletableFuture<Optional<FateId>> fateIdFuture;\n+    try (var seeder = store.beginSeeding()) {\n+      fateIdFuture = seeder.attemptToSeedTransaction(fateOp, fateKey, repo, autoCleanUp);\n+    }\n+    try {\n+      return fateIdFuture.get();\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n   /**\n    * Contains the necessary utilities for setting up (and shutting down) a ZooKeeper instance for\n    * use in testing MetaFateStore\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5353"", ""pr_id"": 5353, ""issue_id"": 5014, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Add identifier to split points for automated vs user created split points\n**Is your feature request related to a problem? Please describe.**\r\nWhen split points are automatically created due to tablet sizes, these split points persist until a user runs a merge. \r\nIf a merge is run automatically, then it might remove split points that were purposely set by the user.\r\n\r\n**Describe the solution you'd like**\r\nSplit points created automatically by accumulo should be identified as such so they can easily be removed."", ""issue_word_count"": 84, ""test_files_count"": 8, ""non_test_files_count"": 17, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java"", ""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/merge/FindMergeableRangeTask.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/CountFiles.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeInfo.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/TableRangeOp.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/UnreserveSystemMerge.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/VerifyMergeability.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader12to13.java"", ""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java"", ""test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/TabletMergeabilityIT.java""], ""pr_changed_test_files"": [""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java"", ""test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/TabletMergeabilityIT.java""], ""base_commit"": ""0c79e44e2df29f8f9d8617fb698167a60c0512bb"", ""head_commit"": ""3afdb84a702f86d6e54d5042f6bb59e7c8fcb896"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5353"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5353"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-16T19:37:49.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\nindex 8545cd38982..739fc3ee706 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.accumulo.core.client.admin;\n \n+import java.time.Duration;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n@@ -357,4 +358,14 @@ boolean testClassLoad(final String className, final String asTypeName)\n    * @since 2.1.0\n    */\n   InstanceId getInstanceId();\n+\n+  /**\n+   * Return the current manager time. This duration represents the amount of time an accumulo\n+   * manager process has been running. The duration is persisted and should only increase over the\n+   * lifetime of an Accumulo instance.\n+   *\n+   * @return current time\n+   * @since 4.0.0\n+   */\n+  Duration getManagerTime() throws AccumuloException, AccumuloSecurityException;\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\nindex 7c22b7d8a87..fb2c5878f85 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n@@ -470,6 +470,12 @@ public InstanceId getInstanceId() {\n     return context.getInstanceID();\n   }\n \n+  @Override\n+  public Duration getManagerTime() throws AccumuloException, AccumuloSecurityException {\n+    return Duration.ofNanos(ThriftClientTypes.MANAGER.execute(context,\n+        client -> client.getManagerTimeNanos(TraceUtil.traceInfo(), context.rpcCreds())));\n+  }\n+\n   @Override\n   public ServerId getServer(ServerId.Type type, String resourceGroup, String host, int port) {\n     Objects.requireNonNull(type, \""type parameter cannot be null\"");\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 4f3b8fb984e..3be26b2f195 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -401,6 +401,11 @@ public enum Property {\n           + \"" are performed (e.g. Bulk Import). This property specifies the maximum number of threads in a\""\n           + \"" ThreadPool in the Manager that will be used to request these refresh operations.\"",\n       \""4.0.0\""),\n+  MANAGER_TABLET_MERGEABILITY_INTERVAL(\""manager.tablet.mergeability.interval\"", \""24h\"",\n+      PropertyType.TIMEDURATION,\n+      \""Time to wait between scanning tables to identify ranges of tablets that can be \""\n+          + \"" auto-merged. Valid ranges will be have merge fate ops submitted.\"",\n+      \""4.0.0\""),\n   MANAGER_BULK_TIMEOUT(\""manager.bulk.timeout\"", \""5m\"", PropertyType.TIMEDURATION,\n       \""The time to wait for a tablet server to process a bulk import request.\"", \""1.4.3\""),\n   MANAGER_RENAME_THREADS(\""manager.rename.threadpool.size\"", \""20\"", PropertyType.COUNT,\n@@ -906,6 +911,9 @@ public enum Property {\n   TABLE_ONDEMAND_UNLOADER(\""tserver.ondemand.tablet.unloader\"",\n       \""org.apache.accumulo.core.spi.ondemand.DefaultOnDemandTabletUnloader\"", PropertyType.CLASSNAME,\n       \""The class that will be used to determine which on-demand Tablets to unload.\"", \""4.0.0\""),\n+  TABLE_MAX_MERGEABILITY_THRESHOLD(\""table.mergeability.threshold\"", \"".25\"", PropertyType.FRACTION,\n+      \""A range of tablets are eligible for automatic merging until the combined size of RFiles reaches this percentage of the split threshold.\"",\n+      \""4.0.0\""),\n \n   // Crypto-related properties\n   @Experimental\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex dd521e3a0c7..28a92d4204b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -106,6 +106,7 @@ public enum FateOperation {\n     NAMESPACE_RENAME(TFateOperation.NAMESPACE_RENAME),\n     SHUTDOWN_TSERVER(null),\n     SYSTEM_SPLIT(null),\n+    SYSTEM_MERGE(null),\n     TABLE_BULK_IMPORT2(TFateOperation.TABLE_BULK_IMPORT2),\n     TABLE_CANCEL_COMPACT(TFateOperation.TABLE_CANCEL_COMPACT),\n     TABLE_CLONE(TFateOperation.TABLE_CLONE),\n@@ -124,7 +125,7 @@ public enum FateOperation {\n \n     private final TFateOperation top;\n     private static final EnumSet<FateOperation> nonThriftOps =\n-        EnumSet.of(COMMIT_COMPACTION, SHUTDOWN_TSERVER, SYSTEM_SPLIT);\n+        EnumSet.of(COMMIT_COMPACTION, SHUTDOWN_TSERVER, SYSTEM_SPLIT, SYSTEM_MERGE);\n \n     FateOperation(TFateOperation top) {\n       this.top = top;\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateKey.java b/core/src/main/java/org/apache/accumulo/core/fate/FateKey.java\nindex 8942149a6ff..017ac7955c9 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateKey.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateKey.java\n@@ -119,8 +119,12 @@ public static FateKey forCompactionCommit(ExternalCompactionId compactionId) {\n     return new FateKey(FateKeyType.COMPACTION_COMMIT, compactionId);\n   }\n \n+  public static FateKey forMerge(KeyExtent extent) {\n+    return new FateKey(FateKeyType.MERGE, extent);\n+  }\n+\n   public enum FateKeyType {\n-    SPLIT, COMPACTION_COMMIT\n+    SPLIT, COMPACTION_COMMIT, MERGE\n   }\n \n   private static byte[] serialize(FateKeyType type, KeyExtent ke) {\n@@ -151,6 +155,7 @@ private static Optional<KeyExtent> deserializeKeyExtent(FateKeyType type, DataIn\n       throws IOException {\n     switch (type) {\n       case SPLIT:\n+      case MERGE:\n         return Optional.of(KeyExtent.readFrom(buffer));\n       case COMPACTION_COMMIT:\n         return Optional.empty();\n@@ -163,6 +168,7 @@ private static Optional<ExternalCompactionId> deserializeCompactionId(FateKeyTyp\n       DataInputBuffer buffer) throws IOException {\n     switch (type) {\n       case SPLIT:\n+      case MERGE:\n         return Optional.empty();\n       case COMPACTION_COMMIT:\n         return Optional.of(ExternalCompactionId.of(buffer.readUTF()));\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\nindex ef6536da098..e1801b6eca9 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n@@ -20,8 +20,10 @@\n \n import java.io.FileNotFoundException;\n import java.io.IOException;\n+import java.time.Duration;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.TreeMap;\n import java.util.stream.Collectors;\n \n@@ -48,6 +50,7 @@\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.spi.crypto.CryptoService;\n import org.apache.accumulo.core.spi.fs.VolumeChooserEnvironment;\n+import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.store.TablePropKey;\n import org.apache.accumulo.server.fs.VolumeChooserEnvironmentImpl;\n@@ -80,12 +83,15 @@ public static class InitialTablet {\n     final Text endRow;\n     final Text extent;\n     final String[] files;\n+    final TabletMergeabilityMetadata mergeability;\n \n-    InitialTablet(TableId tableId, String dirName, Text prevEndRow, Text endRow, String... files) {\n+    InitialTablet(TableId tableId, String dirName, Text prevEndRow, Text endRow,\n+        TabletMergeabilityMetadata mergeability, String... files) {\n       this.tableId = tableId;\n       this.dirName = dirName;\n       this.prevEndRow = prevEndRow;\n       this.endRow = endRow;\n+      this.mergeability = Objects.requireNonNull(mergeability);\n       this.files = files;\n       this.extent = new Text(MetadataSchema.TabletsSection.encodeRow(this.tableId, this.endRow));\n     }\n@@ -94,8 +100,8 @@ private Map<Key,Value> createEntries() {\n       KeyExtent keyExtent = new KeyExtent(tableId, endRow, prevEndRow);\n       var builder = TabletMetadata.builder(keyExtent).putDirName(dirName)\n           .putTime(new MetadataTime(0, TimeType.LOGICAL))\n-          .putTabletAvailability(TabletAvailability.HOSTED)\n-          .putTabletMergeability(TabletMergeabilityMetadata.never()).putPrevEndRow(prevEndRow);\n+          .putTabletAvailability(TabletAvailability.HOSTED).putTabletMergeability(mergeability)\n+          .putPrevEndRow(prevEndRow);\n       for (String file : files) {\n         builder.putFile(new ReferencedTabletFile(new Path(file)).insert(), new DataFileValue(0, 0));\n       }\n@@ -152,8 +158,12 @@ void initialize(VolumeManager fs, String rootTabletDirUri, String rootTabletFile\n     createDirectories(fs, rootTabletDirUri, tableMetadataTabletDirUri, defaultMetadataTabletDirUri,\n         fateTableDefaultTabletDirUri, scanRefTableDefaultTabletDirUri);\n \n-    InitialTablet fateTablet = createFateRefTablet(context);\n-    InitialTablet scanRefTablet = createScanRefTablet(context);\n+    // For a new system mark the fate tablet and scan ref tablet as always mergeable.\n+    // Because this is a new system we can just use 0 for the time as that is what the Manager\n+    // will initialize with when starting\n+    var always = TabletMergeabilityMetadata.always(SteadyTime.from(Duration.ZERO));\n+    InitialTablet fateTablet = createFateRefTablet(context, always);\n+    InitialTablet scanRefTablet = createScanRefTablet(context, always);\n \n     // populate the metadata tablet with info about the fate and scan ref tablets\n     String ext = FileOperations.getNewFileExtension(DefaultConfiguration.getInstance());\n@@ -161,11 +171,13 @@ void initialize(VolumeManager fs, String rootTabletDirUri, String rootTabletFile\n     createMetadataFile(fs, metadataFileName, fateTablet, scanRefTablet);\n \n     // populate the root tablet with info about the metadata table's two initial tablets\n-    InitialTablet tablesTablet =\n-        new InitialTablet(AccumuloTable.METADATA.tableId(), TABLE_TABLETS_TABLET_DIR, null,\n-            SPLIT_POINT, StoredTabletFile.of(new Path(metadataFileName)).getMetadataPath());\n+    // For the default tablet we want to make that mergeable, but don't make the TabletsSection\n+    // tablet mergeable. This will prevent tablets from each either from being auto merged\n+    InitialTablet tablesTablet = new InitialTablet(AccumuloTable.METADATA.tableId(),\n+        TABLE_TABLETS_TABLET_DIR, null, SPLIT_POINT, TabletMergeabilityMetadata.never(),\n+        StoredTabletFile.of(new Path(metadataFileName)).getMetadataPath());\n     InitialTablet defaultTablet = new InitialTablet(AccumuloTable.METADATA.tableId(),\n-        defaultMetadataTabletDirName, SPLIT_POINT, null);\n+        defaultMetadataTabletDirName, SPLIT_POINT, null, always);\n     createMetadataFile(fs, rootTabletFileUri, tablesTablet, defaultTablet);\n   }\n \n@@ -231,18 +243,22 @@ private void createMetadataFile(VolumeManager volmanager, String fileName,\n     tabletWriter.close();\n   }\n \n-  public InitialTablet createScanRefTablet(ServerContext context) throws IOException {\n+  public InitialTablet createScanRefTablet(ServerContext context,\n+      TabletMergeabilityMetadata mergeability) throws IOException {\n     setTableProperties(context, AccumuloTable.SCAN_REF.tableId(), initConfig.getScanRefTableConf());\n \n     return new InitialTablet(AccumuloTable.SCAN_REF.tableId(),\n-        MetadataSchema.TabletsSection.ServerColumnFamily.DEFAULT_TABLET_DIR_NAME, null, null);\n+        MetadataSchema.TabletsSection.ServerColumnFamily.DEFAULT_TABLET_DIR_NAME, null, null,\n+        mergeability);\n   }\n \n-  public InitialTablet createFateRefTablet(ServerContext context) throws IOException {\n+  public InitialTablet createFateRefTablet(ServerContext context,\n+      TabletMergeabilityMetadata mergeability) throws IOException {\n     setTableProperties(context, AccumuloTable.FATE.tableId(), initConfig.getFateTableConf());\n \n     return new InitialTablet(AccumuloTable.FATE.tableId(),\n-        MetadataSchema.TabletsSection.ServerColumnFamily.DEFAULT_TABLET_DIR_NAME, null, null);\n+        MetadataSchema.TabletsSection.ServerColumnFamily.DEFAULT_TABLET_DIR_NAME, null, null,\n+        mergeability);\n   }\n \n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java b/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\nindex 4efb6025204..0dac7412cf2 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\n@@ -258,7 +258,8 @@ public void executeFateOperation(TInfo tinfo, TCredentials c, TFateId opid, TFat\n         manager.fate(type).seedTransaction(op, fateId,\n             new TraceRepo<>(new CreateTable(c.getPrincipal(), tableName, timeType, options,\n                 splitsPath, splitCount, splitsDirsPath, initialTableState,\n-                initialTabletAvailability, namespaceId, TabletMergeability.never())),\n+                // Set the default tablet to be auto-mergeable with other tablets if it is split\n+                initialTabletAvailability, namespaceId, TabletMergeability.always())),\n             autoCleanup, goalMessage);\n \n         break;\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex a3eb271b37b..f5fc39fda0e 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -123,6 +123,7 @@\n import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.core.zookeeper.ZcStat;\n import org.apache.accumulo.manager.compaction.coordinator.CompactionCoordinator;\n+import org.apache.accumulo.manager.merge.FindMergeableRangeTask;\n import org.apache.accumulo.manager.metrics.BalancerMetrics;\n import org.apache.accumulo.manager.metrics.ManagerMetrics;\n import org.apache.accumulo.manager.recovery.RecoveryManager;\n@@ -1372,6 +1373,12 @@ boolean canSuspendTablets() {\n     ThreadPools.watchCriticalScheduledTask(context.getScheduledExecutor()\n         .scheduleWithFixedDelay(() -> ScanServerMetadataEntries.clean(context), 10, 10, MINUTES));\n \n+    var tabletMergeabilityInterval =\n+        getConfiguration().getDuration(Property.MANAGER_TABLET_MERGEABILITY_INTERVAL);\n+    ThreadPools.watchCriticalScheduledTask(context.getScheduledExecutor().scheduleWithFixedDelay(\n+        new FindMergeableRangeTask(this), tabletMergeabilityInterval.toMillis(),\n+        tabletMergeabilityInterval.toMillis(), MILLISECONDS));\n+\n     // Make sure that we have a secret key (either a new one or an old one from ZK) before we start\n     // the manager client service.\n     Thread authenticationTokenKeyManagerThread = null;\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/merge/FindMergeableRangeTask.java b/server/manager/src/main/java/org/apache/accumulo/manager/merge/FindMergeableRangeTask.java\nnew file mode 100644\nindex 00000000000..a55fded16d4\n--- /dev/null\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/merge/FindMergeableRangeTask.java\n@@ -0,0 +1,293 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.manager.merge;\n+\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n+import static org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason.MAX_FILE_COUNT;\n+import static org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason.MAX_TOTAL_SIZE;\n+import static org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason.NOT_CONTIGUOUS;\n+import static org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason.TABLET_MERGEABILITY;\n+\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.NamespaceId;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.dataImpl.KeyExtent;\n+import org.apache.accumulo.core.fate.Fate.FateOperation;\n+import org.apache.accumulo.core.fate.FateInstanceType;\n+import org.apache.accumulo.core.fate.FateKey;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n+import org.apache.accumulo.core.metadata.schema.filters.TabletMetadataFilter;\n+import org.apache.accumulo.core.util.time.SteadyTime;\n+import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.tableOps.TraceRepo;\n+import org.apache.accumulo.manager.tableOps.merge.MergeInfo.Operation;\n+import org.apache.accumulo.manager.tableOps.merge.TableRangeOp;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.io.Text;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Sets;\n+\n+/**\n+ * This task is used to scan tables to find ranges of tablets that can be merged together\n+ * automatically.\n+ */\n+public class FindMergeableRangeTask implements Runnable {\n+\n+  private static final Logger log = LoggerFactory.getLogger(FindMergeableRangeTask.class);\n+\n+  private static final TabletMergeabilityFilter FILTER = new TabletMergeabilityFilter();\n+\n+  private final Manager manager;\n+\n+  public FindMergeableRangeTask(Manager manager) {\n+    this.manager = Objects.requireNonNull(manager);\n+    log.debug(\""Creating FindMergeableRangeTask\"");\n+  }\n+\n+  @Override\n+  public void run() {\n+    var context = manager.getContext();\n+    Map<TableId,String> tables = context.getTableIdToNameMap();\n+\n+    log.debug(\""Starting FindMergeableRangeTask\"");\n+\n+    for (Entry<TableId,String> table : tables.entrySet()) {\n+      TableId tableId = table.getKey();\n+      String tableName = table.getValue();\n+\n+      // Read the table configuration to compute the max total file size of a mergeable range.\n+      // The max size is a percentage of the configured split threshold and we do not want\n+      // to exceed this limit.\n+      long threshold =\n+          context.getTableConfiguration(tableId).getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n+      double mergeabilityThreshold = context.getTableConfiguration(tableId)\n+          .getFraction(Property.TABLE_MAX_MERGEABILITY_THRESHOLD);\n+      if (mergeabilityThreshold <= 0) {\n+        log.trace(\""Skipping FindMergeableRangeTask for table {}, {}} is set to {}\"", tableName,\n+            Property.TABLE_MAX_MERGEABILITY_THRESHOLD.getKey(), mergeabilityThreshold);\n+        continue;\n+      }\n+\n+      long maxFileCount =\n+          context.getTableConfiguration(tableId).getCount(Property.TABLE_MERGE_FILE_MAX);\n+      long maxTotalSize = (long) (threshold * mergeabilityThreshold);\n+\n+      log.debug(\""Checking {} for tablets that can be merged\"", tableName);\n+      log.trace(\""maxFileCount: {}, maxTotalSize:{}, splitThreshold:{}, mergeabilityThreshold:{}\"",\n+          maxFileCount, maxTotalSize, threshold, mergeabilityThreshold);\n+      try {\n+        NamespaceId namespaceId = context.getNamespaceId(tableId);\n+        var type = FateInstanceType.fromTableId(tableId);\n+\n+        try (var tablets = context.getAmple().readTablets().forTable(tableId)\n+            .fetch(PREV_ROW, FILES, MERGEABILITY).filter(FILTER).build()) {\n+\n+          final MergeableRange current =\n+              new MergeableRange(tableId, manager.getSteadyTime(), maxFileCount, maxTotalSize);\n+\n+          for (var tm : tablets) {\n+            log.trace(\""Checking tablet {}, {}\"", tm.getExtent(), tm.getTabletMergeability());\n+            // If there was an error adding the next tablet to the range then\n+            // the existing range is complete as we can't add more tablets so\n+            // submit a merge fate op and reset to find more merge ranges\n+            current.add(tm).ifPresent(error -> {\n+              submit(current, type, table, namespaceId);\n+              current.resetAndAdd(tm);\n+            });\n+          }\n+\n+          // Try and submit any outstanding mergeable tablets\n+          submit(current, type, table, namespaceId);\n+        }\n+\n+      } catch (Exception e) {\n+        log.error(\""Failed to generate system merges for {}\"", tableName, e);\n+      }\n+    }\n+\n+  }\n+\n+  void submit(MergeableRange range, FateInstanceType type, Entry<TableId,String> table,\n+      NamespaceId namespaceId) {\n+    if (range.tabletCount < 2) {\n+      return;\n+    }\n+\n+    TableId tableId = table.getKey();\n+    String tableName = table.getValue();\n+\n+    log.debug(\""Table {} found {} tablets that can be merged for table\"", tableName,\n+        range.tabletCount);\n+\n+    final Text startRow = range.startRow != null ? range.startRow : new Text(\""\"");\n+    final Text endRow = range.endRow != null ? range.endRow : new Text(\""\"");\n+\n+    final String startRowStr = StringUtils.defaultIfBlank(startRow.toString(), \""-inf\"");\n+    final String endRowStr = StringUtils.defaultIfBlank(endRow.toString(), \""+inf\"");\n+    log.debug(\""FindMergeableRangeTask: Creating merge op: {} from startRow: {} to endRow: {}\"",\n+        tableId, startRowStr, endRowStr);\n+    var fateKey = FateKey.forMerge(new KeyExtent(tableId, range.endRow, range.startRow));\n+\n+    manager.fate(type).seedTransaction(FateOperation.SYSTEM_MERGE, fateKey,\n+        new TraceRepo<>(\n+            new TableRangeOp(Operation.SYSTEM_MERGE, namespaceId, tableId, startRow, endRow)),\n+        true);\n+  }\n+\n+  public enum UnmergeableReason {\n+    NOT_CONTIGUOUS, MAX_FILE_COUNT, MAX_TOTAL_SIZE, TABLET_MERGEABILITY;\n+\n+    // Cache the Optional() reason objects as we will re-use these over and over\n+    private static final Optional<UnmergeableReason> NOT_CONTIGUOUS_OPT =\n+        Optional.of(NOT_CONTIGUOUS);\n+    private static final Optional<UnmergeableReason> MAX_FILE_COUNT_OPT =\n+        Optional.of(MAX_FILE_COUNT);\n+    private static final Optional<UnmergeableReason> MAX_TOTAL_SIZE_OPT =\n+        Optional.of(MAX_TOTAL_SIZE);\n+    private static final Optional<UnmergeableReason> TABLET_MERGEABILITY_OPT =\n+        Optional.of(TABLET_MERGEABILITY);\n+\n+    public Optional<UnmergeableReason> optional() {\n+      switch (this) {\n+        case NOT_CONTIGUOUS:\n+          return NOT_CONTIGUOUS_OPT;\n+        case MAX_FILE_COUNT:\n+          return MAX_FILE_COUNT_OPT;\n+        case MAX_TOTAL_SIZE:\n+          return MAX_TOTAL_SIZE_OPT;\n+        case TABLET_MERGEABILITY:\n+          return TABLET_MERGEABILITY_OPT;\n+        default:\n+          throw new IllegalArgumentException(\""Unexpected enum type\"");\n+      }\n+    }\n+  }\n+\n+  public static class MergeableRange {\n+    final SteadyTime currentTime;\n+    final TableId tableId;\n+    final long maxFileCount;\n+    final long maxTotalSize;\n+\n+    Text startRow;\n+    Text endRow;\n+    int tabletCount;\n+    long totalFileCount;\n+    long totalFileSize;\n+\n+    public MergeableRange(TableId tableId, SteadyTime currentTime, long maxFileCount,\n+        long maxTotalSize) {\n+      this.tableId = tableId;\n+      this.currentTime = currentTime;\n+      this.maxFileCount = maxFileCount;\n+      this.maxTotalSize = maxTotalSize;\n+    }\n+\n+    public Optional<UnmergeableReason> add(TabletMetadata tm) {\n+      var failure = validate(tm);\n+      if (failure.isEmpty()) {\n+        tabletCount++;\n+        log.trace(\""Adding tablet {} to MergeableRange\"", tm.getExtent());\n+        if (tabletCount == 1) {\n+          startRow = tm.getPrevEndRow();\n+        }\n+        endRow = tm.getEndRow();\n+        totalFileCount += tm.getFiles().size();\n+        totalFileSize += tm.getFileSize();\n+      }\n+      return failure;\n+    }\n+\n+    private Optional<UnmergeableReason> validate(TabletMetadata tm) {\n+      Preconditions.checkArgument(tableId.equals(tm.getTableId()), \""Unexpected tableId seen %s\"",\n+          tm.getTableId());\n+\n+      if (tabletCount > 0) {\n+        // This is at least the second tablet seen so there should not be a null prevEndRow\n+        Preconditions.checkState(tm.getPrevEndRow() != null,\n+            \""Unexpected null prevEndRow found for %s\"", tm.getExtent());\n+        // If this is not the first tablet, then verify its prevEndRow matches\n+        // the last endRow tracked, the server filter will skip tablets marked as never\n+        if (!tm.getPrevEndRow().equals(endRow)) {\n+          return NOT_CONTIGUOUS.optional();\n+        }\n+      }\n+\n+      if (!tm.getTabletMergeability().isMergeable(currentTime)) {\n+        return TABLET_MERGEABILITY.optional();\n+      }\n+\n+      if (totalFileCount + tm.getFiles().size() > maxFileCount) {\n+        return MAX_FILE_COUNT.optional();\n+      }\n+\n+      if (totalFileSize + tm.getFileSize() > maxTotalSize) {\n+        return MAX_TOTAL_SIZE.optional();\n+      }\n+\n+      return Optional.empty();\n+    }\n+\n+    void resetAndAdd(TabletMetadata tm) {\n+      reset();\n+      add(tm);\n+    }\n+\n+    void reset() {\n+      startRow = null;\n+      endRow = null;\n+      tabletCount = 0;\n+      totalFileCount = 0;\n+      totalFileSize = 0;\n+    }\n+  }\n+\n+  // Filter out never merge tablets to cut down on what we need to check\n+  // We need steady time to check other tablets which is not available in the filter\n+  public static class TabletMergeabilityFilter extends TabletMetadataFilter {\n+\n+    public static final Set<ColumnType> COLUMNS = Sets.immutableEnumSet(MERGEABILITY);\n+\n+    private final static Predicate<TabletMetadata> IS_NOT_NEVER =\n+        tabletMetadata -> !tabletMetadata.getTabletMergeability().isNever();\n+\n+    @Override\n+    public Set<TabletMetadata.ColumnType> getColumns() {\n+      return COLUMNS;\n+    }\n+\n+    @Override\n+    protected Predicate<TabletMetadata> acceptTablet() {\n+      return IS_NOT_NEVER;\n+    }\n+  }\n+}\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/CountFiles.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/CountFiles.java\nindex 4083eecc864..f108419a513 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/CountFiles.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/CountFiles.java\n@@ -19,6 +19,7 @@\n package org.apache.accumulo.manager.tableOps.merge;\n \n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n+import static org.apache.accumulo.manager.tableOps.merge.MergeInfo.Operation.SYSTEM_MERGE;\n \n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.fate.FateId;\n@@ -28,6 +29,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Preconditions;\n+\n public class CountFiles extends ManagerRepo {\n   private static final Logger log = LoggerFactory.getLogger(CountFiles.class);\n   private static final long serialVersionUID = 1L;\n@@ -39,6 +42,9 @@ public CountFiles(MergeInfo mergeInfo) {\n \n   @Override\n   public Repo<Manager> call(FateId fateId, Manager env) throws Exception {\n+    // SYSTEM_MERGE should be executing VerifyMergeability repo, which already\n+    // will count files\n+    Preconditions.checkState(data.op != SYSTEM_MERGE, \""Unexpected op %s\"", SYSTEM_MERGE);\n \n     var range = data.getReserveExtent();\n \n@@ -75,10 +81,13 @@ public Repo<Manager> call(FateId fateId, Manager env) throws Exception {\n     if (totalFiles >= maxFiles) {\n       return new UnreserveAndError(data, totalFiles, maxFiles);\n     } else {\n-      if (data.op == MergeInfo.Operation.MERGE) {\n-        return new MergeTablets(data);\n-      } else {\n-        return new DeleteRows(data);\n+      switch (data.op) {\n+        case MERGE:\n+          return new MergeTablets(data);\n+        case DELETE:\n+          return new DeleteRows(data);\n+        default:\n+          throw new IllegalStateException(\""Unknown op \"" + data.op);\n       }\n     }\n   }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeInfo.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeInfo.java\nindex 0da5159b657..6b84e16e7f0 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeInfo.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeInfo.java\n@@ -19,6 +19,7 @@\n package org.apache.accumulo.manager.tableOps.merge;\n \n import java.io.Serializable;\n+import java.util.Objects;\n \n import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n@@ -37,7 +38,11 @@ public class MergeInfo implements Serializable {\n   private static final long serialVersionUID = 1L;\n \n   public enum Operation {\n-    MERGE, DELETE,\n+    MERGE, SYSTEM_MERGE, DELETE;\n+\n+    public boolean isMergeOp() {\n+      return this == MERGE || this == SYSTEM_MERGE;\n+    }\n   }\n \n   final TableId tableId;\n@@ -60,7 +65,7 @@ private MergeInfo(TableId tableId, NamespaceId namespaceId, byte[] startRow, byt\n     this.namespaceId = namespaceId;\n     this.startRow = startRow;\n     this.endRow = endRow;\n-    this.op = op;\n+    this.op = Objects.requireNonNull(op);\n     this.mergeRangeSet = mergeRange != null;\n     if (mergeRange != null) {\n       mergeStartRow =\n@@ -102,6 +107,7 @@ public KeyExtent getMergeExtent() {\n   public KeyExtent getReserveExtent() {\n     switch (op) {\n       case MERGE:\n+      case SYSTEM_MERGE:\n         return getOriginalExtent();\n       case DELETE: {\n         if (endRow == null) {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java\nindex ffaa6adcd25..9a8a4c3dc09 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/ReserveTablets.java\n@@ -128,6 +128,14 @@ public long isReady(FateId fateId, Manager env) throws Exception {\n \n   @Override\n   public Repo<Manager> call(FateId fateId, Manager environment) throws Exception {\n-    return new CountFiles(data);\n+    switch (data.op) {\n+      case SYSTEM_MERGE:\n+        return new VerifyMergeability(data);\n+      case MERGE:\n+      case DELETE:\n+        return new CountFiles(data);\n+      default:\n+        throw new IllegalStateException(\""Unknown op \"" + data.op);\n+    }\n   }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/TableRangeOp.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/TableRangeOp.java\nindex ddbbce5b7e4..ea63940db6a 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/TableRangeOp.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/TableRangeOp.java\n@@ -57,8 +57,7 @@ public TableRangeOp(MergeInfo.Operation op, NamespaceId namespaceId, TableId tab\n   @Override\n   public Repo<Manager> call(FateId fateId, Manager env) throws Exception {\n \n-    if (AccumuloTable.ROOT.tableId().equals(data.tableId)\n-        && MergeInfo.Operation.MERGE.equals(data.op)) {\n+    if (AccumuloTable.ROOT.tableId().equals(data.tableId) && data.op.isMergeOp()) {\n       log.warn(\""Attempt to merge tablets for {} does nothing. It is not splittable.\"",\n           AccumuloTable.ROOT.tableName());\n     }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/UnreserveSystemMerge.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/UnreserveSystemMerge.java\nnew file mode 100644\nindex 00000000000..a7201a06ab9\n--- /dev/null\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/UnreserveSystemMerge.java\n@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.manager.tableOps.merge;\n+\n+import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n+import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n+import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.Repo;\n+import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason;\n+import org.apache.accumulo.manager.tableOps.ManagerRepo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class UnreserveSystemMerge extends ManagerRepo {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger log = LoggerFactory.getLogger(UnreserveSystemMerge.class);\n+  private final MergeInfo mergeInfo;\n+  private final long maxFileCount;\n+  private final long maxTotalSize;\n+  private final UnmergeableReason reason;\n+\n+  public UnreserveSystemMerge(MergeInfo mergeInfo, UnmergeableReason reason, long maxFileCount,\n+      long maxTotalSize) {\n+    this.mergeInfo = mergeInfo;\n+    this.reason = reason;\n+    this.maxFileCount = maxFileCount;\n+    this.maxTotalSize = maxTotalSize;\n+  }\n+\n+  @Override\n+  public Repo<Manager> call(FateId fateId, Manager environment) throws Exception {\n+    FinishTableRangeOp.removeOperationIds(log, mergeInfo, fateId, environment);\n+    throw new AcceptableThriftTableOperationException(mergeInfo.tableId.toString(), null,\n+        mergeInfo.op.isMergeOp() ? TableOperation.MERGE : TableOperation.DELETE_RANGE,\n+        TableOperationExceptionType.OTHER, formatReason());\n+  }\n+\n+  public UnmergeableReason getReason() {\n+    return reason;\n+  }\n+\n+  private String formatReason() {\n+    switch (reason) {\n+      case MAX_FILE_COUNT:\n+        return \""Aborted merge because it would produce a tablet with more files than the configured limit of \""\n+            + maxFileCount;\n+      case MAX_TOTAL_SIZE:\n+        return \""Aborted merge because it would produce a tablet with a file size larger than the configured limit of \""\n+            + maxTotalSize;\n+      // This state should not happen as VerifyMergeability repo checks consistency but adding it\n+      // just in case\n+      case TABLET_MERGEABILITY:\n+        return \""Aborted merge because one ore more tablets in the merge range are unmergeable.\"";\n+      case NOT_CONTIGUOUS:\n+        return \""Aborted merge because the tablets in a range do not form a linked list.\"";\n+      default:\n+        throw new IllegalArgumentException(\""Unknown Reason\"");\n+    }\n+\n+  }\n+}\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/VerifyMergeability.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/VerifyMergeability.java\nnew file mode 100644\nindex 00000000000..43b6c1459eb\n--- /dev/null\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/VerifyMergeability.java\n@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.manager.tableOps.merge;\n+\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n+\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.Repo;\n+import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.merge.FindMergeableRangeTask.MergeableRange;\n+import org.apache.accumulo.manager.tableOps.ManagerRepo;\n+import org.apache.accumulo.manager.tableOps.merge.MergeInfo.Operation;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Preconditions;\n+\n+public class VerifyMergeability extends ManagerRepo {\n+  private static final Logger log = LoggerFactory.getLogger(VerifyMergeability.class);\n+  private static final long serialVersionUID = 1L;\n+  private final MergeInfo data;\n+\n+  public VerifyMergeability(MergeInfo mergeInfo) {\n+    this.data = mergeInfo;\n+    Preconditions.checkArgument(data.op == Operation.SYSTEM_MERGE, \""Must be a System Merge\"");\n+  }\n+\n+  @Override\n+  public Repo<Manager> call(FateId fateId, Manager env) throws Exception {\n+\n+    var range = data.getReserveExtent();\n+\n+    var currentTime = env.getSteadyTime();\n+    var context = env.getContext();\n+    var tableConf = context.getTableConfiguration(data.tableId);\n+    var splitThreshold = tableConf.getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n+    var maxMergeabilityThreshold = tableConf.getFraction(Property.TABLE_MAX_MERGEABILITY_THRESHOLD);\n+\n+    // max percentage of split threshold\n+    long maxTotalSize = (long) (splitThreshold * maxMergeabilityThreshold);\n+    long maxFileCount = env.getContext().getTableConfiguration(data.getOriginalExtent().tableId())\n+        .getCount(Property.TABLE_MERGE_FILE_MAX);\n+\n+    log.debug(\""Validating system merge for {} with range {}\"", fateId, range);\n+\n+    final var mr = new MergeableRange(data.tableId, currentTime, maxFileCount, maxTotalSize);\n+    try (var tablets = env.getContext().getAmple().readTablets().forTable(data.tableId)\n+        .overlapping(range.prevEndRow(), range.endRow()).fetch(FILES, MERGEABILITY)\n+        .checkConsistency().build()) {\n+\n+      for (var tabletMetadata : tablets) {\n+        var error = mr.add(tabletMetadata);\n+        if (error.isPresent()) {\n+          return new UnreserveSystemMerge(data, error.orElseThrow(), maxFileCount, maxTotalSize);\n+        }\n+      }\n+    }\n+\n+    return new MergeTablets(data);\n+  }\n+\n+}\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\nindex 89a0713ff0b..c95f5343661 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\n@@ -59,6 +59,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;\n import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.schema.Section;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.util.Encoding;\n@@ -289,7 +290,11 @@ public void createScanServerRefTable(ServerContext context) {\n     try {\n       FileSystemInitializer initializer = new FileSystemInitializer(\n           new InitialConfiguration(context.getHadoopConf(), context.getSiteConfiguration()));\n-      FileSystemInitializer.InitialTablet scanRefTablet = initializer.createScanRefTablet(context);\n+      // For upgrading an existing system set to never merge. If the mergeability is changed\n+      // then we would look to use the thrift client to look up the current Manager time to\n+      // set as part of the mergeability metadata\n+      FileSystemInitializer.InitialTablet scanRefTablet =\n+          initializer.createScanRefTablet(context, TabletMergeabilityMetadata.never());\n       // Add references to the Metadata Table\n       try (BatchWriter writer = context.createBatchWriter(AccumuloTable.METADATA.tableName())) {\n         writer.addMutation(scanRefTablet.createMutation());\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader12to13.java b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader12to13.java\nindex 3c7736d2a95..44b516ea3f0 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader12to13.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader12to13.java\n@@ -49,6 +49,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;\n import org.apache.accumulo.core.metadata.schema.RootTabletMetadata;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.schema.Section;\n@@ -134,8 +135,11 @@ private void createFateTable(ServerContext context) {\n     try {\n       FileSystemInitializer initializer = new FileSystemInitializer(\n           new InitialConfiguration(context.getHadoopConf(), context.getSiteConfiguration()));\n+      // For upgrading an existing system set to never merge. If the mergeability is changed\n+      // then we would look to use the thrift client to look up the current Manager time to\n+      // set as part of the mergeability metadata\n       FileSystemInitializer.InitialTablet fateTableTableTablet =\n-          initializer.createFateRefTablet(context);\n+          initializer.createFateRefTablet(context, TabletMergeabilityMetadata.never());\n       // Add references to the Metadata Table\n       try (BatchWriter writer = context.createBatchWriter(AccumuloTable.METADATA.tableName())) {\n         writer.addMutation(fateTableTableTablet.createMutation());\n"", ""test_patch"": ""diff --git a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java\nindex 2f5ec912fb7..8b4283f31db 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleUtil.java\n@@ -20,6 +20,9 @@\n \n import static org.apache.accumulo.test.ample.metadata.TestAmple.testAmpleServerContext;\n \n+import java.time.Duration;\n+\n+import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.manager.Manager;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.ample.metadata.TestAmple.TestServerAmpleImpl;\n@@ -35,4 +38,14 @@ public static Manager mockWithAmple(ServerContext context, TestServerAmpleImpl a\n     return manager;\n   }\n \n+  public static Manager mockWithAmple(ServerContext context, TestServerAmpleImpl ample,\n+      Duration currentTime) {\n+    Manager manager = EasyMock.mock(Manager.class);\n+    EasyMock.expect(manager.getContext()).andReturn(testAmpleServerContext(context, ample))\n+        .atLeastOnce();\n+    EasyMock.expect(manager.getSteadyTime()).andReturn(SteadyTime.from(currentTime)).anyTimes();\n+    EasyMock.replay(manager);\n+    return manager;\n+  }\n+\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java b/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\nindex 4d8f8c44fd4..4fb1a27d52a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\n@@ -187,12 +187,13 @@ public void createMetadataFromExisting(AccumuloClient client, TableId tableId,\n                 WholeRowIterator.decodeRow(entry.getKey(), entry.getValue());\n             Text row = decodedRow.firstKey().getRow();\n             Mutation m = new Mutation(row);\n-\n             decodedRow.entrySet().stream().filter(e -> includeColumn.test(e.getKey(), e.getValue()))\n                 .forEach(e -> m.put(e.getKey().getColumnFamily(), e.getKey().getColumnQualifier(),\n                     e.getKey().getColumnVisibilityParsed(), e.getKey().getTimestamp(),\n                     e.getValue()));\n-            bw.addMutation(m);\n+            if (!m.getUpdates().isEmpty()) {\n+              bw.addMutation(m);\n+            }\n           }\n         }\n       }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\nindex 27de329921e..644a2909a31 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n@@ -31,6 +31,7 @@\n \n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.EnumSet;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -52,6 +53,7 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n+import org.apache.accumulo.core.fate.FateKey.FateKeyType;\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.FateIdStatus;\n@@ -619,7 +621,9 @@ protected void testConcurrent(FateStore<TestEnv> store, ServerContext sctx) thro\n \n       assertEquals(1, idsSeen);\n       assertEquals(1, store.list(FateKey.FateKeyType.SPLIT).count());\n-      assertEquals(0, store.list(FateKey.FateKeyType.COMPACTION_COMMIT).count());\n+      // All other types should be a count of 0\n+      Arrays.stream(FateKeyType.values()).filter(t -> !t.equals(FateKey.FateKeyType.SPLIT))\n+          .forEach(t -> assertEquals(0, store.list(t).count()));\n \n       for (var future : futures) {\n         if (future.get().isPresent()) {\n@@ -632,8 +636,9 @@ protected void testConcurrent(FateStore<TestEnv> store, ServerContext sctx) thro\n         }\n       }\n \n-      assertEquals(0, store.list(FateKey.FateKeyType.SPLIT).count());\n-      assertEquals(0, store.list(FateKey.FateKeyType.COMPACTION_COMMIT).count());\n+      // All types should be a count of 0\n+      assertTrue(\n+          Arrays.stream(FateKeyType.values()).allMatch(t -> store.list(t).findAny().isEmpty()));\n \n     } finally {\n       executor.shutdown();\n@@ -676,6 +681,7 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n     TableId tid1 = TableId.of(\""test\"");\n     var extent1 = new KeyExtent(tid1, new Text(\""m\""), null);\n     var extent2 = new KeyExtent(tid1, null, new Text(\""m\""));\n+    var extent3 = new KeyExtent(tid1, new Text(\""z\""), new Text(\""m\""));\n     var fateKey1 = FateKey.forSplit(extent1);\n     var fateKey2 = FateKey.forSplit(extent2);\n \n@@ -687,8 +693,12 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n     var fateKey3 = FateKey.forCompactionCommit(cid1);\n     var fateKey4 = FateKey.forCompactionCommit(cid2);\n \n+    // use one overlapping extent and one different\n+    var fateKey5 = FateKey.forMerge(extent1);\n+    var fateKey6 = FateKey.forMerge(extent3);\n+\n     Map<FateKey,FateId> fateKeyIds = new HashMap<>();\n-    for (FateKey fateKey : List.of(fateKey1, fateKey2, fateKey3, fateKey4)) {\n+    for (FateKey fateKey : List.of(fateKey1, fateKey2, fateKey3, fateKey4, fateKey5, fateKey6)) {\n       var fateId =\n           seedTransaction(store, TEST_FATE_OP, fateKey, new TestRepo(), true).orElseThrow();\n       fateKeyIds.put(fateKey, fateId);\n@@ -698,10 +708,10 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n     allIds.addAll(fateKeyIds.values());\n     allIds.add(id1);\n     assertEquals(allIds, store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n-    assertEquals(5, allIds.size());\n+    assertEquals(7, allIds.size());\n \n-    assertEquals(4, fateKeyIds.size());\n-    assertEquals(4, fateKeyIds.values().stream().distinct().count());\n+    assertEquals(6, fateKeyIds.size());\n+    assertEquals(6, fateKeyIds.values().stream().distinct().count());\n \n     HashSet<KeyExtent> seenExtents = new HashSet<>();\n     store.list(FateKey.FateKeyType.SPLIT).forEach(fateKey -> {\n@@ -709,9 +719,18 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n       assertNotNull(fateKeyIds.remove(fateKey));\n       assertTrue(seenExtents.add(fateKey.getKeyExtent().orElseThrow()));\n     });\n+    assertEquals(4, fateKeyIds.size());\n+    assertEquals(Set.of(extent1, extent2), seenExtents);\n \n+    // clear set as one overlaps\n+    seenExtents.clear();\n+    store.list(FateKeyType.MERGE).forEach(fateKey -> {\n+      assertEquals(FateKey.FateKeyType.MERGE, fateKey.getType());\n+      assertNotNull(fateKeyIds.remove(fateKey));\n+      assertTrue(seenExtents.add(fateKey.getKeyExtent().orElseThrow()));\n+    });\n     assertEquals(2, fateKeyIds.size());\n-    assertEquals(Set.of(extent1, extent2), seenExtents);\n+    assertEquals(Set.of(extent1, extent3), seenExtents);\n \n     HashSet<ExternalCompactionId> seenCids = new HashSet<>();\n     store.list(FateKey.FateKeyType.COMPACTION_COMMIT).forEach(fateKey -> {\n@@ -722,6 +741,7 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n \n     assertEquals(0, fateKeyIds.size());\n     assertEquals(Set.of(cid1, cid2), seenCids);\n+\n     // Cleanup so we don't interfere with other tests\n     store.list()\n         .forEach(fateIdStatus -> store.tryReserve(fateIdStatus.getFateId()).orElseThrow().delete());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\nindex 617eba0ba16..59548cc7384 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n@@ -25,18 +25,22 @@\n import static org.apache.accumulo.test.ample.metadata.TestAmple.not;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.assertNoCompactionMetadata;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertInstanceOf;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n import static org.junit.jupiter.api.Assertions.assertNull;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.nio.file.Path;\n+import java.time.Duration;\n import java.util.Arrays;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n@@ -45,6 +49,7 @@\n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.client.BatchWriter;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n+import org.apache.accumulo.core.client.admin.TabletMergeability;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.clientImpl.TabletMergeabilityUtil;\n import org.apache.accumulo.core.clientImpl.thrift.ThriftTableOperationException;\n@@ -59,6 +64,7 @@\n import org.apache.accumulo.core.metadata.schema.Ample;\n import org.apache.accumulo.core.metadata.schema.Ample.DataLevel;\n import org.apache.accumulo.core.metadata.schema.Ample.TabletsMutator;\n+import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SplitColumnFamily;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n@@ -70,6 +76,7 @@\n import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.merge.FindMergeableRangeTask.UnmergeableReason;\n import org.apache.accumulo.manager.tableOps.ManagerRepo;\n import org.apache.accumulo.manager.tableOps.compact.CompactionDriver;\n import org.apache.accumulo.manager.tableOps.merge.DeleteRows;\n@@ -77,10 +84,13 @@\n import org.apache.accumulo.manager.tableOps.merge.MergeInfo.Operation;\n import org.apache.accumulo.manager.tableOps.merge.MergeTablets;\n import org.apache.accumulo.manager.tableOps.merge.ReserveTablets;\n+import org.apache.accumulo.manager.tableOps.merge.UnreserveSystemMerge;\n+import org.apache.accumulo.manager.tableOps.merge.VerifyMergeability;\n import org.apache.accumulo.manager.tableOps.split.AllocateDirsAndEnsureOnline;\n import org.apache.accumulo.manager.tableOps.split.FindSplits;\n import org.apache.accumulo.manager.tableOps.split.PreSplit;\n import org.apache.accumulo.manager.tableOps.split.SplitInfo;\n+import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.LargeSplitRowIT;\n import org.apache.accumulo.test.ample.metadata.TestAmple;\n import org.apache.accumulo.test.ample.metadata.TestAmple.TestServerAmpleImpl;\n@@ -107,7 +117,7 @@ public static void teardown() {\n   }\n \n   @ParameterizedTest\n-  @EnumSource(MergeInfo.Operation.class)\n+  @EnumSource(value = MergeInfo.Operation.class, names = {\""MERGE\"", \""DELETE\""})\n   public void testNoWalsMergeRepos(MergeInfo.Operation operation) throws Exception {\n     String[] tableNames = getUniqueNames(2);\n     String metadataTable = tableNames[0] + operation;\n@@ -163,6 +173,108 @@ public void testNoWalsMergeRepos(MergeInfo.Operation operation) throws Exception\n     }\n   }\n \n+  @Test\n+  public void testVerifyMergeability() throws Exception {\n+    String[] tableNames = getUniqueNames(2);\n+    String metadataTable = tableNames[0];\n+    String userTable = tableNames[1];\n+\n+    try (ClientContext client =\n+        (ClientContext) Accumulo.newClient().from(getClientProps()).build()) {\n+\n+      SortedMap<Text,TabletMergeability> splits = new TreeMap<>();\n+      splits.put(new Text(\""a\""), TabletMergeability.always());\n+      splits.put(new Text(\""b\""), TabletMergeability.always());\n+      splits.put(new Text(\""c\""), TabletMergeability.never());\n+      splits.put(new Text(\""d\""), TabletMergeability.after(Duration.ofDays(2)));\n+      splits.put(new Text(\""e\""), TabletMergeability.always());\n+\n+      client.tableOperations().create(userTable,\n+          new NewTableConfiguration().setProperties(Map.of(Property.TABLE_SPLIT_THRESHOLD.getKey(),\n+              \""10K\"", Property.TABLE_MAJC_RATIO.getKey(), \""9999\"",\n+              Property.TABLE_MERGE_FILE_MAX.getKey(), \""10\"")).withSplits(splits));\n+      TableId tableId = TableId.of(client.tableOperations().tableIdMap().get(userTable));\n+\n+      // Set up Test ample and manager\n+      TestAmple.createMetadataTable(client, metadataTable);\n+      TestServerAmpleImpl testAmple = (TestServerAmpleImpl) TestAmple\n+          .create(getCluster().getServerContext(), Map.of(DataLevel.USER, metadataTable));\n+      testAmple.createMetadataFromExisting(client, tableId);\n+      Manager manager =\n+          mockWithAmple(getCluster().getServerContext(), testAmple, Duration.ofDays(1));\n+\n+      // Create a test fate id\n+      var fateId = FateId.from(FateInstanceType.USER, UUID.randomUUID());\n+\n+      // Tablet c is set to never merge\n+      MergeInfo mergeInfo = new MergeInfo(tableId, manager.getContext().getNamespaceId(tableId),\n+          null, new Text(\""c\"").getBytes(), Operation.SYSTEM_MERGE);\n+      var repo = new VerifyMergeability(mergeInfo).call(fateId, manager);\n+      assertInstanceOf(UnreserveSystemMerge.class, repo);\n+      assertEquals(UnmergeableReason.TABLET_MERGEABILITY,\n+          ((UnreserveSystemMerge) repo).getReason());\n+\n+      // Tablets a and b are always merge\n+      mergeInfo = new MergeInfo(tableId, manager.getContext().getNamespaceId(tableId), null,\n+          new Text(\""b\"").getBytes(), Operation.SYSTEM_MERGE);\n+      assertInstanceOf(MergeTablets.class, new VerifyMergeability(mergeInfo).call(fateId, manager));\n+\n+      var context = manager.getContext();\n+\n+      // split threshold is 10k so default max merge size is 2500 bytes.\n+      // this adds 6 files of 450 each which puts the tablets over teh 2500 threshold\n+      addFileMetadata(context, tableId, null, new Text(\""c\""), 3, 450);\n+\n+      // Data written to the first two tablets totals 2700 bytes and is too large\n+      repo = new VerifyMergeability(mergeInfo).call(fateId, manager);\n+      assertInstanceOf(UnreserveSystemMerge.class, repo);\n+      assertEquals(UnmergeableReason.MAX_TOTAL_SIZE, ((UnreserveSystemMerge) repo).getReason());\n+\n+      // Not enough time has passed for Tablet, should be able to merge d and e\n+      mergeInfo = new MergeInfo(tableId, manager.getContext().getNamespaceId(tableId),\n+          new Text(\""c\"").getBytes(), new Text(\""e\"").getBytes(), Operation.SYSTEM_MERGE);\n+      repo = new VerifyMergeability(mergeInfo).call(fateId, manager);\n+      assertInstanceOf(UnreserveSystemMerge.class, repo);\n+      assertEquals(UnmergeableReason.TABLET_MERGEABILITY,\n+          ((UnreserveSystemMerge) repo).getReason());\n+\n+      // update time to 3 days so enough time has passed\n+      manager = mockWithAmple(getCluster().getServerContext(), testAmple, Duration.ofDays(3));\n+      assertInstanceOf(MergeTablets.class, new VerifyMergeability(mergeInfo).call(fateId, manager));\n+\n+      // last 3 tablets should total 9 files which is < max of 10\n+      mergeInfo = new MergeInfo(tableId, manager.getContext().getNamespaceId(tableId),\n+          new Text(\""c\"").getBytes(), null, Operation.SYSTEM_MERGE);\n+      addFileMetadata(context, tableId, new Text(\""c\""), null, 3, 10);\n+      assertInstanceOf(MergeTablets.class, new VerifyMergeability(mergeInfo).call(fateId, manager));\n+\n+      // last 3 tablets should total 12 files which is > max of 10\n+      addFileMetadata(context, tableId, new Text(\""c\""), null, 4, 10);\n+      repo = new VerifyMergeability(mergeInfo).call(fateId, manager);\n+      assertInstanceOf(UnreserveSystemMerge.class, repo);\n+      assertEquals(UnmergeableReason.MAX_FILE_COUNT, ((UnreserveSystemMerge) repo).getReason());\n+    }\n+  }\n+\n+  private void addFileMetadata(ServerContext context, TableId tableId, Text start, Text end,\n+      int numFiles, int fileSize) {\n+    try (\n+        var tablets =\n+            context.getAmple().readTablets().forTable(tableId).overlapping(start, end).build();\n+        var tabletsMutator = context.getAmple().mutateTablets()) {\n+      for (var tabletMeta : tablets) {\n+        var tabletMutator = tabletsMutator.mutateTablet(tabletMeta.getExtent());\n+        for (int i = 0; i < numFiles; i++) {\n+          StoredTabletFile f = StoredTabletFile.of(new org.apache.hadoop.fs.Path(\n+              \""file:///accumulo/tables/1/\"" + tabletMeta.getDirName() + \""/F\"" + i + \"".rf\""));\n+          DataFileValue dfv = new DataFileValue(fileSize, 100);\n+          tabletMutator.putFile(f, dfv);\n+        }\n+        tabletMutator.mutate();\n+      }\n+    }\n+  }\n+\n   @Test\n   public void testSplitOffline() throws Exception {\n     String[] tableNames = getUniqueNames(2);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\nindex e97c239e952..5a72053d93c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n@@ -53,7 +53,6 @@\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n@@ -127,12 +126,7 @@ public void addSplitTest() throws Exception {\n       verifyData(c, tableName, 2L);\n \n       TableId id = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n-      try (TabletsMetadata tm =\n-          getCluster().getServerContext().getAmple().readTablets().forTable(id).build()) {\n-        // Default for user created tablets should be mergeability set to NEVER\n-        tm.stream().forEach(tablet -> assertEquals(TabletMergeabilityMetadata.never(),\n-            tablet.getTabletMergeability()));\n-      }\n+      verifySplits(id, TabletMergeabilityUtil.userDefaultSplits(splits));\n     }\n   }\n \n@@ -360,9 +354,9 @@ private void verifySplits(TableId id, SortedMap<Text,TabletMergeability> splits)\n         getCluster().getServerContext().getAmple().readTablets().forTable(id).build()) {\n       tm.stream().forEach(t -> {\n         var split = t.getEndRow();\n-        // default tablet should be set to never\n+        // default tablet should be set to always\n         if (split == null) {\n-          assertEquals(TabletMergeability.never(),\n+          assertEquals(TabletMergeability.always(),\n               t.getTabletMergeability().getTabletMergeability());\n         } else {\n           assertTrue(addedSplits.remove(split));\n@@ -381,9 +375,9 @@ private void verifySplitsWithApi(AccumuloClient c, String tableName,\n     c.tableOperations().getTabletInformation(tableName, new Range()).forEach(ti -> {\n       var tmInfo = ti.getTabletMergeabilityInfo();\n       var split = ti.getTabletId().getEndRow();\n-      // default tablet should always be set to never\n+      // default tablet should always be set to always\n       if (split == null) {\n-        assertEquals(TabletMergeability.never(),\n+        assertEquals(TabletMergeability.always(),\n             ti.getTabletMergeabilityInfo().getTabletMergeability());\n       } else {\n         assertTrue(addedSplits.remove(split));\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java b/test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java\nindex bc3176258ab..40cf9b0f512 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/CreateInitialSplitsIT.java\n@@ -139,10 +139,10 @@ public void testCreateInitialSplitsWithMergeability() throws TableExistsExceptio\n     var tableId = getCluster().getServerContext().getTableId(tableName);\n     try (var tablets =\n         getCluster().getServerContext().getAmple().readTablets().forTable(tableId).build()) {\n-      // default tablet (null end row) should have a default TabletMergeability of never for user\n+      // default tablet (null end row) should have a default TabletMergeability of always for user\n       // created tablets\n       assertTrue(tablets.stream()\n-          .anyMatch(tm -> tm.getEndRow() == null && tm.getTabletMergeability().isNever()));\n+          .anyMatch(tm -> tm.getEndRow() == null && tm.getTabletMergeability().isAlways()));\n       // other splits should be created with a duration of 10 seconds\n       assertEquals(10, tablets.stream().filter(tm -> tm.getTabletMergeability().getDelay()\n           .map(delay -> delay.equals(splitDuration)).orElse(false)).count());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java b/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\nindex a985d3969a6..661afa74b22 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\n@@ -262,11 +262,9 @@ public void tabletShouldSplit() throws Exception {\n           }\n           if (TabletColumnFamily.MERGEABILITY_COLUMN.getColumnQualifier()\n               .equals(entry.getKey().getColumnQualifier())) {\n-            // Default tablet should be set to NEVER, all newly generated system splits should be\n+            // Default tablet should be set to ALWAYS, all newly generated system splits should be\n             // set to ALWAYS\n-            var mergeability =\n-                extent.endRow() == null ? TabletMergeability.never() : TabletMergeability.always();\n-            assertEquals(mergeability,\n+            assertEquals(TabletMergeability.always(),\n                 TabletMergeabilityMetadata.fromValue(entry.getValue()).getTabletMergeability());\n           }\n           count++;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/TabletMergeabilityIT.java b/test/src/main/java/org/apache/accumulo/test/functional/TabletMergeabilityIT.java\nnew file mode 100644\nindex 00000000000..eb93dad34a9\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/TabletMergeabilityIT.java\n@@ -0,0 +1,451 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.test.functional;\n+\n+import static org.apache.accumulo.test.TestIngest.generateRow;\n+import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.countTablets;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+\n+import org.apache.accumulo.core.client.Accumulo;\n+import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.BatchWriter;\n+import org.apache.accumulo.core.client.admin.CompactionConfig;\n+import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n+import org.apache.accumulo.core.client.admin.TabletAvailability;\n+import org.apache.accumulo.core.client.admin.TabletMergeability;\n+import org.apache.accumulo.core.clientImpl.TabletMergeabilityUtil;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Range;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.dataImpl.KeyExtent;\n+import org.apache.accumulo.core.metadata.AccumuloTable;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n+import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.harness.MiniClusterConfigurationCallback;\n+import org.apache.accumulo.harness.SharedMiniClusterBase;\n+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;\n+import org.apache.accumulo.server.ServerContext;\n+import org.apache.accumulo.test.TestIngest;\n+import org.apache.accumulo.test.VerifyIngest;\n+import org.apache.accumulo.test.VerifyIngest.VerifyParams;\n+import org.apache.accumulo.test.util.FileMetadataUtil;\n+import org.apache.accumulo.test.util.Wait;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+\n+public class TabletMergeabilityIT extends SharedMiniClusterBase {\n+\n+  @Override\n+  protected Duration defaultTimeout() {\n+    return Duration.ofMinutes(5);\n+  }\n+\n+  @BeforeAll\n+  public static void setup() throws Exception {\n+    SharedMiniClusterBase.startMiniClusterWithConfig(new Callback());\n+  }\n+\n+  @AfterAll\n+  public static void teardown() {\n+    SharedMiniClusterBase.stopMiniCluster();\n+  }\n+\n+  private static class Callback implements MiniClusterConfigurationCallback {\n+    @Override\n+    public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration coreSite) {\n+      // Configure a short period of time to run the auto merge thread for testing\n+      cfg.setProperty(Property.MANAGER_TABLET_MERGEABILITY_INTERVAL, \""3s\"");\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityAlwaysUserTable() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      c.tableOperations().create(tableName);\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+      testMergeabilityAlways(c, tableName, \""\"", Set.of(new KeyExtent(tableId, null, null)));\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityMetadata() throws Exception {\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      var splitPoint = MetadataSchema.TabletsSection.getRange().getEndKey().getRow();\n+      // Test merge with new splits added after splitPoint default tablet (which is mergeable)\n+      // Should keep tablets section tablet on merge\n+      testMergeabilityAlways(c, AccumuloTable.METADATA.tableName(), \""~\"",\n+          Set.of(new KeyExtent(AccumuloTable.METADATA.tableId(), null, splitPoint),\n+              new KeyExtent(AccumuloTable.METADATA.tableId(), splitPoint, null)));\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityFate() throws Exception {\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      testMergeabilityAlways(c, AccumuloTable.FATE.tableName(), \""\"",\n+          Set.of(new KeyExtent(AccumuloTable.FATE.tableId(), null, null)));\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityScanRef() throws Exception {\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      testMergeabilityAlways(c, AccumuloTable.SCAN_REF.tableName(), \""\"",\n+          Set.of(new KeyExtent(AccumuloTable.SCAN_REF.tableId(), null, null)));\n+    }\n+  }\n+\n+  private void testMergeabilityAlways(AccumuloClient c, String tableName, String newSplitPrefix,\n+      Set<KeyExtent> expectedMergedExtents) throws Exception {\n+    var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+    TreeSet<Text> splits = new TreeSet<>();\n+    splits.add(new Text(newSplitPrefix + String.format(\""%09d\"", 333)));\n+    splits.add(new Text(newSplitPrefix + String.format(\""%09d\"", 666)));\n+    splits.add(new Text(newSplitPrefix + String.format(\""%09d\"", 999)));\n+\n+    // create splits with mergeabilty disabled so the task does not merge them away\n+    // The default tablet is always mergeable, but it is currently the only one that is mergeable,\n+    // so nothing will merge\n+    c.tableOperations().putSplits(tableName, TabletMergeabilityUtil.userDefaultSplits(splits));\n+    Wait.waitFor(() -> countTablets(getCluster().getServerContext(), tableName, tm -> true)\n+        == splits.size() + expectedMergedExtents.size(), 5000, 200);\n+\n+    // update new splits to always mergeable so the task can now merge tablets\n+    c.tableOperations().putSplits(tableName, TabletMergeabilityUtil.systemDefaultSplits(splits));\n+\n+    // Wait for merge, and check extents\n+    Wait.waitFor(\n+        () -> hasExactTablets(getCluster().getServerContext(), tableId, expectedMergedExtents),\n+        10000, 2000);\n+  }\n+\n+  @Test\n+  public void testMergeabilityMultipleRanges() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      c.tableOperations().create(tableName);\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      SortedMap<Text,TabletMergeability> splits = new TreeMap<>();\n+      splits.put(new Text(String.format(\""%09d\"", 333)), TabletMergeability.never());\n+      splits.put(new Text(String.format(\""%09d\"", 555)), TabletMergeability.never());\n+      splits.put(new Text(String.format(\""%09d\"", 666)), TabletMergeability.never());\n+      splits.put(new Text(String.format(\""%09d\"", 999)), TabletMergeability.never());\n+\n+      c.tableOperations().putSplits(tableName, splits);\n+      Wait.waitFor(() -> countTablets(getCluster().getServerContext(), tableName, tm -> true) == 5,\n+          5000, 500);\n+\n+      splits.put(new Text(String.format(\""%09d\"", 333)), TabletMergeability.always());\n+      splits.put(new Text(String.format(\""%09d\"", 555)), TabletMergeability.always());\n+      // Keep tablet 666 as never, this should cause two fate jobs for merging\n+      splits.put(new Text(String.format(\""%09d\"", 999)), TabletMergeability.always());\n+      c.tableOperations().putSplits(tableName, splits);\n+\n+      // Wait for merge, we should have 3 tablets\n+      // 333 and 555 should be merged into 555\n+      // 666\n+      // 999 and default merged into default\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, new Text(String.format(\""%09d\"", 555)), null),\n+              new KeyExtent(tableId, new Text(String.format(\""%09d\"", 666)),\n+                  new Text(String.format(\""%09d\"", 555))),\n+              new KeyExtent(tableId, null, new Text(String.format(\""%09d\"", 666))))),\n+          10000, 200);\n+\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityThresholdMultipleRanges() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      Map<String,String> props = new HashMap<>();\n+      props.put(Property.TABLE_SPLIT_THRESHOLD.getKey(), \""32K\"");\n+      props.put(Property.TABLE_MAX_MERGEABILITY_THRESHOLD.getKey(), \"".5\"");\n+      c.tableOperations().create(tableName, new NewTableConfiguration()\n+          .withInitialTabletAvailability(TabletAvailability.HOSTED).setProperties(props));\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      SortedMap<Text,TabletMergeability> splits = new TreeMap<>();\n+      // Create new tablets that won't merge automatically\n+      for (int i = 10000; i <= 90000; i += 10000) {\n+        splits.put(row(i), TabletMergeability.never());\n+      }\n+\n+      c.tableOperations().putSplits(tableName, splits);\n+      // Verify we now have 10 tablets\n+      // [row_0000010000, row_0000020000, row_0000030000, row_0000040000, row_0000050000,\n+      // row_0000060000, row_0000070000, row_0000080000, row_0000090000, default]\n+      Wait.waitFor(() -> countTablets(getCluster().getServerContext(), tableName, tm -> true) == 10,\n+          5000, 500);\n+\n+      // Insert rows into each tablet with different numbers of rows\n+      // Tablets with end rows row_0000020000 - row_0000040000, row_0000060000 - row_0000080000,\n+      // default will have 1000 rows\n+      // Tablets with end rows row_0000010000, row_0000050000, row_0000090000 will have 5000 rows\n+      try (BatchWriter bw = c.createBatchWriter(tableName)) {\n+        final var value = StringUtils.repeat(\""a\"", 1024);\n+        for (int i = 0; i < 100000; i += 10000) {\n+          var rows = 1000;\n+          if (i % 40000 == 0) {\n+            rows = 5000;\n+          }\n+          for (int j = 0; j < rows; j++) {\n+            Mutation m = new Mutation(row(i + j));\n+            m.put(new Text(\""cf1\""), new Text(\""cq1\""), new Value(value));\n+            bw.addMutation(m);\n+          }\n+        }\n+      }\n+      c.tableOperations().flush(tableName, null, null, true);\n+\n+      // Set all 10 tablets to be auto-mergeable\n+      for (int i = 10000; i <= 90000; i += 10000) {\n+        splits.put(row(i), TabletMergeability.always());\n+      }\n+      c.tableOperations().putSplits(tableName, splits);\n+\n+      // With the mergeability threshold set to 50% of 32KB we should be able to merge together\n+      // the tablets with 1000 rows, but not 5000 rows. This should produce the following\n+      // 6 tablets after merger.\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, row(10000), null),\n+              new KeyExtent(tableId, row(40000), row(10000)),\n+              new KeyExtent(tableId, row(50000), row(40000)),\n+              new KeyExtent(tableId, row(80000), row(50000)),\n+              new KeyExtent(tableId, row(90000), row(80000)),\n+              new KeyExtent(tableId, null, row(90000)))),\n+          10000, 200);\n+    }\n+  }\n+\n+  @Test\n+  public void testSplitAndMergeAll() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      Map<String,String> props = new HashMap<>();\n+      props.put(Property.TABLE_SPLIT_THRESHOLD.getKey(), \""16K\"");\n+      props.put(Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), \""1K\"");\n+      c.tableOperations().create(tableName, new NewTableConfiguration().setProperties(props)\n+          .withInitialTabletAvailability(TabletAvailability.HOSTED));\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      // Ingest data so tablet will split\n+      VerifyParams params = new VerifyParams(getClientProps(), tableName, 5_000);\n+      TestIngest.ingest(c, params);\n+      c.tableOperations().flush(tableName);\n+      VerifyIngest.verifyIngest(c, params);\n+\n+      // Wait for table to split, should be more than 10 tablets\n+      Wait.waitFor(() -> c.tableOperations().listSplits(tableName).size() > 10, 30000, 200);\n+\n+      // Delete all the data - We can't use deleteRows() as that would merge empty tablets\n+      // Instead, we want the mergeability thread to merge so use a batch deleter and\n+      // compact away the deleted data\n+      var bd = c.createBatchDeleter(tableName, Authorizations.EMPTY, 1);\n+      bd.setRanges(List.of(new Range()));\n+      bd.delete();\n+      c.tableOperations().compact(tableName, new CompactionConfig().setFlush(true));\n+\n+      // Wait for merge back to default tablet\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, null, null))), 30000, 200);\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityThreshold() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      Map<String,String> props = new HashMap<>();\n+      props.put(Property.TABLE_SPLIT_THRESHOLD.getKey(), \""16K\"");\n+      props.put(Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), \""1K\"");\n+      // Set a low threshold to 1% of the split threshold\n+      props.put(Property.TABLE_MAX_MERGEABILITY_THRESHOLD.getKey(), \"".01\"");\n+      c.tableOperations().create(tableName, new NewTableConfiguration().setProperties(props)\n+          .withInitialTabletAvailability(TabletAvailability.HOSTED));\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      // Ingest data so tablet will split\n+      VerifyParams params = new VerifyParams(getClientProps(), tableName, 5_000);\n+      TestIngest.ingest(c, params);\n+      c.tableOperations().flush(tableName);\n+      VerifyIngest.verifyIngest(c, params);\n+\n+      // Wait for table to split, should be more than 10 tablets\n+      Wait.waitFor(() -> c.tableOperations().listSplits(tableName).size() > 10, 10000, 200);\n+\n+      // Set the split threshold back to the default of 5 MB. There's not a lot of data so normally\n+      // we could merge back to 1 tablet, but the threshold is too low at 1% so it should not merge\n+      // yet.\n+      c.tableOperations().setProperty(tableName, Property.TABLE_SPLIT_THRESHOLD.getKey(), \""5m\"");\n+\n+      // Should not merge so make sure it throws IllegalStateException\n+      assertThrows(IllegalStateException.class,\n+          () -> Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+              Set.of(new KeyExtent(tableId, null, null))), 5000, 500));\n+      // Make sure we failed because of exact tablets and not a different IllegalStateException\n+      assertFalse(hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, null, null))));\n+\n+      // With a 10% threshold we should be able to merge\n+      c.tableOperations().setProperty(tableName, Property.TABLE_MAX_MERGEABILITY_THRESHOLD.getKey(),\n+          \"".1\"");\n+\n+      // Wait for merge back to default tablet\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, null, null))), 10000, 200);\n+\n+      // re-verify the data after merge\n+      VerifyIngest.verifyIngest(c, params);\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeAfter() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      c.tableOperations().create(tableName);\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      TreeSet<Text> splits = new TreeSet<>();\n+      splits.add(new Text(String.format(\""%09d\"", 333)));\n+      splits.add(new Text(String.format(\""%09d\"", 666)));\n+      splits.add(new Text(String.format(\""%09d\"", 999)));\n+\n+      var delay = Duration.ofSeconds(5);\n+      var startTime = c.instanceOperations().getManagerTime();\n+      c.tableOperations().putSplits(tableName, TabletMergeabilityUtil.splitsWithDefault(splits,\n+          TabletMergeability.after(Duration.ofSeconds(5))));\n+\n+      Wait.waitFor(() -> countTablets(getCluster().getServerContext(), tableName, tm -> true) == 4,\n+          5000, 200);\n+\n+      // Wait for merge back to default tablet\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, null, null))), 10000, 200);\n+\n+      var elapsed = c.instanceOperations().getManagerTime().minus(startTime);\n+      assertTrue(elapsed.compareTo(delay) > 0);\n+    }\n+  }\n+\n+  @Test\n+  public void testMergeabilityMaxFiles() throws Exception {\n+    String tableName = getUniqueNames(1)[0];\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+      Map<String,String> props = new HashMap<>();\n+      // disable compactions and set a low merge file max\n+      props.put(Property.TABLE_MAJC_RATIO.getKey(), \""9999\"");\n+      props.put(Property.TABLE_MERGE_FILE_MAX.getKey(), \""3\"");\n+      c.tableOperations().create(tableName, new NewTableConfiguration().setProperties(props)\n+          .withInitialTabletAvailability(TabletAvailability.HOSTED));\n+      var tableId = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      // Create new tablets that won't merge automatically\n+      SortedMap<Text,TabletMergeability> splits = new TreeMap<>();\n+      for (int i = 500; i < 5000; i += 500) {\n+        splits.put(row(i), TabletMergeability.never());\n+      }\n+      c.tableOperations().putSplits(tableName, splits);\n+\n+      // Verify we now have 10 tablets\n+      Wait.waitFor(() -> countTablets(getCluster().getServerContext(), tableName, tm -> true) == 10,\n+          5000, 500);\n+\n+      // Ingest data so tablet will split, each tablet will have several files because\n+      // of the flush setting\n+      VerifyParams params = new VerifyParams(getClientProps(), tableName, 5_000);\n+      params.startRow = 0;\n+      params.flushAfterRows = 100;\n+      TestIngest.ingest(c, params);\n+      VerifyIngest.verifyIngest(c, params);\n+\n+      assertTrue(FileMetadataUtil.countFiles(getCluster().getServerContext(), tableName) > 3);\n+\n+      // Mark all tablets as mergeable\n+      for (int i = 500; i < 5000; i += 500) {\n+        splits.put(row(i), TabletMergeability.always());\n+      }\n+      c.tableOperations().putSplits(tableName, splits);\n+\n+      // Should not merge as we set max file count to only 3 and there are more files than that\n+      // per tablet, so make sure it throws IllegalStateException\n+      assertThrows(IllegalStateException.class,\n+          () -> Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+              Set.of(new KeyExtent(tableId, null, null))), 5000, 500));\n+      // Make sure tablets is still 10, not merged\n+      assertEquals(10, countTablets(getCluster().getServerContext(), tableName, tm -> true));\n+\n+      // Set max merge file count back to default of 10k\n+      c.tableOperations().setProperty(tableName, Property.TABLE_MERGE_FILE_MAX.getKey(), \""10000\"");\n+\n+      // Should merge back to 1 tablet\n+      Wait.waitFor(() -> hasExactTablets(getCluster().getServerContext(), tableId,\n+          Set.of(new KeyExtent(tableId, null, null))), 10000, 200);\n+\n+      // re-verify the data after merge\n+      VerifyIngest.verifyIngest(c, params);\n+    }\n+  }\n+\n+  private static boolean hasExactTablets(ServerContext ctx, TableId tableId,\n+      Set<KeyExtent> expected) {\n+    try (var tabletsMetadata = ctx.getAmple().readTablets().forTable(tableId).build()) {\n+      // check for exact tablets by counting tablets that match the expected rows and also\n+      // making sure the number seen equals exactly expected\n+      final var expectedTablets = new HashSet<>(expected);\n+      for (TabletMetadata tm : tabletsMetadata) {\n+        // make sure every tablet seen is contained in the expected set\n+        if (!expectedTablets.remove(tm.getExtent())) {\n+          return false;\n+        }\n+      }\n+      // Verify all tablets seen\n+      return expectedTablets.isEmpty();\n+    }\n+  }\n+\n+  private static Text row(int row) {\n+    return generateRow(row, 0);\n+  }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5350"", ""pr_id"": 5350, ""issue_id"": 4809, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Use ZooKeeper with a base directory (\""chroot\"") instead of always specifying full path\nThis issue was originally described at https://issues.apache.org/jira/browse/ACCUMULO-4407\r\n\r\nWe pass around the instanceId a lot in Accumulo, and the vast majority of the time, it's just so we can pass it as part of the path to a ZooKeeper object that is already constructed. We can avoid a lot of this, and simplify the code quite a bit if we just construct the ZooKeeper object using the instanceId as part of the connection string, so all absolute paths are then relative to the instanceId directory, similar to one might do with `chroot`.\r\n\r\nHere's an example of what the code change would look like:\r\n\r\n```java\r\n    // the current way\r\n    var directZK = new ZooKeeper(zookeepers, 200, null);\r\n    directZK.getChildren(Constants.ZROOT + \""/\"" + instanceId + Constants.ZNAMESPACES, false)\r\n        .forEach(System.out::println);\r\n\r\n    // the better way\r\n    var chrootZK = new ZooKeeper(zookeepers + Constants.ZROOT + \""/\"" + instanceId, 200, null);\r\n    chrootZK.getChildren(Constants.ZNAMESPACES, false).forEach(System.out::println);\r\n```\r\n\r\nWe still need to connect without the instanceId when we look up the instanceId from the instance name (which is needed on the client side), but in all other cases, we don't need it.\r\n"", ""issue_word_count"": 204, ""test_files_count"": 27, ""non_test_files_count"": 73, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/RootClientTabletCache.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/ZookeeperLockChecker.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooUtil.java"", ""core/src/main/java/org/apache/accumulo/core/lock/ServiceLockPaths.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java"", ""core/src/main/java/org/apache/accumulo/core/util/MonitorUtil.java"", ""core/src/main/java/org/apache/accumulo/core/util/tables/TableMap.java"", ""core/src/main/java/org/apache/accumulo/core/util/tables/TableZooHelper.java"", ""core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java"", ""core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java"", ""core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java"", ""core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java"", ""core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java"", ""minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterControl.java"", ""minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/ServerContext.java"", ""server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java"", ""server/base/src/main/java/org/apache/accumulo/server/compaction/CompactionConfigStorage.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/IdBasedPropStoreKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/NamespacePropKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCache.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/PropChangeListener.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/SystemPropKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/TablePropKey.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/util/PropSnapshot.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java"", ""server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropEditor.java"", ""server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java"", ""server/base/src/main/java/org/apache/accumulo/server/log/WalStateManager.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java"", ""server/base/src/main/java/org/apache/accumulo/server/manager/state/DeadServerList.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/RootConditionalWriter.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/RootTabletMutatorImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/ServerAmpleImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorImpl.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/handler/KerberosAuthenticator.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java"", ""server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java"", ""server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java"", ""server/base/src/main/java/org/apache/accumulo/server/tablets/UniqueNameAllocator.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/Admin.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ListInstances.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ZooKeeperMain.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/RootMetadataCheckRunner.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java"", ""server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java"", ""server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java"", ""core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java"", ""core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java"", ""core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java""], ""base_commit"": ""4cba6dee3e89960bbaac2f9c86eb34a3a0426779"", ""head_commit"": ""e28af575393cf42a26889d4a52117ebce55e4a73"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5350"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5350"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-17T21:11:52.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\nindex 736f243d4cd..3dfe1aed959 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\n@@ -234,16 +234,15 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,\n     this.hadoopConf = info.getHadoopConf();\n \n     this.zooSession = memoize(() -> {\n-      var zk = info\n-          .getZooKeeperSupplier(getClass().getSimpleName() + \""(\"" + info.getPrincipal() + \"")\"", \""\"")\n-          .get();\n+      var zk =\n+          info.getZooKeeperSupplier(getClass().getSimpleName() + \""(\"" + info.getPrincipal() + \"")\"",\n+              ZooUtil.getRoot(getInstanceID())).get();\n       zooKeeperOpened.set(true);\n       return zk;\n     });\n \n     this.zooCache = memoize(() -> {\n-      var zc = new ZooCache(getZooSession(),\n-          createPersistentWatcherPaths(ZooUtil.getRoot(getInstanceID())));\n+      var zc = new ZooCache(getZooSession(), createPersistentWatcherPaths());\n       zooCacheCreated.set(true);\n       return zc;\n     });\n@@ -258,8 +257,7 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,\n     this.singletonReservation = Objects.requireNonNull(reservation);\n     this.tableops = new TableOperationsImpl(this);\n     this.namespaceops = new NamespaceOperationsImpl(this, tableops);\n-    this.serverPaths =\n-        Suppliers.memoize(() -> new ServiceLockPaths(this.getZooKeeperRoot(), this.getZooCache()));\n+    this.serverPaths = Suppliers.memoize(() -> new ServiceLockPaths(this.getZooCache()));\n     if (ueh == Threads.UEH) {\n       clientThreadPools = ThreadPools.getServerThreadPools();\n     } else {\n@@ -504,10 +502,6 @@ public InstanceId getInstanceID() {\n     return info.getInstanceId();\n   }\n \n-  public String getZooKeeperRoot() {\n-    return ZooUtil.getRoot(getInstanceID());\n-  }\n-\n   /**\n    * Returns the instance name given at system initialization time.\n    *\n@@ -1090,10 +1084,9 @@ public synchronized ZookeeperLockChecker getTServerLockChecker() {\n       // so, it can't rely on being able to continue to use the same client's ZooCache,\n       // because that client could be closed, and its ZooSession also closed\n       // this needs to be fixed; TODO https://github.com/apache/accumulo/issues/2301\n-      var zk = info.getZooKeeperSupplier(ZookeeperLockChecker.class.getSimpleName(), \""\"").get();\n-      String zkRoot = getZooKeeperRoot();\n-      this.zkLockChecker =\n-          new ZookeeperLockChecker(new ZooCache(zk, Set.of(zkRoot + Constants.ZTSERVERS)), zkRoot);\n+      var zk = info.getZooKeeperSupplier(ZookeeperLockChecker.class.getSimpleName(),\n+          ZooUtil.getRoot(getInstanceID())).get();\n+      this.zkLockChecker = new ZookeeperLockChecker(new ZooCache(zk, Set.of(Constants.ZTSERVERS)));\n     }\n     return this.zkLockChecker;\n   }\n@@ -1107,13 +1100,13 @@ public NamespaceMapping getNamespaces() {\n     return namespaces;\n   }\n \n-  private static Set<String> createPersistentWatcherPaths(String zkRoot) {\n+  private static Set<String> createPersistentWatcherPaths() {\n     Set<String> pathsToWatch = new HashSet<>();\n     for (String path : Set.of(Constants.ZCOMPACTORS, Constants.ZDEADTSERVERS, Constants.ZGC_LOCK,\n         Constants.ZMANAGER_LOCK, Constants.ZMINI_LOCK, Constants.ZMONITOR_LOCK,\n         Constants.ZNAMESPACES, Constants.ZRECOVERY, Constants.ZSSERVERS, Constants.ZTABLES,\n         Constants.ZTSERVERS, Constants.ZUSERS, RootTable.ZROOT_TABLET, Constants.ZTEST_LOCK)) {\n-      pathsToWatch.add(zkRoot + path);\n+      pathsToWatch.add(path);\n     }\n     return pathsToWatch;\n   }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java\nindex e25d2781b15..6ac3f46ca38 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java\n@@ -683,7 +683,7 @@ private void invalidateSession(SessionID sessionId, HostAndPort location)\n \n     long startTime = System.currentTimeMillis();\n \n-    LockID lid = new LockID(context.getZooKeeperRoot() + Constants.ZTSERVERS, sessionId.lockId);\n+    LockID lid = new LockID(Constants.ZTSERVERS, sessionId.lockId);\n \n     while (true) {\n       if (!ServiceLock.isLockHeld(context.getZooCache(), lid)) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java\nindex 3b68fff26c4..99c179ae117 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java\n@@ -53,18 +53,17 @@ public NamespaceMapping(ClientContext context) {\n     this.context = context;\n   }\n \n-  public static void put(final ZooReaderWriter zoo, final String zPath,\n-      final NamespaceId namespaceId, final String namespaceName)\n+  public static void put(final ZooReaderWriter zoo, final NamespaceId namespaceId,\n+      final String namespaceName)\n       throws InterruptedException, KeeperException, AcceptableThriftTableOperationException {\n     requireNonNull(zoo);\n-    requireNonNull(zPath);\n     requireNonNull(namespaceId);\n     requireNonNull(namespaceName);\n     if (Namespace.DEFAULT.id().equals(namespaceId) || Namespace.ACCUMULO.id().equals(namespaceId)) {\n       throw new AssertionError(\n           \""Putting built-in namespaces in map should not be possible after init\"");\n     }\n-    zoo.mutateExisting(zPath, data -> {\n+    zoo.mutateExisting(Constants.ZNAMESPACES, data -> {\n       var namespaces = deserialize(data);\n       final String currentName = namespaces.get(namespaceId.canonical());\n       if (namespaceName.equals(currentName)) {\n@@ -106,18 +105,17 @@ public static void remove(final ZooReaderWriter zoo, final String zPath,\n     });\n   }\n \n-  public static void rename(final ZooReaderWriter zoo, final String zPath,\n-      final NamespaceId namespaceId, final String oldName, final String newName)\n+  public static void rename(final ZooReaderWriter zoo, final NamespaceId namespaceId,\n+      final String oldName, final String newName)\n       throws InterruptedException, KeeperException, AcceptableThriftTableOperationException {\n     requireNonNull(zoo);\n-    requireNonNull(zPath);\n     requireNonNull(namespaceId);\n     requireNonNull(oldName);\n     requireNonNull(newName);\n     if (Namespace.DEFAULT.id().equals(namespaceId) || Namespace.ACCUMULO.id().equals(namespaceId)) {\n       throw new AssertionError(\""Renaming built-in namespaces in map should not be possible\"");\n     }\n-    zoo.mutateExisting(zPath, current -> {\n+    zoo.mutateExisting(Constants.ZNAMESPACES, current -> {\n       var namespaces = deserialize(current);\n       final String currentName = namespaces.get(namespaceId.canonical());\n       if (newName.equals(currentName)) {\n@@ -147,10 +145,9 @@ public static Map<String,String> deserialize(byte[] data) {\n \n   private synchronized void update() {\n     final ZooCache zc = context.getZooCache();\n-    final String zPath = context.getZooKeeperRoot() + Constants.ZNAMESPACES;\n     final ZcStat stat = new ZcStat();\n \n-    byte[] data = zc.get(zPath, stat);\n+    byte[] data = zc.get(Constants.ZNAMESPACES, stat);\n     if (stat.getMzxid() > lastMzxid) {\n       if (data == null) {\n         throw new IllegalStateException(\""namespaces node should not be null\"");\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/RootClientTabletCache.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/RootClientTabletCache.java\nindex 3310542a92f..16eca25bc83 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/RootClientTabletCache.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/RootClientTabletCache.java\n@@ -128,10 +128,9 @@ protected CachedTablet getRootTabletLocation(ClientContext context) {\n       timer = Timer.startNew();\n     }\n \n-    var zpath = RootTabletMetadata.zooPath(context);\n     var zooCache = context.getZooCache();\n-    Location loc = new RootTabletMetadata(new String(zooCache.get(zpath), UTF_8)).toTabletMetadata()\n-        .getLocation();\n+    Location loc = new RootTabletMetadata(new String(zooCache.get(RootTable.ZROOT_TABLET), UTF_8))\n+        .toTabletMetadata().getLocation();\n \n     if (timer != null) {\n       log.trace(\""tid={} Found root tablet at {} in {}\"", Thread.currentThread().getId(), loc,\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ZookeeperLockChecker.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ZookeeperLockChecker.java\nindex 8ae286c79ef..d83525738e5 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ZookeeperLockChecker.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ZookeeperLockChecker.java\n@@ -35,13 +35,11 @@\n public class ZookeeperLockChecker implements TabletServerLockChecker {\n \n   private final ZooCache zc;\n-  private final String root;\n   private final ServiceLockPaths lockPaths;\n \n-  ZookeeperLockChecker(ZooCache zooCache, String zkRoot) {\n+  ZookeeperLockChecker(ZooCache zooCache) {\n     this.zc = requireNonNull(zooCache);\n-    this.root = requireNonNull(zkRoot);\n-    this.lockPaths = new ServiceLockPaths(this.root, this.zc);\n+    this.lockPaths = new ServiceLockPaths(this.zc);\n   }\n \n   public boolean doesTabletServerLockExist(String server) {\n@@ -70,6 +68,6 @@ public boolean isLockHeld(String server, String session) {\n   public void invalidateCache(String tserver) {\n     // The path for the tserver contains a resource group. The resource group is unknown, so can not\n     // construct a prefix. Therefore clear any path that contains the tserver.\n-    zc.clear(path -> path.startsWith(root + Constants.ZTSERVERS) && path.contains(tserver));\n+    zc.clear(path -> path.startsWith(Constants.ZTSERVERS) && path.contains(tserver));\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 28a92d4204b..93f42181198 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -542,6 +542,7 @@ public FateId startTransaction() {\n   public void seedTransaction(FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n     try (var seeder = store.beginSeeding()) {\n+      @SuppressWarnings(\""unused\"")\n       var unused = seeder.attemptToSeedTransaction(fateOp, fateKey, repo, autoCleanUp);\n     }\n   }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooUtil.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooUtil.java\nindex 156b4f14a6b..470b098105b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooUtil.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooUtil.java\n@@ -76,11 +76,12 @@ public LockID(String root, String serializedLID) {\n         throw new IllegalArgumentException(\""Malformed serialized lock id \"" + serializedLID);\n       }\n \n-      if (lastSlash == 0) {\n-        path = root;\n-      } else {\n-        path = root + \""/\"" + sa[0].substring(0, lastSlash);\n+      var tmpPath = root + \""/\"" + sa[0].substring(0, lastSlash);\n+      if (tmpPath.endsWith(\""/\"") && tmpPath.length() > 1) {\n+        tmpPath = tmpPath.substring(0, tmpPath.length() - 1);\n       }\n+      path = tmpPath;\n+\n       node = sa[0].substring(lastSlash + 1);\n       eid = Long.parseUnsignedLong(sa[1], 16);\n     }\n@@ -134,7 +135,7 @@ public int hashCode() {\n   }\n \n   public static String getRoot(final InstanceId instanceId) {\n-    return Constants.ZROOT + \""/\"" + instanceId;\n+    return Constants.ZROOT + \""/\"" + instanceId.canonical();\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/lock/ServiceLockPaths.java b/core/src/main/java/org/apache/accumulo/core/lock/ServiceLockPaths.java\nindex 7e4378ceec0..249ef5a6c63 100644\n--- a/core/src/main/java/org/apache/accumulo/core/lock/ServiceLockPaths.java\n+++ b/core/src/main/java/org/apache/accumulo/core/lock/ServiceLockPaths.java\n@@ -23,6 +23,7 @@\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.List;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -34,6 +35,7 @@\n import java.util.function.Predicate;\n \n import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.util.threads.ThreadPoolNames;\n import org.apache.accumulo.core.util.threads.ThreadPools;\n import org.apache.accumulo.core.zookeeper.ZcStat;\n@@ -55,9 +57,11 @@ public static class ServiceLockPath {\n     private final String path;\n \n     /**\n-     * Exists for ServiceLockIt\n+     * Exists for ServiceLockIT\n      */\n-    protected ServiceLockPath(String path) {\n+    protected ServiceLockPath(int uniqParamForTest, String path) {\n+      Preconditions.checkArgument(uniqParamForTest == \""ServiceLockIT\"".hashCode(),\n+          \""this method only intended to be used by ServiceLockIT\"");\n       this.type = null;\n       this.resourceGroup = null;\n       this.server = null;\n@@ -67,8 +71,7 @@ protected ServiceLockPath(String path) {\n     /**\n      * Create a ServiceLockPath for a management process\n      */\n-    private ServiceLockPath(String root, String type) {\n-      requireNonNull(root);\n+    private ServiceLockPath(String type) {\n       this.type = requireNonNull(type);\n       Preconditions.checkArgument(this.type.equals(Constants.ZGC_LOCK)\n           || this.type.equals(Constants.ZMANAGER_LOCK) || this.type.equals(Constants.ZMONITOR_LOCK)\n@@ -78,36 +81,34 @@ private ServiceLockPath(String root, String type) {\n       // a known path, not the server's address.\n       this.resourceGroup = null;\n       this.server = null;\n-      this.path = root + this.type;\n+      this.path = this.type;\n     }\n \n     /**\n      * Create a ServiceLockPath for ZTABLE_LOCKS\n      */\n-    private ServiceLockPath(String root, String type, String content) {\n-      requireNonNull(root);\n+    private ServiceLockPath(String type, String content) {\n       this.type = requireNonNull(type);\n       Preconditions.checkArgument(\n           this.type.equals(Constants.ZTABLE_LOCKS) || this.type.equals(Constants.ZMINI_LOCK),\n           \""Unsupported type: \"" + type);\n       this.resourceGroup = null;\n       this.server = requireNonNull(content);\n-      this.path = root + this.type + \""/\"" + this.server;\n+      this.path = this.type + \""/\"" + this.server;\n     }\n \n     /**\n      * Create a ServiceLockPath for a worker process\n      */\n-    private ServiceLockPath(String root, String type, String resourceGroup, String server) {\n-      requireNonNull(root);\n+    private ServiceLockPath(String type, String resourceGroup, HostAndPort server) {\n       this.type = requireNonNull(type);\n       Preconditions.checkArgument(\n           this.type.equals(Constants.ZCOMPACTORS) || this.type.equals(Constants.ZSSERVERS)\n               || this.type.equals(Constants.ZTSERVERS) || this.type.equals(Constants.ZDEADTSERVERS),\n           \""Unsupported type: \"" + type);\n       this.resourceGroup = requireNonNull(resourceGroup);\n-      this.server = requireNonNull(server);\n-      this.path = root + this.type + \""/\"" + this.resourceGroup + \""/\"" + this.server;\n+      this.server = requireNonNull(server).toString();\n+      this.path = this.type + \""/\"" + this.resourceGroup + \""/\"" + this.server;\n     }\n \n     public String getType() {\n@@ -127,42 +128,12 @@ public boolean equals(Object obj) {\n       if (this == obj) {\n         return true;\n       }\n-      if (obj == null) {\n+      if (obj == null || getClass() != obj.getClass()) {\n         return false;\n       }\n-      if (getClass() != obj.getClass()) {\n-        return false;\n-      }\n-      ServiceLockPath other = (ServiceLockPath) obj;\n-      if (path == null) {\n-        if (other.path != null) {\n-          return false;\n-        }\n-      } else if (!path.equals(other.path)) {\n-        return false;\n-      }\n-      if (resourceGroup == null) {\n-        if (other.resourceGroup != null) {\n-          return false;\n-        }\n-      } else if (!resourceGroup.equals(other.resourceGroup)) {\n-        return false;\n-      }\n-      if (server == null) {\n-        if (other.server != null) {\n-          return false;\n-        }\n-      } else if (!server.equals(other.server)) {\n-        return false;\n-      }\n-      if (type == null) {\n-        if (other.type != null) {\n-          return false;\n-        }\n-      } else if (!type.equals(other.type)) {\n-        return false;\n-      }\n-      return true;\n+      var other = (ServiceLockPath) obj;\n+      return Objects.equals(path, other.path) && Objects.equals(resourceGroup, other.resourceGroup)\n+          && Objects.equals(server, other.server) && Objects.equals(type, other.type);\n     }\n \n     @Override\n@@ -185,51 +156,51 @@ public String toString() {\n \n   private final ExecutorService fetchExectuor;\n \n-  private final String zkRoot;\n   private final ZooCache zooCache;\n \n-  public ServiceLockPaths(String zkRoot, ZooCache zc) {\n-    this.zkRoot = requireNonNull(zkRoot);\n+  public ServiceLockPaths(ZooCache zc) {\n     this.zooCache = requireNonNull(zc);\n     this.fetchExectuor = ThreadPools.getServerThreadPools()\n         .getPoolBuilder(ThreadPoolNames.SERVICE_LOCK_POOL).numCoreThreads(16).build();\n   }\n \n   private static String determineServerType(final String path) {\n-    if (path.contains(Constants.ZGC_LOCK)) {\n+    if (pathStartsWith(path, Constants.ZGC_LOCK)) {\n       return Constants.ZGC_LOCK;\n-    } else if (path.contains(Constants.ZMANAGER_LOCK)) {\n+    } else if (pathStartsWith(path, Constants.ZMANAGER_LOCK)) {\n       return Constants.ZMANAGER_LOCK;\n-    } else if (path.contains(Constants.ZMONITOR_LOCK)) {\n+    } else if (pathStartsWith(path, Constants.ZMONITOR_LOCK)) {\n       return Constants.ZMONITOR_LOCK;\n-    } else if (path.contains(Constants.ZMINI_LOCK)) {\n+    } else if (pathStartsWith(path, Constants.ZMINI_LOCK)) {\n       return Constants.ZMINI_LOCK;\n-    } else if (path.contains(Constants.ZADMIN_LOCK)) {\n+    } else if (pathStartsWith(path, Constants.ZADMIN_LOCK)) {\n       return Constants.ZADMIN_LOCK;\n-    } else if (path.contains(Constants.ZTEST_LOCK)) {\n+    } else if (pathStartsWith(path, Constants.ZTEST_LOCK)) {\n       return Constants.ZTEST_LOCK;\n-    } else if (path.contains(Constants.ZCOMPACTORS)) {\n+    } else if (pathStartsWith(path, Constants.ZCOMPACTORS)) {\n       return Constants.ZCOMPACTORS;\n-    } else if (path.contains(Constants.ZSSERVERS)) {\n+    } else if (pathStartsWith(path, Constants.ZSSERVERS)) {\n       return Constants.ZSSERVERS;\n-    } else if (path.contains(Constants.ZDEADTSERVERS)) {\n-      // This has to be before TSERVERS\n+    } else if (pathStartsWith(path, Constants.ZDEADTSERVERS)) {\n       return Constants.ZDEADTSERVERS;\n-    } else if (path.contains(Constants.ZTSERVERS)) {\n+    } else if (pathStartsWith(path, Constants.ZTSERVERS)) {\n       return Constants.ZTSERVERS;\n     } else {\n       throw new IllegalArgumentException(\""Unhandled to determine server type from path: \"" + path);\n     }\n   }\n \n+  private static boolean pathStartsWith(String path, String prefix) {\n+    return path.equals(prefix) || path.startsWith(prefix + \""/\"");\n+  }\n+\n   /**\n    * Parse a ZooKeeper path string and return a ServiceLockPath\n    */\n   public static ServiceLockPath parse(Optional<String> serverType, String path) {\n-    requireNonNull(serverType);\n     requireNonNull(path);\n \n-    final String type = serverType.orElseGet(() -> determineServerType(path));\n+    final String type = requireNonNull(serverType).orElseGet(() -> determineServerType(path));\n \n     switch (type) {\n       case Constants.ZGC_LOCK:\n@@ -237,22 +208,21 @@ public static ServiceLockPath parse(Optional<String> serverType, String path) {\n       case Constants.ZMONITOR_LOCK:\n       case Constants.ZADMIN_LOCK:\n       case Constants.ZTEST_LOCK:\n-        return new ServiceLockPath(path.substring(0, path.indexOf(type)), type);\n+        return new ServiceLockPath(type);\n       default: {\n         final String[] pathParts = path.replaceFirst(\""/\"", \""\"").split(\""/\"");\n-        Preconditions.checkArgument(pathParts.length >= 4,\n+        Preconditions.checkArgument(pathParts.length >= 2,\n             \""Unhandled zookeeper service path : \"" + path);\n         final String server = pathParts[pathParts.length - 1];\n         final String resourceGroup = pathParts[pathParts.length - 2];\n         switch (type) {\n           case Constants.ZMINI_LOCK:\n-            return new ServiceLockPath(path.substring(0, path.indexOf(type)), type, server);\n+            return new ServiceLockPath(type, server);\n           case Constants.ZCOMPACTORS:\n           case Constants.ZSSERVERS:\n           case Constants.ZTSERVERS:\n           case Constants.ZDEADTSERVERS:\n-            return new ServiceLockPath(path.substring(0, path.indexOf(type)), type, resourceGroup,\n-                server);\n+            return new ServiceLockPath(type, resourceGroup, HostAndPort.fromString(server));\n           default:\n             throw new IllegalArgumentException(\""Unhandled zookeeper service path : \"" + path);\n         }\n@@ -262,56 +232,52 @@ public static ServiceLockPath parse(Optional<String> serverType, String path) {\n   }\n \n   public ServiceLockPath createGarbageCollectorPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZGC_LOCK);\n+    return new ServiceLockPath(Constants.ZGC_LOCK);\n   }\n \n   public ServiceLockPath createManagerPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZMANAGER_LOCK);\n+    return new ServiceLockPath(Constants.ZMANAGER_LOCK);\n   }\n \n   public ServiceLockPath createMiniPath(String miniUUID) {\n-    return new ServiceLockPath(zkRoot, Constants.ZMINI_LOCK, miniUUID);\n+    return new ServiceLockPath(Constants.ZMINI_LOCK, miniUUID);\n   }\n \n   public ServiceLockPath createMonitorPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZMONITOR_LOCK);\n+    return new ServiceLockPath(Constants.ZMONITOR_LOCK);\n   }\n \n   public ServiceLockPath createCompactorPath(String resourceGroup, HostAndPort serverAddress) {\n-    return new ServiceLockPath(zkRoot, Constants.ZCOMPACTORS, resourceGroup,\n-        serverAddress.toString());\n+    return new ServiceLockPath(Constants.ZCOMPACTORS, resourceGroup, serverAddress);\n   }\n \n   public ServiceLockPath createScanServerPath(String resourceGroup, HostAndPort serverAddress) {\n-    return new ServiceLockPath(zkRoot, Constants.ZSSERVERS, resourceGroup,\n-        serverAddress.toString());\n+    return new ServiceLockPath(Constants.ZSSERVERS, resourceGroup, serverAddress);\n   }\n \n   public ServiceLockPath createTableLocksPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZTABLE_LOCKS);\n+    return new ServiceLockPath(Constants.ZTABLE_LOCKS);\n   }\n \n-  public ServiceLockPath createTableLocksPath(String tableId) {\n-    return new ServiceLockPath(zkRoot, Constants.ZTABLE_LOCKS, tableId);\n+  public ServiceLockPath createTableLocksPath(TableId tableId) {\n+    return new ServiceLockPath(Constants.ZTABLE_LOCKS, tableId.canonical());\n   }\n \n   public ServiceLockPath createTabletServerPath(String resourceGroup, HostAndPort serverAddress) {\n-    return new ServiceLockPath(zkRoot, Constants.ZTSERVERS, resourceGroup,\n-        serverAddress.toString());\n+    return new ServiceLockPath(Constants.ZTSERVERS, resourceGroup, serverAddress);\n   }\n \n   public ServiceLockPath createDeadTabletServerPath(String resourceGroup,\n       HostAndPort serverAddress) {\n-    return new ServiceLockPath(zkRoot, Constants.ZDEADTSERVERS, resourceGroup,\n-        serverAddress.toString());\n+    return new ServiceLockPath(Constants.ZDEADTSERVERS, resourceGroup, serverAddress);\n   }\n \n   public ServiceLockPath createAdminLockPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZADMIN_LOCK);\n+    return new ServiceLockPath(Constants.ZADMIN_LOCK);\n   }\n \n   public ServiceLockPath createTestLockPath() {\n-    return new ServiceLockPath(zkRoot, Constants.ZTEST_LOCK);\n+    return new ServiceLockPath(Constants.ZTEST_LOCK);\n   }\n \n   public Set<ServiceLockPath> getCompactor(ResourceGroupPredicate resourceGroupPredicate,\n@@ -443,7 +409,7 @@ private Set<ServiceLockPath> get(final String serverType,\n     requireNonNull(addressSelector);\n \n     final Set<ServiceLockPath> results = ConcurrentHashMap.newKeySet();\n-    final String typePath = zkRoot + serverType;\n+    final String typePath = serverType;\n \n     if (serverType.equals(Constants.ZGC_LOCK) || serverType.equals(Constants.ZMANAGER_LOCK)\n         || serverType.equals(Constants.ZMONITOR_LOCK)) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java\nindex 5eb075c809c..91818935f3a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/RootTabletMetadata.java\n@@ -61,21 +61,16 @@ public class RootTabletMetadata {\n \n   private static final int VERSION = 1;\n \n-  public static String zooPath(ClientContext ctx) {\n-    return ctx.getZooKeeperRoot() + RootTable.ZROOT_TABLET;\n-  }\n-\n   /**\n    * Reads the tablet metadata for the root tablet from zookeeper\n    */\n   public static RootTabletMetadata read(ClientContext ctx) {\n     try {\n-      final String zpath = zooPath(ctx);\n       ZooReader zooReader = ctx.getZooSession().asReader();\n       // attempt (see ZOOKEEPER-1675) to ensure the latest root table metadata is read from\n       // zookeeper\n-      zooReader.sync(zpath);\n-      byte[] bytes = zooReader.getData(zpath);\n+      zooReader.sync(RootTable.ZROOT_TABLET);\n+      byte[] bytes = zooReader.getData(RootTable.ZROOT_TABLET);\n       return new RootTabletMetadata(new String(bytes, UTF_8));\n     } catch (KeeperException | InterruptedException e) {\n       throw new IllegalStateException(e);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\nindex 537caf917ec..e8ab7971adc 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n@@ -193,10 +193,9 @@ public T deleteLocation(Location location) {\n     return getThis();\n   }\n \n-  protected T putZooLock(String zookeeperRoot, ServiceLock zooLock) {\n+  protected T putZooLock(ServiceLock zooLock) {\n     Preconditions.checkState(updatesEnabled, \""Cannot make updates after calling mutate.\"");\n-    ServerColumnFamily.LOCK_COLUMN.put(mutation,\n-        new Value(zooLock.getLockID().serialize(zookeeperRoot + \""/\"")));\n+    ServerColumnFamily.LOCK_COLUMN.put(mutation, new Value(zooLock.getLockID().serialize(\""/\"")));\n     return getThis();\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/MonitorUtil.java b/core/src/main/java/org/apache/accumulo/core/util/MonitorUtil.java\nindex 87ed8448e3c..30ace98a007 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/MonitorUtil.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/MonitorUtil.java\n@@ -39,7 +39,7 @@ public static String getLocation(ClientContext context)\n   static String getLocation(ZooReader zr, ClientContext context)\n       throws KeeperException, InterruptedException {\n     try {\n-      byte[] loc = zr.getData(context.getZooKeeperRoot() + Constants.ZMONITOR_HTTP_ADDR);\n+      byte[] loc = zr.getData(Constants.ZMONITOR_HTTP_ADDR);\n       return loc == null ? null : new String(loc, UTF_8);\n     } catch (NoNodeException e) {\n       // If there's no node advertising the monitor, there's no monitor.\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/tables/TableMap.java b/core/src/main/java/org/apache/accumulo/core/util/tables/TableMap.java\nindex 0fa46b7d41a..8bbdfc77e55 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/tables/TableMap.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/tables/TableMap.java\n@@ -55,14 +55,14 @@ public TableMap(ClientContext context) {\n     // important to read this first\n     this.updateCount = zooCache.getUpdateCount();\n \n-    List<String> tableIds = zooCache.getChildren(context.getZooKeeperRoot() + Constants.ZTABLES);\n+    List<String> tableIds = zooCache.getChildren(Constants.ZTABLES);\n     Map<NamespaceId,String> namespaceIdToNameMap = new HashMap<>();\n     final var tableNameToIdBuilder = ImmutableMap.<String,TableId>builder();\n     final var tableIdToNameBuilder = ImmutableMap.<TableId,String>builder();\n \n     // use StringBuilder to construct zPath string efficiently across many tables\n     StringBuilder zPathBuilder = new StringBuilder();\n-    zPathBuilder.append(context.getZooKeeperRoot()).append(Constants.ZTABLES).append(\""/\"");\n+    zPathBuilder.append(Constants.ZTABLES).append(\""/\"");\n     int prefixLength = zPathBuilder.length();\n \n     for (String tableIdStr : tableIds) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/tables/TableZooHelper.java b/core/src/main/java/org/apache/accumulo/core/util/tables/TableZooHelper.java\nindex 410ddd40d1a..8ecfc6c4e62 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/tables/TableZooHelper.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/tables/TableZooHelper.java\n@@ -130,13 +130,13 @@ private TableMap getCachedTableMap() {\n \n   public boolean tableNodeExists(TableId tableId) {\n     ZooCache zc = context.getZooCache();\n-    List<String> tableIds = zc.getChildren(context.getZooKeeperRoot() + Constants.ZTABLES);\n+    List<String> tableIds = zc.getChildren(Constants.ZTABLES);\n     return tableIds.contains(tableId.canonical());\n   }\n \n   public void clearTableListCache() {\n-    context.getZooCache().clear(context.getZooKeeperRoot() + Constants.ZTABLES);\n-    context.getZooCache().clear(context.getZooKeeperRoot() + Constants.ZNAMESPACES);\n+    context.getZooCache().clear(Constants.ZTABLES);\n+    context.getZooCache().clear(Constants.ZNAMESPACES);\n     instanceToMapCache.invalidateAll();\n   }\n \n@@ -170,8 +170,7 @@ private String _printableTableInfo(String tableName, TableId tableId) {\n    * @return the table state.\n    */\n   public TableState getTableState(TableId tableId, boolean clearCachedState) {\n-    String statePath = context.getZooKeeperRoot() + Constants.ZTABLES + \""/\"" + tableId.canonical()\n-        + Constants.ZTABLE_STATE;\n+    String statePath = Constants.ZTABLES + \""/\"" + tableId.canonical() + Constants.ZTABLE_STATE;\n     if (clearCachedState) {\n       context.getZooCache().clear(statePath);\n       instanceToMapCache.invalidateAll();\n@@ -200,8 +199,7 @@ public NamespaceId getNamespaceId(TableId tableId) throws TableNotFoundException\n     }\n \n     ZooCache zc = context.getZooCache();\n-    byte[] n = zc.get(context.getZooKeeperRoot() + Constants.ZTABLES + \""/\"" + tableId\n-        + Constants.ZTABLE_NAMESPACE);\n+    byte[] n = zc.get(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_NAMESPACE);\n     // We might get null out of ZooCache if this tableID doesn't exist\n     if (n == null) {\n       throw new TableNotFoundException(tableId.canonical(), null, null);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java b/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\nindex 8d35176d2cf..b415e0ba0ea 100644\n--- a/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\n+++ b/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooCache.java\n@@ -228,9 +228,16 @@ synchronized boolean setupWatchers() {\n \n     for (String left : watchedPaths) {\n       for (String right : watchedPaths) {\n-        if (!left.equals(right) && left.contains(right)) {\n+        if (!right.startsWith(\""/\"")) {\n+          throw new IllegalArgumentException(\""Watched path must start with slash: '\"" + right + \""'\"");\n+        }\n+        if (right.length() > 1 && right.endsWith(\""/\"")) {\n+          throw new IllegalArgumentException(\n+              \""Watched path must not end with slash: '\"" + right + \""'\"");\n+        }\n+        if ((left.equals(\""/\"") && right.length() > 1) || right.startsWith(left + \""/\"")) {\n           throw new IllegalArgumentException(\n-              \""Overlapping paths found in paths to watch. left: \"" + left + \"", right: \"" + right);\n+              \""Overlapping paths found in paths to watch. '\"" + left + \""' contains '\"" + right + \""'\"");\n         }\n       }\n     }\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterControl.java b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterControl.java\nindex 57164165cfe..802f5826215 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterControl.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterControl.java\n@@ -274,7 +274,7 @@ public synchronized void stop(ServerType server, String hostname) throws IOExcep\n           try {\n             cluster.stopProcessWithTimeout(managerProcess, 30, TimeUnit.SECONDS);\n             try {\n-              new ZooZap().zap(cluster.getServerContext().getSiteConfiguration(), \""-manager\"");\n+              new ZooZap().zap(cluster.getServerContext(), \""-manager\"");\n             } catch (RuntimeException e) {\n               log.error(\""Error zapping Manager zookeeper lock\"", e);\n             }\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\nindex 7abafcf5836..c898a3df975 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloClusterImpl.java\n@@ -79,7 +79,7 @@\n import org.apache.accumulo.core.conf.SiteConfiguration;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeExistsPolicy;\n import org.apache.accumulo.core.lock.ServiceLock;\n import org.apache.accumulo.core.lock.ServiceLock.AccumuloLockWatcher;\n@@ -104,8 +104,6 @@\n import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n import org.apache.accumulo.minicluster.ServerType;\n import org.apache.accumulo.server.ServerContext;\n-import org.apache.accumulo.server.ServerDirs;\n-import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.accumulo.server.init.Initialize;\n import org.apache.accumulo.server.util.AccumuloStatus;\n import org.apache.accumulo.server.util.PortUtils;\n@@ -510,35 +508,8 @@ public synchronized void start() throws IOException, InterruptedException {\n     MiniAccumuloClusterControl control = getClusterControl();\n \n     if (config.useExistingInstance()) {\n-      AccumuloConfiguration acuConf = config.getAccumuloConfiguration();\n-      Configuration hadoopConf = config.getHadoopConfiguration();\n-      ServerDirs serverDirs = new ServerDirs(acuConf, hadoopConf);\n-\n-      Path instanceIdPath;\n-      try (var fs = getServerContext().getVolumeManager()) {\n-        instanceIdPath = serverDirs.getInstanceIdLocation(fs.getFirst());\n-      } catch (IOException e) {\n-        throw new UncheckedIOException(e);\n-      }\n-\n-      InstanceId instanceIdFromFile =\n-          VolumeManager.getInstanceIDFromHdfs(instanceIdPath, hadoopConf);\n-      ZooReaderWriter zrw = getServerContext().getZooSession().asReaderWriter();\n-\n-      String instanceName = null;\n-      try {\n-        for (String name : zrw.getChildren(Constants.ZROOT + Constants.ZINSTANCES)) {\n-          String instanceNamePath = Constants.ZROOT + Constants.ZINSTANCES + \""/\"" + name;\n-          byte[] bytes = zrw.getData(instanceNamePath);\n-          InstanceId iid = InstanceId.of(new String(bytes, UTF_8));\n-          if (iid.equals(instanceIdFromFile)) {\n-            instanceName = name;\n-          }\n-        }\n-      } catch (KeeperException e) {\n-        throw new IllegalStateException(\""Unable to read instance name from zookeeper.\"", e);\n-      }\n-      if (instanceName == null) {\n+      String instanceName = getServerContext().getInstanceName();\n+      if (instanceName == null || instanceName.isBlank()) {\n         throw new IllegalStateException(\""Unable to read instance name from zookeeper.\"");\n       }\n \n@@ -709,7 +680,7 @@ public void failedToAcquireLock(Exception e) {\n         Property.INSTANCE_ZK_TIMEOUT.getKey(), Property.INSTANCE_ZK_TIMEOUT.getDefaultValue()));\n     final String secret = properties.get(Property.INSTANCE_SECRET.getKey());\n     miniLockZk = new ZooSession(MiniAccumuloClusterImpl.class.getSimpleName() + \"".lock\"",\n-        config.getZooKeepers(), timeout, secret);\n+        config.getZooKeepers() + ZooUtil.getRoot(iid), timeout, secret);\n \n     // It's possible start was called twice...\n     if (miniLock == null) {\n@@ -997,16 +968,17 @@ public synchronized void stop() throws IOException, InterruptedException {\n     // is restarted, then the processes will start right away\n     // and not wait for the old locks to be cleaned up.\n     try {\n-      new ZooZap().zap(getServerContext().getSiteConfiguration(), \""-manager\"", \""-tservers\"",\n-          \""-compactors\"", \""-sservers\"");\n+      new ZooZap().zap(getServerContext(), \""-manager\"", \""-tservers\"", \""-compactors\"", \""-sservers\"");\n     } catch (RuntimeException e) {\n-      log.error(\""Error zapping zookeeper locks\"", e);\n+      if (!e.getMessage().startsWith(\""Accumulo not initialized\"")) {\n+        log.error(\""Error zapping zookeeper locks\"", e);\n+      }\n     }\n \n     // Clear the location of the servers in ZooCache.\n     boolean macStarted = false;\n     try {\n-      getServerContext().getZooKeeperRoot();\n+      ZooUtil.getRoot(getServerContext().getInstanceID());\n       macStarted = true;\n     } catch (IllegalStateException e) {\n       if (!e.getMessage().startsWith(\""Accumulo not initialized\"")) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\nindex acb6c80c311..fa8150a8c7a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/ServerContext.java\n@@ -52,7 +52,6 @@\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.lock.ServiceLock;\n import org.apache.accumulo.core.metadata.schema.Ample;\n import org.apache.accumulo.core.metrics.MetricsInfo;\n@@ -81,7 +80,6 @@\n import org.apache.accumulo.server.tablets.UniqueNameAllocator;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.zookeeper.KeeperException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -95,7 +93,6 @@ public class ServerContext extends ClientContext {\n   private final ServerInfo info;\n   private final ServerDirs serverDirs;\n   private final Supplier<ZooPropStore> propStore;\n-  private final Supplier<String> zkUserPath;\n \n   // lazily loaded resources, only loaded when needed\n   private final Supplier<TableManager> tableManager;\n@@ -120,9 +117,8 @@ private ServerContext(ServerInfo info) {\n \n     // the PropStore shouldn't close the ZooKeeper, since ServerContext is responsible for that\n     @SuppressWarnings(\""resource\"")\n-    var tmpPropStore = memoize(() -> ZooPropStore.initialize(getInstanceID(), getZooSession()));\n+    var tmpPropStore = memoize(() -> ZooPropStore.initialize(getZooSession()));\n     propStore = tmpPropStore;\n-    zkUserPath = memoize(() -> ZooUtil.getRoot(getInstanceID()) + Constants.ZUSERS);\n \n     tableManager = memoize(() -> new TableManager(this));\n     nameAllocator = memoize(() -> new UniqueNameAllocator(this));\n@@ -312,15 +308,8 @@ public static void ensureDataVersionCompatible(int dataVersion) {\n \n   public void waitForZookeeperAndHdfs() {\n     log.info(\""Attempting to talk to zookeeper\"");\n-    while (true) {\n-      try {\n-        getZooSession().asReaderWriter().getChildren(Constants.ZROOT);\n-        break;\n-      } catch (InterruptedException | KeeperException ex) {\n-        log.info(\""Waiting for accumulo to be initialized\"");\n-        sleepUninterruptibly(1, SECONDS);\n-      }\n-    }\n+    // Next line blocks until connection is established\n+    getZooSession();\n     log.info(\""ZooKeeper connected and initialized, attempting to talk to HDFS\"");\n     long sleep = 1000;\n     int unknownHostTries = 3;\n@@ -453,10 +442,6 @@ public AuditedSecurityOperation getSecurityOperation() {\n     return securityOperation.get();\n   }\n \n-  public String zkUserPath() {\n-    return zkUserPath.get();\n-  }\n-\n   public LowMemoryDetector getLowMemoryDetector() {\n     return lowMemoryDetector.get();\n   }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java b/server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java\nindex 05b62ebe759..9bd4292e50f 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/client/ClientServiceHandler.java\n@@ -326,7 +326,7 @@ public Map<String,String> getConfiguration(TInfo tinfo, TCredentials credentials\n     checkSystemPermission(credentials);\n     switch (type) {\n       case CURRENT:\n-        context.getPropStore().getCache().remove(SystemPropKey.of(context));\n+        context.getPropStore().getCache().remove(SystemPropKey.of());\n         return conf(credentials, context.getConfiguration());\n       case SITE:\n         return conf(credentials, context.getSiteConfiguration());\n@@ -340,24 +340,24 @@ public Map<String,String> getConfiguration(TInfo tinfo, TCredentials credentials\n   public Map<String,String> getSystemProperties(TInfo tinfo, TCredentials credentials)\n       throws ThriftSecurityException {\n     checkSystemPermission(credentials);\n-    return context.getPropStore().get(SystemPropKey.of(context)).asMap();\n+    return context.getPropStore().get(SystemPropKey.of()).asMap();\n   }\n \n   @Override\n   public TVersionedProperties getVersionedSystemProperties(TInfo tinfo, TCredentials credentials)\n       throws ThriftSecurityException {\n     checkSystemPermission(credentials);\n-    return Optional.of(context.getPropStore().get(SystemPropKey.of(context)))\n+    return Optional.of(context.getPropStore().get(SystemPropKey.of()))\n         .map(vProps -> new TVersionedProperties(vProps.getDataVersion(), vProps.asMap()))\n         .orElseThrow();\n   }\n \n   @Override\n   public Map<String,String> getTableConfiguration(TInfo tinfo, TCredentials credentials,\n-      String tableName) throws TException, ThriftTableOperationException {\n+      String tableName) throws TException {\n     TableId tableId = checkTableId(context, tableName, null);\n     checkTablePermission(credentials, tableId, TablePermission.ALTER_TABLE);\n-    context.getPropStore().getCache().remove(TablePropKey.of(context, tableId));\n+    context.getPropStore().getCache().remove(TablePropKey.of(tableId));\n     AccumuloConfiguration config = context.getTableConfiguration(tableId);\n     return conf(credentials, config);\n   }\n@@ -367,7 +367,7 @@ public Map<String,String> getTableProperties(TInfo tinfo, TCredentials credentia\n       String tableName) throws TException {\n     final TableId tableId = checkTableId(context, tableName, null);\n     checkTablePermission(credentials, tableId, TablePermission.ALTER_TABLE);\n-    return context.getPropStore().get(TablePropKey.of(context, tableId)).asMap();\n+    return context.getPropStore().get(TablePropKey.of(tableId)).asMap();\n   }\n \n   @Override\n@@ -375,7 +375,7 @@ public TVersionedProperties getVersionedTableProperties(TInfo tinfo, TCredential\n       String tableName) throws TException {\n     final TableId tableId = checkTableId(context, tableName, null);\n     checkTablePermission(credentials, tableId, TablePermission.ALTER_TABLE);\n-    return Optional.of(context.getPropStore().get(TablePropKey.of(context, tableId)))\n+    return Optional.of(context.getPropStore().get(TablePropKey.of(tableId)))\n         .map(vProps -> new TVersionedProperties(vProps.getDataVersion(), vProps.asMap()))\n         .orElseThrow();\n   }\n@@ -488,7 +488,7 @@ public Map<String,String> getNamespaceConfiguration(TInfo tinfo, TCredentials cr\n           TableOperationExceptionType.NAMESPACE_NOTFOUND, why);\n     }\n     checkNamespacePermission(credentials, namespaceId, NamespacePermission.ALTER_NAMESPACE);\n-    context.getPropStore().getCache().remove(NamespacePropKey.of(context, namespaceId));\n+    context.getPropStore().getCache().remove(NamespacePropKey.of(namespaceId));\n     AccumuloConfiguration config = context.getNamespaceConfiguration(namespaceId);\n     return conf(credentials, config);\n \n@@ -501,7 +501,7 @@ public Map<String,String> getNamespaceProperties(TInfo tinfo, TCredentials crede\n     try {\n       namespaceId = Namespaces.getNamespaceId(context, ns);\n       checkNamespacePermission(credentials, namespaceId, NamespacePermission.ALTER_NAMESPACE);\n-      return context.getPropStore().get(NamespacePropKey.of(context, namespaceId)).asMap();\n+      return context.getPropStore().get(NamespacePropKey.of(namespaceId)).asMap();\n \n     } catch (NamespaceNotFoundException e) {\n       String why = \""Could not find namespace while getting configuration.\"";\n@@ -517,7 +517,7 @@ public TVersionedProperties getVersionedNamespaceProperties(TInfo tinfo, TCreden\n     try {\n       namespaceId = Namespaces.getNamespaceId(context, ns);\n       checkNamespacePermission(credentials, namespaceId, NamespacePermission.ALTER_NAMESPACE);\n-      return Optional.of(context.getPropStore().get(NamespacePropKey.of(context, namespaceId)))\n+      return Optional.of(context.getPropStore().get(NamespacePropKey.of(namespaceId)))\n           .map(vProps -> new TVersionedProperties(vProps.getDataVersion(), vProps.asMap()))\n           .orElseThrow();\n     } catch (NamespaceNotFoundException e) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/compaction/CompactionConfigStorage.java b/server/base/src/main/java/org/apache/accumulo/server/compaction/CompactionConfigStorage.java\nindex 970a4f1c9bf..3f3cfd37509 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/compaction/CompactionConfigStorage.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/compaction/CompactionConfigStorage.java\n@@ -44,9 +44,8 @@\n public class CompactionConfigStorage {\n   static final String DELIMITER = \""_\"";\n \n-  private static String createPath(ServerContext context, FateId fateId) {\n-    return context.getZooKeeperRoot() + Constants.ZCOMPACTIONS + \""/\"" + fateId.getType() + DELIMITER\n-        + fateId.getTxUUIDStr();\n+  private static String createPath(FateId fateId) {\n+    return Constants.ZCOMPACTIONS + \""/\"" + fateId.getType() + DELIMITER + fateId.getTxUUIDStr();\n   }\n \n   public static byte[] encodeConfig(CompactionConfig config, TableId tableId) {\n@@ -69,7 +68,7 @@ public static CompactionConfig getConfig(ServerContext context, FateId fateId)\n   public static CompactionConfig getConfig(ServerContext context, FateId fateId,\n       Predicate<TableId> tableIdPredicate) throws InterruptedException, KeeperException {\n     try {\n-      byte[] data = context.getZooSession().asReader().getData(createPath(context, fateId));\n+      byte[] data = context.getZooSession().asReader().getData(createPath(fateId));\n       try (ByteArrayInputStream bais = new ByteArrayInputStream(data);\n           DataInputStream dis = new DataInputStream(bais)) {\n         var tableId = TableId.of(dis.readUTF());\n@@ -89,13 +88,13 @@ public static CompactionConfig getConfig(ServerContext context, FateId fateId,\n \n   public static void setConfig(ServerContext context, FateId fateId, byte[] encConfig)\n       throws InterruptedException, KeeperException {\n-    context.getZooSession().asReaderWriter().putPrivatePersistentData(createPath(context, fateId),\n-        encConfig, ZooUtil.NodeExistsPolicy.SKIP);\n+    context.getZooSession().asReaderWriter().putPrivatePersistentData(createPath(fateId), encConfig,\n+        ZooUtil.NodeExistsPolicy.SKIP);\n   }\n \n   public static void deleteConfig(ServerContext context, FateId fateId)\n       throws InterruptedException, KeeperException {\n-    context.getZooSession().asReaderWriter().delete(createPath(context, fateId));\n+    context.getZooSession().asReaderWriter().delete(createPath(fateId));\n   }\n \n   public static Map<FateId,CompactionConfig> getAllConfig(ServerContext context,\n@@ -103,8 +102,7 @@ public static Map<FateId,CompactionConfig> getAllConfig(ServerContext context,\n \n     Map<FateId,CompactionConfig> configs = new HashMap<>();\n \n-    var children = context.getZooSession().asReader()\n-        .getChildren(context.getZooKeeperRoot() + Constants.ZCOMPACTIONS);\n+    var children = context.getZooSession().asReader().getChildren(Constants.ZCOMPACTIONS);\n     for (var child : children) {\n       String[] fields = child.split(DELIMITER);\n       Preconditions.checkState(fields.length == 2, \""Unexpected child %s\"", child);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java b/server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java\nindex 4a0cfbb48b4..7f0992726f1 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/NamespaceConfiguration.java\n@@ -37,7 +37,7 @@ public class NamespaceConfiguration extends ZooBasedConfiguration {\n \n   public NamespaceConfiguration(ServerContext context, NamespaceId namespaceId,\n       AccumuloConfiguration parent) {\n-    super(log, context, NamespacePropKey.of(context, namespaceId), parent);\n+    super(log, context, NamespacePropKey.of(namespaceId), parent);\n   }\n \n   @Override\n@@ -45,7 +45,7 @@ public String get(Property property) {\n \n     String key = property.getKey();\n \n-    var namespaceId = getPropStoreKey().getId();\n+    var namespaceId = ((NamespacePropKey) getPropStoreKey()).getId();\n     if (namespaceId != null && namespaceId.equals(Namespace.ACCUMULO.id())\n         && isIteratorOrConstraint(key)) {\n       // ignore iterators from parent if system namespace\n@@ -82,7 +82,7 @@ public void getProperties(Map<String,String> props, Predicate<String> filter) {\n   }\n \n   protected NamespaceId getNamespaceId() {\n-    NamespaceId id = (NamespaceId) getPropStoreKey().getId();\n+    var id = ((NamespacePropKey) getPropStoreKey()).getId();\n     if (id == null) {\n       throw new IllegalArgumentException(\n           \""Invalid request for namespace id on \"" + getPropStoreKey());\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java b/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\nindex 5cf3d8d3051..60c16af044c 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfigurationFactory.java\n@@ -78,8 +78,8 @@ public class ServerConfigurationFactory extends ServerConfiguration {\n   public ServerConfigurationFactory(ServerContext context, SiteConfiguration siteConfig) {\n     this.context = context;\n     this.siteConfig = siteConfig;\n-    this.systemConfig = memoize(() -> new SystemConfiguration(context,\n-        SystemPropKey.of(context.getInstanceID()), siteConfig));\n+    this.systemConfig =\n+        memoize(() -> new SystemConfiguration(context, SystemPropKey.of(), siteConfig));\n     tableParentConfigs =\n         Caches.getInstance().createNewBuilder(CacheName.TABLE_PARENT_CONFIGS, false)\n             .expireAfterAccess(CACHE_EXPIRATION_HRS, TimeUnit.HOURS).build();\n@@ -114,7 +114,7 @@ public AccumuloConfiguration getSystemConfiguration() {\n   public TableConfiguration getTableConfiguration(TableId tableId) {\n     return tableConfigs.get(tableId, key -> {\n       if (context.tableNodeExists(tableId)) {\n-        context.getPropStore().registerAsListener(TablePropKey.of(context, tableId), changeWatcher);\n+        context.getPropStore().registerAsListener(TablePropKey.of(tableId), changeWatcher);\n         var conf =\n             new TableConfiguration(context, tableId, getNamespaceConfigurationForTable(tableId));\n         ConfigCheckUtil.validate(conf, \""table id: \"" + tableId.toString());\n@@ -138,8 +138,7 @@ public NamespaceConfiguration getNamespaceConfigurationForTable(TableId tableId)\n   @Override\n   public NamespaceConfiguration getNamespaceConfiguration(NamespaceId namespaceId) {\n     return namespaceConfigs.get(namespaceId, key -> {\n-      context.getPropStore().registerAsListener(NamespacePropKey.of(context, namespaceId),\n-          changeWatcher);\n+      context.getPropStore().registerAsListener(NamespacePropKey.of(namespaceId), changeWatcher);\n       var conf = new NamespaceConfiguration(context, namespaceId, getSystemConfiguration());\n       ConfigCheckUtil.validate(conf, \""namespace id: \"" + namespaceId.toString());\n       return conf;\n@@ -149,21 +148,21 @@ public NamespaceConfiguration getNamespaceConfiguration(NamespaceId namespaceId)\n   private class ChangeWatcher implements PropChangeListener {\n \n     @Override\n-    public void zkChangeEvent(PropStoreKey<?> propStoreKey) {\n+    public void zkChangeEvent(PropStoreKey propStoreKey) {\n       clearLocalOnEvent(propStoreKey);\n     }\n \n     @Override\n-    public void cacheChangeEvent(PropStoreKey<?> propStoreKey) {\n+    public void cacheChangeEvent(PropStoreKey propStoreKey) {\n       clearLocalOnEvent(propStoreKey);\n     }\n \n     @Override\n-    public void deleteEvent(PropStoreKey<?> propStoreKey) {\n+    public void deleteEvent(PropStoreKey propStoreKey) {\n       clearLocalOnEvent(propStoreKey);\n     }\n \n-    private void clearLocalOnEvent(PropStoreKey<?> propStoreKey) {\n+    private void clearLocalOnEvent(PropStoreKey propStoreKey) {\n       // clearing the local secondary cache stored in this class forces a re-read from the prop\n       // store\n       // to guarantee that the updated vales(s) are re-read on a ZooKeeper change.\n@@ -223,7 +222,7 @@ private void verifySnapshotVersions() {\n       keyCount++;\n \n       // rely on store to propagate change event if different\n-      propStore.validateDataVersion(SystemPropKey.of(context),\n+      propStore.validateDataVersion(SystemPropKey.of(),\n           ((ZooBasedConfiguration) getSystemConfiguration()).getDataVersion());\n       // small yield - spread out ZooKeeper calls\n       jitterDelay();\n@@ -231,7 +230,7 @@ private void verifySnapshotVersions() {\n       for (Map.Entry<NamespaceId,NamespaceConfiguration> entry : namespaceConfigs.asMap()\n           .entrySet()) {\n         keyCount++;\n-        PropStoreKey<?> propKey = NamespacePropKey.of(context, entry.getKey());\n+        PropStoreKey propKey = NamespacePropKey.of(entry.getKey());\n         if (!propStore.validateDataVersion(propKey, entry.getValue().getDataVersion())) {\n           keyChangedCount++;\n           namespaceConfigs.invalidate(entry.getKey());\n@@ -243,7 +242,7 @@ private void verifySnapshotVersions() {\n       for (Map.Entry<TableId,TableConfiguration> entry : tableConfigs.asMap().entrySet()) {\n         keyCount++;\n         TableId tid = entry.getKey();\n-        PropStoreKey<?> propKey = TablePropKey.of(context, tid);\n+        PropStoreKey propKey = TablePropKey.of(tid);\n         if (!propStore.validateDataVersion(propKey, entry.getValue().getDataVersion())) {\n           keyChangedCount++;\n           tableConfigs.invalidate(tid);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java b/server/base/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java\nindex d0699e8a1d7..ef0cfcdaed0 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java\n@@ -61,7 +61,7 @@ public class TableConfiguration extends ZooBasedConfiguration {\n   private final Deriver<CryptoService> cryptoServiceDeriver;\n \n   public TableConfiguration(ServerContext context, TableId tableId, NamespaceConfiguration parent) {\n-    super(log, context, TablePropKey.of(context, tableId), parent);\n+    super(log, context, TablePropKey.of(tableId), parent);\n     this.tableId = tableId;\n \n     iteratorConfig = new EnumMap<>(IteratorScope.class);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java b/server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java\nindex 24cbe7260ce..8119cbf30bb 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/ZooBasedConfiguration.java\n@@ -49,11 +49,11 @@ public class ZooBasedConfiguration extends AccumuloConfiguration {\n \n   protected final Logger log;\n   private final AccumuloConfiguration parent;\n-  private final PropStoreKey<?> propStoreKey;\n+  private final PropStoreKey propStoreKey;\n \n   private final PropSnapshot propSnapshot;\n \n-  public ZooBasedConfiguration(Logger log, ServerContext context, PropStoreKey<?> propStoreKey,\n+  public ZooBasedConfiguration(Logger log, ServerContext context, PropStoreKey propStoreKey,\n       AccumuloConfiguration parent) {\n     this.log = requireNonNull(log, \""a Logger must be supplied\"");\n     requireNonNull(context, \""the context cannot be null\"");\n@@ -64,7 +64,7 @@ public ZooBasedConfiguration(Logger log, ServerContext context, PropStoreKey<?>\n   }\n \n   @VisibleForTesting\n-  public void zkChangeEvent(PropStoreKey<?> propStoreKey) {\n+  public void zkChangeEvent(PropStoreKey propStoreKey) {\n     propSnapshot.zkChangeEvent(propStoreKey);\n   }\n \n@@ -103,7 +103,7 @@ public AccumuloConfiguration getParent() {\n     return parent;\n   }\n \n-  public PropStoreKey<?> getPropStoreKey() {\n+  public PropStoreKey getPropStoreKey() {\n     return propStoreKey;\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/IdBasedPropStoreKey.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/IdBasedPropStoreKey.java\nnew file mode 100644\nindex 00000000000..6848db320ca\n--- /dev/null\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/IdBasedPropStoreKey.java\n@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.server.conf.store;\n+\n+import org.apache.accumulo.core.data.AbstractId;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n+\n+/**\n+ * Provides a strongly-typed id for storing properties in ZooKeeper based on a specific AbstractId\n+ * type. The path is based on the AbstractId type, and is the canonical String version of this key.\n+ */\n+public abstract class IdBasedPropStoreKey<ID_TYPE extends AbstractId<ID_TYPE>>\n+    extends PropStoreKey {\n+\n+  private final ID_TYPE id;\n+\n+  protected IdBasedPropStoreKey(final String path, final ID_TYPE id) {\n+    super(path);\n+    this.id = id;\n+  }\n+\n+  public @NonNull ID_TYPE getId() {\n+    return id;\n+  }\n+\n+}\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/NamespacePropKey.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/NamespacePropKey.java\nindex 6ea67cc380a..b92f6d66734 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/NamespacePropKey.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/NamespacePropKey.java\n@@ -21,26 +21,16 @@\n import static org.apache.accumulo.core.Constants.ZCONFIG;\n import static org.apache.accumulo.core.Constants.ZNAMESPACES;\n \n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n-import org.apache.accumulo.server.ServerContext;\n \n-public class NamespacePropKey extends PropStoreKey<NamespaceId> {\n+public class NamespacePropKey extends IdBasedPropStoreKey<NamespaceId> {\n \n-  private NamespacePropKey(final InstanceId instanceId, final String path, final NamespaceId id) {\n-    super(instanceId, path, id);\n+  private NamespacePropKey(final NamespaceId id) {\n+    super(ZNAMESPACES + \""/\"" + id.canonical() + ZCONFIG, id);\n   }\n \n-  public static NamespacePropKey of(final ServerContext context, final NamespaceId id) {\n-    return of(context.getInstanceID(), id);\n+  public static NamespacePropKey of(final NamespaceId id) {\n+    return new NamespacePropKey(id);\n   }\n \n-  public static NamespacePropKey of(final InstanceId instanceId, final NamespaceId id) {\n-    return new NamespacePropKey(instanceId, buildNodePath(instanceId, id), id);\n-  }\n-\n-  private static String buildNodePath(final InstanceId instanceId, final NamespaceId id) {\n-    return ZooUtil.getRoot(instanceId) + ZNAMESPACES + \""/\"" + id.canonical() + ZCONFIG;\n-  }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCache.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCache.java\nindex 41a51f314b6..71264d81f9c 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCache.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropCache.java\n@@ -24,9 +24,9 @@\n public interface PropCache {\n \n   @Nullable\n-  VersionedProperties get(final PropStoreKey<?> propStoreKey);\n+  VersionedProperties get(final PropStoreKey propStoreKey);\n \n-  void remove(final PropStoreKey<?> propStoreKey);\n+  void remove(final PropStoreKey propStoreKey);\n \n   void removeAll();\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropChangeListener.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropChangeListener.java\nindex 313436b4b3d..62ef22b0ecf 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropChangeListener.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropChangeListener.java\n@@ -23,18 +23,18 @@ public interface PropChangeListener {\n   /**\n    * Signal that a ZooKeeper data change event occurred and that the data has changed.\n    */\n-  void zkChangeEvent(final PropStoreKey<?> propStoreKey);\n+  void zkChangeEvent(final PropStoreKey propStoreKey);\n \n   /**\n    * Signal that a cache change event occurred - cache change events occur on eviction or\n    * invalidation of the cache entry. The underlying data may or may not have changed.\n    */\n-  void cacheChangeEvent(final PropStoreKey<?> propStoreKey);\n+  void cacheChangeEvent(final PropStoreKey propStoreKey);\n \n   /**\n    * Signal that the node had been deleted from ZooKeeper.\n    */\n-  void deleteEvent(final PropStoreKey<?> propStoreKey);\n+  void deleteEvent(final PropStoreKey propStoreKey);\n \n   /**\n    * A ZooKeeper connection event (session closed, expired...) and that\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStore.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStore.java\nindex 3d586e43292..ecb0b89d720 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStore.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStore.java\n@@ -35,7 +35,7 @@ public interface PropStore {\n    * @return true if the property node exists, false otherwise.\n    * @throws IllegalStateException if the check fails due to interrupt.\n    */\n-  boolean exists(PropStoreKey<?> propStoreKey);\n+  boolean exists(PropStoreKey propStoreKey);\n \n   /**\n    * Create an initial entry for the PropCacheId. If properties already exist, they are not\n@@ -45,7 +45,7 @@ public interface PropStore {\n    * @param props a map of property k,v pairs\n    * @throws IllegalStateException if the updates fails because of an underlying store exception\n    */\n-  void create(PropStoreKey<?> propStoreKey, Map<String,String> props);\n+  void create(PropStoreKey propStoreKey, Map<String,String> props);\n \n   /**\n    * @param propCacheId the prop cache key\n@@ -54,7 +54,7 @@ public interface PropStore {\n    *         if the properties do not exist for the propCacheId\n    */\n   @NonNull\n-  VersionedProperties get(PropStoreKey<?> propCacheId);\n+  VersionedProperties get(PropStoreKey propCacheId);\n \n   /**\n    * Adds or updates current properties. If the property currently exists it is overwritten,\n@@ -65,7 +65,7 @@ public interface PropStore {\n    * @throws IllegalStateException if the values cannot be written or if an underlying store\n    *         exception occurs.\n    */\n-  void putAll(PropStoreKey<?> propStoreKey, Map<String,String> props);\n+  void putAll(PropStoreKey propStoreKey, Map<String,String> props);\n \n   /**\n    * Replaces all current properties with map provided. If a property is not included in the new\n@@ -79,7 +79,7 @@ public interface PropStore {\n    * @throws java.util.ConcurrentModificationException if the properties changed since reading and\n    *         can not be modified\n    */\n-  void replaceAll(PropStoreKey<?> propStoreKey, long version, Map<String,String> props)\n+  void replaceAll(PropStoreKey propStoreKey, long version, Map<String,String> props)\n       throws ConcurrentModificationException;\n \n   /**\n@@ -88,7 +88,7 @@ void replaceAll(PropStoreKey<?> propStoreKey, long version, Map<String,String> p\n    * @param propStoreKey the prop cache key\n    * @throws IllegalStateException if the updates fails because of an underlying store exception\n    */\n-  void delete(PropStoreKey<?> propStoreKey);\n+  void delete(PropStoreKey propStoreKey);\n \n   /**\n    * Deletes individual properties specified by the set of keys.\n@@ -98,7 +98,7 @@ void replaceAll(PropStoreKey<?> propStoreKey, long version, Map<String,String> p\n    * @throws IllegalStateException if the values cannot be deleted or if an underlying store\n    *         exception occurs.\n    */\n-  void removeProperties(PropStoreKey<?> propStoreKey, Collection<String> keys);\n+  void removeProperties(PropStoreKey propStoreKey, Collection<String> keys);\n \n   /**\n    * External processes can register for notifications if the properties change. Normally processes\n@@ -113,12 +113,12 @@ void replaceAll(PropStoreKey<?> propStoreKey, long version, Map<String,String> p\n    * @param propStoreKey the prop cache key\n    * @param listener a listener\n    */\n-  void registerAsListener(PropStoreKey<?> propStoreKey, PropChangeListener listener);\n+  void registerAsListener(PropStoreKey propStoreKey, PropChangeListener listener);\n \n   PropCache getCache();\n \n   @Nullable\n-  VersionedProperties getIfCached(PropStoreKey<?> propStoreKey);\n+  VersionedProperties getIfCached(PropStoreKey propStoreKey);\n \n   /**\n    * Compare the stored data version with the expected version. Notifies subscribers of the change\n@@ -128,5 +128,5 @@ void replaceAll(PropStoreKey<?> propStoreKey, long version, Map<String,String> p\n    * @param expectedVersion the expected data version\n    * @return true if the stored version matches the provided expected version.\n    */\n-  boolean validateDataVersion(PropStoreKey<?> storeKey, long expectedVersion);\n+  boolean validateDataVersion(PropStoreKey storeKey, long expectedVersion);\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java\nindex 4ee2abd818a..fc1b264d455 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/PropStoreKey.java\n@@ -25,99 +25,75 @@\n import java.util.Comparator;\n import java.util.Objects;\n \n-import org.apache.accumulo.core.data.AbstractId;\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.checkerframework.checker.nullness.qual.NonNull;\n import org.checkerframework.checker.nullness.qual.Nullable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * Provides a strongly-typed id for storing properties in ZooKeeper. The path in ZooKeeper is\n- * determined by the instance id and the type (system, namespace and table), with different root\n- * paths.\n+ * Provides a strongly-typed id for storing properties in ZooKeeper. The provided path is the\n+ * canonical String version of this key, because the path is the location in ZooKeeper where this\n+ * prop store is stored, so the path is the essential characteristic of the key.\n+ *\n  * <p>\n  * Provides utility methods from constructing different id based on type and methods to parse a\n  * ZooKeeper path and return a prop cache id.\n  */\n-public abstract class PropStoreKey<ID_TYPE extends AbstractId<ID_TYPE>>\n-    implements Comparable<PropStoreKey<ID_TYPE>> {\n+public abstract class PropStoreKey implements Comparable<PropStoreKey> {\n \n   private static final Logger log = LoggerFactory.getLogger(PropStoreKey.class);\n \n   // indices for path.split() on config node paths;\n-  public static final int TYPE_TOKEN_POSITION = 3;\n-  public static final int IID_TOKEN_POSITION = 2;\n-  public static final int ID_TOKEN_POSITION = 4;\n+  public static final int TYPE_TOKEN_POSITION = 1;\n+  public static final int ID_TOKEN_POSITION = 2;\n \n   // remove starting slash from constant.\n   public static final String TABLES_NODE_NAME = ZTABLES.substring(1);\n   public static final String NAMESPACE_NODE_NAME = ZNAMESPACES.substring(1);\n   // expected token length for table and namespace config\n-  public static final int EXPECTED_CONFIG_LEN = 6;\n+  public static final int EXPECTED_CONFIG_LEN = 4;\n   // expected token length for sys config\n-  public static final int EXPECTED_SYS_CONFIG_LEN = 4;\n-\n-  protected final InstanceId instanceId;\n-  protected final ID_TYPE id;\n+  public static final int EXPECTED_SYS_CONFIG_LEN = 2;\n \n   private final String path;\n \n-  protected PropStoreKey(final InstanceId instanceId, final String path, final ID_TYPE id) {\n-    this.instanceId = instanceId;\n-    this.path = path;\n-    this.id = id;\n+  protected PropStoreKey(final String path) {\n+    this.path = Objects.requireNonNull(path);\n   }\n \n   public @NonNull String getPath() {\n     return path;\n   }\n \n-  public @NonNull ID_TYPE getId() {\n-    return id;\n-  }\n-\n   /**\n    * Determine the prop cache id from a ZooKeeper path\n    *\n    * @param path the path\n    * @return the prop cache id\n    */\n-  public static @Nullable PropStoreKey<?> fromPath(final String path) {\n+  public static @Nullable PropStoreKey fromPath(final String path) {\n     String[] tokens = path.split(\""/\"");\n \n-    if (tokens.length < 1\n-        || tokens.length != EXPECTED_CONFIG_LEN && tokens.length != EXPECTED_SYS_CONFIG_LEN) {\n+    if (tokens.length != EXPECTED_CONFIG_LEN && tokens.length != EXPECTED_SYS_CONFIG_LEN) {\n       log.warn(\""Path '{}' is an invalid path for a property cache key - bad length\"", path);\n       return null;\n     }\n \n-    InstanceId instanceId = InstanceId.of(tokens[IID_TOKEN_POSITION]);\n-\n-    // needs to start with /accumulo/[instanceId]\n-    if (!path.startsWith(ZooUtil.getRoot(instanceId))) {\n-      log.warn(\n-          \""Path '{}' is invalid for a property cache key, expected to start with /accumulo/{}}\"",\n-          path, instanceId);\n-      return null;\n-    }\n-\n     String nodeName = \""/\"" + tokens[tokens.length - 1];\n     if (tokens.length == EXPECTED_CONFIG_LEN && tokens[TYPE_TOKEN_POSITION].equals(TABLES_NODE_NAME)\n         && nodeName.equals(ZCONFIG)) {\n-      return TablePropKey.of(instanceId, TableId.of(tokens[ID_TOKEN_POSITION]));\n+      return TablePropKey.of(TableId.of(tokens[ID_TOKEN_POSITION]));\n     }\n \n     if (tokens.length == EXPECTED_CONFIG_LEN\n         && tokens[TYPE_TOKEN_POSITION].equals(NAMESPACE_NODE_NAME) && nodeName.equals(ZCONFIG)) {\n-      return NamespacePropKey.of(instanceId, NamespaceId.of(tokens[ID_TOKEN_POSITION]));\n+      return NamespacePropKey.of(NamespaceId.of(tokens[ID_TOKEN_POSITION]));\n     }\n \n     if (tokens.length == EXPECTED_SYS_CONFIG_LEN && nodeName.equals(ZCONFIG)) {\n-      return SystemPropKey.of(instanceId);\n+      return SystemPropKey.of();\n     }\n     // without tokens or it does not end with PROP_NAME_NAME\n     log.warn(\""Path '{}' is an invalid path for a property cache key\"", path);\n@@ -125,8 +101,8 @@ protected PropStoreKey(final InstanceId instanceId, final String path, final ID_\n   }\n \n   @Override\n-  public int compareTo(@NonNull PropStoreKey<ID_TYPE> other) {\n-    return Comparator.comparing(PropStoreKey<ID_TYPE>::getPath).compare(this, other);\n+  public int compareTo(@NonNull PropStoreKey other) {\n+    return Comparator.comparing(PropStoreKey::getPath).compare(this, other);\n   }\n \n   @Override\n@@ -137,11 +113,7 @@ public boolean equals(Object o) {\n     if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-    PropStoreKey<?> that = (PropStoreKey<?>) o;\n-    if (getId().getClass() != that.getId().getClass()) {\n-      return false;\n-    }\n-    return path.equals(that.path);\n+    return path.equals(((PropStoreKey) o).path);\n   }\n \n   @Override\n@@ -151,7 +123,6 @@ public int hashCode() {\n \n   @Override\n   public String toString() {\n-    return this.getClass().getSimpleName() + \""{\"" + getId().getClass().getSimpleName() + \""=\""\n-        + getId().canonical() + \""}\"";\n+    return this.getClass().getSimpleName() + \""{path=\"" + getPath() + \""}\"";\n   }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/SystemPropKey.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/SystemPropKey.java\nindex 9ae84bf2fd1..74379b9ddef 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/SystemPropKey.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/SystemPropKey.java\n@@ -20,26 +20,14 @@\n \n import static org.apache.accumulo.core.Constants.ZCONFIG;\n \n-import org.apache.accumulo.core.data.InstanceId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n-import org.apache.accumulo.server.ServerContext;\n+public class SystemPropKey extends PropStoreKey {\n \n-public class SystemPropKey extends PropStoreKey<InstanceId> {\n-\n-  private SystemPropKey(final InstanceId instanceId, final String path) {\n-    super(instanceId, path, instanceId);\n-  }\n-\n-  public static SystemPropKey of(final ServerContext context) {\n-    return of(context.getInstanceID());\n-  }\n-\n-  public static SystemPropKey of(final InstanceId instanceId) {\n-    return new SystemPropKey(instanceId, buildNodePath(instanceId));\n+  private SystemPropKey(final String path) {\n+    super(path);\n   }\n \n-  private static String buildNodePath(final InstanceId instanceId) {\n-    return ZooUtil.getRoot(instanceId) + ZCONFIG;\n+  public static SystemPropKey of() {\n+    return new SystemPropKey(ZCONFIG);\n   }\n \n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/TablePropKey.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/TablePropKey.java\nindex e6c2c68a5d6..a967f392cce 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/TablePropKey.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/TablePropKey.java\n@@ -21,26 +21,15 @@\n import static org.apache.accumulo.core.Constants.ZCONFIG;\n import static org.apache.accumulo.core.Constants.ZTABLES;\n \n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n-import org.apache.accumulo.server.ServerContext;\n \n-public class TablePropKey extends PropStoreKey<TableId> {\n+public class TablePropKey extends IdBasedPropStoreKey<TableId> {\n \n-  public static TablePropKey of(final ServerContext context, final TableId tableId) {\n-    return of(context.getInstanceID(), tableId);\n+  private TablePropKey(final TableId tableId) {\n+    super(ZTABLES + \""/\"" + tableId.canonical() + ZCONFIG, tableId);\n   }\n \n-  public static TablePropKey of(final InstanceId instanceId, final TableId tableId) {\n-    return new TablePropKey(instanceId, buildNodePath(instanceId, tableId), tableId);\n-  }\n-\n-  private TablePropKey(final InstanceId instanceId, final String path, final TableId tableId) {\n-    super(instanceId, path, tableId);\n-  }\n-\n-  private static String buildNodePath(final InstanceId instanceId, final TableId id) {\n-    return ZooUtil.getRoot(instanceId) + ZTABLES + \""/\"" + id.canonical() + ZCONFIG;\n+  public static TablePropKey of(final TableId tableId) {\n+    return new TablePropKey(tableId);\n   }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java\nindex e6395c4b118..46b62a4362d 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImpl.java\n@@ -50,9 +50,9 @@ public class PropCacheCaffeineImpl implements PropCache {\n       ThreadPools.getServerThreadPools().getPoolBuilder(\""caffeine.prop.cache.tasks\"")\n           .numCoreThreads(1).numMaxThreads(20).withTimeOut(60L, SECONDS).build();\n \n-  private final LoadingCache<PropStoreKey<?>,VersionedProperties> cache;\n+  private final LoadingCache<PropStoreKey,VersionedProperties> cache;\n \n-  private PropCacheCaffeineImpl(final CacheLoader<PropStoreKey<?>,VersionedProperties> cacheLoader,\n+  private PropCacheCaffeineImpl(final CacheLoader<PropStoreKey,VersionedProperties> cacheLoader,\n       final Ticker ticker, boolean runTasksInline) {\n     Caffeine<Object,Object> caffeine =\n         Caches.getInstance().createNewBuilder(CacheName.PROP_CACHE, true)\n@@ -68,13 +68,12 @@ private PropCacheCaffeineImpl(final CacheLoader<PropStoreKey<?>,VersionedPropert\n     cache = caffeine.evictionListener(this::evictionNotifier).build(cacheLoader);\n   }\n \n-  void evictionNotifier(PropStoreKey<?> propStoreKey, VersionedProperties value,\n-      RemovalCause cause) {\n+  void evictionNotifier(PropStoreKey propStoreKey, VersionedProperties value, RemovalCause cause) {\n     log.trace(\""Evicted: ID: {} was evicted from cache. Reason: {}\"", propStoreKey, cause);\n   }\n \n   @Override\n-  public @Nullable VersionedProperties get(PropStoreKey<?> propStoreKey) {\n+  public @Nullable VersionedProperties get(PropStoreKey propStoreKey) {\n     log.trace(\""Called get() for {}\"", propStoreKey);\n     try {\n       return cache.get(propStoreKey);\n@@ -85,7 +84,7 @@ void evictionNotifier(PropStoreKey<?> propStoreKey, VersionedProperties value,\n   }\n \n   @Override\n-  public void remove(PropStoreKey<?> propStoreKey) {\n+  public void remove(PropStoreKey propStoreKey) {\n     log.trace(\""clear {} from cache\"", propStoreKey);\n     cache.invalidate(propStoreKey);\n   }\n@@ -105,7 +104,7 @@ public void removeAll() {\n    * @param propStoreKey the property id\n    * @return the version properties if cached, otherwise return null.\n    */\n-  public @Nullable VersionedProperties getIfCached(PropStoreKey<?> propStoreKey) {\n+  public @Nullable VersionedProperties getIfCached(PropStoreKey propStoreKey) {\n     return cache.getIfPresent(propStoreKey);\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java\nindex c31afcbb211..90bd360366e 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTask.java\n@@ -29,7 +29,7 @@\n  */\n public abstract class PropStoreEventTask implements Runnable {\n \n-  private final PropStoreKey<?> propStoreKey;\n+  private final PropStoreKey propStoreKey;\n   private final Set<PropChangeListener> listeners;\n \n   /**\n@@ -48,7 +48,7 @@ private PropStoreEventTask(final Set<PropChangeListener> listeners) {\n    * @param propStoreKey the prop cache key\n    * @param listeners the set of listeners\n    */\n-  private PropStoreEventTask(final PropStoreKey<?> propStoreKey,\n+  private PropStoreEventTask(final PropStoreKey propStoreKey,\n       final Set<PropChangeListener> listeners) {\n     this.propStoreKey = propStoreKey;\n     this.listeners = listeners;\n@@ -56,7 +56,7 @@ private PropStoreEventTask(final PropStoreKey<?> propStoreKey,\n \n   public static class PropStoreZkChangeEventTask extends PropStoreEventTask {\n \n-    PropStoreZkChangeEventTask(final PropStoreKey<?> propStoreKey,\n+    PropStoreZkChangeEventTask(final PropStoreKey propStoreKey,\n         final Set<PropChangeListener> listeners) {\n       super(propStoreKey, listeners);\n     }\n@@ -69,7 +69,7 @@ public void run() {\n \n   public static class PropStoreCacheChangeEventTask extends PropStoreEventTask {\n \n-    PropStoreCacheChangeEventTask(final PropStoreKey<?> propStoreKey,\n+    PropStoreCacheChangeEventTask(final PropStoreKey propStoreKey,\n         final Set<PropChangeListener> listeners) {\n       super(propStoreKey, listeners);\n     }\n@@ -82,7 +82,7 @@ public void run() {\n \n   public static class PropStoreDeleteEventTask extends PropStoreEventTask {\n \n-    PropStoreDeleteEventTask(final PropStoreKey<?> propStoreKey,\n+    PropStoreDeleteEventTask(final PropStoreKey propStoreKey,\n         final Set<PropChangeListener> listeners) {\n       super(propStoreKey, listeners);\n     }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java\nindex d4fdf455f6d..f860e9a5cfb 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/PropStoreWatcher.java\n@@ -64,7 +64,7 @@ public class PropStoreWatcher implements Watcher {\n   private final ReentrantReadWriteLock.WriteLock listenerWriteLock = listenerLock.writeLock();\n \n   // access should be guarded by acquiring the listener read or write lock\n-  private final Map<PropStoreKey<?>,Set<PropChangeListener>> listeners = new HashMap<>();\n+  private final Map<PropStoreKey,Set<PropChangeListener>> listeners = new HashMap<>();\n \n   private final ReadyMonitor zkReadyMonitor;\n \n@@ -72,8 +72,7 @@ public PropStoreWatcher(final ReadyMonitor zkReadyMonitor) {\n     this.zkReadyMonitor = zkReadyMonitor;\n   }\n \n-  public void registerListener(final PropStoreKey<?> propStoreKey,\n-      final PropChangeListener listener) {\n+  public void registerListener(final PropStoreKey propStoreKey, final PropChangeListener listener) {\n     listenerWriteLock.lock();\n     try {\n       Set<PropChangeListener> set = listeners.computeIfAbsent(propStoreKey, s -> new HashSet<>());\n@@ -95,7 +94,7 @@ public void registerListener(final PropStoreKey<?> propStoreKey,\n   public void process(final WatchedEvent event) {\n \n     String path;\n-    PropStoreKey<?> propStoreKey;\n+    PropStoreKey propStoreKey;\n     switch (event.getType()) {\n       case NodeDataChanged:\n         path = event.getPath();\n@@ -164,7 +163,7 @@ public void process(final WatchedEvent event) {\n    *\n    * @param propStoreKey the cache id\n    */\n-  public void signalZkChangeEvent(@NonNull final PropStoreKey<?> propStoreKey) {\n+  public void signalZkChangeEvent(@NonNull final PropStoreKey propStoreKey) {\n     log.trace(\""signal ZooKeeper change event: {}\"", propStoreKey);\n     Set<PropChangeListener> snapshot = getListenerSnapshot(propStoreKey);\n     log.trace(\""Sending change event to: {}\"", snapshot);\n@@ -180,7 +179,7 @@ public void signalZkChangeEvent(@NonNull final PropStoreKey<?> propStoreKey) {\n    *\n    * @param propStoreKey the cache id\n    */\n-  public void signalCacheChangeEvent(final PropStoreKey<?> propStoreKey) {\n+  public void signalCacheChangeEvent(final PropStoreKey propStoreKey) {\n     log.trace(\""cache change event: {}\"", propStoreKey);\n     Set<PropChangeListener> snapshot = getListenerSnapshot(propStoreKey);\n     if (snapshot != null) {\n@@ -195,7 +194,7 @@ public void signalCacheChangeEvent(final PropStoreKey<?> propStoreKey) {\n    *\n    * @param propStoreKey the cache id\n    */\n-  public void listenerCleanup(final PropStoreKey<?> propStoreKey) {\n+  public void listenerCleanup(final PropStoreKey propStoreKey) {\n     listenerWriteLock.lock();\n     try {\n       listeners.remove(propStoreKey);\n@@ -211,7 +210,7 @@ public void listenerCleanup(final PropStoreKey<?> propStoreKey) {\n    * @param propStoreKey the prop cache id\n    * @return an immutable copy of listeners.\n    */\n-  private Set<PropChangeListener> getListenerSnapshot(final PropStoreKey<?> propStoreKey) {\n+  private Set<PropChangeListener> getListenerSnapshot(final PropStoreKey propStoreKey) {\n \n     Set<PropChangeListener> snapshot = null;\n     listenerReadLock.lock();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java\nindex 2ae183122bf..5f644161b7e 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoader.java\n@@ -36,7 +36,7 @@\n \n import com.github.benmanes.caffeine.cache.CacheLoader;\n \n-public class ZooPropLoader implements CacheLoader<PropStoreKey<?>,VersionedProperties> {\n+public class ZooPropLoader implements CacheLoader<PropStoreKey,VersionedProperties> {\n \n   private static final Logger log = LoggerFactory.getLogger(ZooPropLoader.class);\n \n@@ -53,7 +53,7 @@ public ZooPropLoader(final ZooSession zk, final VersionedPropCodec propCodec,\n   }\n \n   @Override\n-  public @Nullable VersionedProperties load(PropStoreKey<?> propStoreKey) {\n+  public @Nullable VersionedProperties load(PropStoreKey propStoreKey) {\n     try {\n       log.trace(\""load called for {}\"", propStoreKey);\n \n@@ -77,14 +77,14 @@ public ZooPropLoader(final ZooSession zk, final VersionedPropCodec propCodec,\n   }\n \n   @Override\n-  public CompletableFuture<? extends VersionedProperties> asyncLoad(PropStoreKey<?> propStoreKey,\n+  public CompletableFuture<? extends VersionedProperties> asyncLoad(PropStoreKey propStoreKey,\n       Executor executor) throws Exception {\n     log.trace(\""asyncLoad called for key: {}\"", propStoreKey);\n     return CacheLoader.super.asyncLoad(propStoreKey, executor);\n   }\n \n   @Override\n-  public CompletableFuture<VersionedProperties> asyncReload(PropStoreKey<?> propStoreKey,\n+  public CompletableFuture<VersionedProperties> asyncReload(PropStoreKey propStoreKey,\n       VersionedProperties oldValue, Executor executor) throws Exception {\n     log.trace(\""asyncReload called for key: {}\"", propStoreKey);\n     return CompletableFuture.supplyAsync(() -> loadIfDifferentVersion(propStoreKey, oldValue),\n@@ -92,7 +92,7 @@ public CompletableFuture<VersionedProperties> asyncReload(PropStoreKey<?> propSt\n   }\n \n   @Override\n-  public @Nullable VersionedProperties reload(PropStoreKey<?> propStoreKey,\n+  public @Nullable VersionedProperties reload(PropStoreKey propStoreKey,\n       VersionedProperties oldValue) throws Exception {\n     log.trace(\""reload called for: {}\"", propStoreKey);\n     return loadIfDifferentVersion(propStoreKey, oldValue);\n@@ -110,7 +110,7 @@ public CompletableFuture<VersionedProperties> asyncReload(PropStoreKey<?> propSt\n    * @return versioned properties that match the values stored in ZooKeeper, or null if the\n    *         properties cannot be retrieved.\n    */\n-  private @Nullable VersionedProperties loadIfDifferentVersion(PropStoreKey<?> propCacheId,\n+  private @Nullable VersionedProperties loadIfDifferentVersion(PropStoreKey propCacheId,\n       VersionedProperties currentValue) {\n     requireNonNull(propCacheId, \""propCacheId cannot be null\"");\n     try {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java\nindex 6369eda0afb..fc2ed1a7928 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/store/impl/ZooPropStore.java\n@@ -27,7 +27,7 @@\n import java.util.Objects;\n import java.util.function.BiFunction;\n \n-import org.apache.accumulo.core.data.InstanceId;\n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n@@ -62,14 +62,13 @@ public class ZooPropStore implements PropStore, PropChangeListener {\n    * ReadyMonitor and a PropStore watcher allowing them to be mocked. If the optional components are\n    * passed as null an internal instance is created.\n    *\n-   * @param instanceId the instance id\n    * @param zk a ZooKeeper client\n    * @param monitor a ready monitor. Optional, if null, one is created.\n    * @param watcher a watcher. Optional, if null, one is created.\n    * @param ticker a synthetic clock used for testing. Optional, if null, one is created.\n    */\n-  ZooPropStore(final InstanceId instanceId, final ZooSession zk, final ReadyMonitor monitor,\n-      final PropStoreWatcher watcher, final Ticker ticker) {\n+  ZooPropStore(final ZooSession zk, final ReadyMonitor monitor, final PropStoreWatcher watcher,\n+      final Ticker ticker) {\n \n     this.zrw = zk.asReaderWriter();\n \n@@ -85,33 +84,32 @@ public class ZooPropStore implements PropStore, PropChangeListener {\n     } else {\n       this.cache = new PropCacheCaffeineImpl.Builder(propLoader).forTests(ticker).build();\n     }\n-\n     try {\n-      var path = ZooUtil.getRoot(instanceId);\n-      if (zrw.exists(path, propStoreWatcher)) {\n-        log.debug(\""Have a ZooKeeper connection and found instance node: {}\"", instanceId);\n+      if (zrw.exists(\""/\"", propStoreWatcher)) {\n+        log.debug(\""Have a ZooKeeper connection and found instance node: {}\"");\n         zkReadyMon.setReady();\n       } else {\n-        throw new IllegalStateException(\""Instance may not have been initialized, root node: \"" + path\n-            + \"" does not exist in ZooKeeper\"");\n+        throw new IllegalStateException(\n+            \""Instance may not have been initialized, provided root node path does not exist in ZooKeeper\"");\n       }\n     } catch (InterruptedException ex) {\n       Thread.currentThread().interrupt();\n       throw new IllegalStateException(\n-          \""Interrupted trying to read root node \"" + instanceId + \"" from ZooKeeper\"", ex);\n-    } catch (KeeperException ex) {\n-      throw new IllegalStateException(\""Failed to read root node \"" + instanceId + \"" from ZooKeeper\"",\n+          \""Interrupted trying to read instance root node from ZooKeeper located at \""\n+              + Constants.ZROOT + \""/<uuid>\"",\n           ex);\n+    } catch (KeeperException ex) {\n+      throw new IllegalStateException(\""Failed to read instance root node from ZooKeeper located at \""\n+          + Constants.ZROOT + \""/<uuid>\"", ex);\n     }\n   }\n \n-  public static ZooPropStore initialize(@NonNull final InstanceId instanceId,\n-      @NonNull final ZooSession zk) {\n-    return new ZooPropStore(instanceId, zk, null, null, null);\n+  public static ZooPropStore initialize(@NonNull final ZooSession zk) {\n+    return new ZooPropStore(zk, null, null, null);\n   }\n \n   @Override\n-  public boolean exists(final PropStoreKey<?> propStoreKey) {\n+  public boolean exists(final PropStoreKey propStoreKey) {\n     try {\n       if (zrw.exists(propStoreKey.getPath())) {\n         return true;\n@@ -126,7 +124,7 @@ public boolean exists(final PropStoreKey<?> propStoreKey) {\n   }\n \n   @Override\n-  public void create(PropStoreKey<?> propStoreKey, Map<String,String> props) {\n+  public void create(PropStoreKey propStoreKey, Map<String,String> props) {\n \n     try {\n       VersionedProperties vProps = new VersionedProperties(props);\n@@ -147,7 +145,7 @@ public void create(PropStoreKey<?> propStoreKey, Map<String,String> props) {\n    * @throws IllegalStateException if the updates fails because of an underlying store exception\n    */\n   @Override\n-  public @NonNull VersionedProperties get(final PropStoreKey<?> propStoreKey) {\n+  public @NonNull VersionedProperties get(final PropStoreKey propStoreKey) {\n     checkZkConnection(); // if ZK not connected, block, do not just return a cached value.\n     propStoreWatcher.registerListener(propStoreKey, this);\n \n@@ -173,7 +171,7 @@ public void create(PropStoreKey<?> propStoreKey, Map<String,String> props) {\n    * @throws KeeperException if a ZooKeeper exception occurs\n    * @throws InterruptedException if the ZooKeeper read was interrupted.\n    */\n-  public static @Nullable VersionedProperties readFromZk(final PropStoreKey<?> propStoreKey,\n+  public static @Nullable VersionedProperties readFromZk(final PropStoreKey propStoreKey,\n       final PropStoreWatcher watcher, final ZooReader zooReader)\n       throws IOException, KeeperException, InterruptedException {\n     try {\n@@ -207,7 +205,7 @@ public void create(PropStoreKey<?> propStoreKey, Map<String,String> props) {\n    *         exception occurs.\n    */\n   @Override\n-  public void putAll(@NonNull PropStoreKey<?> propStoreKey, @NonNull Map<String,String> props) {\n+  public void putAll(@NonNull PropStoreKey propStoreKey, @NonNull Map<String,String> props) {\n     if (props.isEmpty()) {\n       return; // no props - noop\n     }\n@@ -215,13 +213,13 @@ public void putAll(@NonNull PropStoreKey<?> propStoreKey, @NonNull Map<String,St\n   }\n \n   @Override\n-  public void replaceAll(@NonNull PropStoreKey<?> propStoreKey, long version,\n+  public void replaceAll(@NonNull PropStoreKey propStoreKey, long version,\n       @NonNull Map<String,String> props) {\n     mutateVersionedProps(propStoreKey, VersionedProperties::replaceAll, version, props);\n   }\n \n   @Override\n-  public void removeProperties(@NonNull PropStoreKey<?> propStoreKey,\n+  public void removeProperties(@NonNull PropStoreKey propStoreKey,\n       @NonNull Collection<String> keys) {\n     if (keys.isEmpty()) {\n       return; // no keys - noop.\n@@ -230,7 +228,7 @@ public void removeProperties(@NonNull PropStoreKey<?> propStoreKey,\n   }\n \n   @Override\n-  public void delete(@NonNull PropStoreKey<?> propStoreKey) {\n+  public void delete(@NonNull PropStoreKey propStoreKey) {\n     Objects.requireNonNull(propStoreKey, \""prop store delete() - Must provide propCacheId\"");\n     try {\n       log.trace(\""called delete() for: {}\"", propStoreKey);\n@@ -243,7 +241,7 @@ public void delete(@NonNull PropStoreKey<?> propStoreKey) {\n     }\n   }\n \n-  private <T> void mutateVersionedProps(PropStoreKey<?> propStoreKey,\n+  private <T> void mutateVersionedProps(PropStoreKey propStoreKey,\n       BiFunction<VersionedProperties,T,VersionedProperties> action, T changes) {\n \n     log.trace(\""mutateVersionedProps called for: {}\"", propStoreKey);\n@@ -281,7 +279,7 @@ private <T> void mutateVersionedProps(PropStoreKey<?> propStoreKey,\n     }\n   }\n \n-  private <T> void mutateVersionedProps(PropStoreKey<?> propStoreKey,\n+  private <T> void mutateVersionedProps(PropStoreKey propStoreKey,\n       BiFunction<VersionedProperties,T,VersionedProperties> action, long existingVersion,\n       T changes) {\n \n@@ -330,7 +328,7 @@ private <T> void mutateVersionedProps(PropStoreKey<?> propStoreKey,\n   }\n \n   @Override\n-  public void registerAsListener(PropStoreKey<?> propStoreKey, PropChangeListener listener) {\n+  public void registerAsListener(PropStoreKey propStoreKey, PropChangeListener listener) {\n     propStoreWatcher.registerListener(propStoreKey, listener);\n   }\n \n@@ -347,7 +345,7 @@ private void checkZkConnection() {\n   }\n \n   @Override\n-  public void zkChangeEvent(PropStoreKey<?> propStoreKey) {\n+  public void zkChangeEvent(PropStoreKey propStoreKey) {\n     log.trace(\""Received change event from ZooKeeper for: {} removed from cache\"", propStoreKey);\n     cache.remove(propStoreKey);\n   }\n@@ -360,12 +358,12 @@ public void zkChangeEvent(PropStoreKey<?> propStoreKey) {\n    * @param propStoreKey the prop cache id.\n    */\n   @Override\n-  public void cacheChangeEvent(PropStoreKey<?> propStoreKey) {\n+  public void cacheChangeEvent(PropStoreKey propStoreKey) {\n     log.trace(\""zkChangeEvent: {}\"", propStoreKey);\n   }\n \n   @Override\n-  public void deleteEvent(PropStoreKey<?> propStoreKey) {\n+  public void deleteEvent(PropStoreKey propStoreKey) {\n     log.trace(\""deleteEvent: {}\"", propStoreKey);\n     cache.remove(propStoreKey);\n   }\n@@ -387,7 +385,7 @@ public void connectionEvent() {\n    * @throws IllegalStateException if an interrupt occurs. The interrupt status is reasserted and\n    *         usually best to not otherwise try to handle the exception.\n    */\n-  private VersionedProperties readPropsFromZk(PropStoreKey<?> propStoreKey)\n+  private VersionedProperties readPropsFromZk(PropStoreKey propStoreKey)\n       throws KeeperException, IOException {\n     try {\n       Stat stat = new Stat();\n@@ -408,12 +406,12 @@ public PropCache getCache() {\n   }\n \n   @Override\n-  public @Nullable VersionedProperties getIfCached(PropStoreKey<?> propStoreKey) {\n+  public @Nullable VersionedProperties getIfCached(PropStoreKey propStoreKey) {\n     return cache.getIfCached(propStoreKey);\n   }\n \n   @Override\n-  public boolean validateDataVersion(PropStoreKey<?> storeKey, long expectedVersion) {\n+  public boolean validateDataVersion(PropStoreKey storeKey, long expectedVersion) {\n     try {\n       Stat stat = zrw.getStatus(storeKey.getPath());\n       log.trace(\""data version sync: stat returned: {} for {}\"", stat, storeKey);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/util/PropSnapshot.java b/server/base/src/main/java/org/apache/accumulo/server/conf/util/PropSnapshot.java\nindex 3f47cbbcbdf..03581114934 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/util/PropSnapshot.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/util/PropSnapshot.java\n@@ -46,16 +46,16 @@ public class PropSnapshot implements PropChangeListener {\n   private final Lock updateLock = new ReentrantLock();\n   private final AtomicBoolean needsUpdate = new AtomicBoolean(true);\n   private final AtomicReference<VersionedProperties> vPropRef = new AtomicReference<>();\n-  private final PropStoreKey<?> propStoreKey;\n+  private final PropStoreKey propStoreKey;\n   private final PropStore propStore;\n \n-  public static PropSnapshot create(final PropStoreKey<?> propStoreKey, final PropStore propStore) {\n+  public static PropSnapshot create(final PropStoreKey propStoreKey, final PropStore propStore) {\n     var ps = new PropSnapshot(propStoreKey, propStore);\n     propStore.registerAsListener(propStoreKey, ps);\n     return ps;\n   }\n \n-  private PropSnapshot(final PropStoreKey<?> propStoreKey, final PropStore propStore) {\n+  private PropSnapshot(final PropStoreKey propStoreKey, final PropStore propStore) {\n     this.propStoreKey = propStoreKey;\n     this.propStore = propStore;\n   }\n@@ -110,7 +110,7 @@ private void updateSnapshot() {\n   }\n \n   @Override\n-  public void zkChangeEvent(final PropStoreKey<?> eventPropKey) {\n+  public void zkChangeEvent(final PropStoreKey eventPropKey) {\n     if (propStoreKey.equals(eventPropKey)) {\n       requireUpdate();\n       log.debug(\""Saw zk change event for {} - update properties required\"", propStoreKey);\n@@ -118,7 +118,7 @@ public void zkChangeEvent(final PropStoreKey<?> eventPropKey) {\n   }\n \n   @Override\n-  public void cacheChangeEvent(final PropStoreKey<?> eventPropKey) {\n+  public void cacheChangeEvent(final PropStoreKey eventPropKey) {\n     if (propStoreKey.equals(eventPropKey)) {\n       requireUpdate();\n       log.debug(\""Saw cache change event for {} - update properties required\"", propStoreKey);\n@@ -126,7 +126,7 @@ public void cacheChangeEvent(final PropStoreKey<?> eventPropKey) {\n   }\n \n   @Override\n-  public void deleteEvent(final PropStoreKey<?> eventPropKey) {\n+  public void deleteEvent(final PropStoreKey eventPropKey) {\n     if (propStoreKey.equals(eventPropKey)) {\n       requireUpdate();\n       log.debug(\""Received property delete event for {} - update properties required\"", propStoreKey);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java b/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java\nindex bd8bd9e0d5f..dcd79f63564 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooInfoViewer.java\n@@ -329,7 +329,7 @@ private Map<String,VersionedProperties> fetchNamespaceProps(InstanceId iid, ZooR\n \n     filteredIds.forEach((nid, name) -> {\n       try {\n-        var key = NamespacePropKey.of(iid, nid);\n+        var key = NamespacePropKey.of(nid);\n         log.trace(\""fetch props from path: {}\"", key.getPath());\n         var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);\n         results.put(name, props);\n@@ -346,7 +346,6 @@ private Map<String,VersionedProperties> fetchNamespaceProps(InstanceId iid, ZooR\n \n   private Map<String,VersionedProperties> fetchTableProps(final ServerContext context,\n       final List<String> tables) {\n-    var iid = context.getInstanceID();\n     var zooReader = context.getZooSession().asReader();\n \n     Set<String> cmdOptTables = new TreeSet<>(tables);\n@@ -367,7 +366,7 @@ private Map<String,VersionedProperties> fetchTableProps(final ServerContext cont\n \n     filteredIds.forEach((tid, name) -> {\n       try {\n-        var key = TablePropKey.of(iid, tid);\n+        var key = TablePropKey.of(tid);\n         log.trace(\""fetch props from path: {}\"", key.getPath());\n         var props = ZooPropStore.readFromZk(key, nullWatcher, zooReader);\n         results.put(name, props);\n@@ -405,7 +404,7 @@ private void printSortedProps(final PrintWriter writer,\n \n   private VersionedProperties fetchSystemProp(final InstanceId iid, final ZooReader zooReader)\n       throws Exception {\n-    SystemPropKey propKey = SystemPropKey.of(iid);\n+    SystemPropKey propKey = SystemPropKey.of();\n     return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropEditor.java b/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropEditor.java\nindex e96a90370b2..1a7ad47c7b9 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropEditor.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/conf/util/ZooPropEditor.java\n@@ -35,9 +35,9 @@\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n-import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.codec.VersionedProperties;\n+import org.apache.accumulo.server.conf.store.IdBasedPropStoreKey;\n import org.apache.accumulo.server.conf.store.NamespacePropKey;\n import org.apache.accumulo.server.conf.store.PropStoreKey;\n import org.apache.accumulo.server.conf.store.SystemPropKey;\n@@ -84,11 +84,10 @@ public void execute(String[] args) throws Exception {\n     opts.parseArgs(ZooPropEditor.class.getName(), args);\n \n     var siteConfig = opts.getSiteConfiguration();\n-    try (var zk = new ZooSession(getClass().getSimpleName(), siteConfig);\n-        var context = new ServerContext(siteConfig)) {\n-      var zrw = zk.asReaderWriter();\n+    try (var context = new ServerContext(siteConfig)) {\n+      var zrw = context.getZooSession().asReaderWriter();\n \n-      PropStoreKey<?> propKey = getPropKey(context, opts);\n+      var propKey = getPropKey(context, opts);\n       switch (opts.getCmdMode()) {\n         case SET:\n           setProperty(context, propKey, opts);\n@@ -106,7 +105,7 @@ public void execute(String[] args) throws Exception {\n     }\n   }\n \n-  private void setProperty(final ServerContext context, final PropStoreKey<?> propKey,\n+  private void setProperty(final ServerContext context, final PropStoreKey propKey,\n       final Opts opts) {\n     LOG.trace(\""set {}\"", propKey);\n \n@@ -133,11 +132,11 @@ private void setProperty(final ServerContext context, final PropStoreKey<?> prop\n       }\n     } catch (Exception ex) {\n       throw new IllegalStateException(\n-          \""Failed to set property for \"" + targetName + \"" (id: \"" + propKey.getId() + \"")\"", ex);\n+          \""Failed to set property for \"" + targetName + \"" (path: \"" + propKey.getPath() + \"")\"", ex);\n     }\n   }\n \n-  private void deleteProperty(final ServerContext context, final PropStoreKey<?> propKey,\n+  private void deleteProperty(final ServerContext context, final PropStoreKey propKey,\n       VersionedProperties versionedProperties, final Opts opts) {\n     LOG.trace(\""delete {} - {}\"", propKey, opts.deleteOpt);\n     String p = opts.deleteOpt.trim();\n@@ -155,7 +154,7 @@ private void deleteProperty(final ServerContext context, final PropStoreKey<?> p\n     LOG.info(\""{}: deleted {}\"", targetName, p);\n   }\n \n-  private void printProperties(final ServerContext context, final PropStoreKey<?> propKey,\n+  private void printProperties(final ServerContext context, final PropStoreKey propKey,\n       final VersionedProperties props) {\n     LOG.trace(\""print {}\"", propKey);\n \n@@ -180,7 +179,8 @@ private void printProperties(final ServerContext context, final PropStoreKey<?>\n       writer.printf(\"": Property scope: %s\\n\"", scope);\n       writer.printf(\"": ZooKeeper path: %s\\n\"", propKey.getPath());\n       writer.printf(\"": Name: %s\\n\"", getDisplayName(propKey, context));\n-      writer.printf(\"": Id: %s\\n\"", propKey.getId());\n+      writer.printf(\"": Id: %s\\n\"", propKey instanceof IdBasedPropStoreKey\n+          ? ((IdBasedPropStoreKey<?>) propKey).getId() : \""N/A\"");\n       writer.printf(\"": Data version: %d\\n\"", props.getDataVersion());\n       writer.printf(\"": Timestamp: %s\\n\"", props.getTimestampISO());\n \n@@ -194,8 +194,7 @@ private void printProperties(final ServerContext context, final PropStoreKey<?>\n     }\n   }\n \n-  private VersionedProperties readPropNode(final PropStoreKey<?> propKey,\n-      final ZooReader zooReader) {\n+  private VersionedProperties readPropNode(final PropStoreKey propKey, final ZooReader zooReader) {\n     try {\n       return ZooPropStore.readFromZk(propKey, nullWatcher, zooReader);\n     } catch (IOException | KeeperException | InterruptedException ex) {\n@@ -203,23 +202,22 @@ private VersionedProperties readPropNode(final PropStoreKey<?> propKey,\n     }\n   }\n \n-  private PropStoreKey<?> getPropKey(final ServerContext context, final ZooPropEditor.Opts opts) {\n-    var iid = context.getInstanceID();\n+  private PropStoreKey getPropKey(final ServerContext context, final ZooPropEditor.Opts opts) {\n \n     // either tid or table name option provided, get the table id\n     if (!opts.tableOpt.isEmpty() || !opts.tableIdOpt.isEmpty()) {\n       TableId tid = getTableId(context, opts);\n-      return TablePropKey.of(iid, tid);\n+      return TablePropKey.of(tid);\n     }\n \n     // either nid of namespace name provided, get the namespace id.\n     if (!opts.namespaceOpt.isEmpty() || !opts.namespaceIdOpt.isEmpty()) {\n       NamespaceId nid = getNamespaceId(context, opts);\n-      return NamespacePropKey.of(iid, nid);\n+      return NamespacePropKey.of(nid);\n     }\n \n     // no table or namespace, assume system.\n-    return SystemPropKey.of(iid);\n+    return SystemPropKey.of();\n   }\n \n   private TableId getTableId(final ServerContext context, final ZooPropEditor.Opts opts) {\n@@ -242,13 +240,15 @@ private NamespaceId getNamespaceId(final ServerContext context, final ZooPropEdi\n             () -> new IllegalArgumentException(\""Could not find namespace \"" + opts.namespaceOpt));\n   }\n \n-  private String getDisplayName(final PropStoreKey<?> propStoreKey, final ServerContext context) {\n+  private String getDisplayName(final PropStoreKey propStoreKey, final ServerContext context) {\n \n     if (propStoreKey instanceof TablePropKey) {\n-      return context.getTableIdToNameMap().getOrDefault(propStoreKey.getId(), \""unknown\"");\n+      return context.getTableIdToNameMap().getOrDefault(((TablePropKey) propStoreKey).getId(),\n+          \""unknown\"");\n     }\n     if (propStoreKey instanceof NamespacePropKey) {\n-      return context.getNamespaceIdToNameMap().getOrDefault(propStoreKey.getId(), \""unknown\"");\n+      return context.getNamespaceIdToNameMap()\n+          .getOrDefault(((NamespacePropKey) propStoreKey).getId(), \""unknown\"");\n     }\n     if (propStoreKey instanceof SystemPropKey) {\n       return \""system\"";\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\nindex 0d2988423c4..df14723134a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n@@ -400,8 +400,7 @@ private void validateServerFamily(ArrayList<Short> violations, ColumnUpdate colu\n         String lockId = new String(columnUpdate.getValue(), UTF_8);\n \n         try {\n-          lockHeld = ServiceLock.isLockHeld(context.getZooCache(),\n-              new ZooUtil.LockID(context.getZooKeeperRoot(), lockId));\n+          lockHeld = ServiceLock.isLockHeld(context.getZooCache(), new ZooUtil.LockID(\""\"", lockId));\n         } catch (Exception e) {\n           log.debug(\""Failed to verify lock was held {} {}\"", lockId, e.getMessage());\n         }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\nindex e1801b6eca9..27721e90c59 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n@@ -211,7 +211,7 @@ private void initSystemTablesConfig(final ServerContext context)\n   private void setTableProperties(final ServerContext context, TableId tableId,\n       HashMap<String,String> props) {\n     var propStore = context.getPropStore();\n-    TablePropKey tablePropKey = TablePropKey.of(context, tableId);\n+    TablePropKey tablePropKey = TablePropKey.of(tableId);\n     if (propStore.exists(tablePropKey)) {\n       propStore.putAll(tablePropKey, props);\n     } else {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java b/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\nindex 0b1a5045df3..b3df97de5a8 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/Initialize.java\n@@ -129,7 +129,7 @@ private static void printInitializeFailureMessages(InitialConfiguration initConf\n   }\n \n   private boolean doInit(ZooReaderWriter zoo, Opts opts, VolumeManager fs,\n-      InitialConfiguration initConfig) {\n+      InitialConfiguration initConfig) throws InterruptedException, KeeperException {\n     String instanceNamePath;\n     String instanceName;\n     String rootUser;\n@@ -159,6 +159,7 @@ private boolean doInit(ZooReaderWriter zoo, Opts opts, VolumeManager fs,\n     InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n     ZooKeeperInitializer zki = new ZooKeeperInitializer();\n     zki.initializeConfig(instanceId, zoo);\n+    zki.initInstanceNameAndId(zoo, instanceId, opts.clearInstanceName, instanceNamePath);\n \n     try (ServerContext context =\n         ServerContext.initialize(initConfig.getSiteConf(), instanceName, instanceId)) {\n@@ -169,8 +170,7 @@ private boolean doInit(ZooReaderWriter zoo, Opts opts, VolumeManager fs,\n       String rootTabletFileUri = new Path(fs.choose(chooserEnv, initConfig.getVolumeUris())\n           + SEPARATOR + TABLE_DIR + SEPARATOR + AccumuloTable.ROOT.tableId() + SEPARATOR\n           + rootTabletDirName + SEPARATOR + \""00000_00000.\"" + ext).toString();\n-      zki.initialize(context, opts.clearInstanceName, instanceNamePath, rootTabletDirName,\n-          rootTabletFileUri);\n+      zki.initialize(context, rootTabletDirName, rootTabletFileUri);\n \n       if (!createDirs(fs, instanceId, initConfig.getVolumeUris())) {\n         throw new IOException(\""Problem creating directories on \"" + fs.getVolumes());\n@@ -546,6 +546,12 @@ public void execute(final String[] args) {\n     } catch (IOException e) {\n       log.error(\""Problem trying to get Volume configuration\"", e);\n       success = false;\n+    } catch (InterruptedException e) {\n+      log.error(\""Thread was interrupted when trying to get Volume configuration\"", e);\n+      success = false;\n+    } catch (KeeperException e) {\n+      log.error(\""ZooKeeper error when trying to get Volume configuration\"", e);\n+      success = false;\n     } finally {\n       SingletonManager.setMode(Mode.CLOSED);\n       if (!success) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java b/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\nindex 3089189f94e..0ef60f0f495 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\n@@ -75,13 +75,13 @@ void initializeConfig(final InstanceId instanceId, final ZooReaderWriter zoo) {\n \n       String zkInstanceRoot = ZooUtil.getRoot(instanceId);\n       zoo.putPersistentData(zkInstanceRoot, EMPTY_BYTE_ARRAY, ZooUtil.NodeExistsPolicy.SKIP);\n-      var sysPropPath = SystemPropKey.of(instanceId).getPath();\n+      var sysPropPath = SystemPropKey.of().getPath();\n       VersionedProperties vProps = new VersionedProperties();\n       // skip if the encoded props node exists\n       if (zoo.exists(sysPropPath)) {\n         return;\n       }\n-      var created = zoo.putPrivatePersistentData(sysPropPath,\n+      var created = zoo.putPrivatePersistentData(zkInstanceRoot + sysPropPath,\n           VersionedPropCodec.getDefault().toBytes(vProps), ZooUtil.NodeExistsPolicy.FAIL);\n       if (!created) {\n         throw new IllegalStateException(\n@@ -92,29 +92,12 @@ void initializeConfig(final InstanceId instanceId, final ZooReaderWriter zoo) {\n     }\n   }\n \n-  void initialize(final ServerContext context, final boolean clearInstanceName,\n-      final String instanceNamePath, final String rootTabletDirName, final String rootTabletFileUri)\n-      throws KeeperException, InterruptedException {\n-    // setup basic data in zookeeper\n-\n-    ZooReaderWriter zoo = context.getZooSession().asReaderWriter();\n-    InstanceId instanceId = context.getInstanceID();\n-\n-    zoo.putPersistentData(Constants.ZROOT + Constants.ZINSTANCES, new byte[0],\n-        ZooUtil.NodeExistsPolicy.SKIP, ZooDefs.Ids.OPEN_ACL_UNSAFE);\n-\n-    // setup instance name\n-    if (clearInstanceName) {\n-      zoo.recursiveDelete(instanceNamePath, ZooUtil.NodeMissingPolicy.SKIP);\n-    }\n-    zoo.putPersistentData(instanceNamePath, instanceId.canonical().getBytes(UTF_8),\n+  void initialize(final ServerContext context, final String rootTabletDirName,\n+      final String rootTabletFileUri) throws KeeperException, InterruptedException {\n+    ZooReaderWriter zrwChroot = context.getZooSession().asReaderWriter();\n+    zrwChroot.putPersistentData(Constants.ZTABLES, Constants.ZTABLES_INITIAL_ID,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-\n-    // setup the instance\n-    String zkInstanceRoot = context.getZooKeeperRoot();\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZTABLES, Constants.ZTABLES_INITIAL_ID,\n-        ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZNAMESPACES,\n+    zrwChroot.putPersistentData(Constants.ZNAMESPACES,\n         NamespaceMapping\n             .serialize(Map.of(Namespace.DEFAULT.id().canonical(), Namespace.DEFAULT.name(),\n                 Namespace.ACCUMULO.id().canonical(), Namespace.ACCUMULO.name())),\n@@ -135,42 +118,41 @@ void initialize(final ServerContext context, final boolean clearInstanceName,\n     initScanRefTableState(context);\n     initFateTableState(context);\n \n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZTSERVERS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZTSERVERS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + RootTable.ZROOT_TABLET,\n+    zrwChroot.putPersistentData(RootTable.ZROOT_TABLET,\n         getInitialRootTabletJson(rootTabletDirName, rootTabletFileUri),\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + RootTable.ZROOT_TABLET_GC_CANDIDATES,\n+    zrwChroot.putPersistentData(RootTable.ZROOT_TABLET_GC_CANDIDATES,\n         new RootGcCandidates().toJson().getBytes(UTF_8), ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZMANAGERS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZMANAGERS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZMANAGER_LOCK, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZMANAGER_LOCK, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZMANAGER_GOAL_STATE,\n+    zrwChroot.putPersistentData(Constants.ZMANAGER_GOAL_STATE,\n         ManagerGoalState.NORMAL.toString().getBytes(UTF_8), ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZGC, EMPTY_BYTE_ARRAY,\n-        ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZGC_LOCK, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZGC, EMPTY_BYTE_ARRAY, ZooUtil.NodeExistsPolicy.FAIL);\n+    zrwChroot.putPersistentData(Constants.ZGC_LOCK, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZTABLE_LOCKS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZTABLE_LOCKS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZHDFS_RESERVATIONS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZHDFS_RESERVATIONS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZNEXT_FILE, ZERO_CHAR_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZNEXT_FILE, ZERO_CHAR_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZRECOVERY, ZERO_CHAR_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZRECOVERY, ZERO_CHAR_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZMONITOR, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZMONITOR, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZMONITOR_LOCK, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZMONITOR_LOCK, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + WalStateManager.ZWALS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(WalStateManager.ZWALS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZCOMPACTORS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZCOMPACTORS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZSSERVERS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZSSERVERS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZCOMPACTIONS, EMPTY_BYTE_ARRAY,\n+    zrwChroot.putPersistentData(Constants.ZCOMPACTIONS, EMPTY_BYTE_ARRAY,\n         ZooUtil.NodeExistsPolicy.FAIL);\n   }\n \n@@ -219,4 +201,20 @@ public void initFateTableState(ServerContext context) {\n     }\n   }\n \n+  public void initInstanceNameAndId(ZooReaderWriter zoo, InstanceId instanceId,\n+      final boolean clearInstanceName, final String instanceNamePath)\n+      throws InterruptedException, KeeperException {\n+\n+    // setup basic data in zookeeper\n+    zoo.putPersistentData(Constants.ZROOT + Constants.ZINSTANCES, new byte[0],\n+        ZooUtil.NodeExistsPolicy.SKIP, ZooDefs.Ids.OPEN_ACL_UNSAFE);\n+\n+    // setup instance name\n+    if (clearInstanceName) {\n+      zoo.recursiveDelete(instanceNamePath, ZooUtil.NodeMissingPolicy.SKIP);\n+    }\n+    zoo.putPersistentData(instanceNamePath, instanceId.canonical().getBytes(UTF_8),\n+        ZooUtil.NodeExistsPolicy.FAIL);\n+  }\n+\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/log/WalStateManager.java b/server/base/src/main/java/org/apache/accumulo/server/log/WalStateManager.java\nindex da4f51dd356..21cfef697aa 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/log/WalStateManager.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/log/WalStateManager.java\n@@ -27,7 +27,6 @@\n import java.util.Map.Entry;\n import java.util.UUID;\n \n-import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeExistsPolicy;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeMissingPolicy;\n@@ -86,22 +85,19 @@ public enum WalState {\n     UNREFERENCED\n   }\n \n-  private final ClientContext context;\n   private final ZooReaderWriter zoo;\n \n   private volatile boolean checkedExistance = false;\n \n   public WalStateManager(ServerContext context) {\n-    this.context = context;\n     this.zoo = context.getZooSession().asReaderWriter();\n   }\n \n   private String root() throws WalMarkerException {\n-    String root = context.getZooKeeperRoot() + ZWALS;\n \n     try {\n-      if (!checkedExistance && !zoo.exists(root)) {\n-        zoo.putPersistentData(root, new byte[0], NodeExistsPolicy.SKIP);\n+      if (!checkedExistance && !zoo.exists(ZWALS)) {\n+        zoo.putPersistentData(ZWALS, new byte[0], NodeExistsPolicy.SKIP);\n       }\n \n       checkedExistance = true;\n@@ -109,7 +105,7 @@ private String root() throws WalMarkerException {\n       throw new WalMarkerException(e);\n     }\n \n-    return root;\n+    return ZWALS;\n   }\n \n   // Tablet server exists\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java b/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\nindex 82689c0ee28..923f7567288 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/LiveTServerSet.java\n@@ -329,16 +329,15 @@ public void accept(WatchedEvent event) {\n     // its important that these event are propagated by ZooCache, because this ensures when reading\n     // zoocache that is has already processed the event and cleared\n     // relevant nodes before code below reads from zoocache\n-    final String tserverZPath = context.getZooKeeperRoot() + Constants.ZTSERVERS;\n-    if (event.getPath() != null && event.getPath().startsWith(tserverZPath)) {\n-      if (event.getPath().equals(tserverZPath)) {\n+    if (event.getPath() != null && event.getPath().startsWith(Constants.ZTSERVERS)) {\n+      if (event.getPath().equals(Constants.ZTSERVERS)) {\n         scanServers();\n       } else if (event.getPath().contains(Constants.ZTSERVERS)) {\n         // It's possible that the path contains more than the tserver address, it\n         // could contain it's children. We need to fix the path before parsing it\n-        // path should be: zooKeeperRoot + Constants.ZTSERVERS + \""/\"" + resourceGroup + \""/\"" address\n+        // path should be: Constants.ZTSERVERS + \""/\"" + resourceGroup + \""/\"" address\n         String pathToUse = null;\n-        String remaining = event.getPath().substring(tserverZPath.length() + 1);\n+        String remaining = event.getPath().substring(Constants.ZTSERVERS.length() + 1);\n         int numSlashes = StringUtils.countMatches(remaining, '/');\n         if (numSlashes == 1) {\n           // event path is the server\n@@ -348,7 +347,7 @@ public void accept(WatchedEvent event) {\n           int idx = remaining.indexOf(\""/\"");\n           String rg = remaining.substring(0, idx);\n           String server = remaining.substring(idx + 1, remaining.indexOf(\""/\"", idx + 1));\n-          pathToUse = tserverZPath + \""/\"" + rg + \""/\"" + server;\n+          pathToUse = Constants.ZTSERVERS + \""/\"" + rg + \""/\"" + server;\n         } else {\n           // malformed path\n           pathToUse = null;\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/manager/state/DeadServerList.java b/server/base/src/main/java/org/apache/accumulo/server/manager/state/DeadServerList.java\nindex d12f4e9363a..eedb2152b65 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/manager/state/DeadServerList.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/manager/state/DeadServerList.java\n@@ -48,12 +48,11 @@ public class DeadServerList {\n   private static final String RESOURCE_GROUP = \""UNKNOWN\"";\n   private final ServerContext ctx;\n   private final ZooReaderWriter zoo;\n-  private final String path;\n+  private static final String path = Constants.ZDEADTSERVERS + \""/\"" + RESOURCE_GROUP;\n \n   public DeadServerList(ServerContext context) {\n     this.ctx = context;\n     this.zoo = context.getZooSession().asReaderWriter();\n-    this.path = context.getZooKeeperRoot() + Constants.ZDEADTSERVERS + \""/\"" + RESOURCE_GROUP;\n     try {\n       zoo.mkdirs(path);\n     } catch (Exception ex) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\nindex 81194358715..24aaedccfe7 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/ConditionalTabletMutatorImpl.java\n@@ -389,7 +389,7 @@ public void submit(Ample.RejectionHandler rejectionCheck) {\n       mutation.addCondition(c);\n     }\n     if (putServerLock) {\n-      this.putZooLock(context.getZooKeeperRoot(), lock);\n+      this.putZooLock(lock);\n     }\n     getMutation();\n     mutationConsumer.accept(mutation);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/RootConditionalWriter.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/RootConditionalWriter.java\nindex 9f61097f8e9..570152e0775 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/RootConditionalWriter.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/RootConditionalWriter.java\n@@ -89,15 +89,13 @@ public Result write(ConditionalMutation mutation) {\n \n     ServerConditionalMutation scm = new ServerConditionalMutation(tcm);\n \n-    String zpath = context.getZooKeeperRoot() + RootTable.ZROOT_TABLET;\n-\n-    context.getZooCache().clear(zpath);\n+    context.getZooCache().clear(RootTable.ZROOT_TABLET);\n \n     List<ServerConditionalMutation> okMutations = new ArrayList<>();\n     List<TCMResult> results = new ArrayList<>();\n \n     try {\n-      context.getZooSession().asReaderWriter().mutateExisting(zpath, currVal -> {\n+      context.getZooSession().asReaderWriter().mutateExisting(RootTable.ZROOT_TABLET, currVal -> {\n         String currJson = new String(currVal, UTF_8);\n \n         var rtm = new RootTabletMetadata(currJson);\n@@ -133,7 +131,7 @@ public Result write(ConditionalMutation mutation) {\n     }\n \n     // TODO this is racy...\n-    context.getZooCache().clear(zpath);\n+    context.getZooCache().clear(RootTable.ZROOT_TABLET);\n \n     return getResult(okMutations, results, mutation);\n   }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/RootTabletMutatorImpl.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/RootTabletMutatorImpl.java\nindex 3f429b3c715..5d65ed33dce 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/RootTabletMutatorImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/RootTabletMutatorImpl.java\n@@ -86,9 +86,6 @@ public ServerContext getServerContext() {\n   @Override\n   public void mutate() {\n \n-    if (putServerLock) {\n-      this.putZooLock(this.context.getZooKeeperRoot(), lock);\n-    }\n     Mutation mutation = getMutation();\n \n     MetadataConstraints metaConstraint = new MetadataConstraints();\n@@ -100,23 +97,23 @@ public void mutate() {\n     }\n \n     try {\n-      String zpath = context.getZooKeeperRoot() + RootTable.ZROOT_TABLET;\n \n-      context.getZooCache().clear(zpath);\n+      context.getZooCache().clear(RootTable.ZROOT_TABLET);\n \n       // TODO examine implementation of getZooReaderWriter().mutate()\n       // TODO for efficiency this should maybe call mutateExisting\n-      context.getZooSession().asReaderWriter().mutateOrCreate(zpath, new byte[0], currVal -> {\n-        String currJson = new String(currVal, UTF_8);\n-        var rtm = new RootTabletMetadata(currJson);\n-        rtm.update(mutation);\n-        String newJson = rtm.toJson();\n-        log.debug(\""mutation: from:[{}] to: [{}]\"", currJson, newJson);\n-        return newJson.getBytes(UTF_8);\n-      });\n+      context.getZooSession().asReaderWriter().mutateOrCreate(RootTable.ZROOT_TABLET, new byte[0],\n+          currVal -> {\n+            String currJson = new String(currVal, UTF_8);\n+            var rtm = new RootTabletMetadata(currJson);\n+            rtm.update(mutation);\n+            String newJson = rtm.toJson();\n+            log.debug(\""mutation: from:[{}] to: [{}]\"", currJson, newJson);\n+            return newJson.getBytes(UTF_8);\n+          });\n \n       // TODO this is racy...\n-      context.getZooCache().clear(zpath);\n+      context.getZooCache().clear(RootTable.ZROOT_TABLET);\n \n       if (closeAfterMutate != null) {\n         closeAfterMutate.close();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/ServerAmpleImpl.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/ServerAmpleImpl.java\nindex 59671a0070c..27ca3da6550 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/ServerAmpleImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/ServerAmpleImpl.java\n@@ -103,24 +103,24 @@ public ConditionalTabletsMutator conditionallyMutateTablets() {\n   }\n \n   private void mutateRootGcCandidates(Consumer<RootGcCandidates> mutator) {\n-    String zpath = context.getZooKeeperRoot() + ZROOT_TABLET_GC_CANDIDATES;\n     try {\n       // TODO calling create seems unnecessary and is possibly racy and inefficient\n-      context.getZooSession().asReaderWriter().mutateOrCreate(zpath, new byte[0], currVal -> {\n-        String currJson = new String(currVal, UTF_8);\n-        RootGcCandidates rgcc = new RootGcCandidates(currJson);\n-        log.debug(\""Root GC candidates before change : {}\"", currJson);\n-        mutator.accept(rgcc);\n-        String newJson = rgcc.toJson();\n-        log.debug(\""Root GC candidates after change  : {}\"", newJson);\n-        if (newJson.length() > 262_144) {\n-          log.warn(\n-              \""Root tablet deletion candidates stored in ZK at {} are getting large ({} bytes), is\""\n-                  + \"" Accumulo GC process running?  Large nodes may cause problems for Zookeeper!\"",\n-              zpath, newJson.length());\n-        }\n-        return newJson.getBytes(UTF_8);\n-      });\n+      context.getZooSession().asReaderWriter().mutateOrCreate(ZROOT_TABLET_GC_CANDIDATES,\n+          new byte[0], currVal -> {\n+            String currJson = new String(currVal, UTF_8);\n+            RootGcCandidates rgcc = new RootGcCandidates(currJson);\n+            log.debug(\""Root GC candidates before change : {}\"", currJson);\n+            mutator.accept(rgcc);\n+            String newJson = rgcc.toJson();\n+            log.debug(\""Root GC candidates after change  : {}\"", newJson);\n+            if (newJson.length() > 262_144) {\n+              log.warn(\n+                  \""Root tablet deletion candidates stored in ZK at {} are getting large ({} bytes), is\""\n+                      + \"" Accumulo GC process running?  Large nodes may cause problems for Zookeeper!\"",\n+                  ZROOT_TABLET_GC_CANDIDATES, newJson.length());\n+            }\n+            return newJson.getBytes(UTF_8);\n+          });\n     } catch (Exception e) {\n       throw new IllegalStateException(e);\n     }\n@@ -234,8 +234,7 @@ public Iterator<GcCandidate> getGcCandidates(DataLevel level) {\n       var zooReader = context.getZooSession().asReader();\n       byte[] jsonBytes;\n       try {\n-        jsonBytes =\n-            zooReader.getData(context.getZooKeeperRoot() + RootTable.ZROOT_TABLET_GC_CANDIDATES);\n+        jsonBytes = zooReader.getData(RootTable.ZROOT_TABLET_GC_CANDIDATES);\n       } catch (KeeperException | InterruptedException e) {\n         throw new IllegalStateException(e);\n       }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorImpl.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorImpl.java\nindex af360a1df6d..431d6816584 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorImpl.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/TabletMutatorImpl.java\n@@ -46,7 +46,7 @@ class TabletMutatorImpl extends TabletMutatorBase<Ample.TabletMutator>\n   public void mutate() {\n     try {\n       if (putServerLock) {\n-        this.putZooLock(this.context.getZooKeeperRoot(), lock);\n+        this.putZooLock(lock);\n       }\n       writer.addMutation(getMutation());\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java b/server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java\nindex da12470b87b..c8529953ab7 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/SecurityOperation.java\n@@ -26,6 +26,7 @@\n import java.util.Set;\n import java.util.function.Supplier;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.NamespaceNotFoundException;\n import org.apache.accumulo.core.client.TableNotFoundException;\n@@ -73,7 +74,6 @@ public class SecurityOperation {\n   private final PermissionHandler permHandle;\n   private final boolean isKerberos;\n   private final Supplier<String> rootUserName;\n-  private final String zkUserPath;\n \n   protected final ServerContext context;\n \n@@ -102,9 +102,8 @@ public static PermissionHandler getPermHandler(ServerContext context) {\n   protected SecurityOperation(ServerContext context, Authorizor author, Authenticator authent,\n       PermissionHandler pm) {\n     this.context = context;\n-    zkUserPath = context.zkUserPath();\n     rootUserName =\n-        Suppliers.memoize(() -> new String(context.getZooCache().get(zkUserPath), UTF_8));\n+        Suppliers.memoize(() -> new String(context.getZooCache().get(Constants.ZUSERS), UTF_8));\n     authorizor = author;\n     authenticator = authent;\n     permHandle = pm;\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/handler/KerberosAuthenticator.java b/server/base/src/main/java/org/apache/accumulo/server/security/handler/KerberosAuthenticator.java\nindex 71ac8e94c40..67102c55228 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/handler/KerberosAuthenticator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/handler/KerberosAuthenticator.java\n@@ -24,6 +24,7 @@\n import java.util.HashSet;\n import java.util.Set;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;\n import org.apache.accumulo.core.client.security.tokens.KerberosToken;\n@@ -51,7 +52,6 @@ public class KerberosAuthenticator implements Authenticator {\n \n   private final ZKAuthenticator zkAuthenticator = new ZKAuthenticator();\n   private ServerContext context;\n-  private String zkUserPath;\n   private UserImpersonation impersonation;\n \n   @Override\n@@ -59,7 +59,6 @@ public void initialize(ServerContext context) {\n     this.context = context;\n     impersonation = new UserImpersonation(context.getConfiguration());\n     zkAuthenticator.initialize(context);\n-    zkUserPath = context.zkUserPath();\n   }\n \n   @Override\n@@ -68,9 +67,10 @@ public boolean validSecurityHandlers() {\n   }\n \n   private void createUserNodeInZk(String principal) throws KeeperException, InterruptedException {\n-    context.getZooCache().clear(zkUserPath + \""/\"" + principal);\n+    context.getZooCache().clear(Constants.ZUSERS + \""/\"" + principal);\n     ZooReaderWriter zoo = context.getZooSession().asReaderWriter();\n-    zoo.putPrivatePersistentData(zkUserPath + \""/\"" + principal, new byte[0], NodeExistsPolicy.FAIL);\n+    zoo.putPrivatePersistentData(Constants.ZUSERS + \""/\"" + principal, new byte[0],\n+        NodeExistsPolicy.FAIL);\n   }\n \n   @Override\n@@ -78,16 +78,16 @@ public void initializeSecurity(String principal, byte[] token) {\n     try {\n       // remove old settings from zookeeper first, if any\n       ZooReaderWriter zoo = context.getZooSession().asReaderWriter();\n-      context.getZooCache().clear((path) -> path.startsWith(zkUserPath));\n-      if (zoo.exists(zkUserPath)) {\n-        zoo.recursiveDelete(zkUserPath, NodeMissingPolicy.SKIP);\n-        log.info(\""Removed {}/ from zookeeper\"", zkUserPath);\n+      context.getZooCache().clear((path) -> path.startsWith(Constants.ZUSERS));\n+      if (zoo.exists(Constants.ZUSERS)) {\n+        zoo.recursiveDelete(Constants.ZUSERS, NodeMissingPolicy.SKIP);\n+        log.info(\""Removed {}/ from zookeeper\"", Constants.ZUSERS);\n       }\n \n       // prep parent node of users with root username\n       // ACCUMULO-4140 The root user needs to be stored un-base64 encoded in the znode's value\n       byte[] principalData = principal.getBytes(UTF_8);\n-      zoo.putPersistentData(zkUserPath, principalData, NodeExistsPolicy.FAIL);\n+      zoo.putPersistentData(Constants.ZUSERS, principalData, NodeExistsPolicy.FAIL);\n \n       // Create the root user in ZK using base64 encoded name (since the name is included in the\n       // znode)\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java\nindex 7cf087ac478..9191d7f690f 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthenticator.java\n@@ -24,6 +24,7 @@\n import java.util.Set;\n import java.util.TreeSet;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.security.tokens.AuthenticationToken;\n@@ -42,12 +43,10 @@ public final class ZKAuthenticator implements Authenticator {\n   private static final Logger log = LoggerFactory.getLogger(ZKAuthenticator.class);\n \n   private ServerContext context;\n-  private String zkUserPath;\n \n   @Override\n   public void initialize(ServerContext context) {\n     this.context = context;\n-    zkUserPath = context.zkUserPath();\n   }\n \n   @Override\n@@ -55,14 +54,14 @@ public void initializeSecurity(String principal, byte[] token) {\n     try {\n       // remove old settings from zookeeper first, if any\n       ZooReaderWriter zoo = context.getZooSession().asReaderWriter();\n-      context.getZooCache().clear((path) -> path.startsWith(zkUserPath));\n-      if (zoo.exists(zkUserPath)) {\n-        zoo.recursiveDelete(zkUserPath, NodeMissingPolicy.SKIP);\n-        log.info(\""Removed {}/ from zookeeper\"", zkUserPath);\n+      context.getZooCache().clear((path) -> path.startsWith(Constants.ZUSERS));\n+      if (zoo.exists(Constants.ZUSERS)) {\n+        zoo.recursiveDelete(Constants.ZUSERS, NodeMissingPolicy.SKIP);\n+        log.info(\""Removed {}/ from zookeeper\"", Constants.ZUSERS);\n       }\n \n       // prep parent node of users with root username\n-      zoo.putPersistentData(zkUserPath, principal.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n+      zoo.putPersistentData(Constants.ZUSERS, principal.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n \n       constructUser(principal, ZKSecurityTool.createPass(token));\n     } catch (KeeperException | AccumuloException | InterruptedException e) {\n@@ -77,7 +76,7 @@ public void initializeSecurity(String principal, byte[] token) {\n    */\n   private void constructUser(String user, byte[] pass)\n       throws KeeperException, InterruptedException {\n-    String userPath = zkUserPath + \""/\"" + user;\n+    String userPath = Constants.ZUSERS + \""/\"" + user;\n     context.getZooCache().clear((path) -> path.startsWith(userPath));\n     context.getZooSession().asReaderWriter().putPrivatePersistentData(userPath, pass,\n         NodeExistsPolicy.FAIL);\n@@ -85,7 +84,7 @@ private void constructUser(String user, byte[] pass)\n \n   @Override\n   public Set<String> listUsers() {\n-    return new TreeSet<>(context.getZooCache().getChildren(zkUserPath));\n+    return new TreeSet<>(context.getZooCache().getChildren(Constants.ZUSERS));\n   }\n \n   @Override\n@@ -114,7 +113,7 @@ public void createUser(String principal, AuthenticationToken token)\n   @Override\n   public void dropUser(String user) throws AccumuloSecurityException {\n     try {\n-      String userPath = zkUserPath + \""/\"" + user;\n+      String userPath = Constants.ZUSERS + \""/\"" + user;\n       context.getZooCache().clear((path) -> path.startsWith(userPath));\n       context.getZooSession().asReaderWriter().recursiveDelete(userPath, NodeMissingPolicy.FAIL);\n     } catch (InterruptedException e) {\n@@ -138,7 +137,7 @@ public void changePassword(String principal, AuthenticationToken token)\n     PasswordToken pt = (PasswordToken) token;\n     if (userExists(principal)) {\n       try {\n-        String userPath = zkUserPath + \""/\"" + principal;\n+        String userPath = Constants.ZUSERS + \""/\"" + principal;\n         context.getZooCache().clear(userPath);\n         context.getZooSession().asReaderWriter().putPrivatePersistentData(userPath,\n             ZKSecurityTool.createPass(pt.getPassword()), NodeExistsPolicy.OVERWRITE);\n@@ -160,7 +159,7 @@ public void changePassword(String principal, AuthenticationToken token)\n \n   @Override\n   public boolean userExists(String user) {\n-    return context.getZooCache().get(zkUserPath + \""/\"" + user) != null;\n+    return context.getZooCache().get(Constants.ZUSERS + \""/\"" + user) != null;\n   }\n \n   @Override\n@@ -176,7 +175,7 @@ public boolean authenticateUser(String principal, AuthenticationToken token)\n     }\n     PasswordToken pt = (PasswordToken) token;\n     byte[] zkData;\n-    String zpath = zkUserPath + \""/\"" + principal;\n+    String zpath = Constants.ZUSERS + \""/\"" + principal;\n     zkData = context.getZooCache().get(zpath);\n     boolean result = authenticateUser(principal, pt, zkData);\n     if (!result) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java\nindex a63eee6e827..e844761b230 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKAuthorizor.java\n@@ -23,6 +23,7 @@\n import java.nio.ByteBuffer;\n import java.util.List;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.clientImpl.thrift.SecurityErrorCode;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n@@ -39,20 +40,18 @@\n public class ZKAuthorizor implements Authorizor {\n   private static final Logger log = LoggerFactory.getLogger(ZKAuthorizor.class);\n \n-  private final String ZKUserAuths = \""/Authorizations\"";\n+  private static final String ZKUserAuths = \""/Authorizations\"";\n \n   private ServerContext context;\n-  private String zkUserPath;\n \n   @Override\n   public void initialize(ServerContext context) {\n     this.context = context;\n-    zkUserPath = context.zkUserPath();\n   }\n \n   @Override\n   public Authorizations getCachedUserAuthorizations(String user) {\n-    byte[] authsBytes = context.getZooCache().get(zkUserPath + \""/\"" + user + ZKUserAuths);\n+    byte[] authsBytes = context.getZooCache().get(Constants.ZUSERS + \""/\"" + user + ZKUserAuths);\n     if (authsBytes != null) {\n       return ZKSecurityTool.convertAuthorizations(authsBytes);\n     }\n@@ -72,12 +71,12 @@ public void initializeSecurity(TCredentials itw, String rootuser)\n     // create the root user with no record-level authorizations\n     try {\n       // prep parent node of users with root username\n-      if (!zoo.exists(zkUserPath)) {\n-        zoo.putPersistentData(zkUserPath, rootuser.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n+      if (!zoo.exists(Constants.ZUSERS)) {\n+        zoo.putPersistentData(Constants.ZUSERS, rootuser.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n       }\n \n       initUser(rootuser);\n-      zoo.putPersistentData(zkUserPath + \""/\"" + rootuser + ZKUserAuths,\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + rootuser + ZKUserAuths,\n           ZKSecurityTool.convertAuthorizations(Authorizations.EMPTY), NodeExistsPolicy.FAIL);\n     } catch (KeeperException | InterruptedException e) {\n       log.error(\""{}\"", e.getMessage(), e);\n@@ -89,7 +88,7 @@ public void initializeSecurity(TCredentials itw, String rootuser)\n   public void initUser(String user) throws AccumuloSecurityException {\n     ZooReaderWriter zoo = context.getZooSession().asReaderWriter();\n     try {\n-      zoo.putPersistentData(zkUserPath + \""/\"" + user, new byte[0], NodeExistsPolicy.SKIP);\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + user, new byte[0], NodeExistsPolicy.SKIP);\n     } catch (KeeperException e) {\n       log.error(\""{}\"", e.getMessage(), e);\n       throw new AccumuloSecurityException(user, SecurityErrorCode.CONNECTION_ERROR, e);\n@@ -103,8 +102,8 @@ public void initUser(String user) throws AccumuloSecurityException {\n   public void dropUser(String user) throws AccumuloSecurityException {\n     try {\n       context.getZooSession().asReaderWriter()\n-          .recursiveDelete(zkUserPath + \""/\"" + user + ZKUserAuths, NodeMissingPolicy.SKIP);\n-      context.getZooCache().clear((path) -> path.startsWith(zkUserPath + \""/\"" + user));\n+          .recursiveDelete(Constants.ZUSERS + \""/\"" + user + ZKUserAuths, NodeMissingPolicy.SKIP);\n+      context.getZooCache().clear((path) -> path.startsWith(Constants.ZUSERS + \""/\"" + user));\n     } catch (InterruptedException e) {\n       log.error(\""{}\"", e.getMessage(), e);\n       throw new IllegalStateException(e);\n@@ -122,7 +121,7 @@ public void dropUser(String user) throws AccumuloSecurityException {\n   public void changeAuthorizations(String user, Authorizations authorizations)\n       throws AccumuloSecurityException {\n     try {\n-      String userAuths = zkUserPath + \""/\"" + user + ZKUserAuths;\n+      String userAuths = Constants.ZUSERS + \""/\"" + user + ZKUserAuths;\n       context.getZooCache().clear(userAuths);\n       context.getZooSession().asReaderWriter().putPersistentData(userAuths,\n           ZKSecurityTool.convertAuthorizations(authorizations), NodeExistsPolicy.OVERWRITE);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java\nindex 4318b9448b9..b419f8d4bf9 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/security/handler/ZKPermHandler.java\n@@ -54,20 +54,14 @@ public class ZKPermHandler implements PermissionHandler {\n \n   private ServerContext ctx;\n   private ZooReaderWriter zoo;\n-  private String zkUserPath;\n-  private String ZKTablePath;\n-  private String ZKNamespacePath;\n-  private final String ZKUserSysPerms = \""/System\"";\n-  private final String ZKUserTablePerms = \""/Tables\"";\n-  private final String ZKUserNamespacePerms = \""/Namespaces\"";\n+  private static final String ZKUserSysPerms = \""/System\"";\n+  private static final String ZKUserTablePerms = \""/Tables\"";\n+  private static final String ZKUserNamespacePerms = \""/Namespaces\"";\n \n   @Override\n   public void initialize(ServerContext context) {\n     this.ctx = context;\n     zoo = context.getZooSession().asReaderWriter();\n-    zkUserPath = context.zkUserPath();\n-    ZKTablePath = context.getZooKeeperRoot() + Constants.ZTABLES;\n-    ZKNamespacePath = context.getZooKeeperRoot() + Constants.ZNAMESPACES;\n   }\n \n   @Override\n@@ -75,7 +69,7 @@ public boolean hasTablePermission(String user, String table, TablePermission per\n       throws TableNotFoundException {\n     byte[] serializedPerms;\n     try {\n-      String path = zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n+      String path = Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n       zoo.sync(path);\n       serializedPerms = zoo.getData(path);\n     } catch (KeeperException e) {\n@@ -83,7 +77,7 @@ public boolean hasTablePermission(String user, String table, TablePermission per\n         // maybe the table was just deleted?\n         try {\n           // check for existence:\n-          zoo.getData(ZKTablePath + \""/\"" + table);\n+          zoo.getData(Constants.ZTABLES + \""/\"" + table);\n           // it's there, you don't have permission\n           return false;\n         } catch (InterruptedException ex) {\n@@ -113,7 +107,7 @@ public boolean hasTablePermission(String user, String table, TablePermission per\n   @Override\n   public boolean hasCachedTablePermission(String user, String table, TablePermission permission) {\n     byte[] serializedPerms =\n-        ctx.getZooCache().get(zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table);\n+        ctx.getZooCache().get(Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table);\n     if (serializedPerms != null) {\n       return ZKSecurityTool.convertTablePermissions(serializedPerms).contains(permission);\n     }\n@@ -125,7 +119,7 @@ public boolean hasNamespacePermission(String user, String namespace,\n       NamespacePermission permission) throws NamespaceNotFoundException {\n     byte[] serializedPerms;\n     try {\n-      String path = zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n+      String path = Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n       zoo.sync(path);\n       serializedPerms = zoo.getData(path);\n     } catch (KeeperException e) {\n@@ -133,7 +127,7 @@ public boolean hasNamespacePermission(String user, String namespace,\n         // maybe the namespace was just deleted?\n         try {\n           // check for existence:\n-          zoo.getData(ZKNamespacePath + \""/\"" + namespace);\n+          zoo.getData(Constants.ZNAMESPACES + \""/\"" + namespace);\n           // it's there, you don't have permission\n           return false;\n         } catch (InterruptedException ex) {\n@@ -164,8 +158,8 @@ public boolean hasNamespacePermission(String user, String namespace,\n   @Override\n   public boolean hasCachedNamespacePermission(String user, String namespace,\n       NamespacePermission permission) {\n-    byte[] serializedPerms =\n-        ctx.getZooCache().get(zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace);\n+    byte[] serializedPerms = ctx.getZooCache()\n+        .get(Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace);\n     if (serializedPerms != null) {\n       return ZKSecurityTool.convertNamespacePermissions(serializedPerms).contains(permission);\n     }\n@@ -175,7 +169,7 @@ public boolean hasCachedNamespacePermission(String user, String namespace,\n   @Override\n   public void grantSystemPermission(String user, SystemPermission permission)\n       throws AccumuloSecurityException {\n-    final String sysPermPath = zkUserPath + \""/\"" + user + ZKUserSysPerms;\n+    final String sysPermPath = Constants.ZUSERS + \""/\"" + user + ZKUserSysPerms;\n     try {\n       byte[] permBytes = ctx.getZooCache().get(sysPermPath);\n       Set<SystemPermission> perms;\n@@ -203,7 +197,7 @@ public void grantSystemPermission(String user, SystemPermission permission)\n   public void grantTablePermission(String user, String table, TablePermission permission)\n       throws AccumuloSecurityException {\n     Set<TablePermission> tablePerms;\n-    final String tablePermPath = zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n+    final String tablePermPath = Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n     byte[] serializedPerms = ctx.getZooCache().get(tablePermPath);\n     if (serializedPerms != null) {\n       tablePerms = ZKSecurityTool.convertTablePermissions(serializedPerms);\n@@ -229,7 +223,8 @@ public void grantTablePermission(String user, String table, TablePermission perm\n   @Override\n   public void grantNamespacePermission(String user, String namespace,\n       NamespacePermission permission) throws AccumuloSecurityException {\n-    final String nsPermPath = zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n+    final String nsPermPath =\n+        Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n     Set<NamespacePermission> namespacePerms;\n     byte[] serializedPerms = ctx.getZooCache().get(nsPermPath);\n     if (serializedPerms != null) {\n@@ -256,7 +251,7 @@ public void grantNamespacePermission(String user, String namespace,\n   @Override\n   public void revokeSystemPermission(String user, SystemPermission permission)\n       throws AccumuloSecurityException {\n-    final String sysPermPath = zkUserPath + \""/\"" + user + ZKUserSysPerms;\n+    final String sysPermPath = Constants.ZUSERS + \""/\"" + user + ZKUserSysPerms;\n     byte[] sysPermBytes = ctx.getZooCache().get(sysPermPath);\n \n     // User had no system permission, nothing to revoke.\n@@ -284,7 +279,7 @@ public void revokeSystemPermission(String user, SystemPermission permission)\n   @Override\n   public void revokeTablePermission(String user, String table, TablePermission permission)\n       throws AccumuloSecurityException {\n-    final String tablePermPath = zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n+    final String tablePermPath = Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n     byte[] serializedPerms = ctx.getZooCache().get(tablePermPath);\n \n     // User had no table permission, nothing to revoke.\n@@ -315,7 +310,8 @@ public void revokeTablePermission(String user, String table, TablePermission per\n   @Override\n   public void revokeNamespacePermission(String user, String namespace,\n       NamespacePermission permission) throws AccumuloSecurityException {\n-    final String nsPermPath = zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n+    final String nsPermPath =\n+        Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n     byte[] serializedPerms = ctx.getZooCache().get(nsPermPath);\n \n     // User had no namespace permission, nothing to revoke.\n@@ -348,8 +344,8 @@ public void revokeNamespacePermission(String user, String namespace,\n   @Override\n   public void cleanTablePermissions(String table) throws AccumuloSecurityException {\n     try {\n-      for (String user : ctx.getZooCache().getChildren(zkUserPath)) {\n-        final String tablePermPath = zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n+      for (String user : ctx.getZooCache().getChildren(Constants.ZUSERS)) {\n+        final String tablePermPath = Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n         ctx.getZooCache().clear((path) -> path.startsWith(tablePermPath));\n         zoo.recursiveDelete(tablePermPath, NodeMissingPolicy.SKIP);\n       }\n@@ -365,8 +361,9 @@ public void cleanTablePermissions(String table) throws AccumuloSecurityException\n   @Override\n   public void cleanNamespacePermissions(String namespace) throws AccumuloSecurityException {\n     try {\n-      for (String user : ctx.getZooCache().getChildren(zkUserPath)) {\n-        final String nsPermPath = zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n+      for (String user : ctx.getZooCache().getChildren(Constants.ZUSERS)) {\n+        final String nsPermPath =\n+            Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n         ctx.getZooCache().clear((path) -> path.startsWith(nsPermPath));\n         zoo.recursiveDelete(nsPermPath, NodeMissingPolicy.SKIP);\n       }\n@@ -402,12 +399,12 @@ public void initializeSecurity(TCredentials itw, String rootuser)\n \n     try {\n       // prep parent node of users with root username\n-      if (!zoo.exists(zkUserPath)) {\n-        zoo.putPersistentData(zkUserPath, rootuser.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n+      if (!zoo.exists(Constants.ZUSERS)) {\n+        zoo.putPersistentData(Constants.ZUSERS, rootuser.getBytes(UTF_8), NodeExistsPolicy.FAIL);\n       }\n \n       initUser(rootuser);\n-      zoo.putPersistentData(zkUserPath + \""/\"" + rootuser + ZKUserSysPerms,\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + rootuser + ZKUserSysPerms,\n           ZKSecurityTool.convertSystemPermissions(rootPerms), NodeExistsPolicy.FAIL);\n       for (Entry<TableId,Set<TablePermission>> entry : tablePerms.entrySet()) {\n         createTablePerm(rootuser, entry.getKey(), entry.getValue());\n@@ -424,10 +421,10 @@ public void initializeSecurity(TCredentials itw, String rootuser)\n   @Override\n   public void initUser(String user) throws AccumuloSecurityException {\n     try {\n-      zoo.putPersistentData(zkUserPath + \""/\"" + user, new byte[0], NodeExistsPolicy.SKIP);\n-      zoo.putPersistentData(zkUserPath + \""/\"" + user + ZKUserTablePerms, new byte[0],\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + user, new byte[0], NodeExistsPolicy.SKIP);\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms, new byte[0],\n           NodeExistsPolicy.SKIP);\n-      zoo.putPersistentData(zkUserPath + \""/\"" + user + ZKUserNamespacePerms, new byte[0],\n+      zoo.putPersistentData(Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms, new byte[0],\n           NodeExistsPolicy.SKIP);\n     } catch (KeeperException e) {\n       log.error(\""{}\"", e.getMessage(), e);\n@@ -444,7 +441,7 @@ public void initUser(String user) throws AccumuloSecurityException {\n    */\n   private void createTablePerm(String user, TableId table, Set<TablePermission> perms)\n       throws KeeperException, InterruptedException {\n-    final String tablePermPath = zkUserPath + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n+    final String tablePermPath = Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms + \""/\"" + table;\n     ctx.getZooCache().clear((path) -> path.startsWith(tablePermPath));\n     zoo.putPersistentData(tablePermPath, ZKSecurityTool.convertTablePermissions(perms),\n         NodeExistsPolicy.FAIL);\n@@ -456,7 +453,8 @@ private void createTablePerm(String user, TableId table, Set<TablePermission> pe\n    */\n   private void createNamespacePerm(String user, NamespaceId namespace,\n       Set<NamespacePermission> perms) throws KeeperException, InterruptedException {\n-    final String nsPermPath = zkUserPath + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n+    final String nsPermPath =\n+        Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms + \""/\"" + namespace;\n     ctx.getZooCache().clear((path) -> path.startsWith(nsPermPath));\n     zoo.putPersistentData(nsPermPath, ZKSecurityTool.convertNamespacePermissions(perms),\n         NodeExistsPolicy.FAIL);\n@@ -465,10 +463,11 @@ private void createNamespacePerm(String user, NamespaceId namespace,\n   @Override\n   public void cleanUser(String user) throws AccumuloSecurityException {\n     try {\n-      zoo.recursiveDelete(zkUserPath + \""/\"" + user + ZKUserSysPerms, NodeMissingPolicy.SKIP);\n-      zoo.recursiveDelete(zkUserPath + \""/\"" + user + ZKUserTablePerms, NodeMissingPolicy.SKIP);\n-      zoo.recursiveDelete(zkUserPath + \""/\"" + user + ZKUserNamespacePerms, NodeMissingPolicy.SKIP);\n-      ctx.getZooCache().clear((path) -> path.startsWith(zkUserPath + \""/\"" + user));\n+      zoo.recursiveDelete(Constants.ZUSERS + \""/\"" + user + ZKUserSysPerms, NodeMissingPolicy.SKIP);\n+      zoo.recursiveDelete(Constants.ZUSERS + \""/\"" + user + ZKUserTablePerms, NodeMissingPolicy.SKIP);\n+      zoo.recursiveDelete(Constants.ZUSERS + \""/\"" + user + ZKUserNamespacePerms,\n+          NodeMissingPolicy.SKIP);\n+      ctx.getZooCache().clear((path) -> path.startsWith(Constants.ZUSERS + \""/\"" + user));\n     } catch (InterruptedException e) {\n       log.error(\""{}\"", e.getMessage(), e);\n       throw new IllegalStateException(e);\n@@ -486,7 +485,7 @@ public void cleanUser(String user) throws AccumuloSecurityException {\n   public boolean hasSystemPermission(String user, SystemPermission permission) {\n     byte[] perms;\n     try {\n-      String path = zkUserPath + \""/\"" + user + ZKUserSysPerms;\n+      String path = Constants.ZUSERS + \""/\"" + user + ZKUserSysPerms;\n       zoo.sync(path);\n       perms = zoo.getData(path);\n     } catch (KeeperException e) {\n@@ -508,7 +507,7 @@ public boolean hasSystemPermission(String user, SystemPermission permission) {\n \n   @Override\n   public boolean hasCachedSystemPermission(String user, SystemPermission permission) {\n-    byte[] perms = ctx.getZooCache().get(zkUserPath + \""/\"" + user + ZKUserSysPerms);\n+    byte[] perms = ctx.getZooCache().get(Constants.ZUSERS + \""/\"" + user + ZKUserSysPerms);\n     if (perms == null) {\n       return false;\n     }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java b/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\nindex be97a3bf0bb..afd0476b20b 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\n@@ -32,11 +32,9 @@\n import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeExistsPolicy;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeMissingPolicy;\n import org.apache.accumulo.core.manager.state.tables.TableState;\n@@ -65,35 +63,30 @@ public class TableManager {\n   private static final byte[] ZERO_BYTE = {'0'};\n \n   private final ServerContext context;\n-  private final String zkRoot;\n-  private final InstanceId instanceID;\n   private final ZooReaderWriter zoo;\n \n   public static void prepareNewNamespaceState(final ServerContext context, NamespaceId namespaceId,\n       String namespace, NodeExistsPolicy existsPolicy)\n       throws KeeperException, InterruptedException {\n     final PropStore propStore = context.getPropStore();\n-    final InstanceId instanceId = context.getInstanceID();\n     log.debug(\""Creating ZooKeeper entries for new namespace {} (ID: {})\"", namespace, namespaceId);\n-    context.getZooSession().asReaderWriter().putPersistentData(\n-        context.getZooKeeperRoot() + Constants.ZNAMESPACES + \""/\"" + namespaceId, new byte[0],\n-        existsPolicy);\n-    var propKey = NamespacePropKey.of(instanceId, namespaceId);\n+    context.getZooSession().asReaderWriter()\n+        .putPersistentData(Constants.ZNAMESPACES + \""/\"" + namespaceId, new byte[0], existsPolicy);\n+    var propKey = NamespacePropKey.of(namespaceId);\n     if (!propStore.exists(propKey)) {\n       propStore.create(propKey, Map.of());\n     }\n   }\n \n-  public static void prepareNewTableState(ZooReaderWriter zoo, PropStore propStore,\n-      InstanceId instanceId, TableId tableId, NamespaceId namespaceId, String tableName,\n-      TableState state, NodeExistsPolicy existsPolicy)\n+  public static void prepareNewTableState(ZooReaderWriter zoo, PropStore propStore, TableId tableId,\n+      NamespaceId namespaceId, String tableName, TableState state, NodeExistsPolicy existsPolicy)\n       throws KeeperException, InterruptedException {\n     // state gets created last\n     log.debug(\""Creating ZooKeeper entries for new table {} (ID: {}) in namespace (ID: {})\"",\n         tableName, tableId, namespaceId);\n     Pair<String,String> qualifiedTableName = TableNameUtil.qualify(tableName);\n     tableName = qualifiedTableName.getSecond();\n-    String zTablePath = ZooUtil.getRoot(instanceId) + Constants.ZTABLES + \""/\"" + tableId;\n+    String zTablePath = Constants.ZTABLES + \""/\"" + tableId;\n     zoo.putPersistentData(zTablePath, new byte[0], existsPolicy);\n     zoo.putPersistentData(zTablePath + Constants.ZTABLE_NAMESPACE,\n         namespaceId.canonical().getBytes(UTF_8), existsPolicy);\n@@ -102,7 +95,7 @@ public static void prepareNewTableState(ZooReaderWriter zoo, PropStore propStore\n     zoo.putPersistentData(zTablePath + Constants.ZTABLE_FLUSH_ID, ZERO_BYTE, existsPolicy);\n     zoo.putPersistentData(zTablePath + Constants.ZTABLE_STATE, state.name().getBytes(UTF_8),\n         existsPolicy);\n-    var propKey = TablePropKey.of(instanceId, tableId);\n+    var propKey = TablePropKey.of(tableId);\n     if (!propStore.exists(propKey)) {\n       propStore.create(propKey, Map.of());\n     }\n@@ -111,14 +104,12 @@ public static void prepareNewTableState(ZooReaderWriter zoo, PropStore propStore\n   public static void prepareNewTableState(final ServerContext context, TableId tableId,\n       NamespaceId namespaceId, String tableName, TableState state, NodeExistsPolicy existsPolicy)\n       throws KeeperException, InterruptedException {\n-    prepareNewTableState(context.getZooSession().asReaderWriter(), context.getPropStore(),\n-        context.getInstanceID(), tableId, namespaceId, tableName, state, existsPolicy);\n+    prepareNewTableState(context.getZooSession().asReaderWriter(), context.getPropStore(), tableId,\n+        namespaceId, tableName, state, existsPolicy);\n   }\n \n   public TableManager(ServerContext context) {\n     this.context = context;\n-    zkRoot = context.getZooKeeperRoot();\n-    instanceID = context.getInstanceID();\n     zoo = context.getZooSession().asReaderWriter();\n     // add our Watcher to the shared ZooCache\n     context.getZooCache().addZooCacheWatcher(new TableStateWatcher());\n@@ -132,7 +123,7 @@ public TableState getTableState(TableId tableId) {\n   public synchronized void transitionTableState(final TableId tableId, final TableState newState,\n       final EnumSet<TableState> expectedCurrStates) {\n     Preconditions.checkArgument(newState != TableState.UNKNOWN);\n-    String statePath = zkRoot + Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE;\n+    String statePath = Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE;\n \n     try {\n       zoo.mutateOrCreate(statePath, newState.name().getBytes(UTF_8), currData -> {\n@@ -178,9 +169,9 @@ public synchronized void transitionTableState(final TableId tableId, final Table\n \n   private void updateTableStateCache() {\n     synchronized (tableStateCache) {\n-      for (String tableId : context.getZooCache().getChildren(zkRoot + Constants.ZTABLES)) {\n-        if (context.getZooCache()\n-            .get(zkRoot + Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE) != null) {\n+      for (String tableId : context.getZooCache().getChildren(Constants.ZTABLES)) {\n+        if (context.getZooCache().get(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE)\n+            != null) {\n           updateTableStateCache(TableId.of(tableId));\n         }\n       }\n@@ -190,8 +181,8 @@ private void updateTableStateCache() {\n   public TableState updateTableStateCache(TableId tableId) {\n     synchronized (tableStateCache) {\n       TableState tState = TableState.UNKNOWN;\n-      byte[] data = context.getZooCache()\n-          .get(zkRoot + Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE);\n+      byte[] data =\n+          context.getZooCache().get(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE);\n       if (data != null) {\n         String sState = new String(data, UTF_8);\n         try {\n@@ -207,7 +198,7 @@ public TableState updateTableStateCache(TableId tableId) {\n \n   public void addTable(TableId tableId, NamespaceId namespaceId, String tableName)\n       throws KeeperException, InterruptedException, NamespaceNotFoundException {\n-    prepareNewTableState(zoo, context.getPropStore(), instanceID, tableId, namespaceId, tableName,\n+    prepareNewTableState(zoo, context.getPropStore(), tableId, namespaceId, tableName,\n         TableState.NEW, NodeExistsPolicy.OVERWRITE);\n     updateTableStateCache(tableId);\n   }\n@@ -215,17 +206,15 @@ public void addTable(TableId tableId, NamespaceId namespaceId, String tableName)\n   public void cloneTable(TableId srcTableId, TableId tableId, String tableName,\n       NamespaceId namespaceId, Map<String,String> propertiesToSet, Set<String> propertiesToExclude)\n       throws KeeperException, InterruptedException {\n-    prepareNewTableState(zoo, context.getPropStore(), instanceID, tableId, namespaceId, tableName,\n+    prepareNewTableState(zoo, context.getPropStore(), tableId, namespaceId, tableName,\n         TableState.NEW, NodeExistsPolicy.OVERWRITE);\n \n-    String srcTablePath =\n-        context.getZooKeeperRoot() + Constants.ZTABLES + \""/\"" + srcTableId + Constants.ZCONFIG;\n-    String newTablePath =\n-        context.getZooKeeperRoot() + Constants.ZTABLES + \""/\"" + tableId + Constants.ZCONFIG;\n+    String srcTablePath = Constants.ZTABLES + \""/\"" + srcTableId + Constants.ZCONFIG;\n+    String newTablePath = Constants.ZTABLES + \""/\"" + tableId + Constants.ZCONFIG;\n     zoo.recursiveCopyPersistentOverwrite(srcTablePath, newTablePath);\n \n-    PropUtil.setProperties(context, TablePropKey.of(context, tableId), propertiesToSet);\n-    PropUtil.removeProperties(context, TablePropKey.of(context, tableId), propertiesToExclude);\n+    PropUtil.setProperties(context, TablePropKey.of(tableId), propertiesToSet);\n+    PropUtil.removeProperties(context, TablePropKey.of(tableId), propertiesToExclude);\n \n     updateTableStateCache(tableId);\n   }\n@@ -233,9 +222,9 @@ public void cloneTable(TableId srcTableId, TableId tableId, String tableName,\n   public void removeTable(TableId tableId) throws KeeperException, InterruptedException {\n     synchronized (tableStateCache) {\n       tableStateCache.remove(tableId);\n-      zoo.recursiveDelete(zkRoot + Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE,\n+      zoo.recursiveDelete(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE,\n           NodeMissingPolicy.SKIP);\n-      zoo.recursiveDelete(zkRoot + Constants.ZTABLES + \""/\"" + tableId, NodeMissingPolicy.SKIP);\n+      zoo.recursiveDelete(Constants.ZTABLES + \""/\"" + tableId, NodeMissingPolicy.SKIP);\n     }\n   }\n \n@@ -257,11 +246,10 @@ public void accept(WatchedEvent event) {\n       final String zPath = event.getPath();\n       final EventType zType = event.getType();\n \n-      String tablesPrefix = zkRoot + Constants.ZTABLES;\n       TableId tableId = null;\n \n-      if (zPath != null && zPath.startsWith(tablesPrefix + \""/\"")) {\n-        String suffix = zPath.substring(tablesPrefix.length() + 1);\n+      if (zPath != null && zPath.startsWith(Constants.ZTABLES + \""/\"")) {\n+        String suffix = zPath.substring(Constants.ZTABLES.length() + 1);\n         if (suffix.contains(\""/\"")) {\n           String[] sa = suffix.split(\""/\"", 2);\n           if (Constants.ZTABLE_STATE.equals(\""/\"" + sa[1])) {\n@@ -296,9 +284,9 @@ public void accept(WatchedEvent event) {\n           break;\n         case NodeDeleted:\n           if (zPath != null && tableId != null\n-              && (zPath.equals(tablesPrefix + \""/\"" + tableId + Constants.ZTABLE_STATE)\n-                  || zPath.equals(tablesPrefix + \""/\"" + tableId + Constants.ZCONFIG)\n-                  || zPath.equals(tablesPrefix + \""/\"" + tableId + Constants.ZTABLE_NAME))) {\n+              && (zPath.equals(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_STATE)\n+                  || zPath.equals(Constants.ZTABLES + \""/\"" + tableId + Constants.ZCONFIG)\n+                  || zPath.equals(Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_NAME))) {\n             tableStateCache.remove(tableId);\n           }\n           break;\n@@ -326,14 +314,14 @@ public void accept(WatchedEvent event) {\n   public void removeNamespace(NamespaceId namespaceId)\n       throws KeeperException, InterruptedException, AcceptableThriftTableOperationException {\n     try {\n-      NamespaceMapping.remove(zoo, zkRoot + Constants.ZNAMESPACES, namespaceId);\n+      NamespaceMapping.remove(zoo, Constants.ZNAMESPACES, namespaceId);\n     } catch (AcceptableThriftTableOperationException e) {\n       // ignore not found, because that's what we're trying to do anyway\n       if (e.getType() != TableOperationExceptionType.NAMESPACE_NOTFOUND) {\n         throw e;\n       }\n     }\n-    zoo.recursiveDelete(zkRoot + Constants.ZNAMESPACES + \""/\"" + namespaceId, NodeMissingPolicy.SKIP);\n+    zoo.recursiveDelete(Constants.ZNAMESPACES + \""/\"" + namespaceId, NodeMissingPolicy.SKIP);\n   }\n \n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/tablets/UniqueNameAllocator.java b/server/base/src/main/java/org/apache/accumulo/server/tablets/UniqueNameAllocator.java\nindex f88ee0fcaef..761993fa6a0 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/tablets/UniqueNameAllocator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/tablets/UniqueNameAllocator.java\n@@ -43,22 +43,20 @@ public class UniqueNameAllocator {\n   private static final int DEFAULT_MIN = DefaultConfiguration.getInstance().getCount(MIN_PROP);\n \n   private final ServerContext context;\n-  private final String nextNamePath;\n \n   private long next = 0;\n   private long maxAllocated = 0;\n \n   public UniqueNameAllocator(ServerContext context) {\n     this.context = context;\n-    nextNamePath = context.getZooKeeperRoot() + Constants.ZNEXT_FILE;\n   }\n \n   public synchronized String getNextName() {\n     while (next >= maxAllocated) {\n       final int allocate = getAllocation();\n       try {\n-        byte[] max =\n-            context.getZooSession().asReaderWriter().mutateExisting(nextNamePath, currentValue -> {\n+        byte[] max = context.getZooSession().asReaderWriter().mutateExisting(Constants.ZNEXT_FILE,\n+            currentValue -> {\n               long l = Long.parseLong(new String(currentValue, UTF_8), Character.MAX_RADIX);\n               return Long.toString(l + allocate, Character.MAX_RADIX).getBytes(UTF_8);\n             });\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\nindex 47dcd1e09c9..9d9cf95f814 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n@@ -19,7 +19,6 @@\n package org.apache.accumulo.server.util;\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n-import static java.util.Objects.requireNonNull;\n \n import java.io.BufferedWriter;\n import java.io.File;\n@@ -724,17 +723,6 @@ private static void stopTabletServer(final ClientContext context, List<String> s\n     }\n   }\n \n-  /**\n-   * Get the parent ZNode for tservers for the given instance\n-   *\n-   * @param context ClientContext\n-   * @return The tservers znode for the instance\n-   */\n-  static String getTServersZkPath(ClientContext context) {\n-    requireNonNull(context);\n-    return context.getZooKeeperRoot() + Constants.ZTSERVERS;\n-  }\n-\n   /**\n    * Look up the TabletServers in ZooKeeper and try to find a sessionID for this server reference\n    *\n@@ -973,9 +961,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n     validateFateUserInput(fateOpsCommand);\n \n     AdminUtil<Admin> admin = new AdminUtil<>();\n-    final String zkRoot = context.getZooKeeperRoot();\n     var zTableLocksPath = context.getServerPaths().createTableLocksPath();\n-    String fateZkPath = zkRoot + Constants.ZFATE;\n     var zk = context.getZooSession();\n     ServiceLock adminLock = null;\n     Map<FateInstanceType,FateStore<Admin>> fateStores;\n@@ -986,7 +972,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n         cancelSubmittedFateTxs(context, fateOpsCommand.fateIdList);\n       } else if (fateOpsCommand.fail) {\n         adminLock = createAdminLock(context);\n-        fateStores = createFateStores(context, zk, fateZkPath, adminLock);\n+        fateStores = createFateStores(context, zk, adminLock);\n         for (String fateIdStr : fateOpsCommand.fateIdList) {\n           if (!admin.prepFail(fateStores, fateIdStr)) {\n             throw new AccumuloException(\""Could not fail transaction: \"" + fateIdStr);\n@@ -994,7 +980,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n         }\n       } else if (fateOpsCommand.delete) {\n         adminLock = createAdminLock(context);\n-        fateStores = createFateStores(context, zk, fateZkPath, adminLock);\n+        fateStores = createFateStores(context, zk, adminLock);\n         for (String fateIdStr : fateOpsCommand.fateIdList) {\n           if (!admin.prepDelete(fateStores, fateIdStr)) {\n             throw new AccumuloException(\""Could not delete transaction: \"" + fateIdStr);\n@@ -1010,7 +996,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n             getCmdLineStatusFilters(fateOpsCommand.states);\n         EnumSet<FateInstanceType> typesFilter =\n             getCmdLineInstanceTypeFilters(fateOpsCommand.instanceTypes);\n-        readOnlyFateStores = createReadOnlyFateStores(context, zk, fateZkPath);\n+        readOnlyFateStores = createReadOnlyFateStores(context, zk, Constants.ZFATE);\n         admin.print(readOnlyFateStores, zk, zTableLocksPath, new Formatter(System.out),\n             fateIdFilter, statusFilter, typesFilter);\n         // print line break at the end\n@@ -1019,7 +1005,7 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n \n       if (fateOpsCommand.summarize) {\n         if (readOnlyFateStores == null) {\n-          readOnlyFateStores = createReadOnlyFateStores(context, zk, fateZkPath);\n+          readOnlyFateStores = createReadOnlyFateStores(context, zk, Constants.ZFATE);\n         }\n         summarizeFateTx(context, fateOpsCommand, admin, readOnlyFateStores, zTableLocksPath);\n       }\n@@ -1031,10 +1017,9 @@ private void executeFateOpsCommand(ServerContext context, FateOpsCommand fateOps\n   }\n \n   private Map<FateInstanceType,FateStore<Admin>> createFateStores(ServerContext context,\n-      ZooSession zk, String fateZkPath, ServiceLock adminLock)\n-      throws InterruptedException, KeeperException {\n+      ZooSession zk, ServiceLock adminLock) throws InterruptedException, KeeperException {\n     var lockId = adminLock.getLockID();\n-    MetaFateStore<Admin> mfs = new MetaFateStore<>(fateZkPath, zk, lockId, null);\n+    MetaFateStore<Admin> mfs = new MetaFateStore<>(zk, lockId, null);\n     UserFateStore<Admin> ufs =\n         new UserFateStore<>(context, AccumuloTable.FATE.tableName(), lockId, null);\n     return Map.of(FateInstanceType.META, mfs, FateInstanceType.USER, ufs);\n@@ -1043,7 +1028,7 @@ private Map<FateInstanceType,FateStore<Admin>> createFateStores(ServerContext co\n   private Map<FateInstanceType,ReadOnlyFateStore<Admin>>\n       createReadOnlyFateStores(ServerContext context, ZooSession zk, String fateZkPath)\n           throws InterruptedException, KeeperException {\n-    MetaFateStore<Admin> readOnlyMFS = new MetaFateStore<>(fateZkPath, zk, null, null);\n+    MetaFateStore<Admin> readOnlyMFS = new MetaFateStore<>(zk, null, null);\n     UserFateStore<Admin> readOnlyUFS =\n         new UserFateStore<>(context, AccumuloTable.FATE.tableName(), null, null);\n     return Map.of(FateInstanceType.META, readOnlyMFS, FateInstanceType.USER, readOnlyUFS);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java b/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java\nindex c6523cf3ed5..5538e875f7c 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java\n@@ -106,9 +106,8 @@ private static void verifyAccumuloIsDown(ServerContext context, String oldPasswo\n         new ZooSession(ChangeSecret.class.getSimpleName() + \"".verifyAccumuloIsDown(oldPassword)\"",\n             conf.get(Property.INSTANCE_ZK_HOST),\n             (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT), oldPassword)) {\n-      String root = context.getZooKeeperRoot();\n       final List<String> ephemerals = new ArrayList<>();\n-      recurse(oldZk.asReaderWriter(), root, (zoo, path) -> {\n+      recurse(oldZk.asReaderWriter(), \""\"", (zoo, path) -> {\n         Stat stat = zoo.getStatus(path);\n         if (stat.getEphemeralOwner() != 0) {\n           ephemerals.add(path);\n@@ -139,8 +138,7 @@ private static void rewriteZooKeeperInstance(final ServerContext context,\n \n       final var orig = oldZk.asReaderWriter();\n       final var new_ = newZk.asReaderWriter();\n-      String root = context.getZooKeeperRoot();\n-      recurse(orig, root, (zoo, path) -> {\n+      recurse(orig, \""\"", (zoo, path) -> {\n         String newPath =\n             path.replace(context.getInstanceID().canonical(), newInstanceId.canonical());\n         byte[] data = zoo.getData(path);\n@@ -226,7 +224,7 @@ private static void deleteInstance(ServerContext context, String oldPass) throws\n         (int) conf.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT), oldPass)) {\n \n       var orig = oldZk.asReaderWriter();\n-      orig.recursiveDelete(context.getZooKeeperRoot(), NodeMissingPolicy.SKIP);\n+      orig.recursiveDelete(\""\"", NodeMissingPolicy.SKIP);\n     }\n   }\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ListInstances.java b/server/base/src/main/java/org/apache/accumulo/server/util/ListInstances.java\nindex 76a37505fab..6be707062bf 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ListInstances.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ListInstances.java\n@@ -34,7 +34,6 @@\n import org.apache.accumulo.core.conf.SiteConfiguration;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.lock.ServiceLock;\n import org.apache.accumulo.core.lock.ServiceLockData;\n import org.apache.accumulo.core.lock.ServiceLockData.ThriftService;\n@@ -169,8 +168,8 @@ private static String getManager(ZooSession zs, InstanceId iid, boolean printErr\n     }\n \n     try {\n-      var zLockManagerPath = ServiceLockPaths.parse(Optional.of(Constants.ZMANAGER_LOCK),\n-          ZooUtil.getRoot(iid) + Constants.ZMANAGER_LOCK);\n+      var zLockManagerPath =\n+          ServiceLockPaths.parse(Optional.of(Constants.ZMANAGER_LOCK), Constants.ZMANAGER_LOCK);\n       Optional<ServiceLockData> sld = ServiceLock.getLockData(zs, zLockManagerPath);\n       if (sld.isEmpty()) {\n         return null;\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java b/server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java\nindex 4e38572dd31..1f5bd0634c4 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/MetadataTableUtil.java\n@@ -93,8 +93,7 @@ public class MetadataTableUtil {\n   private MetadataTableUtil() {}\n \n   public static void putLockID(ServerContext context, ServiceLock zooLock, Mutation m) {\n-    ServerColumnFamily.LOCK_COLUMN.put(m,\n-        new Value(zooLock.getLockID().serialize(context.getZooKeeperRoot() + \""/\"")));\n+    ServerColumnFamily.LOCK_COLUMN.put(m, new Value(zooLock.getLockID().serialize(\""/\"")));\n   }\n \n   public static void deleteTable(TableId tableId, boolean insertDeletes, ServerContext context,\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java b/server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java\nindex b49acae2481..3c94d22596d 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/PropUtil.java\n@@ -37,26 +37,25 @@ private PropUtil() {}\n    *         or other failure to read properties from the cache / backend store\n    * @throws IllegalArgumentException if a provided property is not valid\n    */\n-  public static void setProperties(final ServerContext context, final PropStoreKey<?> propStoreKey,\n+  public static void setProperties(final ServerContext context, final PropStoreKey propStoreKey,\n       final Map<String,String> properties) throws IllegalArgumentException {\n     PropUtil.validateProperties(context, propStoreKey, properties);\n     context.getPropStore().putAll(propStoreKey, properties);\n   }\n \n-  public static void removeProperties(final ServerContext context,\n-      final PropStoreKey<?> propStoreKey, final Collection<String> propertyNames) {\n+  public static void removeProperties(final ServerContext context, final PropStoreKey propStoreKey,\n+      final Collection<String> propertyNames) {\n     context.getPropStore().removeProperties(propStoreKey, propertyNames);\n   }\n \n-  public static void replaceProperties(final ServerContext context,\n-      final PropStoreKey<?> propStoreKey, final long version, final Map<String,String> properties)\n-      throws IllegalArgumentException {\n+  public static void replaceProperties(final ServerContext context, final PropStoreKey propStoreKey,\n+      final long version, final Map<String,String> properties) throws IllegalArgumentException {\n     PropUtil.validateProperties(context, propStoreKey, properties);\n     context.getPropStore().replaceAll(propStoreKey, version, properties);\n   }\n \n   protected static void validateProperties(final ServerContext context,\n-      final PropStoreKey<?> propStoreKey, final Map<String,String> properties)\n+      final PropStoreKey propStoreKey, final Map<String,String> properties)\n       throws IllegalArgumentException {\n     for (Map.Entry<String,String> prop : properties.entrySet()) {\n       if (!Property.isValidProperty(prop.getKey(), prop.getValue())) {\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java b/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\nindex ff88b17863e..8d646c1ce9a 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\n@@ -27,6 +27,7 @@\n import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.lock.ServiceLockData;\n import org.apache.accumulo.core.lock.ServiceLockPaths.AddressSelector;\n import org.apache.accumulo.core.lock.ServiceLockPaths.ServiceLockPath;\n@@ -56,7 +57,7 @@ public ServiceStatusCmd() {}\n   public void execute(final ServerContext context, final boolean json, final boolean noHosts) {\n \n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\""zooRoot: {}\"", context.getZooKeeperRoot());\n+      LOG.trace(\""zooRoot: {}\"", ZooUtil.getRoot(context.getInstanceID()));\n     }\n \n     final Map<ServiceStatusReport.ReportKey,StatusSummary> services = new TreeMap<>();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java b/server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java\nindex b8589ab21ab..8ef760b7723 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/SystemPropUtil.java\n@@ -36,14 +36,14 @@ public class SystemPropUtil {\n \n   public static void setSystemProperty(ServerContext context, String property, String value)\n       throws IllegalArgumentException {\n-    final SystemPropKey key = SystemPropKey.of(context);\n+    final SystemPropKey key = SystemPropKey.of();\n     context.getPropStore().putAll(key,\n         Map.of(validateSystemProperty(context, key, property, value), value));\n   }\n \n   public static void modifyProperties(ServerContext context, long version,\n       Map<String,String> properties) throws IllegalArgumentException {\n-    final SystemPropKey key = SystemPropKey.of(context);\n+    final SystemPropKey key = SystemPropKey.of();\n     final Map<String,\n         String> checkedProperties = properties.entrySet().stream()\n             .collect(Collectors.toMap(\n@@ -64,7 +64,7 @@ public static void removeSystemProperty(ServerContext context, String property)\n   }\n \n   public static void removePropWithoutDeprecationWarning(ServerContext context, String property) {\n-    context.getPropStore().removeProperties(SystemPropKey.of(context), List.of(property));\n+    context.getPropStore().removeProperties(SystemPropKey.of(), List.of(property));\n   }\n \n   private static String validateSystemProperty(ServerContext context, SystemPropKey key,\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ZooKeeperMain.java b/server/base/src/main/java/org/apache/accumulo/server/util/ZooKeeperMain.java\nindex b7cb4d5732e..9165343992d 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ZooKeeperMain.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ZooKeeperMain.java\n@@ -20,6 +20,7 @@\n \n import org.apache.accumulo.core.cli.Help;\n import org.apache.accumulo.core.conf.SiteConfiguration;\n+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.start.spi.KeywordExecutable;\n \n@@ -69,7 +70,7 @@ public void execute(final String[] args) throws Exception {\n       }\n       System.out.println(\""The accumulo instance id is \"" + context.getInstanceID());\n       if (!opts.servers.contains(\""/\"")) {\n-        opts.servers += context.getZooKeeperRoot();\n+        opts.servers += ZooUtil.getRoot(context.getInstanceID());\n       }\n       org.apache.zookeeper.ZooKeeperMain\n           .main(new String[] {\""-server\"", opts.servers, \""-timeout\"", \""\"" + (opts.timeout * 1000)});\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java b/server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java\nindex cc4d7235915..2922718010e 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ZooZap.java\n@@ -32,7 +32,6 @@\n import org.apache.accumulo.core.lock.ServiceLockPaths.ServiceLockPath;\n import org.apache.accumulo.core.singletons.SingletonManager;\n import org.apache.accumulo.core.singletons.SingletonManager.Mode;\n-import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.security.SecurityUtil;\n import org.apache.accumulo.start.spi.KeywordExecutable;\n@@ -86,19 +85,19 @@ public static void main(String[] args) throws Exception {\n \n   @Override\n   public void execute(String[] args) throws Exception {\n-    try {\n-      var siteConf = SiteConfiguration.auto();\n+    var siteConf = SiteConfiguration.auto();\n+    try (var context = new ServerContext(siteConf)) {\n       // Login as the server on secure HDFS\n       if (siteConf.getBoolean(Property.INSTANCE_RPC_SASL_ENABLED)) {\n         SecurityUtil.serverLogin(siteConf);\n       }\n-      zap(siteConf, args);\n+      zap(context, args);\n     } finally {\n       SingletonManager.setMode(Mode.CLOSED);\n     }\n   }\n \n-  public void zap(SiteConfiguration siteConf, String... args) {\n+  public void zap(ServerContext context, String... args) {\n     Opts opts = new Opts();\n     opts.parseArgs(keyword(), args);\n \n@@ -107,81 +106,71 @@ public void zap(SiteConfiguration siteConf, String... args) {\n       return;\n     }\n \n-    try (var zk = new ZooSession(getClass().getSimpleName(), siteConf)) {\n-      // Login as the server on secure HDFS\n-      if (siteConf.getBoolean(Property.INSTANCE_RPC_SASL_ENABLED)) {\n-        SecurityUtil.serverLogin(siteConf);\n+    var zrw = context.getZooSession().asReaderWriter();\n+    if (opts.zapManager) {\n+      ServiceLockPath managerLockPath = context.getServerPaths().createManagerPath();\n+      try {\n+        zapDirectory(zrw, managerLockPath, opts);\n+      } catch (KeeperException | InterruptedException e) {\n+        e.printStackTrace();\n       }\n+    }\n \n-      try (var context = new ServerContext(siteConf)) {\n-        final ZooReaderWriter zrw = context.getZooSession().asReaderWriter();\n-        if (opts.zapManager) {\n-          ServiceLockPath managerLockPath = context.getServerPaths().createManagerPath();\n-          try {\n-            zapDirectory(zrw, managerLockPath, opts);\n-          } catch (KeeperException | InterruptedException e) {\n-            e.printStackTrace();\n-          }\n-        }\n+    ResourceGroupPredicate rgp;\n+    if (!opts.resourceGroup.isEmpty()) {\n+      rgp = rg -> rg.equals(opts.resourceGroup);\n+    } else {\n+      rgp = rg -> true;\n+    }\n \n-        ResourceGroupPredicate rgp;\n-        if (!opts.resourceGroup.isEmpty()) {\n-          rgp = rg -> rg.equals(opts.resourceGroup);\n-        } else {\n-          rgp = rg -> true;\n+    if (opts.zapTservers) {\n+      try {\n+        Set<ServiceLockPath> tserverLockPaths =\n+            context.getServerPaths().getTabletServer(rgp, AddressSelector.all(), false);\n+        Set<String> tserverResourceGroupPaths = new HashSet<>();\n+        tserverLockPaths.forEach(p -> tserverResourceGroupPaths\n+            .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n+        for (String group : tserverResourceGroupPaths) {\n+          message(\""Deleting tserver \"" + group + \"" from zookeeper\"", opts);\n+          zrw.recursiveDelete(group.toString(), NodeMissingPolicy.SKIP);\n         }\n+      } catch (KeeperException | InterruptedException e) {\n+        log.error(\""{}\"", e.getMessage(), e);\n+      }\n+    }\n \n-        if (opts.zapTservers) {\n-          try {\n-            Set<ServiceLockPath> tserverLockPaths =\n-                context.getServerPaths().getTabletServer(rgp, AddressSelector.all(), false);\n-            Set<String> tserverResourceGroupPaths = new HashSet<>();\n-            tserverLockPaths.forEach(p -> tserverResourceGroupPaths\n-                .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n-            for (String group : tserverResourceGroupPaths) {\n-              message(\""Deleting tserver \"" + group + \"" from zookeeper\"", opts);\n-              zrw.recursiveDelete(group.toString(), NodeMissingPolicy.SKIP);\n-            }\n-          } catch (KeeperException | InterruptedException e) {\n-            log.error(\""{}\"", e.getMessage(), e);\n-          }\n+    if (opts.zapCompactors) {\n+      Set<ServiceLockPath> compactorLockPaths =\n+          context.getServerPaths().getCompactor(rgp, AddressSelector.all(), false);\n+      Set<String> compactorResourceGroupPaths = new HashSet<>();\n+      compactorLockPaths.forEach(p -> compactorResourceGroupPaths\n+          .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n+      try {\n+        for (String group : compactorResourceGroupPaths) {\n+          message(\""Deleting compactor \"" + group + \"" from zookeeper\"", opts);\n+          zrw.recursiveDelete(group, NodeMissingPolicy.SKIP);\n         }\n+      } catch (KeeperException | InterruptedException e) {\n+        log.error(\""Error deleting compactors from zookeeper, {}\"", e.getMessage(), e);\n+      }\n \n-        if (opts.zapCompactors) {\n-          Set<ServiceLockPath> compactorLockPaths =\n-              context.getServerPaths().getCompactor(rgp, AddressSelector.all(), false);\n-          Set<String> compactorResourceGroupPaths = new HashSet<>();\n-          compactorLockPaths.forEach(p -> compactorResourceGroupPaths\n-              .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n-          try {\n-            for (String group : compactorResourceGroupPaths) {\n-              message(\""Deleting compactor \"" + group + \"" from zookeeper\"", opts);\n-              zrw.recursiveDelete(group, NodeMissingPolicy.SKIP);\n-            }\n-          } catch (KeeperException | InterruptedException e) {\n-            log.error(\""Error deleting compactors from zookeeper, {}\"", e.getMessage(), e);\n-          }\n-\n-        }\n+    }\n \n-        if (opts.zapScanServers) {\n-          Set<ServiceLockPath> sserverLockPaths =\n-              context.getServerPaths().getScanServer(rgp, AddressSelector.all(), false);\n-          Set<String> sserverResourceGroupPaths = new HashSet<>();\n-          sserverLockPaths.forEach(p -> sserverResourceGroupPaths\n-              .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n-\n-          try {\n-            for (String group : sserverResourceGroupPaths) {\n-              message(\""Deleting sserver \"" + group + \"" from zookeeper\"", opts);\n-              zrw.recursiveDelete(group, NodeMissingPolicy.SKIP);\n-            }\n-          } catch (KeeperException | InterruptedException e) {\n-            log.error(\""{}\"", e.getMessage(), e);\n-          }\n+    if (opts.zapScanServers) {\n+      Set<ServiceLockPath> sserverLockPaths =\n+          context.getServerPaths().getScanServer(rgp, AddressSelector.all(), false);\n+      Set<String> sserverResourceGroupPaths = new HashSet<>();\n+      sserverLockPaths.forEach(p -> sserverResourceGroupPaths\n+          .add(p.toString().substring(0, p.toString().lastIndexOf('/'))));\n+\n+      try {\n+        for (String group : sserverResourceGroupPaths) {\n+          message(\""Deleting sserver \"" + group + \"" from zookeeper\"", opts);\n+          zrw.recursiveDelete(group, NodeMissingPolicy.SKIP);\n         }\n+      } catch (KeeperException | InterruptedException e) {\n+        log.error(\""{}\"", e.getMessage(), e);\n       }\n-\n     }\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/RootMetadataCheckRunner.java b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/RootMetadataCheckRunner.java\nindex 9ebc3189124..94c4a52268b 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/RootMetadataCheckRunner.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/RootMetadataCheckRunner.java\n@@ -86,8 +86,8 @@ public Admin.CheckCommand.CheckStatus runCheck(ServerContext context, ServerUtil\n     status = checkRequiredColumns(context, status);\n \n     log.trace(\""********** Looking for invalid columns **********\"");\n-    final String path = context.getZooKeeperRoot() + RootTable.ZROOT_TABLET;\n-    final String json = new String(context.getZooSession().asReader().getData(path), UTF_8);\n+    final String json =\n+        new String(context.getZooSession().asReader().getData(RootTable.ZROOT_TABLET), UTF_8);\n     final var rtm = new RootTabletMetadata(json);\n     status = checkColumns(context, rtm.getKeyValues().iterator(), status);\n \n@@ -97,10 +97,9 @@ public Admin.CheckCommand.CheckStatus runCheck(ServerContext context, ServerUtil\n \n   @Override\n   public Admin.CheckCommand.CheckStatus checkRequiredColumns(ServerContext context,\n-      Admin.CheckCommand.CheckStatus status)\n-      throws TableNotFoundException, InterruptedException, KeeperException {\n-    final String path = context.getZooKeeperRoot() + RootTable.ZROOT_TABLET;\n-    final String json = new String(context.getZooSession().asReader().getData(path), UTF_8);\n+      Admin.CheckCommand.CheckStatus status) throws InterruptedException, KeeperException {\n+    final String json =\n+        new String(context.getZooSession().asReader().getData(RootTable.ZROOT_TABLET), UTF_8);\n     final var rtm = new RootTabletMetadata(json);\n     final Set<Text> rowsSeen = new HashSet<>();\n     final Set<ColumnFQ> requiredColFQs = new HashSet<>(requiredColFQs());\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\nindex 58a5f68ced0..7c97e043453 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/checkCommand/TableLocksCheckRunner.java\n@@ -21,7 +21,6 @@\n import java.util.List;\n import java.util.Map;\n \n-import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.fate.AdminUtil;\n@@ -60,11 +59,9 @@ private static Admin.CheckCommand.CheckStatus checkTableLocks(ServerContext cont\n       Admin.CheckCommand.CheckStatus status)\n       throws InterruptedException, KeeperException, AccumuloException, AccumuloSecurityException {\n     final AdminUtil<Admin> admin = new AdminUtil<>();\n-    final String zkRoot = context.getZooKeeperRoot();\n     final var zTableLocksPath = context.getServerPaths().createTableLocksPath();\n-    final String fateZkPath = zkRoot + Constants.ZFATE;\n     final var zk = context.getZooSession();\n-    final MetaFateStore<Admin> mfs = new MetaFateStore<>(fateZkPath, zk, null, null);\n+    final MetaFateStore<Admin> mfs = new MetaFateStore<>(zk, null, null);\n     final UserFateStore<Admin> ufs =\n         new UserFateStore<>(context, AccumuloTable.FATE.tableName(), null, null);\n \n\ndiff --git a/server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java b/server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java\nindex d47f75c36b3..3a79855d8d7 100644\n--- a/server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java\n+++ b/server/gc/src/main/java/org/apache/accumulo/gc/GCRun.java\n@@ -50,6 +50,7 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n+import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.gc.GcCandidate;\n import org.apache.accumulo.core.gc.Reference;\n import org.apache.accumulo.core.gc.ReferenceFile;\n@@ -227,19 +228,17 @@ public Stream<Reference> getReferences() {\n \n   @Override\n   public Map<TableId,TableState> getTableIDs() throws InterruptedException {\n-    final String tablesPath = context.getZooKeeperRoot() + Constants.ZTABLES;\n     final ZooReader zr = context.getZooSession().asReader();\n     int retries = 1;\n     IllegalStateException ioe = null;\n     while (retries <= 10) {\n       try {\n-        zr.sync(tablesPath);\n+        zr.sync(Constants.ZTABLES);\n         final Map<TableId,TableState> tids = new HashMap<>();\n-        for (String table : zr.getChildren(tablesPath)) {\n+        for (String table : zr.getChildren(Constants.ZTABLES)) {\n           TableId tableId = TableId.of(table);\n           TableState tableState = null;\n-          String statePath = context.getZooKeeperRoot() + Constants.ZTABLES + \""/\""\n-              + tableId.canonical() + Constants.ZTABLE_STATE;\n+          String statePath = Constants.ZTABLES + \""/\"" + tableId.canonical() + Constants.ZTABLE_STATE;\n           try {\n             byte[] state = zr.getData(statePath);\n             if (state == null) {\n@@ -267,11 +266,11 @@ public Map<TableId,TableState> getTableIDs() throws InterruptedException {\n   }\n \n   @Override\n-  public void deleteConfirmedCandidates(SortedMap<String,GcCandidate> confirmedDeletes)\n-      throws TableNotFoundException {\n+  public void deleteConfirmedCandidates(SortedMap<String,GcCandidate> confirmedDeletes) {\n     final VolumeManager fs = context.getVolumeManager();\n     var metadataLocation = level == Ample.DataLevel.ROOT\n-        ? context.getZooKeeperRoot() + \"" for \"" + AccumuloTable.ROOT.tableName() : level.metaTable();\n+        ? ZooUtil.getRoot(context.getInstanceID()) + \"" for \"" + AccumuloTable.ROOT.tableName()\n+        : level.metaTable();\n \n     if (inSafeMode()) {\n       System.out.println(\""SAFEMODE: There are \"" + confirmedDeletes.size()\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex f5fc39fda0e..edeef321a6b 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -493,7 +493,7 @@ protected Manager(ConfigOpts opts, Function<SiteConfiguration,ServerContext> ser\n       final long tokenUpdateInterval =\n           aconf.getTimeInMillis(Property.GENERAL_DELEGATION_TOKEN_UPDATE_INTERVAL);\n       keyDistributor = new ZooAuthenticationKeyDistributor(context.getZooSession(),\n-          context.getZooKeeperRoot() + Constants.ZDELEGATION_TOKEN_KEYS);\n+          Constants.ZDELEGATION_TOKEN_KEYS);\n       authenticationTokenKeyManager = new AuthenticationTokenKeyManager(context.getSecretManager(),\n           keyDistributor, tokenUpdateInterval, tokenLifetime);\n       delegationTokensAvailable = true;\n@@ -511,8 +511,7 @@ public TServerConnection getConnection(TServerInstance server) {\n \n   void setManagerGoalState(ManagerGoalState state) {\n     try {\n-      getContext().getZooSession().asReaderWriter().putPersistentData(\n-          getContext().getZooKeeperRoot() + Constants.ZMANAGER_GOAL_STATE,\n+      getContext().getZooSession().asReaderWriter().putPersistentData(Constants.ZMANAGER_GOAL_STATE,\n           state.name().getBytes(UTF_8), NodeExistsPolicy.OVERWRITE);\n     } catch (Exception ex) {\n       log.error(\""Unable to set manager goal state in zookeeper\"");\n@@ -522,8 +521,8 @@ void setManagerGoalState(ManagerGoalState state) {\n   ManagerGoalState getManagerGoalState() {\n     while (true) {\n       try {\n-        byte[] data = getContext().getZooSession().asReaderWriter()\n-            .getData(getContext().getZooKeeperRoot() + Constants.ZMANAGER_GOAL_STATE);\n+        byte[] data =\n+            getContext().getZooSession().asReaderWriter().getData(Constants.ZMANAGER_GOAL_STATE);\n         return ManagerGoalState.valueOf(new String(data, UTF_8));\n       } catch (Exception e) {\n         log.error(\""Problem getting real goal state from zookeeper: \"", e);\n@@ -1138,7 +1137,6 @@ private List<TabletMigration> checkMigrationSanity(Set<TabletServerId> current,\n   @Override\n   public void run() {\n     final ServerContext context = getContext();\n-    final String zroot = context.getZooKeeperRoot();\n \n     // ACCUMULO-4424 Put up the Thrift servers before getting the lock as a sign of process health\n     // when a hot-standby\n@@ -1207,20 +1205,20 @@ public void run() {\n \n     ZooReaderWriter zReaderWriter = context.getZooSession().asReaderWriter();\n     try {\n-      zReaderWriter.getChildren(zroot + Constants.ZRECOVERY, new Watcher() {\n+      zReaderWriter.getChildren(Constants.ZRECOVERY, new Watcher() {\n         @Override\n         public void process(WatchedEvent event) {\n           nextEvent.event(\""Noticed recovery changes %s\"", event.getType());\n           try {\n             // watcher only fires once, add it back\n-            zReaderWriter.getChildren(zroot + Constants.ZRECOVERY, this);\n+            zReaderWriter.getChildren(Constants.ZRECOVERY, this);\n           } catch (Exception e) {\n             log.error(\""Failed to add log recovery watcher back\"", e);\n           }\n         }\n       });\n     } catch (KeeperException | InterruptedException e) {\n-      throw new IllegalStateException(\""Unable to read \"" + zroot + Constants.ZRECOVERY, e);\n+      throw new IllegalStateException(\""Unable to read \"" + Constants.ZRECOVERY, e);\n     }\n \n     MetricsInfo metricsInfo = getContext().getMetricsInfo();\n@@ -1355,8 +1353,7 @@ boolean canSuspendTablets() {\n       Predicate<ZooUtil.LockID> isLockHeld =\n           lock -> ServiceLock.isLockHeld(context.getZooCache(), lock);\n       var metaInstance = initializeFateInstance(context,\n-          new MetaFateStore<>(context.getZooKeeperRoot() + Constants.ZFATE, context.getZooSession(),\n-              managerLock.getLockID(), isLockHeld));\n+          new MetaFateStore<>(context.getZooSession(), managerLock.getLockID(), isLockHeld));\n       var userInstance = initializeFateInstance(context, new UserFateStore<>(context,\n           AccumuloTable.FATE.tableName(), managerLock.getLockID(), isLockHeld));\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java b/server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java\nindex 9da0c99408b..de7eac7ce0f 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/ManagerClientServiceHandler.java\n@@ -114,8 +114,7 @@ public long initiateFlush(TInfo tinfo, TCredentials c, String tableIdStr)\n       throw new ThriftSecurityException(c.getPrincipal(), SecurityErrorCode.PERMISSION_DENIED);\n     }\n \n-    String zTablePath = manager.getContext().getZooKeeperRoot() + Constants.ZTABLES + \""/\"" + tableId\n-        + Constants.ZTABLE_FLUSH_ID;\n+    String zTablePath = Constants.ZTABLES + \""/\"" + tableId + Constants.ZTABLE_FLUSH_ID;\n \n     ZooReaderWriter zoo = manager.getContext().getZooSession().asReaderWriter();\n     byte[] fid;\n@@ -264,9 +263,8 @@ public void modifyTableProperties(TInfo tinfo, TCredentials credentials, String\n     }\n \n     try {\n-      PropUtil.replaceProperties(manager.getContext(),\n-          TablePropKey.of(manager.getContext(), tableId), properties.getVersion(),\n-          properties.getProperties());\n+      PropUtil.replaceProperties(manager.getContext(), TablePropKey.of(tableId),\n+          properties.getVersion(), properties.getProperties());\n     } catch (ConcurrentModificationException cme) {\n       log.warn(\""Error modifying table properties, properties have changed\"", cme);\n       throw new ThriftConcurrentModificationException(cme.getMessage());\n@@ -467,9 +465,8 @@ public void modifyNamespaceProperties(TInfo tinfo, TCredentials credentials, Str\n     }\n \n     try {\n-      PropUtil.replaceProperties(manager.getContext(),\n-          NamespacePropKey.of(manager.getContext(), namespaceId), properties.getVersion(),\n-          properties.getProperties());\n+      PropUtil.replaceProperties(manager.getContext(), NamespacePropKey.of(namespaceId),\n+          properties.getVersion(), properties.getProperties());\n     } catch (ConcurrentModificationException cme) {\n       log.warn(\""Error modifying namespace properties, properties have changed\"", cme);\n       throw new ThriftConcurrentModificationException(cme.getMessage());\n@@ -506,11 +503,11 @@ private void alterNamespaceProperty(TCredentials c, String namespace, String pro\n \n     try {\n       if (value == null) {\n-        PropUtil.removeProperties(manager.getContext(),\n-            NamespacePropKey.of(manager.getContext(), namespaceId), List.of(property));\n+        PropUtil.removeProperties(manager.getContext(), NamespacePropKey.of(namespaceId),\n+            List.of(property));\n       } else {\n-        PropUtil.setProperties(manager.getContext(),\n-            NamespacePropKey.of(manager.getContext(), namespaceId), Map.of(property, value));\n+        PropUtil.setProperties(manager.getContext(), NamespacePropKey.of(namespaceId),\n+            Map.of(property, value));\n       }\n     } catch (IllegalStateException ex) {\n       // race condition on delete... namespace no longer exists? An undelying ZooKeeper.NoNode\n@@ -535,13 +532,13 @@ private void alterTableProperty(TCredentials c, String tableName, String propert\n \n     try {\n       if (op == TableOperation.REMOVE_PROPERTY) {\n-        PropUtil.removeProperties(manager.getContext(),\n-            TablePropKey.of(manager.getContext(), tableId), List.of(property));\n+        PropUtil.removeProperties(manager.getContext(), TablePropKey.of(tableId),\n+            List.of(property));\n       } else if (op == TableOperation.SET_PROPERTY) {\n         if (value == null || value.isEmpty()) {\n           value = \""\"";\n         }\n-        PropUtil.setProperties(manager.getContext(), TablePropKey.of(manager.getContext(), tableId),\n+        PropUtil.setProperties(manager.getContext(), TablePropKey.of(tableId),\n             Map.of(property, value));\n       }\n     } catch (IllegalStateException ex) {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java b/server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java\nindex 807c9aed1b0..dcf2b33d171 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/ManagerTime.java\n@@ -46,7 +46,6 @@\n public class ManagerTime {\n   private static final Logger log = LoggerFactory.getLogger(ManagerTime.class);\n \n-  private final String zPath;\n   private final ZooReaderWriter zk;\n   private final Manager manager;\n \n@@ -89,12 +88,11 @@ public class ManagerTime {\n   private final AtomicReference<Duration> skewAmount;\n \n   public ManagerTime(Manager manager, AccumuloConfiguration conf) throws IOException {\n-    this.zPath = manager.getContext().getZooKeeperRoot() + Constants.ZMANAGER_TICK;\n     this.zk = manager.getContext().getZooSession().asReaderWriter();\n     this.manager = manager;\n \n     try {\n-      zk.putPersistentData(zPath, \""0\"".getBytes(UTF_8), NodeExistsPolicy.SKIP);\n+      zk.putPersistentData(Constants.ZMANAGER_TICK, \""0\"".getBytes(UTF_8), NodeExistsPolicy.SKIP);\n       skewAmount = new AtomicReference<>(updateSkew(getZkTime()));\n     } catch (Exception ex) {\n       throw new IOException(\""Error updating manager time\"", ex);\n@@ -136,7 +134,7 @@ public void run() {\n       case UNLOAD_METADATA_TABLETS:\n       case UNLOAD_ROOT_TABLET:\n         try {\n-          zk.putPersistentData(zPath, serialize(fromSkew(skewAmount.get())),\n+          zk.putPersistentData(Constants.ZMANAGER_TICK, serialize(fromSkew(skewAmount.get())),\n               NodeExistsPolicy.OVERWRITE);\n         } catch (Exception ex) {\n           if (log.isDebugEnabled()) {\n@@ -147,7 +145,7 @@ public void run() {\n   }\n \n   private SteadyTime getZkTime() throws InterruptedException, KeeperException {\n-    return deserialize(zk.getData(zPath));\n+    return deserialize(zk.getData(Constants.ZMANAGER_TICK));\n   }\n \n   /**\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex 6854aec7136..a9d5d539ea2 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -27,7 +27,6 @@\n import java.util.Arrays;\n import java.util.Objects;\n import java.util.Optional;\n-import java.util.Set;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex 693fdb2437b..dd7782f79a1 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -46,6 +46,7 @@\n import java.util.function.UnaryOperator;\n import java.util.stream.Stream;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.Fate;\n@@ -77,18 +78,16 @@ public class MetaFateStore<T> extends AbstractFateStore<T> {\n \n   private static final Logger log = LoggerFactory.getLogger(MetaFateStore.class);\n   private static final FateInstanceType fateInstanceType = FateInstanceType.META;\n-  private String path;\n   private ZooSession zk;\n   private ZooReaderWriter zrw;\n \n   private String getTXPath(FateId fateId) {\n-    return path + \""/tx_\"" + fateId.getTxUUIDStr();\n+    return Constants.ZFATE + \""/tx_\"" + fateId.getTxUUIDStr();\n   }\n \n   /**\n    * Constructs a MetaFateStore\n    *\n-   * @param path the path in ZK where the fate data will reside\n    * @param zk the {@link ZooSession}\n    * @param lockID the {@link ZooUtil.LockID} held by the process creating this store. Should be\n    *        null if this store will be used as read-only (will not be used to reserve transactions)\n@@ -96,21 +95,20 @@ private String getTXPath(FateId fateId) {\n    *        time of invocation. If the store is used for a {@link Fate} which runs a dead\n    *        reservation cleaner, this should be non-null, otherwise null is fine\n    */\n-  public MetaFateStore(String path, ZooSession zk, ZooUtil.LockID lockID,\n-      Predicate<ZooUtil.LockID> isLockHeld) throws KeeperException, InterruptedException {\n-    this(path, zk, lockID, isLockHeld, DEFAULT_MAX_DEFERRED, DEFAULT_FATE_ID_GENERATOR);\n+  public MetaFateStore(ZooSession zk, ZooUtil.LockID lockID, Predicate<ZooUtil.LockID> isLockHeld)\n+      throws KeeperException, InterruptedException {\n+    this(zk, lockID, isLockHeld, DEFAULT_MAX_DEFERRED, DEFAULT_FATE_ID_GENERATOR);\n   }\n \n   @VisibleForTesting\n-  public MetaFateStore(String path, ZooSession zk, ZooUtil.LockID lockID,\n-      Predicate<ZooUtil.LockID> isLockHeld, int maxDeferred, FateIdGenerator fateIdGenerator)\n+  public MetaFateStore(ZooSession zk, ZooUtil.LockID lockID, Predicate<ZooUtil.LockID> isLockHeld,\n+      int maxDeferred, FateIdGenerator fateIdGenerator)\n       throws KeeperException, InterruptedException {\n     super(lockID, isLockHeld, maxDeferred, fateIdGenerator);\n-    this.path = path;\n     this.zk = zk;\n     this.zrw = zk.asReaderWriter();\n \n-    this.zrw.putPersistentData(path, new byte[0], NodeExistsPolicy.SKIP);\n+    this.zrw.putPersistentData(Constants.ZFATE, new byte[0], NodeExistsPolicy.SKIP);\n   }\n \n   @Override\n@@ -579,7 +577,7 @@ protected FateTxStore<T> newUnreservedFateTxStore(FateId fateId) {\n   @Override\n   protected Stream<FateIdStatus> getTransactions(EnumSet<TStatus> statuses) {\n     try {\n-      Stream<FateIdStatus> stream = zrw.getChildren(path).stream().map(strTxid -> {\n+      Stream<FateIdStatus> stream = zrw.getChildren(Constants.ZFATE).stream().map(strTxid -> {\n         String txUUIDStr = strTxid.split(\""_\"")[1];\n         FateId fateId = FateId.from(fateInstanceType, txUUIDStr);\n         // Memoizing for two reasons. First the status or reservation may never be requested, so\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java b/core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java\nindex 7e0470aae80..07487939b1e 100644\n--- a/core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/clientImpl/ZookeeperLockCheckerTest.java\n@@ -26,11 +26,8 @@\n import static org.easymock.EasyMock.reset;\n import static org.easymock.EasyMock.verify;\n \n-import java.util.UUID;\n import java.util.function.Predicate;\n \n-import org.apache.accumulo.core.data.InstanceId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooCache;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n@@ -43,10 +40,8 @@ public class ZookeeperLockCheckerTest {\n \n   @BeforeEach\n   public void setUp() {\n-    var instanceId = InstanceId.of(UUID.randomUUID());\n     zc = createMock(ZooCache.class);\n     context = createMock(ClientContext.class);\n-    expect(context.getZooKeeperRoot()).andReturn(ZooUtil.getRoot(instanceId)).anyTimes();\n     expect(context.getZooCache()).andReturn(zc).anyTimes();\n     replay(context, zc);\n   }\n@@ -58,7 +53,7 @@ public void tearDown() {\n \n   @Test\n   public void testInvalidateCache() {\n-    var zklc = new ZookeeperLockChecker(zc, context.getZooKeeperRoot());\n+    var zklc = new ZookeeperLockChecker(zc);\n \n     verify(zc);\n     reset(zc);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java b/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\nindex e68db784e63..d56b4b0993b 100644\n--- a/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/crypto/CryptoTest.java\n@@ -583,7 +583,7 @@ private void testOverlappingWrites(Scope scope) throws Exception {\n     assertTrue(ce.getMessage().contains(\""closing previous\""));\n \n     es1.close();\n-    var es2 = encrypter.encryptStream(out2);\n+    assertNotNull(encrypter.encryptStream(out2));\n   }\n \n   private ArrayList<Key> testData() {\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java b/core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java\nindex c58a6a363a6..80b81fe424e 100644\n--- a/core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/lock/ServiceLockPathsTest.java\n@@ -45,6 +45,7 @@\n import java.util.Set;\n import java.util.UUID;\n \n+import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.lock.ServiceLockData.ThriftService;\n import org.apache.accumulo.core.lock.ServiceLockPaths.AddressSelector;\n import org.apache.accumulo.core.lock.ServiceLockPaths.ServiceLockPath;\n@@ -59,7 +60,6 @@\n \n public class ServiceLockPathsTest {\n \n-  private static final String ROOT = \""/accumulo/instance_id\"";\n   private static final String TEST_RESOURCE_GROUP = \""TEST_RG\"";\n   private static final String HOSTNAME = \""localhost:9876\"";\n   private static final String HOSTNAME_NO_LOCK = \""localhost:9877\"";\n@@ -71,7 +71,7 @@ public class ServiceLockPathsTest {\n   @BeforeEach\n   public void setupMocks() {\n     zc = createMock(ZooCache.class);\n-    paths = new ServiceLockPaths(ROOT, zc);\n+    paths = new ServiceLockPaths(zc);\n   }\n \n   @AfterEach\n@@ -88,13 +88,13 @@ public void testPathGeneration() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZGC_LOCK, slp.getType());\n-    assertEquals(ROOT + ZGC_LOCK, slp.toString());\n+    assertEquals(ZGC_LOCK, slp.toString());\n \n     slp = paths.createManagerPath();\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMANAGER_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMANAGER_LOCK, slp.toString());\n+    assertEquals(ZMANAGER_LOCK, slp.toString());\n \n     assertThrows(NullPointerException.class, () -> paths.createMiniPath(null));\n     String miniUUID = UUID.randomUUID().toString();\n@@ -102,13 +102,13 @@ public void testPathGeneration() {\n     assertEquals(miniUUID, slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMINI_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n+    assertEquals(ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n \n     slp = paths.createMonitorPath();\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMONITOR_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMONITOR_LOCK, slp.toString());\n+    assertEquals(ZMONITOR_LOCK, slp.toString());\n \n     // Test worker process path creation\n     assertThrows(NullPointerException.class, () -> paths.createCompactorPath(null, null));\n@@ -118,7 +118,7 @@ public void testPathGeneration() {\n     assertEquals(HOSTNAME, slp.getServer());\n     assertEquals(TEST_RESOURCE_GROUP, slp.getResourceGroup());\n     assertEquals(ZCOMPACTORS, slp.getType());\n-    assertEquals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n+    assertEquals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n \n     assertThrows(NullPointerException.class, () -> paths.createDeadTabletServerPath(null, null));\n     assertThrows(NullPointerException.class,\n@@ -127,7 +127,7 @@ public void testPathGeneration() {\n     assertEquals(HOSTNAME, slp.getServer());\n     assertEquals(TEST_RESOURCE_GROUP, slp.getResourceGroup());\n     assertEquals(ZDEADTSERVERS, slp.getType());\n-    assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n+    assertEquals(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n \n     assertThrows(NullPointerException.class, () -> paths.createScanServerPath(null, null));\n     assertThrows(NullPointerException.class,\n@@ -136,7 +136,7 @@ public void testPathGeneration() {\n     assertEquals(HOSTNAME, slp.getServer());\n     assertEquals(TEST_RESOURCE_GROUP, slp.getResourceGroup());\n     assertEquals(ZSSERVERS, slp.getType());\n-    assertEquals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n+    assertEquals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n \n     assertThrows(NullPointerException.class, () -> paths.createTabletServerPath(null, null));\n     assertThrows(NullPointerException.class,\n@@ -145,12 +145,12 @@ public void testPathGeneration() {\n     assertEquals(HOSTNAME, slp.getServer());\n     assertEquals(TEST_RESOURCE_GROUP, slp.getResourceGroup());\n     assertEquals(ZTSERVERS, slp.getType());\n-    assertEquals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n+    assertEquals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp.toString());\n   }\n \n   @Test\n   public void testGetGarbageCollectorNotRunning() {\n-    expect(zc.getChildren(ROOT + ZGC_LOCK)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZGC_LOCK)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     var slp = paths.getGarbageCollector(true);\n@@ -159,7 +159,7 @@ public void testGetGarbageCollectorNotRunning() {\n \n   @Test\n   public void testGetGarbageCollectorNoLock() {\n-    expect(zc.getChildren(ROOT + ZGC_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n+    expect(zc.getChildren(ZGC_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n     replay(zc);\n \n     var slp = paths.getGarbageCollector(false);\n@@ -167,7 +167,7 @@ public void testGetGarbageCollectorNoLock() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZGC_LOCK, slp.getType());\n-    assertEquals(ROOT + ZGC_LOCK, slp.toString());\n+    assertEquals(ZGC_LOCK, slp.toString());\n   }\n \n   @Test\n@@ -177,8 +177,8 @@ public void testGetGarbageCollector() {\n     String svcLock2 = ServiceLock.ZLOCK_PREFIX + uuid.toString() + \""#0000000002\"";\n     var sld = new ServiceLockData(uuid, HOSTNAME, ThriftService.GC, TEST_RESOURCE_GROUP);\n \n-    expect(zc.getChildren(ROOT + ZGC_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.get(EasyMock.eq(ROOT + ZGC_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n+    expect(zc.getChildren(ZGC_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n+    expect(zc.get(EasyMock.eq(ZGC_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n         .andReturn(sld.serialize());\n     replay(zc);\n \n@@ -187,12 +187,12 @@ public void testGetGarbageCollector() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZGC_LOCK, slp.getType());\n-    assertEquals(ROOT + ZGC_LOCK, slp.toString());\n+    assertEquals(ZGC_LOCK, slp.toString());\n   }\n \n   @Test\n   public void testGetManagerNotRunning() {\n-    expect(zc.getChildren(ROOT + ZMANAGER_LOCK)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZMANAGER_LOCK)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     var slp = paths.getManager(true);\n@@ -201,7 +201,7 @@ public void testGetManagerNotRunning() {\n \n   @Test\n   public void testGetManagerNoLock() {\n-    expect(zc.getChildren(ROOT + ZMANAGER_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n+    expect(zc.getChildren(ZMANAGER_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n     replay(zc);\n \n     var slp = paths.getManager(false);\n@@ -209,7 +209,7 @@ public void testGetManagerNoLock() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMANAGER_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMANAGER_LOCK, slp.toString());\n+    assertEquals(ZMANAGER_LOCK, slp.toString());\n   }\n \n   @Test\n@@ -219,8 +219,8 @@ public void testGetManager() {\n     String svcLock2 = ServiceLock.ZLOCK_PREFIX + uuid.toString() + \""#0000000002\"";\n     var sld = new ServiceLockData(uuid, HOSTNAME, ThriftService.MANAGER, TEST_RESOURCE_GROUP);\n \n-    expect(zc.getChildren(ROOT + ZMANAGER_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.get(EasyMock.eq(ROOT + ZMANAGER_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n+    expect(zc.getChildren(ZMANAGER_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n+    expect(zc.get(EasyMock.eq(ZMANAGER_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n         .andReturn(sld.serialize());\n     replay(zc);\n \n@@ -229,12 +229,12 @@ public void testGetManager() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMANAGER_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMANAGER_LOCK, slp.toString());\n+    assertEquals(ZMANAGER_LOCK, slp.toString());\n   }\n \n   @Test\n   public void testGetMonitorNotRunning() {\n-    expect(zc.getChildren(ROOT + ZMONITOR_LOCK)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZMONITOR_LOCK)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     var slp = paths.getMonitor(true);\n@@ -243,7 +243,7 @@ public void testGetMonitorNotRunning() {\n \n   @Test\n   public void testGetMonitorNoLock() {\n-    expect(zc.getChildren(ROOT + ZMONITOR_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n+    expect(zc.getChildren(ZMONITOR_LOCK)).andReturn(List.of(HOSTNAME_NO_LOCK)).anyTimes();\n     replay(zc);\n \n     var slp = paths.getMonitor(false);\n@@ -251,7 +251,7 @@ public void testGetMonitorNoLock() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMONITOR_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMONITOR_LOCK, slp.toString());\n+    assertEquals(ZMONITOR_LOCK, slp.toString());\n   }\n \n   @Test\n@@ -261,8 +261,8 @@ public void testGetMonitor() {\n     String svcLock2 = ServiceLock.ZLOCK_PREFIX + uuid.toString() + \""#0000000002\"";\n     var sld = new ServiceLockData(uuid, HOSTNAME, ThriftService.NONE, TEST_RESOURCE_GROUP);\n \n-    expect(zc.getChildren(ROOT + ZMONITOR_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.get(EasyMock.eq(ROOT + ZMONITOR_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n+    expect(zc.getChildren(ZMONITOR_LOCK)).andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n+    expect(zc.get(EasyMock.eq(ZMONITOR_LOCK + \""/\"" + svcLock1), EasyMock.isA(ZcStat.class)))\n         .andReturn(sld.serialize());\n     replay(zc);\n \n@@ -271,12 +271,12 @@ public void testGetMonitor() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMONITOR_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMONITOR_LOCK, slp.toString());\n+    assertEquals(ZMONITOR_LOCK, slp.toString());\n   }\n \n   @Test\n   public void testGetCompactorsNotRunning() {\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZCOMPACTORS)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     assertThrows(NullPointerException.class, () -> paths.getCompactor(null, null, true));\n@@ -299,33 +299,32 @@ public void testGetCompactors() {\n     var sld2 =\n         new ServiceLockData(uuid, HOSTNAME, ThriftService.COMPACTOR, DEFAULT_RESOURCE_GROUP_NAME);\n \n-    expect(zc.getChildren(ROOT + ZCOMPACTORS))\n+    expect(zc.getChildren(ZCOMPACTORS))\n         .andReturn(List.of(TEST_RESOURCE_GROUP, DEFAULT_RESOURCE_GROUP_NAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc.getChildren(\n-        ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n     expect(zc.get(\n-        EasyMock\n-            .eq(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+        EasyMock.eq(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld1.serialize()).anyTimes();\n-    expect(zc.get(EasyMock.eq(\n-        ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+    expect(zc.get(\n+        EasyMock\n+            .eq(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld2.serialize()).anyTimes();\n \n-    expect(zc.getChildren(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n-        .andReturn(null).anyTimes();\n-    expect(zc.get(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n+    expect(zc.getChildren(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n         .andReturn(null).anyTimes();\n+    expect(zc.get(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\"")).andReturn(null)\n+        .anyTimes();\n     replay(zc);\n \n     // query for all\n@@ -336,13 +335,13 @@ public void testGetCompactors() {\n       assertTrue(path.getServer().equals(HOSTNAME) || path.getServer().equals(HOSTNAME_NO_LOCK));\n       assertTrue(path.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)\n           || path.getResourceGroup().equals(TEST_RESOURCE_GROUP));\n-      assertTrue(path.toString()\n-          .equals(ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n-          || path.toString().equals(\n-              ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n-          || path.toString().equals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n-          || path.toString()\n-              .equals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n+      assertTrue(\n+          path.toString().equals(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n+              || path.toString().equals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n     }\n \n     // query for all with locks\n@@ -353,10 +352,9 @@ public void testGetCompactors() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZCOMPACTORS, slp1.getType());\n     if (slp1.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-          slp1.toString());\n+      assertEquals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n     } else if (slp1.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n+      assertEquals(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n           slp1.toString());\n     } else {\n       fail(\""wrong resource group\"");\n@@ -365,10 +363,9 @@ public void testGetCompactors() {\n     assertEquals(HOSTNAME, slp2.getServer());\n     assertEquals(ZCOMPACTORS, slp2.getType());\n     if (slp2.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-          slp2.toString());\n+      assertEquals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n     } else if (slp2.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n+      assertEquals(ZCOMPACTORS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n           slp2.toString());\n     } else {\n       fail(\""wrong resource group\"");\n@@ -387,7 +384,7 @@ public void testGetCompactors() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZCOMPACTORS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a specific server\n     results =\n@@ -398,7 +395,7 @@ public void testGetCompactors() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZCOMPACTORS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZCOMPACTORS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a wrong server\n     for (boolean withLock : new boolean[] {true, false}) {\n@@ -413,7 +410,7 @@ public void testGetCompactors() {\n \n   @Test\n   public void testGetScanServersNotRunning() {\n-    expect(zc.getChildren(ROOT + ZSSERVERS)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZSSERVERS)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     assertThrows(NullPointerException.class, () -> paths.getScanServer(null, null, true));\n@@ -437,30 +434,30 @@ public void testGetScanServers() {\n     var sld2 =\n         new ServiceLockData(uuid, HOSTNAME, ThriftService.TABLET_SCAN, DEFAULT_RESOURCE_GROUP_NAME);\n \n-    expect(zc.getChildren(ROOT + ZSSERVERS))\n+    expect(zc.getChildren(ZSSERVERS))\n         .andReturn(List.of(TEST_RESOURCE_GROUP, DEFAULT_RESOURCE_GROUP_NAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc\n-        .getChildren(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n+    expect(\n+        zc.get(EasyMock.eq(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+            EasyMock.isA(ZcStat.class)))\n+        .andReturn(sld1.serialize()).anyTimes();\n     expect(zc.get(\n-        EasyMock.eq(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n-        EasyMock.isA(ZcStat.class))).andReturn(sld1.serialize()).anyTimes();\n-    expect(zc.get(\n-        EasyMock.eq(\n-            ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+        EasyMock\n+            .eq(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld2.serialize()).anyTimes();\n \n-    expect(zc.getChildren(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n+    expect(zc.getChildren(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n         .andReturn(null).anyTimes();\n     replay(zc);\n \n@@ -472,13 +469,13 @@ public void testGetScanServers() {\n       assertTrue(path.getServer().equals(HOSTNAME) || path.getServer().equals(HOSTNAME_NO_LOCK));\n       assertTrue(path.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)\n           || path.getResourceGroup().equals(TEST_RESOURCE_GROUP));\n-      assertTrue(path.toString()\n-          .equals(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n-          || path.toString()\n-              .equals(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n-          || path.toString().equals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n-          || path.toString()\n-              .equals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n+      assertTrue(\n+          path.toString().equals(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n+              || path.toString().equals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n     }\n \n     // query for all with lock\n@@ -489,10 +486,9 @@ public void testGetScanServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZSSERVERS, slp1.getType());\n     if (slp1.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+      assertEquals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n     } else if (slp1.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n-          slp1.toString());\n+      assertEquals(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME, slp1.toString());\n     } else {\n       fail(\""wrong resource group\"");\n     }\n@@ -500,10 +496,9 @@ public void testGetScanServers() {\n     assertEquals(HOSTNAME, slp2.getServer());\n     assertEquals(ZSSERVERS, slp2.getType());\n     if (slp2.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n+      assertEquals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n     } else if (slp2.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n-          slp2.toString());\n+      assertEquals(ZSSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME, slp2.toString());\n     } else {\n       fail(\""wrong resource group\"");\n     }\n@@ -522,7 +517,7 @@ public void testGetScanServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZSSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a specific server\n     results =\n@@ -533,7 +528,7 @@ public void testGetScanServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZSSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZSSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a wrong server\n     results = paths.getScanServer(rg -> rg.equals(TEST_RESOURCE_GROUP),\n@@ -543,7 +538,7 @@ public void testGetScanServers() {\n \n   @Test\n   public void testGetTabletServersNotRunning() {\n-    expect(zc.getChildren(ROOT + ZTSERVERS)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZTSERVERS)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     assertThrows(NullPointerException.class, () -> paths.getTabletServer(null, null, true));\n@@ -567,30 +562,30 @@ public void testGetTabletServers() {\n     var sld2 =\n         new ServiceLockData(uuid, HOSTNAME, ThriftService.TABLET_SCAN, DEFAULT_RESOURCE_GROUP_NAME);\n \n-    expect(zc.getChildren(ROOT + ZTSERVERS))\n+    expect(zc.getChildren(ZTSERVERS))\n         .andReturn(List.of(TEST_RESOURCE_GROUP, DEFAULT_RESOURCE_GROUP_NAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n         .andReturn(List.of(HOSTNAME, HOSTNAME_NO_LOCK)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc\n-        .getChildren(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK))\n         .andReturn(List.of()).anyTimes();\n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n+    expect(\n+        zc.get(EasyMock.eq(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+            EasyMock.isA(ZcStat.class)))\n+        .andReturn(sld1.serialize()).anyTimes();\n     expect(zc.get(\n-        EasyMock.eq(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n-        EasyMock.isA(ZcStat.class))).andReturn(sld1.serialize()).anyTimes();\n-    expect(zc.get(\n-        EasyMock.eq(\n-            ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+        EasyMock\n+            .eq(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld2.serialize()).anyTimes();\n \n-    expect(zc.getChildren(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n+    expect(zc.getChildren(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n         .andReturn(null).anyTimes();\n     replay(zc);\n \n@@ -602,13 +597,13 @@ public void testGetTabletServers() {\n       assertTrue(path.getServer().equals(HOSTNAME) || path.getServer().equals(HOSTNAME_NO_LOCK));\n       assertTrue(path.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)\n           || path.getResourceGroup().equals(TEST_RESOURCE_GROUP));\n-      assertTrue(path.toString()\n-          .equals(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n-          || path.toString()\n-              .equals(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n-          || path.toString().equals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n-          || path.toString()\n-              .equals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n+      assertTrue(\n+          path.toString().equals(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME_NO_LOCK)\n+              || path.toString().equals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME)\n+              || path.toString()\n+                  .equals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME_NO_LOCK));\n     }\n \n     // query for all with lock\n@@ -619,10 +614,9 @@ public void testGetTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZTSERVERS, slp1.getType());\n     if (slp1.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+      assertEquals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n     } else if (slp1.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n-          slp1.toString());\n+      assertEquals(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME, slp1.toString());\n     } else {\n       fail(\""wrong resource group\"");\n     }\n@@ -630,10 +624,9 @@ public void testGetTabletServers() {\n     assertEquals(HOSTNAME, slp2.getServer());\n     assertEquals(ZTSERVERS, slp2.getType());\n     if (slp2.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n+      assertEquals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n     } else if (slp2.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n-          slp2.toString());\n+      assertEquals(ZTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME, slp2.toString());\n     } else {\n       fail(\""wrong resource group\"");\n     }\n@@ -652,7 +645,7 @@ public void testGetTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZTSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a specific server\n     results = paths.getTabletServer(rg -> rg.equals(TEST_RESOURCE_GROUP), AddressSelector.exact(hp),\n@@ -663,7 +656,7 @@ public void testGetTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZTSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n+    assertEquals(ZTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a wrong server\n     results = paths.getTabletServer(rg -> rg.equals(TEST_RESOURCE_GROUP),\n@@ -673,7 +666,7 @@ public void testGetTabletServers() {\n \n   @Test\n   public void testGetDeadTabletServersNone() {\n-    expect(zc.getChildren(ROOT + ZDEADTSERVERS)).andReturn(List.of()).anyTimes();\n+    expect(zc.getChildren(ZDEADTSERVERS)).andReturn(List.of()).anyTimes();\n     replay(zc);\n \n     assertThrows(NullPointerException.class, () -> paths.getDeadTabletServer(null, null, false));\n@@ -697,29 +690,28 @@ public void testGetDeadTabletServers() {\n     var sld2 =\n         new ServiceLockData(uuid, HOSTNAME, ThriftService.TABLET_SCAN, DEFAULT_RESOURCE_GROUP_NAME);\n \n-    expect(zc.getChildren(ROOT + ZDEADTSERVERS))\n+    expect(zc.getChildren(ZDEADTSERVERS))\n         .andReturn(List.of(TEST_RESOURCE_GROUP, DEFAULT_RESOURCE_GROUP_NAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP))\n-        .andReturn(List.of(HOSTNAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n+    expect(zc.getChildren(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP)).andReturn(List.of(HOSTNAME))\n+        .anyTimes();\n+    expect(zc.getChildren(ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME))\n         .andReturn(List.of(HOSTNAME)).anyTimes();\n-    expect(zc.getChildren(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n-    expect(zc.get(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n+    expect(zc.get(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME))\n         .andReturn(new byte[0]).anyTimes();\n-    expect(\n-        zc.getChildren(ROOT + ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n+    expect(zc.getChildren(ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME))\n         .andReturn(List.of(svcLock1, svcLock2)).anyTimes();\n     expect(zc.get(\n-        EasyMock\n-            .eq(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+        EasyMock.eq(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld1.serialize()).anyTimes();\n-    expect(zc.get(EasyMock.eq(\n-        ROOT + ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n+    expect(zc.get(\n+        EasyMock.eq(\n+            ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME + \""/\"" + svcLock1),\n         EasyMock.isA(ZcStat.class))).andReturn(sld2.serialize()).anyTimes();\n \n-    expect(zc.get(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\""))\n-        .andReturn(null).anyTimes();\n+    expect(zc.get(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/localhost:1234\"")).andReturn(null)\n+        .anyTimes();\n     replay(zc);\n \n     // query for all\n@@ -731,10 +723,9 @@ public void testGetDeadTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZDEADTSERVERS, slp1.getType());\n     if (slp1.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-          slp1.toString());\n+      assertEquals(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n     } else if (slp1.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n+      assertEquals(ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n           slp1.toString());\n     } else {\n       fail(\""wrong resource group\"");\n@@ -743,10 +734,9 @@ public void testGetDeadTabletServers() {\n     assertEquals(HOSTNAME, slp2.getServer());\n     assertEquals(ZDEADTSERVERS, slp2.getType());\n     if (slp2.getResourceGroup().equals(TEST_RESOURCE_GROUP)) {\n-      assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-          slp2.toString());\n+      assertEquals(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp2.toString());\n     } else if (slp2.getResourceGroup().equals(DEFAULT_RESOURCE_GROUP_NAME)) {\n-      assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n+      assertEquals(ZDEADTSERVERS + \""/\"" + DEFAULT_RESOURCE_GROUP_NAME + \""/\"" + HOSTNAME,\n           slp2.toString());\n     } else {\n       fail(\""wrong resource group\"");\n@@ -766,8 +756,7 @@ public void testGetDeadTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZDEADTSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-        slp1.toString());\n+    assertEquals(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a specific server\n     results = paths.getDeadTabletServer(rg -> rg.equals(TEST_RESOURCE_GROUP),\n@@ -778,8 +767,7 @@ public void testGetDeadTabletServers() {\n     assertEquals(HOSTNAME, slp1.getServer());\n     assertEquals(ZDEADTSERVERS, slp1.getType());\n     assertEquals(TEST_RESOURCE_GROUP, slp1.getResourceGroup());\n-    assertEquals(ROOT + ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME,\n-        slp1.toString());\n+    assertEquals(ZDEADTSERVERS + \""/\"" + TEST_RESOURCE_GROUP + \""/\"" + HOSTNAME, slp1.toString());\n \n     // query for a wrong server\n     results = paths.getDeadTabletServer(rg -> rg.equals(TEST_RESOURCE_GROUP),\n@@ -797,21 +785,21 @@ public void testTableLocksPath() {\n     assertNull(slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZTABLE_LOCKS, slp.getType());\n-    assertEquals(ROOT + ZTABLE_LOCKS, slp.toString());\n+    assertEquals(ZTABLE_LOCKS, slp.toString());\n \n-    slp = paths.createTableLocksPath(\""1\"");\n+    slp = paths.createTableLocksPath(TableId.of(\""1\""));\n     assertEquals(\""1\"", slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZTABLE_LOCKS, slp.getType());\n-    assertEquals(ROOT + ZTABLE_LOCKS + \""/1\"", slp.toString());\n+    assertEquals(ZTABLE_LOCKS + \""/1\"", slp.toString());\n \n     // There is no get method\n \n     // Parsing is not supported\n     assertThrows(IllegalArgumentException.class,\n-        () -> ServiceLockPaths.parse(Optional.of(ZTABLE_LOCKS), ROOT + ZTABLE_LOCKS));\n+        () -> ServiceLockPaths.parse(Optional.of(ZTABLE_LOCKS), ZTABLE_LOCKS));\n     assertThrows(IllegalArgumentException.class,\n-        () -> ServiceLockPaths.parse(Optional.of(ZTABLE_LOCKS), ROOT + ZTABLE_LOCKS + \""/1\""));\n+        () -> ServiceLockPaths.parse(Optional.of(ZTABLE_LOCKS), ZTABLE_LOCKS + \""/1\""));\n   }\n \n   @Test\n@@ -827,17 +815,17 @@ public void testMiniPath() {\n     assertEquals(miniUUID, slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMINI_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n+    assertEquals(ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n \n     // There is no get method\n \n     // Parsing is not supported\n     assertThrows(IllegalArgumentException.class,\n-        () -> ServiceLockPaths.parse(Optional.of(ZMINI_LOCK), ROOT + ZMINI_LOCK));\n-    slp = ServiceLockPaths.parse(Optional.of(ZMINI_LOCK), ROOT + ZMINI_LOCK + \""/\"" + miniUUID);\n+        () -> ServiceLockPaths.parse(Optional.of(ZMINI_LOCK), ZMINI_LOCK));\n+    slp = ServiceLockPaths.parse(Optional.of(ZMINI_LOCK), ZMINI_LOCK + \""/\"" + miniUUID);\n     assertEquals(miniUUID, slp.getServer());\n     assertNull(slp.getResourceGroup());\n     assertEquals(ZMINI_LOCK, slp.getType());\n-    assertEquals(ROOT + ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n+    assertEquals(ZMINI_LOCK + \""/\"" + miniUUID, slp.toString());\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java b/core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java\nindex 2432f33a397..c822af8a646 100644\n--- a/core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/util/MonitorUtilTest.java\n@@ -43,7 +43,6 @@ public class MonitorUtilTest {\n   public void beforeEachTest() {\n     zr = mock(ZooReader.class);\n     context = mock(ClientContext.class);\n-    expect(context.getZooKeeperRoot()).andReturn(\""/root\"");\n   }\n \n   @AfterEach\n@@ -53,7 +52,7 @@ public void afterEachTest() {\n \n   @Test\n   public void testNodeFound() throws Exception {\n-    expect(zr.getData(\""/root\"" + Constants.ZMONITOR_HTTP_ADDR))\n+    expect(zr.getData(Constants.ZMONITOR_HTTP_ADDR))\n         .andReturn(\""http://example.org/\"".getBytes(UTF_8));\n     replay(zr, context);\n     assertEquals(\""http://example.org/\"", MonitorUtil.getLocation(zr, context));\n@@ -61,7 +60,7 @@ public void testNodeFound() throws Exception {\n \n   @Test\n   public void testNoNodeFound() throws Exception {\n-    expect(zr.getData(\""/root\"" + Constants.ZMONITOR_HTTP_ADDR)).andThrow(new NoNodeException());\n+    expect(zr.getData(Constants.ZMONITOR_HTTP_ADDR)).andThrow(new NoNodeException());\n     replay(zr, context);\n     assertNull(MonitorUtil.getLocation(zr, context));\n   }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java b/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java\nindex 9f7cbc31a8d..def7ac66258 100644\n--- a/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/zookeeper/ZooCacheTest.java\n@@ -76,8 +76,7 @@ long getZKClientObjectVersion() {\n \n   }\n \n-  private static final String root =\n-      Constants.ZROOT + \""/\"" + UUID.randomUUID().toString() + Constants.ZTSERVERS;\n+  private static final String root = Constants.ZROOT + \""/\"" + UUID.randomUUID().toString();\n   private static final String ZPATH = root + \""/testPath\"";\n   private static final byte[] DATA = {(byte) 1, (byte) 2, (byte) 3, (byte) 4};\n   private static final List<String> CHILDREN = java.util.Arrays.asList(\""huey\"", \""dewey\"", \""louie\"");\n@@ -94,25 +93,36 @@ public void setUp() {\n   @Test\n   @SuppressWarnings(\""unchecked\"")\n   public void testOverlappingPaths() throws Exception {\n-    expect(zk.getConnectionCounter()).andReturn(2L).times(2);\n+    expect(zk.getConnectionCounter()).andReturn(2L).times(7);\n     expect(zk.addPersistentRecursiveWatchers(isA(Set.class), isA(Watcher.class))).andReturn(3L);\n     replay(zk);\n-    assertThrows(IllegalArgumentException.class,\n-        () -> new ZooCache(zk, Set.of(root, root + \""/localhost:9995\"")));\n-\n-    Set<String> goodPaths = Set.of(\""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/compactors\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/dead/tservers\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/gc/lock\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/managers/lock\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/namespaces\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/recovery\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/root_tablet\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/sservers\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/tables\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/tservers\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/users\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/mini\"",\n-        \""/accumulo/8247eee6-a176-4e19-baf7-e3da965fe050/monitor/lock\"");\n+    var e = assertThrows(IllegalArgumentException.class,\n+        () -> new ZooCache(zk, Set.of(\""/test/localhost:9995\"", \""/test\"")));\n+    assertEquals(\n+        \""Overlapping paths found in paths to watch. '/test' contains '/test/localhost:9995'\"",\n+        e.getMessage());\n+    e = assertThrows(IllegalArgumentException.class,\n+        () -> new ZooCache(zk, Set.of(\""/test/localhost:9995\"", \""/test\"")));\n+    assertEquals(\n+        \""Overlapping paths found in paths to watch. '/test' contains '/test/localhost:9995'\"",\n+        e.getMessage());\n+    e = assertThrows(IllegalArgumentException.class, () -> new ZooCache(zk, Set.of(\""/test\"", \""/\"")));\n+    assertEquals(\""Overlapping paths found in paths to watch. '/' contains '/test'\"", e.getMessage());\n+    e = assertThrows(IllegalArgumentException.class,\n+        () -> new ZooCache(zk, Set.of(\""/test/\"", \""/test\"")));\n+    assertEquals(\""Watched path must not end with slash: '/test/'\"", e.getMessage());\n+    e = assertThrows(IllegalArgumentException.class,\n+        () -> new ZooCache(zk, Set.of(\""/test\"", \""/test/\"")));\n+    assertEquals(\""Watched path must not end with slash: '/test/'\"", e.getMessage());\n+    e = assertThrows(IllegalArgumentException.class,\n+        () -> new ZooCache(zk, Set.of(\""/test\"", \""test\"")));\n+    assertEquals(\""Watched path must start with slash: 'test'\"", e.getMessage());\n+\n+    // \""/co\"" is added because it is a prefix of /compactors, but being a prefix is not enough to\n+    // overlap; it needs to actually be a child of the other, not just have a similar name\n+    Set<String> goodPaths = Set.of(\""/co\"", \""/compactors\"", \""/dead/tservers\"", \""/gc/lock\"",\n+        \""/managers/lock\"", \""/namespaces\"", \""/recovery\"", \""/root_tablet\"", \""/sservers\"", \""/tables\"",\n+        \""/tservers\"", \""/users\"", \""/mini\"", \""/monitor/lock\"");\n     new ZooCache(zk, goodPaths);\n     verify(zk);\n   }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java b/server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java\nindex d01755d5c9e..f8fb60ec681 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/MockServerContext.java\n@@ -26,7 +26,6 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.server.conf.store.PropStore;\n import org.easymock.EasyMock;\n@@ -58,7 +57,6 @@ public static ServerContext getMockContextWithPropStore(final InstanceId instanc\n \n     ServerContext sc = createMock(ServerContext.class);\n     expect(sc.getInstanceID()).andReturn(instanceID).anyTimes();\n-    expect(sc.getZooKeeperRoot()).andReturn(ZooUtil.getRoot(instanceID)).anyTimes();\n     expect(sc.getPropStore()).andReturn(propStore).anyTimes();\n     return sc;\n   }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java\nindex 755df592459..ffec34fd93a 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/AccumuloConfigurationIsPropertySetTest.java\n@@ -86,7 +86,7 @@ public class AccumuloConfigurationIsPropertySetTest extends WithTestNames {\n       LoggerFactory.getLogger(AccumuloConfigurationIsPropertySetTest.class);\n   private static final InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n \n-  private final SystemPropKey sysPropKey = SystemPropKey.of(instanceId);\n+  private final SystemPropKey sysPropKey = SystemPropKey.of();\n   private final ArrayList<Object> mocks = new ArrayList<>();\n \n   private ZooPropStore propStore;\n@@ -162,7 +162,7 @@ public void testDefaultConfiguration() {\n   @Test\n   public void testNamespaceConfiguration() {\n     var namespaceId = NamespaceId.of(\""namespace\"");\n-    var nsPropKey = NamespacePropKey.of(instanceId, namespaceId);\n+    var nsPropKey = NamespacePropKey.of(namespaceId);\n \n     var setOnParent = Set.of(TABLE_BLOOM_SIZE);\n     var parent = new ConfigurationCopy(setToMap(setOnParent));\n@@ -256,10 +256,10 @@ public void testSystemConfiguration() {\n   @Test\n   public void testTableConfiguration() {\n     var namespaceId = NamespaceId.of(\""namespace\"");\n-    var nsPropKey = NamespacePropKey.of(instanceId, namespaceId);\n+    var nsPropKey = NamespacePropKey.of(namespaceId);\n \n     var tableId = TableId.of(\""3\"");\n-    var tablePropKey = TablePropKey.of(instanceId, tableId);\n+    var tablePropKey = TablePropKey.of(tableId);\n \n     var setOnNamespace = Set.of(TABLE_FILE_MAX);\n     var nsProps = new VersionedProperties(2, Instant.now(), setToMap(setOnNamespace));\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java\nindex 2b57617b38c..2c841513d52 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/NamespaceConfigurationTest.java\n@@ -73,7 +73,7 @@ public void setUp() {\n     parent = createMock(AccumuloConfiguration.class);\n     reset(propStore);\n \n-    var nsPropStoreKey = NamespacePropKey.of(iid, NSID);\n+    var nsPropStoreKey = NamespacePropKey.of(NSID);\n     expect(propStore.get(eq(nsPropStoreKey))).andReturn(new VersionedProperties(123, Instant.now(),\n         Map.of(Property.INSTANCE_SECRET.getKey(), \""sekrit\""))).anyTimes();\n     propStore.registerAsListener(eq(nsPropStoreKey), anyObject());\n@@ -118,7 +118,7 @@ public void testGet_InParent() {\n   @Test\n   public void testGet_SkipParentIfAccumuloNS() {\n     reset(propStore);\n-    var nsPropKey = NamespacePropKey.of(iid, Namespace.ACCUMULO.id());\n+    var nsPropKey = NamespacePropKey.of(Namespace.ACCUMULO.id());\n     expect(propStore.get(eq(nsPropKey))).andReturn(new VersionedProperties(Map.of(\""a\"", \""b\"")))\n         .anyTimes();\n     propStore.registerAsListener(eq(nsPropKey), anyObject());\n@@ -142,7 +142,7 @@ public void testGetProperties() {\n     replay(parent);\n     reset(propStore);\n \n-    var nsPropKey = NamespacePropKey.of(iid, NSID);\n+    var nsPropKey = NamespacePropKey.of(NSID);\n     expect(propStore.get(eq(nsPropKey)))\n         .andReturn(\n             new VersionedProperties(123, Instant.now(), Map.of(\""foo\"", \""bar\"", \""tick\"", \""tock\"")))\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java\nindex 2743b2a7f28..e3bebbe02e9 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/ServerConfigurationFactoryTest.java\n@@ -37,7 +37,6 @@\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.codec.VersionedProperties;\n import org.apache.accumulo.server.conf.store.NamespacePropKey;\n@@ -65,11 +64,11 @@ public class ServerConfigurationFactoryTest {\n   @BeforeEach\n   public void setUp() {\n     propStore = createMock(ZooPropStore.class);\n-    expect(propStore.get(eq(SystemPropKey.of(IID)))).andReturn(new VersionedProperties(Map.of()))\n+    expect(propStore.get(eq(SystemPropKey.of()))).andReturn(new VersionedProperties(Map.of()))\n         .anyTimes();\n-    expect(propStore.get(eq(TablePropKey.of(IID, TID))))\n-        .andReturn(new VersionedProperties(Map.of())).anyTimes();\n-    expect(propStore.get(eq(NamespacePropKey.of(IID, NSID))))\n+    expect(propStore.get(eq(TablePropKey.of(TID)))).andReturn(new VersionedProperties(Map.of()))\n+        .anyTimes();\n+    expect(propStore.get(eq(NamespacePropKey.of(NSID))))\n         .andReturn(new VersionedProperties(Map.of())).anyTimes();\n \n     propStore.registerAsListener(anyObject(), anyObject());\n@@ -80,7 +79,6 @@ public void setUp() {\n     expectLastCall().anyTimes();\n \n     context = createMock(ServerContext.class);\n-    expect(context.getZooKeeperRoot()).andReturn(ZooUtil.getRoot(IID)).anyTimes();\n     expect(context.getInstanceID()).andReturn(IID).anyTimes();\n     expect(context.getZooKeepers()).andReturn(ZK_HOST).anyTimes();\n     expect(context.getZooKeepersSessionTimeOut()).andReturn(ZK_TIMEOUT).anyTimes();\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java\nindex 7249231b0b6..f2a90dc711a 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/SystemConfigurationTest.java\n@@ -77,7 +77,7 @@ public void initMocks() {\n     propStore.registerAsListener(anyObject(), anyObject());\n     expectLastCall().anyTimes();\n \n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties sysProps =\n         new VersionedProperties(1, Instant.now(), Map.of(GC_PORT.getKey(), \""1234\"",\n             TSERV_SCAN_MAX_OPENFILES.getKey(), \""19\"", TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n@@ -104,7 +104,7 @@ public void testFromDefault() {\n \n   @Test\n   public void testFromFixed() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n \n     assertEquals(\""9997\"", sysConfig.get(TSERV_CLIENTPORT)); // default\n     assertEquals(\""1234\"", sysConfig.get(GC_PORT)); // fixed sys config\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java\nindex 9c606737730..73066064456 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/TableConfigurationTest.java\n@@ -87,17 +87,17 @@ public void initMocks() {\n     propStore.registerAsListener(anyObject(), anyObject());\n     expectLastCall().anyTimes();\n \n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties sysProps =\n         new VersionedProperties(1, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n     expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).times(2);\n \n-    var nsPropKey = NamespacePropKey.of(instanceId, NID);\n+    var nsPropKey = NamespacePropKey.of(NID);\n     VersionedProperties nsProps = new VersionedProperties(2, Instant.now(),\n         Map.of(TABLE_FILE_MAX.getKey(), \""21\"", TABLE_BLOOM_ENABLED.getKey(), \""false\""));\n     expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();\n \n-    var tablePropKey = TablePropKey.of(instanceId, TID);\n+    var tablePropKey = TablePropKey.of(TID);\n     VersionedProperties tableProps =\n         new VersionedProperties(3, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n     expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();\n@@ -134,7 +134,7 @@ public void testGet_InZK() {\n     Property p = Property.INSTANCE_SECRET;\n     reset(propStore);\n \n-    var propKey = TablePropKey.of(instanceId, TID);\n+    var propKey = TablePropKey.of(TID);\n     expect(propStore.get(eq(propKey)))\n         .andReturn(new VersionedProperties(37, Instant.now(), Map.of(p.getKey(), \""sekrit\"")))\n         .anyTimes();\n@@ -153,15 +153,15 @@ public void testGet_InParent() {\n     String expectedPass = \""aPassword1\"";\n \n     reset(propStore);\n-    expect(propStore.get(eq(NamespacePropKey.of(instanceId, NID))))\n-        .andReturn(new VersionedProperties(13, Instant.now(), Map.of(TABLE_FILE_MAX.getKey(), \""123\"",\n-            Property.INSTANCE_SECRET.getKey(), expectedPass)))\n+    expect(propStore.get(eq(NamespacePropKey.of(NID)))).andReturn(new VersionedProperties(13,\n+        Instant.now(),\n+        Map.of(TABLE_FILE_MAX.getKey(), \""123\"", Property.INSTANCE_SECRET.getKey(), expectedPass)))\n+        .anyTimes();\n+    expect(propStore.get(eq(TablePropKey.of(TID)))).andReturn(new VersionedProperties(Map.of()))\n         .anyTimes();\n-    expect(propStore.get(eq(TablePropKey.of(instanceId, TID))))\n-        .andReturn(new VersionedProperties(Map.of())).anyTimes();\n     replay(propStore);\n \n-    nsConfig.zkChangeEvent(NamespacePropKey.of(instanceId, NID));\n+    nsConfig.zkChangeEvent(NamespacePropKey.of(NID));\n \n     assertEquals(\""123\"", tableConfig.get(TABLE_FILE_MAX)); // from ns\n     assertEquals(\""aPassword1\"", tableConfig.get(INSTANCE_SECRET)); // from sys\n@@ -175,13 +175,13 @@ public void testGetProperties() {\n \n     reset(propStore);\n \n-    expect(propStore.get(eq(SystemPropKey.of(instanceId))))\n+    expect(propStore.get(eq(SystemPropKey.of())))\n         .andReturn(new VersionedProperties(1, Instant.now(), Map.of()));\n \n-    expect(propStore.get(eq(NamespacePropKey.of(instanceId, NID))))\n+    expect(propStore.get(eq(NamespacePropKey.of(NID))))\n         .andReturn(new VersionedProperties(2, Instant.now(), Map.of(\""dog\"", \""bark\"", \""cat\"", \""meow\"")));\n \n-    expect(propStore.get(eq(TablePropKey.of(instanceId, TID))))\n+    expect(propStore.get(eq(TablePropKey.of(TID))))\n         .andReturn(new VersionedProperties(4, Instant.now(), Map.of(\""foo\"", \""bar\"", \""tick\"", \""tock\"")))\n         .anyTimes();\n \n@@ -189,8 +189,8 @@ public void testGetProperties() {\n \n     Map<String,String> props = new java.util.HashMap<>();\n \n-    tableConfig.zkChangeEvent(TablePropKey.of(instanceId, TID));\n-    nsConfig.zkChangeEvent(NamespacePropKey.of(instanceId, NID));\n+    tableConfig.zkChangeEvent(TablePropKey.of(TID));\n+    nsConfig.zkChangeEvent(NamespacePropKey.of(NID));\n \n     tableConfig.getProperties(props, all);\n \n@@ -211,21 +211,20 @@ public void testGetFilteredProperties() {\n \n     reset(propStore);\n \n-    expect(propStore.get(eq(SystemPropKey.of(instanceId))))\n+    expect(propStore.get(eq(SystemPropKey.of())))\n         .andReturn(new VersionedProperties(1, Instant.now(), Map.of()));\n \n-    expect(propStore.get(eq(NamespacePropKey.of(instanceId, NID))))\n-        .andReturn(new VersionedProperties(2, Instant.now(),\n-            Map.of(\""dog\"", \""bark\"", \""cat\"", \""meow\"", \""filter\"", \""from_parent\"")));\n+    expect(propStore.get(eq(NamespacePropKey.of(NID)))).andReturn(new VersionedProperties(2,\n+        Instant.now(), Map.of(\""dog\"", \""bark\"", \""cat\"", \""meow\"", \""filter\"", \""from_parent\"")));\n \n-    expect(propStore.get(eq(TablePropKey.of(instanceId, TID)))).andReturn(new VersionedProperties(4,\n+    expect(propStore.get(eq(TablePropKey.of(TID)))).andReturn(new VersionedProperties(4,\n         Instant.now(), Map.of(\""filter\"", \""not_returned_by_table\"", \""foo\"", \""bar\"", \""tick\"", \""tock\"")))\n         .anyTimes();\n \n     replay(propStore);\n \n-    tableConfig.zkChangeEvent(TablePropKey.of(instanceId, TID));\n-    nsConfig.zkChangeEvent(NamespacePropKey.of(instanceId, NID));\n+    tableConfig.zkChangeEvent(TablePropKey.of(TID));\n+    nsConfig.zkChangeEvent(NamespacePropKey.of(NID));\n \n     Map<String,String> props = new java.util.HashMap<>();\n \n@@ -250,7 +249,7 @@ public void testInvalidateCache() {\n \n     reset(propStore);\n \n-    var propKey = TablePropKey.of(instanceId, TID);\n+    var propKey = TablePropKey.of(TID);\n \n     expect(propStore.get(eq(propKey)))\n         .andReturn(new VersionedProperties(23, Instant.now(), Map.of(p.getKey(), \""invalid\"")))\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java\nindex 78c5f708c8c..70f1e01b1d9 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/ZooBasedConfigurationTest.java\n@@ -96,7 +96,7 @@ public void defaultInitializationTest() {\n \n   @Test\n   public void defaultSysConfigTest() {\n-    var sysKey = SystemPropKey.of(instanceId);\n+    var sysKey = SystemPropKey.of();\n \n     var siteConfig = SiteConfiguration.empty().build();\n     expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();\n@@ -113,7 +113,7 @@ public void defaultSysConfigTest() {\n \n   @Test\n   public void get() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n \n     var siteConfig = SiteConfiguration.empty().build();\n     expect(context.getSiteConfiguration()).andReturn(siteConfig).anyTimes();\n@@ -134,7 +134,7 @@ public void get() {\n   @Test\n   public void getPropertiesTest() {\n     var tableId = TableId.of(\""t1\"");\n-    var tablePropKey = TablePropKey.of(instanceId, tableId);\n+    var tablePropKey = TablePropKey.of(tableId);\n \n     VersionedProperties tProps =\n         new VersionedProperties(Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n@@ -147,7 +147,7 @@ public void getPropertiesTest() {\n \n     VersionedProperties nProps =\n         new VersionedProperties(Map.of(TABLE_SPLIT_THRESHOLD.getKey(), \""3G\""));\n-    expect(propStore.get(eq(NamespacePropKey.of(instanceId, nsId)))).andReturn(nProps).once();\n+    expect(propStore.get(eq(NamespacePropKey.of(nsId)))).andReturn(nProps).once();\n \n     replay(context, propStore);\n \n@@ -173,7 +173,7 @@ public void getPropertiesTest() {\n \n   @Test\n   public void systemPropTest() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties vProps =\n         new VersionedProperties(99, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n     expect(propStore.get(eq(sysPropKey))).andReturn(vProps).once();\n@@ -190,7 +190,7 @@ public void systemPropTest() {\n \n   @Test\n   public void loadFailBecauseNodeNotExistTest() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n \n     expect(propStore.get(eq(sysPropKey))).andThrow(new IllegalStateException(\""fake no node\""))\n         .once();\n@@ -207,17 +207,17 @@ public void loadFailBecauseNodeNotExistTest() {\n    */\n   @Test\n   public void tablePropTest() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties sysProps =\n         new VersionedProperties(1, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n     expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).once();\n \n-    var nsPropKey = NamespacePropKey.of(instanceId, NamespaceId.of(\""ns1\""));\n+    var nsPropKey = NamespacePropKey.of(NamespaceId.of(\""ns1\""));\n     VersionedProperties nsProps =\n         new VersionedProperties(2, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""false\""));\n     expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""ns1.table1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""ns1.table1\""));\n     VersionedProperties tableProps =\n         new VersionedProperties(3, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n     expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();\n@@ -278,18 +278,18 @@ public void tablePropTest() {\n    */\n   @Test\n   public void updateCountTest() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties sysProps = new VersionedProperties(100, Instant.now(), Map.of());\n     expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).once();\n     expect(propStore.get(eq(sysPropKey))).andThrow(new IllegalStateException(\""fake no node\""))\n         .anyTimes();\n     // mock node deleted after event\n \n-    var nsPropKey = NamespacePropKey.of(instanceId, NamespaceId.of(\""ns1\""));\n+    var nsPropKey = NamespacePropKey.of(NamespaceId.of(\""ns1\""));\n     VersionedProperties nsProps = new VersionedProperties(20, Instant.now(), Map.of());\n     expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""ns1.table1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""ns1.table1\""));\n     VersionedProperties tableProps = new VersionedProperties(3, Instant.now(), Map.of());\n     expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();\n \n@@ -317,15 +317,15 @@ public void updateCountTest() {\n    */\n   @Test\n   public void updateCountTableTest() {\n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n     VersionedProperties sysProps = new VersionedProperties(100, Instant.now(), Map.of());\n     expect(propStore.get(eq(sysPropKey))).andReturn(sysProps).once();\n \n-    var nsPropKey = NamespacePropKey.of(instanceId, NamespaceId.of(\""ns1\""));\n+    var nsPropKey = NamespacePropKey.of(NamespaceId.of(\""ns1\""));\n     VersionedProperties nsProps = new VersionedProperties(20, Instant.now(), Map.of());\n     expect(propStore.get(eq(nsPropKey))).andReturn(nsProps).once();\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""ns1.table1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""ns1.table1\""));\n     VersionedProperties tableProps = new VersionedProperties(3, Instant.now(), Map.of());\n     expect(propStore.get(eq(tablePropKey))).andReturn(tableProps).once();\n \n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java\nindex 80abef3aa77..2a86a17e629 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/store/PropStoreKeyTest.java\n@@ -23,20 +23,15 @@\n import static org.apache.accumulo.core.Constants.ZROOT;\n import static org.apache.accumulo.core.Constants.ZTABLES;\n import static org.easymock.EasyMock.createMock;\n-import static org.easymock.EasyMock.expect;\n import static org.easymock.EasyMock.replay;\n import static org.easymock.EasyMock.verify;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertNull;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n-import java.util.UUID;\n-\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.server.ServerContext;\n import org.junit.jupiter.api.Test;\n import org.slf4j.Logger;\n@@ -45,11 +40,9 @@\n public class PropStoreKeyTest {\n   private static final Logger log = LoggerFactory.getLogger(PropStoreKeyTest.class);\n \n-  private final InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n-\n   @Test\n   public void systemType() {\n-    var propKey = SystemPropKey.of(instanceId);\n+    var propKey = SystemPropKey.of();\n     log.info(\""name: {}\"", propKey);\n     assertTrue(propKey.getPath().endsWith(ZCONFIG));\n   }\n@@ -57,10 +50,9 @@ public void systemType() {\n   @Test\n   public void systemTypeFromContext() {\n     ServerContext context = createMock(ServerContext.class);\n-    expect(context.getInstanceID()).andReturn(instanceId).once();\n     replay(context);\n \n-    var propKey = SystemPropKey.of(context);\n+    var propKey = SystemPropKey.of();\n     log.info(\""propKey: {}\"", propKey);\n     assertTrue(propKey.getPath().endsWith(ZCONFIG));\n     verify(context);\n@@ -68,7 +60,7 @@ public void systemTypeFromContext() {\n \n   @Test\n   public void namespaceType() {\n-    var propKey = NamespacePropKey.of(instanceId, NamespaceId.of(\""a\""));\n+    var propKey = NamespacePropKey.of(NamespaceId.of(\""a\""));\n     log.info(\""propKey: {}\"", propKey);\n     assertTrue(propKey.getPath().endsWith(ZCONFIG) && propKey.getPath().contains(ZNAMESPACES));\n     log.info(\""propKey: {}\"", propKey);\n@@ -77,17 +69,16 @@ public void namespaceType() {\n   @Test\n   public void namespaceTypeFromContext() {\n     ServerContext context = createMock(ServerContext.class);\n-    expect(context.getInstanceID()).andReturn(instanceId).once();\n     replay(context);\n \n-    var propKey = NamespacePropKey.of(context, NamespaceId.of(\""a\""));\n+    var propKey = NamespacePropKey.of(NamespaceId.of(\""a\""));\n     assertTrue(propKey.getPath().endsWith(ZCONFIG) && propKey.getPath().contains(ZNAMESPACES));\n     verify(context);\n   }\n \n   @Test\n   public void tableType() {\n-    var propKey = TablePropKey.of(instanceId, TableId.of(\""a\""));\n+    var propKey = TablePropKey.of(TableId.of(\""a\""));\n     log.info(\""propKey: {}\"", propKey);\n     assertTrue(propKey.getPath().endsWith(ZCONFIG) && propKey.getPath().contains(ZTABLES));\n     log.info(\""propKey: {}\"", propKey);\n@@ -95,19 +86,17 @@ public void tableType() {\n \n   @Test\n   public void fromPathTest() {\n-    var t1 = PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZTABLES + \""/t1\"" + ZCONFIG);\n-    assertNotNull(t1);\n-    assertEquals(TableId.of(\""t1\""), t1.getId());\n-\n-    var n1 = PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZNAMESPACES + \""/n1\"" + ZCONFIG);\n-    assertNotNull(n1);\n-    assertEquals(NamespaceId.of(\""n1\""), n1.getId());\n-    assertNotNull(n1.getId());\n-\n-    var s1 = PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZCONFIG);\n-    assertNotNull(s1);\n-    // system config returns instance id as id placeholder\n-    assertEquals(instanceId, s1.getId());\n+    var t1 = PropStoreKey.fromPath(ZTABLES + \""/t1\"" + ZCONFIG);\n+    assertTrue(t1 instanceof TablePropKey);\n+    assertEquals(TableId.of(\""t1\""), ((TablePropKey) t1).getId());\n+\n+    var n1 = PropStoreKey.fromPath(ZNAMESPACES + \""/n1\"" + ZCONFIG);\n+    assertTrue(n1 instanceof NamespacePropKey);\n+    assertEquals(NamespaceId.of(\""n1\""), ((NamespacePropKey) n1).getId());\n+\n+    var s1 = PropStoreKey.fromPath(ZCONFIG);\n+    assertFalse(s1 instanceof IdBasedPropStoreKey);\n+    assertTrue(s1 instanceof SystemPropKey);\n   }\n \n   @Test\n@@ -116,31 +105,25 @@ public void invalidKeysTest() {\n     assertNull(PropStoreKey.fromPath(ZROOT));\n \n     // not a system config\n-    assertTrue(\n-        PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZCONFIG) instanceof SystemPropKey);\n+    assertTrue(PropStoreKey.fromPath(ZCONFIG) instanceof SystemPropKey);\n     assertNull(PropStoreKey.fromPath(\""/foo\""));\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + \""/foo\""));\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZCONFIG + \""/foo\""));\n-\n-    assertTrue(PropStoreKey\n-        .fromPath(ZooUtil.getRoot(instanceId) + ZTABLES + \""/a\"" + ZCONFIG) instanceof TablePropKey);\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZTABLES + ZCONFIG));\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + \""/invalid/a\"" + ZCONFIG));\n-    assertNull(\n-        PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZTABLES + \""/a\"" + ZCONFIG + \""/foo\""));\n-\n-    assertTrue(PropStoreKey.fromPath(\n-        ZooUtil.getRoot(instanceId) + ZNAMESPACES + \""/a\"" + ZCONFIG) instanceof NamespacePropKey);\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZNAMESPACES + ZCONFIG));\n-    assertNull(PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + \""/invalid/a\"" + ZCONFIG));\n-    assertNull(\n-        PropStoreKey.fromPath(ZooUtil.getRoot(instanceId) + ZNAMESPACES + \""/a\"" + ZCONFIG + \""/foo\""));\n+    assertNull(PropStoreKey.fromPath(ZCONFIG + \""/foo\""));\n+\n+    assertTrue(PropStoreKey.fromPath(ZTABLES + \""/a\"" + ZCONFIG) instanceof TablePropKey);\n+    assertNull(PropStoreKey.fromPath(ZTABLES + ZCONFIG));\n+    assertNull(PropStoreKey.fromPath(\""/invalid/a\"" + ZCONFIG));\n+    assertNull(PropStoreKey.fromPath(ZTABLES + \""/a\"" + ZCONFIG + \""/foo\""));\n+\n+    assertTrue(PropStoreKey.fromPath(ZNAMESPACES + \""/a\"" + ZCONFIG) instanceof NamespacePropKey);\n+    assertNull(PropStoreKey.fromPath(ZNAMESPACES + ZCONFIG));\n+    assertNull(PropStoreKey.fromPath(\""/invalid/a\"" + ZCONFIG));\n+    assertNull(PropStoreKey.fromPath(ZNAMESPACES + \""/a\"" + ZCONFIG + \""/foo\""));\n   }\n \n   @Test\n   public void getBasePathTest() {\n-    assertTrue(SystemPropKey.of(instanceId).getPath().endsWith(\""/config\""));\n-    assertTrue(NamespacePropKey.of(instanceId, NamespaceId.of(\""123\"")).getPath().endsWith(ZCONFIG));\n-    assertTrue(TablePropKey.of(instanceId, TableId.of(\""456\"")).getPath().endsWith(ZCONFIG));\n+    assertTrue(SystemPropKey.of().getPath().endsWith(\""/config\""));\n+    assertTrue(NamespacePropKey.of(NamespaceId.of(\""123\"")).getPath().endsWith(ZCONFIG));\n+    assertTrue(TablePropKey.of(TableId.of(\""456\"")).getPath().endsWith(ZCONFIG));\n   }\n }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java\nindex 3789bb3f419..adde8c8b51e 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropCacheCaffeineImplTest.java\n@@ -66,8 +66,7 @@ public void init() {\n     ticker = new TestTicker();\n     instanceId = InstanceId.of(UUID.randomUUID());\n \n-    tablePropKey =\n-        TablePropKey.of(instanceId, TableId.of(\""t\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n+    tablePropKey = TablePropKey.of(TableId.of(\""t\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n \n     Map<String,String> props =\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\"");\n@@ -105,8 +104,8 @@ public void getTest() {\n       justification = \""random used for testing with variable names\"")\n   @Test\n   public void getNoCacheTest() {\n-    var table2PropKey = TablePropKey.of(instanceId,\n-        TableId.of(\""t2\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n+    var table2PropKey =\n+        TablePropKey.of(TableId.of(\""t2\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n \n     expect(zooPropLoader.load(eq(table2PropKey))).andReturn(vProps).once();\n \n@@ -131,8 +130,8 @@ public void removeTest() {\n       justification = \""random used for testing with variable names\"")\n   @Test\n   public void removeAllTest() {\n-    var table2PropKey = TablePropKey.of(instanceId,\n-        TableId.of(\""t2\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n+    var table2PropKey =\n+        TablePropKey.of(TableId.of(\""t2\"" + ThreadLocalRandom.current().nextInt(1, 1000)));\n \n     expect(zooPropLoader.load(eq(tablePropKey))).andReturn(vProps).once();\n     expect(zooPropLoader.load(eq(table2PropKey))).andReturn(vProps).once();\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java\nindex 99c4b04bacf..47ce602499f 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/PropStoreEventTest.java\n@@ -34,14 +34,10 @@\n \n import java.time.Instant;\n import java.util.Map;\n-import java.util.UUID;\n \n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n-import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.codec.VersionedPropCodec;\n import org.apache.accumulo.server.conf.codec.VersionedProperties;\n import org.apache.accumulo.server.conf.store.PropChangeListener;\n@@ -58,26 +54,19 @@\n \n public class PropStoreEventTest {\n   private final VersionedPropCodec propCodec = VersionedPropCodec.getDefault();\n-  private InstanceId instanceId;\n \n   // mocks\n-  private ServerContext context;\n   private ZooSession zk;\n   private ZooReaderWriter zrw;\n   private ReadyMonitor readyMonitor;\n \n   @BeforeEach\n   public void initCommonMocks() throws Exception {\n-    instanceId = InstanceId.of(UUID.randomUUID());\n-    context = createMock(ServerContext.class);\n     zk = createMock(ZooSession.class);\n     zrw = createMock(ZooReaderWriter.class);\n     expect(zk.asReaderWriter()).andReturn(zrw).anyTimes();\n-    expect(context.getZooSession()).andReturn(zk).anyTimes();\n-    expect(context.getZooKeepersSessionTimeOut()).andReturn(500).anyTimes();\n-    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();\n \n-    expect(zrw.exists(eq(ZooUtil.getRoot(instanceId)), anyObject())).andReturn(true).anyTimes();\n+    expect(zrw.exists(eq(\""/\""), anyObject())).andReturn(true).anyTimes();\n \n     readyMonitor = createMock(ReadyMonitor.class);\n \n@@ -85,13 +74,13 @@ public void initCommonMocks() throws Exception {\n \n   @AfterEach\n   public void verifyMocks() {\n-    verify(context, zk, zrw, readyMonitor);\n+    verify(zk, zrw, readyMonitor);\n   }\n \n   @Test\n   public void zkChangeEventTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n \n@@ -101,9 +90,9 @@ public void zkChangeEventTest() throws Exception {\n     readyMonitor.setReady();\n     expectLastCall().once();\n \n-    replay(context, zk, zrw, readyMonitor, zkEvent);\n+    replay(zk, zrw, readyMonitor, zkEvent);\n \n-    PropStore propStore = new ZooPropStore(instanceId, zk, readyMonitor, watcher, null);\n+    PropStore propStore = new ZooPropStore(zk, readyMonitor, watcher, null);\n     StoreTestListener listener = new StoreTestListener();\n \n     propStore.registerAsListener(tablePropKey, listener);\n@@ -120,7 +109,7 @@ public void zkChangeEventTest() throws Exception {\n   @Test\n   public void deleteEventTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n \n@@ -131,9 +120,9 @@ public void deleteEventTest() throws Exception {\n     readyMonitor.setReady();\n     expectLastCall().once();\n \n-    replay(context, zk, zrw, readyMonitor, zkEvent);\n+    replay(zk, zrw, readyMonitor, zkEvent);\n \n-    PropStore propStore = new ZooPropStore(instanceId, zk, readyMonitor, watcher, null);\n+    PropStore propStore = new ZooPropStore(zk, readyMonitor, watcher, null);\n \n     StoreTestListener listener = new StoreTestListener();\n \n@@ -150,7 +139,7 @@ public void deleteEventTest() throws Exception {\n   @Test\n   public void disconnectEventTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n \n@@ -163,9 +152,9 @@ public void disconnectEventTest() throws Exception {\n     readyMonitor.clearReady();\n     expectLastCall();\n \n-    replay(context, zk, zrw, readyMonitor, zkEvent);\n+    replay(zk, zrw, readyMonitor, zkEvent);\n \n-    PropStore propStore = new ZooPropStore(instanceId, zk, readyMonitor, watcher, null);\n+    PropStore propStore = new ZooPropStore(zk, readyMonitor, watcher, null);\n \n     StoreTestListener listener = new StoreTestListener();\n \n@@ -182,7 +171,7 @@ public void disconnectEventTest() throws Exception {\n   @Test\n   public void closedEventTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n \n@@ -197,9 +186,9 @@ public void closedEventTest() throws Exception {\n     readyMonitor.setClosed();\n     expectLastCall();\n \n-    replay(context, zk, zrw, readyMonitor, zkEvent);\n+    replay(zk, zrw, readyMonitor, zkEvent);\n \n-    PropStore propStore = new ZooPropStore(instanceId, zk, readyMonitor, watcher, null);\n+    PropStore propStore = new ZooPropStore(zk, readyMonitor, watcher, null);\n \n     StoreTestListener listener = new StoreTestListener();\n \n@@ -215,15 +204,15 @@ public void closedEventTest() throws Exception {\n   @Test\n   public void cacheChangeEventTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n     readyMonitor.setReady();\n     expectLastCall().once();\n \n-    replay(context, zk, zrw, readyMonitor);\n+    replay(zk, zrw, readyMonitor);\n \n-    PropStore propStore = new ZooPropStore(instanceId, zk, readyMonitor, watcher, null);\n+    PropStore propStore = new ZooPropStore(zk, readyMonitor, watcher, null);\n \n     StoreTestListener listener = new StoreTestListener();\n \n@@ -237,7 +226,7 @@ public void cacheChangeEventTest() throws Exception {\n \n   @Test\n   public void validateWatcherSetTest() throws Exception {\n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""a1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""a1\""));\n \n     Map<String,String> props1 =\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\"");\n@@ -258,7 +247,7 @@ public void validateWatcherSetTest() throws Exception {\n \n     PropStoreWatcher watcher = new PropStoreWatcher(readyMonitor);\n \n-    replay(context, zk, zrw, readyMonitor);\n+    replay(zk, zrw, readyMonitor);\n \n     ZooPropLoader loader = new ZooPropLoader(zk, propCodec, watcher);\n \n@@ -283,17 +272,17 @@ private static class StoreTestListener implements PropChangeListener {\n     private int connectionEventCount = 0;\n \n     @Override\n-    public void zkChangeEvent(PropStoreKey<?> propStoreKey) {\n+    public void zkChangeEvent(PropStoreKey propStoreKey) {\n       zkChangeEventCount++;\n     }\n \n     @Override\n-    public void cacheChangeEvent(PropStoreKey<?> propStoreKey) {\n+    public void cacheChangeEvent(PropStoreKey propStoreKey) {\n       cacheChangeEventCount++;\n     }\n \n     @Override\n-    public void deleteEvent(PropStoreKey<?> propStoreKey) {\n+    public void deleteEvent(PropStoreKey propStoreKey) {\n       deleteEventCount++;\n     }\n \n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java\nindex 333535207ae..d6f8085ff70 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropLoaderTest.java\n@@ -39,10 +39,8 @@\n import java.time.Instant;\n import java.util.HashMap;\n import java.util.Map;\n-import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n \n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n@@ -61,9 +59,8 @@\n public class ZooPropLoaderTest {\n \n   private PropCacheCaffeineImplTest.TestTicker ticker;\n-  private InstanceId instanceId;\n   private ZooSession zk;\n-  private PropStoreKey<?> propStoreKey;\n+  private PropStoreKey propStoreKey;\n   private VersionedPropCodec propCodec;\n \n   // mocks\n@@ -75,9 +72,8 @@ public class ZooPropLoaderTest {\n   @BeforeEach\n   public void initCommonMocks() {\n     ticker = new PropCacheCaffeineImplTest.TestTicker();\n-    instanceId = InstanceId.of(UUID.randomUUID());\n \n-    propStoreKey = TablePropKey.of(instanceId, TableId.of(\""1abc\""));\n+    propStoreKey = TablePropKey.of(TableId.of(\""1abc\""));\n     propCodec = VersionedPropCodec.getDefault();\n \n     // mocks\n@@ -286,8 +282,8 @@ public void getIfCachedTest() {\n \n   @Test\n   public void removeTest() throws Exception {\n-    final var sysPropKey = SystemPropKey.of(instanceId);\n-    final var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""t1\""));\n+    final var sysPropKey = SystemPropKey.of();\n+    final var tablePropKey = TablePropKey.of(TableId.of(\""t1\""));\n \n     final VersionedProperties defaultProps = new VersionedProperties();\n     final byte[] bytes = propCodec.toBytes(defaultProps);\n@@ -334,8 +330,8 @@ public void removeTest() throws Exception {\n \n   @Test\n   public void removeAllTest() throws Exception {\n-    final var sysPropKey = SystemPropKey.of(instanceId);\n-    final var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""t1\""));\n+    final var sysPropKey = SystemPropKey.of();\n+    final var tablePropKey = TablePropKey.of(TableId.of(\""t1\""));\n \n     final VersionedProperties defaultProps = new VersionedProperties();\n     final byte[] bytes = propCodec.toBytes(defaultProps);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java\nindex c91e4e8b0bf..3257ffa0c26 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/store/impl/ZooPropStoreTest.java\n@@ -39,15 +39,11 @@\n import java.time.Instant;\n import java.util.Map;\n import java.util.Set;\n-import java.util.UUID;\n \n import org.apache.accumulo.core.conf.Property;\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n-import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.server.conf.codec.VersionedPropCodec;\n import org.apache.accumulo.server.conf.codec.VersionedProperties;\n import org.apache.accumulo.server.conf.store.PropStore;\n@@ -62,44 +58,38 @@\n public class ZooPropStoreTest {\n \n   private final VersionedPropCodec propCodec = VersionedPropCodec.getDefault();\n-  private InstanceId instanceId;\n \n   // mocks\n-  private ServerContext context;\n   private ZooSession zk;\n   private ZooReaderWriter zrw;\n \n   @BeforeEach\n   public void init() throws Exception {\n-    instanceId = InstanceId.of(UUID.randomUUID());\n-    context = createMock(ServerContext.class);\n     zk = createMock(ZooSession.class);\n     zrw = createMock(ZooReaderWriter.class);\n-    expect(context.getZooSession()).andReturn(zk).anyTimes();\n     expect(zk.asReaderWriter()).andReturn(zrw).anyTimes();\n     expect(zk.getSessionTimeout()).andReturn(2_000).anyTimes();\n-    expect(context.getInstanceID()).andReturn(instanceId).anyTimes();\n \n-    expect(zrw.exists(eq(ZooUtil.getRoot(instanceId)), anyObject())).andReturn(true).anyTimes();\n+    expect(zrw.exists(eq(\""/\""), anyObject())).andReturn(true).anyTimes();\n   }\n \n   @AfterEach\n   public void verifyMock() {\n-    verify(context, zk, zrw);\n+    verify(zk, zrw);\n   }\n \n   @Test\n   public void create() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""propStoreKey\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""propStoreKey\""));\n \n     Capture<byte[]> bytes = newCapture();\n     expect(zrw.putPrivatePersistentData(eq(propStoreKey.getPath()), capture(bytes), anyObject()))\n         .andReturn(true).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     propStore.create(propStoreKey,\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\""));\n@@ -119,7 +109,7 @@ public void create() throws Exception {\n   @Test\n   public void getTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""propStoreKey\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""propStoreKey\""));\n \n     var vProps = new VersionedProperties(Map.of(Property.TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n \n@@ -136,9 +126,9 @@ public void getTest() throws Exception {\n           return propCodec.toBytes(vProps);\n         }).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     assertNotNull(propStore.get(propStoreKey)); // first call will fetch from ZooKeeper\n     assertNotNull(propStore.get(propStoreKey)); // next call will fetch from cache.\n@@ -151,7 +141,7 @@ public void getTest() throws Exception {\n   @Test\n   public void versionTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""table1\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""table1\""));\n     Map<String,String> props =\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\"");\n \n@@ -171,9 +161,9 @@ public void versionTest() throws Exception {\n           return propCodec.toBytes(new VersionedProperties(props));\n         }).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n     var vProps = propStore.get(propStoreKey);\n     assertNotNull(vProps);\n     assertEquals(expectedVersion, vProps.getDataVersion());\n@@ -187,7 +177,7 @@ public void versionTest() throws Exception {\n   @Test\n   public void putAllTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""table1\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""table1\""));\n \n     var initialProps = new VersionedProperties(0, Instant.now(),\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\""));\n@@ -219,9 +209,9 @@ public void putAllTest() throws Exception {\n           return true;\n         }).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     Map<String,String> updateProps =\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""4321\"", TABLE_SPLIT_THRESHOLD.getKey(), \""123M\"");\n@@ -232,7 +222,7 @@ public void putAllTest() throws Exception {\n   @Test\n   public void removeTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""table1\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""table1\""));\n \n     var initialProps = new VersionedProperties(123, Instant.now(),\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\""));\n@@ -262,9 +252,9 @@ public void removeTest() throws Exception {\n           return true;\n         }).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     Set<String> deleteNames =\n         Set.of(TABLE_BULK_MAX_TABLETS.getKey(), TABLE_SPLIT_THRESHOLD.getKey());\n@@ -275,7 +265,7 @@ public void removeTest() throws Exception {\n   @Test\n   public void removeWithExceptionsTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""table1\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""table1\""));\n \n     // return \""bad data\""\n     Capture<Stat> stat = newCapture();\n@@ -296,9 +286,9 @@ public void removeWithExceptionsTest() throws Exception {\n     expect(zrw.getData(eq(propStoreKey.getPath()), anyObject(Stat.class)))\n         .andThrow(new InterruptedException(\""mock forced interrupt exception\"")).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     Set<String> deleteNames =\n         Set.of(TABLE_BULK_MAX_TABLETS.getKey(), TABLE_SPLIT_THRESHOLD.getKey());\n@@ -317,7 +307,7 @@ public void removeWithExceptionsTest() throws Exception {\n   @Test\n   public void validateWatcherSetTest() throws Exception {\n \n-    var tablePropKey = TablePropKey.of(instanceId, TableId.of(\""table1\""));\n+    var tablePropKey = TablePropKey.of(TableId.of(\""table1\""));\n     Map<String,String> props =\n         Map.of(TABLE_BULK_MAX_TABLETS.getKey(), \""1234\"", TABLE_FILE_BLOCK_SIZE.getKey(), \""512M\"");\n \n@@ -346,12 +336,12 @@ public void validateWatcherSetTest() throws Exception {\n           return propCodec.toBytes(new VersionedProperties(13, Instant.now(), props));\n         }).once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n     ReadyMonitor monitor = new TestReadyMonitor(\""testmon\"", 2000);\n     PropStoreWatcher watcher = new TestWatcher(monitor);\n \n-    ZooPropStore propStore = new ZooPropStore(instanceId, zk, monitor, watcher, null);\n+    ZooPropStore propStore = new ZooPropStore(zk, monitor, watcher, null);\n \n     assertNotNull(propStore.get(tablePropKey));\n \n@@ -379,7 +369,7 @@ public TestReadyMonitor(String resourceName, long timeout) {\n     }\n   }\n \n-  private static class TestWatcher extends PropStoreWatcher {\n+  private class TestWatcher extends PropStoreWatcher {\n     public TestWatcher(ReadyMonitor zkReadyMonitor) {\n       super(zkReadyMonitor);\n     }\n@@ -388,7 +378,7 @@ public TestWatcher(ReadyMonitor zkReadyMonitor) {\n   @Test\n   public void deleteTest() throws Exception {\n \n-    var propStoreKey = TablePropKey.of(instanceId, TableId.of(\""propStoreKey\""));\n+    var propStoreKey = TablePropKey.of(TableId.of(\""propStoreKey\""));\n \n     var vProps = new VersionedProperties(Map.of(Property.TABLE_BLOOM_ENABLED.getKey(), \""true\""));\n \n@@ -409,9 +399,9 @@ public void deleteTest() throws Exception {\n     zrw.delete(eq(propStoreKey.getPath()));\n     expectLastCall().once();\n \n-    replay(context, zk, zrw);\n+    replay(zk, zrw);\n \n-    PropStore propStore = ZooPropStore.initialize(instanceId, zk);\n+    PropStore propStore = ZooPropStore.initialize(zk);\n \n     assertNotNull(propStore.get(propStoreKey)); // first call will fetch from ZooKeeper\n     assertNotNull(propStore.get(propStoreKey)); // next call will fetch from cache.\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java\nindex 155db98260e..af420570b0b 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/util/PropSnapshotTest.java\n@@ -32,9 +32,7 @@\n \n import java.time.Instant;\n import java.util.Map;\n-import java.util.UUID;\n \n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.server.conf.codec.VersionedProperties;\n import org.apache.accumulo.server.conf.store.PropStore;\n import org.apache.accumulo.server.conf.store.SystemPropKey;\n@@ -45,12 +43,10 @@\n \n class PropSnapshotTest {\n \n-  private InstanceId instanceId;\n   private PropStore propStore;\n \n   @BeforeEach\n   public void init() {\n-    instanceId = InstanceId.of(UUID.randomUUID());\n     propStore = createMock(ZooPropStore.class);\n     propStore.registerAsListener(anyObject(), anyObject());\n     expectLastCall().anyTimes();\n@@ -64,15 +60,15 @@ public void verifyMocks() {\n   @Test\n   public void getTest() {\n     // init props\n-    expect(propStore.get(eq(SystemPropKey.of(instanceId))))\n+    expect(propStore.get(eq(SystemPropKey.of())))\n         .andReturn(new VersionedProperties(123, Instant.now(), Map.of(\""k1\"", \""v1\"", \""k2\"", \""v2\"")))\n         .once();\n     // after update\n-    expect(propStore.get(eq(SystemPropKey.of(instanceId))))\n+    expect(propStore.get(eq(SystemPropKey.of())))\n         .andReturn(new VersionedProperties(124, Instant.now(), Map.of(\""k3\"", \""v3\""))).once();\n \n     replay(propStore);\n-    PropSnapshot snapshot = PropSnapshot.create(SystemPropKey.of(instanceId), propStore);\n+    PropSnapshot snapshot = PropSnapshot.create(SystemPropKey.of(), propStore);\n \n     assertEquals(\""v1\"", snapshot.getVersionedProperties().asMap().get(\""k1\""));\n     assertEquals(\""v2\"", snapshot.getVersionedProperties().asMap().get(\""k2\""));\n@@ -88,7 +84,7 @@ public void getTest() {\n   @Test\n   public void eventChangeTest() {\n \n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n \n     expect(propStore.get(eq(sysPropKey))).andReturn(\n         new VersionedProperties(99, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\"")))\n@@ -112,7 +108,7 @@ public void eventChangeTest() {\n   @Test\n   public void deleteEventTest() {\n \n-    var sysPropKey = SystemPropKey.of(instanceId);\n+    var sysPropKey = SystemPropKey.of();\n \n     expect(propStore.get(eq(sysPropKey))).andReturn(\n         new VersionedProperties(123, Instant.now(), Map.of(TABLE_BLOOM_ENABLED.getKey(), \""true\"")))\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java b/server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java\nindex bbaed9f4bf0..fc0ee95f74b 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/conf/util/ZooInfoViewerTest.java\n@@ -48,7 +48,6 @@\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.server.MockServerContext;\n import org.apache.accumulo.server.ServerContext;\n@@ -237,8 +236,9 @@ public void propTest() throws Exception {\n     var sysPropBytes = propCodec\n         .toBytes(new VersionedProperties(123, Instant.now(), Map.of(\""s1\"", \""sv1\"", \""s2\"", \""sv2\"")));\n     Capture<Stat> sStat = newCapture();\n-    expect(zk.getData(eq(SystemPropKey.of(iid).getPath()), isA(PropStoreWatcher.class),\n-        capture(sStat))).andAnswer(() -> {\n+    expect(\n+        zk.getData(eq(SystemPropKey.of().getPath()), isA(PropStoreWatcher.class), capture(sStat)))\n+        .andAnswer(() -> {\n           Stat s = sStat.getValue();\n           s.setCtime(System.currentTimeMillis());\n           s.setMtime(System.currentTimeMillis());\n@@ -255,7 +255,7 @@ public void propTest() throws Exception {\n         propCodec.toBytes(new VersionedProperties(123, Instant.now(), Map.of(\""n1\"", \""nv1\"")));\n     NamespaceId nsId = NamespaceId.of(\""a\"");\n     Capture<Stat> nsStat = newCapture();\n-    expect(zk.getData(eq(NamespacePropKey.of(iid, nsId).getPath()), isA(PropStoreWatcher.class),\n+    expect(zk.getData(eq(NamespacePropKey.of(nsId).getPath()), isA(PropStoreWatcher.class),\n         capture(nsStat))).andAnswer(() -> {\n           Stat s = nsStat.getValue();\n           s.setCtime(System.currentTimeMillis());\n@@ -269,14 +269,13 @@ public void propTest() throws Exception {\n     var mockTableIdMap = Map.of(TableId.of(\""t\""), \""t_table\"");\n     expect(context.getTableIdToNameMap()).andReturn(mockTableIdMap).once();\n \n-    var tBasePath = ZooUtil.getRoot(iid) + ZTABLES;\n-\n     var tProps = new VersionedProperties(123, Instant.now(), Map.of(\""t1\"", \""tv1\""));\n     var tPropBytes = propCodec.toBytes(tProps);\n     TableId tid = TableId.of(\""t\"");\n     Capture<Stat> stat = newCapture();\n-    expect(zk.getData(eq(TablePropKey.of(iid, tid).getPath()), isA(PropStoreWatcher.class),\n-        capture(stat))).andAnswer(() -> {\n+    expect(\n+        zk.getData(eq(TablePropKey.of(tid).getPath()), isA(PropStoreWatcher.class), capture(stat)))\n+        .andAnswer(() -> {\n           Stat s = stat.getValue();\n           s.setCtime(System.currentTimeMillis());\n           s.setMtime(System.currentTimeMillis());\n@@ -286,7 +285,7 @@ public void propTest() throws Exception {\n           return tPropBytes;\n         }).once();\n \n-    expect(zk.getData(tBasePath + \""/t\"" + ZTABLE_NAMESPACE, null, null))\n+    expect(zk.getData(ZTABLES + \""/t\"" + ZTABLE_NAMESPACE, null, null))\n         .andReturn(\""+default\"".getBytes(UTF_8)).anyTimes();\n \n     context.close();\n@@ -294,7 +293,7 @@ public void propTest() throws Exception {\n \n     replay(context, zk);\n \n-    NamespacePropKey nsKey = NamespacePropKey.of(iid, nsId);\n+    NamespacePropKey nsKey = NamespacePropKey.of(nsId);\n     log.trace(\""namespace base path: {}\"", nsKey.getPath());\n \n     String testFileName = \""./target/zoo-info-viewer-\"" + System.currentTimeMillis() + \"".txt\"";\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java b/server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java\nindex c141eba5490..7df9383416a 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/metadata/ConditionalTabletsMutatorImplTest.java\n@@ -93,11 +93,10 @@ public void close() {\n   public void testRejectionHandler() {\n \n     ServerContext context = EasyMock.createMock(ServerContext.class);\n-    EasyMock.expect(context.getZooKeeperRoot()).andReturn(\""/some/path\"").anyTimes();\n     ServiceLock lock = EasyMock.createMock(ServiceLock.class);\n     LockID lid = EasyMock.createMock(LockID.class);\n     EasyMock.expect(lock.getLockID()).andReturn(lid).anyTimes();\n-    EasyMock.expect(lid.serialize(\""/some/path/\"")).andReturn(\""/some/path/1234\"").anyTimes();\n+    EasyMock.expect(lid.serialize(\""/\"")).andReturn(\""/1234\"").anyTimes();\n     EasyMock.expect(context.getServiceLock()).andReturn(lock).anyTimes();\n     EasyMock.replay(context, lock, lid);\n \n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\nindex fba25de8e9b..be333bd1d1a 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/delegation/ZooAuthenticationKeyWatcherTest.java\n@@ -42,7 +42,6 @@\n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.zookeeper.KeeperException.NoNodeException;\n import org.apache.zookeeper.WatchedEvent;\n@@ -70,7 +69,6 @@ public static void setupKeyGenerator() throws Exception {\n \n   private ZooSession zk;\n   private InstanceId instanceId;\n-  private String baseNode;\n   private long tokenLifetime = DAYS.toMillis(7);\n   private AuthenticationTokenSecretManager secretManager;\n   private ZooAuthenticationKeyWatcher keyWatcher;\n@@ -79,12 +77,12 @@ public static void setupKeyGenerator() throws Exception {\n   public void setupMocks() {\n     zk = createMock(ZooSession.class);\n     instanceId = InstanceId.of(UUID.randomUUID());\n-    baseNode = ZooUtil.getRoot(instanceId) + Constants.ZDELEGATION_TOKEN_KEYS;\n     secretManager = new AuthenticationTokenSecretManager(instanceId, tokenLifetime);\n \n     expect(zk.asReader()).andReturn(new ZooReader(zk)).once();\n     replay(zk);\n-    keyWatcher = new ZooAuthenticationKeyWatcher(secretManager, zk, baseNode);\n+    keyWatcher =\n+        new ZooAuthenticationKeyWatcher(secretManager, zk, Constants.ZDELEGATION_TOKEN_KEYS);\n     reset(zk);\n   }\n \n@@ -95,9 +93,11 @@ public void verifyMocks() {\n \n   @Test\n   public void testBaseNodeCreated() throws Exception {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeCreated, null, baseNode);\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeCreated, null, Constants.ZDELEGATION_TOKEN_KEYS);\n \n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(Collections.emptyList());\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher))\n+        .andReturn(Collections.emptyList());\n     replay(zk);\n \n     keyWatcher.process(event);\n@@ -107,15 +107,18 @@ public void testBaseNodeCreated() throws Exception {\n \n   @Test\n   public void testBaseNodeCreatedWithChildren() throws Exception {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeCreated, null, baseNode);\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeCreated, null, Constants.ZDELEGATION_TOKEN_KEYS);\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     byte[] serializedKey1 = serialize(key1), serializedKey2 = serialize(key2);\n     List<String> children = Arrays.asList(\""1\"", \""2\"");\n \n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n-    expect(zk.getData(baseNode + \""/1\"", keyWatcher, null)).andReturn(serializedKey1);\n-    expect(zk.getData(baseNode + \""/2\"", keyWatcher, null)).andReturn(serializedKey2);\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/1\"", keyWatcher, null))\n+        .andReturn(serializedKey1);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"", keyWatcher, null))\n+        .andReturn(serializedKey2);\n     replay(zk);\n \n     keyWatcher.process(event);\n@@ -127,15 +130,18 @@ public void testBaseNodeCreatedWithChildren() throws Exception {\n \n   @Test\n   public void testBaseNodeChildrenChanged() throws Exception {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeChildrenChanged, null, baseNode);\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeChildrenChanged, null, Constants.ZDELEGATION_TOKEN_KEYS);\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     byte[] serializedKey1 = serialize(key1), serializedKey2 = serialize(key2);\n     List<String> children = Arrays.asList(\""1\"", \""2\"");\n \n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n-    expect(zk.getData(baseNode + \""/1\"", keyWatcher, null)).andReturn(serializedKey1);\n-    expect(zk.getData(baseNode + \""/2\"", keyWatcher, null)).andReturn(serializedKey2);\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/1\"", keyWatcher, null))\n+        .andReturn(serializedKey1);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"", keyWatcher, null))\n+        .andReturn(serializedKey2);\n     replay(zk);\n \n     keyWatcher.process(event);\n@@ -147,7 +153,8 @@ public void testBaseNodeChildrenChanged() throws Exception {\n \n   @Test\n   public void testBaseNodeDeleted() {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeDeleted, null, baseNode);\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeDeleted, null, Constants.ZDELEGATION_TOKEN_KEYS);\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n@@ -165,7 +172,8 @@ public void testBaseNodeDeleted() {\n \n   @Test\n   public void testBaseNodeDataChanged() {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeDataChanged, null, baseNode);\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeDataChanged, null, Constants.ZDELEGATION_TOKEN_KEYS);\n \n     replay(zk);\n \n@@ -177,7 +185,8 @@ public void testBaseNodeDataChanged() {\n \n   @Test\n   public void testChildChanged() throws Exception {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeCreated, null, baseNode + \""/2\"");\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeCreated, null, Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"");\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     secretManager.addKey(key1);\n@@ -197,7 +206,8 @@ public void testChildChanged() throws Exception {\n \n   @Test\n   public void testChildDeleted() {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeDeleted, null, baseNode + \""/1\"");\n+    WatchedEvent event =\n+        new WatchedEvent(EventType.NodeDeleted, null, Constants.ZDELEGATION_TOKEN_KEYS + \""/1\"");\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     secretManager.addKey(key1);\n@@ -215,7 +225,8 @@ public void testChildDeleted() {\n \n   @Test\n   public void testChildChildrenChanged() {\n-    WatchedEvent event = new WatchedEvent(EventType.NodeChildrenChanged, null, baseNode + \""/2\"");\n+    WatchedEvent event = new WatchedEvent(EventType.NodeChildrenChanged, null,\n+        Constants.ZDELEGATION_TOKEN_KEYS + \""/2\"");\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(2, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n     secretManager.addKey(key1);\n@@ -235,7 +246,7 @@ public void testChildChildrenChanged() {\n \n   @Test\n   public void testInitialUpdateNoNode() throws Exception {\n-    expect(zk.exists(baseNode, keyWatcher)).andReturn(null);\n+    expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(null);\n \n     replay(zk);\n \n@@ -251,11 +262,11 @@ public void testInitialUpdateWithKeys() throws Exception {\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n-    expect(zk.exists(baseNode, keyWatcher)).andReturn(new Stat());\n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n-    expect(zk.getData(baseNode + \""/\"" + key1.getKeyId(), keyWatcher, null))\n+    expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key1.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key1));\n-    expect(zk.getData(baseNode + \""/\"" + key2.getKeyId(), keyWatcher, null))\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key2.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key2));\n \n     replay(zk);\n@@ -286,11 +297,11 @@ private void lostZooKeeperBase(WatchedEvent disconnectEvent, WatchedEvent reconn\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey()),\n         key2 = new AuthenticationKey(5, key1.getExpirationDate(), 20000L, keyGen.generateKey());\n \n-    expect(zk.exists(baseNode, keyWatcher)).andReturn(new Stat());\n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n-    expect(zk.getData(baseNode + \""/\"" + key1.getKeyId(), keyWatcher, null))\n+    expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key1.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key1));\n-    expect(zk.getData(baseNode + \""/\"" + key2.getKeyId(), keyWatcher, null))\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key2.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key2));\n \n     replay(zk);\n@@ -306,11 +317,11 @@ private void lostZooKeeperBase(WatchedEvent disconnectEvent, WatchedEvent reconn\n \n     reset(zk);\n \n-    expect(zk.exists(baseNode, keyWatcher)).andReturn(new Stat());\n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n-    expect(zk.getData(baseNode + \""/\"" + key1.getKeyId(), keyWatcher, null))\n+    expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key1.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key1));\n-    expect(zk.getData(baseNode + \""/\"" + key2.getKeyId(), keyWatcher, null))\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key2.getKeyId(), keyWatcher, null))\n         .andReturn(serialize(key2));\n \n     replay(zk);\n@@ -329,11 +340,11 @@ public void missingKeyAfterGetChildren() throws Exception {\n     List<String> children = Arrays.asList(\""1\"");\n     AuthenticationKey key1 = new AuthenticationKey(1, 0L, 10000L, keyGen.generateKey());\n \n-    expect(zk.exists(baseNode, keyWatcher)).andReturn(new Stat());\n+    expect(zk.exists(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(new Stat());\n     // We saw key1\n-    expect(zk.getChildren(baseNode, keyWatcher)).andReturn(children);\n+    expect(zk.getChildren(Constants.ZDELEGATION_TOKEN_KEYS, keyWatcher)).andReturn(children);\n     // but it was gone when we tried to access it (manager deleted it)\n-    expect(zk.getData(baseNode + \""/\"" + key1.getKeyId(), keyWatcher, null))\n+    expect(zk.getData(Constants.ZDELEGATION_TOKEN_KEYS + \""/\"" + key1.getKeyId(), keyWatcher, null))\n         .andThrow(new NoNodeException());\n \n     replay(zk);\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java b/server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java\nindex 4eec2b650a9..84763e1ca5b 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/security/handler/ZKAuthenticatorTest.java\n@@ -37,8 +37,6 @@\n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n-import org.apache.accumulo.core.data.InstanceId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.security.SystemPermission;\n import org.apache.accumulo.core.security.TablePermission;\n@@ -140,18 +138,13 @@ public void testUserAuthentication() throws Exception {\n     byte[] newHash = ZKSecurityTool.createPass(rawPass.clone());\n \n     // mocking zk interaction\n-    var instanceId = InstanceId.of(\""example\"");\n     ZooSession zk = createMock(ZooSession.class);\n     ServerContext context = MockServerContext.getWithMockZK(zk);\n     ZooCache zc = createMock(ZooCache.class);\n-    expect(context.zkUserPath()).andReturn(ZooUtil.getRoot(instanceId) + Constants.ZUSERS)\n-        .anyTimes();\n     expect(zk.getChildren(anyObject(), anyObject())).andReturn(Arrays.asList(principal)).anyTimes();\n-    expect(zk.exists(ZooUtil.getRoot(instanceId) + Constants.ZUSERS + \""/\"" + principal, null))\n-        .andReturn(new Stat()).anyTimes();\n+    expect(zk.exists(Constants.ZUSERS + \""/\"" + principal, null)).andReturn(new Stat()).anyTimes();\n     expect(context.getZooCache()).andReturn(zc).anyTimes();\n-    expect(zc.get(ZooUtil.getRoot(instanceId) + Constants.ZUSERS + \""/\"" + principal))\n-        .andReturn(newHash);\n+    expect(zc.get(Constants.ZUSERS + \""/\"" + principal)).andReturn(newHash);\n     replay(context, zk, zc);\n \n     // creating authenticator\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java\nindex 74b590ff354..56c060b5cc6 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/AdminTest.java\n@@ -45,12 +45,10 @@\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.lock.ServiceLockData;\n import org.apache.accumulo.core.lock.ServiceLockData.ThriftService;\n import org.apache.accumulo.core.lock.ServiceLockPaths;\n@@ -70,29 +68,12 @@\n \n public class AdminTest {\n \n-  @Test\n-  public void testZooKeeperTserverPath() {\n-    ClientContext context = createMock(ClientContext.class);\n-    InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n-\n-    expect(context.getZooKeeperRoot()).andReturn(ZooUtil.getRoot(instanceId));\n-\n-    replay(context);\n-\n-    assertEquals(ZooUtil.getRoot(instanceId) + Constants.ZTSERVERS,\n-        Admin.getTServersZkPath(context));\n-\n-    verify(context);\n-  }\n-\n   @Test\n   public void testQualifySessionId() throws KeeperException, InterruptedException {\n     ClientContext ctx = createMock(ClientContext.class);\n     ZooCache zc = createMock(ZooCache.class);\n-    InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n \n-    String root = ZooUtil.getRoot(instanceId);\n-    String type = root + Constants.ZTSERVERS;\n+    String type = Constants.ZTSERVERS;\n     String group = type + \""/\"" + Constants.DEFAULT_RESOURCE_GROUP_NAME;\n     String server = \""localhost:12345\"";\n     final long session = 123456789L;\n@@ -114,7 +95,7 @@ public void testQualifySessionId() throws KeeperException, InterruptedException\n           stat.setEphemeralOwner(session);\n           return new byte[0];\n         });\n-    expect(ctx.getServerPaths()).andReturn(new ServiceLockPaths(root, zc)).anyTimes();\n+    expect(ctx.getServerPaths()).andReturn(new ServiceLockPaths(zc)).anyTimes();\n     replay(ctx, zc);\n \n     assertEquals(server + \""[\"" + Long.toHexString(session) + \""]\"",\n@@ -127,17 +108,15 @@ public void testQualifySessionId() throws KeeperException, InterruptedException\n   public void testCannotQualifySessionId() throws KeeperException, InterruptedException {\n     ClientContext ctx = createMock(ClientContext.class);\n     ZooCache zc = createMock(ZooCache.class);\n-    InstanceId instanceId = InstanceId.of(UUID.randomUUID());\n \n-    String root = ZooUtil.getRoot(instanceId);\n-    String type = root + Constants.ZTSERVERS;\n+    String type = Constants.ZTSERVERS;\n     String group = type + \""/\"" + Constants.DEFAULT_RESOURCE_GROUP_NAME;\n     String server = \""localhost:12345\"";\n \n     String serverPath = group + \""/\"" + server;\n     expect(zc.getChildren(type)).andReturn(List.of(Constants.DEFAULT_RESOURCE_GROUP_NAME));\n     expect(zc.getChildren(serverPath)).andReturn(Collections.emptyList());\n-    expect(ctx.getServerPaths()).andReturn(new ServiceLockPaths(root, zc)).anyTimes();\n+    expect(ctx.getServerPaths()).andReturn(new ServiceLockPaths(zc)).anyTimes();\n     replay(ctx, zc);\n \n     // A server that isn't in ZooKeeper. Can't qualify it, should return the original\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java\nindex e16fc75e450..3baad4305e5 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/PropUtilTest.java\n@@ -66,7 +66,7 @@ public void testSetClasspathContextFails() {\n     AccumuloConfiguration conf = createMock(AccumuloConfiguration.class);\n     InstanceId iid = createMock(InstanceId.class);\n     TableId tid = createMock(TableId.class);\n-    TablePropKey tkey = TablePropKey.of(iid, tid);\n+    TablePropKey tkey = TablePropKey.of(tid);\n \n     expect(ctx.getConfiguration()).andReturn(conf).once();\n     expect(conf.get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY))\n@@ -87,7 +87,7 @@ public void testSetClasspathContext() {\n     AccumuloConfiguration conf = createMock(AccumuloConfiguration.class);\n     InstanceId iid = createMock(InstanceId.class);\n     TableId tid = createMock(TableId.class);\n-    TablePropKey tkey = TablePropKey.of(iid, tid);\n+    TablePropKey tkey = TablePropKey.of(tid);\n \n     expect(ctx.getConfiguration()).andReturn(conf).once();\n     expect(conf.get(Property.GENERAL_CONTEXT_CLASSLOADER_FACTORY))\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\nindex 9d83992bfd3..2d81f8b6c76 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\n@@ -38,9 +38,7 @@\n import java.util.UUID;\n \n import org.apache.accumulo.core.Constants;\n-import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.fate.zookeeper.ZooReader;\n-import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n import org.apache.accumulo.core.lock.ServiceLockPaths;\n import org.apache.accumulo.core.zookeeper.ZcStat;\n import org.apache.accumulo.core.zookeeper.ZooCache;\n@@ -59,22 +57,19 @@ public class ServiceStatusCmdTest {\n \n   private static final Logger LOG = LoggerFactory.getLogger(ServiceStatusCmdTest.class);\n \n-  private String zRoot;\n   private ServerContext context;\n   private ZooSession zooReader;\n   private ZooCache zooCache;\n \n   @BeforeEach\n   public void populateContext() {\n-    zRoot = ZooUtil.getRoot(InstanceId.of(UUID.randomUUID()));\n     context = createMock(ServerContext.class);\n     zooReader = createMock(ZooSession.class);\n     zooCache = createMock(ZooCache.class);\n     expect(zooReader.asReader()).andReturn(new ZooReader(zooReader)).anyTimes();\n     expect(context.getZooCache()).andReturn(zooCache).anyTimes();\n     expect(context.getZooSession()).andReturn(zooReader).anyTimes();\n-    expect(context.getZooKeeperRoot()).andReturn(zRoot).anyTimes();\n-    expect(context.getServerPaths()).andReturn(new ServiceLockPaths(zRoot, zooCache)).anyTimes();\n+    expect(context.getServerPaths()).andReturn(new ServiceLockPaths(zooCache)).anyTimes();\n     replay(context);\n   }\n \n@@ -97,7 +92,7 @@ public void testManagerHosts() throws Exception {\n     String lock3Data =\n         \""{\\\""descriptors\\\"":[{\\\""uuid\\\"":\\\""6effb690-c29c-4e0b-92ff-f6b308385a42\\\"",\\\""service\\\"":\\\""MANAGER\\\"",\\\""address\\\"":\\\""hostA:9999\\\"",\\\""group\\\"":\\\""manager1\\\""}]}\"";\n \n-    String lockPath = zRoot + Constants.ZMANAGER_LOCK;\n+    String lockPath = Constants.ZMANAGER_LOCK;\n     expect(zooReader.getChildren(lockPath, null))\n         .andReturn(List.of(lock1Name, lock2Name, lock3Name));\n     expect(zooReader.getData(lockPath + \""/\"" + lock1Name, null, null))\n@@ -142,7 +137,7 @@ public void testMonitorHosts() throws Exception {\n     String host2 =\n         \""{\\\""descriptors\\\"":[{\\\""uuid\\\"":\\\""87465459-9c8f-4f95-b4c6-ef3029030d05\\\"",\\\""service\\\"":\\\""NONE\\\"",\\\""address\\\"":\\\""hostB\\\"",\\\""group\\\"":\\\""default\\\""}]}\"";\n \n-    String lockPath = zRoot + Constants.ZMONITOR_LOCK;\n+    String lockPath = Constants.ZMONITOR_LOCK;\n     expect(zooReader.getChildren(lockPath, null)).andReturn(List.of(lock1Name, lock2Name));\n     expect(zooReader.getData(lockPath + \""/\"" + lock1Name, null, null))\n         .andReturn(host1.getBytes(UTF_8));\n@@ -218,7 +213,7 @@ public void testTServerHosts() throws Exception {\n             + \""\\\"",\\\""group\\\"":\\\""default\\\""},{\\\""uuid\\\"":\\\""d0e29f70-1eb5-4dc5-9ad6-2466ab56ea32\\\"",\\\""service\\\"":\\\""TABLET_INGEST\\\"",\\\""address\\\"":\\\""\""\n             + host3 + \""\\\"",\\\""group\\\"":\\\""default\\\""}]}\"";\n \n-    String basePath = zRoot + Constants.ZTSERVERS;\n+    String basePath = Constants.ZTSERVERS;\n     expect(zooCache.getChildren(basePath))\n         .andReturn(List.of(Constants.DEFAULT_RESOURCE_GROUP_NAME));\n     expect(zooCache.getChildren(basePath + \""/\"" + Constants.DEFAULT_RESOURCE_GROUP_NAME))\n@@ -309,7 +304,7 @@ public void testScanServerHosts() throws Exception {\n             + \""\\\"",\\\""group\\\"":\\\""default\\\""},{\\\""uuid\\\"":\\\""f408fed7-ce93-40d2-8e60-63e8a3daf416\\\"",\\\""service\\\"":\\\""CLIENT\\\"",\\\""address\\\"":\\\""\""\n             + host4 + \""\\\"",\\\""group\\\"":\\\""default\\\""}]}\"";\n \n-    String lockPath = zRoot + Constants.ZSSERVERS;\n+    String lockPath = Constants.ZSSERVERS;\n     expect(zooCache.getChildren(lockPath))\n         .andReturn(List.of(Constants.DEFAULT_RESOURCE_GROUP_NAME, \""sg1\""));\n     expect(zooCache.getChildren(lockPath + \""/\"" + Constants.DEFAULT_RESOURCE_GROUP_NAME))\n@@ -377,7 +372,7 @@ public void testCompactorStatus() throws Exception {\n     String lock4data =\n         \""{\\\""descriptors\\\"":[{\\\""uuid\\\"":\\\""6effb690-c29c-4e0b-92ff-f6b308385a42\\\"",\\\""service\\\"":\\\""COMPACTOR\\\"",\\\""address\\\"":\\\""hostD:9091\\\"",\\\""group\\\"":\\\""q2\\\""}]}\"";\n \n-    String lockPath = zRoot + Constants.ZCOMPACTORS;\n+    String lockPath = Constants.ZCOMPACTORS;\n     expect(zooCache.getChildren(lockPath)).andReturn(List.of(\""q1\"", \""q2\"", \""q3\""));\n     expect(zooCache.getChildren(lockPath + \""/q1\"")).andReturn(List.of(\""hostA:8080\"", \""hostC:8081\""));\n     expect(zooCache.getChildren(lockPath + \""/q2\"")).andReturn(List.of(\""hostB:9090\"", \""hostD:9091\""));\n@@ -414,7 +409,7 @@ public void testCompactorStatus() throws Exception {\n   public void testGcHosts() throws Exception {\n     replay(zooCache);\n \n-    String lockPath = zRoot + ZGC_LOCK;\n+    String lockPath = ZGC_LOCK;\n     UUID uuid1 = UUID.randomUUID();\n     String lock1Name = \""zlock#\"" + uuid1 + \""#0000000001\"";\n     UUID uuid2 = UUID.randomUUID();\n@@ -473,7 +468,7 @@ public void zkNodeDeletedTest() throws Exception {\n         \""{\\\""descriptors\\\"":[{\\\""uuid\\\"":\\\""6effb690-c29c-4e0b-92ff-f6b308385a42\\\"",\\\""service\\\"":\\\""MANAGER\\\"",\\\""address\\\"":\\\""\""\n             + host3 + \""\\\"",\\\""group\\\"":\\\""manager1\\\""}]}\"";\n \n-    String lockPath = zRoot + Constants.ZMANAGER_LOCK;\n+    String lockPath = Constants.ZMANAGER_LOCK;\n     expect(zooReader.getChildren(lockPath, null))\n         .andReturn(List.of(lock1Name, lock2Name, lock3Name));\n     expect(zooReader.getData(lockPath + \""/\"" + lock1Name, null, null))\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5341"", ""pr_id"": 5341, ""issue_id"": 5201, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Bulk import times scale with the number of tablets in a table.\n**Describe the bug**\r\n\r\nWhen bulk importing into N tablets the bulk import v2 code will scan all tablet in the metadata table between the minimum and maximum tablet being imported into.   For example if importing into 10 tablets into a table with 100K tablets its possible that the bulk import scans all 100K tablets, it depends on where the minimum and maximum tablet in the 10 fall in the 100k.\r\n\r\n**Expected behavior**\r\n\r\nIdeally the amount of scanning done would be directly related to the number of tablets being bulk imported and not the number of tablets int he table.  This would be a large change to the way the code works.   A good first step would be to add some logging to the current code that captures how much time this behavior is wasting.  Then further decisions could be made about improving the code based on that."", ""issue_word_count"": 159, ""test_files_count"": 5, ""non_test_files_count"": 4, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/util/PeekingIterator.java"", ""core/src/test/java/org/apache/accumulo/core/util/PeekingIteratorTest.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFiles.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BulkNewMetadataSkipIT.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/util/PeekingIteratorTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/BulkNewMetadataSkipIT.java""], ""base_commit"": ""6501f755c4e24edba0d45d06715fbd0e9f0fb189"", ""head_commit"": ""4fbd481b9a31a1b622e9ef16b521504a30442cd1"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5341"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5341"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-28T16:22:39.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex ff0cad4baba..39955d0998a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -1220,6 +1220,18 @@ public enum Property {\n       \""The maximum number of tablets allowed for one bulk import file. Value of 0 is Unlimited. \""\n           + \""This property is only enforced in the new bulk import API.\"",\n       \""2.1.0\""),\n+  TABLE_BULK_SKIP_THRESHOLD(\""table.bulk.metadata.skip.distance\"", \""0\"", PropertyType.COUNT,\n+      \""When performing bulk v2 imports to a table, the Manager iterates over the tables metadata\""\n+          + \"" tablets sequentially. When importing files into a small table or into all or a majority\""\n+          + \"" of tablets of a large table then the tablet metadata information for most tablets will be needed.\""\n+          + \"" However, when importing files into a small number of non-contiguous tablets in a large table, then\""\n+          + \"" the Manager will look at each tablets metadata when it could be skipped. The value of this\""\n+          + \"" property tells the Manager if, and when, it should set up a new scanner over the metadata\""\n+          + \"" table instead of just iterating over tablet metadata to find the matching tablet. Setting up\""\n+          + \"" a new scanner is analogous to performing a seek in an iterator, but it has a cost. A value of zero (default) disables\""\n+          + \"" this feature. A non-zero value enables this feature and the Manager will setup a new scanner\""\n+          + \"" when the tablet metadata distance is above the supplied value.\"",\n+      \""2.1.4\""),\n   TABLE_DURABILITY(\""table.durability\"", \""sync\"", PropertyType.DURABILITY,\n       \""The durability used to write to the write-ahead log. Legal values are:\""\n           + \"" none, which skips the write-ahead log; log, which sends the data to the\""\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/PeekingIterator.java b/core/src/main/java/org/apache/accumulo/core/util/PeekingIterator.java\nindex f410174e739..c7b85ec159f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/PeekingIterator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/PeekingIterator.java\n@@ -19,6 +19,9 @@\n package org.apache.accumulo.core.util;\n \n import java.util.Iterator;\n+import java.util.function.Predicate;\n+\n+import com.google.common.base.Preconditions;\n \n public class PeekingIterator<E> implements Iterator<E> {\n \n@@ -91,4 +94,38 @@ public boolean hasNext() {\n     }\n     return top != null;\n   }\n+\n+  /**\n+   * Advances the underlying iterator looking for a match, inspecting up to {@code limit} elements\n+   * from the iterator. If this method finds a match to the predicate, then it will return true and\n+   * will be positioned before the matching element (peek() and next() will return the matching\n+   * element). If this method does not find a match because the underlying iterator ended before\n+   * {@code limit}, then it will return false and hasNext will also return false. Otherwise, if this\n+   * method does not find a match, then it will return false and be positioned before the limit\n+   * element (peek() and next() will return the {@code limit} element).\n+   *\n+   * @param predicate condition that we are looking for, parameter could be null, so the Predicate\n+   *        implementation needs to handle this.\n+   * @param limit number of times that we should look for a match, parameter must be a positive int\n+   * @return true if an element matched the predicate or false otherwise. When true hasNext() will\n+   *         return true and peek() and next() will return the matching element. When false\n+   *         hasNext() may return false if the end has been reached, or hasNext() may return true in\n+   *         which case peek() and next() will return the element {@code limit} positions ahead of\n+   *         where this iterator was before this method was called.\n+   */\n+  public boolean findWithin(Predicate<E> predicate, int limit) {\n+    Preconditions.checkArgument(limit > 0);\n+    for (int i = 0; i < limit; i++) {\n+      if (predicate.test(peek())) {\n+        return true;\n+      } else if (i < (limit - 1)) {\n+        if (hasNext()) {\n+          next();\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFiles.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFiles.java\nindex dfa4b982789..abed9684aa2 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFiles.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFiles.java\n@@ -123,7 +123,9 @@ public long isReady(long tid, Manager manager) throws Exception {\n           .forTable(bulkInfo.tableId).overlapping(startRow, null).checkConsistency()\n           .fetch(PREV_ROW, LOCATION, LOADED).build();\n \n-      return loadFiles(loader, bulkInfo, bulkDir, lmi, tmf, manager, tid);\n+      int skip = manager.getContext().getTableConfiguration(bulkInfo.tableId)\n+          .getCount(Property.TABLE_BULK_SKIP_THRESHOLD);\n+      return loadFiles(loader, bulkInfo, bulkDir, lmi, tmf, manager, tid, skip);\n     }\n   }\n \n@@ -352,8 +354,8 @@ static class ImportTimingStats {\n    */\n   // visible for testing\n   static long loadFiles(Loader loader, BulkInfo bulkInfo, Path bulkDir,\n-      LoadMappingIterator loadMapIter, TabletsMetadataFactory factory, Manager manager, long tid)\n-      throws Exception {\n+      LoadMappingIterator loadMapIter, TabletsMetadataFactory factory, Manager manager, long tid,\n+      int skipDistance) throws Exception {\n     PeekingIterator<Map.Entry<KeyExtent,Bulk.Files>> lmi = new PeekingIterator<>(loadMapIter);\n     Map.Entry<KeyExtent,Bulk.Files> loadMapEntry = lmi.peek();\n \n@@ -366,15 +368,33 @@ static long loadFiles(Loader loader, BulkInfo bulkInfo, Path bulkDir,\n \n     ImportTimingStats importTimingStats = new ImportTimingStats();\n     Timer timer = Timer.startNew();\n-    try (TabletsMetadata tabletsMetadata = factory.newTabletsMetadata(startRow)) {\n \n-      Iterator<TabletMetadata> tabletIter = tabletsMetadata.iterator();\n+    TabletsMetadata tabletsMetadata = factory.newTabletsMetadata(startRow);\n+    try {\n+      PeekingIterator<TabletMetadata> pi = new PeekingIterator<>(tabletsMetadata.iterator());\n       while (lmi.hasNext()) {\n         loadMapEntry = lmi.next();\n+        // If the user set the TABLE_BULK_SKIP_THRESHOLD property, then only look\n+        // at the next skipDistance tablets before recreating the iterator\n+        if (skipDistance > 0) {\n+          final KeyExtent loadMapKey = loadMapEntry.getKey();\n+          if (!pi.findWithin(\n+              tm -> PREV_COMP.compare(tm.getPrevEndRow(), loadMapKey.prevEndRow()) >= 0,\n+              skipDistance)) {\n+            log.debug(\n+                \""Next load mapping range {} not found in {} tablets, recreating TabletMetadata to jump ahead\"",\n+                loadMapKey.prevEndRow(), skipDistance);\n+            tabletsMetadata.close();\n+            tabletsMetadata = factory.newTabletsMetadata(loadMapKey.prevEndRow());\n+            pi = new PeekingIterator<>(tabletsMetadata.iterator());\n+          }\n+        }\n         List<TabletMetadata> tablets =\n-            findOverlappingTablets(fmtTid, loadMapEntry.getKey(), tabletIter, importTimingStats);\n+            findOverlappingTablets(fmtTid, loadMapEntry.getKey(), pi, importTimingStats);\n         loader.load(tablets, loadMapEntry.getValue());\n       }\n+    } finally {\n+      tabletsMetadata.close();\n     }\n     Duration totalProcessingTime = timer.elapsed();\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java\nindex 56116365b49..a8689034c07 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImport.java\n@@ -46,6 +46,7 @@\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n+import org.apache.accumulo.core.util.PeekingIterator;\n import org.apache.accumulo.manager.Manager;\n import org.apache.accumulo.manager.tableOps.ManagerRepo;\n import org.apache.accumulo.manager.tableOps.Utils;\n@@ -104,6 +105,8 @@ public long isReady(long tid, Manager manager) throws Exception {\n   @VisibleForTesting\n   interface TabletIterFactory {\n     Iterator<KeyExtent> newTabletIter(Text startRow);\n+\n+    void close();\n   }\n \n   private static boolean equals(Function<KeyExtent,Text> extractor, KeyExtent ke1, KeyExtent ke2) {\n@@ -116,82 +119,100 @@ private static boolean equals(Function<KeyExtent,Text> extractor, KeyExtent ke1,\n    */\n   @VisibleForTesting\n   static KeyExtent validateLoadMapping(String tableId, LoadMappingIterator lmi,\n-      TabletIterFactory tabletIterFactory, int maxNumTablets, long tid) throws Exception {\n+      TabletIterFactory tabletIterFactory, int maxNumTablets, long tid, int skip) throws Exception {\n+\n     var currRange = lmi.next();\n \n     Text startRow = currRange.getKey().prevEndRow();\n \n-    Iterator<KeyExtent> tabletIter = tabletIterFactory.newTabletIter(startRow);\n+    PeekingIterator<KeyExtent> pi =\n+        new PeekingIterator<>(tabletIterFactory.newTabletIter(startRow));\n \n-    KeyExtent currTablet = tabletIter.next();\n+    try {\n+      KeyExtent currTablet = pi.next();\n \n-    var fileCounts = new HashMap<String,Integer>();\n-    int count;\n+      var fileCounts = new HashMap<String,Integer>();\n+      int count;\n \n-    KeyExtent firstTablet = currRange.getKey();\n-    KeyExtent lastTablet = currRange.getKey();\n+      KeyExtent firstTablet = currRange.getKey();\n+      KeyExtent lastTablet = currRange.getKey();\n \n-    if (!tabletIter.hasNext() && equals(KeyExtent::prevEndRow, currTablet, currRange.getKey())\n-        && equals(KeyExtent::endRow, currTablet, currRange.getKey())) {\n-      currRange = null;\n-    }\n+      if (!pi.hasNext() && equals(KeyExtent::prevEndRow, currTablet, currRange.getKey())\n+          && equals(KeyExtent::endRow, currTablet, currRange.getKey())) {\n+        currRange = null;\n+      }\n \n-    while (tabletIter.hasNext()) {\n+      while (pi.hasNext()) {\n \n-      if (currRange == null) {\n-        if (!lmi.hasNext()) {\n-          break;\n+        if (currRange == null) {\n+          if (!lmi.hasNext()) {\n+            break;\n+          }\n+          currRange = lmi.next();\n+          lastTablet = currRange.getKey();\n+        }\n+        // If the user set the TABLE_BULK_SKIP_THRESHOLD property, then only look\n+        // at the next skipDistance tablets before recreating the iterator\n+        if (!equals(KeyExtent::prevEndRow, currTablet, currRange.getKey()) && skip > 0\n+            && currRange.getKey().prevEndRow() != null) {\n+          final KeyExtent search = currRange.getKey();\n+          if (!pi.findWithin((ke) -> Objects.equals(ke.prevEndRow(), search.prevEndRow()), skip)) {\n+            log.debug(\n+                \""Tablet metadata for prevEndRow {} not found in {} tablets from current tablet {}, recreating TabletMetadata to jump ahead\"",\n+                search.prevEndRow(), skip, currTablet);\n+            tabletIterFactory.close();\n+            pi = new PeekingIterator<>(tabletIterFactory.newTabletIter(search.prevEndRow()));\n+            currTablet = pi.next();\n+          }\n+        }\n+        while (!equals(KeyExtent::prevEndRow, currTablet, currRange.getKey()) && pi.hasNext()) {\n+          currTablet = pi.next();\n         }\n-        currRange = lmi.next();\n-        lastTablet = currRange.getKey();\n-      }\n \n-      while (!equals(KeyExtent::prevEndRow, currTablet, currRange.getKey())\n-          && tabletIter.hasNext()) {\n-        currTablet = tabletIter.next();\n-      }\n+        boolean matchedPrevRow = equals(KeyExtent::prevEndRow, currTablet, currRange.getKey());\n \n-      boolean matchedPrevRow = equals(KeyExtent::prevEndRow, currTablet, currRange.getKey());\n+        if (matchedPrevRow && firstTablet == null) {\n+          firstTablet = currTablet;\n+        }\n \n-      if (matchedPrevRow && firstTablet == null) {\n-        firstTablet = currTablet;\n-      }\n+        count = matchedPrevRow ? 1 : 0;\n \n-      count = matchedPrevRow ? 1 : 0;\n+        while (!equals(KeyExtent::endRow, currTablet, currRange.getKey()) && pi.hasNext()) {\n+          currTablet = pi.next();\n+          count++;\n+        }\n \n-      while (!equals(KeyExtent::endRow, currTablet, currRange.getKey()) && tabletIter.hasNext()) {\n-        currTablet = tabletIter.next();\n-        count++;\n-      }\n+        if (!matchedPrevRow || !equals(KeyExtent::endRow, currTablet, currRange.getKey())) {\n+          break;\n+        }\n \n-      if (!matchedPrevRow || !equals(KeyExtent::endRow, currTablet, currRange.getKey())) {\n-        break;\n+        if (maxNumTablets > 0) {\n+          int fc = count;\n+          currRange.getValue()\n+              .forEach(fileInfo -> fileCounts.merge(fileInfo.getFileName(), fc, Integer::sum));\n+        }\n+        currRange = null;\n       }\n \n-      if (maxNumTablets > 0) {\n-        int fc = count;\n-        currRange.getValue()\n-            .forEach(fileInfo -> fileCounts.merge(fileInfo.getFileName(), fc, Integer::sum));\n+      if (currRange != null || lmi.hasNext()) {\n+        // merge happened after the mapping was generated and before the table lock was acquired\n+        throw new AcceptableThriftTableOperationException(tableId, null, TableOperation.BULK_IMPORT,\n+            TableOperationExceptionType.BULK_CONCURRENT_MERGE, \""Concurrent merge happened\"");\n       }\n-      currRange = null;\n-    }\n-\n-    if (currRange != null || lmi.hasNext()) {\n-      // merge happened after the mapping was generated and before the table lock was acquired\n-      throw new AcceptableThriftTableOperationException(tableId, null, TableOperation.BULK_IMPORT,\n-          TableOperationExceptionType.BULK_CONCURRENT_MERGE, \""Concurrent merge happened\"");\n-    }\n \n-    if (maxNumTablets > 0) {\n-      fileCounts.values().removeIf(c -> c <= maxNumTablets);\n-      if (!fileCounts.isEmpty()) {\n-        throw new AcceptableThriftTableOperationException(tableId, null, TableOperation.BULK_IMPORT,\n-            TableOperationExceptionType.OTHER, \""Files overlap the configured max (\"" + maxNumTablets\n-                + \"") number of tablets: \"" + new TreeMap<>(fileCounts));\n+      if (maxNumTablets > 0) {\n+        fileCounts.values().removeIf(c -> c <= maxNumTablets);\n+        if (!fileCounts.isEmpty()) {\n+          throw new AcceptableThriftTableOperationException(tableId, null,\n+              TableOperation.BULK_IMPORT, TableOperationExceptionType.OTHER,\n+              \""Files overlap the configured max (\"" + maxNumTablets + \"") number of tablets: \""\n+                  + new TreeMap<>(fileCounts));\n+        }\n       }\n+      return new KeyExtent(firstTablet.tableId(), lastTablet.endRow(), firstTablet.prevEndRow());\n+    } finally {\n+      tabletIterFactory.close();\n     }\n-\n-    return new KeyExtent(firstTablet.tableId(), lastTablet.endRow(), firstTablet.prevEndRow());\n   }\n \n   private KeyExtent checkForMerge(final long tid, final Manager manager) throws Exception {\n@@ -205,13 +226,29 @@ private KeyExtent checkForMerge(final long tid, final Manager manager) throws Ex\n     try (LoadMappingIterator lmi =\n         BulkSerialize.readLoadMapping(bulkDir.toString(), bulkInfo.tableId, fs::open)) {\n \n-      TabletIterFactory tabletIterFactory =\n-          startRow -> TabletsMetadata.builder(manager.getContext()).forTable(bulkInfo.tableId)\n-              .overlapping(startRow, null).checkConsistency().fetch(PREV_ROW).build().stream()\n-              .map(TabletMetadata::getExtent).iterator();\n+      TabletIterFactory tabletIterFactory = new TabletIterFactory() {\n+\n+        TabletsMetadata tm = null;\n+\n+        @Override\n+        public Iterator<KeyExtent> newTabletIter(Text startRow) {\n+          tm = TabletsMetadata.builder(manager.getContext()).forTable(bulkInfo.tableId)\n+              .overlapping(startRow, null).checkConsistency().fetch(PREV_ROW).build();\n+          return tm.stream().map(TabletMetadata::getExtent).iterator();\n+        }\n \n+        @Override\n+        public void close() {\n+          if (tm != null) {\n+            tm.close();\n+          }\n+        }\n+      };\n+\n+      int skip = manager.getContext().getTableConfiguration(bulkInfo.tableId)\n+          .getCount(Property.TABLE_BULK_SKIP_THRESHOLD);\n       return validateLoadMapping(bulkInfo.tableId.canonical(), lmi, tabletIterFactory, maxTablets,\n-          tid);\n+          tid, skip);\n     }\n   }\n \n@@ -224,7 +261,6 @@ public Repo<Manager> call(final long tid, final Manager manager) throws Exceptio\n         Optional.ofNullable(tabletsRange.prevEndRow()).map(Text::getBytes).orElse(null);\n     bulkInfo.lastSplit =\n         Optional.ofNullable(tabletsRange.endRow()).map(Text::getBytes).orElse(null);\n-\n     log.trace(\""{} first split:{} last split:{}\"", FateTxId.formatTid(tid), tabletsRange.prevEndRow(),\n         tabletsRange.endRow());\n \n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/util/PeekingIteratorTest.java b/core/src/test/java/org/apache/accumulo/core/util/PeekingIteratorTest.java\nnew file mode 100644\nindex 00000000000..27bf76a7252\n--- /dev/null\n+++ b/core/src/test/java/org/apache/accumulo/core/util/PeekingIteratorTest.java\n@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.util;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.util.Iterator;\n+import java.util.stream.IntStream;\n+\n+import org.junit.jupiter.api.Test;\n+\n+public class PeekingIteratorTest {\n+\n+  @Test\n+  public void testPeek() {\n+    Iterator<Integer> ints = IntStream.range(1, 11).iterator();\n+    PeekingIterator<Integer> peek = new PeekingIterator<>(ints);\n+\n+    assertEquals(1, peek.peek());\n+\n+    for (int i = 1; i < 11; i++) {\n+      assertTrue(peek.hasNext());\n+      assertEquals(i, peek.next());\n+      if (i == 10) {\n+        assertNull(peek.peek());\n+      } else {\n+        assertEquals(i + 1, peek.peek());\n+      }\n+    }\n+\n+    assertFalse(peek.hasNext());\n+    assertNull(peek.next());\n+  }\n+\n+  @Test\n+  public void testFind() {\n+\n+    Iterator<Integer> ints = IntStream.range(1, 11).iterator();\n+    PeekingIterator<Integer> peek = new PeekingIterator<>(ints);\n+\n+    assertThrows(IllegalArgumentException.class, () -> peek.findWithin(e -> false, -1));\n+    assertEquals(1, peek.peek());\n+    assertTrue(peek.findWithin((x) -> x != null && x == 4, 5));\n+    assertTrue(peek.hasNext());\n+    assertEquals(4, peek.next());\n+    assertEquals(5, peek.peek());\n+\n+    // Advance the iterator 2 times looking for 7.\n+    // This will return false, but will advance\n+    // twice leaving the iterator at 6.\n+    assertFalse(peek.findWithin((x) -> x != null && x == 7, 2));\n+\n+    assertTrue(peek.hasNext());\n+    assertEquals(6, peek.peek());\n+    assertEquals(6, peek.next());\n+\n+    assertTrue(peek.findWithin((x) -> x != null && x == 8, 2));\n+    assertTrue(peek.hasNext());\n+    assertEquals(8, peek.next());\n+\n+    // Try to advance past the end\n+    assertFalse(peek.findWithin((x) -> x != null && x == 7, 3));\n+    assertFalse(peek.hasNext());\n+    assertNull(peek.next());\n+\n+  }\n+\n+}\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java\nindex 9afa0ceb576..31d8cfd828f 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/LoadFilesTest.java\n@@ -162,7 +162,7 @@ private Map<String,HashSet<KeyExtent>> runLoadFilesLoad(Map<KeyExtent,String> lo\n     Path bulkDir = EasyMock.createMock(Path.class);\n     EasyMock.replay(manager, bulkDir);\n \n-    LoadFiles.loadFiles(cl, info, bulkDir, lmi, tmf, manager, txid);\n+    LoadFiles.loadFiles(cl, info, bulkDir, lmi, tmf, manager, txid, 0);\n     EasyMock.verify(manager, bulkDir);\n     List<CaptureLoader.LoadResult> results = cl.getLoadResults();\n     assertEquals(loadRanges.size(), results.size());\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java\nindex 9ff5945e219..5fb2d8181fb 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/bulkVer2/PrepBulkImportTest.java\n@@ -31,6 +31,7 @@\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n@@ -88,29 +89,37 @@ Iterable<List<KeyExtent>> powerSet(KeyExtent... extents) {\n     }).iterator();\n   }\n \n-  private void runTest(List<KeyExtent> loadRanges, List<KeyExtent> tabletRanges) throws Exception {\n+  private void runTest(List<KeyExtent> loadRanges, List<KeyExtent> tabletRanges, int skipDistance)\n+      throws Exception {\n     Map<KeyExtent,String> lrm = new HashMap<>();\n     loadRanges.forEach(e -> lrm.put(e, \""f1 f2 f3\""));\n-    runTest(lrm, tabletRanges, 100);\n+    runTest(lrm, tabletRanges, 100, skipDistance);\n   }\n \n   public void runTest(Map<KeyExtent,String> loadRanges, List<KeyExtent> tabletRanges,\n-      int maxTablets) throws Exception {\n-    TabletIterFactory tabletIterFactory = startRow -> {\n-      int start = -1;\n-\n-      if (startRow == null) {\n-        start = 0;\n-      } else {\n-        for (int i = 0; i < tabletRanges.size(); i++) {\n-          if (tabletRanges.get(i).contains(startRow)) {\n-            start = i;\n-            break;\n+      int maxTablets, int skipDistance) throws Exception {\n+    TabletIterFactory tabletIterFactory = new TabletIterFactory() {\n+\n+      @Override\n+      public Iterator<KeyExtent> newTabletIter(Text startRow) {\n+        int start = -1;\n+\n+        if (startRow == null) {\n+          start = 0;\n+        } else {\n+          for (int i = 0; i < tabletRanges.size(); i++) {\n+            if (tabletRanges.get(i).contains(startRow)) {\n+              start = i;\n+              break;\n+            }\n           }\n         }\n+\n+        return tabletRanges.subList(start, tabletRanges.size()).iterator();\n       }\n \n-      return tabletRanges.subList(start, tabletRanges.size()).iterator();\n+      @Override\n+      public void close() {}\n     };\n \n     var sortedExtents = loadRanges.keySet().stream().sorted().collect(Collectors.toList());\n@@ -120,8 +129,8 @@ public void runTest(Map<KeyExtent,String> loadRanges, List<KeyExtent> tabletRang\n         .map(Text::toString).orElse(null);\n \n     try (LoadMappingIterator lmi = createLoadMappingIter(loadRanges)) {\n-      var extent =\n-          PrepBulkImport.validateLoadMapping(\""1\"", lmi, tabletIterFactory, maxTablets, 10001);\n+      var extent = PrepBulkImport.validateLoadMapping(\""1\"", lmi, tabletIterFactory, maxTablets,\n+          10001, skipDistance);\n       assertEquals(nke(minPrevEndRow, maxPrevEndRow), extent, loadRanges + \"" \"" + tabletRanges);\n     }\n   }\n@@ -154,23 +163,24 @@ static String toRangeStrings(Collection<KeyExtent> extents) {\n         .collect(Collectors.joining(\"",\""));\n   }\n \n-  public void runExceptionTest(List<KeyExtent> loadRanges, List<KeyExtent> tabletRanges) {\n+  public void runExceptionTest(List<KeyExtent> loadRanges, List<KeyExtent> tabletRanges,\n+      int skipDistance) {\n     String message = \""expected \"" + toRangeStrings(loadRanges) + \"" to fail against \""\n         + toRangeStrings(tabletRanges);\n     assertThrows(AcceptableThriftTableOperationException.class,\n-        () -> runTest(loadRanges, tabletRanges), message);\n+        () -> runTest(loadRanges, tabletRanges, skipDistance), message);\n   }\n \n   @Test\n   public void testSingleTablet() throws Exception {\n-    runTest(Arrays.asList(nke(null, null)), Arrays.asList(nke(null, null)));\n+    runTest(Arrays.asList(nke(null, null)), Arrays.asList(nke(null, null)), 0);\n \n     for (List<KeyExtent> loadRanges : powerSet(nke(null, \""b\""), nke(\""b\"", \""k\""), nke(\""k\"", \""r\""),\n         nke(\""r\"", null))) {\n       if (loadRanges.isEmpty()) {\n         continue;\n       }\n-      runExceptionTest(loadRanges, Arrays.asList(nke(null, null)));\n+      runExceptionTest(loadRanges, Arrays.asList(nke(null, null)), 0);\n     }\n   }\n \n@@ -186,8 +196,8 @@ public void testNominal() throws Exception {\n \n       List<String> requiredRows = List.of(\""b\"", \""m\"", \""r\"", \""v\"");\n       for (Set<String> otherRows : Sets.powerSet(Set.of(\""a\"", \""c\"", \""q\"", \""t\"", \""x\""))) {\n-        runTest(loadRanges,\n-            createExtents(Stream.concat(requiredRows.stream(), otherRows.stream())));\n+        runTest(loadRanges, createExtents(Stream.concat(requiredRows.stream(), otherRows.stream())),\n+            0);\n       }\n     }\n   }\n@@ -217,14 +227,14 @@ public void testException() {\n         // test will all but one of the rows in the load mapping\n         for (Set<String> otherRows : Sets.powerSet(Set.of(\""a\"", \""c\"", \""q\"", \""t\"", \""x\""))) {\n           runExceptionTest(loadRanges,\n-              createExtents(Stream.concat(rows2.stream(), otherRows.stream())));\n+              createExtents(Stream.concat(rows2.stream(), otherRows.stream())), 0);\n         }\n       }\n \n       if (rows.size() > 1) {\n         // test with none of the rows in the load mapping\n         for (Set<String> otherRows : Sets.powerSet(Set.of(\""a\"", \""c\"", \""q\"", \""t\"", \""x\""))) {\n-          runExceptionTest(loadRanges, createExtents(otherRows.stream()));\n+          runExceptionTest(loadRanges, createExtents(otherRows.stream()), 0);\n         }\n       }\n     }\n@@ -250,14 +260,14 @@ public void testTooManyTablets() throws Exception {\n         int totalTablets = requiredRows.size() + otherRows.size() + 1;\n \n         if (totalTablets > maxTablets) {\n-          runTooManyTest(loadRanges, tablets, \""{f2=\"" + totalTablets + \""}\"", maxTablets);\n+          runTooManyTest(loadRanges, tablets, \""{f2=\"" + totalTablets + \""}\"", maxTablets, 2);\n         } else {\n-          runTest(loadRanges, createExtents(tablets), maxTablets);\n+          runTest(loadRanges, createExtents(tablets), maxTablets, 2);\n         }\n       }\n \n       runTest(loadRanges, createExtents(Stream.concat(requiredRows.stream(), otherRows.stream())),\n-          0);\n+          0, 2);\n     }\n \n     loadRanges.clear();\n@@ -267,20 +277,20 @@ public void testTooManyTablets() throws Exception {\n     loadRanges.put(nke(\""re\"", \""rz\""), \""f4\"");\n \n     runTooManyTest(loadRanges, Stream.of(\""ca\"", \""cd\"", \""cz\"", \""e\"", \""ma\"", \""md\"", \""mm\"", \""re\"", \""rz\""),\n-        \""{f3=4}\"", 3);\n+        \""{f3=4}\"", 3, 2);\n     runTooManyTest(loadRanges, Stream.of(\""b\"", \""ca\"", \""cd\"", \""cz\"", \""e\"", \""ma\"", \""md\"", \""mm\"", \""re\"", \""rz\""),\n-        \""{f3=4}\"", 3);\n+        \""{f3=4}\"", 3, 2);\n     runTooManyTest(loadRanges,\n         Stream.of(\""ca\"", \""cd\"", \""cz\"", \""e\"", \""ma\"", \""md\"", \""mm\"", \""re\"", \""rf\"", \""rh\"", \""rm\"", \""rz\""),\n-        \""{f3=4, f4=4}\"", 3);\n+        \""{f3=4, f4=4}\"", 3, 2);\n     runTooManyTest(loadRanges,\n-        Stream.of(\""ca\"", \""cd\"", \""cz\"", \""e\"", \""ma\"", \""mm\"", \""re\"", \""rf\"", \""rh\"", \""rm\"", \""rz\""), \""{f4=4}\"", 3);\n+        Stream.of(\""ca\"", \""cd\"", \""cz\"", \""e\"", \""ma\"", \""mm\"", \""re\"", \""rf\"", \""rh\"", \""rm\"", \""rz\""), \""{f4=4}\"", 3, 2);\n   }\n \n   private void runTooManyTest(Map<KeyExtent,String> loadRanges, Stream<String> tablets,\n-      String expectedMessage, int maxTablets) {\n+      String expectedMessage, int maxTablets, int skipDistance) {\n     var exception = assertThrows(ThriftTableOperationException.class,\n-        () -> runTest(loadRanges, createExtents(tablets), maxTablets));\n+        () -> runTest(loadRanges, createExtents(tablets), maxTablets, skipDistance));\n     String message = exception.toString();\n     assertTrue(exception.toString().contains(expectedMessage), expectedMessage + \"" -- \"" + message);\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java b/test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java\nindex 683461b8c3c..1cb6bbf953b 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/BulkNewIT.java\n@@ -169,7 +169,7 @@ private void testSingleTabletSingleFile(AccumuloClient c, boolean offline, boole\n \n     String dir = getDir(\""/testSingleTabletSingleFileNoSplits-\"");\n \n-    String h1 = writeData(dir + \""/f1.\"", aconf, 0, 332);\n+    String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 332);\n \n     c.tableOperations().importDirectory(dir).to(tableName).tableTime(setTime).load();\n     // running again with ignoreEmptyDir set to true will not throw an exception\n@@ -258,7 +258,7 @@ private void testSingleTabletSingleFileNoSplits(AccumuloClient c, boolean offlin\n \n     String dir = getDir(\""/testSingleTabletSingleFileNoSplits-\"");\n \n-    String h1 = writeData(dir + \""/f1.\"", aconf, 0, 333);\n+    String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n \n     c.tableOperations().importDirectory(dir).to(tableName).load();\n \n@@ -291,7 +291,7 @@ public void testBadPermissions() throws Exception {\n \n       String dir = getDir(\""/testBadPermissions-\"");\n \n-      writeData(dir + \""/f1.\"", aconf, 0, 333);\n+      writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n \n       Path rFilePath = new Path(dir, \""f1.\"" + RFile.EXTENSION);\n       FsPermission originalPerms = fs.getFileStatus(rFilePath).getPermission();\n@@ -339,21 +339,21 @@ private void testBulkFile(boolean offline, boolean usePlan) throws Exception {\n       out.close();\n \n       // 1 Tablet 0333-null\n-      String h1 = writeData(dir + \""/f1.\"", aconf, 0, 333);\n+      String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n       hashes.get(\""0333\"").add(h1);\n \n       // 2 Tablets 0666-0334, 0999-0667\n-      String h2 = writeData(dir + \""/f2.\"", aconf, 334, 999);\n+      String h2 = writeData(fs, dir + \""/f2.\"", aconf, 334, 999);\n       hashes.get(\""0666\"").add(h2);\n       hashes.get(\""0999\"").add(h2);\n \n       // 2 Tablets 1333-1000, 1666-1334\n-      String h3 = writeData(dir + \""/f3.\"", aconf, 1000, 1499);\n+      String h3 = writeData(fs, dir + \""/f3.\"", aconf, 1000, 1499);\n       hashes.get(\""1333\"").add(h3);\n       hashes.get(\""1666\"").add(h3);\n \n       // 2 Tablets 1666-1334, >1666\n-      String h4 = writeData(dir + \""/f4.\"", aconf, 1500, 1999);\n+      String h4 = writeData(fs, dir + \""/f4.\"", aconf, 1500, 1999);\n       hashes.get(\""1666\"").add(h4);\n       hashes.get(\""null\"").add(h4);\n \n@@ -393,21 +393,21 @@ private void testBulkFileMax(boolean usePlan) throws Exception {\n       out.close();\n \n       // 1 Tablet 0333-null\n-      String h1 = writeData(dir + \""/f1.\"", aconf, 0, 333);\n+      String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n       hashes.get(\""0333\"").add(h1);\n \n       // 3 Tablets 0666-0334, 0999-0667, 1333-1000\n-      String h2 = writeData(dir + \""/bad-file.\"", aconf, 334, 1333);\n+      String h2 = writeData(fs, dir + \""/bad-file.\"", aconf, 334, 1333);\n       hashes.get(\""0666\"").add(h2);\n       hashes.get(\""0999\"").add(h2);\n       hashes.get(\""1333\"").add(h2);\n \n       // 1 Tablet 1666-1334\n-      String h3 = writeData(dir + \""/f3.\"", aconf, 1334, 1499);\n+      String h3 = writeData(fs, dir + \""/f3.\"", aconf, 1334, 1499);\n       hashes.get(\""1666\"").add(h3);\n \n       // 2 Tablets 1666-1334, >1666\n-      String h4 = writeData(dir + \""/f4.\"", aconf, 1500, 1999);\n+      String h4 = writeData(fs, dir + \""/f4.\"", aconf, 1500, 1999);\n       hashes.get(\""1666\"").add(h4);\n       hashes.get(\""null\"").add(h4);\n \n@@ -453,8 +453,8 @@ public void testBadLoadPlans() throws Exception {\n \n       String dir = getDir(\""/testBulkFile-\"");\n \n-      writeData(dir + \""/f1.\"", aconf, 0, 333);\n-      writeData(dir + \""/f2.\"", aconf, 0, 666);\n+      writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n+      writeData(fs, dir + \""/f2.\"", aconf, 0, 666);\n \n       final var importMappingOptions = c.tableOperations().importDirectory(dir).to(tableName);\n \n@@ -487,12 +487,12 @@ public void testComputeLoadPlan() throws Exception {\n       String dir = getDir(\""/testBulkFile-\"");\n \n       Map<String,Set<String>> hashes = new HashMap<>();\n-      String h1 = writeData(dir + \""/f1.\"", aconf, 0, 333);\n+      String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n       hashes.put(\""0333\"", new HashSet<>(List.of(h1)));\n-      String h2 = writeData(dir + \""/f2.\"", aconf, 0, 666);\n+      String h2 = writeData(fs, dir + \""/f2.\"", aconf, 0, 666);\n       hashes.get(\""0333\"").add(h2);\n       hashes.put(\""0666\"", new HashSet<>(List.of(h2)));\n-      String h3 = writeData(dir + \""/f3.\"", aconf, 334, 700);\n+      String h3 = writeData(fs, dir + \""/f3.\"", aconf, 334, 700);\n       hashes.get(\""0666\"").add(h3);\n       hashes.put(\""0999\"", new HashSet<>(List.of(h3)));\n       hashes.put(\""1333\"", Set.of());\n@@ -571,7 +571,7 @@ public void testEndOfFirstTablet() throws Exception {\n \n       addSplits(c, tableName, \""0333\"");\n \n-      var h1 = writeData(dir + \""/f1.\"", aconf, 333, 333);\n+      var h1 = writeData(fs, dir + \""/f1.\"", aconf, 333, 333);\n \n       c.tableOperations().importDirectory(dir).to(tableName).load();\n \n@@ -593,7 +593,7 @@ public void testExceptionInMetadataUpdate() throws Exception {\n \n       String dir = getDir(\""/testExceptionInMetadataUpdate-\"");\n \n-      String h1 = writeData(dir + \""/f1.\"", aconf, 0, 333);\n+      String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 333);\n \n       var executor = Executors.newSingleThreadExecutor();\n       // With the constraint configured that makes tservers throw an exception on bulk import, the\n@@ -630,8 +630,8 @@ private void addSplits(AccumuloClient client, String tableName, String splitStri\n     client.tableOperations().addSplits(tableName, splits);\n   }\n \n-  private void verifyData(AccumuloClient client, String table, int start, int end, boolean setTime)\n-      throws Exception {\n+  private static void verifyData(AccumuloClient client, String table, int start, int end,\n+      boolean setTime) throws Exception {\n     try (Scanner scanner = client.createScanner(table, Authorizations.EMPTY)) {\n \n       Iterator<Entry<Key,Value>> iter = scanner.iterator();\n@@ -664,7 +664,7 @@ private void verifyData(AccumuloClient client, String table, int start, int end,\n     }\n   }\n \n-  private void verifyMetadata(AccumuloClient client, String tableName,\n+  public static void verifyMetadata(AccumuloClient client, String tableName,\n       Map<String,Set<String>> expectedHashes) {\n \n     Set<String> endRowsSeen = new HashSet<>();\n@@ -691,7 +691,7 @@ private void verifyMetadata(AccumuloClient client, String tableName,\n \n   @SuppressFBWarnings(value = {\""PATH_TRAVERSAL_IN\"", \""WEAK_MESSAGE_DIGEST_SHA1\""},\n       justification = \""path provided by test; sha-1 is okay for test\"")\n-  private String hash(String filename) {\n+  public static String hash(String filename) {\n     try {\n       byte[] data = Files.readAllBytes(Paths.get(filename.replaceFirst(\""^file:\"", \""\"")));\n       byte[] hash = MessageDigest.getInstance(\""SHA1\"").digest(data);\n@@ -701,13 +701,12 @@ private String hash(String filename) {\n     }\n   }\n \n-  private static String row(int r) {\n+  public static String row(int r) {\n     return String.format(\""%04d\"", r);\n   }\n \n-  private String writeData(String file, AccumuloConfiguration aconf, int s, int e)\n-      throws Exception {\n-    FileSystem fs = getCluster().getFileSystem();\n+  public static String writeData(FileSystem fs, String file, AccumuloConfiguration aconf, int s,\n+      int e) throws Exception {\n     String filename = file + RFile.EXTENSION;\n     try (FileSKVWriter writer = FileOperations.getInstance().newWriterBuilder()\n         .forFile(filename, fs, fs.getConf(), NoCryptoServiceFactory.NONE)\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/BulkNewMetadataSkipIT.java b/test/src/main/java/org/apache/accumulo/test/functional/BulkNewMetadataSkipIT.java\nnew file mode 100644\nindex 00000000000..8f286d8f7ce\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/BulkNewMetadataSkipIT.java\n@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.test.functional;\n+\n+import static org.apache.accumulo.test.functional.BulkNewIT.hash;\n+import static org.apache.accumulo.test.functional.BulkNewIT.row;\n+import static org.apache.accumulo.test.functional.BulkNewIT.verifyMetadata;\n+import static org.apache.accumulo.test.functional.BulkNewIT.writeData;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+import java.util.stream.IntStream;\n+\n+import org.apache.accumulo.core.client.Accumulo;\n+import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.Scanner;\n+import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n+import org.apache.accumulo.core.conf.AccumuloConfiguration;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.LoadPlan;\n+import org.apache.accumulo.core.data.LoadPlan.RangeType;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.file.FileOperations;\n+import org.apache.accumulo.core.file.FileSKVWriter;\n+import org.apache.accumulo.core.file.rfile.RFile;\n+import org.apache.accumulo.core.metadata.MetadataTable;\n+import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.core.spi.crypto.NoCryptoServiceFactory;\n+import org.apache.accumulo.harness.AccumuloClusterHarness;\n+import org.apache.accumulo.minicluster.MemoryUnit;\n+import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.hadoop.io.Text;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+/**\n+ * This test creates a table with 1000 splits and then imports files into a sparse set of the\n+ * tablets. This test also splits the metadata table such that the tablet metadata for each tablet\n+ * of the test table is in its own metadata tablet. The test then runs with different values for the\n+ * TABLE_BULK_SKIP_THRESHOLD property starting with zero (disabled) then increasing.\n+ *\n+ * This test uses AccumuloClusterHarness instead of SharedMiniClusterBase so that we don't have to\n+ * re-merge the metadata table and delete the test table. Doing these two things, and then waiting\n+ * for balancing, takes a long time. It's faster to just start with a clean instance for each test\n+ * run.\n+ */\n+public class BulkNewMetadataSkipIT extends AccumuloClusterHarness {\n+\n+  @Override\n+  public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {\n+    cfg.setMemory(ServerType.TABLET_SERVER, 512, MemoryUnit.MEGABYTE);\n+    cfg.setProperty(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL, \""3s\"");\n+    cfg.setProperty(Property.TSERV_ASSIGNMENT_MAXCONCURRENT, \""25\"");\n+    cfg.setNumTservers(1);\n+    hadoopCoreSite.set(\""fs.file.impl\"", RawLocalFileSystem.class.getName());\n+  }\n+\n+  public static String writeNonContiguousData(FileSystem fs, String file,\n+      AccumuloConfiguration aconf, int[] rows) throws Exception {\n+    String filename = file + RFile.EXTENSION;\n+    try (FileSKVWriter writer = FileOperations.getInstance().newWriterBuilder()\n+        .forFile(filename, fs, fs.getConf(), NoCryptoServiceFactory.NONE)\n+        .withTableConfiguration(aconf).build()) {\n+      writer.startDefaultLocalityGroup();\n+      for (int i : rows) {\n+        writer.append(new Key(new Text(row(i))), new Value(Integer.toString(i)));\n+      }\n+    }\n+    return hash(filename);\n+  }\n+\n+  @BeforeEach\n+  @Override\n+  public void setupCluster() throws Exception {\n+    super.setupCluster();\n+    // prime the zk connection\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {}\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {0, 0, 2, 4, 8, 16, 32, 64, 128})\n+  public void test(int skipDistance) throws Exception {\n+\n+    final String tableName = getUniqueNames(1)[0] + \""_\"" + skipDistance;\n+    final AccumuloConfiguration aconf = getCluster().getServerContext().getConfiguration();\n+    final FileSystem fs = getCluster().getFileSystem();\n+    final String rootPath = getCluster().getTemporaryPath().toString();\n+    final String dir = rootPath + \""/\"" + tableName;\n+\n+    fs.delete(new Path(dir), true);\n+\n+    final SortedSet<Text> splits = new TreeSet<>();\n+    IntStream.rangeClosed(0, 1000).forEach(i -> splits.add(new Text(String.format(\""%04d\"", i))));\n+\n+    final NewTableConfiguration ntc = new NewTableConfiguration();\n+    ntc.setProperties(\n+        Map.of(Property.TABLE_BULK_SKIP_THRESHOLD.getKey(), Integer.toString(skipDistance)));\n+    ntc.withSplits(splits);\n+\n+    final Map<String,Set<String>> hashes = new HashMap<>();\n+    IntStream.rangeClosed(0, 1000).forEach(i -> hashes.put(row(i), new HashSet<>()));\n+    hashes.put(\""null\"", new HashSet<>());\n+\n+    String h1 = writeData(fs, dir + \""/f1.\"", aconf, 0, 11);\n+    IntStream.rangeClosed(0, 11).forEach(i -> hashes.get(row(i)).add(h1));\n+\n+    int[] h2Rows = new int[] {11, 199, 200, 204};\n+    String h2 = writeNonContiguousData(fs, dir + \""/f2.\"", aconf, h2Rows);\n+    for (int i : h2Rows) {\n+      hashes.get(row(i)).add(h2);\n+    }\n+\n+    int[] h3Rows = new int[] {13, 200, 272, 273};\n+    String h3 = writeNonContiguousData(fs, dir + \""/f3.\"", aconf, h3Rows);\n+    for (int i : h3Rows) {\n+      hashes.get(row(i)).add(h3);\n+    }\n+\n+    int[] h4Rows = new int[] {300, 301, 672, 998};\n+    String h4 = writeNonContiguousData(fs, dir + \""/f4.\"", aconf, h4Rows);\n+    for (int i : h4Rows) {\n+      hashes.get(row(i)).add(h4);\n+    }\n+\n+    final LoadPlan loadPlan =\n+        LoadPlan.builder().loadFileTo(\""f1.rf\"", RangeType.FILE, row(0), row(11))\n+            .loadFileTo(\""f2.rf\"", RangeType.TABLE, row(10), row(11))\n+            .loadFileTo(\""f2.rf\"", RangeType.FILE, row(199), row(200))\n+            .loadFileTo(\""f2.rf\"", RangeType.TABLE, row(203), row(204))\n+            .loadFileTo(\""f3.rf\"", RangeType.TABLE, row(12), row(13))\n+            .loadFileTo(\""f3.rf\"", RangeType.TABLE, row(199), row(200))\n+            .loadFileTo(\""f3.rf\"", RangeType.FILE, row(272), row(273))\n+            .loadFileTo(\""f4.rf\"", RangeType.FILE, row(300), row(301))\n+            .loadFileTo(\""f4.rf\"", RangeType.TABLE, row(671), row(672))\n+            .loadFileTo(\""f4.rf\"", RangeType.TABLE, row(997), row(998)).build();\n+\n+    try (AccumuloClient c = Accumulo.newClient().from(getClientProps()).build()) {\n+\n+      c.tableOperations().create(tableName, ntc);\n+      TableId tid = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+\n+      final SortedSet<Text> metadataSplits = new TreeSet<>();\n+      Scanner s = c.createScanner(MetadataTable.NAME, Authorizations.EMPTY);\n+      final String mdTablePrefix = tid.canonical() + \"";\"";\n+      s.forEach(e -> {\n+        final String row = e.getKey().getRow().toString();\n+        if (row.startsWith(mdTablePrefix)) {\n+          metadataSplits.add(new Text(row + \""\\\\x00\""));\n+        }\n+      });\n+      c.tableOperations().addSplits(MetadataTable.NAME, metadataSplits);\n+\n+      c.tableOperations().importDirectory(dir).to(tableName).plan(loadPlan).load();\n+\n+      verifyData(c, tableName, new int[] {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 199, 200, 204,\n+          272, 273, 300, 301, 672, 998}, false);\n+      verifyMetadata(c, tableName, hashes);\n+    }\n+  }\n+\n+  public static void verifyData(AccumuloClient client, String table, int[] expectedRows,\n+      boolean setTime) throws Exception {\n+    try (Scanner scanner = client.createScanner(table, Authorizations.EMPTY)) {\n+\n+      Iterator<Entry<Key,Value>> iter = scanner.iterator();\n+\n+      int count = 0;\n+      while (iter.hasNext()) {\n+        Entry<Key,Value> entry = iter.next();\n+\n+        String expectedRow = String.format(\""%04d\"", expectedRows[count]);\n+\n+        if (!entry.getKey().getRow().equals(new Text(expectedRow))) {\n+          throw new Exception(\""unexpected row \"" + entry.getKey() + \"" \"" + expectedRow);\n+        }\n+\n+        if (Integer.parseInt(entry.getValue().toString()) != expectedRows[count]) {\n+          throw new Exception(\""unexpected value \"" + entry + \"" \"" + expectedRows[count]);\n+        }\n+\n+        if (setTime) {\n+          assertEquals(1L, entry.getKey().getTimestamp());\n+        }\n+\n+        count++;\n+      }\n+    }\n+  }\n+\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5336"", ""pr_id"": 5336, ""issue_id"": 5230, ""repo"": ""apache/accumulo"", ""problem_statement"": ""(Trivial) Rename `TxName`\n**Is your feature request related to a problem? Please describe.**\r\nPrior to https://github.com/apache/accumulo/pull/5218, a String was passed around and stored representing the type of fate operation being performed. This was changed to be a concrete type. `TxName` and similar naming are still used throughout the FATE code, which isn't as representative of what is being passed/stored now.\r\n\r\n**Describe the solution you'd like**\r\nAll mentions of `TxName` or similar should be renamed to something more appropriate like `fateOp`, `txOp`, `txOperation`, etc.\r\n"", ""issue_word_count"": 92, ""test_files_count"": 11, ""non_test_files_count"": 9, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateSummaryReport.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateTxnDetails.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/metrics/fate/FateMetricValues.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java""], ""base_commit"": ""0db1fd914439109d9df4b61c04e6f24e6c5d42a9"", ""head_commit"": ""6d461c8cedd5bb44a777e5a4085dc6ebc44f6aca"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5336"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5336"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-18T15:13:02.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java b/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\nindex 9577ab7ff2b..e89e8227f38 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\n@@ -65,20 +65,20 @@ public static class TransactionStatus {\n     private final FateId fateId;\n     private final FateInstanceType instanceType;\n     private final TStatus status;\n-    private final Fate.FateOperation txName;\n+    private final Fate.FateOperation fateOp;\n     private final List<String> hlocks;\n     private final List<String> wlocks;\n     private final String top;\n     private final long timeCreated;\n \n     private TransactionStatus(FateId fateId, FateInstanceType instanceType, TStatus status,\n-        Fate.FateOperation txName, List<String> hlocks, List<String> wlocks, String top,\n+        Fate.FateOperation fateOp, List<String> hlocks, List<String> wlocks, String top,\n         Long timeCreated) {\n \n       this.fateId = fateId;\n       this.instanceType = instanceType;\n       this.status = status;\n-      this.txName = txName;\n+      this.fateOp = fateOp;\n       this.hlocks = Collections.unmodifiableList(hlocks);\n       this.wlocks = Collections.unmodifiableList(wlocks);\n       this.top = top;\n@@ -105,8 +105,8 @@ public TStatus getStatus() {\n     /**\n      * @return The name of the transaction running.\n      */\n-    public Fate.FateOperation getTxName() {\n-      return txName;\n+    public Fate.FateOperation getFateOp() {\n+      return fateOp;\n     }\n \n     /**\n@@ -353,9 +353,9 @@ public static <T> FateStatus getTransactionStatus(\n         fateIds.forEach(fateId -> {\n \n           ReadOnlyFateTxStore<T> txStore = store.read(fateId);\n-          // tx name will not be set if the tx is not seeded with work (it is NEW)\n-          Fate.FateOperation txName = txStore.getTransactionInfo(Fate.TxInfo.TX_NAME) == null ? null\n-              : ((Fate.FateOperation) txStore.getTransactionInfo(Fate.TxInfo.TX_NAME));\n+          // fate op will not be set if the tx is not seeded with work (it is NEW)\n+          Fate.FateOperation fateOp = txStore.getTransactionInfo(Fate.TxInfo.FATE_OP) == null ? null\n+              : ((Fate.FateOperation) txStore.getTransactionInfo(Fate.TxInfo.FATE_OP));\n \n           List<String> hlocks = heldLocks.remove(fateId);\n \n@@ -381,7 +381,7 @@ public static <T> FateStatus getTransactionStatus(\n \n           if (includeByStatus(status, statusFilter) && includeByFateId(fateId, fateIdFilter)\n               && includeByInstanceType(fateId.getType(), typesFilter)) {\n-            statuses.add(new TransactionStatus(fateId, type, status, txName, hlocks, wlocks, top,\n+            statuses.add(new TransactionStatus(fateId, type, status, fateOp, hlocks, wlocks, top,\n                 timeCreated));\n           }\n         });\n@@ -418,7 +418,7 @@ public void print(Map<FateInstanceType,ReadOnlyFateStore<T>> readOnlyFateStores,\n     for (TransactionStatus txStatus : fateStatus.getTransactions()) {\n       fmt.format(\n           \""%-15s fateId: %s  status: %-18s locked: %-15s locking: %-15s op: %-15s created: %s%n\"",\n-          txStatus.getTxName(), txStatus.getFateId(), txStatus.getStatus(), txStatus.getHeldLocks(),\n+          txStatus.getFateOp(), txStatus.getFateId(), txStatus.getStatus(), txStatus.getHeldLocks(),\n           txStatus.getWaitingLocks(), txStatus.getTop(), txStatus.getTimeCreatedFormatted());\n     }\n     fmt.format(\"" %s transactions\"", fateStatus.getTransactions().size());\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 566dc5e9eed..f041c07313d 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -96,7 +96,7 @@ public class Fate<T> {\n   private final ConcurrentLinkedQueue<Integer> idleCountHistory = new ConcurrentLinkedQueue<>();\n \n   public enum TxInfo {\n-    TX_NAME, AUTO_CLEAN, EXCEPTION, TX_AGEOFF, RETURN_VALUE\n+    FATE_OP, AUTO_CLEAN, EXCEPTION, TX_AGEOFF, RETURN_VALUE\n   }\n \n   public enum FateOperation {\n@@ -538,17 +538,17 @@ public FateId startTransaction() {\n     return store.create();\n   }\n \n-  public void seedTransaction(FateOperation txName, FateKey fateKey, Repo<T> repo,\n+  public void seedTransaction(FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n-    store.seedTransaction(txName, fateKey, repo, autoCleanUp);\n+    store.seedTransaction(fateOp, fateKey, repo, autoCleanUp);\n   }\n \n   // start work in the transaction.. it is safe to call this\n   // multiple times for a transaction... but it will only seed once\n-  public void seedTransaction(FateOperation txName, FateId fateId, Repo<T> repo,\n+  public void seedTransaction(FateOperation fateOp, FateId fateId, Repo<T> repo,\n       boolean autoCleanUp, String goalMessage) {\n     log.info(\""Seeding {} {}\"", fateId, goalMessage);\n-    store.seedTransaction(txName, fateId, repo, autoCleanUp);\n+    store.seedTransaction(fateOp, fateId, repo, autoCleanUp);\n   }\n \n   // check on the transaction\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\nindex 0e8b4344ed6..25fc8fdd476 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n@@ -75,7 +75,7 @@ public interface FateMutator<T> {\n    */\n   FateMutator<T> putUnreserveTx(FateStore.FateReservation reservation);\n \n-  FateMutator<T> putName(byte[] data);\n+  FateMutator<T> putFateOp(byte[] data);\n \n   FateMutator<T> putAutoClean(byte[] data);\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\nindex ea7dd85c571..b742361ccfb 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n@@ -129,8 +129,8 @@ public FateMutator<T> putUnreserveTx(FateStore.FateReservation reservation) {\n   }\n \n   @Override\n-  public FateMutator<T> putName(byte[] data) {\n-    TxInfoColumnFamily.TX_NAME_COLUMN.put(mutation, new Value(data));\n+  public FateMutator<T> putFateOp(byte[] data) {\n+    TxInfoColumnFamily.FATE_OP_COLUMN.put(mutation, new Value(data));\n     return this;\n   }\n \n@@ -161,8 +161,8 @@ public FateMutator<T> putAgeOff(byte[] data) {\n   @Override\n   public FateMutator<T> putTxInfo(TxInfo txInfo, byte[] data) {\n     switch (txInfo) {\n-      case TX_NAME:\n-        putName(data);\n+      case FATE_OP:\n+        putFateOp(data);\n         break;\n       case AUTO_CLEAN:\n         putAutoClean(data);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java b/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\nindex 012e2853ff2..889ceac1e55 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\n@@ -44,8 +44,8 @@ public static class TxInfoColumnFamily {\n     public static final String STR_NAME = \""txinfo\"";\n     public static final Text NAME = new Text(STR_NAME);\n \n-    public static final String TX_NAME = \""txname\"";\n-    public static final ColumnFQ TX_NAME_COLUMN = new ColumnFQ(NAME, new Text(TX_NAME));\n+    public static final String FATE_OP = \""fateop\"";\n+    public static final ColumnFQ FATE_OP_COLUMN = new ColumnFQ(NAME, new Text(FATE_OP));\n \n     public static final String AUTO_CLEAN = \""autoclean\"";\n     public static final ColumnFQ AUTO_CLEAN_COLUMN = new ColumnFQ(NAME, new Text(AUTO_CLEAN));\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\nindex d25e87f15e9..be4b2aec298 100644\n--- a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n+++ b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n@@ -150,9 +150,9 @@ public FateId create() {\n       }\n \n       @Override\n-      public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey,\n+      public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey,\n           Repo<T> repo, boolean autoCleanUp) {\n-        var optional = store.seedTransaction(txName, fateKey, repo, autoCleanUp);\n+        var optional = store.seedTransaction(fateOp, fateKey, repo, autoCleanUp);\n         if (storeLog.isTraceEnabled()) {\n           optional.ifPresentOrElse(fateId -> {\n             storeLog.trace(\""{} seeded {} {} {}\"", fateId, fateKey, toLogString.apply(repo),\n@@ -166,9 +166,9 @@ public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateK\n       }\n \n       @Override\n-      public boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<T> repo,\n+      public boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<T> repo,\n           boolean autoCleanUp) {\n-        boolean seeded = store.seedTransaction(txName, fateId, repo, autoCleanUp);\n+        boolean seeded = store.seedTransaction(fateOp, fateId, repo, autoCleanUp);\n         if (storeLog.isTraceEnabled()) {\n           storeLog.trace(\""{} {} {} {}\"", fateId, seeded ? \""seeded\"" : \""unable to seed\"",\n               toLogString.apply(repo), autoCleanUp);\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateSummaryReport.java b/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateSummaryReport.java\nindex 13b252e693d..704536cd955 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateSummaryReport.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateSummaryReport.java\n@@ -88,7 +88,7 @@ public void gatherTxnStatus(AdminUtil.TransactionStatus txnStatus) {\n     }\n     String top = txnStatus.getTop();\n     stepCounts.merge(Objects.requireNonNullElse(top, \""?\""), 1, Integer::sum);\n-    Fate.FateOperation runningRepo = txnStatus.getTxName();\n+    Fate.FateOperation runningRepo = txnStatus.getFateOp();\n \n     cmdCounts.merge(runningRepo == null ? \""?\"" : runningRepo.name(), 1, Integer::sum);\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateTxnDetails.java b/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateTxnDetails.java\nindex 8d1218e6183..9fc832fbe79 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateTxnDetails.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/fateCommand/FateTxnDetails.java\n@@ -33,7 +33,7 @@ public class FateTxnDetails implements Comparable<FateTxnDetails> {\n \n   private long running;\n   private String status = \""?\"";\n-  private String txName = \""?\"";\n+  private String fateOp = \""?\"";\n   private String step = \""?\"";\n   private String fateId = \""?\"";\n   private List<String> locksHeld = List.of();\n@@ -73,8 +73,8 @@ public FateTxnDetails(final long reportTime, final AdminUtil.TransactionStatus t\n     if (txnStatus.getTop() != null) {\n       step = txnStatus.getTop();\n     }\n-    if (txnStatus.getTxName() != null) {\n-      txName = txnStatus.getTxName().name();\n+    if (txnStatus.getFateOp() != null) {\n+      fateOp = txnStatus.getFateOp().name();\n     }\n     if (txnStatus.getFateId() != null) {\n       fateId = txnStatus.getFateId().canonical();\n@@ -103,8 +103,8 @@ public long getRunning() {\n     return running;\n   }\n \n-  public String getTxName() {\n-    return txName;\n+  public String getFateOp() {\n+    return fateOp;\n   }\n \n   public String getStep() {\n@@ -167,7 +167,7 @@ public String toString() {\n     String hms = String.format(\""%d:%02d:%02d\"", elapsed.toHours(), elapsed.toMinutesPart(),\n         elapsed.toSecondsPart());\n \n-    return hms + \""\\t\"" + fateId + \""\\t\"" + status + \""\\t\"" + txName + \""\\t\"" + step + \""\\theld:\""\n+    return hms + \""\\t\"" + fateId + \""\\t\"" + status + \""\\t\"" + fateOp + \""\\t\"" + step + \""\\theld:\""\n         + locksHeld.toString() + \""\\twaiting:\"" + locksWaiting.toString();\n   }\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/metrics/fate/FateMetricValues.java b/server/manager/src/main/java/org/apache/accumulo/manager/metrics/fate/FateMetricValues.java\nindex 832af642788..8dc94d48591 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/metrics/fate/FateMetricValues.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/metrics/fate/FateMetricValues.java\n@@ -102,7 +102,7 @@ public Map<String,Long> getOpTypeCounters() {\n \n       // incr count for op type for for in_progress transactions.\n       if (ReadOnlyFateStore.TStatus.IN_PROGRESS.equals(tx.getStatus())) {\n-        Fate.FateOperation opType = tx.getTxName();\n+        Fate.FateOperation opType = tx.getFateOp();\n         String opTypeStr = opType == null ? \""UNKNOWN\"" : opType.name();\n         opTypeCounters.merge(opTypeStr, 1L, Long::sum);\n       }\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex 160d8727127..ef3ea18e5af 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -54,7 +54,7 @@ public interface FateStore<T> extends ReadOnlyFateStore<T> {\n    * fateKey. If seeded, sets the following data for the fateId in the store.\n    *\n    * <ul>\n-   * <li>Set the tx name</li>\n+   * <li>Set the fate op</li>\n    * <li>Set the status to SUBMITTED</li>\n    * <li>Set the fate key</li>\n    * <li>Sets autocleanup only if true</li>\n@@ -66,7 +66,7 @@ public interface FateStore<T> extends ReadOnlyFateStore<T> {\n    *         empty optional otherwise. If there was a failure this could return an empty optional\n    *         when it actually succeeded.\n    */\n-  Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey, Repo<T> repo,\n+  Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp);\n \n   /**\n@@ -74,7 +74,7 @@ Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey, Rep\n    * unreserved. If seeded, sets the following data for the fateId in the store.\n    *\n    * <ul>\n-   * <li>Set the tx name</li>\n+   * <li>Set the fate op</li>\n    * <li>Set the status to SUBMITTED</li>\n    * <li>Sets autocleanup only if true</li>\n    * <li>Sets the creation time</li>\n@@ -84,7 +84,7 @@ Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey, Rep\n    *         failures. When there are no failures returns true if seeded and false otherwise. If\n    *         there was a failure this could return false when it actually succeeded.\n    */\n-  boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<T> repo,\n+  boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<T> repo,\n       boolean autoCleanUp);\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex 8268163dadd..e444d2c0023 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -133,12 +133,12 @@ public FateId getFateId() {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey, Repo<T> repo,\n+  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n     final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n     Supplier<FateMutator<T>> mutatorFactory = () -> newMutator(fateId).requireAbsent()\n         .putKey(fateKey).putCreateTime(System.currentTimeMillis());\n-    if (seedTransaction(mutatorFactory, fateKey + \"" \"" + fateId, txName, repo, autoCleanUp)) {\n+    if (seedTransaction(mutatorFactory, fateKey + \"" \"" + fateId, fateOp, repo, autoCleanUp)) {\n       return Optional.of(fateId);\n     } else {\n       return Optional.empty();\n@@ -146,20 +146,20 @@ public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateK\n   }\n \n   @Override\n-  public boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<T> repo,\n+  public boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<T> repo,\n       boolean autoCleanUp) {\n     Supplier<FateMutator<T>> mutatorFactory =\n         () -> newMutator(fateId).requireStatus(TStatus.NEW).requireUnreserved().requireAbsentKey();\n-    return seedTransaction(mutatorFactory, fateId.canonical(), txName, repo, autoCleanUp);\n+    return seedTransaction(mutatorFactory, fateId.canonical(), fateOp, repo, autoCleanUp);\n   }\n \n   private boolean seedTransaction(Supplier<FateMutator<T>> mutatorFactory, String logId,\n-      Fate.FateOperation txName, Repo<T> repo, boolean autoCleanUp) {\n+      Fate.FateOperation fateOp, Repo<T> repo, boolean autoCleanUp) {\n     int maxAttempts = 5;\n     for (int attempt = 0; attempt < maxAttempts; attempt++) {\n       var mutator = mutatorFactory.get();\n       mutator =\n-          mutator.putName(serializeTxInfo(txName)).putRepo(1, repo).putStatus(TStatus.SUBMITTED);\n+          mutator.putFateOp(serializeTxInfo(fateOp)).putRepo(1, repo).putStatus(TStatus.SUBMITTED);\n       if (autoCleanUp) {\n         mutator = mutator.putAutoClean(serializeTxInfo(autoCleanUp));\n       }\n@@ -420,8 +420,8 @@ public Serializable getTransactionInfo(TxInfo txInfo) {\n \n         final ColumnFQ cq;\n         switch (txInfo) {\n-          case TX_NAME:\n-            cq = TxInfoColumnFamily.TX_NAME_COLUMN;\n+          case FATE_OP:\n+            cq = TxInfoColumnFamily.FATE_OP_COLUMN;\n             break;\n           case AUTO_CLEAN:\n             cq = TxInfoColumnFamily.AUTO_CLEAN_COLUMN;\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex 242e0b5e74f..35fdf07c317 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -181,11 +181,11 @@ private Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey, Repo<T> repo,\n+  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey, Repo<T> repo,\n       boolean autoCleanUp) {\n     return createAndReserve(fateKey).map(txStore -> {\n       try {\n-        seedTransaction(txName, repo, autoCleanUp, txStore);\n+        seedTransaction(fateOp, repo, autoCleanUp, txStore);\n         return txStore.getID();\n       } finally {\n         txStore.unreserve(Duration.ZERO);\n@@ -194,12 +194,12 @@ public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateK\n   }\n \n   @Override\n-  public boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<T> repo,\n+  public boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<T> repo,\n       boolean autoCleanUp) {\n     return tryReserve(fateId).map(txStore -> {\n       try {\n         if (txStore.getStatus() == NEW) {\n-          seedTransaction(txName, repo, autoCleanUp, txStore);\n+          seedTransaction(fateOp, repo, autoCleanUp, txStore);\n           return true;\n         }\n         return false;\n@@ -209,7 +209,7 @@ public boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<T>\n     }).orElse(false);\n   }\n \n-  private void seedTransaction(Fate.FateOperation txName, Repo<T> repo, boolean autoCleanUp,\n+  private void seedTransaction(Fate.FateOperation fateOp, Repo<T> repo, boolean autoCleanUp,\n       FateTxStore<T> txStore) {\n     if (txStore.top() == null) {\n       try {\n@@ -223,7 +223,7 @@ private void seedTransaction(Fate.FateOperation txName, Repo<T> repo, boolean au\n     if (autoCleanUp) {\n       txStore.setTransactionInfo(TxInfo.AUTO_CLEAN, autoCleanUp);\n     }\n-    txStore.setTransactionInfo(TxInfo.TX_NAME, txName);\n+    txStore.setTransactionInfo(TxInfo.FATE_OP, fateOp);\n     txStore.setStatus(SUBMITTED);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\nindex 1d2389d6fbc..7e339a2e813 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n@@ -53,13 +53,13 @@ public FateId create() {\n   }\n \n   @Override\n-  public Optional<FateId> seedTransaction(Fate.FateOperation txName, FateKey fateKey,\n+  public Optional<FateId> seedTransaction(Fate.FateOperation fateOp, FateKey fateKey,\n       Repo<String> repo, boolean autoCleanUp) {\n     return Optional.empty();\n   }\n \n   @Override\n-  public boolean seedTransaction(Fate.FateOperation txName, FateId fateId, Repo<String> repo,\n+  public boolean seedTransaction(Fate.FateOperation fateOp, FateId fateId, Repo<String> repo,\n       boolean autoCleanUp) {\n     return false;\n   }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java\nindex 8b880b87679..eeb00627699 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/SummaryReportTest.java\n@@ -73,7 +73,7 @@ public void noTablenameReport() {\n     expect(status1.getTimeCreated()).andReturn(now - TimeUnit.DAYS.toMillis(1)).anyTimes();\n     expect(status1.getStatus()).andReturn(ReadOnlyFateStore.TStatus.IN_PROGRESS).anyTimes();\n     expect(status1.getTop()).andReturn(null).anyTimes();\n-    expect(status1.getTxName()).andReturn(null).anyTimes();\n+    expect(status1.getFateOp()).andReturn(null).anyTimes();\n     expect(status1.getFateId()).andReturn(FateId.from(\""FATE:USER:\"" + UUID.randomUUID())).anyTimes();\n     expect(status1.getHeldLocks()).andReturn(List.of()).anyTimes();\n     expect(status1.getWaitingLocks()).andReturn(List.of()).anyTimes();\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java\nindex c8f2573bab3..c04ce582b88 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/fateCommand/TxnDetailsTest.java\n@@ -58,7 +58,7 @@ void orderingByDuration() {\n     expect(status1.getTimeCreated()).andReturn(now - TimeUnit.DAYS.toMillis(1)).anyTimes();\n     expect(status1.getStatus()).andReturn(ReadOnlyFateStore.TStatus.IN_PROGRESS).anyTimes();\n     expect(status1.getTop()).andReturn(\""step1\"").anyTimes();\n-    expect(status1.getTxName()).andReturn(Fate.FateOperation.TABLE_CREATE).anyTimes();\n+    expect(status1.getFateOp()).andReturn(Fate.FateOperation.TABLE_CREATE).anyTimes();\n     expect(status1.getFateId()).andReturn(FateId.from(\""FATE:USER:\"" + uuid1)).anyTimes();\n     expect(status1.getHeldLocks()).andReturn(List.of()).anyTimes();\n     expect(status1.getWaitingLocks()).andReturn(List.of()).anyTimes();\n@@ -67,7 +67,7 @@ void orderingByDuration() {\n     expect(status2.getTimeCreated()).andReturn(now - TimeUnit.DAYS.toMillis(7)).anyTimes();\n     expect(status2.getStatus()).andReturn(ReadOnlyFateStore.TStatus.IN_PROGRESS).anyTimes();\n     expect(status2.getTop()).andReturn(\""step2\"").anyTimes();\n-    expect(status2.getTxName()).andReturn(Fate.FateOperation.TABLE_DELETE).anyTimes();\n+    expect(status2.getFateOp()).andReturn(Fate.FateOperation.TABLE_DELETE).anyTimes();\n     expect(status2.getFateId()).andReturn(FateId.from(\""FATE:USER:\"" + uuid2)).anyTimes();\n     expect(status2.getHeldLocks()).andReturn(List.of()).anyTimes();\n     expect(status2.getWaitingLocks()).andReturn(List.of()).anyTimes();\n@@ -101,7 +101,7 @@ public void lockTest() {\n     expect(status1.getTimeCreated()).andReturn(now - TimeUnit.DAYS.toMillis(1)).anyTimes();\n     expect(status1.getStatus()).andReturn(ReadOnlyFateStore.TStatus.IN_PROGRESS).anyTimes();\n     expect(status1.getTop()).andReturn(\""step1\"").anyTimes();\n-    expect(status1.getTxName()).andReturn(Fate.FateOperation.TABLE_COMPACT).anyTimes();\n+    expect(status1.getFateOp()).andReturn(Fate.FateOperation.TABLE_COMPACT).anyTimes();\n     expect(status1.getFateId()).andReturn(FateId.from(\""FATE:USER:\"" + UUID.randomUUID())).anyTimes();\n     // incomplete lock info (W unknown ns id, no table))\n     expect(status1.getHeldLocks()).andReturn(List.of(\""R:1\"", \""R:2\"", \""W:a\"")).anyTimes();\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\nindex 4b377e09da8..ec48e6f9229 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n@@ -227,7 +227,7 @@ protected void testInterleaving(FateStore<FeoTestEnv> store, ServerContext sctx)\n       var txStore = store.reserve(fateIds[i]);\n       try {\n         txStore.push(new FirstOp());\n-        txStore.setTransactionInfo(TxInfo.TX_NAME, \""TEST_\"" + i);\n+        txStore.setTransactionInfo(TxInfo.FATE_OP, \""TEST_\"" + i);\n         txStore.setStatus(SUBMITTED);\n       } finally {\n         txStore.unreserve(Duration.ZERO);\n@@ -359,7 +359,7 @@ protected void testNonInterleaving(FateStore<FeoTestEnv> store, ServerContext sc\n       var txStore = store.reserve(fateIds[i]);\n       try {\n         txStore.push(new FirstNonInterleavingOp());\n-        txStore.setTransactionInfo(TxInfo.TX_NAME, \""TEST_\"" + i);\n+        txStore.setTransactionInfo(TxInfo.FATE_OP, \""TEST_\"" + i);\n         txStore.setStatus(SUBMITTED);\n       } finally {\n         txStore.unreserve(Duration.ZERO);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\nindex 292bc477f0e..b76709cd750 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n@@ -532,14 +532,14 @@ protected void testNoWriteAfterDelete(FateStore<TestEnv> store, ServerContext sc\n     assertDoesNotThrow(() -> txStore.push(repo));\n     assertDoesNotThrow(() -> txStore.setStatus(ReadOnlyFateStore.TStatus.SUCCESSFUL));\n     assertDoesNotThrow(txStore::pop);\n-    assertDoesNotThrow(() -> txStore.setTransactionInfo(Fate.TxInfo.TX_NAME, \""name\""));\n+    assertDoesNotThrow(() -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, \""name\""));\n     assertDoesNotThrow(txStore::delete);\n \n     // test that all write ops result in an exception since the tx has been deleted\n     assertThrows(Exception.class, () -> txStore.push(repo));\n     assertThrows(Exception.class, () -> txStore.setStatus(ReadOnlyFateStore.TStatus.SUCCESSFUL));\n     assertThrows(Exception.class, txStore::pop);\n-    assertThrows(Exception.class, () -> txStore.setTransactionInfo(Fate.TxInfo.TX_NAME, \""name\""));\n+    assertThrows(Exception.class, () -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, \""name\""));\n     assertThrows(Exception.class, txStore::delete);\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\nindex b0f3d7ffeae..bcbf599ebde 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\n@@ -466,7 +466,7 @@ protected void testTransactionNameAndStep(FateStore<LatchTestEnv> store, ServerC\n       // Validate transaction name and transaction step from summary command\n \n       for (FateTxnDetails d : report.getFateDetails()) {\n-        assertEquals(\""TABLE_COMPACT\"", d.getTxName());\n+        assertEquals(\""TABLE_COMPACT\"", d.getFateOp());\n         assertEquals(\""CompactionDriver\"", d.getStep());\n         fateIdsStarted.add(d.getFateId());\n       }\n@@ -743,7 +743,7 @@ protected void testFatePrintAndSummaryCommandsWithInProgressTxns(FateStore<Latch\n         List.of(ReadOnlyFateStore.TStatus.NEW, ReadOnlyFateStore.TStatus.UNKNOWN,\n             ReadOnlyFateStore.TStatus.NEW));\n     // None of them should have a name since none of them were seeded with work\n-    assertEquals(status.getTransactions().stream().map(AdminUtil.TransactionStatus::getTxName)\n+    assertEquals(status.getTransactions().stream().map(AdminUtil.TransactionStatus::getFateOp)\n         .collect(Collectors.toList()), Arrays.asList(null, null, null));\n     // None of them should have a Repo since none of them were seeded with work\n     assertEquals(status.getTransactions().stream().map(AdminUtil.TransactionStatus::getTop)\n@@ -832,7 +832,7 @@ private void validateFateDetails(Set<FateTxnDetails> details, int expDetailsSize\n       assertTrue(fateIdsStarted.contains(d.getFateId()));\n       assertEquals(\""NEW\"", d.getStatus());\n       assertEquals(\""?\"", d.getStep());\n-      assertEquals(\""?\"", d.getTxName());\n+      assertEquals(\""?\"", d.getFateOp());\n       assertNotEquals(0, d.getRunning());\n       assertEquals(\""[]\"", d.getLocksHeld().toString());\n       assertEquals(\""[]\"", d.getLocksWaiting().toString());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\nindex d029ebb4892..bdeb5d6695d 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n@@ -101,8 +101,8 @@ protected void testReadWrite(FateStore<TestEnv> store, ServerContext sctx)\n     assertEquals(TStatus.SUBMITTED, txStore.getStatus());\n \n     // Set a name to test setTransactionInfo()\n-    txStore.setTransactionInfo(TxInfo.TX_NAME, \""name\"");\n-    assertEquals(\""name\"", txStore.getTransactionInfo(TxInfo.TX_NAME));\n+    txStore.setTransactionInfo(TxInfo.FATE_OP, \""name\"");\n+    assertEquals(\""name\"", txStore.getTransactionInfo(TxInfo.FATE_OP));\n \n     // Try setting a second test op to test getStack()\n     // when listing or popping TestOperation2 should be first\n@@ -652,7 +652,7 @@ protected void testAbsent(FateStore<TestEnv> store, ServerContext sctx) {\n     assertTrue(store.tryReserve(fateId).isEmpty());\n     assertEquals(TStatus.UNKNOWN, txStore.getStatus());\n     assertNull(txStore.top());\n-    assertNull(txStore.getTransactionInfo(TxInfo.TX_NAME));\n+    assertNull(txStore.getTransactionInfo(TxInfo.FATE_OP));\n     assertEquals(0, txStore.timeCreated());\n     assertEquals(Optional.empty(), txStore.getKey());\n     assertEquals(fateId, txStore.getID());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java b/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\nindex ac2b0b7a107..40690b87335 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java\n@@ -421,10 +421,10 @@ private boolean isCompaction(AdminUtil.TransactionStatus tx) {\n     log.trace(\""Fate id: {}, status: {}\"", tx.getFateId(), tx.getStatus());\n \n     String top = tx.getTop();\n-    Fate.FateOperation txName = tx.getTxName();\n+    Fate.FateOperation fateOp = tx.getFateOp();\n \n-    return top != null && txName != null && top.contains(\""CompactionDriver\"")\n-        && txName == Fate.FateOperation.TABLE_COMPACT;\n+    return top != null && fateOp != null && top.contains(\""CompactionDriver\"")\n+        && fateOp == Fate.FateOperation.TABLE_COMPACT;\n   }\n \n   /**\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5335"", ""pr_id"": 5335, ""issue_id"": 5254, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Filter based on columns declared in TabletMetadataCheck\nIn #5247 a new interface was introduced for atomically checking tablet metadata to see if a tablet metadata could be updated.  This interface allows declaring columns that should be read for the check. Currently in the code nothing is done with those columns and all the tablet columns are read.  The code should be updated to only read the requested columns to avoid parsing unneeded data and loading it into memory for the check."", ""issue_word_count"": 81, ""test_files_count"": 2, ""non_test_files_count"": 5, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheck.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java"", ""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheckTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/TabletMetadataCheckIterator.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionReservationCheck.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheckTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java""], ""base_commit"": ""0db1fd914439109d9df4b61c04e6f24e6c5d42a9"", ""head_commit"": ""a6800cd78e38cbc019c5c61086552657594b2939"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5335"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5335"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-14T22:20:58.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\nindex 0efebce8dbc..c737b0d66be 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n@@ -18,15 +18,28 @@\n  */\n package org.apache.accumulo.core.metadata.schema;\n \n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.MergedColumnFamily.MERGED_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.DIRECTORY_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.DIRECTORY_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_NONCE_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_NONCE_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SplitColumnFamily.UNSPLITTABLE_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SuspendLocationColumn.SUSPEND_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.AVAILABILITY_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.AVAILABILITY_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.MERGEABILITY_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.MERGEABILITY_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.PREV_ROW_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.PREV_ROW_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.REQUESTED_COLUMN;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.REQUESTED_QUAL;\n \n import java.util.Collection;\n@@ -40,10 +53,12 @@\n import java.util.Optional;\n import java.util.OptionalLong;\n import java.util.Set;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.client.admin.TabletAvailability;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.clientImpl.TabletAvailabilityUtil;\n+import org.apache.accumulo.core.data.ArrayByteSequence;\n import org.apache.accumulo.core.data.ByteSequence;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.TableId;\n@@ -58,6 +73,7 @@\n import org.apache.accumulo.core.metadata.StoredTabletFile;\n import org.apache.accumulo.core.metadata.SuspendingTServer;\n import org.apache.accumulo.core.metadata.TServerInstance;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.BulkFileColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ClonedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.CompactedColumnFamily;\n@@ -75,6 +91,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.tabletserver.log.LogEntry;\n+import org.apache.accumulo.core.util.ColumnFQ;\n import org.apache.accumulo.core.zookeeper.ZcStat;\n import org.apache.commons.lang3.builder.ToStringBuilder;\n import org.apache.commons.lang3.builder.ToStringStyle;\n@@ -202,7 +219,152 @@ public enum ColumnType {\n     COMPACTED,\n     USER_COMPACTION_REQUESTED,\n     UNSPLITTABLE,\n-    MERGEABILITY\n+    MERGEABILITY;\n+\n+    public static final Map<ColumnType,Set<Text>> COLUMNS_TO_FAMILIES;\n+    public static final Map<ColumnType,ColumnFQ> COLUMNS_TO_QUALIFIERS;\n+\n+    static {\n+      ImmutableMap.Builder<ColumnType,Set<Text>> colsToFamilies = ImmutableMap.builder();\n+      ImmutableMap.Builder<ColumnType,ColumnFQ> colsToQualifiers = ImmutableMap.builder();\n+\n+      for (ColumnType column : EnumSet.allOf(ColumnType.class)) {\n+        switch (column) {\n+          case CLONED:\n+            colsToFamilies.put(column, Set.of(ClonedColumnFamily.NAME));\n+            break;\n+          case DIR:\n+          case FLUSH_ID:\n+          case TIME:\n+          case OPID:\n+          case SELECTED:\n+          case FLUSH_NONCE:\n+            colsToFamilies.put(column, Set.of(ServerColumnFamily.NAME));\n+            break;\n+          case FILES:\n+            colsToFamilies.put(column, Set.of(DataFileColumnFamily.NAME));\n+            break;\n+          case AVAILABILITY:\n+          case HOSTING_REQUESTED:\n+          case PREV_ROW:\n+          case MERGEABILITY:\n+            colsToFamilies.put(column, Set.of(TabletColumnFamily.NAME));\n+            break;\n+          case LAST:\n+            colsToFamilies.put(column, Set.of(LastLocationColumnFamily.NAME));\n+            break;\n+          case LOADED:\n+            colsToFamilies.put(column, Set.of(BulkFileColumnFamily.NAME));\n+            break;\n+          case LOCATION:\n+            colsToFamilies.put(column,\n+                Set.of(CurrentLocationColumnFamily.NAME, FutureLocationColumnFamily.NAME));\n+            break;\n+          case LOGS:\n+            colsToFamilies.put(column, Set.of(LogColumnFamily.NAME));\n+            break;\n+          case SCANS:\n+            colsToFamilies.put(column, Set.of(ScanFileColumnFamily.NAME));\n+            break;\n+          case SUSPEND:\n+            colsToFamilies.put(column, Set.of(SuspendLocationColumn.NAME));\n+            break;\n+          case ECOMP:\n+            colsToFamilies.put(column, Set.of(ExternalCompactionColumnFamily.NAME));\n+            break;\n+          case MERGED:\n+            colsToFamilies.put(column, Set.of(MergedColumnFamily.NAME));\n+            break;\n+          case COMPACTED:\n+            colsToFamilies.put(column, Set.of(CompactedColumnFamily.NAME));\n+            break;\n+          case USER_COMPACTION_REQUESTED:\n+            colsToFamilies.put(column, Set.of(UserCompactionRequestedColumnFamily.NAME));\n+            break;\n+          case UNSPLITTABLE:\n+            colsToFamilies.put(column, Set.of(SplitColumnFamily.NAME));\n+            break;\n+          default:\n+            throw new IllegalArgumentException(\""Unknown col type \"" + column);\n+        }\n+      }\n+\n+      for (ColumnType column : EnumSet.allOf(ColumnType.class)) {\n+        switch (column) {\n+          case CLONED:\n+          case COMPACTED:\n+          case ECOMP:\n+          case FILES:\n+          case LAST:\n+          case LOADED:\n+          case LOCATION:\n+          case LOGS:\n+          case SCANS:\n+          case USER_COMPACTION_REQUESTED:\n+            break;\n+          case DIR:\n+            colsToQualifiers.put(column, DIRECTORY_COLUMN);\n+            break;\n+          case FLUSH_ID:\n+            colsToQualifiers.put(column, FLUSH_COLUMN);\n+            break;\n+          case TIME:\n+            colsToQualifiers.put(column, TIME_COLUMN);\n+            break;\n+          case OPID:\n+            colsToQualifiers.put(column, OPID_COLUMN);\n+            break;\n+          case SELECTED:\n+            colsToQualifiers.put(column, SELECTED_COLUMN);\n+            break;\n+          case FLUSH_NONCE:\n+            colsToQualifiers.put(column, FLUSH_NONCE_COLUMN);\n+            break;\n+          case AVAILABILITY:\n+            colsToQualifiers.put(column, AVAILABILITY_COLUMN);\n+            break;\n+          case HOSTING_REQUESTED:\n+            colsToQualifiers.put(column, REQUESTED_COLUMN);\n+            break;\n+          case PREV_ROW:\n+            colsToQualifiers.put(column, PREV_ROW_COLUMN);\n+            break;\n+          case MERGEABILITY:\n+            colsToQualifiers.put(column, MERGEABILITY_COLUMN);\n+            break;\n+          case SUSPEND:\n+            colsToQualifiers.put(column, SUSPEND_COLUMN);\n+            break;\n+          case MERGED:\n+            colsToQualifiers.put(column, MERGED_COLUMN);\n+            break;\n+          case UNSPLITTABLE:\n+            colsToQualifiers.put(column, UNSPLITTABLE_COLUMN);\n+            break;\n+          default:\n+            throw new IllegalArgumentException(\""Unknown col type \"" + column);\n+        }\n+      }\n+\n+      COLUMNS_TO_FAMILIES = colsToFamilies.build();\n+      COLUMNS_TO_QUALIFIERS = colsToQualifiers.build();\n+    }\n+\n+    public static Set<ByteSequence> resolveFamilies(Set<ColumnType> columns) {\n+      return columns.stream()\n+          .flatMap(cf -> COLUMNS_TO_FAMILIES.get(cf).stream()\n+              .map(family -> new ArrayByteSequence(family.copyBytes())))\n+          .collect(Collectors.toUnmodifiableSet());\n+    }\n+\n+    public static Set<Text> resolveFamiliesAsText(ColumnType column) {\n+      return COLUMNS_TO_FAMILIES.get(column);\n+    }\n+\n+    public static ColumnFQ resolveQualifier(ColumnType columnType) {\n+      return COLUMNS_TO_QUALIFIERS.get(columnType);\n+    }\n+\n   }\n \n   public static class Location {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheck.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheck.java\nindex 8aed438fe83..e3cd96120e2 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheck.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheck.java\n@@ -20,8 +20,12 @@\n \n import java.util.Collections;\n import java.util.EnumSet;\n+import java.util.Objects;\n import java.util.Set;\n \n+import org.apache.accumulo.core.data.ByteSequence;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n+\n /**\n  * This interface facilitates atomic checks of tablet metadata prior to updating tablet metadata.\n  * The way it is intended to be used is the following.\n@@ -45,13 +49,35 @@ public interface TabletMetadataCheck {\n   Set<TabletMetadata.ColumnType> ALL_COLUMNS =\n       Collections.unmodifiableSet(EnumSet.allOf(TabletMetadata.ColumnType.class));\n \n+  ResolvedColumns ALL_RESOLVED_COLUMNS = new ResolvedColumns(ALL_COLUMNS);\n+\n   boolean canUpdate(TabletMetadata tabletMetadata);\n \n   /**\n-   * Determines what tablet metadata columns are read on the server side. Return\n-   * {@link #ALL_COLUMNS} to read all of a tablets metadata.\n+   * Determines what tablet metadata columns/families are read on the server side. Return\n+   * {@link #ALL_RESOLVED_COLUMNS} to read all of a tablets metadata. If all columns are included,\n+   * the families set will be empty which means read all families.\n    */\n-  default Set<TabletMetadata.ColumnType> columnsToRead() {\n-    return ALL_COLUMNS;\n+  default ResolvedColumns columnsToRead() {\n+    return ALL_RESOLVED_COLUMNS;\n+  }\n+\n+  class ResolvedColumns {\n+    private final Set<TabletMetadata.ColumnType> columns;\n+    private final Set<ByteSequence> families;\n+\n+    public ResolvedColumns(Set<ColumnType> columns) {\n+      this.columns = Objects.requireNonNull(columns);\n+      this.families = columns.equals(ALL_COLUMNS) ? Set.of() : ColumnType.resolveFamilies(columns);\n+    }\n+\n+    public EnumSet<ColumnType> getColumns() {\n+      return EnumSet.copyOf(columns);\n+    }\n+\n+    public Set<ByteSequence> getFamilies() {\n+      return families;\n+    }\n   }\n+\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\nindex 04115dfaeac..26d157eb195 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\n@@ -21,12 +21,7 @@\n import static com.google.common.base.Preconditions.checkState;\n import static java.util.stream.Collectors.groupingBy;\n import static java.util.stream.Collectors.toList;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.DIRECTORY_COLUMN;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.FLUSH_COLUMN;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.OPID_COLUMN;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_COLUMN;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_COLUMN;\n-import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.PREV_ROW_COLUMN;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n@@ -64,21 +59,6 @@\n import org.apache.accumulo.core.metadata.AccumuloTable;\n import org.apache.accumulo.core.metadata.schema.Ample.DataLevel;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.BulkFileColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ClonedColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.CompactedColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.CurrentLocationColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.FutureLocationColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.LastLocationColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.LogColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.MergedColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ScanFileColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SplitColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SuspendLocationColumn;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n-import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.filters.TabletMetadataFilter;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -325,84 +305,15 @@ public Options checkConsistency() {\n     @Override\n     public Options fetch(ColumnType... colsToFetch) {\n       Preconditions.checkArgument(colsToFetch.length > 0);\n-\n-      for (ColumnType colToFetch : colsToFetch) {\n-\n-        fetchedCols.add(colToFetch);\n-\n-        switch (colToFetch) {\n-          case CLONED:\n-            families.add(ClonedColumnFamily.NAME);\n-            break;\n-          case DIR:\n-            qualifiers.add(DIRECTORY_COLUMN);\n-            break;\n-          case FILES:\n-            families.add(DataFileColumnFamily.NAME);\n-            break;\n-          case FLUSH_ID:\n-            qualifiers.add(FLUSH_COLUMN);\n-            break;\n-          case AVAILABILITY:\n-            qualifiers.add(TabletsSection.TabletColumnFamily.AVAILABILITY_COLUMN);\n-            break;\n-          case HOSTING_REQUESTED:\n-            qualifiers.add(TabletsSection.TabletColumnFamily.REQUESTED_COLUMN);\n-            break;\n-          case LAST:\n-            families.add(LastLocationColumnFamily.NAME);\n-            break;\n-          case LOADED:\n-            families.add(BulkFileColumnFamily.NAME);\n-            break;\n-          case LOCATION:\n-            families.add(CurrentLocationColumnFamily.NAME);\n-            families.add(FutureLocationColumnFamily.NAME);\n-            break;\n-          case LOGS:\n-            families.add(LogColumnFamily.NAME);\n-            break;\n-          case PREV_ROW:\n-            qualifiers.add(PREV_ROW_COLUMN);\n-            break;\n-          case SCANS:\n-            families.add(ScanFileColumnFamily.NAME);\n-            break;\n-          case SUSPEND:\n-            families.add(SuspendLocationColumn.SUSPEND_COLUMN.getColumnFamily());\n-            break;\n-          case TIME:\n-            qualifiers.add(TIME_COLUMN);\n-            break;\n-          case ECOMP:\n-            families.add(ExternalCompactionColumnFamily.NAME);\n-            break;\n-          case MERGED:\n-            families.add(MergedColumnFamily.NAME);\n-            break;\n-          case OPID:\n-            qualifiers.add(OPID_COLUMN);\n-            break;\n-          case SELECTED:\n-            qualifiers.add(SELECTED_COLUMN);\n-            break;\n-          case COMPACTED:\n-            families.add(CompactedColumnFamily.NAME);\n-            break;\n-          case USER_COMPACTION_REQUESTED:\n-            families.add(UserCompactionRequestedColumnFamily.NAME);\n-            break;\n-          case UNSPLITTABLE:\n-            qualifiers.add(SplitColumnFamily.UNSPLITTABLE_COLUMN);\n-            break;\n-          case MERGEABILITY:\n-            qualifiers.add(TabletColumnFamily.MERGEABILITY_COLUMN);\n-            break;\n-          default:\n-            throw new IllegalArgumentException(\""Unknown col type \"" + colToFetch);\n+      for (var col : fetchedCols) {\n+        fetchedCols.add(col);\n+        var qualifier = ColumnType.resolveQualifier(col);\n+        if (qualifier != null) {\n+          qualifiers.add(qualifier);\n+        } else {\n+          families.addAll(ColumnType.resolveFamiliesAsText(col));\n         }\n       }\n-\n       return this;\n     }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/TabletMetadataCheckIterator.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/TabletMetadataCheckIterator.java\nindex ba08be04a70..e149d025964 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/TabletMetadataCheckIterator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/TabletMetadataCheckIterator.java\n@@ -23,11 +23,9 @@\n \n import java.io.IOException;\n import java.util.Collection;\n-import java.util.EnumSet;\n import java.util.Map;\n import java.util.NoSuchElementException;\n import java.util.Objects;\n-import java.util.Set;\n \n import org.apache.accumulo.core.classloader.ClassLoaderUtil;\n import org.apache.accumulo.core.client.IteratorSetting;\n@@ -98,11 +96,12 @@ public void seek(Range range, Collection<ByteSequence> columnFamilies, boolean i\n \n     var colsToRead = check.columnsToRead();\n \n-    source.seek(new Range(tabletRow), Set.of(), false);\n+    source.seek(new Range(tabletRow), colsToRead.getFamilies(),\n+        !colsToRead.getFamilies().isEmpty());\n \n     if (source.hasTop()) {\n       var tabletMetadata = TabletMetadata.convertRow(new IteratorAdapter(source),\n-          EnumSet.copyOf(colsToRead), false, false);\n+          colsToRead.getColumns(), false, false);\n \n       // TODO checking the prev end row here is redundant w/ other checks that ample currently\n       // does.. however we could try to make all checks eventually use this class\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionReservationCheck.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionReservationCheck.java\nindex f9c2d5841c3..f1cd9e6e839 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionReservationCheck.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionReservationCheck.java\n@@ -35,7 +35,6 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.metadata.StoredTabletFile;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n-import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.TabletMetadataCheck;\n import org.apache.accumulo.core.spi.compaction.CompactionKind;\n import org.apache.accumulo.core.util.time.SteadyTime;\n@@ -51,6 +50,11 @@ public class CompactionReservationCheck implements TabletMetadataCheck {\n \n   private static final Logger log = LoggerFactory.getLogger(CompactionReservationCheck.class);\n \n+  // Cache the ResolvedColumns statically so we do not need to recreate the set of\n+  // columns and families for each mutation\n+  private static final ResolvedColumns RESOLVED_COLUMNS = new ResolvedColumns(\n+      Set.of(PREV_ROW, OPID, SELECTED, FILES, ECOMP, USER_COMPACTION_REQUESTED));\n+\n   private CompactionKind kind;\n   private List<String> jobFilesStr;\n   private Long steadyTimeNanos;\n@@ -163,7 +167,7 @@ public boolean canUpdate(TabletMetadata tablet) {\n   }\n \n   @Override\n-  public Set<ColumnType> columnsToRead() {\n-    return Set.of(PREV_ROW, OPID, SELECTED, FILES, ECOMP, USER_COMPACTION_REQUESTED);\n+  public ResolvedColumns columnsToRead() {\n+    return RESOLVED_COLUMNS;\n   }\n }\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheckTest.java b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheckTest.java\nnew file mode 100644\nindex 00000000000..8ca09257860\n--- /dev/null\n+++ b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataCheckTest.java\n@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.metadata.schema;\n+\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.DIR;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.ECOMP;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.SCANS;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.SELECTED;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.util.EnumSet;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import org.apache.accumulo.core.data.ArrayByteSequence;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ScanFileColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadataCheck.ResolvedColumns;\n+import org.junit.jupiter.api.Test;\n+\n+public class TabletMetadataCheckTest {\n+\n+  @Test\n+  public void testResolvedColumns() {\n+    // check columns is empty set when all columns is used\n+    var resolved1 = new ResolvedColumns(TabletMetadataCheck.ALL_COLUMNS);\n+    assertTrue(resolved1.getFamilies().isEmpty());\n+    assertTrue(TabletMetadataCheck.ALL_RESOLVED_COLUMNS.getFamilies().isEmpty());\n+\n+    // Add some column types and verify resolved families is not empty and is correct\n+    var expectedColumnTypes = EnumSet.of(PREV_ROW, SELECTED, FILES, ECOMP, SCANS, DIR);\n+    var expectedFamilies = Set\n+        .of(ServerColumnFamily.NAME, TabletColumnFamily.NAME, DataFileColumnFamily.NAME,\n+            ScanFileColumnFamily.NAME, ExternalCompactionColumnFamily.NAME)\n+        .stream().map(family -> new ArrayByteSequence(family.copyBytes()))\n+        .collect(Collectors.toSet());\n+    var resolved2 = new ResolvedColumns(expectedColumnTypes);\n+    assertEquals(expectedColumnTypes, resolved2.getColumns());\n+    assertEquals(ColumnType.resolveFamilies(resolved2.getColumns()), resolved2.getFamilies());\n+    assertEquals(expectedFamilies, resolved2.getFamilies());\n+  }\n+\n+}\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java\nindex 65540deea0b..3812fdd6d00 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/CompactionReservationCheckTest.java\n@@ -19,19 +19,26 @@\n package org.apache.accumulo.manager.compaction;\n \n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.ECOMP;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.FILES;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.OPID;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.SELECTED;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.USER_COMPACTION_REQUESTED;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertSame;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.net.URI;\n import java.time.Duration;\n+import java.util.EnumSet;\n import java.util.Set;\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n \n+import org.apache.accumulo.core.data.ArrayByteSequence;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.fate.FateId;\n@@ -41,8 +48,14 @@\n import org.apache.accumulo.core.metadata.schema.CompactionMetadata;\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ExternalCompactionColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n+import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n import org.apache.accumulo.core.metadata.schema.TabletOperationType;\n import org.apache.accumulo.core.spi.compaction.CompactionKind;\n@@ -214,6 +227,27 @@ public void testCanReserve() throws Exception {\n         () -> canReserveUser(null, CompactionKind.USER, Set.of(file1, file2), fateId1, time));\n   }\n \n+  @Test\n+  public void testResolvedColumns() throws Exception {\n+    var resolved1 = new CompactionReservationCheck(CompactionKind.SYSTEM, Set.of(), null, false,\n+        SteadyTime.from(Duration.ZERO), 100L).columnsToRead();\n+    var resolved2 = new CompactionReservationCheck(CompactionKind.SYSTEM, Set.of(), null, false,\n+        SteadyTime.from(Duration.ZERO), 100L).columnsToRead();\n+    var expectedColumnTypes =\n+        EnumSet.of(PREV_ROW, OPID, SELECTED, FILES, ECOMP, USER_COMPACTION_REQUESTED);\n+    var expectedFamilies = Set\n+        .of(ServerColumnFamily.NAME, TabletColumnFamily.NAME, DataFileColumnFamily.NAME,\n+            ExternalCompactionColumnFamily.NAME, UserCompactionRequestedColumnFamily.NAME)\n+        .stream().map(family -> new ArrayByteSequence(family.copyBytes()))\n+        .collect(Collectors.toSet());\n+\n+    // Verify same object is re-used across instances\n+    assertSame(resolved1, resolved2);\n+    assertEquals(expectedColumnTypes, resolved1.getColumns());\n+    assertEquals(ColumnType.resolveFamilies(expectedColumnTypes), resolved1.getFamilies());\n+    assertEquals(expectedFamilies, resolved1.getFamilies());\n+  }\n+\n   private boolean canReserveSystem(TabletMetadata tabletMetadata, CompactionKind kind,\n       Set<StoredTabletFile> jobFiles, boolean deletingSelected, SteadyTime steadyTime) {\n     var check = new CompactionReservationCheck(CompactionKind.SYSTEM, jobFiles, null,\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5301"", ""pr_id"": 5301, ""issue_id"": 5130, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Support dedicating threads to specific types of Fate transactions\nIn main the `Manager` instantiates two instances of Fate, one for the `MetaFateStore` and one for the `UserFateStore`. Each instance of Fate contains a single thread pool of `Property.MANAGER_FATE_THREADPOOL_SIZE` size which is used to process Fate transactions. The threads in the Fate thread pool try to process transactions in a fair manner, by spending time on a different transaction in the queue when done with the current transactions' step (see #3852 and `FateInterleavingIT` for more information). The default size of `MANAGER_FATE_THREADPOOL_SIZE` is 4, which may need to be increased in main, could allow for some transactions to go a long time without making progress (It's possible that this could be the cause of the long delete table transaction times we are seeing in integration tests).\r\n\r\nIt may be advisable to dedicate some number of threads to specific transaction types. For example, maybe 1 thread dedicated to creating and deleting tables, and a larger thread pool dedicated to bulk imports and compactions. I'm initially thinking of a change to the `MANAGER_FATE_THREADPOOL_SIZE` property such that instead of it being a count of the number of threads, the property is renamed to `MANAGER_FATE_THREADPOOL_CONFIGURATION` and contains a json map where the key is a comma separated list of Fate transaction names and the value is the number of threads, like:\r\n```\r\n{\r\n  \""SET_AVAILABILITY,CREATE_TABLE,DELETE_TABLE,CREATE_NAMESPACE,DELETE_NAMESPACE,RENAME_NAMESPACE\"": 1,\r\n  \""BULK_IMPORT\"": 2,\r\n  \""COMPACT\"": 4,\r\n  \""MERGE,SPLIT,IMPORT_TABLE,EXPORT_TABLE\"": 2\r\n}\r\n```\r\n\r\nThe Manager would parse this configuration and start one Fate instance for each Key,Value pair in the map. The Fate instance would be responsible for managing only the types that it's given. Currently `MANAGER_FATE_THREADPOOL_SIZE` is watched and the size of the threadpool changes when the property changes. That may be harder to do with this idea. I think the name of the transaction is currently serialized into the transaction and I think the transactions can be filtered by type, so this would build upon that capability."", ""issue_word_count"": 332, ""test_files_count"": 29, ""non_test_files_count"": 7, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java"", ""core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateExecutor.java"", ""core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java"", ""core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java"", ""core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FatePoolResizeIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateTestUtil.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java"", ""test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java"", ""core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FatePoolResizeIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateTestUtil.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java"", ""test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java""], ""base_commit"": ""359712e526b4760862574adcaa6c178c853662dc"", ""head_commit"": ""ef74f608065beeae4ff4c5347bf3fb0324f2b3ee"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5301"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5301"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-24T17:39:40.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 62ae03cf69f..8ad5c085329 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -434,15 +434,45 @@ public enum Property {\n   MANAGER_FATE_METRICS_MIN_UPDATE_INTERVAL(\""manager.fate.metrics.min.update.interval\"", \""60s\"",\n       PropertyType.TIMEDURATION, \""Limit calls from metric sinks to zookeeper to update interval.\"",\n       \""1.9.3\""),\n-  MANAGER_FATE_THREADPOOL_SIZE(\""manager.fate.threadpool.size\"", \""64\"", PropertyType.COUNT,\n-      \""The number of threads used to run fault-tolerant executions (FATE).\""\n-          + \"" These are primarily table operations like merge.\"",\n+  @Deprecated(since = \""4.0.0\"")\n+  MANAGER_FATE_THREADPOOL_SIZE(\""manager.fate.threadpool.size\"", \""64\"",\n+      PropertyType.FATE_THREADPOOL_SIZE,\n+      \""Previously, the number of threads used to run fault-tolerant executions (FATE).\""\n+          + \"" This is no longer used in 4.0+. MANAGER_FATE_USER_CONFIG and\""\n+          + \"" MANAGER_FATE_META_CONFIG are the replacement and must be set instead.\"",\n       \""1.4.3\""),\n+  MANAGER_FATE_USER_CONFIG(\""manager.fate.user.config\"",\n+      \""{\\\""TABLE_CREATE,TABLE_DELETE,TABLE_RENAME,TABLE_ONLINE,TABLE_OFFLINE,NAMESPACE_CREATE,\""\n+          + \""NAMESPACE_DELETE,NAMESPACE_RENAME,TABLE_TABLET_AVAILABILITY,SHUTDOWN_TSERVER,\""\n+          + \""TABLE_BULK_IMPORT2,TABLE_COMPACT,TABLE_CANCEL_COMPACT,TABLE_MERGE,TABLE_DELETE_RANGE,\""\n+          + \""TABLE_SPLIT,TABLE_CLONE,TABLE_IMPORT,TABLE_EXPORT,SYSTEM_MERGE\\\"": 4,\""\n+          + \""\\\""COMMIT_COMPACTION\\\"": 4,\\\""SYSTEM_SPLIT\\\"": 4}\"",\n+      PropertyType.FATE_USER_CONFIG,\n+      \""The number of threads used to run fault-tolerant executions (FATE) on user\""\n+          + \""tables. These are primarily table operations like merge. Each key/value \""\n+          + \""of the provided JSON corresponds to one thread pool. Each key is a list of one or \""\n+          + \""more FATE operations and each value is the number of threads that will be assigned \""\n+          + \""to the pool.\"",\n+      \""4.0.0\""),\n+  MANAGER_FATE_META_CONFIG(\""manager.fate.meta.config\"",\n+      \""{\\\""TABLE_CREATE,TABLE_DELETE,TABLE_RENAME,TABLE_ONLINE,TABLE_OFFLINE,NAMESPACE_CREATE,\""\n+          + \""NAMESPACE_DELETE,NAMESPACE_RENAME,TABLE_TABLET_AVAILABILITY,SHUTDOWN_TSERVER,\""\n+          + \""TABLE_BULK_IMPORT2,TABLE_COMPACT,TABLE_CANCEL_COMPACT,TABLE_MERGE,TABLE_DELETE_RANGE,\""\n+          + \""TABLE_SPLIT,TABLE_CLONE,TABLE_IMPORT,TABLE_EXPORT,SYSTEM_MERGE\\\"": 4,\""\n+          + \""\\\""COMMIT_COMPACTION\\\"": 4,\\\""SYSTEM_SPLIT\\\"": 4}\"",\n+      PropertyType.FATE_META_CONFIG,\n+      \""The number of threads used to run fault-tolerant executions (FATE) on Accumulo\""\n+          + \""system tables. These are primarily table operations like merge. Each key/value \""\n+          + \""of the provided JSON corresponds to one thread pool. Each key is a list of one or \""\n+          + \""more FATE operations and each value is the number of threads that will be assigned \""\n+          + \""to the pool.\"",\n+      \""4.0.0\""),\n   MANAGER_FATE_IDLE_CHECK_INTERVAL(\""manager.fate.idle.check.interval\"", \""60m\"",\n       PropertyType.TIMEDURATION,\n       \""The interval at which to check if the number of idle Fate threads has consistently been zero.\""\n-          + \"" The way this is checked is an approximation. Logs a warning in the Manager log to increase\""\n-          + \"" MANAGER_FATE_THREADPOOL_SIZE. A value of zero disables this check and has a maximum value of 60m.\"",\n+          + \"" The way this is checked is an approximation. Logs a warning in the Manager log to change\""\n+          + \"" MANAGER_FATE_USER_CONFIG or MANAGER_FATE_META_CONFIG. A value less than a minute disables\""\n+          + \"" this check and has a maximum value of 60m.\"",\n       \""4.0.0\""),\n   MANAGER_STATUS_THREAD_POOL_SIZE(\""manager.status.threadpool.size\"", \""0\"", PropertyType.COUNT,\n       \""The number of threads to use when fetching the tablet server status for balancing.  Zero \""\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java b/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\nindex 4d3339b13b8..23e2075de3f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/PropertyType.java\n@@ -22,7 +22,9 @@\n \n import java.io.IOException;\n import java.util.Arrays;\n+import java.util.HashSet;\n import java.util.Objects;\n+import java.util.Set;\n import java.util.function.Function;\n import java.util.function.Predicate;\n import java.util.regex.Matcher;\n@@ -30,6 +32,7 @@\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n+import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.file.rfile.RFile;\n import org.apache.commons.lang3.Range;\n import org.apache.hadoop.fs.Path;\n@@ -39,6 +42,7 @@\n import com.fasterxml.jackson.databind.DeserializationFeature;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Preconditions;\n+import com.google.gson.JsonParser;\n \n /**\n  * Types of {@link Property} values. Each type has a short name, a description, and a regex which\n@@ -136,9 +140,8 @@ public enum PropertyType {\n           + \"" interpreted based on the context of the property to which it applies.\""),\n \n   JSON(\""json\"", new ValidJson(),\n-      \""An arbitrary string that is represents a valid, parsable generic json object. The validity \""\n+      \""An arbitrary string that represents a valid, parsable generic json object. The validity \""\n           + \""of the json object in the context of the property usage is not checked by this type.\""),\n-\n   BOOLEAN(\""boolean\"", in(false, null, \""true\"", \""false\""),\n       \""Has a value of either 'true' or 'false' (case-insensitive)\""),\n \n@@ -147,8 +150,23 @@ public enum PropertyType {\n   FILENAME_EXT(\""file name extension\"", in(true, RFile.EXTENSION),\n       \""One of the currently supported filename extensions for storing table data files. \""\n           + \""Currently, only \"" + RFile.EXTENSION + \"" is supported.\""),\n-\n-  VOLUMES(\""volumes\"", new ValidVolumes(), \""See instance.volumes documentation\"");\n+  VOLUMES(\""volumes\"", new ValidVolumes(), \""See instance.volumes documentation\""),\n+  FATE_USER_CONFIG(ValidUserFateConfig.NAME, new ValidUserFateConfig(),\n+      \""An arbitrary string that: 1. Represents a valid, parsable generic json object. \""\n+          + \""2. the keys of the json are strings which contain a comma-separated list of fate operations. \""\n+          + \""3. the values of the json are integers which represent the number of threads assigned to the fate operations. \""\n+          + \""4. all possible user fate operations are present in the json. \""\n+          + \""5. no fate operations are repeated.\""),\n+\n+  FATE_META_CONFIG(ValidMetaFateConfig.NAME, new ValidMetaFateConfig(),\n+      \""An arbitrary string that: 1. Represents a valid, parsable generic json object. \""\n+          + \""2. the keys of the json are strings which contain a comma-separated list of fate operations. \""\n+          + \""3. the values of the json are integers which represent the number of threads assigned to the fate operations. \""\n+          + \""4. all possible meta fate operations are present in the json. \""\n+          + \""5. no fate operations are repeated.\""),\n+  FATE_THREADPOOL_SIZE(\""(deprecated) Manager FATE thread pool size\"", new FateThreadPoolSize(),\n+      \""No format check. Allows any value to be set but will warn the user that the\""\n+          + \"" property is no longer used.\"");\n \n   private final String shortname;\n   private final String format;\n@@ -414,4 +432,87 @@ public static IntStream parse(String portRange) {\n     }\n \n   }\n+\n+  private static class ValidFateConfig implements Predicate<String> {\n+    private static final Logger log = LoggerFactory.getLogger(ValidFateConfig.class);\n+    private final Set<Fate.FateOperation> allFateOps;\n+    private final String name;\n+\n+    private ValidFateConfig(Set<Fate.FateOperation> allFateOps, String name) {\n+      this.allFateOps = allFateOps;\n+      this.name = name;\n+    }\n+\n+    @Override\n+    public boolean test(String s) {\n+      final Set<Fate.FateOperation> seenFateOps;\n+\n+      try {\n+        final var json = JsonParser.parseString(s).getAsJsonObject();\n+        seenFateOps = new HashSet<>();\n+\n+        for (var entry : json.entrySet()) {\n+          var key = entry.getKey();\n+          var val = entry.getValue().getAsInt();\n+          if (val <= 0) {\n+            log.warn(\n+                \""Invalid entry {} in {}. Must be a valid thread pool size. Property was unchanged.\"",\n+                entry, name);\n+            return false;\n+          }\n+          var fateOpsStrArr = key.split(\"",\"");\n+          for (String fateOpStr : fateOpsStrArr) {\n+            Fate.FateOperation fateOp = Fate.FateOperation.valueOf(fateOpStr);\n+            if (seenFateOps.contains(fateOp)) {\n+              log.warn(\""Duplicate fate operation {} seen in {}. Property was unchanged.\"", fateOp,\n+                  name);\n+              return false;\n+            }\n+            seenFateOps.add(fateOp);\n+          }\n+        }\n+      } catch (Exception e) {\n+        log.warn(\""Exception from attempting to set {}. Property was unchanged.\"", name, e);\n+        return false;\n+      }\n+\n+      var allFateOpsSeen = allFateOps.equals(seenFateOps);\n+      if (!allFateOpsSeen) {\n+        log.warn(\n+            \""Not all fate operations found in {}. Expected to see {} but saw {}. Property was unchanged.\"",\n+            name, allFateOps, seenFateOps);\n+      }\n+      return allFateOpsSeen;\n+    }\n+  }\n+\n+  private static class ValidUserFateConfig extends ValidFateConfig {\n+    private static final String NAME = \""fate user config\"";\n+\n+    private ValidUserFateConfig() {\n+      super(Fate.FateOperation.getAllUserFateOps(), NAME);\n+    }\n+  }\n+\n+  private static class ValidMetaFateConfig extends ValidFateConfig {\n+    private static final String NAME = \""fate meta config\"";\n+\n+    private ValidMetaFateConfig() {\n+      super(Fate.FateOperation.getAllMetaFateOps(), NAME);\n+    }\n+  }\n+\n+  private static class FateThreadPoolSize implements Predicate<String> {\n+    private static final Logger log = LoggerFactory.getLogger(FateThreadPoolSize.class);\n+\n+    @Override\n+    public boolean test(String s) {\n+      log.warn(\n+          \""The manager fate thread pool size property is no longer used. See the {} and {} for \""\n+              + \""the replacements to this property.\"",\n+          ValidUserFateConfig.NAME, ValidMetaFateConfig.NAME);\n+      return true;\n+    }\n+  }\n+\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 60d3f427c06..4658718480b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -18,40 +18,37 @@\n  */\n package org.apache.accumulo.core.fate;\n \n-import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;\n-import static java.util.concurrent.TimeUnit.MILLISECONDS;\n-import static java.util.concurrent.TimeUnit.MINUTES;\n import static java.util.concurrent.TimeUnit.SECONDS;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.FAILED;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.FAILED_IN_PROGRESS;\n-import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.IN_PROGRESS;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.NEW;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUBMITTED;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUCCESSFUL;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.UNKNOWN;\n-import static org.apache.accumulo.core.util.ShutdownUtil.isIOException;\n+import static org.apache.accumulo.core.util.threads.ThreadPoolNames.META_DEAD_RESERVATION_CLEANER_POOL;\n+import static org.apache.accumulo.core.util.threads.ThreadPoolNames.USER_DEAD_RESERVATION_CLEANER_POOL;\n \n import java.time.Duration;\n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.EnumSet;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.TreeSet;\n import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.LinkedTransferQueue;\n-import java.util.concurrent.RejectedExecutionException;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.ScheduledThreadPoolExecutor;\n-import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TransferQueue;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Function;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n-import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n@@ -59,17 +56,14 @@\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.logging.FateLogger;\n import org.apache.accumulo.core.manager.thrift.TFateOperation;\n-import org.apache.accumulo.core.util.ShutdownUtil;\n-import org.apache.accumulo.core.util.Timer;\n import org.apache.accumulo.core.util.UtilWaitThread;\n import org.apache.accumulo.core.util.threads.ThreadPools;\n-import org.apache.accumulo.core.util.threads.Threads;\n import org.apache.thrift.TApplicationException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.base.Preconditions;\n+import com.google.gson.JsonParser;\n \n /**\n  * Fault tolerant executor\n@@ -77,13 +71,10 @@\n public class Fate<T> {\n \n   private static final Logger log = LoggerFactory.getLogger(Fate.class);\n-  private final Logger runnerLog = LoggerFactory.getLogger(TransactionRunner.class);\n \n   private final FateStore<T> store;\n-  private final T environment;\n-  private final ScheduledThreadPoolExecutor fatePoolWatcher;\n-  private final ExecutorService transactionExecutor;\n-  private final Set<TransactionRunner> runningTxRunners;\n+  private final ScheduledThreadPoolExecutor fatePoolsWatcher;\n+  private final AtomicInteger needMoreThreadsWarnCount = new AtomicInteger(0);\n   private final ExecutorService deadResCleanerExecutor;\n \n   private static final EnumSet<TStatus> FINISHED_STATES = EnumSet.of(FAILED, SUCCESSFUL, UNKNOWN);\n@@ -92,9 +83,7 @@ public class Fate<T> {\n   private static final Duration POOL_WATCHER_DELAY = Duration.ofSeconds(30);\n \n   private final AtomicBoolean keepRunning = new AtomicBoolean(true);\n-  private final TransferQueue<FateId> workQueue;\n-  private final Thread workFinder;\n-  private final ConcurrentLinkedQueue<Integer> idleCountHistory = new ConcurrentLinkedQueue<>();\n+  private final Set<FateExecutor<T>> fateExecutors = new HashSet<>();\n \n   public enum TxInfo {\n     FATE_OP, AUTO_CLEAN, EXCEPTION, TX_AGEOFF, RETURN_VALUE\n@@ -125,8 +114,12 @@ public enum FateOperation {\n     TABLE_TABLET_AVAILABILITY(TFateOperation.TABLE_TABLET_AVAILABILITY);\n \n     private final TFateOperation top;\n-    private static final EnumSet<FateOperation> nonThriftOps =\n-        EnumSet.of(COMMIT_COMPACTION, SHUTDOWN_TSERVER, SYSTEM_SPLIT, SYSTEM_MERGE);\n+    private static final Set<FateOperation> nonThriftOps = Collections.unmodifiableSet(\n+        EnumSet.of(COMMIT_COMPACTION, SHUTDOWN_TSERVER, SYSTEM_SPLIT, SYSTEM_MERGE));\n+    private static final Set<FateOperation> allUserFateOps =\n+        Collections.unmodifiableSet(EnumSet.allOf(FateOperation.class));\n+    private static final Set<FateOperation> allMetaFateOps =\n+        Collections.unmodifiableSet(EnumSet.allOf(FateOperation.class));\n \n     FateOperation(TFateOperation top) {\n       this.top = top;\n@@ -136,7 +129,7 @@ public static FateOperation fromThrift(TFateOperation top) {\n       return FateOperation.valueOf(top.name());\n     }\n \n-    public static EnumSet<FateOperation> getNonThriftOps() {\n+    public static Set<FateOperation> getNonThriftOps() {\n       return nonThriftOps;\n     }\n \n@@ -146,260 +139,90 @@ public TFateOperation toThrift() {\n       }\n       return top;\n     }\n-  }\n \n-  /**\n-   * A single thread that finds transactions to work on and queues them up. Do not want each worker\n-   * thread going to the store and looking for work as it would place more load on the store.\n-   */\n-  private class WorkFinder implements Runnable {\n-\n-    @Override\n-    public void run() {\n-      while (keepRunning.get()) {\n-        try {\n-          store.runnable(keepRunning, fateId -> {\n-            while (keepRunning.get()) {\n-              try {\n-                // The reason for calling transfer instead of queueing is avoid rescanning the\n-                // storage layer and adding the same thing over and over. For example if all threads\n-                // were busy, the queue size was 100, and there are three runnable things in the\n-                // store. Do not want to keep scanning the store adding those same 3 runnable things\n-                // until the queue is full.\n-                if (workQueue.tryTransfer(fateId, 100, MILLISECONDS)) {\n-                  break;\n-                }\n-              } catch (InterruptedException e) {\n-                throw new IllegalStateException(e);\n-              }\n-            }\n-          });\n-        } catch (Exception e) {\n-          if (keepRunning.get()) {\n-            log.warn(\""Failure while attempting to find work for fate\"", e);\n-          } else {\n-            log.debug(\""Failure while attempting to find work for fate\"", e);\n-          }\n+    public static Set<FateOperation> getAllUserFateOps() {\n+      return allUserFateOps;\n+    }\n \n-          workQueue.clear();\n-        }\n-      }\n+    public static Set<FateOperation> getAllMetaFateOps() {\n+      return allMetaFateOps;\n     }\n   }\n \n-  private class TransactionRunner implements Runnable {\n-    // used to signal a TransactionRunner to stop in the case where there are too many running\n-    // i.e., the property for the pool size decreased and we have excess TransactionRunners\n-    private final AtomicBoolean stop = new AtomicBoolean(false);\n-\n-    private Optional<FateTxStore<T>> reserveFateTx() throws InterruptedException {\n-      while (keepRunning.get() && !stop.get()) {\n-        FateId unreservedFateId = workQueue.poll(100, MILLISECONDS);\n-\n-        if (unreservedFateId == null) {\n-          continue;\n-        }\n-        var optionalopStore = store.tryReserve(unreservedFateId);\n-        if (optionalopStore.isPresent()) {\n-          return optionalopStore;\n-        }\n-      }\n-\n-      return Optional.empty();\n+  // The fate pools watcher:\n+  // - Maintains a TransactionRunner per available thread per pool/FateExecutor. Does so by\n+  // periodically checking the pools for an inactive thread (i.e., a thread running a\n+  // TransactionRunner died or the pool size was increased in the property), resizing the pool and\n+  // submitting new runners as needed. Also safely stops the necessary number of TransactionRunners\n+  // if the pool size in the property was decreased.\n+  // - Warns the user to consider increasing the pool size (or splitting the fate ops assigned to\n+  // that pool into separate pools) for any pool that does not often have any idle threads.\n+  private class FatePoolsWatcher implements Runnable {\n+    private final T environment;\n+    private final AccumuloConfiguration conf;\n+\n+    private FatePoolsWatcher(T environment, AccumuloConfiguration conf) {\n+      this.environment = environment;\n+      this.conf = conf;\n     }\n \n     @Override\n     public void run() {\n-      runningTxRunners.add(this);\n-      try {\n-        while (keepRunning.get() && !stop.get()) {\n-          FateTxStore<T> txStore = null;\n-          ExecutionState state = new ExecutionState();\n-          try {\n-            var optionalopStore = reserveFateTx();\n-            if (optionalopStore.isPresent()) {\n-              txStore = optionalopStore.orElseThrow();\n-            } else {\n-              continue;\n-            }\n-            state.status = txStore.getStatus();\n-            state.op = txStore.top();\n-            if (state.status == FAILED_IN_PROGRESS) {\n-              processFailed(txStore, state.op);\n-            } else if (state.status == SUBMITTED || state.status == IN_PROGRESS) {\n-              try {\n-                execute(txStore, state);\n-                if (state.op != null && state.deferTime != 0) {\n-                  // The current op is not ready to execute\n-                  continue;\n-                }\n-              } catch (StackOverflowException e) {\n-                // the op that failed to push onto the stack was never executed, so no need to undo\n-                // it just transition to failed and undo the ops that executed\n-                transitionToFailed(txStore, e);\n-                continue;\n-              } catch (Exception e) {\n-                blockIfHadoopShutdown(txStore.getID(), e);\n-                transitionToFailed(txStore, e);\n-                continue;\n-              }\n-\n-              if (state.op == null) {\n-                // transaction is finished\n-                String ret = state.prevOp.getReturn();\n-                if (ret != null) {\n-                  txStore.setTransactionInfo(TxInfo.RETURN_VALUE, ret);\n-                }\n-                txStore.setStatus(SUCCESSFUL);\n-                doCleanUp(txStore);\n-              }\n-            }\n-          } catch (Exception e) {\n-            runnerLog.error(\""Uncaught exception in FATE runner thread.\"", e);\n-          } finally {\n-            if (txStore != null) {\n-              txStore.unreserve(Duration.ofMillis(state.deferTime));\n+      // Read from the config here and here only. Must avoid reading the same property from the\n+      // config more than once since it can change at any point in this execution\n+      var poolConfigs = getPoolConfigurations(conf);\n+      var idleCheckIntervalMillis = conf.getTimeInMillis(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL);\n+\n+      // shutdown task: shutdown fate executors whose set of fate operations are no longer present\n+      // in the config\n+      synchronized (fateExecutors) {\n+        final var fateExecutorsIter = fateExecutors.iterator();\n+        while (fateExecutorsIter.hasNext()) {\n+          var fateExecutor = fateExecutorsIter.next();\n+\n+          // if this fate executors set of fate ops is no longer present in the config...\n+          if (!poolConfigs.containsKey(fateExecutor.getFateOps())) {\n+            if (!fateExecutor.isShutdown()) {\n+              log.debug(\""The config for {} has changed invalidating {}. Gracefully shutting down \""\n+                  + \""the FateExecutor.\"", getFateConfigProp(), fateExecutor);\n+              fateExecutor.initiateShutdown();\n+            } else if (fateExecutor.isShutdown() && fateExecutor.isAlive()) {\n+              log.debug(\""{} has been shutdown, but is still actively working on transactions.\"",\n+                  fateExecutor);\n+            } else if (fateExecutor.isShutdown() && !fateExecutor.isAlive()) {\n+              log.debug(\""{} has been shutdown and all threads have safely terminated.\"",\n+                  fateExecutor);\n+              fateExecutorsIter.remove();\n             }\n           }\n         }\n-      } finally {\n-        log.trace(\""A TransactionRunner is exiting...\"");\n-        Preconditions.checkState(runningTxRunners.remove(this));\n       }\n-    }\n-\n-    private class ExecutionState {\n-      Repo<T> prevOp = null;\n-      Repo<T> op = null;\n-      long deferTime = 0;\n-      TStatus status;\n-    }\n-\n-    // Executes as many steps of a fate operation as possible\n-    private void execute(final FateTxStore<T> txStore, final ExecutionState state)\n-        throws Exception {\n-      while (state.op != null && state.deferTime == 0) {\n-        state.deferTime = executeIsReady(txStore.getID(), state.op);\n-\n-        if (state.deferTime == 0) {\n-          if (state.status == SUBMITTED) {\n-            txStore.setStatus(IN_PROGRESS);\n-            state.status = IN_PROGRESS;\n-          }\n-\n-          state.prevOp = state.op;\n-          state.op = executeCall(txStore.getID(), state.op);\n \n-          if (state.op != null) {\n-            // persist the completion of this step before starting to run the next so in the case of\n-            // process death the completed steps are not rerun\n-            txStore.push(state.op);\n+      // replacement task: at this point, the existing FateExecutors that were invalidated by the\n+      // config changes have started shutdown or finished shutdown. Now create any new replacement\n+      // FateExecutors needed\n+      for (var poolConfig : poolConfigs.entrySet()) {\n+        var configFateOps = poolConfig.getKey();\n+        var configPoolSize = poolConfig.getValue();\n+        synchronized (fateExecutors) {\n+          if (fateExecutors.stream().map(FateExecutor::getFateOps)\n+              .noneMatch(fo -> fo.equals(configFateOps))) {\n+            fateExecutors\n+                .add(new FateExecutor<>(Fate.this, environment, configFateOps, configPoolSize));\n           }\n         }\n       }\n-    }\n-\n-    /**\n-     * The Hadoop Filesystem registers a java shutdown hook that closes the file system. This can\n-     * cause threads to get spurious IOException. If this happens, instead of failing a FATE\n-     * transaction just wait for process to die. When the manager start elsewhere the FATE\n-     * transaction can resume.\n-     */\n-    private void blockIfHadoopShutdown(FateId fateId, Exception e) {\n-      if (ShutdownUtil.isShutdownInProgress()) {\n-\n-        if (e instanceof AcceptableException) {\n-          log.debug(\""Ignoring exception possibly caused by Hadoop Shutdown hook. {} \"", fateId, e);\n-        } else if (isIOException(e)) {\n-          log.info(\""Ignoring exception likely caused by Hadoop Shutdown hook. {} \"", fateId, e);\n-        } else {\n-          // sometimes code will catch an IOException caused by the hadoop shutdown hook and throw\n-          // another exception without setting the cause.\n-          log.warn(\""Ignoring exception possibly caused by Hadoop Shutdown hook. {} \"", fateId, e);\n-        }\n-\n-        while (true) {\n-          // Nothing is going to work well at this point, so why even try. Just wait for the end,\n-          // preventing this FATE thread from processing further work and likely failing.\n-          sleepUninterruptibly(1, MINUTES);\n-        }\n-      }\n-    }\n \n-    private void transitionToFailed(FateTxStore<T> txStore, Exception e) {\n-      final String msg = \""Failed to execute Repo \"" + txStore.getID();\n-      // Certain FATE ops that throw exceptions don't need to be propagated up to the Monitor\n-      // as a warning. They're a normal, handled failure condition.\n-      if (e instanceof AcceptableException) {\n-        var tableOpEx = (AcceptableThriftTableOperationException) e;\n-        log.info(\""{} for table:{}({}) saw acceptable exception: {}\"", msg, tableOpEx.getTableName(),\n-            tableOpEx.getTableId(), tableOpEx.getDescription());\n-      } else {\n-        log.warn(msg, e);\n-      }\n-      txStore.setTransactionInfo(TxInfo.EXCEPTION, e);\n-      txStore.setStatus(FAILED_IN_PROGRESS);\n-      log.info(\""Updated status for Repo with {} to FAILED_IN_PROGRESS\"", txStore.getID());\n-    }\n-\n-    private void processFailed(FateTxStore<T> txStore, Repo<T> op) {\n-      while (op != null) {\n-        undo(txStore.getID(), op);\n-\n-        txStore.pop();\n-        op = txStore.top();\n-      }\n-\n-      txStore.setStatus(FAILED);\n-      doCleanUp(txStore);\n-    }\n-\n-    private void doCleanUp(FateTxStore<T> txStore) {\n-      Boolean autoClean = (Boolean) txStore.getTransactionInfo(TxInfo.AUTO_CLEAN);\n-      if (autoClean != null && autoClean) {\n-        txStore.delete();\n-      } else {\n-        // no longer need persisted operations, so delete them to save space in case\n-        // TX is never cleaned up...\n-        while (txStore.top() != null) {\n-          txStore.pop();\n+      // resize task: see description for FateExecutor.resizeFateExecutor\n+      synchronized (fateExecutors) {\n+        for (var fateExecutor : fateExecutors) {\n+          if (fateExecutor.isShutdown()) {\n+            continue;\n+          }\n+          fateExecutor.resizeFateExecutor(poolConfigs, idleCheckIntervalMillis);\n         }\n       }\n     }\n-\n-    private void undo(FateId fateId, Repo<T> op) {\n-      try {\n-        op.undo(fateId, environment);\n-      } catch (Exception e) {\n-        log.warn(\""Failed to undo Repo, \"" + fateId, e);\n-      }\n-    }\n-\n-    private boolean flagStop() {\n-      return stop.compareAndSet(false, true);\n-    }\n-\n-    private boolean isFlaggedToStop() {\n-      return stop.get();\n-    }\n-\n-  }\n-\n-  protected long executeIsReady(FateId fateId, Repo<T> op) throws Exception {\n-    var startTime = Timer.startNew();\n-    var deferTime = op.isReady(fateId, environment);\n-    log.debug(\""Running {}.isReady() {} took {} ms and returned {}\"", op.getName(), fateId,\n-        startTime.elapsed(MILLISECONDS), deferTime);\n-    return deferTime;\n-  }\n-\n-  protected Repo<T> executeCall(FateId fateId, Repo<T> op) throws Exception {\n-    var startTime = Timer.startNew();\n-    var next = op.call(fateId, environment);\n-    log.debug(\""Running {}.call() {} took {} ms and returned {}\"", op.getName(), fateId,\n-        startTime.elapsed(MILLISECONDS), next == null ? \""null\"" : next.getName());\n-\n-    return next;\n   }\n \n   /**\n@@ -415,102 +238,29 @@ public void run() {\n   }\n \n   /**\n-   * Creates a Fault-tolerant executor.\n+   * Creates a Fault-tolerant executor for the given store type.\n    *\n+   * @param runDeadResCleaner Whether this FATE should run a dead reservation cleaner. The real\n+   *        FATEs need have a cleaner, but may be undesirable in testing.\n    * @param toLogStrFunc A function that converts Repo to Strings that are suitable for logging\n    */\n   public Fate(T environment, FateStore<T> store, boolean runDeadResCleaner,\n       Function<Repo<T>,String> toLogStrFunc, AccumuloConfiguration conf) {\n     this.store = FateLogger.wrap(store, toLogStrFunc, false);\n-    this.environment = environment;\n-    final ThreadPoolExecutor pool = ThreadPools.getServerThreadPools().createExecutorService(conf,\n-        Property.MANAGER_FATE_THREADPOOL_SIZE, true);\n-    this.workQueue = new LinkedTransferQueue<>();\n-    this.runningTxRunners = Collections.synchronizedSet(new HashSet<>());\n-    this.fatePoolWatcher =\n+\n+    this.fatePoolsWatcher =\n         ThreadPools.getServerThreadPools().createGeneralScheduledExecutorService(conf);\n-    ThreadPools.watchCriticalScheduledTask(fatePoolWatcher.scheduleWithFixedDelay(() -> {\n-      // resize the pool if the property changed\n-      ThreadPools.resizePool(pool, conf, Property.MANAGER_FATE_THREADPOOL_SIZE);\n-      final int configured = conf.getCount(Property.MANAGER_FATE_THREADPOOL_SIZE);\n-      final int needed = configured - runningTxRunners.size();\n-      if (needed > 0) {\n-        // If the pool grew, then ensure that there is a TransactionRunner for each thread\n-        for (int i = 0; i < needed; i++) {\n-          try {\n-            pool.execute(new TransactionRunner());\n-          } catch (RejectedExecutionException e) {\n-            // RejectedExecutionException could be shutting down\n-            if (pool.isShutdown()) {\n-              // The exception is expected in this case, no need to spam the logs.\n-              log.trace(\""Error adding transaction runner to FaTE executor pool.\"", e);\n-            } else {\n-              // This is bad, FaTE may no longer work!\n-              log.error(\""Error adding transaction runner to FaTE executor pool.\"", e);\n-            }\n-            break;\n-          }\n-        }\n-        idleCountHistory.clear();\n-      } else if (needed < 0) {\n-        // If we need the pool to shrink, then ensure excess TransactionRunners are safely stopped.\n-        // Flag the necessary number of TransactionRunners to safely stop when they are done work\n-        // on a transaction.\n-        int numFlagged =\n-            (int) runningTxRunners.stream().filter(TransactionRunner::isFlaggedToStop).count();\n-        int numToStop = -1 * (numFlagged + needed);\n-        for (var runner : runningTxRunners) {\n-          if (numToStop <= 0) {\n-            break;\n-          }\n-          if (runner.flagStop()) {\n-            log.trace(\""Flagging a TransactionRunner to stop...\"");\n-            numToStop--;\n-          }\n-        }\n-      } else {\n-        // The property did not change, but should it based on idle Fate threads? Maintain\n-        // count of the last X minutes of idle Fate threads. If zero 95% of the time, then suggest\n-        // that the MANAGER_FATE_THREADPOOL_SIZE be increased.\n-        final long interval = Math.min(60, TimeUnit.MILLISECONDS\n-            .toMinutes(conf.getTimeInMillis(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL)));\n-        if (interval == 0) {\n-          idleCountHistory.clear();\n-        } else {\n-          if (idleCountHistory.size() >= interval * 2) { // this task runs every 30s\n-            int zeroFateThreadsIdleCount = 0;\n-            for (Integer idleConsumerCount : idleCountHistory) {\n-              if (idleConsumerCount == 0) {\n-                zeroFateThreadsIdleCount++;\n-              }\n-            }\n-            boolean needMoreThreads =\n-                (zeroFateThreadsIdleCount / (double) idleCountHistory.size()) >= 0.95;\n-            if (needMoreThreads) {\n-              log.warn(\n-                  \""All Fate threads appear to be busy for the last {} minutes,\""\n-                      + \"" consider increasing property: {}\"",\n-                  interval, Property.MANAGER_FATE_THREADPOOL_SIZE.getKey());\n-              // Clear the history so that we don't log for interval minutes.\n-              idleCountHistory.clear();\n-            } else {\n-              while (idleCountHistory.size() >= interval * 2) {\n-                idleCountHistory.remove();\n-              }\n-            }\n-          }\n-          idleCountHistory.add(workQueue.getWaitingConsumerCount());\n-        }\n-      }\n-    }, INITIAL_DELAY.toSeconds(), getPoolWatcherDelay().toSeconds(), SECONDS));\n-    this.transactionExecutor = pool;\n+    ThreadPools.watchCriticalScheduledTask(\n+        fatePoolsWatcher.scheduleWithFixedDelay(new FatePoolsWatcher(environment, conf),\n+            INITIAL_DELAY.toSeconds(), getPoolWatcherDelay().toSeconds(), SECONDS));\n \n     ScheduledExecutorService deadResCleanerExecutor = null;\n     if (runDeadResCleaner) {\n       // Create a dead reservation cleaner for this store that will periodically clean up\n       // reservations held by dead processes, if they exist.\n       deadResCleanerExecutor = ThreadPools.getServerThreadPools().createScheduledExecutorService(1,\n-          store.type() + \""-dead-reservation-cleaner-pool\"");\n+          store.type() == FateInstanceType.USER ? USER_DEAD_RESERVATION_CLEANER_POOL.poolName\n+              : META_DEAD_RESERVATION_CLEANER_POOL.poolName);\n       ScheduledFuture<?> deadReservationCleaner =\n           deadResCleanerExecutor.scheduleWithFixedDelay(new DeadReservationCleaner(),\n               INITIAL_DELAY.toSeconds(), getDeadResCleanupDelay().toSeconds(), SECONDS);\n@@ -518,8 +268,51 @@ public Fate(T environment, FateStore<T> store, boolean runDeadResCleaner,\n     }\n     this.deadResCleanerExecutor = deadResCleanerExecutor;\n \n-    this.workFinder = Threads.createThread(\""Fate work finder\"", new WorkFinder());\n-    this.workFinder.start();\n+    startFateExecutors(environment, conf, fateExecutors);\n+  }\n+\n+  protected void startFateExecutors(T environment, AccumuloConfiguration conf,\n+      Set<FateExecutor<T>> fateExecutors) {\n+    for (var poolConf : getPoolConfigurations(conf).entrySet()) {\n+      // no fate threads are running at this point; fine not to synchronize\n+      fateExecutors\n+          .add(new FateExecutor<>(this, environment, poolConf.getKey(), poolConf.getValue()));\n+    }\n+  }\n+\n+  /**\n+   * Returns a map of the current pool configurations as set in the given config. Each key is a set\n+   * of fate operations and each value is an integer for the number of threads assigned to work\n+   * those fate operations.\n+   */\n+  protected Map<Set<FateOperation>,Integer> getPoolConfigurations(AccumuloConfiguration conf) {\n+    Map<Set<FateOperation>,Integer> poolConfigs = new HashMap<>();\n+    final var json = JsonParser.parseString(conf.get(getFateConfigProp())).getAsJsonObject();\n+\n+    for (var entry : json.entrySet()) {\n+      var key = entry.getKey();\n+      var val = entry.getValue().getAsInt();\n+      var fateOpsStrArr = key.split(\"",\"");\n+      Set<FateOperation> fateOpsSet = Arrays.stream(fateOpsStrArr).map(FateOperation::valueOf)\n+          .collect(Collectors.toCollection(TreeSet::new));\n+\n+      poolConfigs.put(fateOpsSet, val);\n+    }\n+\n+    return poolConfigs;\n+  }\n+\n+  protected AtomicBoolean getKeepRunning() {\n+    return keepRunning;\n+  }\n+\n+  protected FateStore<T> getStore() {\n+    return store;\n+  }\n+\n+  protected Property getFateConfigProp() {\n+    return this.store.type() == FateInstanceType.USER ? Property.MANAGER_FATE_USER_CONFIG\n+        : Property.MANAGER_FATE_META_CONFIG;\n   }\n \n   public Duration getDeadResCleanupDelay() {\n@@ -530,9 +323,42 @@ public Duration getPoolWatcherDelay() {\n     return POOL_WATCHER_DELAY;\n   }\n \n+  /**\n+   * Returns the number of TransactionRunners active for the FateExecutor assigned to work on the\n+   * given set of fate operations. \""Active\"" meaning it is waiting for a transaction to work on or\n+   * actively working on one. Returns 0 if no such FateExecutor exists. This should only be used for\n+   * testing\n+   */\n+  @VisibleForTesting\n+  public int getTxRunnersActive(Set<FateOperation> fateOps) {\n+    synchronized (fateExecutors) {\n+      for (var fateExecutor : fateExecutors) {\n+        if (fateExecutor.getFateOps().equals(fateOps)) {\n+          return fateExecutor.getRunningTxRunners().size();\n+        }\n+      }\n+    }\n+    return 0;\n+  }\n+\n+  /**\n+   * Returns the total number of TransactionRunners active across all FateExecutors. This should\n+   * only be used for testing\n+   */\n+  @VisibleForTesting\n+  public int getTotalTxRunnersActive() {\n+    synchronized (fateExecutors) {\n+      return fateExecutors.stream().mapToInt(fe -> fe.getRunningTxRunners().size()).sum();\n+    }\n+  }\n+\n+  /**\n+   * Returns how many times it is warned that a pool size should be increased. Should only be used\n+   * for testing or in {@link FateExecutor}\n+   */\n   @VisibleForTesting\n-  public int getTxRunnersActive() {\n-    return runningTxRunners.size();\n+  public AtomicInteger getNeedMoreThreadsWarnCount() {\n+    return needMoreThreadsWarnCount;\n   }\n \n   // get a transaction id back to the requester before doing any work\n@@ -664,64 +490,92 @@ public Stream<FateKey> list(FateKey.FateKeyType type) {\n    * Initiates shutdown of background threads and optionally waits on them.\n    */\n   public void shutdown(long timeout, TimeUnit timeUnit) {\n+    log.info(\""Shutting down {} FATE\"", store.type());\n+\n     if (keepRunning.compareAndSet(true, false)) {\n-      fatePoolWatcher.shutdown();\n-      transactionExecutor.shutdown();\n-      workFinder.interrupt();\n+      synchronized (fateExecutors) {\n+        for (var fateExecutor : fateExecutors) {\n+          fateExecutor.initiateShutdown();\n+        }\n+      }\n       if (deadResCleanerExecutor != null) {\n         deadResCleanerExecutor.shutdown();\n       }\n+      fatePoolsWatcher.shutdown();\n     }\n \n     if (timeout > 0) {\n       long start = System.nanoTime();\n+      try {\n+        waitForAllFateExecShutdown(start, timeout, timeUnit);\n+        waitForDeadResCleanerShutdown(start, timeout, timeUnit);\n+        waitForFatePoolsWatcherShutdown(start, timeout, timeUnit);\n+\n+        if (anyFateExecutorIsAlive() || deadResCleanerIsAlive() || fatePoolsWatcherIsAlive()) {\n+          log.warn(\n+              \""Waited for {}ms for all fate {} background threads to stop, but some are still running. \""\n+                  + \""fate executor threads:{} dead reservation cleaner thread:{} \""\n+                  + \""fate pools watcher thread:{}\"",\n+              TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start), store.type(),\n+              anyFateExecutorIsAlive(), deadResCleanerIsAlive(), fatePoolsWatcherIsAlive());\n+        }\n+      } catch (InterruptedException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n \n-      while ((System.nanoTime() - start) < timeUnit.toNanos(timeout) && (workFinder.isAlive()\n-          || !transactionExecutor.isTerminated() || !fatePoolWatcher.isTerminated()\n-          || (deadResCleanerExecutor != null && !deadResCleanerExecutor.isTerminated()))) {\n-        try {\n-          if (!fatePoolWatcher.awaitTermination(1, SECONDS)) {\n-            log.debug(\""Fate {} is waiting for pool watcher to terminate\"", store.type());\n-            continue;\n-          }\n+    // interrupt the background threads\n+    synchronized (fateExecutors) {\n+      for (var fateExecutor : fateExecutors) {\n+        fateExecutor.shutdownNow();\n+        fateExecutor.getIdleCountHistory().clear();\n+      }\n+    }\n+    if (deadResCleanerExecutor != null) {\n+      deadResCleanerExecutor.shutdownNow();\n+    }\n+    fatePoolsWatcher.shutdownNow();\n+  }\n \n-          if (!transactionExecutor.awaitTermination(1, SECONDS)) {\n-            log.debug(\""Fate {} is waiting for worker threads to terminate\"", store.type());\n-            continue;\n-          }\n+  private boolean anyFateExecutorIsAlive() {\n+    synchronized (fateExecutors) {\n+      return fateExecutors.stream().anyMatch(FateExecutor::isAlive);\n+    }\n+  }\n \n-          if (deadResCleanerExecutor != null\n-              && !deadResCleanerExecutor.awaitTermination(1, SECONDS)) {\n-            log.debug(\""Fate {} is waiting for dead reservation cleaner thread to terminate\"",\n-                store.type());\n-            continue;\n-          }\n+  private boolean deadResCleanerIsAlive() {\n+    return deadResCleanerExecutor != null && !deadResCleanerExecutor.isTerminated();\n+  }\n \n-          workFinder.join(1_000);\n-          if (workFinder.isAlive()) {\n-            log.debug(\""Fate {} is waiting for work finder thread to terminate\"", store.type());\n-            workFinder.interrupt();\n-          }\n-        } catch (InterruptedException e) {\n-          throw new RuntimeException(e);\n-        }\n+  private boolean fatePoolsWatcherIsAlive() {\n+    return !fatePoolsWatcher.isTerminated();\n+  }\n+\n+  private void waitForAllFateExecShutdown(long start, long timeout, TimeUnit timeUnit)\n+      throws InterruptedException {\n+    synchronized (fateExecutors) {\n+      for (var fateExecutor : fateExecutors) {\n+        fateExecutor.waitForShutdown(start, timeout, timeUnit);\n       }\n+    }\n+  }\n \n-      if (workFinder.isAlive() || !transactionExecutor.isTerminated()\n-          || (deadResCleanerExecutor != null && !deadResCleanerExecutor.isTerminated())) {\n-        log.warn(\n-            \""Waited for {}ms for all fate {} background threads to stop, but some are still running. workFinder:{} transactionExecutor:{} deadResCleanerExecutor:{}\"",\n-            TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start), store.type(),\n-            workFinder.isAlive(), !transactionExecutor.isTerminated(),\n-            (deadResCleanerExecutor != null && !deadResCleanerExecutor.isTerminated()));\n+  private void waitForDeadResCleanerShutdown(long start, long timeout, TimeUnit timeUnit)\n+      throws InterruptedException {\n+    while (((System.nanoTime() - start) < timeUnit.toNanos(timeout)) && deadResCleanerIsAlive()) {\n+      if (deadResCleanerExecutor != null && !deadResCleanerExecutor.awaitTermination(1, SECONDS)) {\n+        log.debug(\""Fate {} is waiting for dead reservation cleaner thread to terminate\"",\n+            store.type());\n       }\n     }\n+  }\n \n-    // interrupt the background threads\n-    transactionExecutor.shutdownNow();\n-    if (deadResCleanerExecutor != null) {\n-      deadResCleanerExecutor.shutdownNow();\n+  private void waitForFatePoolsWatcherShutdown(long start, long timeout, TimeUnit timeUnit)\n+      throws InterruptedException {\n+    while (((System.nanoTime() - start) < timeUnit.toNanos(timeout)) && fatePoolsWatcherIsAlive()) {\n+      if (!fatePoolsWatcher.awaitTermination(1, SECONDS)) {\n+        log.debug(\""Fate {} is waiting for fate pools watcher thread to terminate\"", store.type());\n+      }\n     }\n-    idleCountHistory.clear();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateExecutor.java b/core/src/main/java/org/apache/accumulo/core/fate/FateExecutor.java\nnew file mode 100644\nindex 00000000000..7397fd3e696\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateExecutor.java\n@@ -0,0 +1,557 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.fate;\n+\n+import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.MINUTES;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.FAILED;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.FAILED_IN_PROGRESS;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.IN_PROGRESS;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUBMITTED;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUCCESSFUL;\n+import static org.apache.accumulo.core.util.ShutdownUtil.isIOException;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.LinkedTransferQueue;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TransferQueue;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n+import org.apache.accumulo.core.fate.Fate.TxInfo;\n+import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n+import org.apache.accumulo.core.util.ShutdownUtil;\n+import org.apache.accumulo.core.util.Timer;\n+import org.apache.accumulo.core.util.threads.ThreadPoolNames;\n+import org.apache.accumulo.core.util.threads.ThreadPools;\n+import org.apache.accumulo.core.util.threads.Threads;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * Handles finding and working on FATE work. Only finds/works on fate operations that it is assigned\n+ * to work on defined by 'fateOps'\n+ */\n+public class FateExecutor<T> {\n+  private static final Logger log = LoggerFactory.getLogger(FateExecutor.class);\n+  private final Logger runnerLog = LoggerFactory.getLogger(TransactionRunner.class);\n+\n+  private final T environment;\n+  private final Fate<T> fate;\n+  private final Thread workFinder;\n+  private final TransferQueue<FateId> workQueue;\n+  private final String poolName;\n+  private final ThreadPoolExecutor transactionExecutor;\n+  private final Set<TransactionRunner> runningTxRunners;\n+  private final Set<Fate.FateOperation> fateOps;\n+  private final ConcurrentLinkedQueue<Integer> idleCountHistory = new ConcurrentLinkedQueue<>();\n+\n+  public FateExecutor(Fate<T> fate, T environment, Set<Fate.FateOperation> fateOps, int poolSize) {\n+    final String operatesOn = fate.getStore().type().name().toLowerCase() + \"".\""\n+        + fateOps.stream().map(fo -> fo.name().toLowerCase()).collect(Collectors.joining(\"".\""));\n+    final String transactionRunnerPoolName =\n+        ThreadPoolNames.MANAGER_FATE_POOL_PREFIX.poolName + operatesOn;\n+    final String workFinderThreadName = \""fate.work.finder.\"" + operatesOn;\n+\n+    this.fate = fate;\n+    this.environment = environment;\n+    this.fateOps = Collections.unmodifiableSet(fateOps);\n+    this.workQueue = new LinkedTransferQueue<>();\n+    this.runningTxRunners = Collections.synchronizedSet(new HashSet<>());\n+    this.poolName = transactionRunnerPoolName;\n+    this.transactionExecutor =\n+        ThreadPools.getServerThreadPools().getPoolBuilder(transactionRunnerPoolName)\n+            .numCoreThreads(poolSize).enableThreadPoolMetrics().build();\n+\n+    this.workFinder = Threads.createThread(workFinderThreadName, new WorkFinder());\n+    this.workFinder.start();\n+  }\n+\n+  /**\n+   * resize the pool to match the config as necessary and submit new TransactionRunners if the pool\n+   * grew, stop TransactionRunners if the pool shrunk, and potentially suggest resizing the pool if\n+   * the load is consistently high.\n+   */\n+  protected void resizeFateExecutor(Map<Set<Fate.FateOperation>,Integer> poolConfigs,\n+      long idleCheckIntervalMillis) {\n+    final var pool = transactionExecutor;\n+    final var runningTxRunners = getRunningTxRunners();\n+    final int configured = poolConfigs.get(fateOps);\n+    ThreadPools.resizePool(pool, () -> configured, poolName);\n+    final int needed = configured - runningTxRunners.size();\n+    if (needed > 0) {\n+      // If the pool grew, then ensure that there is a TransactionRunner for each thread\n+      for (int i = 0; i < needed; i++) {\n+        try {\n+          pool.execute(new TransactionRunner());\n+        } catch (RejectedExecutionException e) {\n+          // RejectedExecutionException could be shutting down\n+          if (pool.isShutdown()) {\n+            // The exception is expected in this case, no need to spam the logs.\n+            log.trace(\""Expected error adding transaction runner to FaTE executor pool. \""\n+                + \""The pool is shutdown.\"", e);\n+          } else {\n+            // This is bad, FaTE may no longer work!\n+            log.error(\""Unexpected error adding transaction runner to FaTE executor pool.\"", e);\n+          }\n+          break;\n+        }\n+      }\n+      idleCountHistory.clear();\n+    } else if (needed < 0) {\n+      // If we need the pool to shrink, then ensure excess TransactionRunners are safely\n+      // stopped.\n+      // Flag the necessary number of TransactionRunners to safely stop when they are done\n+      // work on a transaction.\n+      int numFlagged = (int) runningTxRunners.stream()\n+          .filter(FateExecutor.TransactionRunner::isFlaggedToStop).count();\n+      int numToStop = -1 * (numFlagged + needed);\n+      for (var runner : runningTxRunners) {\n+        if (numToStop <= 0) {\n+          break;\n+        }\n+        if (runner.flagStop()) {\n+          log.trace(\""Flagging a TransactionRunner to stop...\"");\n+          numToStop--;\n+        }\n+      }\n+    } else {\n+      // The pool size did not change, but should it based on idle Fate threads? Maintain\n+      // count of the last X minutes of idle Fate threads. If zero 95% of the time, then\n+      // suggest that the pool size be increased or the fate ops assigned to that pool be\n+      // split into separate pools.\n+      final long interval = Math.min(60, TimeUnit.MILLISECONDS.toMinutes(idleCheckIntervalMillis));\n+      var fateConfigProp = fate.getFateConfigProp();\n+\n+      if (interval == 0) {\n+        idleCountHistory.clear();\n+      } else {\n+        if (idleCountHistory.size() >= interval * 2) { // this task runs every 30s\n+          int zeroFateThreadsIdleCount = 0;\n+          for (Integer idleConsumerCount : idleCountHistory) {\n+            if (idleConsumerCount == 0) {\n+              zeroFateThreadsIdleCount++;\n+            }\n+          }\n+          boolean needMoreThreads =\n+              (zeroFateThreadsIdleCount / (double) idleCountHistory.size()) >= 0.95;\n+          if (needMoreThreads) {\n+            fate.getNeedMoreThreadsWarnCount().incrementAndGet();\n+            log.warn(\n+                \""All {} Fate threads working on the fate ops {} appear to be busy for \""\n+                    + \""the last {} minutes. Consider increasing the value for the \""\n+                    + \""entry in the property {} or splitting the fate ops across \""\n+                    + \""multiple entries/pools.\"",\n+                fate.getStore().type(), fateOps, interval, fateConfigProp.getKey());\n+            // Clear the history so that we don't log for interval minutes.\n+            idleCountHistory.clear();\n+          } else {\n+            while (idleCountHistory.size() >= interval * 2) {\n+              idleCountHistory.remove();\n+            }\n+          }\n+        }\n+        idleCountHistory.add(workQueue.getWaitingConsumerCount());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * @return an unmodifiable, shallow copy of the currently running transaction runners\n+   */\n+  protected Set<TransactionRunner> getRunningTxRunners() {\n+    Set<TransactionRunner> copy;\n+    synchronized (runningTxRunners) {\n+      copy = new HashSet<>(runningTxRunners);\n+    }\n+    return Collections.unmodifiableSet(copy);\n+  }\n+\n+  protected Set<Fate.FateOperation> getFateOps() {\n+    return fateOps;\n+  }\n+\n+  /**\n+   * Initiates the shutdown of this FateExecutor. This means the pool executing TransactionRunners\n+   * will no longer accept new TransactionRunners, the currently running TransactionRunners will\n+   * terminate after they are done with their current transaction, if applicable, and the work\n+   * finder is interrupted. {@link #isShutdown()} returns true after this is called.\n+   */\n+  protected void initiateShutdown() {\n+    transactionExecutor.shutdown();\n+    synchronized (runningTxRunners) {\n+      runningTxRunners.forEach(TransactionRunner::flagStop);\n+    }\n+    workFinder.interrupt();\n+  }\n+\n+  /**\n+   * @return true if {@link #initiateShutdown()} has previously been called on this FateExecutor.\n+   *         The FateExecutor may or may not still have running threads. To check that, see\n+   *         {@link #isAlive()}\n+   */\n+  protected boolean isShutdown() {\n+    return transactionExecutor.isShutdown();\n+  }\n+\n+  protected void shutdownNow() {\n+    transactionExecutor.shutdownNow();\n+  }\n+\n+  protected void waitForShutdown(long start, long timeout, TimeUnit timeUnit)\n+      throws InterruptedException {\n+    if (timeout > 0) {\n+      while (((System.nanoTime() - start) < timeUnit.toNanos(timeout)) && isAlive()) {\n+        if (!transactionExecutor.awaitTermination(1, SECONDS)) {\n+          log.debug(\""Fate {} is waiting for worker threads for fate ops {} to terminate\"",\n+              fate.getStore().type(), fateOps);\n+          continue;\n+        }\n+\n+        workFinder.join(1_000);\n+        if (workFinder.isAlive()) {\n+          log.debug(\""Fate {} is waiting for work finder thread for fate ops {} to terminate\"",\n+              fate.getStore().type(), fateOps);\n+          workFinder.interrupt();\n+        }\n+      }\n+\n+      if (isAlive()) {\n+        log.warn(\n+            \""Waited for {}ms for the {} fate executor operating on fate ops {} to stop, but it\""\n+                + \"" is still running. Summary of run state of its threads: work finder:{}\""\n+                + \"" transaction executor:{}\"",\n+            TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start), fate.getStore().type(),\n+            fateOps, workFinder.isAlive(), !transactionExecutor.isTerminated());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * This fate executor is defined as being alive if any of its threads are running\n+   */\n+  protected boolean isAlive() {\n+    return !transactionExecutor.isTerminated() || workFinder.isAlive();\n+  }\n+\n+  protected ConcurrentLinkedQueue<Integer> getIdleCountHistory() {\n+    return idleCountHistory;\n+  }\n+\n+  /**\n+   * A single thread that finds transactions to work on and queues them up. Do not want each worker\n+   * thread going to the store and looking for work as it would place more load on the store.\n+   */\n+  private class WorkFinder implements Runnable {\n+\n+    @Override\n+    public void run() {\n+      while (fate.getKeepRunning().get() && !isShutdown()) {\n+        try {\n+          fate.getStore().runnable(fate.getKeepRunning(), fateIdStatus -> {\n+            // The FateId with the fate operation 'fateOp' is workable by this FateExecutor if\n+            // 1) This FateExecutor is assigned to work on 'fateOp' ('fateOp' is in 'fateOps')\n+            // 2) The transaction was cancelled while NEW. This is an edge case that needs to be\n+            // handled since this is the only case where we will have a runnable transaction\n+            // that doesn't have a name. We allow any FateExecutor to work on this since this case\n+            // should be rare and won't put much load on any one FateExecutor\n+            var status = fateIdStatus.getStatus();\n+            var fateOp = fateIdStatus.getFateOperation().orElse(null);\n+            if ((fateOp != null && fateOps.contains(fateOp))\n+                || txCancelledWhileNew(status, fateOp)) {\n+              while (fate.getKeepRunning().get() && !isShutdown()) {\n+                try {\n+                  // The reason for calling transfer instead of queueing is avoid rescanning the\n+                  // storage layer and adding the same thing over and over. For example if all\n+                  // threads were busy, the queue size was 100, and there are three runnable things\n+                  // in the store. Do not want to keep scanning the store adding those same 3\n+                  // runnable things until the queue is full.\n+                  if (workQueue.tryTransfer(fateIdStatus.getFateId(), 100, MILLISECONDS)) {\n+                    break;\n+                  }\n+                } catch (InterruptedException e) {\n+                  throw new IllegalStateException(e);\n+                }\n+              }\n+            }\n+          });\n+        } catch (Exception e) {\n+          if (!fate.getKeepRunning().get() || isShutdown()) {\n+            log.debug(\""Expected failure while attempting to find work for fate: either fate is \""\n+                + \""being shutdown and therefore all fate threads are being shutdown or the \""\n+                + \""fate threads assigned to work on {} were invalidated by config changes \""\n+                + \""and are being shutdown\"", fateOps, e);\n+          } else {\n+            log.warn(\""Unexpected failure while attempting to find work for fate\"", e);\n+          }\n+\n+          workQueue.clear();\n+        }\n+      }\n+    }\n+\n+    private boolean txCancelledWhileNew(TStatus status, Fate.FateOperation fateOp) {\n+      if (fateOp == null) {\n+        // The only time a transaction should be runnable and not have a fate operation is if\n+        // it was cancelled while NEW (and now is FAILED_IN_PROGRESS)\n+        Preconditions.checkState(status == FAILED_IN_PROGRESS);\n+        return true;\n+      }\n+      return false;\n+    }\n+  }\n+\n+  protected class TransactionRunner implements Runnable {\n+    // used to signal a TransactionRunner to stop in the case where there are too many running\n+    // i.e.,\n+    // 1. the property for the pool size decreased so we have to stop excess TransactionRunners\n+    // or\n+    // 2. this FateExecutor is no longer valid from config changes so we need to shutdown this\n+    // FateExecutor\n+    private final AtomicBoolean stop = new AtomicBoolean(false);\n+\n+    private Optional<FateTxStore<T>> reserveFateTx() throws InterruptedException {\n+      while (fate.getKeepRunning().get() && !stop.get()) {\n+        FateId unreservedFateId = workQueue.poll(100, MILLISECONDS);\n+\n+        if (unreservedFateId == null) {\n+          continue;\n+        }\n+        var optionalopStore = fate.getStore().tryReserve(unreservedFateId);\n+        if (optionalopStore.isPresent()) {\n+          return optionalopStore;\n+        }\n+      }\n+\n+      return Optional.empty();\n+    }\n+\n+    @Override\n+    public void run() {\n+      runningTxRunners.add(this);\n+      try {\n+        while (fate.getKeepRunning().get() && !stop.get()) {\n+          FateTxStore<T> txStore = null;\n+          ExecutionState state = new ExecutionState();\n+          try {\n+            var optionalopStore = reserveFateTx();\n+            if (optionalopStore.isPresent()) {\n+              txStore = optionalopStore.orElseThrow();\n+            } else {\n+              continue;\n+            }\n+            state.status = txStore.getStatus();\n+            state.op = txStore.top();\n+            if (state.status == FAILED_IN_PROGRESS) {\n+              processFailed(txStore, state.op);\n+            } else if (state.status == SUBMITTED || state.status == IN_PROGRESS) {\n+              try {\n+                execute(txStore, state);\n+                if (state.op != null && state.deferTime != 0) {\n+                  // The current op is not ready to execute\n+                  continue;\n+                }\n+              } catch (StackOverflowException e) {\n+                // the op that failed to push onto the stack was never executed, so no need to undo\n+                // it just transition to failed and undo the ops that executed\n+                transitionToFailed(txStore, e);\n+                continue;\n+              } catch (Exception e) {\n+                blockIfHadoopShutdown(txStore.getID(), e);\n+                transitionToFailed(txStore, e);\n+                continue;\n+              }\n+\n+              if (state.op == null) {\n+                // transaction is finished\n+                String ret = state.prevOp.getReturn();\n+                if (ret != null) {\n+                  txStore.setTransactionInfo(TxInfo.RETURN_VALUE, ret);\n+                }\n+                txStore.setStatus(SUCCESSFUL);\n+                doCleanUp(txStore);\n+              }\n+            }\n+          } catch (Exception e) {\n+            runnerLog.error(\""Uncaught exception in FATE runner thread.\"", e);\n+          } finally {\n+            if (txStore != null) {\n+              txStore.unreserve(Duration.ofMillis(state.deferTime));\n+            }\n+          }\n+        }\n+      } finally {\n+        log.trace(\""A TransactionRunner is exiting...\"");\n+        Preconditions.checkState(runningTxRunners.remove(this));\n+      }\n+    }\n+\n+    private class ExecutionState {\n+      Repo<T> prevOp = null;\n+      Repo<T> op = null;\n+      long deferTime = 0;\n+      TStatus status;\n+    }\n+\n+    // Executes as many steps of a fate operation as possible\n+    private void execute(final FateTxStore<T> txStore, final ExecutionState state)\n+        throws Exception {\n+      while (state.op != null && state.deferTime == 0) {\n+        state.deferTime = executeIsReady(txStore.getID(), state.op);\n+\n+        if (state.deferTime == 0) {\n+          if (state.status == SUBMITTED) {\n+            txStore.setStatus(IN_PROGRESS);\n+            state.status = IN_PROGRESS;\n+          }\n+\n+          state.prevOp = state.op;\n+          state.op = executeCall(txStore.getID(), state.op);\n+\n+          if (state.op != null) {\n+            // persist the completion of this step before starting to run the next so in the case of\n+            // process death the completed steps are not rerun\n+            txStore.push(state.op);\n+          }\n+        }\n+      }\n+    }\n+\n+    /**\n+     * The Hadoop Filesystem registers a java shutdown hook that closes the file system. This can\n+     * cause threads to get spurious IOException. If this happens, instead of failing a FATE\n+     * transaction just wait for process to die. When the manager start elsewhere the FATE\n+     * transaction can resume.\n+     */\n+    private void blockIfHadoopShutdown(FateId fateId, Exception e) {\n+      if (ShutdownUtil.isShutdownInProgress()) {\n+\n+        if (e instanceof AcceptableException) {\n+          log.debug(\""Ignoring exception possibly caused by Hadoop Shutdown hook. {} \"", fateId, e);\n+        } else if (isIOException(e)) {\n+          log.info(\""Ignoring exception likely caused by Hadoop Shutdown hook. {} \"", fateId, e);\n+        } else {\n+          // sometimes code will catch an IOException caused by the hadoop shutdown hook and throw\n+          // another exception without setting the cause.\n+          log.warn(\""Ignoring exception possibly caused by Hadoop Shutdown hook. {} \"", fateId, e);\n+        }\n+\n+        while (true) {\n+          // Nothing is going to work well at this point, so why even try. Just wait for the end,\n+          // preventing this FATE thread from processing further work and likely failing.\n+          sleepUninterruptibly(1, MINUTES);\n+        }\n+      }\n+    }\n+\n+    private void transitionToFailed(FateTxStore<T> txStore, Exception e) {\n+      final String msg = \""Failed to execute Repo \"" + txStore.getID();\n+      // Certain FATE ops that throw exceptions don't need to be propagated up to the Monitor\n+      // as a warning. They're a normal, handled failure condition.\n+      if (e instanceof AcceptableException) {\n+        var tableOpEx = (AcceptableThriftTableOperationException) e;\n+        log.info(\""{} for table:{}({}) saw acceptable exception: {}\"", msg, tableOpEx.getTableName(),\n+            tableOpEx.getTableId(), tableOpEx.getDescription());\n+      } else {\n+        log.warn(msg, e);\n+      }\n+      txStore.setTransactionInfo(TxInfo.EXCEPTION, e);\n+      txStore.setStatus(FAILED_IN_PROGRESS);\n+      log.info(\""Updated status for Repo with {} to FAILED_IN_PROGRESS\"", txStore.getID());\n+    }\n+\n+    private void processFailed(FateTxStore<T> txStore, Repo<T> op) {\n+      while (op != null) {\n+        undo(txStore.getID(), op);\n+\n+        txStore.pop();\n+        op = txStore.top();\n+      }\n+\n+      txStore.setStatus(FAILED);\n+      doCleanUp(txStore);\n+    }\n+\n+    private void doCleanUp(FateTxStore<T> txStore) {\n+      Boolean autoClean = (Boolean) txStore.getTransactionInfo(TxInfo.AUTO_CLEAN);\n+      if (autoClean != null && autoClean) {\n+        txStore.delete();\n+      } else {\n+        // no longer need persisted operations, so delete them to save space in case\n+        // TX is never cleaned up...\n+        while (txStore.top() != null) {\n+          txStore.pop();\n+        }\n+      }\n+    }\n+\n+    private void undo(FateId fateId, Repo<T> op) {\n+      try {\n+        op.undo(fateId, environment);\n+      } catch (Exception e) {\n+        log.warn(\""Failed to undo Repo, \"" + fateId, e);\n+      }\n+    }\n+\n+    protected boolean flagStop() {\n+      return stop.compareAndSet(false, true);\n+    }\n+\n+    protected boolean isFlaggedToStop() {\n+      return stop.get();\n+    }\n+\n+  }\n+\n+  protected long executeIsReady(FateId fateId, Repo<T> op) throws Exception {\n+    var startTime = Timer.startNew();\n+    var deferTime = op.isReady(fateId, environment);\n+    log.debug(\""Running {}.isReady() {} took {} ms and returned {}\"", op.getName(), fateId,\n+        startTime.elapsed(MILLISECONDS), deferTime);\n+    return deferTime;\n+  }\n+\n+  protected Repo<T> executeCall(FateId fateId, Repo<T> op) throws Exception {\n+    var startTime = Timer.startNew();\n+    var next = op.call(fateId, environment);\n+    log.debug(\""Running {}.call() {} took {} ms and returned {}\"", op.getName(), fateId,\n+        startTime.elapsed(MILLISECONDS), next == null ? \""null\"" : next.getName());\n+\n+    return next;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return String.format(\""FateExecutor:{FateOps=%s,PoolSize:%s}\"", fateOps, runningTxRunners.size());\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\nindex 854452ce642..29ba43fa0a1 100644\n--- a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n+++ b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n@@ -140,7 +140,7 @@ public Stream<FateKey> list(FateKey.FateKeyType type) {\n       }\n \n       @Override\n-      public void runnable(AtomicBoolean keepWaiting, Consumer<FateId> idConsumer) {\n+      public void runnable(AtomicBoolean keepWaiting, Consumer<FateIdStatus> idConsumer) {\n         store.runnable(keepWaiting, idConsumer);\n       }\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\nindex 3584fb23a04..e7e9cf50f3b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\n@@ -44,7 +44,9 @@ public enum ThreadPoolNames {\n   IMPORT_TABLE_RENAME_POOL(\""accumulo.pool.import.table.rename\""),\n   INSTANCE_OPS_COMPACTIONS_FINDER_POOL(\""accumulo.pool.instance.ops.active.compactions.finder\""),\n   INSTANCE_OPS_SCANS_FINDER_POOL(\""accumulo.pool.instance.ops.active.scans.finder\""),\n-  MANAGER_FATE_POOL(\""accumulo.pool.manager.fate\""),\n+  MANAGER_FATE_POOL_PREFIX(\""accumulo.pool.manager.fate.\""),\n+  USER_DEAD_RESERVATION_CLEANER_POOL(\""accumulo.pool.manager.fate.user.dead.reservation.cleaner\""),\n+  META_DEAD_RESERVATION_CLEANER_POOL(\""accumulo.pool.manager.fate.meta.dead.reservation.cleaner\""),\n   MANAGER_STATUS_POOL(\""accumulo.pool.manager.status\""),\n   MANAGER_UPGRADE_COORDINATOR_METADATA_POOL(\""accumulo.pool.manager.upgrade.metadata\""),\n   METADATA_DEFAULT_SPLIT_POOL(\""accumulo.pool.metadata.tablet.default.splitter\""),\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java\nindex 2fc26fc91df..6b8e96f3386 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPools.java\n@@ -28,7 +28,6 @@\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.GC_DELETE_POOL;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.GC_WAL_DELETE_POOL;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.GENERAL_SERVER_POOL;\n-import static org.apache.accumulo.core.util.threads.ThreadPoolNames.MANAGER_FATE_POOL;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.MANAGER_STATUS_POOL;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.SCHED_FUTURE_CHECKER_POOL;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.TSERVER_ASSIGNMENT_POOL;\n@@ -289,12 +288,6 @@ public ThreadPoolExecutor createExecutorService(final AccumuloConfiguration conf\n       case GENERAL_THREADPOOL_SIZE:\n         return createScheduledExecutorService(conf.getCount(p), GENERAL_SERVER_POOL.poolName,\n             emitThreadPoolMetrics);\n-      case MANAGER_FATE_THREADPOOL_SIZE:\n-        builder = getPoolBuilder(MANAGER_FATE_POOL).numCoreThreads(conf.getCount(p));\n-        if (emitThreadPoolMetrics) {\n-          builder.enableThreadPoolMetrics();\n-        }\n-        return builder.build();\n       case MANAGER_STATUS_THREAD_POOL_SIZE:\n         builder = getPoolBuilder(MANAGER_STATUS_POOL);\n         int threads = conf.getCount(p);\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\nindex 9b7d7965d61..e917246182f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\n@@ -165,7 +165,7 @@ public FateTxStore<T> reserve(FateId fateId) {\n       EnumSet.of(TStatus.SUBMITTED, TStatus.FAILED_IN_PROGRESS);\n \n   @Override\n-  public void runnable(AtomicBoolean keepWaiting, Consumer<FateId> idConsumer) {\n+  public void runnable(AtomicBoolean keepWaiting, Consumer<FateIdStatus> idConsumer) {\n \n     AtomicLong seen = new AtomicLong(0);\n \n@@ -192,7 +192,7 @@ public void runnable(AtomicBoolean keepWaiting, Consumer<FateId> idConsumer) {\n               return fateIdStatus.getFateReservation().isEmpty();\n             }).forEach(fateIdStatus -> {\n               seen.incrementAndGet();\n-              idConsumer.accept(fateIdStatus.getFateId());\n+              idConsumer.accept(fateIdStatus);\n             });\n       }\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java\nindex 8a7efd5f91a..263a9b090b9 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/ReadOnlyFateStore.java\n@@ -128,6 +128,8 @@ interface FateIdStatus {\n     Optional<FateStore.FateReservation> getFateReservation();\n \n     TStatus getStatus();\n+\n+    Optional<Fate.FateOperation> getFateOperation();\n   }\n \n   /**\n@@ -161,7 +163,7 @@ interface FateIdStatus {\n    * is found or until the keepWaiting parameter is false. It will return once all runnable ids\n    * found were passed to the consumer.\n    */\n-  void runnable(AtomicBoolean keepWaiting, Consumer<FateId> idConsumer);\n+  void runnable(AtomicBoolean keepWaiting, Consumer<FateIdStatus> idConsumer);\n \n   /**\n    * Returns true if the deferred map was cleared and if deferred executions are currently disabled\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex 2a6efbed0dc..de292ec0c49 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -274,21 +274,24 @@ protected Stream<FateIdStatus> getTransactions(EnumSet<TStatus> statuses) {\n       RowFateStatusFilter.configureScanner(scanner, statuses);\n       TxColumnFamily.STATUS_COLUMN.fetch(scanner);\n       TxColumnFamily.RESERVATION_COLUMN.fetch(scanner);\n+      TxInfoColumnFamily.FATE_OP_COLUMN.fetch(scanner);\n       return scanner.stream().onClose(scanner::close).map(e -> {\n         String txUUIDStr = e.getKey().getRow().toString();\n         FateId fateId = FateId.from(fateInstanceType, txUUIDStr);\n         SortedMap<Key,Value> rowMap;\n         TStatus status = TStatus.UNKNOWN;\n         FateReservation reservation = null;\n+        Fate.FateOperation fateOp = null;\n         try {\n           rowMap = WholeRowIterator.decodeRow(e.getKey(), e.getValue());\n         } catch (IOException ex) {\n           throw new RuntimeException(ex);\n         }\n-        // expect status and optionally reservation\n-        Preconditions.checkState(rowMap.size() == 1 || rowMap.size() == 2,\n-            \""Invalid row seen: %s. Expected to see one entry for the status and optionally an \""\n-                + \""entry for the fate reservation\"",\n+        // Always expect a status, optionally expect a fate operation (present if seeded)\n+        // and optionally expect a fate reservation (present if currently reserved)\n+        Preconditions.checkState(rowMap.size() >= 1 && rowMap.size() <= 3,\n+            \""Invalid row seen: %s. Expected to see tx status and optionally a fate op and \""\n+                + \""optionally a fate reservation\"",\n             rowMap);\n         for (Entry<Key,Value> entry : rowMap.entrySet()) {\n           Text colf = entry.getKey().getColumnFamily();\n@@ -301,12 +304,16 @@ protected Stream<FateIdStatus> getTransactions(EnumSet<TStatus> statuses) {\n             case TxColumnFamily.RESERVATION:\n               reservation = FateReservation.deserialize(val.get());\n               break;\n+            case TxInfoColumnFamily.FATE_OP:\n+              fateOp = (Fate.FateOperation) deserializeTxInfo(TxInfo.FATE_OP, val.get());\n+              break;\n             default:\n               throw new IllegalStateException(\""Unexpected column seen: \"" + colf + \"":\"" + colq);\n           }\n         }\n         final TStatus finalStatus = status;\n         final Optional<FateReservation> finalReservation = Optional.ofNullable(reservation);\n+        final Optional<Fate.FateOperation> finalFateOp = Optional.ofNullable(fateOp);\n         return new FateIdStatusBase(fateId) {\n           @Override\n           public TStatus getStatus() {\n@@ -317,6 +324,11 @@ public TStatus getStatus() {\n           public Optional<FateReservation> getFateReservation() {\n             return finalReservation;\n           }\n+\n+          @Override\n+          public Optional<Fate.FateOperation> getFateOperation() {\n+            return finalFateOp;\n+          }\n         };\n       });\n     } catch (TableNotFoundException e) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex dd7782f79a1..2d769bce6a5 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -594,6 +594,12 @@ public TStatus getStatus() {\n           public Optional<FateReservation> getFateReservation() {\n             return nodeSupplier.get().reservation;\n           }\n+\n+          @Override\n+          public Optional<Fate.FateOperation> getFateOperation() {\n+            var fateOp = (Fate.FateOperation) nodeSupplier.get().txInfo.get(TxInfo.FATE_OP);\n+            return Optional.ofNullable(fateOp);\n+          }\n         };\n       });\n \n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\nindex 38212140ba2..4fce2c26e2d 100644\n--- a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTest.java\n@@ -135,9 +135,14 @@ public void testJson() {\n   }\n \n   @Test\n+  @SuppressWarnings(\""deprecation\"")\n   public void testPropertyValidation() {\n \n     for (Property property : Property.values()) {\n+      if (property == Property.MANAGER_FATE_THREADPOOL_SIZE) {\n+        // deprecated and unused property, no need to test\n+        continue;\n+      }\n       PropertyType propertyType = property.getType();\n       String invalidValue, validValue = property.getDefaultValue();\n       LOG.debug(\""Testing property: {} with type: {}\"", property.getKey(), propertyType);\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java\nindex 6f870aa5751..55cd151283d 100644\n--- a/core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/conf/PropertyTypeTest.java\n@@ -28,6 +28,7 @@\n import java.util.stream.Stream;\n \n import org.apache.accumulo.core.WithTestNames;\n+import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.file.rfile.RFile;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n@@ -230,4 +231,84 @@ public void testTypeVOLUMES() {\n         \""hdfs:/volA,hdfs :/::/volB\"");\n   }\n \n+  @Test\n+  public void testTypeFATE_USER_CONFIG() {\n+    var allUserFateOps = Fate.FateOperation.getAllUserFateOps();\n+    int poolSize1 = allUserFateOps.size() / 2;\n+    var validPool1Ops =\n+        allUserFateOps.stream().map(Enum::name).limit(poolSize1).collect(Collectors.joining(\"",\""));\n+    var validPool2Ops =\n+        allUserFateOps.stream().map(Enum::name).skip(poolSize1).collect(Collectors.joining(\"",\""));\n+    // should be valid: one pool for all ops, order should not matter, all ops split across\n+    // multiple pools (note validated in the same order as described here)\n+    valid(\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + validPool2Ops + \"",\"" + validPool1Ops + \""\\\"": 10}\"",\n+        \""{\\\""\"" + validPool1Ops + \""\\\"": 2, \\\""\"" + validPool2Ops + \""\\\"": 3}\"");\n+    // should be invalid: invalid json, null, missing FateOperation, pool size of 0, pool size of\n+    // -1, invalid pool size, invalid key, same FateOperation repeated in a different pool, invalid\n+    // FateOperation (note validated in the same order as described here)\n+    var invalidPool1Ops =\n+        allUserFateOps.stream().map(Enum::name).limit(poolSize1).collect(Collectors.joining(\"",\""));\n+    var invalidPool2Ops = allUserFateOps.stream().map(Enum::name).skip(poolSize1 + 1)\n+        .collect(Collectors.joining(\"",\""));\n+    invalid(\""\"", null, \""{\\\""\"" + invalidPool1Ops + \""\\\"": 2, \\\""\"" + invalidPool2Ops + \""\\\"": 3}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 0}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": -1}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": x}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"", \""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10, \\\""\""\n+            + allUserFateOps.stream().map(Enum::name).limit(1).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + allUserFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \"",INVALID_FATEOP\\\"": 10}\"");\n+  }\n+\n+  @Test\n+  public void testTypeFATE_META_CONFIG() {\n+    var allMetaFateOps = Fate.FateOperation.getAllMetaFateOps();\n+    int poolSize1 = allMetaFateOps.size() / 2;\n+    var validPool1Ops =\n+        allMetaFateOps.stream().map(Enum::name).limit(poolSize1).collect(Collectors.joining(\"",\""));\n+    var validPool2Ops =\n+        allMetaFateOps.stream().map(Enum::name).skip(poolSize1).collect(Collectors.joining(\"",\""));\n+    // should be valid: one pool for all ops, order should not matter, all ops split across\n+    // multiple pools (note validated in the same order as described here)\n+    valid(\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + validPool2Ops + \"",\"" + validPool1Ops + \""\\\"": 10}\"",\n+        \""{\\\""\"" + validPool1Ops + \""\\\"": 2, \\\""\"" + validPool2Ops + \""\\\"": 3}\"");\n+\n+    var invalidPool1Ops =\n+        allMetaFateOps.stream().map(Enum::name).limit(poolSize1).collect(Collectors.joining(\"",\""));\n+    var invalidPool2Ops = allMetaFateOps.stream().map(Enum::name).skip(poolSize1 + 1)\n+        .collect(Collectors.joining(\"",\""));\n+    // should be invalid: invalid json, null, missing FateOperation, pool size of 0, pool size of\n+    // -1, invalid pool size, invalid key, same FateOperation repeated in a different pool, invalid\n+    // FateOperation (note validated in the same order as described here)\n+    invalid(\""\"", null, \""{\\\""\"" + invalidPool1Ops + \""\\\"": 2, \\\""\"" + invalidPool2Ops + \""\\\"": 3}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 0}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": -1}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": x}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"", \""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10, \\\""\""\n+            + allMetaFateOps.stream().map(Enum::name).limit(1).collect(Collectors.joining(\"",\""))\n+            + \""\\\"": 10}\"",\n+        \""{\\\""\"" + allMetaFateOps.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+            + \"",INVALID_FATEOP\\\"": 10}\"");\n+  }\n+\n+  @Test\n+  public void testTypeFATE_THREADPOOL_SIZE() {\n+    // nothing to test, this type is used for a deprecated property and will accept any prop value.\n+  }\n+\n }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\nindex c9288b90890..8e881fddad5 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n@@ -253,6 +253,12 @@ public Optional<FateReservation> getFateReservation() {\n         throw new UnsupportedOperationException(\n             \""Only the 'reserved' set should be used for reservations in the test store\"");\n       }\n+\n+      @Override\n+      public Optional<Fate.FateOperation> getFateOperation() {\n+        throw new UnsupportedOperationException(\""Test not configured or expected to be calling \""\n+            + \""this method. Functionality can be added if needed.\"");\n+      }\n     });\n   }\n \n@@ -267,7 +273,7 @@ public Stream<FateKey> list(FateKey.FateKeyType type) {\n   }\n \n   @Override\n-  public void runnable(AtomicBoolean keepWaiting, Consumer<FateId> idConsumer) {\n+  public void runnable(AtomicBoolean keepWaiting, Consumer<FateIdStatus> idConsumer) {\n     throw new UnsupportedOperationException();\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\nindex 77c4c240405..2235b429b6c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n@@ -59,6 +59,7 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n@@ -241,7 +242,10 @@ public static void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuratio\n     cfg.setProperty(Property.COMPACTOR_MIN_JOB_WAIT_TIME, \""100ms\"");\n     cfg.setProperty(Property.COMPACTOR_MAX_JOB_WAIT_TIME, \""1s\"");\n     cfg.setProperty(Property.GENERAL_THREADPOOL_SIZE, \""10\"");\n-    cfg.setProperty(Property.MANAGER_FATE_THREADPOOL_SIZE, \""10\"");\n+    cfg.setProperty(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\"" + Fate.FateOperation\n+        .getAllUserFateOps().stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 10}\"");\n+    cfg.setProperty(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\"" + Fate.FateOperation\n+        .getAllMetaFateOps().stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 10}\"");\n     cfg.setProperty(Property.MANAGER_TABLET_GROUP_WATCHER_INTERVAL, \""1s\"");\n     // use raw local file system so walogs sync and flush will work\n     coreSite.set(\""fs.file.impl\"", RawLocalFileSystem.class.getName());\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\nindex 6d453a39322..d71ae6ea25c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n@@ -32,7 +32,7 @@\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.row;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.verify;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.writeData;\n-import static org.apache.accumulo.test.fate.FateStoreUtil.seedTransaction;\n+import static org.apache.accumulo.test.fate.FateTestUtil.seedTransaction;\n import static org.apache.accumulo.test.util.FileMetadataUtil.countFencedFiles;\n import static org.apache.accumulo.test.util.FileMetadataUtil.splitFilesIntoRanges;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\nindex ec48e6f9229..f798e0d93da 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateExecutionOrderIT.java\n@@ -20,6 +20,7 @@\n \n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUBMITTED;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUCCESSFUL;\n+import static org.apache.accumulo.test.fate.FateTestUtil.TEST_FATE_OP;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n@@ -45,8 +46,6 @@\n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n import org.apache.accumulo.core.client.admin.TabletAvailability;\n-import org.apache.accumulo.core.conf.ConfigurationCopy;\n-import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Value;\n@@ -57,7 +56,6 @@\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.server.ServerContext;\n-import org.apache.accumulo.test.fate.FateTestRunner.TestEnv;\n import org.apache.hadoop.io.Text;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n@@ -194,10 +192,8 @@ private void waitFor(FateStore<FeoTestEnv> store, FateId txid) throws Exception\n   }\n \n   protected Fate<FeoTestEnv> initializeFate(AccumuloClient client, FateStore<FeoTestEnv> store) {\n-    ConfigurationCopy config = new ConfigurationCopy();\n-    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n-    config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, \""1\"");\n-    return new Fate<>(new FeoTestEnv(client), store, false, r -> r + \""\"", config);\n+    return new Fate<>(new FeoTestEnv(client), store, false, r -> r + \""\"",\n+        FateTestUtil.createTestFateConfig(1));\n   }\n \n   private static Entry<FateId,String> toIdStep(Entry<Key,Value> e) {\n@@ -227,7 +223,7 @@ protected void testInterleaving(FateStore<FeoTestEnv> store, ServerContext sctx)\n       var txStore = store.reserve(fateIds[i]);\n       try {\n         txStore.push(new FirstOp());\n-        txStore.setTransactionInfo(TxInfo.FATE_OP, \""TEST_\"" + i);\n+        txStore.setTransactionInfo(TxInfo.FATE_OP, TEST_FATE_OP);\n         txStore.setStatus(SUBMITTED);\n       } finally {\n         txStore.unreserve(Duration.ZERO);\n@@ -359,7 +355,7 @@ protected void testNonInterleaving(FateStore<FeoTestEnv> store, ServerContext sc\n       var txStore = store.reserve(fateIds[i]);\n       try {\n         txStore.push(new FirstNonInterleavingOp());\n-        txStore.setTransactionInfo(TxInfo.FATE_OP, \""TEST_\"" + i);\n+        txStore.setTransactionInfo(TxInfo.FATE_OP, TEST_FATE_OP);\n         txStore.setStatus(SUBMITTED);\n       } finally {\n         txStore.unreserve(Duration.ZERO);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\nindex b76709cd750..bb70e6dffdb 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n@@ -25,7 +25,7 @@\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.NEW;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUBMITTED;\n import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.UNKNOWN;\n-import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n+import static org.apache.accumulo.test.fate.FateTestUtil.TEST_FATE_OP;\n import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n@@ -43,8 +43,6 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicLong;\n \n-import org.apache.accumulo.core.conf.ConfigurationCopy;\n-import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.fate.FateId;\n@@ -532,14 +530,15 @@ protected void testNoWriteAfterDelete(FateStore<TestEnv> store, ServerContext sc\n     assertDoesNotThrow(() -> txStore.push(repo));\n     assertDoesNotThrow(() -> txStore.setStatus(ReadOnlyFateStore.TStatus.SUCCESSFUL));\n     assertDoesNotThrow(txStore::pop);\n-    assertDoesNotThrow(() -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, \""name\""));\n+    assertDoesNotThrow(() -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, TEST_FATE_OP));\n     assertDoesNotThrow(txStore::delete);\n \n     // test that all write ops result in an exception since the tx has been deleted\n     assertThrows(Exception.class, () -> txStore.push(repo));\n     assertThrows(Exception.class, () -> txStore.setStatus(ReadOnlyFateStore.TStatus.SUCCESSFUL));\n     assertThrows(Exception.class, txStore::pop);\n-    assertThrows(Exception.class, () -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, \""name\""));\n+    assertThrows(Exception.class,\n+        () -> txStore.setTransactionInfo(Fate.TxInfo.FATE_OP, TEST_FATE_OP));\n     assertThrows(Exception.class, txStore::delete);\n   }\n \n@@ -553,10 +552,8 @@ private void submitDeferred(Fate<TestEnv> fate, ServerContext sctx, Set<FateId>\n   }\n \n   protected Fate<TestEnv> initializeFate(FateStore<TestEnv> store) {\n-    ConfigurationCopy config = new ConfigurationCopy();\n-    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n-    config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, \""1\"");\n-    return new Fate<>(new TestEnv(), store, false, r -> r + \""\"", config);\n+    return new Fate<>(new TestEnv(), store, false, r -> r + \""\"",\n+        FateTestUtil.createTestFateConfig(1));\n   }\n \n   protected abstract TStatus getTxStatus(ServerContext sctx, FateId fateId);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\nindex bd6f7a22a10..f887fd40621 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateOpsCommandsIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n+import static org.apache.accumulo.test.fate.FateTestUtil.TEST_FATE_OP;\n import static org.easymock.EasyMock.expect;\n import static org.easymock.EasyMock.replay;\n import static org.easymock.EasyMock.verify;\n@@ -769,6 +769,11 @@ public ReadOnlyFateStore.TStatus getStatus() {\n       public Optional<FateStore.FateReservation> getFateReservation() {\n         return Optional.empty();\n       }\n+\n+      @Override\n+      public Optional<Fate.FateOperation> getFateOperation() {\n+        return Optional.empty();\n+      }\n     };\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FatePoolResizeIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FatePoolResizeIT.java\ndeleted file mode 100644\nindex 53f529613e7..00000000000\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FatePoolResizeIT.java\n+++ /dev/null\n@@ -1,167 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.test.fate;\n-\n-import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicInteger;\n-\n-import org.apache.accumulo.core.conf.ConfigurationCopy;\n-import org.apache.accumulo.core.conf.Property;\n-import org.apache.accumulo.core.fate.Fate;\n-import org.apache.accumulo.core.fate.FateId;\n-import org.apache.accumulo.core.fate.FateStore;\n-import org.apache.accumulo.core.fate.Repo;\n-import org.apache.accumulo.harness.SharedMiniClusterBase;\n-import org.apache.accumulo.server.ServerContext;\n-import org.apache.accumulo.test.util.Wait;\n-import org.junit.jupiter.api.Test;\n-\n-public abstract class FatePoolResizeIT extends SharedMiniClusterBase\n-    implements FateTestRunner<FatePoolResizeIT.PoolResizeTestEnv> {\n-\n-  @Test\n-  public void testIncreaseSize() throws Exception {\n-    executeTest(this::testIncreaseSize);\n-  }\n-\n-  protected void testIncreaseSize(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n-      throws Exception {\n-    final int numThreads = 10;\n-    final int newNumThreads = 20;\n-    final ConfigurationCopy config = initConfig(numThreads);\n-    final var env = new PoolResizeTestEnv();\n-    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n-    try {\n-      // create a tx for all future threads. For now, only some of the txns will be workable\n-      for (int i = 0; i < newNumThreads; i++) {\n-        var fateId = fate.startTransaction();\n-        fate.seedTransaction(TEST_FATE_OP, fateId, new PoolResizeTestRepo(), true, \""testing\"");\n-      }\n-      // wait for all available threads to be working on a tx\n-      Wait.waitFor(() -> env.numWorkers.get() == numThreads);\n-      assertEquals(numThreads, fate.getTxRunnersActive());\n-      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, String.valueOf(newNumThreads));\n-      // wait for new config to be detected, new runners to be created, and for these runners to\n-      // pick up the rest of the available txns\n-      Wait.waitFor(() -> env.numWorkers.get() == newNumThreads);\n-      assertEquals(newNumThreads, fate.getTxRunnersActive());\n-      // finish work\n-      env.isReadyLatch.countDown();\n-      Wait.waitFor(() -> env.numWorkers.get() == 0);\n-      // workers should still be running since we haven't shutdown, just not working on anything\n-      assertEquals(newNumThreads, fate.getTxRunnersActive());\n-    } finally {\n-      fate.shutdown(1, TimeUnit.MINUTES);\n-      assertEquals(0, fate.getTxRunnersActive());\n-    }\n-  }\n-\n-  @Test\n-  public void testDecreaseSize() throws Exception {\n-    executeTest(this::testDecreaseSize);\n-  }\n-\n-  protected void testDecreaseSize(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n-      throws Exception {\n-    final int numThreads = 20;\n-    final int newNumThreads = 10;\n-    final ConfigurationCopy config = initConfig(numThreads);\n-    final var env = new PoolResizeTestEnv();\n-    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n-    try {\n-      // create a tx for each thread\n-      for (int i = 0; i < numThreads; i++) {\n-        var fateId = fate.startTransaction();\n-        fate.seedTransaction(TEST_FATE_OP, fateId, new PoolResizeTestRepo(), true, \""testing\"");\n-      }\n-      // wait for all threads to be working on a tx\n-      Wait.waitFor(() -> env.numWorkers.get() == numThreads);\n-      assertEquals(numThreads, fate.getTxRunnersActive());\n-      config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, String.valueOf(newNumThreads));\n-      // ensure another execution of the pool watcher task occurs after we change the size\n-      Thread.sleep(fate.getPoolWatcherDelay().toMillis() * 2);\n-      // at this point, FATE should detect that there are more transaction runners running than\n-      // configured. None can be safely stopped yet, (still in progress - haven't passed isReady).\n-      // We ensure none have been unexpectedly stopped, then we allow isReady to pass and the txns\n-      // to complete\n-      assertEquals(numThreads, env.numWorkers.get());\n-      env.isReadyLatch.countDown();\n-      // wait for the pool size to be decreased to the expected value\n-      Wait.waitFor(() -> fate.getTxRunnersActive() == newNumThreads);\n-      // wait for all threads to have completed their tx\n-      Wait.waitFor(() -> env.numWorkers.get() == 0);\n-      // wait a bit longer to ensure no more workers than expected were stopped\n-      Thread.sleep(5_000);\n-      assertEquals(newNumThreads, fate.getTxRunnersActive());\n-    } finally {\n-      fate.shutdown(1, TimeUnit.MINUTES);\n-      assertEquals(0, fate.getTxRunnersActive());\n-    }\n-  }\n-\n-  private ConfigurationCopy initConfig(int numThreads) {\n-    ConfigurationCopy config = new ConfigurationCopy();\n-    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n-    config.set(Property.MANAGER_FATE_THREADPOOL_SIZE, String.valueOf(numThreads));\n-    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""60m\"");\n-    return config;\n-  }\n-\n-  public static class PoolResizeTestRepo implements Repo<PoolResizeTestEnv> {\n-    private static final long serialVersionUID = 1L;\n-\n-    @Override\n-    public long isReady(FateId fateId, PoolResizeTestEnv environment) throws Exception {\n-      environment.numWorkers.incrementAndGet();\n-      environment.isReadyLatch.await();\n-      return 0;\n-    }\n-\n-    @Override\n-    public String getName() {\n-      return this.getClass().getSimpleName();\n-    }\n-\n-    @Override\n-    public Repo<PoolResizeTestEnv> call(FateId fateId, PoolResizeTestEnv environment)\n-        throws Exception {\n-      environment.numWorkers.decrementAndGet();\n-      return null;\n-    }\n-\n-    @Override\n-    public void undo(FateId fateId, PoolResizeTestEnv environment) throws Exception {\n-\n-    }\n-\n-    @Override\n-    public String getReturn() {\n-      return null;\n-    }\n-  }\n-\n-  public static class PoolResizeTestEnv extends FateTestRunner.TestEnv {\n-    private final AtomicInteger numWorkers = new AtomicInteger(0);\n-    private final CountDownLatch isReadyLatch = new CountDownLatch(1);\n-  }\n-}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FatePoolsWatcherIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FatePoolsWatcherIT.java\nnew file mode 100644\nindex 00000000000..1245b24e47b\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FatePoolsWatcherIT.java\n@@ -0,0 +1,721 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.test.fate;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import org.apache.accumulo.core.conf.AccumuloConfiguration;\n+import org.apache.accumulo.core.conf.ConfigurationCopy;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateInstanceType;\n+import org.apache.accumulo.core.fate.FateStore;\n+import org.apache.accumulo.core.fate.Repo;\n+import org.apache.accumulo.harness.SharedMiniClusterBase;\n+import org.apache.accumulo.server.ServerContext;\n+import org.apache.accumulo.test.util.Wait;\n+import org.junit.jupiter.api.Test;\n+\n+/**\n+ * Tests the functionality of the FATE pools watcher task\n+ */\n+public abstract class FatePoolsWatcherIT extends SharedMiniClusterBase\n+    implements FateTestRunner<FatePoolsWatcherIT.PoolResizeTestEnv> {\n+\n+  private static final Set<Fate.FateOperation> ALL_USER_FATE_OPS =\n+      Fate.FateOperation.getAllUserFateOps();\n+  private static final Set<Fate.FateOperation> ALL_META_FATE_OPS =\n+      Fate.FateOperation.getAllMetaFateOps();\n+  private static final Set<Fate.FateOperation> USER_FATE_OPS_SET1 =\n+      ALL_USER_FATE_OPS.stream().limit(ALL_USER_FATE_OPS.size() / 2).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> USER_FATE_OPS_SET2 =\n+      ALL_USER_FATE_OPS.stream().skip(ALL_USER_FATE_OPS.size() / 2).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> USER_FATE_OPS_SET3 =\n+      ALL_USER_FATE_OPS.stream().skip(ALL_USER_FATE_OPS.size() / 2 + 1).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> USER_FATE_OPS_SET4 = ALL_USER_FATE_OPS.stream()\n+      .skip(ALL_USER_FATE_OPS.size() / 2).limit(1).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> META_FATE_OPS_SET1 =\n+      ALL_META_FATE_OPS.stream().limit(ALL_META_FATE_OPS.size() / 2).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> META_FATE_OPS_SET2 =\n+      ALL_META_FATE_OPS.stream().skip(ALL_META_FATE_OPS.size() / 2).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> META_FATE_OPS_SET3 =\n+      ALL_META_FATE_OPS.stream().skip(ALL_META_FATE_OPS.size() / 2 + 1).collect(Collectors.toSet());\n+  private static final Set<Fate.FateOperation> META_FATE_OPS_SET4 = ALL_META_FATE_OPS.stream()\n+      .skip(ALL_META_FATE_OPS.size() / 2).limit(1).collect(Collectors.toSet());\n+\n+  @Test\n+  public void testIncrease1() throws Exception {\n+    executeTest(this::testIncrease1);\n+  }\n+\n+  protected void testIncrease1(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n+      throws Exception {\n+    // Tests changing the config for the FATE thread pools from\n+    // {<half the FATE ops/SET1>}: 4 <-- FateExecutor1\n+    // {<other half/SET2>}: 5 <-- FateExecutor2\n+    // ---->\n+    // {<half the FATE ops/SET1>}: 10 <-- FateExecutor1\n+    // {<other half minus one/SET3>}: 9 <-- FateExecutor3\n+    // {<remaining FATE op/SET4>}: 8 <-- FateExecutor4\n+    // This tests inc size of FATE thread pools for FateExecutors with unchanged fate ops, stopping\n+    // FateExecutors that are no longer valid (while ensuring none are stopped while in progress on\n+    // a transaction), and creating new FateExecutors as needed. Essentially, FateExecutor1's pool\n+    // size should be increased and FateExecutor3 and 4 should replace 2.\n+\n+    boolean allAssertsOccurred = false;\n+    final ConfigurationCopy config = initConfigIncTest1();\n+    final var env = new PoolResizeTestEnv();\n+    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n+    boolean isUserStore = store.type() == FateInstanceType.USER;\n+    final Set<Fate.FateOperation> set1 = isUserStore ? USER_FATE_OPS_SET1 : META_FATE_OPS_SET1;\n+    final Set<Fate.FateOperation> set2 = isUserStore ? USER_FATE_OPS_SET2 : META_FATE_OPS_SET2;\n+    final Set<Fate.FateOperation> set3 = isUserStore ? USER_FATE_OPS_SET3 : META_FATE_OPS_SET3;\n+    final Set<Fate.FateOperation> set4 = isUserStore ? USER_FATE_OPS_SET4 : META_FATE_OPS_SET4;\n+    final Fate.FateOperation fateOpFromSet1 = set1.iterator().next();\n+    final Fate.FateOperation fateOpFromSet2 = set2.iterator().next();\n+    final int numWorkersSet1 = 4;\n+    final int newNumWorkersSet1 = 10;\n+    final int numWorkersSet2 = 5;\n+    final int numWorkersSet3 = 9;\n+    final int numWorkersSet4 = 8;\n+\n+    try {\n+      // create one transaction for each FateExecutor to work on\n+      fate.seedTransaction(fateOpFromSet1, fate.startTransaction(), new PoolResizeTestRepo(), true,\n+          \""testing\"");\n+      fate.seedTransaction(fateOpFromSet2, fate.startTransaction(), new PoolResizeTestRepo(), true,\n+          \""testing\"");\n+\n+      // wait for the FateExecutors to work on the transactions\n+      Wait.waitFor(() -> env.numWorkers.get() == 2);\n+      // sum has been verified, verify each term\n+      Map<Fate.FateOperation,\n+          Long> seenCounts = store.list()\n+              .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+                  && fateIdStatus.getFateReservation().isPresent())\n+              .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+                  Collectors.counting()));\n+      Map<Fate.FateOperation,Long> expectedCounts = Map.of(fateOpFromSet1, 1L, fateOpFromSet2, 1L);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      // wait for all transaction runners to be active\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive() == numWorkersSet1 + numWorkersSet2);\n+      // sum has been verified, verify each term\n+      assertEquals(numWorkersSet1, fate.getTxRunnersActive(set1));\n+      assertEquals(numWorkersSet2, fate.getTxRunnersActive(set2));\n+\n+      changeConfigIncTest1(config);\n+\n+      // After changing the config, the fate pool watcher should detect the change and increase the\n+      // pool size for the pool assigned to work on SET1\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive()\n+          == newNumWorkersSet1 + 1 + numWorkersSet3 + numWorkersSet4);\n+      // sum has been verified, verify each term\n+      assertEquals(newNumWorkersSet1, fate.getTxRunnersActive(set1));\n+      // The FateExecutor assigned to SET2 is no longer valid after the config change, so a\n+      // shutdown should be initiated and all the runners but the one working on a transaction\n+      // should be stopped.\n+      assertEquals(1, fate.getTxRunnersActive(set2));\n+      // New FateExecutors should be created for SET3 and SET4\n+      assertEquals(numWorkersSet3, fate.getTxRunnersActive(set3));\n+      assertEquals(numWorkersSet4, fate.getTxRunnersActive(set4));\n+\n+      // num actively executing tasks should not be affected\n+      assertEquals(2, env.numWorkers.get());\n+      // sum has been verified, verify each term\n+      seenCounts = store.list()\n+          .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+              && fateIdStatus.getFateReservation().isPresent())\n+          .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+              Collectors.counting()));\n+      expectedCounts = Map.of(fateOpFromSet1, 1L, fateOpFromSet2, 1L);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      // finish work\n+      env.isReadyLatch.countDown();\n+\n+      Wait.waitFor(() -> env.numWorkers.get() == 0);\n+\n+      // workers should still be running: we haven't shutdown FATE, just not working on anything\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive()\n+          == newNumWorkersSet1 + numWorkersSet3 + numWorkersSet4);\n+      // sum has been verified, verify each term\n+      assertEquals(newNumWorkersSet1, fate.getTxRunnersActive(set1));\n+      // The FateExecutor for SET2 should have finished work and be shutdown now since it was\n+      // previously invalidated by the config change and has since finished its assigned txn\n+      assertEquals(0, fate.getTxRunnersActive(set2));\n+      assertEquals(numWorkersSet3, fate.getTxRunnersActive(set3));\n+      assertEquals(numWorkersSet4, fate.getTxRunnersActive(set4));\n+      allAssertsOccurred = true;\n+    } catch (Exception e) {\n+      System.out.println(\""Failure: \"" + e);\n+    } finally {\n+      fate.shutdown(30, TimeUnit.SECONDS);\n+      assertTrue(allAssertsOccurred);\n+      assertEquals(0, fate.getTotalTxRunnersActive());\n+    }\n+  }\n+\n+  @Test\n+  public void testIncrease2() throws Exception {\n+    executeTest(this::testIncrease2);\n+  }\n+\n+  protected void testIncrease2(FateStore<PoolResizeTestEnv> store, ServerContext sctx) {\n+    // Tests changing the config for the FATE thread pools from\n+    // {<All FATE ops>}: 2 <-- FateExecutor1\n+    // ---->\n+    // {<All FATE ops>}: 3 <-- FateExecutor1\n+    // when 3 transactions need to be worked on. Ensures after the config change, the third tx\n+    // is picked up.\n+    boolean allAssertsOccurred = false;\n+    final ConfigurationCopy config = FateTestUtil.createTestFateConfig(2);\n+    final var env = new PoolResizeTestEnv();\n+    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n+    final int numWorkers = 2;\n+    final int newNumWorkers = 3;\n+    final Set<Fate.FateOperation> allFateOps =\n+        store.type() == FateInstanceType.USER ? ALL_USER_FATE_OPS : ALL_META_FATE_OPS;\n+    final var fateOp = FateTestUtil.TEST_FATE_OP;\n+    try {\n+      for (int i = 0; i < newNumWorkers; i++) {\n+        fate.seedTransaction(fateOp, fate.startTransaction(), new PoolResizeTestRepo(), true,\n+            \""testing\"");\n+      }\n+\n+      // wait for the 2 threads to pick up 2 of the 3 transactions\n+      Wait.waitFor(() -> env.numWorkers.get() == numWorkers);\n+      Map<Fate.FateOperation,\n+          Long> seenCounts = store.list()\n+              .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+                  && fateIdStatus.getFateReservation().isPresent())\n+              .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+                  Collectors.counting()));\n+      Map<Fate.FateOperation,Long> expectedCounts = Map.of(fateOp, (long) numWorkers);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive() == numWorkers);\n+      assertEquals(numWorkers, fate.getTxRunnersActive(allFateOps));\n+\n+      // increase the pool size\n+      changeConfigIncTest2(config, newNumWorkers);\n+\n+      // wait for the final tx to be picked up\n+      Wait.waitFor(() -> env.numWorkers.get() == newNumWorkers);\n+      seenCounts = store.list()\n+          .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+              && fateIdStatus.getFateReservation().isPresent())\n+          .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+              Collectors.counting()));\n+      expectedCounts = Map.of(fateOp, (long) newNumWorkers);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive() == newNumWorkers);\n+      assertEquals(newNumWorkers, fate.getTxRunnersActive(allFateOps));\n+\n+      // finish work\n+      env.isReadyLatch.countDown();\n+\n+      Wait.waitFor(() -> env.numWorkers.get() == 0);\n+\n+      // workers should still be running: we haven't shutdown FATE, just not working on anything\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive() == newNumWorkers);\n+      assertEquals(newNumWorkers, fate.getTxRunnersActive(allFateOps));\n+\n+      allAssertsOccurred = true;\n+    } catch (Exception e) {\n+      System.out.println(\""Failure: \"" + e);\n+    } finally {\n+      fate.shutdown(30, TimeUnit.SECONDS);\n+      assertTrue(allAssertsOccurred);\n+      assertEquals(0, fate.getTotalTxRunnersActive());\n+    }\n+  }\n+\n+  @Test\n+  public void testDecrease() throws Exception {\n+    executeTest(this::testDecrease);\n+  }\n+\n+  protected void testDecrease(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n+      throws Exception {\n+    // Tests changing the config for the FATE thread pools from\n+    // {<half the FATE ops/SET1>}: 4 <-- FateExecutor1\n+    // {<other half minus one/SET3>}: 5 <-- FateExecutor2\n+    // {<remaining FATE op/SET4>}: 6 <-- FateExecutor3\n+    // ---->\n+    // {<half the FATE ops/SET1>}: 3 <-- FateExecutor1\n+    // {<other half/SET2>}: 2 <-- FateExecutor4\n+    // This tests dec size of FATE thread pools for FateExecutors with unchanged fate ops, stopping\n+    // FateExecutors that are no longer valid (while ensuring none are stopped while in progress on\n+    // a transaction), and creating new FateExecutors as needed. Essentially, FateExecutor1's pool\n+    // size should shrink and FateExecutor4 should replace 2 and 3.\n+    boolean allAssertsOccurred = false;\n+    final ConfigurationCopy config = initConfigDecTest();\n+    final var env = new PoolResizeTestEnv();\n+    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n+    boolean isUserStore = store.type() == FateInstanceType.USER;\n+    final Set<Fate.FateOperation> set1 = isUserStore ? USER_FATE_OPS_SET1 : META_FATE_OPS_SET1;\n+    final Set<Fate.FateOperation> set2 = isUserStore ? USER_FATE_OPS_SET2 : META_FATE_OPS_SET2;\n+    final Set<Fate.FateOperation> set3 = isUserStore ? USER_FATE_OPS_SET3 : META_FATE_OPS_SET3;\n+    final Set<Fate.FateOperation> set4 = isUserStore ? USER_FATE_OPS_SET4 : META_FATE_OPS_SET4;\n+    final Fate.FateOperation fateOpFromSet1 = set1.iterator().next();\n+    final Fate.FateOperation fateOpFromSet3 = set3.iterator().next();\n+    final Fate.FateOperation fateOpFromSet4 = set4.iterator().next();\n+    final int numWorkersSet1 = 4;\n+    final int newNumWorkersSet1 = 3;\n+    final int numWorkersSet2 = 2;\n+    final int numWorkersSet3 = 5;\n+    final int numWorkersSet4 = 6;\n+\n+    try {\n+      // create a tx for each thread\n+      for (int i = 0; i < numWorkersSet1; i++) {\n+        fate.seedTransaction(fateOpFromSet1, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      for (int i = 0; i < numWorkersSet3; i++) {\n+        fate.seedTransaction(fateOpFromSet3, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      for (int i = 0; i < numWorkersSet4; i++) {\n+        fate.seedTransaction(fateOpFromSet4, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      // wait for all threads to be working on a tx\n+      Wait.waitFor(() -> env.numWorkers.get() == numWorkersSet1 + numWorkersSet3 + numWorkersSet4);\n+      // sum has been verified, verify each term\n+      Map<Fate.FateOperation,\n+          Long> seenCounts = store.list()\n+              .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+                  && fateIdStatus.getFateReservation().isPresent())\n+              .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+                  Collectors.counting()));\n+      Map<Fate.FateOperation,Long> expectedCounts = Map.of(fateOpFromSet1, (long) numWorkersSet1,\n+          fateOpFromSet3, (long) numWorkersSet3, fateOpFromSet4, (long) numWorkersSet4);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      // wait for all transaction runners to be active\n+      Wait.waitFor(\n+          () -> fate.getTotalTxRunnersActive() == numWorkersSet1 + numWorkersSet3 + numWorkersSet4);\n+      // sum has been verified, verify each term\n+      assertEquals(numWorkersSet1, fate.getTxRunnersActive(set1));\n+      // this has not been set in the config yet so shouldn't exist\n+      assertEquals(0, fate.getTxRunnersActive(set2));\n+      assertEquals(numWorkersSet3, fate.getTxRunnersActive(set3));\n+      assertEquals(numWorkersSet4, fate.getTxRunnersActive(set4));\n+\n+      changeConfigDecTest(config);\n+\n+      // wait for another execution of the pool watcher task. This is signified by the start of\n+      // the executor for SET2.\n+      // At this point, FATE should detect that there are more tx runners running than configured\n+      // for the FateExecutor working on SET1, and should detect that the FateExecutors assigned to\n+      // SET3 and SET4 are no longer valid. None can safely be stopped yet, (still in progress -\n+      // haven't passed isReady). We ensure none have been unexpectedly stopped and that the new\n+      // executor has started (for SET2), then we allow isReady to pass and the txns to complete.\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive()\n+          == numWorkersSet1 + numWorkersSet2 + numWorkersSet3 + numWorkersSet4);\n+      // sum has been verified, verify each term\n+      assertEquals(numWorkersSet1, fate.getTxRunnersActive(set1));\n+      assertEquals(numWorkersSet2, fate.getTxRunnersActive(set2));\n+      assertEquals(numWorkersSet3, fate.getTxRunnersActive(set3));\n+      assertEquals(numWorkersSet4, fate.getTxRunnersActive(set4));\n+\n+      // num actively executing tasks should not be affected\n+      assertEquals(numWorkersSet1 + numWorkersSet3 + numWorkersSet4, env.numWorkers.get());\n+      // sum has been verified, verify each term\n+      seenCounts = store.list()\n+          .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+              && fateIdStatus.getFateReservation().isPresent())\n+          .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+              Collectors.counting()));\n+      expectedCounts = Map.of(fateOpFromSet1, (long) numWorkersSet1, fateOpFromSet3,\n+          (long) numWorkersSet3, fateOpFromSet4, (long) numWorkersSet4);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      // finish work\n+      env.isReadyLatch.countDown();\n+\n+      // Wait for the expected changes to occur after work completes. The executors that were set\n+      // to shutdown (the executors assigned to SET3 and SET4) should have successfully shutdown,\n+      // the pool size for the executor for SET1 should have been updated, and the executor for\n+      // SET2 should still be running.\n+      Wait.waitFor(() -> env.numWorkers.get() == 0);\n+\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive() == newNumWorkersSet1 + numWorkersSet2);\n+      // sum has been verified, verify each term\n+      assertEquals(newNumWorkersSet1, fate.getTxRunnersActive(set1));\n+      assertEquals(numWorkersSet2, fate.getTxRunnersActive(set2));\n+      // the FateExecutors for SET3 and SET4 should be fully shutdown now\n+      assertEquals(0, fate.getTxRunnersActive(set3));\n+      assertEquals(0, fate.getTxRunnersActive(set4));\n+\n+      // wait a bit longer to ensure another iteration of the pool watcher check doesn't change\n+      // anything: everything is as expected now\n+      Thread.sleep(fate.getPoolWatcherDelay().toMillis() + 1_000);\n+\n+      assertEquals(newNumWorkersSet1 + numWorkersSet2, fate.getTotalTxRunnersActive());\n+      assertEquals(newNumWorkersSet1, fate.getTxRunnersActive(set1));\n+      assertEquals(numWorkersSet2, fate.getTxRunnersActive(set2));\n+      assertEquals(0, fate.getTxRunnersActive(set3));\n+      assertEquals(0, fate.getTxRunnersActive(set4));\n+      allAssertsOccurred = true;\n+    } catch (Exception e) {\n+      System.out.println(\""Failure: \"" + e);\n+    } finally {\n+      fate.shutdown(30, TimeUnit.SECONDS);\n+      assertTrue(allAssertsOccurred);\n+      assertEquals(0, fate.getTotalTxRunnersActive());\n+    }\n+  }\n+\n+  @Test\n+  public void testIdleCountHistory() throws Exception {\n+    executeTest(this::testIdleCountHistory);\n+  }\n+\n+  protected void testIdleCountHistory(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n+      throws Exception {\n+    // Tests that a warning to increase pool size is logged when expected\n+    boolean allAssertsOccurred = false;\n+    var config = configIdleHistoryTest();\n+    final var env = new PoolResizeTestEnv();\n+    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n+    try {\n+      // We have two worker threads. Submit 3 transactions that won't complete yet so we can check\n+      // for a warning\n+      for (int i = 0; i < 3; i++) {\n+        fate.seedTransaction(FateTestUtil.TEST_FATE_OP, fate.startTransaction(),\n+            new PoolResizeTestRepo(), true, \""testing\"");\n+      }\n+      Wait.waitFor(() -> fate.getNeedMoreThreadsWarnCount().get() >= 1, 60_000, 1_000);\n+      // can finish work now\n+      env.isReadyLatch.countDown();\n+      Wait.waitFor(() -> env.numWorkers.get() == 0);\n+      allAssertsOccurred = true;\n+    } catch (Exception e) {\n+      System.out.println(\""Failure: \"" + e);\n+    } finally {\n+      fate.shutdown(30, TimeUnit.SECONDS);\n+      assertTrue(allAssertsOccurred);\n+      assertEquals(0, fate.getTotalTxRunnersActive());\n+    }\n+  }\n+\n+  @Test\n+  public void testFatePoolsPartitioning() throws Exception {\n+    executeTest(this::testFatePoolsPartitioning);\n+  }\n+\n+  protected void testFatePoolsPartitioning(FateStore<PoolResizeTestEnv> store, ServerContext sctx)\n+      throws Exception {\n+    // Ensures FATE ops are correctly partitioned between the pools. Configures 4 FateExecutors:\n+    // FateExecutor1 with 2 threads operating on 1/4 of FATE ops\n+    // FateExecutor2 with 3 threads operating on 1/4 of FATE ops\n+    // FateExecutor3 with 4 threads operating on 1/4 of FATE ops\n+    // FateExecutor4 with 5 threads operating on 1/4 of FATE ops\n+    // Seeds:\n+    // 5 transactions on FateExecutor1\n+    // 6 transactions on FateExecutor2\n+    // 1 transactions on FateExecutor3\n+    // 4 transactions on FateExecutor4\n+    // Ensures that we only see min(configured threads, transactions seeded) ever running\n+    // Also ensures that FateExecutors do not pick up any work that they shouldn't\n+    final int numThreadsPool1 = 2;\n+    final int numThreadsPool2 = 3;\n+    final int numThreadsPool3 = 4;\n+    final int numThreadsPool4 = 5;\n+    final int numSeedPool1 = 5;\n+    final int numSeedPool2 = 6;\n+    final int numSeedPool3 = 1;\n+    final int numSeedPool4 = 4;\n+    final long expectedRunningPool1 = Math.min(numThreadsPool1, numSeedPool1);\n+    final long expectedRunningPool2 = Math.min(numThreadsPool2, numSeedPool2);\n+    final long expectedRunningPool3 = Math.min(numThreadsPool3, numSeedPool3);\n+    final long expectedRunningPool4 = Math.min(numThreadsPool4, numSeedPool4);\n+\n+    final int numUserOpsPerPool = ALL_USER_FATE_OPS.size() / 4;\n+    final int numMetaOpsPerPool = ALL_META_FATE_OPS.size() / 4;\n+\n+    final Set<Fate.FateOperation> userPool1 =\n+        ALL_USER_FATE_OPS.stream().limit(numUserOpsPerPool).collect(Collectors.toSet());\n+    final Set<Fate.FateOperation> userPool2 = ALL_USER_FATE_OPS.stream().skip(numUserOpsPerPool)\n+        .limit(numUserOpsPerPool).collect(Collectors.toSet());\n+    final Set<Fate.FateOperation> userPool3 = ALL_USER_FATE_OPS.stream().skip(numUserOpsPerPool * 2)\n+        .limit(numUserOpsPerPool).collect(Collectors.toSet());\n+    // no limit for pool 4 in case total num ops is odd\n+    final Set<Fate.FateOperation> userPool4 =\n+        ALL_USER_FATE_OPS.stream().skip(numUserOpsPerPool * 3).collect(Collectors.toSet());\n+\n+    final Set<Fate.FateOperation> metaPool1 =\n+        ALL_META_FATE_OPS.stream().limit(numMetaOpsPerPool).collect(Collectors.toSet());\n+    final Set<Fate.FateOperation> metaPool2 = ALL_META_FATE_OPS.stream().skip(numMetaOpsPerPool)\n+        .limit(numMetaOpsPerPool).collect(Collectors.toSet());\n+    final Set<Fate.FateOperation> metaPool3 = ALL_META_FATE_OPS.stream().skip(numMetaOpsPerPool * 2)\n+        .limit(numMetaOpsPerPool).collect(Collectors.toSet());\n+    // no limit for pool 4 in case total num ops is odd\n+    final Set<Fate.FateOperation> metaPool4 =\n+        ALL_META_FATE_OPS.stream().skip(numMetaOpsPerPool * 3).collect(Collectors.toSet());\n+\n+    final ConfigurationCopy config = new ConfigurationCopy();\n+    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n+    config.set(Property.MANAGER_FATE_USER_CONFIG,\n+        String.format(\""{\\\""%s\\\"": %s, \\\""%s\\\"": %s, \\\""%s\\\"": %s, \\\""%s\\\"": %s}\"",\n+            userPool1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool1,\n+            userPool2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool2,\n+            userPool3.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool3,\n+            userPool4.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool4));\n+    config.set(Property.MANAGER_FATE_META_CONFIG,\n+        String.format(\""{\\\""%s\\\"": %s, \\\""%s\\\"": %s, \\\""%s\\\"": %s, \\\""%s\\\"": %s}\"",\n+            metaPool1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool1,\n+            metaPool2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool2,\n+            metaPool3.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool3,\n+            metaPool4.stream().map(Enum::name).collect(Collectors.joining(\"",\"")), numThreadsPool4));\n+    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""60m\"");\n+\n+    final boolean isUserStore = store.type() == FateInstanceType.USER;\n+\n+    final Fate.FateOperation fateOpFromPool1 =\n+        isUserStore ? userPool1.iterator().next() : metaPool1.iterator().next();\n+    final Fate.FateOperation fateOpFromPool2 =\n+        isUserStore ? userPool2.iterator().next() : metaPool2.iterator().next();\n+    final Fate.FateOperation fateOpFromPool3 =\n+        isUserStore ? userPool3.iterator().next() : metaPool3.iterator().next();\n+    final Fate.FateOperation fateOpFromPool4 =\n+        isUserStore ? userPool4.iterator().next() : metaPool4.iterator().next();\n+\n+    final Set<Fate.FateOperation> pool1 = isUserStore ? userPool1 : metaPool1;\n+    final Set<Fate.FateOperation> pool2 = isUserStore ? userPool2 : metaPool2;\n+    final Set<Fate.FateOperation> pool3 = isUserStore ? userPool3 : metaPool3;\n+    final Set<Fate.FateOperation> pool4 = isUserStore ? userPool4 : metaPool4;\n+\n+    boolean allAssertsOccurred = false;\n+    final var env = new PoolResizeTestEnv();\n+    final Fate<PoolResizeTestEnv> fate = new FastFate<>(env, store, false, r -> r + \""\"", config);\n+\n+    try {\n+      // seeding pool1/FateExecutor1\n+      for (int i = 0; i < numSeedPool1; i++) {\n+        fate.seedTransaction(fateOpFromPool1, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      // seeding pool2/FateExecutor2\n+      for (int i = 0; i < numSeedPool2; i++) {\n+        fate.seedTransaction(fateOpFromPool2, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      // seeding pool3/FateExecutor3\n+      for (int i = 0; i < numSeedPool3; i++) {\n+        fate.seedTransaction(fateOpFromPool3, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+      // seeding pool4/FateExecutor4\n+      for (int i = 0; i < numSeedPool4; i++) {\n+        fate.seedTransaction(fateOpFromPool4, fate.startTransaction(), new PoolResizeTestRepo(),\n+            true, \""testing\"");\n+      }\n+\n+      // wait for the threads to be working on the transactions\n+      Wait.waitFor(() -> env.numWorkers.get() == expectedRunningPool1 + expectedRunningPool2\n+          + expectedRunningPool3 + expectedRunningPool4);\n+      // sum has been verified, verify each term\n+      Map<Fate.FateOperation,\n+          Long> seenCounts = store.list()\n+              .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+                  && fateIdStatus.getFateReservation().isPresent())\n+              .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+                  Collectors.counting()));\n+      Map<Fate.FateOperation,Long> expectedCounts =\n+          Map.of(fateOpFromPool1, expectedRunningPool1, fateOpFromPool2, expectedRunningPool2,\n+              fateOpFromPool3, expectedRunningPool3, fateOpFromPool4, expectedRunningPool4);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      // wait for all transaction runners to be active\n+      Wait.waitFor(() -> fate.getTotalTxRunnersActive()\n+          == numThreadsPool1 + numThreadsPool2 + numThreadsPool3 + numThreadsPool4);\n+      // sum has been verified, verify each term\n+      assertEquals(numThreadsPool1, fate.getTxRunnersActive(pool1));\n+      assertEquals(numThreadsPool2, fate.getTxRunnersActive(pool2));\n+      assertEquals(numThreadsPool3, fate.getTxRunnersActive(pool3));\n+      assertEquals(numThreadsPool4, fate.getTxRunnersActive(pool4));\n+\n+      // wait a bit longer to ensure another iteration of the pool watcher check doesn't change\n+      // anything\n+      Thread.sleep(fate.getPoolWatcherDelay().toMillis() + 1_000);\n+\n+      assertEquals(env.numWorkers.get(), expectedRunningPool1 + expectedRunningPool2\n+          + expectedRunningPool3 + expectedRunningPool4);\n+      // sum has been verified, verify each term\n+      seenCounts = store.list()\n+          .filter(fateIdStatus -> fateIdStatus.getFateOperation().isPresent()\n+              && fateIdStatus.getFateReservation().isPresent())\n+          .collect(Collectors.groupingBy(fis -> fis.getFateOperation().orElseThrow(),\n+              Collectors.counting()));\n+      expectedCounts =\n+          Map.of(fateOpFromPool1, expectedRunningPool1, fateOpFromPool2, expectedRunningPool2,\n+              fateOpFromPool3, expectedRunningPool3, fateOpFromPool4, expectedRunningPool4);\n+      assertEquals(expectedCounts, seenCounts);\n+\n+      assertEquals(fate.getTotalTxRunnersActive(),\n+          numThreadsPool1 + numThreadsPool2 + numThreadsPool3 + numThreadsPool4);\n+      // sum has been verified, verify each term\n+      assertEquals(numThreadsPool1, fate.getTxRunnersActive(pool1));\n+      assertEquals(numThreadsPool2, fate.getTxRunnersActive(pool2));\n+      assertEquals(numThreadsPool3, fate.getTxRunnersActive(pool3));\n+      assertEquals(numThreadsPool4, fate.getTxRunnersActive(pool4));\n+\n+      // can finish work now\n+      env.isReadyLatch.countDown();\n+      Wait.waitFor(() -> env.numWorkers.get() == 0);\n+      allAssertsOccurred = true;\n+    } finally {\n+      fate.shutdown(30, TimeUnit.SECONDS);\n+      assertTrue(allAssertsOccurred);\n+      assertEquals(0, fate.getTotalTxRunnersActive());\n+    }\n+  }\n+\n+  private ConfigurationCopy initConfigIncTest1() {\n+    // {<half the FATE ops/SET1>}: 4\n+    // {<other half/SET2>}: 5\n+    ConfigurationCopy config = new ConfigurationCopy();\n+    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\""\n+        + USER_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 4,\\\""\""\n+        + USER_FATE_OPS_SET2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 5}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\""\n+        + META_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 4,\\\""\""\n+        + META_FATE_OPS_SET2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 5}\"");\n+    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""60m\"");\n+    return config;\n+  }\n+\n+  private void changeConfigIncTest1(ConfigurationCopy config) {\n+    // {<half the FATE ops/SET1>}: 10\n+    // {<other half minus one/SET3>}: 9\n+    // {<remaining FATE op/SET4>}: 8\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\""\n+        + USER_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 10,\""\n+        + \""\\\""\"" + USER_FATE_OPS_SET3.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 9,\\\""\"" + USER_FATE_OPS_SET4.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 8}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\""\n+        + META_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 10,\""\n+        + \""\\\""\"" + META_FATE_OPS_SET3.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 9,\\\""\"" + META_FATE_OPS_SET4.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 8}\"");\n+  }\n+\n+  private void changeConfigIncTest2(ConfigurationCopy config, int numThreads) {\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\"" + Fate.FateOperation.getAllUserFateOps()\n+        .stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": \"" + numThreads + \""}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\"" + Fate.FateOperation.getAllMetaFateOps()\n+        .stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": \"" + numThreads + \""}\"");\n+  }\n+\n+  private ConfigurationCopy initConfigDecTest() {\n+    // {<half the FATE ops/SET1>}: 4\n+    // {<other half minus one/SET3>}: 5\n+    // {<remaining FATE op/SET4>}: 6\n+    ConfigurationCopy config = new ConfigurationCopy();\n+    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\""\n+        + USER_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 4,\""\n+        + \""\\\""\"" + USER_FATE_OPS_SET3.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 5,\\\""\"" + USER_FATE_OPS_SET4.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 6}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\""\n+        + META_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 4,\""\n+        + \""\\\""\"" + META_FATE_OPS_SET3.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 5,\\\""\"" + META_FATE_OPS_SET4.stream().map(Enum::name).collect(Collectors.joining(\"",\""))\n+        + \""\\\"": 6}\"");\n+    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""60m\"");\n+    return config;\n+  }\n+\n+  private void changeConfigDecTest(ConfigurationCopy config) {\n+    // {<half the FATE ops/SET1>}: 3\n+    // {<other half/SET2>}: 2\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\""\n+        + USER_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 3,\\\""\""\n+        + USER_FATE_OPS_SET2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 2}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\""\n+        + META_FATE_OPS_SET1.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 3,\\\""\""\n+        + META_FATE_OPS_SET2.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 2}\"");\n+  }\n+\n+  private AccumuloConfiguration configIdleHistoryTest() {\n+    ConfigurationCopy config = new ConfigurationCopy();\n+    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\""\n+        + ALL_USER_FATE_OPS.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 2}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\""\n+        + ALL_META_FATE_OPS.stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": 2}\"");\n+    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""1m\"");\n+    return config;\n+  }\n+\n+  public static class PoolResizeTestRepo implements Repo<PoolResizeTestEnv> {\n+    private static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public long isReady(FateId fateId, PoolResizeTestEnv environment) throws Exception {\n+      environment.numWorkers.incrementAndGet();\n+      environment.isReadyLatch.await();\n+      return 0;\n+    }\n+\n+    @Override\n+    public String getName() {\n+      return this.getClass().getSimpleName();\n+    }\n+\n+    @Override\n+    public Repo<PoolResizeTestEnv> call(FateId fateId, PoolResizeTestEnv environment)\n+        throws Exception {\n+      environment.numWorkers.decrementAndGet();\n+      return null;\n+    }\n+\n+    @Override\n+    public void undo(FateId fateId, PoolResizeTestEnv environment) throws Exception {\n+\n+    }\n+\n+    @Override\n+    public String getReturn() {\n+      return null;\n+    }\n+  }\n+\n+  public static class PoolResizeTestEnv extends TestEnv {\n+    private final AtomicInteger numWorkers = new AtomicInteger(0);\n+    private final CountDownLatch isReadyLatch = new CountDownLatch(1);\n+  }\n+}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\nindex 644a2909a31..94dd4f690d5 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n@@ -19,8 +19,8 @@\n package org.apache.accumulo.test.fate;\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n-import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n-import static org.apache.accumulo.test.fate.FateStoreUtil.seedTransaction;\n+import static org.apache.accumulo.test.fate.FateTestUtil.TEST_FATE_OP;\n+import static org.apache.accumulo.test.fate.FateTestUtil.seedTransaction;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertInstanceOf;\n@@ -104,8 +104,8 @@ protected void testReadWrite(FateStore<TestEnv> store, ServerContext sctx)\n     assertEquals(TStatus.SUBMITTED, txStore.getStatus());\n \n     // Set a name to test setTransactionInfo()\n-    txStore.setTransactionInfo(TxInfo.FATE_OP, \""name\"");\n-    assertEquals(\""name\"", txStore.getTransactionInfo(TxInfo.FATE_OP));\n+    txStore.setTransactionInfo(TxInfo.FATE_OP, TEST_FATE_OP);\n+    assertEquals(TEST_FATE_OP, txStore.getTransactionInfo(TxInfo.FATE_OP));\n \n     // Try setting a second test op to test getStack()\n     // when listing or popping TestOperation2 should be first\n@@ -197,7 +197,8 @@ protected void testDeferredOverflow(FateStore<TestEnv> store, ServerContext sctx\n     try {\n       // Run and verify all 10 transactions still exist and were not\n       // run because of the deferral time of all the transactions\n-      future = executor.submit(() -> store.runnable(keepRunning, transactions::remove));\n+      future = executor.submit(() -> store.runnable(keepRunning,\n+          fateIdStatus -> transactions.remove(fateIdStatus.getFateId())));\n       Thread.sleep(2000);\n       assertEquals(10, transactions.size());\n       // Setting this flag to false should terminate the task if sleeping\n@@ -222,7 +223,8 @@ protected void testDeferredOverflow(FateStore<TestEnv> store, ServerContext sctx\n       // Run and verify all 11 transactions were processed\n       // and removed from the store\n       keepRunning.set(true);\n-      future = executor.submit(() -> store.runnable(keepRunning, transactions::remove));\n+      future = executor.submit(() -> store.runnable(keepRunning,\n+          fateIdStatus -> transactions.remove(fateIdStatus.getFateId())));\n       Wait.waitFor(transactions::isEmpty);\n       // Setting this flag to false should terminate the task if sleeping\n       keepRunning.set(false);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java b/test/src/main/java/org/apache/accumulo/test/fate/FateTestUtil.java\nsimilarity index 79%\nrename from test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\nrename to test/src/main/java/org/apache/accumulo/test/fate/FateTestUtil.java\nindex 5b12b0f3cdd..8dd795757df 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreUtil.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateTestUtil.java\n@@ -24,12 +24,15 @@\n import java.io.File;\n import java.util.Optional;\n import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n import org.apache.accumulo.core.client.admin.TabletAvailability;\n import org.apache.accumulo.core.client.admin.TabletInformation;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n+import org.apache.accumulo.core.conf.ConfigurationCopy;\n+import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.fate.FateId;\n@@ -45,13 +48,12 @@\n import com.google.common.collect.MoreCollectors;\n \n /**\n- * A class with utilities for testing {@link org.apache.accumulo.core.fate.user.UserFateStore} and\n- * {@link org.apache.accumulo.core.fate.zookeeper.MetaFateStore}\n+ * A class with utilities for testing FATE\n  */\n-public class FateStoreUtil {\n-  // A FateOperation for testing purposes when a FateOperation is needed but whose value doesn't\n-  // matter\n-  public static final Fate.FateOperation TEST_FATE_OP = Fate.FateOperation.TABLE_CREATE;\n+public class FateTestUtil {\n+  // A FateOperation for testing purposes when a FateOperation is needed and whose value needs to\n+  // be a FateOperation workable by USER and META FATEs\n+  public static final Fate.FateOperation TEST_FATE_OP = Fate.FateOperation.TABLE_COMPACT;\n \n   /**\n    * Create the fate table with the exact configuration as the real Fate user instance table\n@@ -90,6 +92,22 @@ public static <T> Optional<FateId> seedTransaction(FateStore<T> store, Fate.Fate\n     }\n   }\n \n+  /**\n+   * Returns a config with all FATE operations assigned to a single pool of size numThreads for both\n+   * USER and META FATE operations\n+   */\n+  public static ConfigurationCopy createTestFateConfig(int numThreads) {\n+    ConfigurationCopy config = new ConfigurationCopy();\n+    // this value isn't important, just needs to be set\n+    config.set(Property.GENERAL_THREADPOOL_SIZE, \""2\"");\n+    config.set(Property.MANAGER_FATE_USER_CONFIG, \""{\\\""\"" + Fate.FateOperation.getAllUserFateOps()\n+        .stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": \"" + numThreads + \""}\"");\n+    config.set(Property.MANAGER_FATE_META_CONFIG, \""{\\\""\"" + Fate.FateOperation.getAllMetaFateOps()\n+        .stream().map(Enum::name).collect(Collectors.joining(\"",\"")) + \""\\\"": \"" + numThreads + \""}\"");\n+    config.set(Property.MANAGER_FATE_IDLE_CHECK_INTERVAL, \""60m\"");\n+    return config;\n+  }\n+\n   /**\n    * Contains the necessary utilities for setting up (and shutting down) a ZooKeeper instance for\n    * use in testing MetaFateStore\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java b/test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java\nindex a5e75416b60..7ff60e5beda 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FlakyFate.java\n@@ -18,10 +18,12 @@\n  */\n package org.apache.accumulo.test.fate;\n \n+import java.util.Set;\n import java.util.function.Function;\n \n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.fate.Fate;\n+import org.apache.accumulo.core.fate.FateExecutor;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.Repo;\n@@ -39,22 +41,38 @@ public FlakyFate(T environment, FateStore<T> store, Function<Repo<T>,String> toL\n   }\n \n   @Override\n-  protected Repo<T> executeCall(FateId fateId, Repo<T> repo) throws Exception {\n-    /*\n-     * This function call assumes that isRead was already called once. So it runs\n-     * call(),isReady(),call() to simulate a situation like isReady(), call(), fault, isReady()\n-     * again, call() again.\n-     */\n-    var next1 = super.executeCall(fateId, repo);\n-    Preconditions.checkState(super.executeIsReady(fateId, repo) == 0);\n-    var next2 = super.executeCall(fateId, repo);\n-    // do some basic checks to ensure similar things were returned\n-    if (next1 == null) {\n-      Preconditions.checkState(next2 == null);\n-    } else {\n-      Preconditions.checkState(next2 != null);\n-      Preconditions.checkState(next1.getClass().equals(next2.getClass()));\n+  protected void startFateExecutors(T environment, AccumuloConfiguration conf,\n+      Set<FateExecutor<T>> fateExecutors) {\n+    for (var poolConfig : getPoolConfigurations(conf).entrySet()) {\n+      fateExecutors.add(\n+          new FlakyFateExecutor<>(this, environment, poolConfig.getKey(), poolConfig.getValue()));\n+    }\n+  }\n+\n+  private static class FlakyFateExecutor<T> extends FateExecutor<T> {\n+    private FlakyFateExecutor(Fate<T> fate, T environment, Set<FateOperation> fateOps,\n+        int poolSize) {\n+      super(fate, environment, fateOps, poolSize);\n+    }\n+\n+    @Override\n+    protected Repo<T> executeCall(FateId fateId, Repo<T> repo) throws Exception {\n+      /*\n+       * This function call assumes that isReady was already called once. So it runs\n+       * call(),isReady(),call() to simulate a situation like isReady(), call(), fault, isReady()\n+       * again, call() again.\n+       */\n+      var next1 = super.executeCall(fateId, repo);\n+      Preconditions.checkState(super.executeIsReady(fateId, repo) == 0);\n+      var next2 = super.executeCall(fateId, repo);\n+      // do some basic checks to ensure similar things were returned\n+      if (next1 == null) {\n+        Preconditions.checkState(next2 == null);\n+      } else {\n+        Preconditions.checkState(next2 != null);\n+        Preconditions.checkState(next1.getClass().equals(next2.getClass()));\n+      }\n+      return next2;\n     }\n-    return next2;\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java b/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java\nindex f4eb390701e..472f9263c92 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/MultipleStoresIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.TEST_FATE_OP;\n+import static org.apache.accumulo.test.fate.FateTestUtil.TEST_FATE_OP;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n@@ -37,8 +37,8 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Predicate;\n \n+import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n-import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateStore;\n@@ -249,30 +249,32 @@ private void testMultipleFateInstances(TestStoreFactory<SleepingTestEnv> testSto\n     Fate<SleepingTestEnv> fate2 =\n         new Fate<>(testEnv2, store2, false, Object::toString, DefaultConfiguration.getInstance());\n \n-    for (int i = 0; i < numFateIds; i++) {\n-      FateId fateId;\n-      // Start half the txns using fate1, and the other half using fate2\n-      if (i % 2 == 0) {\n-        fateId = fate1.startTransaction();\n-        fate1.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n-      } else {\n-        fateId = fate2.startTransaction();\n-        fate2.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+    try {\n+      for (int i = 0; i < numFateIds; i++) {\n+        FateId fateId;\n+        // Start half the txns using fate1, and the other half using fate2\n+        if (i % 2 == 0) {\n+          fateId = fate1.startTransaction();\n+          fate1.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+        } else {\n+          fateId = fate2.startTransaction();\n+          fate2.seedTransaction(TEST_FATE_OP, fateId, new SleepingTestRepo(), true, \""test\"");\n+        }\n+        allIds.add(fateId);\n       }\n-      allIds.add(fateId);\n-    }\n-    assertEquals(numFateIds, allIds.size());\n+      assertEquals(numFateIds, allIds.size());\n \n-    // Should be able to wait for completion on any fate instance\n-    for (FateId fateId : allIds) {\n-      fate2.waitForCompletion(fateId);\n+      // Should be able to wait for completion on any fate instance\n+      for (FateId fateId : allIds) {\n+        fate2.waitForCompletion(fateId);\n+      }\n+      // Ensure that all txns have been executed and have only been executed once\n+      assertTrue(Collections.disjoint(testEnv1.executedOps, testEnv2.executedOps));\n+      assertEquals(allIds, Sets.union(testEnv1.executedOps, testEnv2.executedOps));\n+    } finally {\n+      fate1.shutdown(1, TimeUnit.MINUTES);\n+      fate2.shutdown(1, TimeUnit.MINUTES);\n     }\n-    // Ensure that all txns have been executed and have only been executed once\n-    assertTrue(Collections.disjoint(testEnv1.executedOps, testEnv2.executedOps));\n-    assertEquals(allIds, Sets.union(testEnv1.executedOps, testEnv2.executedOps));\n-\n-    fate1.shutdown(1, TimeUnit.MINUTES);\n-    fate2.shutdown(1, TimeUnit.MINUTES);\n   }\n \n   @Test\n@@ -287,9 +289,10 @@ private void testDeadReservationsCleanup(TestStoreFactory<LatchTestEnv> testStor\n     // reserved using the old LockID should be cleaned up by Fate's DeadReservationCleaner,\n     // then picked up by the new Fate/store.\n \n+    // > 1 to have some concurrency to simulate realistic fate scenario\n+    final int numThreads = 10;\n     // One transaction for each FATE worker thread\n-    final int numFateIds =\n-        Integer.parseInt(Property.MANAGER_FATE_THREADPOOL_SIZE.getDefaultValue());\n+    final int numFateIds = numThreads;\n     final Set<FateId> allIds = new HashSet<>();\n     final LatchTestEnv testEnv1 = new LatchTestEnv();\n     final LatchTestEnv testEnv2 = new LatchTestEnv();\n@@ -297,71 +300,77 @@ private void testDeadReservationsCleanup(TestStoreFactory<LatchTestEnv> testStor\n     final ZooUtil.LockID lock2 = new ZooUtil.LockID(\""/locks\"", \""L2\"", 52);\n     final Set<ZooUtil.LockID> liveLocks = new HashSet<>();\n     final Predicate<ZooUtil.LockID> isLockHeld = liveLocks::contains;\n+    final AccumuloConfiguration config = FateTestUtil.createTestFateConfig(numThreads);\n     Map<FateId,FateStore.FateReservation> reservations;\n \n     final FateStore<LatchTestEnv> store1 = testStoreFactory.create(lock1, isLockHeld);\n     liveLocks.add(lock1);\n+    Fate<LatchTestEnv> fate1 = null;\n+    Fate<LatchTestEnv> fate2 = null;\n \n-    FastFate<LatchTestEnv> fate1 = new FastFate<>(testEnv1, store1, true, Object::toString,\n-        DefaultConfiguration.getInstance());\n-\n-    // Ensure nothing is reserved yet\n-    assertTrue(store1.getActiveReservations().isEmpty());\n+    try {\n+      fate1 = new FastFate<>(testEnv1, store1, true, Object::toString, config);\n+      // Ensure nothing is reserved yet\n+      assertTrue(store1.getActiveReservations().isEmpty());\n \n-    // Create transactions\n-    for (int i = 0; i < numFateIds; i++) {\n-      FateId fateId;\n-      fateId = fate1.startTransaction();\n-      fate1.seedTransaction(TEST_FATE_OP, fateId, new LatchTestRepo(), true, \""test\"");\n-      allIds.add(fateId);\n+      // Create transactions\n+      for (int i = 0; i < numFateIds; i++) {\n+        FateId fateId;\n+        fateId = fate1.startTransaction();\n+        fate1.seedTransaction(TEST_FATE_OP, fateId, new LatchTestRepo(), true, \""test\"");\n+        allIds.add(fateId);\n+      }\n+      assertEquals(numFateIds, allIds.size());\n+\n+      // Wait for all the fate worker threads to start working on the transactions\n+      Wait.waitFor(() -> testEnv1.numWorkers.get() == numFateIds);\n+      // Each fate worker will be hung up working (IN_PROGRESS) on a single transaction\n+\n+      // Verify store1 has the transactions reserved and that they were reserved with lock1\n+      reservations = store1.getActiveReservations();\n+      assertEquals(allIds, reservations.keySet());\n+      reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n+\n+      final FateStore<LatchTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld);\n+\n+      // Verify store2 can see the reserved transactions even though they were reserved using\n+      // store1\n+      reservations = store2.getActiveReservations();\n+      assertEquals(allIds, reservations.keySet());\n+      reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n+\n+      // Simulate what would happen if the Manager using the Fate object (fate1) died.\n+      // isLockHeld would return false for the LockId of the Manager that died (in this case, lock1)\n+      // and true for the new Manager's lock (lock2)\n+      liveLocks.remove(lock1);\n+      liveLocks.add(lock2);\n+\n+      // Create the new Fate/start the Fate threads (the work finder and the workers).\n+      // Don't run another dead reservation cleaner since we already have one running from fate1.\n+      fate2 = new Fate<>(testEnv2, store2, false, Object::toString, config);\n+\n+      // Wait for the \""dead\"" reservations to be deleted and picked up again (reserved using\n+      // fate2/store2/lock2 now).\n+      // They are considered \""dead\"" if they are held by lock1 in this test. We don't have to worry\n+      // about fate1/store1/lock1 being used to reserve the transactions again since all\n+      // the workers for fate1 are hung up\n+      Wait.waitFor(() -> {\n+        Map<FateId,FateStore.FateReservation> store2Reservations = store2.getActiveReservations();\n+        boolean allReservedWithLock2 =\n+            store2Reservations.values().stream().allMatch(entry -> entry.getLockID().equals(lock2));\n+        return store2Reservations.keySet().equals(allIds) && allReservedWithLock2;\n+      }, fate1.getDeadResCleanupDelay().toMillis() * 2);\n+    } finally {\n+      // Finish work and shutdown\n+      testEnv1.workersLatch.countDown();\n+      testEnv2.workersLatch.countDown();\n+      if (fate1 != null) {\n+        fate1.shutdown(1, TimeUnit.MINUTES);\n+      }\n+      if (fate2 != null) {\n+        fate2.shutdown(1, TimeUnit.MINUTES);\n+      }\n     }\n-    assertEquals(numFateIds, allIds.size());\n-\n-    // Wait for all the fate worker threads to start working on the transactions\n-    Wait.waitFor(() -> testEnv1.numWorkers.get() == numFateIds);\n-    // Each fate worker will be hung up working (IN_PROGRESS) on a single transaction\n-\n-    // Verify store1 has the transactions reserved and that they were reserved with lock1\n-    reservations = store1.getActiveReservations();\n-    assertEquals(allIds, reservations.keySet());\n-    reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n-\n-    final FateStore<LatchTestEnv> store2 = testStoreFactory.create(lock2, isLockHeld);\n-\n-    // Verify store2 can see the reserved transactions even though they were reserved using\n-    // store1\n-    reservations = store2.getActiveReservations();\n-    assertEquals(allIds, reservations.keySet());\n-    reservations.values().forEach(res -> assertEquals(lock1, res.getLockID()));\n-\n-    // Simulate what would happen if the Manager using the Fate object (fate1) died.\n-    // isLockHeld would return false for the LockId of the Manager that died (in this case, lock1)\n-    // and true for the new Manager's lock (lock2)\n-    liveLocks.remove(lock1);\n-    liveLocks.add(lock2);\n-\n-    // Create the new Fate/start the Fate threads (the work finder and the workers).\n-    // Don't run another dead reservation cleaner since we already have one running from fate1.\n-    Fate<LatchTestEnv> fate2 =\n-        new Fate<>(testEnv2, store2, false, Object::toString, DefaultConfiguration.getInstance());\n-\n-    // Wait for the \""dead\"" reservations to be deleted and picked up again (reserved using\n-    // fate2/store2/lock2 now).\n-    // They are considered \""dead\"" if they are held by lock1 in this test. We don't have to worry\n-    // about fate1/store1/lock1 being used to reserve the transactions again since all\n-    // the workers for fate1 are hung up\n-    Wait.waitFor(() -> {\n-      Map<FateId,FateStore.FateReservation> store2Reservations = store2.getActiveReservations();\n-      boolean allReservedWithLock2 =\n-          store2Reservations.values().stream().allMatch(entry -> entry.getLockID().equals(lock2));\n-      return store2Reservations.keySet().equals(allIds) && allReservedWithLock2;\n-    }, fate1.getDeadResCleanupDelay().toMillis() * 2);\n-\n-    // Finish work and shutdown\n-    testEnv1.workersLatch.countDown();\n-    testEnv2.workersLatch.countDown();\n-    fate1.shutdown(1, TimeUnit.MINUTES);\n-    fate2.shutdown(1, TimeUnit.MINUTES);\n   }\n \n   public static class SleepingTestRepo implements Repo<SleepingTestEnv> {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\nindex 0c7450c42df..95938a19624 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateIT.java\n@@ -35,7 +35,7 @@\n import org.apache.accumulo.core.zookeeper.ZooSession;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.fate.FateIT;\n-import org.apache.accumulo.test.fate.FateStoreUtil;\n+import org.apache.accumulo.test.fate.FateTestUtil;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.zookeeper.KeeperException;\n import org.junit.jupiter.api.AfterAll;\n@@ -48,18 +48,18 @@ public class MetaFateIT extends FateIT {\n \n   @BeforeAll\n   public static void setup() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.setup(tempDir);\n+    FateTestUtil.MetaFateZKSetup.setup(tempDir);\n   }\n \n   @AfterAll\n   public static void teardown() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.teardown();\n+    FateTestUtil.MetaFateZKSetup.teardown();\n   }\n \n   @Override\n   public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n       FateIdGenerator fateIdGenerator) throws Exception {\n-    var zk = FateStoreUtil.MetaFateZKSetup.getZk();\n+    var zk = FateTestUtil.MetaFateZKSetup.getZk();\n     ServerContext sctx = createMock(ServerContext.class);\n     expect(sctx.getZooSession()).andReturn(zk).anyTimes();\n     replay(sctx);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolResizeIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\nsimilarity index 86%\nrename from test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolResizeIT.java\nrename to test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\nindex 94c97034b20..a1d42fdcaaf 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolResizeIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFatePoolsWatcherIT.java\n@@ -28,30 +28,30 @@\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.server.ServerContext;\n-import org.apache.accumulo.test.fate.FatePoolResizeIT;\n-import org.apache.accumulo.test.fate.FateStoreUtil;\n+import org.apache.accumulo.test.fate.FatePoolsWatcherIT;\n+import org.apache.accumulo.test.fate.FateTestUtil;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.io.TempDir;\n \n-public class MetaFatePoolResizeIT extends FatePoolResizeIT {\n+public class MetaFatePoolsWatcherIT extends FatePoolsWatcherIT {\n   @TempDir\n   private static File tempDir;\n \n   @BeforeAll\n   public static void setup() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.setup(tempDir);\n+    FateTestUtil.MetaFateZKSetup.setup(tempDir);\n   }\n \n   @AfterAll\n   public static void teardown() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.teardown();\n+    FateTestUtil.MetaFateZKSetup.teardown();\n   }\n \n   @Override\n   public void executeTest(FateTestExecutor<PoolResizeTestEnv> testMethod, int maxDeferred,\n       AbstractFateStore.FateIdGenerator fateIdGenerator) throws Exception {\n-    var zk = FateStoreUtil.MetaFateZKSetup.getZk();\n+    var zk = FateTestUtil.MetaFateZKSetup.getZk();\n     ServerContext sctx = createMock(ServerContext.class);\n     expect(sctx.getZooSession()).andReturn(zk).anyTimes();\n     replay(sctx);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\nindex 2620b448a3b..bebc0900e59 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStatusEnforcementIT.java\n@@ -24,7 +24,7 @@\n \n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.test.fate.FateStatusEnforcementIT;\n-import org.apache.accumulo.test.fate.FateStoreUtil;\n+import org.apache.accumulo.test.fate.FateTestUtil;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n@@ -36,17 +36,17 @@ public class MetaFateStatusEnforcementIT extends FateStatusEnforcementIT {\n \n   @BeforeAll\n   public static void beforeAllSetup() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.setup(tempDir);\n+    FateTestUtil.MetaFateZKSetup.setup(tempDir);\n   }\n \n   @AfterAll\n   public static void afterAllTeardown() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.teardown();\n+    FateTestUtil.MetaFateZKSetup.teardown();\n   }\n \n   @BeforeEach\n   public void beforeEachSetup() throws Exception {\n-    store = new MetaFateStore<>(FateStoreUtil.MetaFateZKSetup.getZk(), createDummyLockID(), null);\n+    store = new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(), createDummyLockID(), null);\n     fateId = store.create();\n     txStore = store.reserve(fateId);\n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\nindex 34a1344e998..310570b79d6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaFateStoreFateIT.java\n@@ -46,7 +46,7 @@\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeExistsPolicy;\n import org.apache.accumulo.server.ServerContext;\n import org.apache.accumulo.test.fate.FateStoreIT;\n-import org.apache.accumulo.test.fate.FateStoreUtil;\n+import org.apache.accumulo.test.fate.FateTestUtil;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.Tag;\n@@ -59,21 +59,21 @@ public class MetaFateStoreFateIT extends FateStoreIT {\n \n   @BeforeAll\n   public static void setup() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.setup(tempDir);\n+    FateTestUtil.MetaFateZKSetup.setup(tempDir);\n   }\n \n   @AfterAll\n   public static void teardown() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.teardown();\n+    FateTestUtil.MetaFateZKSetup.teardown();\n   }\n \n   @Override\n   public void executeTest(FateTestExecutor<TestEnv> testMethod, int maxDeferred,\n       FateIdGenerator fateIdGenerator) throws Exception {\n     ServerContext sctx = createMock(ServerContext.class);\n-    expect(sctx.getZooSession()).andReturn(FateStoreUtil.MetaFateZKSetup.getZk()).anyTimes();\n+    expect(sctx.getZooSession()).andReturn(FateTestUtil.MetaFateZKSetup.getZk()).anyTimes();\n     replay(sctx);\n-    MetaFateStore<TestEnv> store = new MetaFateStore<>(FateStoreUtil.MetaFateZKSetup.getZk(),\n+    MetaFateStore<TestEnv> store = new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(),\n         createDummyLockID(), null, maxDeferred, fateIdGenerator);\n \n     // Check that the store has no transactions before and after each test\n@@ -113,7 +113,7 @@ protected void deleteKey(FateId fateId, ServerContext sctx) {\n       // (excluding the FateKey in the new object), and replace the zk node with this new FateData\n       String txPath = Constants.ZFATE + \""/tx_\"" + fateId.getTxUUIDStr();\n       Object currentNode = serializedCons.newInstance(\n-          new Object[] {FateStoreUtil.MetaFateZKSetup.getZk().asReader().getData(txPath)});\n+          new Object[] {FateTestUtil.MetaFateZKSetup.getZk().asReader().getData(txPath)});\n       TStatus currentStatus = (TStatus) status.get(currentNode);\n       Optional<FateStore.FateReservation> currentReservation =\n           getCurrentReservation(reservation, currentNode);\n@@ -125,7 +125,7 @@ protected void deleteKey(FateId fateId, ServerContext sctx) {\n       Object newNode = fateDataCons.newInstance(currentStatus, currentReservation.orElse(null),\n           null, currentRepoDeque, currentTxInfo);\n \n-      FateStoreUtil.MetaFateZKSetup.getZk().asReaderWriter().putPersistentData(txPath,\n+      FateTestUtil.MetaFateZKSetup.getZk().asReaderWriter().putPersistentData(txPath,\n           (byte[]) nodeSerialize.invoke(newNode), NodeExistsPolicy.OVERWRITE);\n     } catch (Exception e) {\n       throw new IllegalStateException(e);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java\nindex 42bd81e3231..29471f86616 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/meta/MetaMultipleStoresIT.java\n@@ -24,7 +24,7 @@\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n-import org.apache.accumulo.test.fate.FateStoreUtil;\n+import org.apache.accumulo.test.fate.FateTestUtil;\n import org.apache.accumulo.test.fate.MultipleStoresIT;\n import org.apache.zookeeper.KeeperException;\n import org.junit.jupiter.api.AfterAll;\n@@ -37,12 +37,12 @@ public class MetaMultipleStoresIT extends MultipleStoresIT {\n \n   @BeforeAll\n   public static void setup() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.setup(tempDir);\n+    FateTestUtil.MetaFateZKSetup.setup(tempDir);\n   }\n \n   @AfterAll\n   public static void teardown() throws Exception {\n-    FateStoreUtil.MetaFateZKSetup.teardown();\n+    FateTestUtil.MetaFateZKSetup.teardown();\n   }\n \n   @Override\n@@ -61,7 +61,7 @@ static class SleepingEnvMetaStoreFactory implements TestStoreFactory<SleepingTes\n     @Override\n     public FateStore<SleepingTestEnv> create(ZooUtil.LockID lockID,\n         Predicate<ZooUtil.LockID> isLockHeld) throws InterruptedException, KeeperException {\n-      return new MetaFateStore<>(FateStoreUtil.MetaFateZKSetup.getZk(), lockID, isLockHeld);\n+      return new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(), lockID, isLockHeld);\n     }\n   }\n \n@@ -69,7 +69,7 @@ static class LatchEnvMetaStoreFactory implements TestStoreFactory<LatchTestEnv>\n     @Override\n     public FateStore<LatchTestEnv> create(ZooUtil.LockID lockID,\n         Predicate<ZooUtil.LockID> isLockHeld) throws InterruptedException, KeeperException {\n-      return new MetaFateStore<>(FateStoreUtil.MetaFateZKSetup.getZk(), lockID, isLockHeld);\n+      return new MetaFateStore<>(FateTestUtil.MetaFateZKSetup.getZk(), lockID, isLockHeld);\n     }\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java\nindex ec16596df67..3f0d53516ab 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateExecutionOrderIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate.user;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n import static org.apache.accumulo.test.fate.TestLock.createDummyLockID;\n \n import org.apache.accumulo.core.client.Accumulo;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\nindex 26b8103e4d6..0c5ba1cd6bf 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate.user;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n import static org.apache.accumulo.test.fate.TestLock.createDummyLockID;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolResizeIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\nsimilarity index 91%\nrename from test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolResizeIT.java\nrename to test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\nindex 77ae75cd63b..67f5cfb1b07 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolResizeIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFatePoolsWatcherIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate.user;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n import static org.apache.accumulo.test.fate.TestLock.createDummyLockID;\n \n import org.apache.accumulo.core.client.Accumulo;\n@@ -26,11 +26,11 @@\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n-import org.apache.accumulo.test.fate.FatePoolResizeIT;\n+import org.apache.accumulo.test.fate.FatePoolsWatcherIT;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n \n-public class UserFatePoolResizeIT extends FatePoolResizeIT {\n+public class UserFatePoolsWatcherIT extends FatePoolsWatcherIT {\n \n   private String table;\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\nindex 904882fcf29..d2d8c3ec125 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStatusEnforcementIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate.user;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n import static org.apache.accumulo.test.fate.TestLock.createDummyLockID;\n \n import org.apache.accumulo.core.client.Accumulo;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\nindex 71af0824961..a112445d993 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateStoreFateIT.java\n@@ -19,7 +19,7 @@\n package org.apache.accumulo.test.fate.user;\n \n import static org.apache.accumulo.core.fate.user.UserFateStore.getRowId;\n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n import static org.apache.accumulo.test.fate.TestLock.createDummyLockID;\n \n import org.apache.accumulo.core.client.Accumulo;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\nindex 507a4b1b862..dff619c01f6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserMultipleStoresIT.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.test.fate.user;\n \n-import static org.apache.accumulo.test.fate.FateStoreUtil.createFateTable;\n+import static org.apache.accumulo.test.fate.FateTestUtil.createFateTable;\n \n import java.util.function.Predicate;\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5275"", ""pr_id"": 5275, ""issue_id"": 5269, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Add metric that shows data size of compaction queues\nIn #5252 compactions were updated to be limited by data size of compaction jobs instead of the number on entires.  Need to add a new metric that exposes the current data size of a compaction queue so the property where the max data size is set can be tuned."", ""issue_word_count"": 58, ""test_files_count"": 3, ""non_test_files_count"": 3, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/metrics/Metric.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/QueueMetrics.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java"", ""test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java""], ""pr_changed_test_files"": [""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java"", ""test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java""], ""base_commit"": ""ee81a3c30d3a34604c33c042abb631db5952388b"", ""head_commit"": ""4ec02ab6cf4ab8e7c165d4a0d7f7df40b7afaa0a"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5275"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5275"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-22T16:45:49.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/metrics/Metric.java b/core/src/main/java/org/apache/accumulo/core/metrics/Metric.java\nindex da61305fc21..e5ddf3be6a4 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metrics/Metric.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metrics/Metric.java\n@@ -54,12 +54,14 @@ public enum Metric {\n       MetricDocSection.COMPACTION),\n   COMPACTOR_JOB_PRIORITY_QUEUES(\""accumulo.compaction.queue.count\"", MetricType.GAUGE,\n       \""Number of priority queues for compaction jobs.\"", MetricDocSection.COMPACTION),\n-  COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH(\""accumulo.compaction.queue.length\"", MetricType.GAUGE,\n-      \""Length of priority queue.\"", MetricDocSection.COMPACTION),\n+  COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE(\""accumulo.compaction.queue.max.size\"", MetricType.GAUGE,\n+      \""The maximum size in bytes of all jobs.\"", MetricDocSection.COMPACTION),\n   COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_DEQUEUED(\""accumulo.compaction.queue.jobs.dequeued\"",\n       MetricType.GAUGE, \""Count of dequeued jobs.\"", MetricDocSection.COMPACTION),\n   COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED(\""accumulo.compaction.queue.jobs.queued\"",\n       MetricType.GAUGE, \""Count of queued jobs.\"", MetricDocSection.COMPACTION),\n+  COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE(\""accumulo.compaction.queue.jobs.size\"", MetricType.GAUGE,\n+      \""Size of queued jobs in bytes.\"", MetricDocSection.COMPACTION),\n   COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_REJECTED(\""accumulo.compaction.queue.jobs.rejected\"",\n       MetricType.GAUGE, \""Count of rejected jobs.\"", MetricDocSection.COMPACTION),\n   COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_PRIORITY(\""accumulo.compaction.queue.jobs.priority\"",\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/QueueMetrics.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/QueueMetrics.java\nindex aa429d6999d..b0771bc2fe2 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/QueueMetrics.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/QueueMetrics.java\n@@ -27,7 +27,8 @@\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_PRIORITY;\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED;\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_REJECTED;\n-import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH;\n+import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE;\n+import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE;\n import static org.apache.accumulo.core.metrics.MetricsUtil.formatString;\n \n import java.util.HashMap;\n@@ -59,6 +60,7 @@ public class QueueMetrics implements MetricsProducer {\n   private static class QueueMeters {\n     private final Gauge length;\n     private final Gauge jobsQueued;\n+    private final Gauge jobsQueuedSize;\n     private final Gauge jobsDequeued;\n     private final Gauge jobsRejected;\n     private final Gauge jobsLowestPriority;\n@@ -72,8 +74,8 @@ public QueueMeters(MeterRegistry meterRegistry, CompactorGroupId cgid,\n       var queueId = formatString(cgid.canonical());\n \n       length =\n-          Gauge.builder(COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH.getName(), queue, q -> q.getMaxSize())\n-              .description(COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH.getDescription())\n+          Gauge.builder(COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE.getName(), queue, q -> q.getMaxSize())\n+              .description(COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE.getDescription())\n               .tags(List.of(Tag.of(\""queue.id\"", queueId))).register(meterRegistry);\n \n       jobsQueued = Gauge\n@@ -82,6 +84,12 @@ public QueueMeters(MeterRegistry meterRegistry, CompactorGroupId cgid,\n           .description(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED.getDescription())\n           .tags(List.of(Tag.of(\""queue.id\"", queueId))).register(meterRegistry);\n \n+      jobsQueuedSize = Gauge\n+          .builder(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE.getName(), queue,\n+              q -> q.getQueuedJobsSize())\n+          .description(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE.getDescription())\n+          .tags(List.of(Tag.of(\""queue.id\"", queueId))).register(meterRegistry);\n+\n       jobsDequeued = Gauge\n           .builder(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_DEQUEUED.getName(), queue,\n               q -> q.getDequeuedJobs())\n@@ -134,6 +142,7 @@ private void removeMeters(MeterRegistry registry) {\n       registry.remove(jobsMaxAge);\n       registry.remove(jobsAvgAge);\n       registry.remove(jobsQueueTimer);\n+      registry.remove(jobsQueuedSize);\n     }\n   }\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\nindex f183b50b86f..9a36f07f561 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\n@@ -233,12 +233,6 @@ public synchronized long getMaxSize() {\n     return maxSize.get();\n   }\n \n-  public synchronized void setMaxSize(long maxSize) {\n-    Preconditions.checkArgument(maxSize > 0,\n-        \""Maximum size of the Compaction job priority queue must be greater than 0\"");\n-    this.maxSize.set(maxSize);\n-  }\n-\n   public long getRejectedJobs() {\n     return rejectedJobs.get();\n   }\n@@ -251,6 +245,10 @@ public synchronized long getQueuedJobs() {\n     return jobQueue.entrySize();\n   }\n \n+  public synchronized long getQueuedJobsSize() {\n+    return jobQueue.dataSize();\n+  }\n+\n   public synchronized long getLowestPriority() {\n     if (jobQueue.isEmpty()) {\n       return 0;\n"", ""test_patch"": ""diff --git a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\nindex 01c18798754..6617ad5cef2 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\n@@ -20,7 +20,6 @@\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.util.ArrayList;\n@@ -339,16 +338,4 @@ public void testAsyncCancelCleanup() {\n         < 2 * (CompactionJobPriorityQueue.FUTURE_CHECK_THRESHOLD + CANCEL_THRESHOLD));\n     assertTrue(maxFuturesSize > 2 * CompactionJobPriorityQueue.FUTURE_CHECK_THRESHOLD);\n   }\n-\n-  @Test\n-  public void testChangeMaxSize() {\n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100, mj -> 1);\n-    assertEquals(100, queue.getMaxSize());\n-    queue.setMaxSize(50);\n-    assertEquals(50, queue.getMaxSize());\n-    assertThrows(IllegalArgumentException.class, () -> queue.setMaxSize(0));\n-    assertThrows(IllegalArgumentException.class, () -> queue.setMaxSize(-1));\n-    // Make sure previous value was not changed after invalid setting\n-    assertEquals(50, queue.getMaxSize());\n-  }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java b/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\nindex 2a032565953..3a8a6a601b4 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\n@@ -22,7 +22,8 @@\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_PRIORITY;\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED;\n import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_REJECTED;\n-import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH;\n+import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE;\n+import static org.apache.accumulo.core.metrics.Metric.COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n@@ -337,7 +338,9 @@ public void testQueueMetrics() throws Exception {\n     }\n \n     boolean sawMetricsQ1 = false;\n-    while (!sawMetricsQ1) {\n+    boolean sawMetricsQ1Size = false;\n+\n+    while (!sawMetricsQ1 || !sawMetricsQ1Size) {\n       while (!queueMetrics.isEmpty()) {\n         var qm = queueMetrics.take();\n         if (qm.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED.getName())\n@@ -346,7 +349,14 @@ public void testQueueMetrics() throws Exception {\n             sawMetricsQ1 = true;\n           }\n         }\n+        if (qm.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE.getName())\n+            && qm.getTags().containsValue(QUEUE1_METRIC_LABEL)) {\n+          if (Integer.parseInt(qm.getValue()) > 0) {\n+            sawMetricsQ1Size = true;\n+          }\n+        }\n       }\n+\n       // If metrics are not found in the queue, sleep until the next poll.\n       UtilWaitThread.sleep(TestStatsDRegistryFactory.pollingFrequency.toMillis());\n     }\n@@ -366,7 +376,7 @@ public void testQueueMetrics() throws Exception {\n       } else if (metric.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_PRIORITY.getName())\n           && metric.getTags().containsValue(QUEUE1_METRIC_LABEL)) {\n         lowestPriority = Math.max(lowestPriority, Long.parseLong(metric.getValue()));\n-      } else if (metric.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH.getName())\n+      } else if (metric.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE.getName())\n           && metric.getTags().containsValue(QUEUE1_METRIC_LABEL)) {\n         queueSize = Integer.parseInt(metric.getValue());\n       } else if (metric.getName().contains(COMPACTOR_JOB_PRIORITY_QUEUES.getName())) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java b/test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java\nindex a5c777c64d4..e7492968cf5 100644\n--- a/test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/metrics/MetricsIT.java\n@@ -149,9 +149,10 @@ public void confirmMetricsPublished() throws Exception {\n     final int compactionPriorityQueueDequeuedBit = 2;\n     final int compactionPriorityQueueRejectedBit = 3;\n     final int compactionPriorityQueuePriorityBit = 4;\n+    final int compactionPriorityQueueSizeBit = 5;\n \n-    final BitSet trueSet = new BitSet(5);\n-    trueSet.set(0, 4, true);\n+    final BitSet trueSet = new BitSet(6);\n+    trueSet.set(0, 5, true);\n \n     final BitSet queueMetricsSeen = new BitSet(5);\n \n@@ -187,7 +188,7 @@ public void confirmMetricsPublished() throws Exception {\n               seenMetrics.add(metric);\n               expectedMetrics.remove(metric);\n               switch (metric) {\n-                case COMPACTOR_JOB_PRIORITY_QUEUE_LENGTH:\n+                case COMPACTOR_JOB_PRIORITY_QUEUE_MAX_SIZE:\n                   queueMetricsSeen.set(compactionPriorityQueueLengthBit, true);\n                   break;\n                 case COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_QUEUED:\n@@ -202,6 +203,9 @@ public void confirmMetricsPublished() throws Exception {\n                 case COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_PRIORITY:\n                   queueMetricsSeen.set(compactionPriorityQueuePriorityBit, true);\n                   break;\n+                case COMPACTOR_JOB_PRIORITY_QUEUE_JOBS_SIZE:\n+                  queueMetricsSeen.set(compactionPriorityQueueSizeBit, true);\n+                  break;\n                 default:\n                   break;\n               }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5262"", ""pr_id"": 5262, ""issue_id"": 5181, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Improve performance of FateLock code for the case when there are lots of locks.\n**Is your feature request related to a problem? Please describe.**\r\n\r\nAttempted to intiate many compaction for many small ranges of a table.  As more compactions were started the fate operations for each took longer and longer to get a FateLock.  Looking at jstacks the code was  [here](https://github.com/apache/accumulo/blob/32d3ca8efe04d43e08c1ad2a23f7da560ef006f7/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/compact/CompactRange.java#L83-L87) and  called [this](https://github.com/apache/accumulo/blob/32d3ca8efe04d43e08c1ad2a23f7da560ef006f7/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java#L106) and would spend time there.  Each fate operation that was added would make the getData calls for all others that had been added before.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAttempt to avoid the calls to getData by encoding the information in the node name.  The information stored in the data for the node is a fate uuid and whether its a read or write lock.  This information could be encoded in the node name.  Then the single RPC to zookeeper to getChildren would get call needed information.  This would avoid making the RPC for each child to getData.\r\n\r\n#5180 will improve the situation somewhat, but there could still be a lot of RPCs.\r\n\r\n**Additional context**\r\n\r\nNot sure if this would be suitable for an IT, but would may be good to try.  Could try to make an IT for this creates a table w/ 1K tablets and then spins up 1K compactions via API one per tablet.   Can try running this before and after the change and looking at the CompactRange.isReady times.   Could try increasing 1K also and looking at the time differences as that increases.\r\n\r\n"", ""issue_word_count"": 294, ""test_files_count"": 4, ""non_test_files_count"": 3, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLock.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java"", ""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java"", ""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/FateLockTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java"", ""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/FateLockTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java""], ""base_commit"": ""5c39aac379036dcacead35c8f85ac8f67e3ecbc2"", ""head_commit"": ""33e9bf610354627787fd9316ef9da27550319603"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5262"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5262"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-01T21:36:45.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java b/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\nindex a5b1c5645af..9577ab7ff2b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/AdminUtil.java\n@@ -33,12 +33,14 @@\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.SortedSet;\n import java.util.stream.Stream;\n \n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.FateIdStatus;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.ReadOnlyFateTxStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n+import org.apache.accumulo.core.fate.zookeeper.DistributedReadWriteLock.LockType;\n import org.apache.accumulo.core.fate.zookeeper.FateLock;\n import org.apache.accumulo.core.fate.zookeeper.FateLock.FateLockPath;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeMissingPolicy;\n@@ -275,24 +277,21 @@ private void findLocks(ZooSession zk, final ServiceLockPath lockPath,\n     List<String> lockedIds = zr.getChildren(lockPath.toString());\n \n     for (String id : lockedIds) {\n-\n       try {\n-\n         FateLockPath fLockPath = FateLock.path(lockPath + \""/\"" + id);\n-        List<String> lockNodes =\n-            FateLock.validateAndSort(fLockPath, zr.getChildren(fLockPath.toString()));\n+        SortedSet<FateLock.NodeName> lockNodes =\n+            FateLock.validateAndWarn(fLockPath, zr.getChildren(fLockPath.toString()));\n \n         int pos = 0;\n         boolean sawWriteLock = false;\n \n-        for (String node : lockNodes) {\n+        for (FateLock.NodeName node : lockNodes) {\n           try {\n-            byte[] data = zr.getData(lockPath + \""/\"" + id + \""/\"" + node);\n-            // Example data: \""READ:<FateId>\"". FateId contains ':' hence the limit of 2\n-            String[] lda = new String(data, UTF_8).split(\"":\"", 2);\n-            FateId fateId = FateId.from(lda[1]);\n+            FateLock.FateLockEntry fateLockEntry = node.fateLockEntry.get();\n+            var fateId = fateLockEntry.getFateId();\n+            var lockType = fateLockEntry.getLockType();\n \n-            if (lda[0].charAt(0) == 'W') {\n+            if (lockType == LockType.WRITE) {\n               sawWriteLock = true;\n             }\n \n@@ -300,13 +299,14 @@ private void findLocks(ZooSession zk, final ServiceLockPath lockPath,\n \n             if (pos == 0) {\n               locks = heldLocks;\n-            } else if (lda[0].charAt(0) == 'R' && !sawWriteLock) {\n+            } else if (lockType == LockType.READ && !sawWriteLock) {\n               locks = heldLocks;\n             } else {\n               locks = waitingLocks;\n             }\n \n-            locks.computeIfAbsent(fateId, k -> new ArrayList<>()).add(lda[0].charAt(0) + \"":\"" + id);\n+            locks.computeIfAbsent(fateId, k -> new ArrayList<>())\n+                .add(lockType.name().charAt(0) + \"":\"" + id);\n \n           } catch (Exception e) {\n             log.error(\""{}\"", e.getMessage(), e);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLock.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLock.java\nindex 1f36ee7a92b..1b52a477d73 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLock.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLock.java\n@@ -27,6 +27,7 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.locks.Condition;\n import java.util.concurrent.locks.Lock;\n+import java.util.function.BiPredicate;\n import java.util.function.Supplier;\n \n import org.apache.accumulo.core.fate.FateId;\n@@ -50,9 +51,10 @@ public enum LockType {\n   // them,\n   // a writer only runs when they are at the top of the queue.\n   public interface QueueLock {\n-    SortedMap<Long,Supplier<FateLockEntry>> getEarlierEntries(long entry);\n+    SortedMap<Long,Supplier<FateLockEntry>>\n+        getEntries(BiPredicate<Long,Supplier<FateLockEntry>> predicate);\n \n-    void removeEntry(long entry);\n+    void removeEntry(FateLockEntry data, long seq);\n \n     long addEntry(FateLockEntry entry);\n   }\n@@ -115,7 +117,9 @@ public boolean tryLock() {\n         entry = qlock.addEntry(FateLockEntry.from(this.getType(), this.fateId));\n         log.info(\""Added lock entry {} fateId {} lockType {}\"", entry, fateId, getType());\n       }\n-      SortedMap<Long,Supplier<FateLockEntry>> entries = qlock.getEarlierEntries(entry);\n+\n+      SortedMap<Long,Supplier<FateLockEntry>> entries =\n+          qlock.getEntries((seq, lockData) -> seq <= entry);\n       for (Entry<Long,Supplier<FateLockEntry>> entry : entries.entrySet()) {\n         if (entry.getKey().equals(this.entry)) {\n           return true;\n@@ -150,7 +154,7 @@ public void unlock() {\n         return;\n       }\n       log.debug(\""Removing lock entry {} fateId {} lockType {}\"", entry, this.fateId, getType());\n-      qlock.removeEntry(entry);\n+      qlock.removeEntry(FateLockEntry.from(this.getType(), this.fateId), entry);\n       entry = -1;\n     }\n \n@@ -181,7 +185,8 @@ public boolean tryLock() {\n         entry = qlock.addEntry(FateLockEntry.from(this.getType(), this.fateId));\n         log.info(\""Added lock entry {} fateId {} lockType {}\"", entry, this.fateId, getType());\n       }\n-      SortedMap<Long,Supplier<FateLockEntry>> entries = qlock.getEarlierEntries(entry);\n+      SortedMap<Long,Supplier<FateLockEntry>> entries =\n+          qlock.getEntries((seq, locData) -> seq <= entry);\n       Iterator<Entry<Long,Supplier<FateLockEntry>>> iterator = entries.entrySet().iterator();\n       if (!iterator.hasNext()) {\n         throw new IllegalStateException(\""Did not find our own lock in the queue: \"" + this.entry\n@@ -200,19 +205,26 @@ public DistributedReadWriteLock(QueueLock qlock, FateId fateId) {\n   }\n \n   public static DistributedLock recoverLock(QueueLock qlock, FateId fateId) {\n-    SortedMap<Long,Supplier<FateLockEntry>> entries = qlock.getEarlierEntries(Long.MAX_VALUE);\n-    for (Entry<Long,Supplier<FateLockEntry>> entry : entries.entrySet()) {\n-      FateLockEntry lockEntry = entry.getValue().get();\n-      if (fateId.equals(lockEntry.getFateId())) {\n+    SortedMap<Long,Supplier<FateLockEntry>> entries =\n+        qlock.getEntries((seq, lockData) -> lockData.get().fateId.equals(fateId));\n+\n+    switch (entries.size()) {\n+      case 0:\n+        return null;\n+      case 1:\n+        var entry = entries.entrySet().iterator().next();\n+        FateLockEntry lockEntry = entry.getValue().get();\n         switch (lockEntry.getLockType()) {\n           case READ:\n             return new ReadLock(qlock, lockEntry.getFateId(), entry.getKey());\n           case WRITE:\n             return new WriteLock(qlock, lockEntry.getFateId(), entry.getKey());\n+          default:\n+            throw new IllegalStateException(\""Unknown lock type \"" + lockEntry.getLockType());\n         }\n-      }\n+      default:\n+        throw new IllegalStateException(\""Found more than one lock node \"" + entries);\n     }\n-    return null;\n   }\n \n   @Override\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java\nindex 74c3065c702..66b8191f2ed 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/FateLock.java\n@@ -18,16 +18,16 @@\n  */\n package org.apache.accumulo.core.fate.zookeeper;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n import static java.util.Objects.requireNonNull;\n \n-import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n import java.util.Objects;\n import java.util.SortedMap;\n+import java.util.SortedSet;\n import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.function.BiPredicate;\n import java.util.function.Supplier;\n \n import org.apache.accumulo.core.fate.FateId;\n@@ -41,6 +41,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Preconditions;\n import com.google.common.base.Suppliers;\n \n /**\n@@ -49,7 +50,7 @@\n public class FateLock implements QueueLock {\n   private static final Logger log = LoggerFactory.getLogger(FateLock.class);\n \n-  private static final String PREFIX = \""flock#\"";\n+  static final String PREFIX = \""flock#\"";\n \n   private final ZooReaderWriter zoo;\n   private final FateLockPath path;\n@@ -67,7 +68,7 @@ public String toString() {\n     }\n   }\n \n-  public static class FateLockEntry {\n+  public static class FateLockEntry implements Comparable<FateLockEntry> {\n     final LockType lockType;\n     final FateId fateId;\n \n@@ -76,26 +77,10 @@ private FateLockEntry(LockType lockType, FateId fateId) {\n       this.fateId = Objects.requireNonNull(fateId);\n     }\n \n-    private FateLockEntry(byte[] entry) {\n-      if (entry == null || entry.length < 1) {\n-        throw new IllegalArgumentException();\n-      }\n-\n-      int split = -1;\n-      for (int i = 0; i < entry.length; i++) {\n-        if (entry[i] == ':') {\n-          split = i;\n-          break;\n-        }\n-      }\n-\n-      if (split == -1) {\n-        throw new IllegalArgumentException();\n-      }\n-\n-      this.lockType = LockType.valueOf(new String(entry, 0, split, UTF_8));\n-      this.fateId =\n-          FateId.from(new String(Arrays.copyOfRange(entry, split + 1, entry.length), UTF_8));\n+    private FateLockEntry(String entry) {\n+      var fields = entry.split(\"":\"", 2);\n+      this.lockType = LockType.valueOf(fields[0]);\n+      this.fateId = FateId.from(fields[1]);\n     }\n \n     public LockType getLockType() {\n@@ -106,14 +91,8 @@ public FateId getFateId() {\n       return fateId;\n     }\n \n-    public byte[] serialize() {\n-      byte[] typeBytes = lockType.name().getBytes(UTF_8);\n-      byte[] fateIdBytes = fateId.canonical().getBytes(UTF_8);\n-      byte[] result = new byte[fateIdBytes.length + 1 + typeBytes.length];\n-      System.arraycopy(typeBytes, 0, result, 0, typeBytes.length);\n-      result[typeBytes.length] = ':';\n-      System.arraycopy(fateIdBytes, 0, result, typeBytes.length + 1, fateIdBytes.length);\n-      return result;\n+    public String serialize() {\n+      return lockType.name() + \"":\"" + fateId.canonical();\n     }\n \n     @Override\n@@ -137,9 +116,18 @@ public static FateLockEntry from(LockType lockType, FateId fateId) {\n       return new FateLockEntry(lockType, fateId);\n     }\n \n-    public static FateLockEntry deserialize(byte[] serialized) {\n+    public static FateLockEntry deserialize(String serialized) {\n       return new FateLockEntry(serialized);\n     }\n+\n+    @Override\n+    public int compareTo(FateLockEntry o) {\n+      int cmp = lockType.compareTo(o.lockType);\n+      if (cmp == 0) {\n+        cmp = fateId.compareTo(o.fateId);\n+      }\n+      return cmp;\n+    }\n   }\n \n   public static FateLockPath path(String path) {\n@@ -151,16 +139,59 @@ public FateLock(ZooReaderWriter zrw, FateLockPath path) {\n     this.path = requireNonNull(path);\n   }\n \n+  public static class NodeName implements Comparable<NodeName> {\n+    public final long sequence;\n+    public final Supplier<FateLockEntry> fateLockEntry;\n+\n+    NodeName(String nodeName) {\n+      int len = nodeName.length();\n+      Preconditions.checkArgument(nodeName.startsWith(PREFIX) && nodeName.charAt(len - 11) == '#',\n+          \""Illegal node name %s\"", nodeName);\n+      sequence = Long.parseUnsignedLong(nodeName.substring(len - 10), 10);\n+      // Use a supplier so we don't need to deserialize unless the calling code cares about\n+      // the value for that entry.\n+      fateLockEntry = Suppliers\n+          .memoize(() -> FateLockEntry.deserialize(nodeName.substring(PREFIX.length(), len - 11)));\n+    }\n+\n+    @Override\n+    public int compareTo(NodeName o) {\n+      int cmp = Long.compare(sequence, o.sequence);\n+      if (cmp == 0) {\n+        cmp = fateLockEntry.get().compareTo(o.fateLockEntry.get());\n+      }\n+      return cmp;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (o instanceof NodeName) {\n+        return this.compareTo((NodeName) o) == 0;\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(sequence, fateLockEntry.get());\n+    }\n+  }\n+\n   @Override\n   public long addEntry(FateLockEntry entry) {\n+\n+    String dataString = entry.serialize();\n+    Preconditions.checkState(!dataString.contains(\""#\""));\n+\n     String newPath;\n     try {\n       while (true) {\n         try {\n-          newPath = zoo.putPersistentSequential(path + \""/\"" + PREFIX, entry.serialize());\n+          newPath =\n+              zoo.putPersistentSequential(path + \""/\"" + PREFIX + dataString + \""#\"", new byte[0]);\n           String[] parts = newPath.split(\""/\"");\n           String last = parts[parts.length - 1];\n-          return Long.parseLong(last.substring(PREFIX.length()));\n+          return new NodeName(last).sequence;\n         } catch (NoNodeException nne) {\n           // the parent does not exist so try to create it\n           zoo.putPersistentData(path.toString(), new byte[] {}, NodeExistsPolicy.SKIP);\n@@ -172,7 +203,8 @@ public long addEntry(FateLockEntry entry) {\n   }\n \n   @Override\n-  public SortedMap<Long,Supplier<FateLockEntry>> getEarlierEntries(long entry) {\n+  public SortedMap<Long,Supplier<FateLockEntry>>\n+      getEntries(BiPredicate<Long,Supplier<FateLockEntry>> predicate) {\n     SortedMap<Long,Supplier<FateLockEntry>> result = new TreeMap<>();\n     try {\n       List<String> children = Collections.emptyList();\n@@ -184,17 +216,9 @@ public SortedMap<Long,Supplier<FateLockEntry>> getEarlierEntries(long entry) {\n       }\n \n       for (String name : children) {\n-        // this try catch must be done inside the loop because some subset of the children may exist\n-        try {\n-          long order = Long.parseLong(name.substring(PREFIX.length()));\n-          if (order <= entry) {\n-            byte[] data = zoo.getData(path + \""/\"" + name);\n-            // Use a supplier so we don't need to deserialize unless the calling code cares about\n-            // the value for that entry.\n-            result.put(order, Suppliers.memoize(() -> FateLockEntry.deserialize(data)));\n-          }\n-        } catch (KeeperException.NoNodeException ex) {\n-          // ignored\n+        var parsed = new NodeName(name);\n+        if (predicate.test(parsed.sequence, parsed.fateLockEntry)) {\n+          Preconditions.checkState(result.put(parsed.sequence, parsed.fateLockEntry) == null);\n         }\n       }\n     } catch (KeeperException | InterruptedException ex) {\n@@ -204,9 +228,12 @@ public SortedMap<Long,Supplier<FateLockEntry>> getEarlierEntries(long entry) {\n   }\n \n   @Override\n-  public void removeEntry(long entry) {\n+  public void removeEntry(FateLockEntry data, long entry) {\n+    String dataString = data.serialize();\n+    Preconditions.checkState(!dataString.contains(\""#\""));\n     try {\n-      zoo.recursiveDelete(path + String.format(\""/%s%010d\"", PREFIX, entry), NodeMissingPolicy.SKIP);\n+      zoo.recursiveDelete(path + String.format(\""/%s%s#%010d\"", PREFIX, dataString, entry),\n+          NodeMissingPolicy.SKIP);\n       try {\n         // try to delete the parent if it has no children\n         zoo.delete(path.toString());\n@@ -221,50 +248,25 @@ public void removeEntry(long entry) {\n   /**\n    * Validate and sort child nodes at this lock path by the lock prefix\n    */\n-  public static List<String> validateAndSort(FateLockPath path, List<String> children) {\n+  public static SortedSet<NodeName> validateAndWarn(FateLockPath path, List<String> children) {\n     log.trace(\""validating and sorting children at path {}\"", path);\n-    List<String> validChildren = new ArrayList<>();\n+\n+    SortedSet<NodeName> validChildren = new TreeSet<>();\n+\n     if (children == null || children.isEmpty()) {\n       return validChildren;\n     }\n+\n     children.forEach(c -> {\n       log.trace(\""Validating {}\"", c);\n-      if (c.startsWith(PREFIX)) {\n-        int idx = c.indexOf('#');\n-        String sequenceNum = c.substring(idx + 1);\n-        if (sequenceNum.length() == 10) {\n-          try {\n-            log.trace(\""Testing number format of {}\"", sequenceNum);\n-            Integer.parseInt(sequenceNum);\n-            validChildren.add(c);\n-          } catch (NumberFormatException e) {\n-            log.warn(\""Fate lock found with invalid sequence number format: {} (not a number)\"", c);\n-          }\n-        } else {\n-          log.warn(\""Fate lock found with invalid sequence number format: {} (not 10 characters)\"",\n-              c);\n-        }\n-      } else {\n-        log.warn(\""Fate lock found with invalid lock format: {} (does not start with {})\"", c,\n-            PREFIX);\n+      try {\n+        var fateLockNode = new NodeName(c);\n+        validChildren.add(fateLockNode);\n+      } catch (RuntimeException e) {\n+        log.warn(\""Illegal fate lock node {}\"", c, e);\n       }\n     });\n \n-    if (validChildren.size() > 1) {\n-      validChildren.sort((o1, o2) -> {\n-        // Lock should be of the form:\n-        // lock-sequenceNumber\n-        // Example:\n-        // flock#0000000000\n-\n-        // Lock length - sequenceNumber length\n-        // 16 - 10\n-        int secondHashIdx = 6;\n-        return Integer.valueOf(o1.substring(secondHashIdx))\n-            .compareTo(Integer.valueOf(o2.substring(secondHashIdx)));\n-      });\n-    }\n-    log.trace(\""Children nodes (size: {}): {}\"", validChildren.size(), validChildren);\n     return validChildren;\n   }\n }\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java\nindex bf55b79d4a1..229e8ec07d2 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/DistributedReadWriteLockTest.java\n@@ -27,6 +27,7 @@\n import java.util.concurrent.atomic.AtomicIntegerArray;\n import java.util.concurrent.locks.Lock;\n import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.function.BiPredicate;\n import java.util.function.Supplier;\n \n import org.apache.accumulo.core.fate.FateId;\n@@ -45,14 +46,19 @@ public static class MockQueueLock implements QueueLock {\n     final SortedMap<Long,FateLockEntry> locks = new TreeMap<>();\n \n     @Override\n-    public synchronized SortedMap<Long,Supplier<FateLockEntry>> getEarlierEntries(long entry) {\n+    public synchronized SortedMap<Long,Supplier<FateLockEntry>>\n+        getEntries(BiPredicate<Long,Supplier<FateLockEntry>> predicate) {\n       SortedMap<Long,Supplier<FateLockEntry>> result = new TreeMap<>();\n-      locks.headMap(entry + 1).forEach((k, v) -> result.put(k, () -> v));\n+      locks.forEach((seq, lockData) -> {\n+        if (predicate.test(seq, () -> lockData)) {\n+          result.put(seq, () -> lockData);\n+        }\n+      });\n       return result;\n     }\n \n     @Override\n-    public synchronized void removeEntry(long entry) {\n+    public synchronized void removeEntry(FateLockEntry data, long entry) {\n       synchronized (locks) {\n         locks.remove(entry);\n         locks.notifyAll();\n@@ -147,7 +153,7 @@ public void testFateLockEntrySerDes() {\n     assertEquals(LockType.READ, entry.getLockType());\n     assertEquals(FateId.from(FateInstanceType.USER, uuid), entry.getFateId());\n \n-    byte[] serialized = entry.serialize();\n+    String serialized = entry.serialize();\n     var deserialized = FateLockEntry.deserialize(serialized);\n     assertEquals(entry, deserialized);\n   }\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/FateLockTest.java b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/FateLockTest.java\nnew file mode 100644\nindex 00000000000..b4648c3f90c\n--- /dev/null\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/FateLockTest.java\n@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.fate.zookeeper;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+import java.util.UUID;\n+\n+import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateInstanceType;\n+import org.junit.jupiter.api.Test;\n+\n+public class FateLockTest {\n+\n+  @Test\n+  public void testParsing() {\n+    var fateId = FateId.from(FateInstanceType.USER, UUID.randomUUID());\n+    // ZooKeeper docs state that sequence numbers are formatted using %010d\n+    String lockData = \""WRITE:\"" + fateId.canonical();\n+    var lockNode =\n+        new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + String.format(\""%010d\"", 40));\n+    assertEquals(40, lockNode.sequence);\n+    assertEquals(lockData, lockNode.fateLockEntry.get().serialize());\n+\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(lockData + \""#\"" + String.format(\""%010d\"", 40)));\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + String.format(\""%d\"", 40)));\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + String.format(\""%09d\"", 40)));\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + String.format(\""%011d\"", 40)));\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#abc\""));\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + String.format(\""%010d\"", 40)));\n+\n+    // ZooKeeper docs state that sequence numbers can roll and become negative. The FateLock code\n+    // does not support this, so make sure it fails if this happens.\n+    for (int i : new int[] {Integer.MIN_VALUE, Integer.MIN_VALUE / 2, Integer.MIN_VALUE / 10,\n+        Integer.MIN_VALUE / 1000, -40}) {\n+      var seq = String.format(\""%010d\"", i);\n+      if (seq.length() == 10) {\n+        assertThrows(NumberFormatException.class,\n+            () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + seq));\n+      } else if (seq.length() == 11) {\n+        assertThrows(IllegalArgumentException.class,\n+            () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + seq));\n+      } else {\n+        fail(\""Unexpected length \"" + seq.length());\n+      }\n+    }\n+\n+    // Test a negative number that is not formatted w/ %010d\n+    assertThrows(IllegalArgumentException.class,\n+        () -> new FateLock.NodeName(FateLock.PREFIX + lockData + \""#\"" + String.format(\""%d\"", -40)));\n+  }\n+}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\nindex b3ed3c26fa2..46271c2cafb 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n@@ -99,15 +99,20 @@ public static String row(int r) {\n     return String.format(\""r:%04d\"", r);\n   }\n \n-  public static void compact(final AccumuloClient client, String table1, int modulus,\n-      String expectedQueue, boolean wait)\n-      throws AccumuloSecurityException, TableNotFoundException, AccumuloException {\n+  public static void addCompactionIterators(CompactionConfig config, int modulus,\n+      String expectedQueue) {\n     IteratorSetting iterSetting = new IteratorSetting(100, TestFilter.class);\n     // make sure iterator options make it to compactor process\n     iterSetting.addOption(\""expectedQ\"", expectedQueue);\n     iterSetting.addOption(\""modulus\"", modulus + \""\"");\n-    CompactionConfig config =\n-        new CompactionConfig().setIterators(List.of(iterSetting)).setWait(wait);\n+    config.setIterators(List.of(iterSetting));\n+  }\n+\n+  public static void compact(final AccumuloClient client, String table1, int modulus,\n+      String expectedQueue, boolean wait)\n+      throws AccumuloSecurityException, TableNotFoundException, AccumuloException {\n+    CompactionConfig config = new CompactionConfig().setWait(wait);\n+    addCompactionIterators(config, modulus, expectedQueue);\n     client.tableOperations().compact(table1, config);\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\nindex 99002079d4a..fc1020bcfad 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n@@ -25,6 +25,7 @@\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.GROUP6;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.GROUP8;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.MAX_DATA;\n+import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.addCompactionIterators;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.assertNoCompactionMetadata;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.compact;\n import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.createTable;\n@@ -58,12 +59,14 @@\n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.client.AccumuloClient;\n import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.client.BatchScanner;\n import org.apache.accumulo.core.client.BatchWriter;\n import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n import org.apache.accumulo.core.client.admin.PluginConfig;\n+import org.apache.accumulo.core.client.admin.TabletInformation;\n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n import org.apache.accumulo.core.client.admin.compaction.CompactionSelector;\n import org.apache.accumulo.core.client.admin.compaction.CompressionConfigurer;\n@@ -72,6 +75,7 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.data.TabletId;\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.core.fate.Fate;\n import org.apache.accumulo.core.fate.FateId;\n@@ -514,6 +518,68 @@ public void testManytablets() throws Exception {\n       compact(client, table1, 3, GROUP4, true);\n \n       verify(client, table1, 3);\n+\n+      List<TabletId> tabletIds;\n+      // start a compaction on each tablet\n+      try (var tablets = client.tableOperations().getTabletInformation(table1, new Range())) {\n+        tabletIds = tablets.map(TabletInformation::getTabletId).collect(Collectors.toList());\n+      }\n+      // compact the even tablet with a modulus filter of 2\n+      List<Range> evenRanges = new ArrayList<>();\n+      for (int i = 0; i < tabletIds.size(); i += 2) {\n+        var tabletId = tabletIds.get(i);\n+        CompactionConfig compactionConfig = new CompactionConfig()\n+            .setStartRow(tabletId.getPrevEndRow()).setEndRow(tabletId.getEndRow()).setWait(false);\n+        addCompactionIterators(compactionConfig, 2, GROUP4);\n+        client.tableOperations().compact(table1, compactionConfig);\n+        evenRanges.add(tabletId.toRange());\n+      }\n+\n+      // compact the odd tablets with a modulus filter of 5\n+      List<Range> oddRanges = new ArrayList<>();\n+      for (int i = 1; i < tabletIds.size(); i += 2) {\n+        var tabletId = tabletIds.get(i);\n+        CompactionConfig compactionConfig = new CompactionConfig()\n+            .setStartRow(tabletId.getPrevEndRow()).setEndRow(tabletId.getEndRow()).setWait(false);\n+        addCompactionIterators(compactionConfig, 5, GROUP4);\n+        client.tableOperations().compact(table1, compactionConfig);\n+        oddRanges.add(tabletId.toRange());\n+      }\n+\n+      Wait.waitFor(() -> {\n+        try (BatchScanner scanner = client.createBatchScanner(table1)) {\n+          scanner.setRanges(evenRanges);\n+          // filtered out data that was divisible by 3 and then 2 by compactions, so should end up\n+          // w/ only data divisible by 6\n+          int matching = 0;\n+          int nonMatching = 0;\n+          for (var entry : scanner) {\n+            int val = Integer.parseInt(entry.getValue().toString());\n+            if (val % 6 == 0) {\n+              matching++;\n+            } else {\n+              nonMatching++;\n+            }\n+          }\n+          boolean evenDone = matching > 0 && nonMatching == 0;\n+          // filtered out data that was divisible by 3 and then 5 by compactions, so should end up\n+          // w/ only data divisible by 15\n+          scanner.setRanges(oddRanges);\n+          matching = 0;\n+          nonMatching = 0;\n+          for (var entry : scanner) {\n+            int val = Integer.parseInt(entry.getValue().toString());\n+            if (val % 15 == 0) {\n+              matching++;\n+            } else {\n+              nonMatching++;\n+            }\n+          }\n+          boolean oddDone = matching > 0 && nonMatching == 0;\n+          return evenDone && oddDone;\n+        }\n+      });\n+\n     }\n   }\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5252"", ""pr_id"": 5252, ""issue_id"": 5186, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Modify compaction job queue size limit to be memory based\n**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently the compaction job configuration is based on a range of entries.  An individual entry in the queue can vary in size based on the number files in the tablet and the number of files in the compaction job.  So it is hard to reason about entires.  The goal of limiting the size is to limit memory usage. \r\n\r\n**Describe the solution you'd like**\r\n\r\nHave a single configuration that is a memory upper limit for compaction job queues.  For example the configuration would allow the queue to use up to 50M of memory.  This would be much easier to understand and would work much better at limiting memory used by the queue.  The current configuration based on a range of entries sizes (like the queue can range from 10 to 10000) entries does not control memory usage in a predictable way.\r\n"", ""issue_word_count"": 161, ""test_files_count"": 4, ""non_test_files_count"": 6, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMap.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMapTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java""], ""pr_changed_test_files"": [""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMapTest.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java""], ""base_commit"": ""cd441767c983b556485a325d8e4eb58f91d7980f"", ""head_commit"": ""940c3d9dd5b62813c7318abb17815b88d6841d17"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5252"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5252"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-16T22:59:10.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex c5bcdf4d452..555785f5806 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -465,14 +465,12 @@ public enum Property {\n       \""The number of threads used to seed fate split task, the actual split work is done by fate\""\n           + \"" threads.\"",\n       \""4.0.0\""),\n-\n-  MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE(\n-      \""manager.compaction.major.service.queue.initial.size\"", \""10000\"", PropertyType.COUNT,\n-      \""The initial size of each resource groups compaction job priority queue.\"", \""4.0.0\""),\n-  MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE_FACTOR(\n-      \""manager.compaction.major.service.queue.size.factor\"", \""3.0\"", PropertyType.FRACTION,\n-      \""The dynamic resizing of the compaction job priority queue is based on\""\n-          + \"" the number of compactors for the group multiplied by this factor.\"",\n+  MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE(\""manager.compaction.major.service.queue.size\"",\n+      \""1M\"", PropertyType.MEMORY,\n+      \""The data size of each resource groups compaction job priority queue.  The memory size of \""\n+          + \""each compaction job is estimated and the sum of these sizes per resource group will not \""\n+          + \""exceed this setting. When the size is exceeded the lowest priority jobs are dropped as \""\n+          + \""needed.\"",\n       \""4.0.0\""),\n   SPLIT_PREFIX(\""split.\"", null, PropertyType.PREFIX,\n       \""System wide properties related to splitting tablets.\"", \""3.1.0\""),\n@@ -1460,7 +1458,7 @@ public static boolean isValidTablePropertyKey(String key) {\n       RPC_MAX_MESSAGE_SIZE,\n \n       // compaction coordiantor properties\n-      MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE,\n+      MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE,\n \n       // block cache options\n       GENERAL_CACHE_MANAGER_IMPL, TSERV_DATACACHE_SIZE, TSERV_INDEXCACHE_SIZE,\n\ndiff --git a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java\nindex 5b81fac4684..b5c6667519f 100644\n--- a/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java\n+++ b/minicluster/src/main/java/org/apache/accumulo/miniclusterImpl/MiniAccumuloConfigImpl.java\n@@ -188,8 +188,8 @@ MiniAccumuloConfigImpl initialize() {\n \n       mergeProp(Property.COMPACTOR_PORTSEARCH.getKey(), \""true\"");\n \n-      mergeProp(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE.getKey(),\n-          Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE.getDefaultValue());\n+      mergeProp(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE.getKey(),\n+          Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE.getDefaultValue());\n       mergeProp(Property.COMPACTION_SERVICE_DEFAULT_PLANNER.getKey(),\n           Property.COMPACTION_SERVICE_DEFAULT_PLANNER.getDefaultValue());\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\nindex 4dc42c11379..f1e6f54a5cd 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\n@@ -194,7 +194,7 @@ public class CompactionCoordinator\n   private final Manager manager;\n \n   private final LoadingCache<String,Integer> compactorCounts;\n-  private final int jobQueueInitialSize;\n+  private final long jobQueueInitialSize;\n \n   private volatile long coordinatorStartTime;\n \n@@ -208,8 +208,8 @@ public CompactionCoordinator(ServerContext ctx, SecurityOperation security,\n     this.security = security;\n     this.manager = Objects.requireNonNull(manager);\n \n-    this.jobQueueInitialSize = ctx.getConfiguration()\n-        .getCount(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE);\n+    this.jobQueueInitialSize =\n+        ctx.getConfiguration().getAsBytes(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE);\n \n     this.jobQueues = new CompactionJobQueues(jobQueueInitialSize);\n \n@@ -1121,8 +1121,6 @@ private void cleanUpEmptyCompactorPathInZK() {\n     final String compactorQueuesPath = this.ctx.getZooKeeperRoot() + Constants.ZCOMPACTORS;\n \n     final var zoorw = this.ctx.getZooSession().asReaderWriter();\n-    final double queueSizeFactor = ctx.getConfiguration()\n-        .getFraction(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE_FACTOR);\n \n     try {\n       var groups = zoorw.getChildren(compactorQueuesPath);\n@@ -1139,7 +1137,6 @@ private void cleanUpEmptyCompactorPathInZK() {\n           CompactionJobPriorityQueue queue = getJobQueues().getQueue(cgid);\n           if (queue != null) {\n             queue.clearIfInactive(Duration.ofMinutes(10));\n-            queue.setMaxSize(this.jobQueueInitialSize);\n           }\n         } else {\n           int aliveCompactorsForGroup = 0;\n@@ -1152,16 +1149,8 @@ private void cleanUpEmptyCompactorPathInZK() {\n               aliveCompactorsForGroup++;\n             }\n           }\n-          CompactionJobPriorityQueue queue = getJobQueues().getQueue(cgid);\n-          if (queue != null) {\n-            queue.setMaxSize(Math.min(\n-                Math.max(1, (int) (aliveCompactorsForGroup * queueSizeFactor)), Integer.MAX_VALUE));\n-          }\n-\n         }\n-\n       }\n-\n     } catch (KeeperException | RuntimeException e) {\n       LOG.warn(\""Failed to clean up compactors\"", e);\n     } catch (InterruptedException e) {\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\nindex 1f9738dac78..f183b50b86f 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueue.java\n@@ -31,11 +31,9 @@\n import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n-import java.util.TreeMap;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.atomic.AtomicReference;\n \n@@ -116,8 +114,8 @@ public boolean equals(Object o) {\n   // behavior is not supported with a PriorityQueue. Second a PriorityQueue does not support\n   // efficiently removing entries from anywhere in the queue. Efficient removal is needed for the\n   // case where tablets decided to issues different compaction jobs than what is currently queued.\n-  private final TreeMap<CjpqKey,CompactionJobQueues.MetaJob> jobQueue;\n-  private final AtomicInteger maxSize;\n+  private final SizeTrackingTreeMap<CjpqKey,CompactionJobQueues.MetaJob> jobQueue;\n+  private final AtomicLong maxSize;\n   private final AtomicLong rejectedJobs;\n   private final AtomicLong dequeuedJobs;\n   private final ArrayDeque<CompletableFuture<CompactionJobQueues.MetaJob>> futures;\n@@ -142,9 +140,10 @@ private TabletJobs(long generation, HashSet<CjpqKey> jobs) {\n \n   private final AtomicLong nextSeq = new AtomicLong(0);\n \n-  public CompactionJobPriorityQueue(CompactorGroupId groupId, int maxSize) {\n-    this.jobQueue = new TreeMap<>();\n-    this.maxSize = new AtomicInteger(maxSize);\n+  public CompactionJobPriorityQueue(CompactorGroupId groupId, long maxSize,\n+      SizeTrackingTreeMap.Weigher<CompactionJobQueues.MetaJob> weigher) {\n+    this.jobQueue = new SizeTrackingTreeMap<>(weigher);\n+    this.maxSize = new AtomicLong(maxSize);\n     this.tabletJobs = new HashMap<>();\n     this.groupId = groupId;\n     this.rejectedJobs = new AtomicLong(0);\n@@ -230,11 +229,11 @@ public synchronized int add(TabletMetadata tabletMetadata, Collection<Compaction\n     return jobsAdded;\n   }\n \n-  public synchronized int getMaxSize() {\n+  public synchronized long getMaxSize() {\n     return maxSize.get();\n   }\n \n-  public synchronized void setMaxSize(int maxSize) {\n+  public synchronized void setMaxSize(long maxSize) {\n     Preconditions.checkArgument(maxSize > 0,\n         \""Maximum size of the Compaction job priority queue must be greater than 0\"");\n     this.maxSize.set(maxSize);\n@@ -249,7 +248,7 @@ public long getDequeuedJobs() {\n   }\n \n   public synchronized long getQueuedJobs() {\n-    return jobQueue.size();\n+    return jobQueue.entrySize();\n   }\n \n   public synchronized long getLowestPriority() {\n@@ -332,7 +331,7 @@ private void removePreviousSubmissions(KeyExtent extent, boolean removeJobAges)\n   }\n \n   private CjpqKey addJobToQueue(TabletMetadata tabletMetadata, CompactionJob job) {\n-    if (jobQueue.size() >= maxSize.get()) {\n+    if (jobQueue.dataSize() >= maxSize.get()) {\n       var lastEntry = jobQueue.lastKey();\n       if (job.getPriority() <= lastEntry.job.getPriority()) {\n         // the queue is full and this job has a lower or same priority than the lowest job in the\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java\nindex b9fe1ed424c..2e2dc3cef95 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueues.java\n@@ -48,18 +48,20 @@ public class CompactionJobQueues {\n   private final ConcurrentHashMap<CompactorGroupId,CompactionJobPriorityQueue> priorityQueues =\n       new ConcurrentHashMap<>();\n \n-  private final int queueSize;\n+  private final long queueSize;\n \n   private final Map<DataLevel,AtomicLong> currentGenerations;\n \n-  public CompactionJobQueues(int queueSize) {\n+  private SizeTrackingTreeMap.Weigher<MetaJob> weigher =\n+      val -> val.getTabletMetadata().toString().length() + val.getJob().toString().length();\n+\n+  public CompactionJobQueues(long queueSize) {\n     this.queueSize = queueSize;\n     Map<DataLevel,AtomicLong> cg = new EnumMap<>(DataLevel.class);\n     for (var level : DataLevel.values()) {\n       cg.put(level, new AtomicLong());\n     }\n     currentGenerations = Collections.unmodifiableMap(cg);\n-\n   }\n \n   public void beginFullScan(DataLevel level) {\n@@ -164,7 +166,7 @@ public TabletMetadata getTabletMetadata() {\n    */\n   public CompletableFuture<MetaJob> getAsync(CompactorGroupId groupId) {\n     var pq = priorityQueues.computeIfAbsent(groupId,\n-        gid -> new CompactionJobPriorityQueue(gid, queueSize));\n+        gid -> new CompactionJobPriorityQueue(gid, queueSize, weigher));\n     return pq.getAsync();\n   }\n \n@@ -187,7 +189,7 @@ private void add(TabletMetadata tabletMetadata, CompactorGroupId groupId,\n     }\n \n     var pq = priorityQueues.computeIfAbsent(groupId,\n-        gid -> new CompactionJobPriorityQueue(gid, queueSize));\n+        gid -> new CompactionJobPriorityQueue(gid, queueSize, weigher));\n     pq.add(tabletMetadata, jobs,\n         currentGenerations.get(DataLevel.of(tabletMetadata.getTableId())).get());\n   }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMap.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMap.java\nnew file mode 100644\nindex 00000000000..306a56fb647\n--- /dev/null\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMap.java\n@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.manager.compaction.queue;\n+\n+import java.util.AbstractMap;\n+import java.util.Map;\n+import java.util.TreeMap;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * This class wraps a treemap and tracks the data size of everything added and removed from the\n+ * treemap.\n+ */\n+class SizeTrackingTreeMap<K,V> {\n+\n+  private static class ValueWrapper<V2> {\n+    final V2 val;\n+    final long computedSize;\n+\n+    private ValueWrapper(V2 val, long computedSize) {\n+      this.val = val;\n+      this.computedSize = computedSize;\n+    }\n+  }\n+\n+  private final TreeMap<K,ValueWrapper<V>> map = new TreeMap<>();\n+  private long dataSize = 0;\n+  private Weigher<V> weigher;\n+\n+  private Map.Entry<K,V> unwrap(Map.Entry<K,ValueWrapper<V>> wrapperEntry) {\n+    if (wrapperEntry == null) {\n+      return null;\n+    }\n+    return new AbstractMap.SimpleImmutableEntry<>(wrapperEntry.getKey(),\n+        wrapperEntry.getValue().val);\n+  }\n+\n+  private void incrementDataSize(ValueWrapper<V> val) {\n+    Preconditions.checkState(dataSize >= 0);\n+    dataSize += val.computedSize;\n+  }\n+\n+  private void decrementDataSize(Map.Entry<K,ValueWrapper<V>> entry) {\n+    if (entry != null) {\n+      decrementDataSize(entry.getValue());\n+    }\n+  }\n+\n+  private void decrementDataSize(ValueWrapper<V> val) {\n+    if (val != null) {\n+      Preconditions.checkState(dataSize >= val.computedSize);\n+      dataSize -= val.computedSize;\n+    }\n+  }\n+\n+  interface Weigher<V2> {\n+    long weigh(V2 val);\n+  }\n+\n+  public SizeTrackingTreeMap(Weigher<V> weigher) {\n+    this.weigher = weigher;\n+  }\n+\n+  public boolean isEmpty() {\n+    return map.isEmpty();\n+  }\n+\n+  public long dataSize() {\n+    return dataSize;\n+  }\n+\n+  public int entrySize() {\n+    return map.size();\n+  }\n+\n+  public K lastKey() {\n+    return map.lastKey();\n+  }\n+\n+  public Map.Entry<K,V> firstEntry() {\n+    return unwrap(map.firstEntry());\n+  }\n+\n+  public void remove(K key) {\n+    var prev = map.remove(key);\n+    decrementDataSize(prev);\n+  }\n+\n+  public Map.Entry<K,V> pollFirstEntry() {\n+    var first = map.pollFirstEntry();\n+    decrementDataSize(first);\n+    return unwrap(first);\n+  }\n+\n+  public Map.Entry<K,V> pollLastEntry() {\n+    var last = map.pollLastEntry();\n+    decrementDataSize(last);\n+    return unwrap(last);\n+  }\n+\n+  public void put(K key, V val) {\n+    var wrapped = new ValueWrapper<>(val, weigher.weigh(val));\n+    var prev = map.put(key, wrapped);\n+    decrementDataSize(prev);\n+    incrementDataSize(wrapped);\n+  }\n+\n+  public void clear() {\n+    map.clear();\n+    dataSize = 0;\n+  }\n+}\n"", ""test_patch"": ""diff --git a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\nindex 37213cdc488..01c18798754 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobPriorityQueueTest.java\n@@ -74,7 +74,7 @@ public void testTabletFileReplacement() {\n \n     EasyMock.replay(tm, cj1, cj2);\n \n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2, mj -> 1);\n     assertEquals(1, queue.add(tm, List.of(cj1), 1L));\n \n     MetaJob job = queue.peek();\n@@ -129,7 +129,7 @@ public void testAddEqualToMaxSize() {\n \n     EasyMock.replay(tm, cj1, cj2);\n \n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2, mj -> 1);\n     assertEquals(2, queue.add(tm, List.of(cj1, cj2), 1L));\n \n     EasyMock.verify(tm, cj1, cj2);\n@@ -186,7 +186,7 @@ public void testAddMoreThanMax() {\n \n     EasyMock.replay(tm, cj1, cj2, cj3);\n \n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 2, mj -> 1);\n     assertEquals(2, queue.add(tm, List.of(cj1, cj2, cj3), 1L));\n \n     EasyMock.verify(tm, cj1, cj2, cj3);\n@@ -247,7 +247,7 @@ public void test() {\n \n     TreeSet<CompactionJob> expected = new TreeSet<>(CompactionJobPrioritizer.JOB_COMPARATOR);\n \n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 1000, mj -> 10);\n \n     // create and add 1000 jobs\n     for (int x = 0; x < 1000; x++) {\n@@ -256,7 +256,7 @@ public void test() {\n       expected.add(pair.getSecond());\n     }\n \n-    assertEquals(100, queue.getMaxSize());\n+    assertEquals(1000, queue.getMaxSize());\n     assertEquals(100, queue.getQueuedJobs());\n     assertEquals(900, queue.getRejectedJobs());\n     // There should be 1000 total job ages even though 900 were rejected\n@@ -268,7 +268,7 @@ public void test() {\n     assertTrue(stats.getMaxAge().toMillis() > 0);\n     assertTrue(stats.getAvgAge().toMillis() > 0);\n \n-    // iterate over the expected set and make sure that they next job in the queue\n+    // iterate over the expected set and make sure that the next job in the queue\n     // matches\n     int matchesSeen = 0;\n     for (CompactionJob expectedJob : expected) {\n@@ -312,7 +312,7 @@ public void test() {\n    */\n   @Test\n   public void testAsyncCancelCleanup() {\n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100, mj -> 1);\n \n     List<CompletableFuture<MetaJob>> futures = new ArrayList<>();\n \n@@ -342,7 +342,7 @@ public void testAsyncCancelCleanup() {\n \n   @Test\n   public void testChangeMaxSize() {\n-    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100);\n+    CompactionJobPriorityQueue queue = new CompactionJobPriorityQueue(GROUP, 100, mj -> 1);\n     assertEquals(100, queue.getMaxSize());\n     queue.setMaxSize(50);\n     assertEquals(50, queue.getMaxSize());\n@@ -351,5 +351,4 @@ public void testChangeMaxSize() {\n     // Make sure previous value was not changed after invalid setting\n     assertEquals(50, queue.getMaxSize());\n   }\n-\n }\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java\nindex 3d8933fa386..f63e56cc490 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/CompactionJobQueuesTest.java\n@@ -81,7 +81,7 @@ public void testFullScanHandling() throws Exception {\n     var cg2 = CompactorGroupId.of(\""CG2\"");\n     var cg3 = CompactorGroupId.of(\""CG3\"");\n \n-    CompactionJobQueues jobQueues = new CompactionJobQueues(100);\n+    CompactionJobQueues jobQueues = new CompactionJobQueues(1000000);\n \n     jobQueues.beginFullScan(DataLevel.USER);\n \n@@ -247,7 +247,7 @@ public void testFullScanLevels() throws Exception {\n \n     var cg1 = CompactorGroupId.of(\""CG1\"");\n \n-    CompactionJobQueues jobQueues = new CompactionJobQueues(100);\n+    CompactionJobQueues jobQueues = new CompactionJobQueues(1000000);\n \n     jobQueues.add(tm1, List.of(newJob((short) 1, 5, cg1)));\n     jobQueues.add(tm2, List.of(newJob((short) 2, 6, cg1)));\n@@ -283,7 +283,7 @@ public void testAddPollRaceCondition() throws Exception {\n \n     final int numToAdd = 100_000;\n \n-    CompactionJobQueues jobQueues = new CompactionJobQueues(numToAdd + 1);\n+    CompactionJobQueues jobQueues = new CompactionJobQueues(10000000);\n     CompactorGroupId[] groups =\n         Stream.of(\""G1\"", \""G2\"", \""G3\"").map(CompactorGroupId::of).toArray(CompactorGroupId[]::new);\n \n@@ -342,7 +342,7 @@ public void testAddPollRaceCondition() throws Exception {\n \n   @Test\n   public void testGetAsync() throws Exception {\n-    CompactionJobQueues jobQueues = new CompactionJobQueues(100);\n+    CompactionJobQueues jobQueues = new CompactionJobQueues(1000000);\n \n     var tid = TableId.of(\""1\"");\n     var extent1 = new KeyExtent(tid, new Text(\""z\""), new Text(\""q\""));\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMapTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMapTest.java\nnew file mode 100644\nindex 00000000000..384363228f0\n--- /dev/null\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/compaction/queue/SizeTrackingTreeMapTest.java\n@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.manager.compaction.queue;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.TreeMap;\n+\n+import org.junit.jupiter.api.Test;\n+\n+public class SizeTrackingTreeMapTest {\n+  @Test\n+  public void testSizeTracking() {\n+    List<String> computeSizeCalls = new ArrayList<>();\n+    var stmap = new SizeTrackingTreeMap<Integer,String>(val -> {\n+      computeSizeCalls.add(val);\n+      return val.length();\n+    });\n+\n+    TreeMap<Integer,String> expected = new TreeMap<>();\n+\n+    check(expected, stmap);\n+    assertEquals(List.of(), computeSizeCalls);\n+\n+    stmap.put(3, \""1234567890\"");\n+    expected.put(3, \""1234567890\"");\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\""), computeSizeCalls);\n+\n+    stmap.put(4, \""12345\"");\n+    expected.put(4, \""12345\"");\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\""), computeSizeCalls);\n+\n+    // remove a key that does not exist\n+    stmap.remove(2);\n+    expected.remove(2);\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\""), computeSizeCalls);\n+\n+    // remove a key that does exist\n+    stmap.remove(3);\n+    expected.remove(3);\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\""), computeSizeCalls);\n+\n+    // update an existing key, should decrement the old size and increment the new size\n+    stmap.put(4, \""123\"");\n+    expected.put(4, \""123\"");\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\""), computeSizeCalls);\n+\n+    stmap.put(7, \""123456789012345\"");\n+    expected.put(7, \""123456789012345\"");\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\"", \""123456789012345\""), computeSizeCalls);\n+\n+    stmap.put(11, \""1234567\"");\n+    expected.put(11, \""1234567\"");\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\"", \""123456789012345\"", \""1234567\""),\n+        computeSizeCalls);\n+\n+    assertEquals(expected.pollFirstEntry(), stmap.pollFirstEntry());\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\"", \""123456789012345\"", \""1234567\""),\n+        computeSizeCalls);\n+\n+    assertEquals(expected.pollLastEntry(), stmap.pollLastEntry());\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\"", \""123456789012345\"", \""1234567\""),\n+        computeSizeCalls);\n+\n+    expected.clear();\n+    stmap.clear();\n+    check(expected, stmap);\n+    assertEquals(List.of(\""1234567890\"", \""12345\"", \""123\"", \""123456789012345\"", \""1234567\""),\n+        computeSizeCalls);\n+  }\n+\n+  private void check(TreeMap<Integer,String> expected, SizeTrackingTreeMap<Integer,String> stmap) {\n+    long expectedDataSize = expected.values().stream().mapToLong(String::length).sum();\n+    assertEquals(expectedDataSize, stmap.dataSize());\n+    assertEquals(expected.size(), stmap.entrySize());\n+    assertEquals(expected.isEmpty(), stmap.isEmpty());\n+    assertEquals(expected.firstEntry(), stmap.firstEntry());\n+    if (expected.isEmpty()) {\n+      assertThrows(NoSuchElementException.class, stmap::lastKey);\n+    } else {\n+      assertEquals(expected.lastKey(), stmap.lastKey());\n+    }\n+  }\n+}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java b/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\nindex bdee45f9353..2a032565953 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/CompactionPriorityQueueMetricsIT.java\n@@ -109,7 +109,7 @@ public class CompactionPriorityQueueMetricsIT extends SharedMiniClusterBase {\n   public static final String QUEUE1 = \""METRICSQ1\"";\n   public static final String QUEUE1_METRIC_LABEL = MetricsUtil.formatString(QUEUE1);\n   public static final String QUEUE1_SERVICE = \""Q1\"";\n-  public static final int QUEUE1_SIZE = 6;\n+  public static final int QUEUE1_SIZE = 10 * 1024;\n \n   // Metrics collector Thread\n   final LinkedBlockingQueue<TestStatsDSink.Metric> queueMetrics = new LinkedBlockingQueue<>();\n@@ -202,7 +202,7 @@ public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration conf)\n           Property.COMPACTION_SERVICE_PREFIX.getKey() + QUEUE1_SERVICE + \"".planner.opts.groups\"",\n           \""[{'group':'\"" + QUEUE1 + \""'}]\"");\n \n-      cfg.setProperty(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_INITIAL_SIZE, \""6\"");\n+      cfg.setProperty(Property.MANAGER_COMPACTION_SERVICE_PRIORITY_QUEUE_SIZE, \""10K\"");\n       cfg.getClusterServerConfiguration().addCompactorResourceGroup(QUEUE1, 0);\n \n       // This test waits for dead compactors to be absent in zookeeper. The following setting will\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5231"", ""pr_id"": 5231, ""issue_id"": 4663, ""repo"": ""apache/accumulo"", ""problem_statement"": ""scan servers use tserver prefix for cache manager property\n**Describe the bug**\r\nWhen trying to set the class to use for the cache manager, the scan servers do not use a \r\n`sserver` property prefix. Instead the scan servers only load in a different manager if `tserver.cache.manager.class` is set. \r\n\r\n**Versions (OS, Maven, Java, and others, as appropriate):**\r\n - Affected version(s) of this project: [e.g. 1.10.0] 2.1.x\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior (or a link to an example repository that reproduces the problem):\r\n1. Setup an fluo-uno instance\r\n2. Modify the properties to set `sserver.cache.manager.class=test.example` in the property file.\r\n3. Try starting a scan server (This should succeed)\r\n4. Modify the properties file to set `tserver.cache.manager.class=test.example`\r\n5. Try starting a scan server (This will fail with a class not found error)\r\n\r\n**Expected behavior**\r\nThe scan server should use the `sserver` prefix when loading in instance properties.\r\n\r\n**Additional context**\r\nThere may be additional properties that are still only loaded in via the `tserver` prefix. \r\n"", ""issue_word_count"": 182, ""test_files_count"": 3, ""non_test_files_count"": 2, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/impl/BlockCacheManagerFactory.java"", ""core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java"", ""core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java"", ""core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java"", ""core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java"", ""core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java""], ""base_commit"": ""ae8500331a5b7e751ea5e17c0db731d0ecfec79d"", ""head_commit"": ""29c0a9102d6edd8d6956015a6b269634b7fadea9"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5231"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5231"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-08T14:35:35.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex b1a658ea2f6..981799f6d0d 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -294,6 +294,12 @@ public enum Property {\n           + \"" user-implementations of pluggable Accumulo features, such as the balancer\""\n           + \"" or volume chooser.\"",\n       \""2.0.0\""),\n+  GENERAL_CACHE_MANAGER_IMPL(\""general.block.cache.manager.class\"",\n+      \""org.apache.accumulo.core.file.blockfile.cache.lru.LruBlockCacheManager\"", PropertyType.STRING,\n+      \""Specifies the class name of the block cache factory implementation.\""\n+          + \"" Alternative implementation is\""\n+          + \"" org.apache.accumulo.core.file.blockfile.cache.tinylfu.TinyLfuBlockCacheManager.\"",\n+      \""2.1.4\""),\n   GENERAL_DELEGATION_TOKEN_LIFETIME(\""general.delegation.token.lifetime\"", \""7d\"",\n       PropertyType.TIMEDURATION,\n       \""The length of time that delegation tokens and secret keys are valid.\"", \""1.7.0\""),\n@@ -532,7 +538,9 @@ public enum Property {\n       \""Time to wait for clients to continue scans before closing a session.\"", \""1.3.5\""),\n   TSERV_DEFAULT_BLOCKSIZE(\""tserver.default.blocksize\"", \""1M\"", PropertyType.BYTES,\n       \""Specifies a default blocksize for the tserver caches.\"", \""1.3.5\""),\n-  TSERV_CACHE_MANAGER_IMPL(\""tserver.cache.manager.class\"",\n+  @Deprecated(since = \""2.1.4\"")\n+  @ReplacedBy(property = Property.GENERAL_CACHE_MANAGER_IMPL)\n+  TSERV_CACHE_MANAGER_IMPL(\""general.cache.manager.class\"",\n       \""org.apache.accumulo.core.file.blockfile.cache.lru.LruBlockCacheManager\"", PropertyType.STRING,\n       \""Specifies the class name of the block cache factory implementation.\""\n           + \"" Alternative implementation is\""\n@@ -1873,8 +1881,9 @@ public static boolean isValidTablePropertyKey(String key) {\n       TSERV_MAX_MESSAGE_SIZE, GENERAL_MAX_MESSAGE_SIZE, RPC_MAX_MESSAGE_SIZE,\n \n       // block cache options\n-      TSERV_CACHE_MANAGER_IMPL, TSERV_DATACACHE_SIZE, TSERV_INDEXCACHE_SIZE,\n-      TSERV_SUMMARYCACHE_SIZE, SSERV_DATACACHE_SIZE, SSERV_INDEXCACHE_SIZE, SSERV_SUMMARYCACHE_SIZE,\n+      GENERAL_CACHE_MANAGER_IMPL, TSERV_CACHE_MANAGER_IMPL, TSERV_DATACACHE_SIZE,\n+      TSERV_INDEXCACHE_SIZE, TSERV_SUMMARYCACHE_SIZE, SSERV_DATACACHE_SIZE, SSERV_INDEXCACHE_SIZE,\n+      SSERV_SUMMARYCACHE_SIZE,\n \n       // blocksize options\n       TSERV_DEFAULT_BLOCKSIZE, SSERV_DEFAULT_BLOCKSIZE,\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/impl/BlockCacheManagerFactory.java b/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/impl/BlockCacheManagerFactory.java\nindex b72979d7297..d62a5e8d468 100644\n--- a/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/impl/BlockCacheManagerFactory.java\n+++ b/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/impl/BlockCacheManagerFactory.java\n@@ -39,7 +39,10 @@ public class BlockCacheManagerFactory {\n    */\n   public static synchronized BlockCacheManager getInstance(AccumuloConfiguration conf)\n       throws Exception {\n-    String impl = conf.get(Property.TSERV_CACHE_MANAGER_IMPL);\n+    @SuppressWarnings(\""deprecation\"")\n+    var cacheManagerProp =\n+        conf.resolve(Property.GENERAL_CACHE_MANAGER_IMPL, Property.TSERV_CACHE_MANAGER_IMPL);\n+    String impl = conf.get(cacheManagerProp);\n     Class<? extends BlockCacheManager> clazz =\n         ClassLoaderUtil.loadClass(impl, BlockCacheManager.class);\n     LOG.info(\""Created new block cache manager of type: {}\"", clazz.getSimpleName());\n@@ -55,7 +58,10 @@ public static synchronized BlockCacheManager getInstance(AccumuloConfiguration c\n    */\n   public static synchronized BlockCacheManager getClientInstance(AccumuloConfiguration conf)\n       throws Exception {\n-    String impl = conf.get(Property.TSERV_CACHE_MANAGER_IMPL);\n+    @SuppressWarnings(\""deprecation\"")\n+    var cacheManagerProp =\n+        conf.resolve(Property.GENERAL_CACHE_MANAGER_IMPL, Property.TSERV_CACHE_MANAGER_IMPL);\n+    String impl = conf.get(cacheManagerProp);\n     Class<? extends BlockCacheManager> clazz =\n         Class.forName(impl).asSubclass(BlockCacheManager.class);\n     LOG.info(\""Created new block cache factory of type: {}\"", clazz.getSimpleName());\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java b/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java\nindex 1cbfbd289fe..f6c3266603c 100644\n--- a/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/BlockCacheFactoryTest.java\n@@ -37,7 +37,7 @@ public class BlockCacheFactoryTest {\n   public void testCreateLruBlockCacheFactory() throws Exception {\n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManagerFactory.getInstance(cc);\n   }\n \n@@ -45,7 +45,7 @@ public void testCreateLruBlockCacheFactory() throws Exception {\n   public void testCreateTinyLfuBlockCacheFactory() throws Exception {\n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, TinyLfuBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, TinyLfuBlockCacheManager.class.getName());\n     BlockCacheManagerFactory.getInstance(cc);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java b/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java\nindex b6b0ea39620..e09bad27a84 100644\n--- a/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java\n+++ b/core/src/test/java/org/apache/accumulo/core/file/blockfile/cache/TestLruBlockCache.java\n@@ -57,7 +57,7 @@ public class TestLruBlockCache {\n   @Test\n   public void testConfiguration() {\n     ConfigurationCopy cc = new ConfigurationCopy();\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(1019));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(1000023));\n     cc.set(Property.TSERV_DATACACHE_SIZE, Long.toString(1000027));\n@@ -99,7 +99,7 @@ public void testBackgroundEvictionThread() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n@@ -133,7 +133,7 @@ public void testCacheSimple() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n@@ -191,7 +191,7 @@ public void testCacheEvictionSimple() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n@@ -241,7 +241,7 @@ public void testCacheEvictionTwoPriorities() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n@@ -309,7 +309,7 @@ public void testCacheEvictionThreePriorities() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n@@ -434,7 +434,7 @@ public void testScanResistance() throws Exception {\n \n     DefaultConfiguration dc = DefaultConfiguration.getInstance();\n     ConfigurationCopy cc = new ConfigurationCopy(dc);\n-    cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     BlockCacheManager manager = BlockCacheManagerFactory.getInstance(cc);\n     cc.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(blockSize));\n     cc.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(maxSize));\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java b/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\nindex 082e0d6ea7b..33545639efd 100644\n--- a/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java\n@@ -308,7 +308,7 @@ public void openReader(boolean cfsi) throws IOException {\n \n       DefaultConfiguration dc = DefaultConfiguration.getInstance();\n       ConfigurationCopy cc = new ConfigurationCopy(dc);\n-      cc.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+      cc.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n       try {\n         manager = BlockCacheManagerFactory.getInstance(cc);\n       } catch (Exception e) {\n@@ -1726,7 +1726,7 @@ private void runVersionTest(int version, ConfigurationCopy aconf) throws Excepti\n     byte[] data = baos.toByteArray();\n     SeekableByteArrayInputStream bais = new SeekableByteArrayInputStream(data);\n     FSDataInputStream in2 = new FSDataInputStream(bais);\n-    aconf.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    aconf.set(Property.GENERAL_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n     aconf.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(100000));\n     aconf.set(Property.TSERV_DATACACHE_SIZE, Long.toString(100000000));\n     aconf.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(100000000));\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5220"", ""pr_id"": 5220, ""issue_id"": 4755, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Help flag for admin command does not work for subcommands\n**Describe the bug**\r\nWhen using the `./accumulo admin` command, help flags `-h, --help, -help, -?` will only work at the top level, and will throw an error if set on any subcommand. \r\n\r\n```\r\n./accumulo admin volumes -h\r\nThread 'admin' died.\r\ncom.beust.jcommander.ParameterException: Was passed main parameter '-h' but no main parameter was defined in your arg class\r\n\tat com.beust.jcommander.JCommander.initMainParameterValue(JCommander.java:961)\r\n\tat com.beust.jcommander.JCommander.parseValues(JCommander.java:762)\r\n\tat com.beust.jcommander.JCommander.parse(JCommander.java:363)\r\n\tat com.beust.jcommander.JCommander.parseValues(JCommander.java:803)\r\n\tat com.beust.jcommander.JCommander.parse(JCommander.java:363)\r\n\tat com.beust.jcommander.JCommander.parse(JCommander.java:342)\r\n\tat org.apache.accumulo.server.util.Admin.execute(Admin.java:342)\r\n\tat org.apache.accumulo.start.Main.lambda$execKeyword$0(Main.java:81)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n```\r\n\r\nDue to the amount of subcommands, the top-level usage statement is large and typically scrolls off a terminal window. \r\n\r\n**Versions (OS, Maven, Java, and others, as appropriate):**\r\n - Affected version(s) of this project: [e.g. 1.10.0] 2.1.x\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior (or a link to an example repository that reproduces the problem):\r\n1. Run the accumulo admin command with any subcommand and add any of the help flags `./accumulo admin dumpConfig --help`\r\n2. See a Parameter Exception stack trace\r\n\r\n**Expected behavior**\r\nThe help flag should work with any subcommand used in the admin command"", ""issue_word_count"": 256, ""test_files_count"": 3, ""non_test_files_count"": 2, ""pr_changed_files"": [""core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/Admin.java"", ""server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java""], ""base_commit"": ""25dcfbfe14d9b71e8c9b2e4b079287bbcd6ae476"", ""head_commit"": ""9c842aefe113910ac253ab791abee13ac02d524c"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5220"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5220"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-22T16:10:55.000Z"", ""patch"": ""diff --git a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\nindex ac35bc0f17e..338a5cd0308 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/Admin.java\n@@ -96,26 +96,28 @@\n public class Admin implements KeywordExecutable {\n   private static final Logger log = LoggerFactory.getLogger(Admin.class);\n \n-  static class AdminOpts extends ServerUtilOpts {\n-    @Parameter(names = {\""-f\"", \""--force\""},\n-        description = \""force the given server to stop by removing its lock\"")\n-    boolean force = false;\n+  private static class SubCommandOpts {\n+    @Parameter(names = {\""-h\"", \""-?\"", \""--help\"", \""-help\""}, help = true)\n+    public boolean help = false;\n   }\n \n   @Parameters(commandDescription = \""stop the tablet server on the given hosts\"")\n-  static class StopCommand {\n+  static class StopCommand extends SubCommandOpts {\n+    @Parameter(names = {\""-f\"", \""--force\""},\n+        description = \""force the given server to stop by removing its lock\"")\n+    boolean force = false;\n     @Parameter(description = \""<host> {<host> ... }\"")\n     List<String> args = new ArrayList<>();\n   }\n \n   @Parameters(commandDescription = \""Ping tablet servers.  If no arguments, pings all.\"")\n-  static class PingCommand {\n+  static class PingCommand extends SubCommandOpts {\n     @Parameter(description = \""{<host> ... }\"")\n     List<String> args = new ArrayList<>();\n   }\n \n   @Parameters(commandDescription = \""print tablets that are offline in online tables\"")\n-  static class CheckTabletsCommand {\n+  static class CheckTabletsCommand extends SubCommandOpts {\n     @Parameter(names = \""--fixFiles\"", description = \""Remove dangling file pointers\"")\n     boolean fixFiles = false;\n \n@@ -125,17 +127,17 @@ static class CheckTabletsCommand {\n   }\n \n   @Parameters(commandDescription = \""stop the manager\"")\n-  static class StopManagerCommand {}\n+  static class StopManagerCommand extends SubCommandOpts {}\n \n   @Deprecated(since = \""2.1.0\"")\n   @Parameters(commandDescription = \""stop the master (DEPRECATED -- use stopManager instead)\"")\n   static class StopMasterCommand {}\n \n   @Parameters(commandDescription = \""stop all tablet servers and the manager\"")\n-  static class StopAllCommand {}\n+  static class StopAllCommand extends SubCommandOpts {}\n \n   @Parameters(commandDescription = \""list Accumulo instances in zookeeper\"")\n-  static class ListInstancesCommand {\n+  static class ListInstancesCommand extends SubCommandOpts {\n     @Parameter(names = \""--print-errors\"", description = \""display errors while listing instances\"")\n     boolean printErrors = false;\n     @Parameter(names = \""--print-all\"",\n@@ -144,13 +146,13 @@ static class ListInstancesCommand {\n   }\n \n   @Parameters(commandDescription = \""Accumulo volume utility\"")\n-  static class VolumesCommand {\n+  static class VolumesCommand extends SubCommandOpts {\n     @Parameter(names = {\""-l\"", \""--list\""}, description = \""list volumes currently in use\"")\n     boolean printErrors = false;\n   }\n \n   @Parameters(commandDescription = \""print out non-default configuration settings\"")\n-  static class DumpConfigCommand {\n+  static class DumpConfigCommand extends SubCommandOpts {\n     @Parameter(names = {\""-a\"", \""--all\""},\n         description = \""print the system and all table configurations\"")\n     boolean allConfiguration = false;\n@@ -173,13 +175,13 @@ static class DumpConfigCommand {\n           + \""necessary.\"";\n \n   @Parameters(commandDescription = RV_DEPRECATION_MSG)\n-  static class RandomizeVolumesCommand {\n+  static class RandomizeVolumesCommand extends SubCommandOpts {\n     @Parameter(names = {\""-t\""}, description = \""table to update\"", required = true)\n     String tableName = null;\n   }\n \n   @Parameters(commandDescription = \""Verify all Tablets are assigned to tablet servers\"")\n-  static class VerifyTabletAssignmentsCommand {\n+  static class VerifyTabletAssignmentsCommand extends SubCommandOpts {\n     @Parameter(names = {\""-v\"", \""--verbose\""},\n         description = \""verbose mode (prints locations of tablets)\"")\n     boolean verbose = false;\n@@ -194,14 +196,14 @@ static class ChangeSecretCommand {}\n \n   @Parameters(\n       commandDescription = \""List or delete Tablet Server locks. Default with no arguments is to list the locks.\"")\n-  static class TabletServerLocksCommand {\n+  static class TabletServerLocksCommand extends SubCommandOpts {\n     @Parameter(names = \""-delete\"", description = \""specify a tablet server lock to delete\"")\n     String delete = null;\n   }\n \n   @Parameters(\n       commandDescription = \""Deletes specific instance name or id from zookeeper or cleans up all old instances.\"")\n-  static class DeleteZooInstanceCommand {\n+  static class DeleteZooInstanceCommand extends SubCommandOpts {\n     @Parameter(names = {\""-i\"", \""--instance\""}, description = \""the instance name or id to delete\"")\n     String instance;\n     @Parameter(names = {\""-c\"", \""--clean\""},\n@@ -214,7 +216,7 @@ static class DeleteZooInstanceCommand {\n   }\n \n   @Parameters(commandDescription = \""Restore Zookeeper data from a file.\"")\n-  static class RestoreZooCommand {\n+  static class RestoreZooCommand extends SubCommandOpts {\n     @Parameter(names = \""--overwrite\"")\n     boolean overwrite = false;\n \n@@ -224,7 +226,7 @@ static class RestoreZooCommand {\n \n   @Parameters(commandNames = \""fate\"",\n       commandDescription = \""Operations performed on the Manager FaTE system.\"")\n-  static class FateOpsCommand {\n+  static class FateOpsCommand extends SubCommandOpts {\n     @Parameter(description = \""[<txId>...]\"")\n     List<String> txList = new ArrayList<>();\n \n@@ -255,6 +257,15 @@ static class FateOpsCommand {\n     List<String> states = new ArrayList<>();\n   }\n \n+  @Parameters(commandDescription = \""show service status\"")\n+  public static class ServiceStatusCmdOpts extends SubCommandOpts {\n+    @Parameter(names = \""--json\"", description = \""provide output in json format (--noHosts ignored)\"")\n+    boolean json = false;\n+    @Parameter(names = \""--noHosts\"",\n+        description = \""provide a summary of service counts without host details\"")\n+    boolean noHosts = false;\n+  }\n+\n   public static void main(String[] args) {\n     new Admin().execute(args);\n   }\n@@ -277,13 +288,12 @@ public String description() {\n   @SuppressFBWarnings(value = \""DM_EXIT\"", justification = \""System.exit okay for CLI tool\"")\n   @Override\n   public void execute(final String[] args) {\n-    boolean everything;\n \n-    AdminOpts opts = new AdminOpts();\n+    ServerUtilOpts opts = new ServerUtilOpts();\n     JCommander cl = new JCommander(opts);\n     cl.setProgramName(\""accumulo admin\"");\n \n-    ServiceStatusCmd.Opts serviceStatusCommandOpts = new ServiceStatusCmd.Opts();\n+    ServiceStatusCmdOpts serviceStatusCommandOpts = new ServiceStatusCmdOpts();\n     cl.addCommand(\""serviceStatus\"", serviceStatusCommandOpts);\n \n     ChangeSecretCommand changeSecretCommand = new ChangeSecretCommand();\n@@ -337,11 +347,21 @@ public void execute(final String[] args) {\n \n     cl.parse(args);\n \n-    if (opts.help || cl.getParsedCommand() == null) {\n+    if (cl.getParsedCommand() == null) {\n       cl.usage();\n       return;\n     }\n \n+    for (var command : cl.getCommands().entrySet()) {\n+      var objects = command.getValue().getObjects();\n+      for (var obj : objects) {\n+        if (obj instanceof SubCommandOpts && ((SubCommandOpts) obj).help) {\n+          command.getValue().usage();\n+          return;\n+        }\n+      }\n+    }\n+\n     ServerContext context = opts.getServerContext();\n \n     AccumuloConfiguration conf = context.getConfiguration();\n@@ -380,7 +400,7 @@ public void execute(final String[] args) {\n         }\n \n       } else if (cl.getParsedCommand().equals(\""stop\"")) {\n-        stopTabletServer(context, stopOpts.args, opts.force);\n+        stopTabletServer(context, stopOpts.args, stopOpts.force);\n       } else if (cl.getParsedCommand().equals(\""dumpConfig\"")) {\n         printConfig(context, dumpConfigCommand);\n       } else if (cl.getParsedCommand().equals(\""volumes\"")) {\n@@ -402,15 +422,19 @@ public void execute(final String[] args) {\n       } else if (cl.getParsedCommand().equals(\""fate\"")) {\n         executeFateOpsCommand(context, fateOpsCommand);\n       } else if (cl.getParsedCommand().equals(\""serviceStatus\"")) {\n-        printServiceStatus(context, serviceStatusCommandOpts);\n-      } else {\n-        everything = cl.getParsedCommand().equals(\""stopAll\"");\n+        ServiceStatusCmd ssc = new ServiceStatusCmd();\n+        ssc.execute(context, serviceStatusCommandOpts.json, serviceStatusCommandOpts.noHosts);\n+      } else if (cl.getParsedCommand().equals(\""stopManager\"")\n+          || cl.getParsedCommand().equals(\""stopAll\"")) {\n+        boolean everything = cl.getParsedCommand().equals(\""stopAll\"");\n \n         if (everything) {\n           flushAll(context);\n         }\n \n         stopServer(context, everything);\n+      } else {\n+        cl.usage();\n       }\n \n       if (rc != 0) {\n@@ -430,11 +454,6 @@ public void execute(final String[] args) {\n     }\n   }\n \n-  private static void printServiceStatus(ServerContext context, ServiceStatusCmd.Opts opts) {\n-    ServiceStatusCmd ssc = new ServiceStatusCmd();\n-    ssc.execute(context, opts);\n-  }\n-\n   private static int ping(ClientContext context, List<String> args) {\n \n     InstanceOperations io = context.instanceOperations();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java b/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\nindex 557c7891415..23b0e8aafc2 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/util/ServiceStatusCmd.java\n@@ -39,8 +39,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import com.beust.jcommander.Parameter;\n-import com.beust.jcommander.Parameters;\n import com.google.common.annotations.VisibleForTesting;\n \n public class ServiceStatusCmd {\n@@ -56,7 +54,7 @@ public ServiceStatusCmd() {}\n    * Read the service statuses from ZooKeeper, build the status report and then output the report to\n    * stdout.\n    */\n-  public void execute(final ServerContext context, final Opts opts) {\n+  public void execute(final ServerContext context, final boolean json, final boolean noHosts) {\n \n     ZooReader zooReader = context.getZooReader();\n \n@@ -74,9 +72,9 @@ public void execute(final ServerContext context, final Opts opts) {\n     services.put(ServiceStatusReport.ReportKey.COMPACTOR, getCompactorStatus(zooReader, zooRoot));\n     services.put(ServiceStatusReport.ReportKey.GC, getGcStatus(zooReader, zooRoot));\n \n-    ServiceStatusReport report = new ServiceStatusReport(services, opts.noHosts);\n+    ServiceStatusReport report = new ServiceStatusReport(services, noHosts);\n \n-    if (opts.json) {\n+    if (json) {\n       System.out.println(report.toJson());\n     } else {\n       StringBuilder sb = new StringBuilder(8192);\n@@ -336,15 +334,6 @@ Result<Set<String>> readAllNodesData(final ZooReader zooReader, final String pat\n     return new Result<>(errorCount.get(), hosts);\n   }\n \n-  @Parameters(commandDescription = \""show service status\"")\n-  public static class Opts {\n-    @Parameter(names = \""--json\"", description = \""provide output in json format (--noHosts ignored)\"")\n-    boolean json = false;\n-    @Parameter(names = \""--noHosts\"",\n-        description = \""provide a summary of service counts without host details\"")\n-    boolean noHosts = false;\n-  }\n-\n   /**\n    * Provides explicit method names instead of generic getFirst to get the error count and getSecond\n    * hosts information\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java b/core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java\nindex 7d9c2d44aa1..73be4a70bba 100644\n--- a/core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java\n+++ b/core/src/test/java/org/apache/accumulo/core/cli/TestHelp.java\n@@ -19,9 +19,12 @@\n package org.apache.accumulo.core.cli;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import org.junit.jupiter.api.Test;\n \n+import com.beust.jcommander.Parameter;\n+\n public class TestHelp {\n   protected class HelpStub extends Help {\n     @Override\n@@ -46,4 +49,20 @@ public void testInvalidArgs() {\n     }\n   }\n \n+  @Test\n+  public void testHelpCommand() {\n+    class TestHelpOpt extends HelpStub {\n+      @Parameter(names = {\""--test\""})\n+      boolean test = false;\n+    }\n+\n+    String[] args = {\""--help\"", \""--test\""};\n+    TestHelpOpt opts = new TestHelpOpt();\n+    try {\n+      opts.parseArgs(\""program\"", args);\n+      assertTrue(opts.test);\n+    } catch (RuntimeException e) {\n+      assertEquals(\""0\"", e.getMessage());\n+    }\n+  }\n }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java\nindex d3f77a34d1a..47181b11655 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/AdminCommandsTest.java\n@@ -86,8 +86,8 @@ public void testDumpConfigCommand() {\n \n   // not a command, but easy enough to include here\n   @Test\n-  public void testAdminOpts() {\n-    Admin.AdminOpts opts = new Admin.AdminOpts();\n+  public void testStopOpts() {\n+    Admin.StopCommand opts = new Admin.StopCommand();\n     assertFalse(opts.force);\n   }\n }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java b/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\nindex f4af7497986..ba7e7c47000 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/util/ServiceStatusCmdTest.java\n@@ -398,7 +398,7 @@ void zkNodeDeletedTest() throws Exception {\n   @Test\n   public void testServiceStatusCommandOpts() {\n     replay(zooReader); // needed for @AfterAll verify\n-    ServiceStatusCmd.Opts opts = new ServiceStatusCmd.Opts();\n+    Admin.ServiceStatusCmdOpts opts = new Admin.ServiceStatusCmdOpts();\n     assertFalse(opts.json);\n     assertFalse(opts.noHosts);\n   }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5187"", ""pr_id"": 5187, ""issue_id"": 5014, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Add identifier to split points for automated vs user created split points\n**Is your feature request related to a problem? Please describe.**\r\nWhen split points are automatically created due to tablet sizes, these split points persist until a user runs a merge. \r\nIf a merge is run automatically, then it might remove split points that were purposely set by the user.\r\n\r\n**Describe the solution you'd like**\r\nSplit points created automatically by accumulo should be identified as such so they can easily be removed."", ""issue_word_count"": 84, ""test_files_count"": 10, ""non_test_files_count"": 20, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/admin/TabletMergeability.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMergeabilityMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java"", ""core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java"", ""core/src/main/java/org/apache/accumulo/core/util/time/SteadyTime.java"", ""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java"", ""server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/TableInfo.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/CreateTable.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/PopulateMetadata.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/FindSplits.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/PreSplit.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/SplitInfo.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java"", ""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java"", ""server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java"", ""test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java"", ""test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java"", ""test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java""], ""base_commit"": ""09dc3ce5ac2eb8c895800a5281e747aba1c25f23"", ""head_commit"": ""97aa2a5757b9a14645fd89d87d7470dccd0af59c"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5187"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5187"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-10T18:22:23.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/TabletMergeability.java b/core/src/main/java/org/apache/accumulo/core/client/admin/TabletMergeability.java\nnew file mode 100644\nindex 00000000000..c9d4de31e83\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/TabletMergeability.java\n@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.client.admin;\n+\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * @since 4.0.0\n+ */\n+public class TabletMergeability implements Serializable {\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final TabletMergeability NEVER = new TabletMergeability();\n+  private static final TabletMergeability ALWAYS = new TabletMergeability(Duration.ZERO);\n+\n+  private final Duration delay;\n+\n+  private TabletMergeability(Duration delay) {\n+    this.delay = Objects.requireNonNull(delay);\n+  }\n+\n+  // Edge case for NEVER\n+  private TabletMergeability() {\n+    this.delay = null;\n+  }\n+\n+  /**\n+   * Determines if the configured delay signals a tablet is never eligible to be automatically\n+   * merged.\n+   *\n+   * @return true if never mergeable, else false\n+   */\n+  public boolean isNever() {\n+    return this.delay == null;\n+  }\n+\n+  /**\n+   * Determines if the configured delay signals a tablet is always eligible to be automatically\n+   * merged now. (Has a delay of 0)\n+   *\n+   * @return true if always mergeable now, else false\n+   */\n+  public boolean isAlways() {\n+    return delay != null && this.delay.isZero();\n+  }\n+\n+  /**\n+   * Returns an Optional duration of the delay which is one of:\n+   *\n+   * <ul>\n+   * <li>empty (never)</li>\n+   * <li>0 (now)</li>\n+   * <li>positive delay</li>\n+   * </ul>\n+   *\n+   * @return the configured mergeability delay\n+   */\n+  public Optional<Duration> getDelay() {\n+    return Optional.ofNullable(delay);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    TabletMergeability that = (TabletMergeability) o;\n+    return Objects.equals(delay, that.delay);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Objects.hashCode(delay);\n+  }\n+\n+  @Override\n+  public String toString() {\n+    if (delay == null) {\n+      return \""TabletMergeability=NEVER\"";\n+    }\n+    return \""TabletMergeability=AFTER:\"" + delay.toMillis() + \""ms\"";\n+  }\n+\n+  /**\n+   * Signifies that a tablet is never eligible to be automatically merged.\n+   *\n+   * @return a {@link TabletMergeability} with an empty delay signaling never merge\n+   */\n+  public static TabletMergeability never() {\n+    return NEVER;\n+  }\n+\n+  /**\n+   * Signifies that a tablet is eligible now to be automatically merged\n+   *\n+   * @return a {@link TabletMergeability} with a delay of 0 signaling never merge\n+   */\n+  public static TabletMergeability always() {\n+    return ALWAYS;\n+  }\n+\n+  /**\n+   * Creates a {@link TabletMergeability} that signals a tablet has a delay to a point in the future\n+   * before it is automatically eligible to be merged. The duration must be positive value.\n+   *\n+   * @param delay the duration of the delay\n+   *\n+   * @return a {@link TabletMergeability} from the given delay.\n+   */\n+  public static TabletMergeability after(Duration delay) {\n+    Preconditions.checkArgument(delay.toNanos() >= 0, \""Duration of delay must be greater than 0.\"");\n+    return new TabletMergeability(delay);\n+  }\n+\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\nindex 367ee6fe64a..c78e5661bb8 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/Ample.java\n@@ -393,6 +393,8 @@ interface TabletUpdates<T> {\n \n     T putCloned();\n \n+    T putTabletMergeability(TabletMergeabilityMetadata tabletMergeability);\n+\n     /**\n      * By default the server lock is automatically added to mutations unless this method is set to\n      * false.\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\nindex 5360c98274b..885fd326239 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/MetadataSchema.java\n@@ -158,6 +158,10 @@ public static class TabletColumnFamily {\n       public static final String REQUESTED_QUAL = \""requestToHost\"";\n       public static final ColumnFQ REQUESTED_COLUMN = new ColumnFQ(NAME, new Text(REQUESTED_QUAL));\n \n+      public static final String MERGEABILITY_QUAL = \""mergeability\"";\n+      public static final ColumnFQ MERGEABILITY_COLUMN =\n+          new ColumnFQ(NAME, new Text(MERGEABILITY_QUAL));\n+\n       public static Value encodePrevEndRow(Text per) {\n         if (per == null) {\n           return new Value(new byte[] {0});\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMergeabilityMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMergeabilityMetadata.java\nnew file mode 100644\nindex 00000000000..0880e402d96\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMergeabilityMetadata.java\n@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.metadata.schema;\n+\n+import static org.apache.accumulo.core.util.LazySingletons.GSON;\n+\n+import java.io.Serializable;\n+import java.time.Duration;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.accumulo.core.client.admin.TabletMergeability;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.util.time.SteadyTime;\n+\n+import com.google.common.base.Preconditions;\n+\n+public class TabletMergeabilityMetadata implements Serializable {\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final TabletMergeabilityMetadata NEVER =\n+      new TabletMergeabilityMetadata(TabletMergeability.never());;\n+\n+  private final TabletMergeability tabletMergeability;\n+  private final SteadyTime steadyTime;\n+\n+  private TabletMergeabilityMetadata(TabletMergeability tabletMergeability, SteadyTime steadyTime) {\n+    this.tabletMergeability = Objects.requireNonNull(tabletMergeability);\n+    this.steadyTime = steadyTime;\n+    // This makes sure that SteadyTime is set if TabletMergeability has a delay, and is null\n+    // if TabletMergeability is NEVER as we don't need to store it in that case\n+    Preconditions.checkArgument(tabletMergeability.isNever() == (steadyTime == null),\n+        \""SteadyTime must be set if and only if TabletMergeability delay is >= 0\"");\n+  }\n+\n+  private TabletMergeabilityMetadata(TabletMergeability tabletMergeability) {\n+    this(tabletMergeability, null);\n+  }\n+\n+  public TabletMergeability getTabletMergeability() {\n+    return tabletMergeability;\n+  }\n+\n+  public Optional<SteadyTime> getSteadyTime() {\n+    return Optional.ofNullable(steadyTime);\n+  }\n+\n+  public boolean isMergeable(SteadyTime currentTime) {\n+    if (tabletMergeability.isNever()) {\n+      return false;\n+    }\n+    // Steady time should never be null unless TabletMergeability is NEVER\n+    Preconditions.checkState(steadyTime != null, \""SteadyTime should be set\"");\n+    var totalDelay = steadyTime.getDuration().plus(tabletMergeability.getDelay().orElseThrow());\n+    return currentTime.getDuration().compareTo(totalDelay) >= 0;\n+  }\n+\n+  private static class GSonData {\n+    boolean never;\n+    Long delay;\n+    Long steadyTime;\n+  }\n+\n+  String toJson() {\n+    GSonData jData = new GSonData();\n+    jData.never = tabletMergeability.isNever();\n+    jData.delay = tabletMergeability.getDelay().map(Duration::toNanos).orElse(null);\n+    jData.steadyTime = steadyTime != null ? steadyTime.getNanos() : null;\n+    return GSON.get().toJson(jData);\n+  }\n+\n+  static TabletMergeabilityMetadata fromJson(String json) {\n+    GSonData jData = GSON.get().fromJson(json, GSonData.class);\n+    if (jData.never) {\n+      Preconditions.checkArgument(jData.delay == null && jData.steadyTime == null,\n+          \""delay and steadyTime should be null if mergeability 'never' is true\"");\n+    } else {\n+      Preconditions.checkArgument(jData.delay != null && jData.steadyTime != null,\n+          \""delay and steadyTime should both be set if mergeability 'never' is false\"");\n+    }\n+    TabletMergeability tabletMergeability = jData.never ? TabletMergeability.never()\n+        : TabletMergeability.after(Duration.ofNanos(jData.delay));\n+    SteadyTime steadyTime =\n+        jData.steadyTime != null ? SteadyTime.from(jData.steadyTime, TimeUnit.NANOSECONDS) : null;\n+    return new TabletMergeabilityMetadata(tabletMergeability, steadyTime);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    TabletMergeabilityMetadata that = (TabletMergeabilityMetadata) o;\n+    return Objects.equals(tabletMergeability, that.tabletMergeability)\n+        && Objects.equals(steadyTime, that.steadyTime);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Objects.hash(tabletMergeability, steadyTime);\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \""TabletMergeabilityMetadata{\"" + tabletMergeability + \"", \"" + steadyTime + '}';\n+  }\n+\n+  public static TabletMergeabilityMetadata never() {\n+    return NEVER;\n+  }\n+\n+  public static TabletMergeabilityMetadata always(SteadyTime currentTime) {\n+    return new TabletMergeabilityMetadata(TabletMergeability.always(), currentTime);\n+  }\n+\n+  public static TabletMergeabilityMetadata after(Duration delay, SteadyTime currentTime) {\n+    return new TabletMergeabilityMetadata(TabletMergeability.after(delay), currentTime);\n+  }\n+\n+  public static Value toValue(TabletMergeabilityMetadata tmm) {\n+    return new Value(tmm.toJson());\n+  }\n+\n+  public static TabletMergeabilityMetadata fromValue(Value value) {\n+    return TabletMergeabilityMetadata.fromJson(value.toString());\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\nindex 795ebfafed4..2b59f78431d 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadata.java\n@@ -25,6 +25,7 @@\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.SELECTED_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily.TIME_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.AVAILABILITY_QUAL;\n+import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.MERGEABILITY_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.PREV_ROW_QUAL;\n import static org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily.REQUESTED_QUAL;\n \n@@ -123,6 +124,7 @@ public class TabletMetadata {\n   private final Set<FateId> compacted;\n   private final Set<FateId> userCompactionsRequested;\n   private final UnSplittableMetadata unSplittableMetadata;\n+  private final TabletMergeabilityMetadata mergeability;\n   private final Supplier<Long> fileSize;\n \n   private TabletMetadata(Builder tmBuilder) {\n@@ -155,6 +157,7 @@ private TabletMetadata(Builder tmBuilder) {\n     this.compacted = tmBuilder.compacted.build();\n     this.userCompactionsRequested = tmBuilder.userCompactionsRequested.build();\n     this.unSplittableMetadata = tmBuilder.unSplittableMetadata;\n+    this.mergeability = Objects.requireNonNull(tmBuilder.mergeability);\n     this.fileSize = Suppliers.memoize(() -> {\n       // This code was using a java stream. While profiling SplitMillionIT, the stream was showing\n       // up as hot when scanning 1 million tablets. Converted to a for loop to improve performance.\n@@ -198,7 +201,8 @@ public enum ColumnType {\n     SELECTED,\n     COMPACTED,\n     USER_COMPACTION_REQUESTED,\n-    UNSPLITTABLE\n+    UNSPLITTABLE,\n+    MERGEABILITY\n   }\n \n   public static class Location {\n@@ -439,6 +443,11 @@ public UnSplittableMetadata getUnSplittable() {\n     return unSplittableMetadata;\n   }\n \n+  public TabletMergeabilityMetadata getTabletMergeability() {\n+    ensureFetched(ColumnType.MERGEABILITY);\n+    return mergeability;\n+  }\n+\n   @Override\n   public String toString() {\n     return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE).append(\""tableId\"", tableId)\n@@ -453,7 +462,8 @@ public String toString() {\n         .append(\""operationId\"", operationId).append(\""selectedFiles\"", selectedFiles)\n         .append(\""futureAndCurrentLocationSet\"", futureAndCurrentLocationSet)\n         .append(\""userCompactionsRequested\"", userCompactionsRequested)\n-        .append(\""unSplittableMetadata\"", unSplittableMetadata).toString();\n+        .append(\""unSplittableMetadata\"", unSplittableMetadata).append(\""mergeability\"", mergeability)\n+        .toString();\n   }\n \n   public List<Entry<Key,Value>> getKeyValues() {\n@@ -527,6 +537,9 @@ public static <E extends Entry<Key,Value>> TabletMetadata convertRow(Iterator<E>\n             case REQUESTED_QUAL:\n               tmBuilder.onDemandHostingRequested(true);\n               break;\n+            case MERGEABILITY_QUAL:\n+              tmBuilder.mergeability(TabletMergeabilityMetadata.fromValue(kv.getValue()));\n+              break;\n             default:\n               throw new IllegalStateException(\""Unexpected TabletColumnFamily qualifier: \"" + qual);\n           }\n@@ -689,7 +702,7 @@ static class Builder {\n     private final ImmutableSet.Builder<FateId> compacted = ImmutableSet.builder();\n     private final ImmutableSet.Builder<FateId> userCompactionsRequested = ImmutableSet.builder();\n     private UnSplittableMetadata unSplittableMetadata;\n-    // private Supplier<Long> fileSize;\n+    private TabletMergeabilityMetadata mergeability = TabletMergeabilityMetadata.never();\n \n     void table(TableId tableId) {\n       this.tableId = tableId;\n@@ -799,6 +812,10 @@ void unSplittableMetadata(UnSplittableMetadata unSplittableMetadata) {\n       this.unSplittableMetadata = unSplittableMetadata;\n     }\n \n+    void mergeability(TabletMergeabilityMetadata mergeability) {\n+      this.mergeability = mergeability;\n+    }\n+\n     void keyValue(Entry<Key,Value> kv) {\n       if (this.keyValues == null) {\n         this.keyValues = ImmutableList.builder();\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\nindex 44f1915e0ea..8ca33d9eb79 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMetadataBuilder.java\n@@ -30,6 +30,7 @@\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOADED;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOCATION;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOGS;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGED;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.OPID;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n@@ -312,6 +313,14 @@ public TabletMetadataBuilder putCloned() {\n     return this;\n   }\n \n+  @Override\n+  public TabletMetadataBuilder\n+      putTabletMergeability(TabletMergeabilityMetadata tabletMergeability) {\n+    fetched.add(MERGEABILITY);\n+    internalBuilder.putTabletMergeability(tabletMergeability);\n+    return this;\n+  }\n+\n   @Override\n   public TabletMetadataBuilder automaticallyPutServerLock(boolean b) {\n     throw new UnsupportedOperationException();\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\nindex 6052c73a799..4f39eda7b7e 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletMutatorBase.java\n@@ -390,6 +390,13 @@ public T automaticallyPutServerLock(boolean b) {\n     return getThis();\n   }\n \n+  @Override\n+  public T putTabletMergeability(TabletMergeabilityMetadata tabletMergeability) {\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(mutation,\n+        TabletMergeabilityMetadata.toValue(tabletMergeability));\n+    return getThis();\n+  }\n+\n   public void setCloseAfterMutate(AutoCloseable closeable) {\n     this.closeAfterMutate = closeable;\n   }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\nindex a3914ea0ed0..04115dfaeac 100644\n--- a/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\n+++ b/core/src/main/java/org/apache/accumulo/core/metadata/schema/TabletsMetadata.java\n@@ -77,6 +77,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ScanFileColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SplitColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.SuspendLocationColumn;\n+import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.filters.TabletMetadataFilter;\n@@ -394,6 +395,9 @@ public Options fetch(ColumnType... colsToFetch) {\n           case UNSPLITTABLE:\n             qualifiers.add(SplitColumnFamily.UNSPLITTABLE_COLUMN);\n             break;\n+          case MERGEABILITY:\n+            qualifiers.add(TabletColumnFamily.MERGEABILITY_COLUMN);\n+            break;\n           default:\n             throw new IllegalArgumentException(\""Unknown col type \"" + colToFetch);\n         }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/time/SteadyTime.java b/core/src/main/java/org/apache/accumulo/core/util/time/SteadyTime.java\nindex d16f15c2019..57b348b23d3 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/time/SteadyTime.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/time/SteadyTime.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.accumulo.core.util.time;\n \n+import java.io.Serializable;\n import java.time.Duration;\n import java.util.Objects;\n import java.util.concurrent.TimeUnit;\n@@ -30,7 +31,8 @@\n  * is not expected to represent real world date times, its main use is for computing deltas similar\n  * System.nanoTime but across JVM processes.\n  */\n-public class SteadyTime implements Comparable<SteadyTime> {\n+public class SteadyTime implements Comparable<SteadyTime>, Serializable {\n+  private static final long serialVersionUID = 1L;\n \n   private final Duration time;\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\nindex f91828361f0..0d2988423c4 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/constraints/MetadataConstraints.java\n@@ -59,6 +59,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.Upgrade12to13;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n import org.apache.accumulo.core.metadata.schema.TabletOperationType;\n import org.apache.accumulo.core.metadata.schema.UnSplittableMetadata;\n@@ -97,6 +98,7 @@ public class MetadataConstraints implements Constraint {\n           TabletColumnFamily.REQUESTED_COLUMN,\n           ServerColumnFamily.SELECTED_COLUMN,\n           SplitColumnFamily.UNSPLITTABLE_COLUMN,\n+          TabletColumnFamily.MERGEABILITY_COLUMN,\n           Upgrade12to13.COMPACT_COL);\n \n   @SuppressWarnings(\""deprecation\"")\n@@ -297,6 +299,8 @@ public String getViolationDescription(short violationCode) {\n         return \""Invalid unsplittable column\"";\n       case 4005:\n         return \""Malformed availability value\"";\n+      case 4006:\n+        return \""Malformed mergeability value\"";\n \n     }\n     return null;\n@@ -376,6 +380,13 @@ private void validateTabletFamily(ArrayList<Short> violations, ColumnUpdate colu\n           addViolation(violations, 4005);\n         }\n         break;\n+      case (TabletColumnFamily.MERGEABILITY_QUAL):\n+        try {\n+          TabletMergeabilityMetadata.fromValue(new Value(columnUpdate.getValue()));\n+        } catch (IllegalArgumentException e) {\n+          addViolation(violations, 4006);\n+        }\n+        break;\n     }\n   }\n \n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\nindex b315f1a58c3..ef6536da098 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/FileSystemInitializer.java\n@@ -44,6 +44,7 @@\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema;\n import org.apache.accumulo.core.metadata.schema.MetadataTime;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.spi.crypto.CryptoService;\n import org.apache.accumulo.core.spi.fs.VolumeChooserEnvironment;\n@@ -93,7 +94,8 @@ private Map<Key,Value> createEntries() {\n       KeyExtent keyExtent = new KeyExtent(tableId, endRow, prevEndRow);\n       var builder = TabletMetadata.builder(keyExtent).putDirName(dirName)\n           .putTime(new MetadataTime(0, TimeType.LOGICAL))\n-          .putTabletAvailability(TabletAvailability.HOSTED).putPrevEndRow(prevEndRow);\n+          .putTabletAvailability(TabletAvailability.HOSTED)\n+          .putTabletMergeability(TabletMergeabilityMetadata.never()).putPrevEndRow(prevEndRow);\n       for (String file : files) {\n         builder.putFile(new ReferencedTabletFile(new Path(file)).insert(), new DataFileValue(0, 0));\n       }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java b/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\nindex de1bae81a85..7cdde03dc52 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/FateServiceHandler.java\n@@ -80,6 +80,7 @@\n import org.apache.accumulo.core.manager.thrift.TFateInstanceType;\n import org.apache.accumulo.core.manager.thrift.TFateOperation;\n import org.apache.accumulo.core.manager.thrift.ThriftPropertyException;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.securityImpl.thrift.TCredentials;\n import org.apache.accumulo.core.util.ByteBufferUtil;\n import org.apache.accumulo.core.util.TextUtil;\n@@ -256,7 +257,7 @@ public void executeFateOperation(TInfo tinfo, TCredentials c, TFateId opid, TFat\n         manager.fate(type).seedTransaction(op, fateId,\n             new TraceRepo<>(new CreateTable(c.getPrincipal(), tableName, timeType, options,\n                 splitsPath, splitCount, splitsDirsPath, initialTableState,\n-                initialTabletAvailability, namespaceId)),\n+                initialTabletAvailability, namespaceId, TabletMergeabilityMetadata.never())),\n             autoCleanup, goalMessage);\n \n         break;\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/TableInfo.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/TableInfo.java\nindex 5294c3ef04b..8852a41cd94 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/TableInfo.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/TableInfo.java\n@@ -26,6 +26,7 @@\n import org.apache.accumulo.core.client.admin.TimeType;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.hadoop.fs.Path;\n \n public class TableInfo implements Serializable {\n@@ -51,6 +52,8 @@ public class TableInfo implements Serializable {\n \n   private TabletAvailability initialTabletAvailability;\n \n+  private TabletMergeabilityMetadata initialTabletMergeability;\n+\n   public TabletAvailability getInitialTabletAvailability() {\n     return initialTabletAvailability;\n   }\n@@ -133,4 +136,11 @@ public void setInitialSplitSize(int initialSplitSize) {\n     this.initialSplitSize = initialSplitSize;\n   }\n \n+  public TabletMergeabilityMetadata getInitialTabletMergeability() {\n+    return initialTabletMergeability;\n+  }\n+\n+  public void setInitialTabletMergeability(TabletMergeabilityMetadata initialTabletMergeability) {\n+    this.initialTabletMergeability = initialTabletMergeability;\n+  }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/CreateTable.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/CreateTable.java\nindex 3f5a379c8a4..f0d74597db6 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/CreateTable.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/CreateTable.java\n@@ -30,6 +30,7 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.zookeeper.DistributedReadWriteLock.LockType;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.manager.Manager;\n import org.apache.accumulo.manager.tableOps.ManagerRepo;\n import org.apache.accumulo.manager.tableOps.TableInfo;\n@@ -47,7 +48,8 @@ public class CreateTable extends ManagerRepo {\n \n   public CreateTable(String user, String tableName, TimeType timeType, Map<String,String> props,\n       Path splitPath, int splitCount, Path splitDirsPath, InitialTableState initialTableState,\n-      TabletAvailability initialTabletAvailability, NamespaceId namespaceId) {\n+      TabletAvailability initialTabletAvailability, NamespaceId namespaceId,\n+      TabletMergeabilityMetadata initialTabletMergeability) {\n     tableInfo = new TableInfo();\n     tableInfo.setTableName(tableName);\n     tableInfo.setTimeType(timeType);\n@@ -59,6 +61,7 @@ public CreateTable(String user, String tableName, TimeType timeType, Map<String,\n     tableInfo.setInitialTableState(initialTableState);\n     tableInfo.setSplitDirsPath(splitDirsPath);\n     tableInfo.setInitialTabletAvailability(initialTabletAvailability);\n+    tableInfo.setInitialTabletMergeability(initialTabletMergeability);\n   }\n \n   @Override\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/PopulateMetadata.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/PopulateMetadata.java\nindex 08cfcd194f6..2b987c7a4e0 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/PopulateMetadata.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/create/PopulateMetadata.java\n@@ -92,6 +92,7 @@ private void writeSplitsToMetadataTable(ServerContext context, SortedSet<Text> s\n         tabletMutator.putDirName(dirName);\n         tabletMutator.putTime(new MetadataTime(0, tableInfo.getTimeType()));\n         tabletMutator.putTabletAvailability(tableInfo.getInitialTabletAvailability());\n+        tabletMutator.putTabletMergeability(tableInfo.getInitialTabletMergeability());\n         tabletMutator.mutate();\n \n         prevSplit = split;\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\nindex cdd54ac1439..5b490ab567e 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/merge/MergeTablets.java\n@@ -186,6 +186,7 @@ public Repo<Manager> call(FateId fateId, Manager manager) throws Exception {\n         tabletMutator.putTabletAvailability(\n             DeleteRows.getMergeTabletAvailability(range, tabletAvailabilities));\n         tabletMutator.putPrevEndRow(firstTabletMeta.getPrevEndRow());\n+        tabletMutator.putTabletMergeability(lastTabletMeta.getTabletMergeability());\n \n         // scan entries are related to a hosted tablet, this tablet is not hosted so can safely\n         // delete these\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/FindSplits.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/FindSplits.java\nindex 60073e987a2..c614ce4be2e 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/FindSplits.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/FindSplits.java\n@@ -48,7 +48,7 @@ public class FindSplits extends ManagerRepo {\n   private final SplitInfo splitInfo;\n \n   public FindSplits(KeyExtent extent) {\n-    this.splitInfo = new SplitInfo(extent, new TreeSet<>());\n+    this.splitInfo = new SplitInfo(extent, new TreeSet<>(), true);\n   }\n \n   @Override\n@@ -156,7 +156,7 @@ public Repo<Manager> call(FateId fateId, Manager manager) throws Exception {\n       return null;\n     }\n \n-    return new PreSplit(extent, splits);\n+    return new PreSplit(extent, splits, true);\n   }\n \n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/PreSplit.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/PreSplit.java\nindex 6d89878f955..41d8d039ea7 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/PreSplit.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/PreSplit.java\n@@ -50,12 +50,16 @@ public class PreSplit extends ManagerRepo {\n \n   private final SplitInfo splitInfo;\n \n-  public PreSplit(KeyExtent expectedExtent, SortedSet<Text> splits) {\n+  public PreSplit(KeyExtent expectedExtent, SortedSet<Text> splits, boolean systemCreated) {\n     Objects.requireNonNull(expectedExtent);\n     Objects.requireNonNull(splits);\n     Preconditions.checkArgument(!splits.isEmpty());\n     Preconditions.checkArgument(!expectedExtent.isRootTablet());\n-    this.splitInfo = new SplitInfo(expectedExtent, splits);\n+    this.splitInfo = new SplitInfo(expectedExtent, splits, systemCreated);\n+  }\n+\n+  public PreSplit(KeyExtent expectedExtent, SortedSet<Text> splits) {\n+    this(expectedExtent, splits, false);\n   }\n \n   @Override\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/SplitInfo.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/SplitInfo.java\nindex 7d97e6a34e1..14bac1fe7ef 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/SplitInfo.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/SplitInfo.java\n@@ -36,12 +36,14 @@ public class SplitInfo implements Serializable {\n   private final byte[] prevEndRow;\n   private final byte[] endRow;\n   private final byte[][] splits;\n+  private final boolean systemCreated;\n \n-  public SplitInfo(KeyExtent extent, SortedSet<Text> splits) {\n+  public SplitInfo(KeyExtent extent, SortedSet<Text> splits, boolean systemCreated) {\n     this.tableId = extent.tableId();\n     this.prevEndRow = extent.prevEndRow() == null ? null : TextUtil.getBytes(extent.prevEndRow());\n     this.endRow = extent.endRow() == null ? null : TextUtil.getBytes(extent.endRow());\n     this.splits = new byte[splits.size()][];\n+    this.systemCreated = systemCreated;\n \n     int index = 0;\n     for (var split : splits) {\n@@ -85,4 +87,7 @@ SortedSet<KeyExtent> getTablets() {\n     return tablets;\n   }\n \n+  boolean isSystemCreated() {\n+    return systemCreated;\n+  }\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\nindex ce48d480b14..7da6f36af6d 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/split/UpdateTablets.java\n@@ -35,6 +35,7 @@\n import org.apache.accumulo.core.metadata.schema.Ample;\n import org.apache.accumulo.core.metadata.schema.Ample.ConditionalResult.Status;\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n import org.apache.accumulo.core.metadata.schema.TabletOperationType;\n@@ -218,7 +219,9 @@ private void addNewTablets(FateId fateId, Manager manager, TabletMetadata tablet\n             .debug(\""{} copying compacted marker to new child tablet {}\"", fateId, compactedFateId));\n \n         mutator.putTabletAvailability(tabletMetadata.getTabletAvailability());\n-\n+        mutator.putTabletMergeability(\n+            splitInfo.isSystemCreated() ? TabletMergeabilityMetadata.always(manager.getSteadyTime())\n+                : TabletMergeabilityMetadata.never());\n         tabletMetadata.getLoaded().forEach((k, v) -> mutator.putBulkFile(k.getTabletFile(), v));\n \n         newTabletsFiles.get(newExtent).forEach(mutator::putFile);\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\nindex 9f4ba14def1..1381f44f334 100644\n--- a/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/metadata/schema/TabletMetadataTest.java\n@@ -44,6 +44,7 @@\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.lang.reflect.Constructor;\n+import java.time.Duration;\n import java.util.AbstractMap;\n import java.util.EnumSet;\n import java.util.LinkedHashSet;\n@@ -643,6 +644,8 @@ public void testBuilder() {\n         .putFile(sf1, dfv1).putFile(sf2, dfv2).putBulkFile(rf1, loadedFateId1)\n         .putBulkFile(rf2, loadedFateId2).putFlushId(27).putDirName(\""dir1\"").putScan(sf3).putScan(sf4)\n         .putCompacted(compactFateId1).putCompacted(compactFateId2).putCloned()\n+        .putTabletMergeability(\n+            TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS)))\n         .build(ECOMP, HOSTING_REQUESTED, MERGED, USER_COMPACTION_REQUESTED, UNSPLITTABLE);\n \n     assertEquals(extent, tm.getExtent());\n@@ -662,6 +665,8 @@ public void testBuilder() {\n     assertFalse(tm.hasMerged());\n     assertNull(tm.getUnSplittable());\n     assertEquals(\""OK\"", tm.getCloned());\n+    assertEquals(TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS)),\n+        tm.getTabletMergeability());\n     assertThrows(IllegalStateException.class, tm::getOperationId);\n     assertThrows(IllegalStateException.class, tm::getSuspend);\n     assertThrows(IllegalStateException.class, tm::getTime);\n@@ -688,6 +693,7 @@ public void testBuilder() {\n     assertThrows(IllegalStateException.class, tm2::hasMerged);\n     assertThrows(IllegalStateException.class, tm2::getUserCompactionsRequested);\n     assertThrows(IllegalStateException.class, tm2::getUnSplittable);\n+    assertThrows(IllegalStateException.class, tm2::getTabletAvailability);\n \n     var ecid1 = ExternalCompactionId.generate(UUID.randomUUID());\n     CompactionMetadata ecm =\n@@ -707,7 +713,10 @@ public void testBuilder() {\n         .putSuspension(ser1, SteadyTime.from(45L, TimeUnit.MILLISECONDS))\n         .putTime(new MetadataTime(479, TimeType.LOGICAL)).putWal(le1).putWal(le2)\n         .setHostingRequested().putSelectedFiles(selFiles).setMerged()\n-        .putUserCompactionRequested(selFilesFateId).setUnSplittable(unsplittableMeta).build();\n+        .putUserCompactionRequested(selFilesFateId).setUnSplittable(unsplittableMeta)\n+        .putTabletMergeability(TabletMergeabilityMetadata.after(Duration.ofDays(3),\n+            SteadyTime.from(45L, TimeUnit.MILLISECONDS)))\n+        .build();\n \n     assertEquals(Set.of(ecid1), tm3.getExternalCompactions().keySet());\n     assertEquals(Set.of(sf1, sf2), tm3.getExternalCompactions().get(ecid1).getJobFiles());\n@@ -724,6 +733,11 @@ public void testBuilder() {\n     assertTrue(tm3.hasMerged());\n     assertTrue(tm3.getUserCompactionsRequested().contains(selFilesFateId));\n     assertEquals(unsplittableMeta, tm3.getUnSplittable());\n+    var tmm = tm3.getTabletMergeability();\n+    assertEquals(Duration.ofDays(3), tmm.getTabletMergeability().getDelay().orElseThrow());\n+    assertEquals(SteadyTime.from(45L, TimeUnit.MILLISECONDS), tmm.getSteadyTime().orElseThrow());\n+    assertTrue(tmm.isMergeable(SteadyTime.from(Duration.ofHours(73))));\n+    assertFalse(tmm.isMergeable(SteadyTime.from(Duration.ofHours(72))));\n   }\n \n }\n\ndiff --git a/server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java b/server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java\nindex dfcf970330c..5407d10de58 100644\n--- a/server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java\n+++ b/server/base/src/test/java/org/apache/accumulo/server/constraints/MetadataConstraintsTest.java\n@@ -26,6 +26,7 @@\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.lang.reflect.Method;\n+import java.time.Duration;\n import java.util.Base64;\n import java.util.List;\n import java.util.Set;\n@@ -56,6 +57,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.UserCompactionRequestedColumnFamily;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.UnSplittableMetadata;\n import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.server.ServerContext;\n@@ -639,6 +641,53 @@ public void testDirectoryColumn() {\n     assertEquals((short) 3102, violations.get(0));\n   }\n \n+  @Test\n+  public void testMergeabilityColumn() {\n+    MetadataConstraints mc = new MetadataConstraints();\n+    Mutation m;\n+    List<Short> violations;\n+\n+    // Delay must be >= 0\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m,\n+        new Value(\""{\\\""delay\\\"":-1,\\\""steadyTime\\\"":1,\\\""never\\\""=false}\""));\n+    assertViolation(mc, m, (short) 4006);\n+\n+    // SteadyTime must be null if never is true\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m, new Value(\""{\\\""steadyTime\\\"":1,\\\""never\\\""=true}\""));\n+    assertViolation(mc, m, (short) 4006);\n+\n+    // delay must be null if never is true\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m, new Value(\""{\\\""delay\\\"":1,\\\""never\\\""=true}\""));\n+    assertViolation(mc, m, (short) 4006);\n+\n+    // SteadyTime must be set if delay positive\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m, new Value(\""{\\\""delay\\\"":10,\\\""never\\\""=false}\""));\n+    assertViolation(mc, m, (short) 4006);\n+\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m,\n+        TabletMergeabilityMetadata.toValue(TabletMergeabilityMetadata.never()));\n+    violations = mc.check(createEnv(), m);\n+    assertTrue(violations.isEmpty());\n+\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m, TabletMergeabilityMetadata\n+        .toValue(TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS))));\n+    violations = mc.check(createEnv(), m);\n+    assertTrue(violations.isEmpty());\n+\n+    m = new Mutation(new Text(\""0;foo\""));\n+    TabletColumnFamily.MERGEABILITY_COLUMN.put(m,\n+        TabletMergeabilityMetadata.toValue(TabletMergeabilityMetadata.after(Duration.ofDays(3),\n+            SteadyTime.from(Duration.ofHours(1)))));\n+    violations = mc.check(createEnv(), m);\n+    assertTrue(violations.isEmpty());\n+  }\n+\n   // Encode a row how it would appear in Json\n   private static String encodeRowForMetadata(String row) {\n     try {\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\nindex 76a475105e6..8fd15f78ec7 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/merge/MergeTabletsTest.java\n@@ -32,6 +32,7 @@\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOADED;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOCATION;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.LOGS;\n+import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGEABILITY;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.MERGED;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.OPID;\n import static org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType.PREV_ROW;\n@@ -73,6 +74,7 @@\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n import org.apache.accumulo.core.metadata.schema.MetadataTime;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadataBuilder;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n@@ -110,7 +112,7 @@ public class MergeTabletsTest {\n   private static final Set<TabletMetadata.ColumnType> COLUMNS_HANDLED_BY_MERGE =\n       EnumSet.of(TIME, LOGS, FILES, PREV_ROW, OPID, LOCATION, ECOMP, SELECTED, LOADED,\n           USER_COMPACTION_REQUESTED, MERGED, LAST, SCANS, DIR, CLONED, FLUSH_ID, FLUSH_NONCE,\n-          SUSPEND, AVAILABILITY, HOSTING_REQUESTED, COMPACTED, UNSPLITTABLE);\n+          SUSPEND, AVAILABILITY, HOSTING_REQUESTED, COMPACTED, UNSPLITTABLE, MERGEABILITY);\n \n   /**\n    * The purpose of this test is to catch new tablet metadata columns that were added w/o\n@@ -150,15 +152,18 @@ public void testManyColumns() throws Exception {\n     var availability = TabletAvailability.HOSTED;\n     var lastLocation = TabletMetadata.Location.last(\""1.2.3.4:1234\"", \""123456789\"");\n     var suspendingTServer = SuspendingTServer.fromValue(new Value(\""1.2.3.4:5|56\""));\n-\n-    var tablet1 = TabletMetadata.builder(ke1).putOperation(opid).putDirName(\""td1\"")\n-        .putFile(file3, dfv3).putTime(MetadataTime.parse(\""L3\""))\n-        .putTabletAvailability(TabletAvailability.HOSTED).build(LOCATION, LOGS, FILES, ECOMP,\n-            MERGED, COMPACTED, SELECTED, USER_COMPACTION_REQUESTED, LOADED, CLONED);\n-    var tablet2 = TabletMetadata.builder(ke2).putOperation(opid).putDirName(\""td2\"")\n-        .putFile(file4, dfv4).putTime(MetadataTime.parse(\""L2\""))\n-        .putTabletAvailability(TabletAvailability.HOSTED).build(LOCATION, LOGS, FILES, ECOMP,\n-            MERGED, COMPACTED, SELECTED, USER_COMPACTION_REQUESTED, LOADED, CLONED);\n+    var mergeability = TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS));\n+\n+    var tablet1 =\n+        TabletMetadata.builder(ke1).putOperation(opid).putDirName(\""td1\"").putFile(file3, dfv3)\n+            .putTime(MetadataTime.parse(\""L3\"")).putTabletAvailability(TabletAvailability.HOSTED)\n+            .putTabletMergeability(mergeability).build(LOCATION, LOGS, FILES, ECOMP, MERGED,\n+                COMPACTED, SELECTED, USER_COMPACTION_REQUESTED, LOADED, CLONED);\n+    var tablet2 =\n+        TabletMetadata.builder(ke2).putOperation(opid).putDirName(\""td2\"").putFile(file4, dfv4)\n+            .putTime(MetadataTime.parse(\""L2\"")).putTabletAvailability(TabletAvailability.HOSTED)\n+            .putTabletMergeability(mergeability).build(LOCATION, LOGS, FILES, ECOMP, MERGED,\n+                COMPACTED, SELECTED, USER_COMPACTION_REQUESTED, LOADED, CLONED);\n \n     var tabletFiles = Map.of(file1, dfv1, file2, dfv2);\n \n@@ -193,6 +198,7 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(lastTabletMeta.getSuspend()).andReturn(suspendingTServer).atLeastOnce();\n     EasyMock.expect(lastTabletMeta.getLast()).andReturn(lastLocation).atLeastOnce();\n     EasyMock.expect(lastTabletMeta.getUnSplittable()).andReturn(unsplittableMeta).atLeastOnce();\n+    EasyMock.expect(lastTabletMeta.getTabletMergeability()).andReturn(mergeability).atLeastOnce();\n \n     EasyMock.replay(lastTabletMeta, compactions);\n \n@@ -228,6 +234,10 @@ public void testManyColumns() throws Exception {\n       EasyMock.expect(tabletMutator.deleteSuspension()).andReturn(tabletMutator);\n       EasyMock.expect(tabletMutator.deleteLocation(lastLocation)).andReturn(tabletMutator);\n       EasyMock.expect(tabletMutator.deleteUnSplittable()).andReturn(tabletMutator);\n+      EasyMock\n+          .expect(tabletMutator.putTabletMergeability(\n+              TabletMergeabilityMetadata.always(SteadyTime.from(1, TimeUnit.SECONDS))))\n+          .andReturn(tabletMutator).once();\n \n     });\n \n@@ -376,17 +386,17 @@ public void testTime() throws Exception {\n           .putTime(MetadataTime.parse(times[0])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE);\n+              UNSPLITTABLE, MERGEABILITY);\n       var tablet2 = TabletMetadata.builder(ke2).putOperation(opid).putDirName(\""td2\"")\n           .putTime(MetadataTime.parse(times[1])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE);\n+              UNSPLITTABLE, MERGEABILITY);\n       var tablet3 = TabletMetadata.builder(ke3).putOperation(opid).putDirName(\""td3\"")\n           .putTime(MetadataTime.parse(times[2])).putTabletAvailability(TabletAvailability.HOSTED)\n           .build(LOCATION, LOGS, FILES, ECOMP, MERGED, COMPACTED, SELECTED,\n               USER_COMPACTION_REQUESTED, LOADED, CLONED, SCANS, HOSTING_REQUESTED, SUSPEND, LAST,\n-              UNSPLITTABLE);\n+              UNSPLITTABLE, MERGEABILITY);\n \n       testMerge(List.of(tablet1, tablet2, tablet3), tableId, null, null, tabletMutator -> {\n         EasyMock.expect(tabletMutator.putTime(MetadataTime.parse(\""L30\""))).andReturn(tabletMutator)\n@@ -396,6 +406,9 @@ public void testTime() throws Exception {\n         EasyMock.expect(tabletMutator.putPrevEndRow(ke1.prevEndRow())).andReturn(tabletMutator)\n             .once();\n         EasyMock.expect(tabletMutator.setMerged()).andReturn(tabletMutator).once();\n+        // Current default if not set is NEVER\n+        EasyMock.expect(tabletMutator.putTabletMergeability(TabletMergeabilityMetadata.never()))\n+            .andReturn(tabletMutator).once();\n       });\n     }\n \n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\nindex a317f8375a9..82f2e5949f8 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/tableOps/split/UpdateTabletsTest.java\n@@ -31,6 +31,7 @@\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n \n import org.apache.accumulo.core.client.admin.TabletAvailability;\n import org.apache.accumulo.core.data.TableId;\n@@ -49,12 +50,14 @@\n import org.apache.accumulo.core.metadata.schema.ExternalCompactionId;\n import org.apache.accumulo.core.metadata.schema.MetadataTime;\n import org.apache.accumulo.core.metadata.schema.SelectedFiles;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata.ColumnType;\n import org.apache.accumulo.core.metadata.schema.TabletOperationId;\n import org.apache.accumulo.core.metadata.schema.TabletOperationType;\n import org.apache.accumulo.core.metadata.schema.UnSplittableMetadata;\n import org.apache.accumulo.core.tabletserver.log.LogEntry;\n+import org.apache.accumulo.core.util.time.SteadyTime;\n import org.apache.accumulo.manager.Manager;\n import org.apache.accumulo.manager.split.Splitter;\n import org.apache.accumulo.server.ServerContext;\n@@ -83,13 +86,13 @@ Splitter.FileInfo newFileInfo(String start, String end) {\n    * developer has determined that split code can handle that column OR has opened an issue about\n    * handling it.\n    */\n-  private static final Set<ColumnType> COLUMNS_HANDLED_BY_SPLIT =\n-      EnumSet.of(ColumnType.TIME, ColumnType.LOGS, ColumnType.FILES, ColumnType.PREV_ROW,\n-          ColumnType.OPID, ColumnType.LOCATION, ColumnType.ECOMP, ColumnType.SELECTED,\n-          ColumnType.LOADED, ColumnType.USER_COMPACTION_REQUESTED, ColumnType.MERGED,\n-          ColumnType.LAST, ColumnType.SCANS, ColumnType.DIR, ColumnType.CLONED, ColumnType.FLUSH_ID,\n-          ColumnType.FLUSH_NONCE, ColumnType.SUSPEND, ColumnType.AVAILABILITY,\n-          ColumnType.HOSTING_REQUESTED, ColumnType.COMPACTED, ColumnType.UNSPLITTABLE);\n+  private static final Set<ColumnType> COLUMNS_HANDLED_BY_SPLIT = EnumSet.of(ColumnType.TIME,\n+      ColumnType.LOGS, ColumnType.FILES, ColumnType.PREV_ROW, ColumnType.OPID, ColumnType.LOCATION,\n+      ColumnType.ECOMP, ColumnType.SELECTED, ColumnType.LOADED,\n+      ColumnType.USER_COMPACTION_REQUESTED, ColumnType.MERGED, ColumnType.LAST, ColumnType.SCANS,\n+      ColumnType.DIR, ColumnType.CLONED, ColumnType.FLUSH_ID, ColumnType.FLUSH_NONCE,\n+      ColumnType.SUSPEND, ColumnType.AVAILABILITY, ColumnType.HOSTING_REQUESTED,\n+      ColumnType.COMPACTED, ColumnType.UNSPLITTABLE, ColumnType.MERGEABILITY);\n \n   /**\n    * The purpose of this test is to catch new tablet metadata columns that were added w/o\n@@ -230,6 +233,8 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(splitter.getCachedFileInfo(tableId, file3)).andReturn(newFileInfo(\""d\"", \""f\""));\n     EasyMock.expect(splitter.getCachedFileInfo(tableId, file4)).andReturn(newFileInfo(\""d\"", \""j\""));\n     EasyMock.expect(manager.getSplitter()).andReturn(splitter).atLeastOnce();\n+    EasyMock.expect(manager.getSteadyTime()).andReturn(SteadyTime.from(100_000, TimeUnit.SECONDS))\n+        .atLeastOnce();\n \n     ServiceLock managerLock = EasyMock.mock(ServiceLock.class);\n     EasyMock.expect(context.getServiceLock()).andReturn(managerLock).anyTimes();\n@@ -294,6 +299,11 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(tablet1Mutator.putFile(file1, new DataFileValue(333, 33, 20)))\n         .andReturn(tablet1Mutator);\n     EasyMock.expect(tablet1Mutator.putFile(file2, dfv2)).andReturn(tablet1Mutator);\n+    // SplitInfo marked as system generated so should be set to ALWAYS (0 delay)\n+    EasyMock\n+        .expect(tablet1Mutator.putTabletMergeability(\n+            TabletMergeabilityMetadata.always(SteadyTime.from(100_000, TimeUnit.SECONDS))))\n+        .andReturn(tablet1Mutator);\n     tablet1Mutator.submit(EasyMock.anyObject());\n     EasyMock.expectLastCall().once();\n     EasyMock.expect(tabletsMutator.mutateTablet(newExtent1)).andReturn(tablet1Mutator);\n@@ -310,6 +320,11 @@ public void testManyColumns() throws Exception {\n     EasyMock.expect(tablet2Mutator.putCompacted(ucfid1)).andReturn(tablet2Mutator);\n     EasyMock.expect(tablet2Mutator.putCompacted(ucfid3)).andReturn(tablet2Mutator);\n     EasyMock.expect(tablet2Mutator.putTabletAvailability(availability)).andReturn(tablet2Mutator);\n+    // SplitInfo marked as system generated so should be set to ALWAYS (0 delay)\n+    EasyMock\n+        .expect(tablet2Mutator.putTabletMergeability(\n+            TabletMergeabilityMetadata.always(SteadyTime.from(100_000, TimeUnit.SECONDS))))\n+        .andReturn(tablet2Mutator);\n     EasyMock.expect(tablet2Mutator.putBulkFile(loaded1.getTabletFile(), flid1))\n         .andReturn(tablet2Mutator);\n     EasyMock.expect(tablet2Mutator.putBulkFile(loaded2.getTabletFile(), flid2))\n@@ -367,7 +382,7 @@ public void testManyColumns() throws Exception {\n     // the original tablet\n     SortedSet<Text> splits = new TreeSet<>(List.of(newExtent1.endRow(), newExtent2.endRow()));\n     UpdateTablets updateTablets =\n-        new UpdateTablets(new SplitInfo(origExtent, splits), List.of(dir1, dir2));\n+        new UpdateTablets(new SplitInfo(origExtent, splits, true), List.of(dir1, dir2));\n     updateTablets.call(fateId, manager);\n \n     EasyMock.verify(manager, context, ample, tabletMeta, splitter, tabletsMutator, tablet1Mutator,\n@@ -446,7 +461,7 @@ private static void testError(KeyExtent origExtent, TabletMetadata tm1, FateId f\n     // the original tablet\n     SortedSet<Text> splits = new TreeSet<>(List.of(new Text(\""c\"")));\n     UpdateTablets updateTablets =\n-        new UpdateTablets(new SplitInfo(origExtent, splits), List.of(\""d1\""));\n+        new UpdateTablets(new SplitInfo(origExtent, splits, true), List.of(\""d1\""));\n     updateTablets.call(fateId, manager);\n \n     EasyMock.verify(manager, context, ample);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\nindex 2c8359bf4ef..45593012357 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/TestAmpleIT.java\n@@ -119,6 +119,7 @@ public void testCreateMetadata() throws Exception {\n           assertNotNull(tm.getExtent());\n           assertNotNull(tm.getTabletAvailability());\n           assertNotNull(tm.getTime());\n+          assertNotNull(tm.getTabletMergeability());\n           count.incrementAndGet();\n         });\n       }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java b/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\nindex 0b1c2b9e84c..4d8f8c44fd4 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ample/metadata/TestAmple.java\n@@ -53,6 +53,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.ServerColumnFamily;\n import org.apache.accumulo.core.metadata.schema.MetadataTime;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata.TableOptions;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -149,6 +150,7 @@ public void createMetadata(TableId tableId) {\n         tabletMutator.putDirName(dirName);\n         tabletMutator.putTime(new MetadataTime(0, TimeType.MILLIS));\n         tabletMutator.putTabletAvailability(TabletAvailability.HOSTED);\n+        tabletMutator.putTabletMergeability(TabletMergeabilityMetadata.never());\n         tabletMutator.mutate();\n       } catch (Exception e) {\n         throw new IllegalStateException(e);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\nindex a0cf45ee796..3b524d90bcb 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/ManagerRepoIT.java\n@@ -198,7 +198,7 @@ public void testSplitOffline() throws Exception {\n       assertEquals(opid, testAmple.readTablet(extent).getOperationId());\n \n       var eoRepo = new AllocateDirsAndEnsureOnline(\n-          new SplitInfo(extent, new TreeSet<>(List.of(new Text(\""sp1\"")))));\n+          new SplitInfo(extent, new TreeSet<>(List.of(new Text(\""sp1\""))), true));\n \n       // The repo should delete the opid and throw an exception\n       assertThrows(ThriftTableOperationException.class, () -> eoRepo.call(fateId, manager));\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\nindex 76c62a5460c..72ff2bb8ca3 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/AddSplitIT.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.accumulo.test.functional;\n \n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n import java.time.Duration;\n import java.util.Collection;\n import java.util.Iterator;\n@@ -30,7 +32,10 @@\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n+import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.harness.AccumuloClusterHarness;\n import org.apache.hadoop.io.Text;\n@@ -87,6 +92,13 @@ public void addSplitTest() throws Exception {\n       }\n \n       verifyData(c, tableName, 2L);\n+\n+      TableId id = TableId.of(c.tableOperations().tableIdMap().get(tableName));\n+      try (TabletsMetadata tm = getServerContext().getAmple().readTablets().forTable(id).build()) {\n+        // Default for user created tablets should be mergeability set to NEVER\n+        tm.stream().forEach(tablet -> assertEquals(TabletMergeabilityMetadata.never(),\n+            tablet.getTabletMergeability()));\n+      }\n     }\n   }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java b/test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java\nindex 5a4dd306cce..e805143e672 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/MetadataIT.java\n@@ -54,6 +54,7 @@\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.DeletesSection;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.DataFileColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletMetadata;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -284,10 +285,12 @@ private void testCommonSystemTableConfig(ClientContext client, TableId tableId,\n     assertEquals(maxVersions,\n         tableProps.get(Property.TABLE_ITERATOR_PREFIX.getKey() + \""majc.vers.opt.maxVersions\""));\n \n-    // Verify all tablets are HOSTED\n+    // Verify all tablets are HOSTED and Mergeablity is NEVER\n     try (var tablets = client.getAmple().readTablets().forTable(tableId).build()) {\n       assertTrue(\n           tablets.stream().allMatch(tm -> tm.getTabletAvailability() == TabletAvailability.HOSTED));\n+      assertTrue(tablets.stream()\n+          .allMatch(tm -> tm.getTabletMergeability().equals(TabletMergeabilityMetadata.never())));\n     }\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java b/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\nindex 720e56f93f7..a985d3969a6 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/SplitIT.java\n@@ -57,6 +57,7 @@\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n import org.apache.accumulo.core.client.admin.InstanceOperations;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n+import org.apache.accumulo.core.client.admin.TabletMergeability;\n import org.apache.accumulo.core.client.rfile.RFile;\n import org.apache.accumulo.core.client.rfile.RFileWriter;\n import org.apache.accumulo.core.conf.Property;\n@@ -68,6 +69,7 @@\n import org.apache.accumulo.core.metadata.AccumuloTable;\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.MetadataSchema.TabletsSection.TabletColumnFamily;\n+import org.apache.accumulo.core.metadata.schema.TabletMergeabilityMetadata;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.harness.AccumuloClusterHarness;\n@@ -250,6 +252,7 @@ public void tabletShouldSplit() throws Exception {\n         KeyExtent extent = new KeyExtent(id, null, null);\n         s.setRange(extent.toMetaRange());\n         TabletColumnFamily.PREV_ROW_COLUMN.fetch(s);\n+        TabletColumnFamily.MERGEABILITY_COLUMN.fetch(s);\n         int count = 0;\n         int shortened = 0;\n         for (Entry<Key,Value> entry : s) {\n@@ -257,6 +260,15 @@ public void tabletShouldSplit() throws Exception {\n           if (extent.endRow() != null && extent.endRow().toString().length() < 14) {\n             shortened++;\n           }\n+          if (TabletColumnFamily.MERGEABILITY_COLUMN.getColumnQualifier()\n+              .equals(entry.getKey().getColumnQualifier())) {\n+            // Default tablet should be set to NEVER, all newly generated system splits should be\n+            // set to ALWAYS\n+            var mergeability =\n+                extent.endRow() == null ? TabletMergeability.never() : TabletMergeability.always();\n+            assertEquals(mergeability,\n+                TabletMergeabilityMetadata.fromValue(entry.getValue()).getTabletMergeability());\n+          }\n           count++;\n         }\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5145"", ""pr_id"": 5145, ""issue_id"": 5132, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Watching the server lock differently\nThe half-dead TabletServer problem occurs when the TabletServer's ZooKeeper connection has timed out and the ephemeral lock in the ZooKeeper server has been removed, but the TabletServer's ZooKeeper Watcher has not been notified. When the lock watcher is notified, the TabletServer halts itself. When the lock watcher is not notified, the TabletServer may continue to try to write mutations for tablets that may be hosted somewhere else because the Manager has noticed that the ZooKeeper lock is missing and re-assigned the tablets.\r\n\r\nLooking in the ZooKeeper client code, Watcher notifiications occur in [ClientCnxn.EventThread.run](https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/ClientCnxn.java#L525), specifically in the call to `processEvent`. There is one EventThread created in the ClientCnxn object, so if the Watcher callback method takes a long time or is stuck, then the lock loss notification may not occur in a timely fashion.\r\n\r\nOne solution might be to create a new Thread in the server processes that uses a different ZooKeeper client object specifically to watch the server lock. This Thread could call `ZooKeeper.exists(path, false)` periodically to check that that lock node exists without using a Watcher. This thread could run at a higher priority so that it has a higher probability of getting cpu time when under load, but use `Thread.sleep` between iterations so that it's yielding for some amount of time for other threads."", ""issue_word_count"": 249, ""test_files_count"": 2, ""non_test_files_count"": 9, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/conf/Property.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ServiceLock.java"", ""server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java"", ""server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java"", ""server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java"", ""server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/Manager.java"", ""server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java"", ""test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java""], ""pr_changed_test_files"": [""test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java""], ""base_commit"": ""fdb49e5a92841607670c76884ef7a80d4627a931"", ""head_commit"": ""a3ce01d2189e219453507cac9028372378ee891b"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5145"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5145"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-07T15:24:45.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 0fa037366cb..b1a658ea2f6 100644\n--- a/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -329,6 +329,11 @@ public enum Property {\n           + \"" was changed and it now can accept multiple class names. The metrics spi was introduced in 2.1.3,\""\n           + \"" the deprecated factory is org.apache.accumulo.core.metrics.MeterRegistryFactory.\"",\n       \""2.1.0\""),\n+  GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL(\""general.server.lock.verification.interval\"", \""0\"",\n+      PropertyType.TIMEDURATION,\n+      \""Interval at which the Manager and TabletServer should verify their server locks. A value of zero\""\n+          + \"" disables this check.\"",\n+      \""2.1.4\""),\n   // properties that are specific to manager server behavior\n   MANAGER_PREFIX(\""manager.\"", null, PropertyType.PREFIX,\n       \""Properties in this category affect the behavior of the manager server. \""\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ServiceLock.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ServiceLock.java\nindex 63807d0f374..375c2ef665a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ServiceLock.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ServiceLock.java\n@@ -762,4 +762,27 @@ public static boolean deleteLock(ZooReaderWriter zk, ServiceLockPath path, Strin\n \n     return false;\n   }\n+\n+  /**\n+   * Checks that the lock still exists in ZooKeeper. The typical mechanism for determining if a lock\n+   * is lost depends on a Watcher set on the lock node. There exists a case where the Watcher may\n+   * not get called if another Watcher is stuck waiting on I/O or otherwise hung. In the case where\n+   * this method returns false, then the typical action is to exit the server process.\n+   *\n+   * @return true if lock path still exists, false otherwise and on error\n+   */\n+  public boolean verifyLockAtSource() {\n+    final String lockPath = getLockPath();\n+    if (lockPath == null) {\n+      // lock not set yet or lock was lost\n+      return false;\n+    }\n+    try {\n+      return null != this.zooKeeper.exists(lockPath, false);\n+    } catch (KeeperException | InterruptedException | RuntimeException e) {\n+      LOG.error(\""Error verfiying lock at {}\"", lockPath, e);\n+      return false;\n+    }\n+  }\n+\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java b/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\nindex edfc567109d..c65314a0f12 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/AbstractServer.java\n@@ -19,6 +19,7 @@\n package org.apache.accumulo.server;\n \n import java.util.Objects;\n+import java.util.OptionalInt;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n@@ -26,13 +27,18 @@\n import org.apache.accumulo.core.classloader.ClassLoaderUtil;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.fate.zookeeper.ServiceLock;\n import org.apache.accumulo.core.metrics.MetricsProducer;\n import org.apache.accumulo.core.trace.TraceUtil;\n+import org.apache.accumulo.core.util.Halt;\n+import org.apache.accumulo.core.util.threads.Threads;\n import org.apache.accumulo.server.metrics.ProcessMetrics;\n import org.apache.accumulo.server.security.SecurityUtil;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.base.Preconditions;\n+\n import io.micrometer.core.instrument.MeterRegistry;\n \n public abstract class AbstractServer implements AutoCloseable, MetricsProducer, Runnable {\n@@ -44,6 +50,8 @@ public abstract class AbstractServer implements AutoCloseable, MetricsProducer,\n   private final ProcessMetrics processMetrics;\n   protected final long idleReportingPeriodNanos;\n   private volatile long idlePeriodStartNanos = 0L;\n+  private volatile Thread serverThread;\n+  private volatile Thread verificationThread;\n \n   protected AbstractServer(String appName, ServerOpts opts, String[] args) {\n     this.log = LoggerFactory.getLogger(getClass().getName());\n@@ -99,10 +107,14 @@ protected void updateIdleStatus(boolean isIdle) {\n    */\n   public void runServer() throws Exception {\n     final AtomicReference<Throwable> err = new AtomicReference<>();\n-    Thread service = new Thread(TraceUtil.wrap(this), applicationName);\n-    service.setUncaughtExceptionHandler((thread, exception) -> err.set(exception));\n-    service.start();\n-    service.join();\n+    serverThread = new Thread(TraceUtil.wrap(this), applicationName);\n+    serverThread.setUncaughtExceptionHandler((thread, exception) -> err.set(exception));\n+    serverThread.start();\n+    serverThread.join();\n+    if (verificationThread != null) {\n+      verificationThread.interrupt();\n+      verificationThread.join();\n+    }\n     Throwable thrown = err.get();\n     if (thrown != null) {\n       if (thrown instanceof Error) {\n@@ -139,6 +151,53 @@ public String getApplicationName() {\n     return applicationName;\n   }\n \n+  /**\n+   * Get the ServiceLock for this server process. May return null if called before the lock is\n+   * acquired.\n+   *\n+   * @return lock ServiceLock or null\n+   */\n+  public abstract ServiceLock getLock();\n+\n+  public void startServiceLockVerificationThread() {\n+    Preconditions.checkState(verificationThread == null,\n+        \""verification thread not null, startServiceLockVerificationThread likely called twice\"");\n+    Preconditions.checkState(serverThread != null,\n+        \""server thread is null, no server process is running\"");\n+    final long interval =\n+        getConfiguration().getTimeInMillis(Property.GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL);\n+    if (interval > 0) {\n+      verificationThread = Threads.createThread(\""service-lock-verification-thread\"",\n+          OptionalInt.of(Thread.NORM_PRIORITY + 1), () -> {\n+            while (serverThread.isAlive()) {\n+              ServiceLock lock = getLock();\n+              try {\n+                log.trace(\n+                    \""ServiceLockVerificationThread - checking ServiceLock existence in ZooKeeper\"");\n+                if (lock != null && !lock.verifyLockAtSource()) {\n+                  Halt.halt(\""Lock verification thread could not find lock\"", -1);\n+                }\n+                // Need to sleep, not yield when the thread priority is greater than NORM_PRIORITY\n+                // so that this thread does not get immediately rescheduled.\n+                log.trace(\n+                    \""ServiceLockVerificationThread - ServiceLock exists in ZooKeeper, sleeping for {}ms\"",\n+                    interval);\n+                Thread.sleep(interval);\n+              } catch (InterruptedException e) {\n+                if (serverThread.isAlive()) {\n+                  // throw an Error, which will cause this process to be terminated\n+                  throw new Error(\""Sleep interrupted in ServiceLock verification thread\"");\n+                }\n+              }\n+            }\n+          });\n+      verificationThread.start();\n+    } else {\n+      log.info(\""ServiceLockVerificationThread not started as \""\n+          + Property.GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL.getKey() + \"" is zero\"");\n+    }\n+  }\n+\n   @Override\n   public void close() {}\n \n\ndiff --git a/server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java b/server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java\nindex b6130672e1f..b735d8544dc 100644\n--- a/server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java\n+++ b/server/compaction-coordinator/src/main/java/org/apache/accumulo/coordinator/CompactionCoordinator.java\n@@ -768,6 +768,11 @@ private void cleanUpCompactors() {\n     }\n   }\n \n+  @Override\n+  public ServiceLock getLock() {\n+    return coordinatorLock;\n+  }\n+\n   public static void main(String[] args) throws Exception {\n     try (CompactionCoordinator compactor = new CompactionCoordinator(new ServerOpts(), args)) {\n       compactor.runServer();\n\ndiff --git a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\nindex e03032326f3..b7426ce31bc 100644\n--- a/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n+++ b/server/compactor/src/main/java/org/apache/accumulo/compactor/Compactor.java\n@@ -987,4 +987,10 @@ public String getRunningCompactionId(TInfo tinfo, TCredentials credentials)\n       return eci.canonical();\n     }\n   }\n+\n+  @Override\n+  public ServiceLock getLock() {\n+    return compactorLock;\n+  }\n+\n }\n\ndiff --git a/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java b/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\nindex ce0e8682bb1..bd78388836e 100644\n--- a/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\n+++ b/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java\n@@ -78,6 +78,7 @@ public class SimpleGarbageCollector extends AbstractServer implements Iface {\n       new GCStatus(new GcCycleStats(), new GcCycleStats(), new GcCycleStats(), new GcCycleStats());\n \n   private final GcCycleMetrics gcCycleMetrics = new GcCycleMetrics();\n+  private ServiceLock gcLock;\n \n   SimpleGarbageCollector(ServerOpts opts, String[] args) {\n     super(\""gc\"", opts, args);\n@@ -379,10 +380,9 @@ public void unableToMonitorLockNode(final Exception e) {\n     };\n \n     UUID zooLockUUID = UUID.randomUUID();\n-    ServiceLock lock =\n-        new ServiceLock(getContext().getZooReaderWriter().getZooKeeper(), path, zooLockUUID);\n+    gcLock = new ServiceLock(getContext().getZooReaderWriter().getZooKeeper(), path, zooLockUUID);\n     while (true) {\n-      if (lock.tryLock(lockWatcher,\n+      if (gcLock.tryLock(lockWatcher,\n           new ServerServices(addr.toString(), Service.GC_CLIENT).toString().getBytes(UTF_8))) {\n         log.debug(\""Got GC ZooKeeper lock\"");\n         return;\n@@ -439,4 +439,9 @@ public GcCycleMetrics getGcCycleMetrics() {\n     return gcCycleMetrics;\n   }\n \n+  @Override\n+  public ServiceLock getLock() {\n+    return gcLock;\n+  }\n+\n }\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\nindex 55255751531..a3540fa062a 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/Manager.java\n@@ -1714,6 +1714,7 @@ private void getManagerLock(final ServiceLockPath zManagerLoc)\n       managerLockWatcher.waitForChange();\n \n       if (managerLockWatcher.acquiredLock) {\n+        startServiceLockVerificationThread();\n         break;\n       }\n \n@@ -2001,4 +2002,10 @@ void getAssignments(SortedMap<TServerInstance,TabletServerStatus> currentStatus,\n         assignedOut);\n     tabletBalancer.getAssignments(params);\n   }\n+\n+  @Override\n+  public ServiceLock getLock() {\n+    return managerLock;\n+  }\n+\n }\n\ndiff --git a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\nindex 67fc46015d6..4108282bcb3 100644\n--- a/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n+++ b/server/monitor/src/main/java/org/apache/accumulo/monitor/Monitor.java\n@@ -1048,4 +1048,9 @@ public Optional<HostAndPort> getCoordinatorHost() {\n   public int getLivePort() {\n     return livePort;\n   }\n+\n+  @Override\n+  public ServiceLock getLock() {\n+    return monitorLock;\n+  }\n }\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\nindex 1df158cf013..54764b86725 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java\n@@ -483,7 +483,8 @@ private void splitTablet(Tablet tablet) {\n     }\n   }\n \n-  TreeMap<KeyExtent,TabletData> splitTablet(Tablet tablet, byte[] splitPoint) throws IOException {\n+  protected TreeMap<KeyExtent,TabletData> splitTablet(Tablet tablet, byte[] splitPoint)\n+      throws IOException {\n     long t1 = System.currentTimeMillis();\n \n     TreeMap<KeyExtent,TabletData> tabletInfo = tablet.split(splitPoint);\n@@ -708,6 +709,7 @@ public void unableToMonitorLockNode(final Exception e) {\n           lockSessionId = tabletServerLock.getSessionId();\n           log.debug(\""Obtained tablet server lock {} {}\"", tabletServerLock.getLockPath(),\n               getTabletSession());\n+          startServiceLockVerificationThread();\n           return;\n         }\n         log.info(\""Waiting for tablet server lock\"");\n"", ""test_patch"": ""diff --git a/test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java b/test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java\nindex 343f07e72a0..75ba3d6c36c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/zookeeper/ServiceLockIT.java\n@@ -217,6 +217,7 @@ public void testDeleteParent() throws Exception {\n     ServiceLock zl = getZooLock(parent, UUID.randomUUID());\n \n     assertFalse(zl.isLocked());\n+    assertFalse(zl.verifyLockAtSource());\n \n     ZooReaderWriter zk = szk.getZooReaderWriter();\n \n@@ -235,10 +236,12 @@ public void testDeleteParent() throws Exception {\n \n     assertTrue(lw.locked);\n     assertTrue(zl.isLocked());\n+    assertTrue(zl.verifyLockAtSource());\n     assertNull(lw.exception);\n     assertNull(lw.reason);\n \n     zl.unlock();\n+    assertFalse(zl.verifyLockAtSource());\n   }\n \n   @Test\n@@ -250,6 +253,7 @@ public void testNoParent() throws Exception {\n     ServiceLock zl = getZooLock(parent, UUID.randomUUID());\n \n     assertFalse(zl.isLocked());\n+    assertFalse(zl.verifyLockAtSource());\n \n     TestALW lw = new TestALW();\n \n@@ -259,6 +263,7 @@ public void testNoParent() throws Exception {\n \n     assertFalse(lw.locked);\n     assertFalse(zl.isLocked());\n+    assertFalse(zl.verifyLockAtSource());\n     assertNotNull(lw.exception);\n     assertNull(lw.reason);\n   }\n@@ -275,6 +280,7 @@ public void testDeleteLock() throws Exception {\n     ServiceLock zl = getZooLock(parent, UUID.randomUUID());\n \n     assertFalse(zl.isLocked());\n+    assertFalse(zl.verifyLockAtSource());\n \n     TestALW lw = new TestALW();\n \n@@ -284,6 +290,7 @@ public void testDeleteLock() throws Exception {\n \n     assertTrue(lw.locked);\n     assertTrue(zl.isLocked());\n+    assertTrue(zl.verifyLockAtSource());\n     assertNull(lw.exception);\n     assertNull(lw.reason);\n \n@@ -293,7 +300,7 @@ public void testDeleteLock() throws Exception {\n \n     assertEquals(LockLossReason.LOCK_DELETED, lw.reason);\n     assertNull(lw.exception);\n-\n+    assertFalse(zl.verifyLockAtSource());\n   }\n \n   @Test\n@@ -308,6 +315,7 @@ public void testDeleteWaiting() throws Exception {\n     ServiceLock zl = getZooLock(parent, UUID.randomUUID());\n \n     assertFalse(zl.isLocked());\n+    assertFalse(zl.verifyLockAtSource());\n \n     TestALW lw = new TestALW();\n \n@@ -317,6 +325,7 @@ public void testDeleteWaiting() throws Exception {\n \n     assertTrue(lw.locked);\n     assertTrue(zl.isLocked());\n+    assertTrue(zl.verifyLockAtSource());\n     assertNull(lw.exception);\n     assertNull(lw.reason);\n \n@@ -328,6 +337,7 @@ public void testDeleteWaiting() throws Exception {\n \n     assertFalse(lw2.locked);\n     assertFalse(zl2.isLocked());\n+    assertFalse(zl2.verifyLockAtSource());\n \n     ServiceLock zl3 = getZooLock(parent, UUID.randomUUID());\n \n@@ -356,10 +366,12 @@ public void testDeleteWaiting() throws Exception {\n \n     assertTrue(lw3.locked);\n     assertTrue(zl3.isLocked());\n+    assertTrue(zl3.verifyLockAtSource());\n     assertNull(lw3.exception);\n     assertNull(lw3.reason);\n \n     zl3.unlock();\n+    assertFalse(zl3.verifyLockAtSource());\n \n   }\n \n@@ -380,6 +392,7 @@ public void testUnexpectedEvent() throws Exception {\n       ServiceLock zl = getZooLock(parent, UUID.randomUUID());\n \n       assertFalse(zl.isLocked());\n+      assertFalse(zl.verifyLockAtSource());\n \n       // would not expect data to be set on this node, but it should not cause problems.....\n       zk.setData(parent.toString(), \""foo\"".getBytes(UTF_8), -1);\n@@ -392,6 +405,7 @@ public void testUnexpectedEvent() throws Exception {\n \n       assertTrue(lw.locked);\n       assertTrue(zl.isLocked());\n+      assertTrue(zl.verifyLockAtSource());\n       assertNull(lw.exception);\n       assertNull(lw.reason);\n \n@@ -404,6 +418,7 @@ public void testUnexpectedEvent() throws Exception {\n \n       assertEquals(LockLossReason.LOCK_DELETED, lw.reason);\n       assertNull(lw.exception);\n+      assertFalse(zl.verifyLockAtSource());\n     }\n \n   }\n@@ -476,8 +491,13 @@ public void testLockSerial() throws Exception {\n       assertEquals(\""/zlretryLockSerial/zlock#00000000-0000-0000-0000-aaaaaaaaaaaa#0000000000\"",\n           zl2.getWatching());\n \n+      assertTrue(zl1.verifyLockAtSource());\n+      assertFalse(zl2.verifyLockAtSource());\n+\n       zl1.unlock();\n       assertFalse(zlw1.isLockHeld());\n+      assertFalse(zl1.verifyLockAtSource());\n+      assertFalse(zl2.verifyLockAtSource());\n       zk1.close();\n \n       while (!zlw2.isLockHeld()) {\n@@ -485,7 +505,9 @@ public void testLockSerial() throws Exception {\n       }\n \n       assertTrue(zlw2.isLockHeld());\n+      assertTrue(zl2.verifyLockAtSource());\n       zl2.unlock();\n+      assertFalse(zl2.verifyLockAtSource());\n     }\n \n   }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java b/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java\nnew file mode 100644\nindex 00000000000..b0b81623484\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/HalfDeadServerWatcherIT.java\n@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.test.functional;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.client.Accumulo;\n+import org.apache.accumulo.core.client.AccumuloClient;\n+import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.dataImpl.KeyExtent;\n+import org.apache.accumulo.core.fate.zookeeper.ZooUtil.NodeMissingPolicy;\n+import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.harness.AccumuloClusterHarness;\n+import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;\n+import org.apache.accumulo.server.ServerContext;\n+import org.apache.accumulo.server.ServerOpts;\n+import org.apache.accumulo.test.util.Wait;\n+import org.apache.accumulo.tserver.TabletServer;\n+import org.apache.accumulo.tserver.tablet.Tablet;\n+import org.apache.accumulo.tserver.tablet.TabletData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Text;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.WatchedEvent;\n+import org.apache.zookeeper.Watcher;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Test that validates that the TabletServer will be terminated when the lock is removed in\n+ * ZooKeeper, but a Watcher in the TabletServer is preventing the LockWatcher to be invoked.\n+ */\n+public class HalfDeadServerWatcherIT extends AccumuloClusterHarness {\n+\n+  public static class HalfDeadTabletServer extends TabletServer {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HalfDeadTabletServer.class);\n+\n+    public static void main(String[] args) throws Exception {\n+      try (HalfDeadTabletServer tserver = new HalfDeadTabletServer(new ServerOpts(), args)) {\n+        tserver.runServer();\n+      }\n+    }\n+\n+    public static class StuckWatcher implements Watcher {\n+      private static final Logger LOG = LoggerFactory.getLogger(StuckWatcher.class);\n+\n+      @Override\n+      public void process(WatchedEvent event) {\n+        LOG.info(\""started sleeping...\"");\n+        while (true) {\n+          LOG.info(\""still sleeping...\"");\n+          UtilWaitThread.sleep(2000);\n+        }\n+      }\n+\n+    }\n+\n+    protected HalfDeadTabletServer(ServerOpts opts, String[] args) {\n+      super(opts, args);\n+    }\n+\n+    @Override\n+    protected TreeMap<KeyExtent,TabletData> splitTablet(Tablet tablet, byte[] splitPoint)\n+        throws IOException {\n+      LOG.info(\""In HalfDeadServerWatcherIT::splitTablet\"");\n+      TreeMap<KeyExtent,TabletData> results = super.splitTablet(tablet, splitPoint);\n+      if (!tablet.getExtent().isMeta()) {\n+        final TableId tid = tablet.getExtent().tableId();\n+        final String zooRoot = this.getContext().getZooKeeperRoot();\n+        final String tableZPath = zooRoot + Constants.ZTABLES + \""/\"" + tid.canonical();\n+        try {\n+          this.getContext().getZooReaderWriter().exists(tableZPath, new StuckWatcher());\n+        } catch (KeeperException | InterruptedException e) {\n+          LOG.error(\""Error setting watch at: {}\"", tableZPath, e);\n+        }\n+        LOG.info(\""Set StuckWatcher at: {}\"", tableZPath);\n+      }\n+      return results;\n+    }\n+  }\n+\n+  private static final AtomicBoolean USE_VERIFICATION_THREAD = new AtomicBoolean(false);\n+\n+  @Override\n+  public void configureMiniCluster(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {\n+    if (USE_VERIFICATION_THREAD.get()) {\n+      cfg.setProperty(Property.GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL, \""10s\"");\n+    } else {\n+      cfg.setProperty(Property.GENERAL_SERVER_LOCK_VERIFICATION_INTERVAL, \""0\"");\n+    }\n+    cfg.setServerClass(ServerType.TABLET_SERVER, HalfDeadTabletServer.class);\n+    cfg.setNumCompactors(0);\n+    cfg.setNumScanServers(0);\n+    cfg.setNumTservers(1);\n+  }\n+\n+  @AfterEach\n+  public void afterTest() throws Exception {\n+    getCluster().getClusterControl().stopAllServers(ServerType.TABLET_SERVER);\n+    super.teardownCluster();\n+    USE_VERIFICATION_THREAD.set(!USE_VERIFICATION_THREAD.get());\n+  }\n+\n+  @Test\n+  public void testOne() throws Exception {\n+    if (USE_VERIFICATION_THREAD.get()) {\n+      // This test should use the verification thread, which should\n+      // end the TabletServer, throw an Exception on the ping call,\n+      // and return true\n+      assertTrue(testTabletServerWithStuckWatcherDies());\n+    } else {\n+      // This test should time out\n+      IllegalStateException e =\n+          assertThrows(IllegalStateException.class, () -> testTabletServerWithStuckWatcherDies());\n+      assertTrue(e.getMessage().contains(\""Timeout exceeded\""));\n+    }\n+  }\n+\n+  @Test\n+  public void testTwo() throws Exception {\n+    if (USE_VERIFICATION_THREAD.get()) {\n+      // This test should use the verification thread, which should\n+      // end the TabletServer, throw an Exception on the ping call,\n+      // and return true\n+      assertTrue(testTabletServerWithStuckWatcherDies());\n+    } else {\n+      // This test should time out\n+      IllegalStateException e =\n+          assertThrows(IllegalStateException.class, () -> testTabletServerWithStuckWatcherDies());\n+      assertTrue(e.getMessage().contains(\""Timeout exceeded\""));\n+    }\n+  }\n+\n+  public boolean testTabletServerWithStuckWatcherDies() throws Exception {\n+    try (AccumuloClient client = Accumulo.newClient().from(getClientProps()).build()) {\n+      String tableName = getUniqueNames(1)[0];\n+      client.tableOperations().create(tableName);\n+\n+      // add splits to the table, which should set a StuckWatcher on the table node in zookeeper\n+      TreeSet<Text> splits = new TreeSet<>();\n+      splits.add(new Text(\""j\""));\n+      splits.add(new Text(\""t\""));\n+      client.tableOperations().addSplits(tableName, splits);\n+\n+      // delete the table, which should invoke the watcher\n+      client.tableOperations().delete(tableName);\n+\n+      final List<String> tservers = client.instanceOperations().getTabletServers();\n+      assertEquals(1, tservers.size());\n+\n+      // Delete the lock for the TabletServer\n+      final ServerContext ctx = getServerContext();\n+      final String zooRoot = ctx.getZooKeeperRoot();\n+      ctx.getZooReaderWriter().recursiveDelete(\n+          zooRoot + Constants.ZTSERVERS + \""/\"" + tservers.get(0), NodeMissingPolicy.FAIL);\n+\n+      Wait.waitFor(() -> pingServer(client, tservers.get(0)) == false, 60_000);\n+      return true;\n+    }\n+\n+  }\n+\n+  private boolean pingServer(AccumuloClient client, String server) {\n+    final boolean lockVerificationThreadInUse = USE_VERIFICATION_THREAD.get();\n+    try {\n+      client.instanceOperations().ping(server);\n+      return true;\n+    } catch (AccumuloException e) {\n+      if (lockVerificationThreadInUse) {\n+        // If the lock verification thread is in use, the the TabletServer\n+        // should shut down and the call to ping will throw an Exception\n+        return false;\n+      } else {\n+        // With the lock verification thread disabled, the StuckWatcher\n+        // should prevent the TabletServer from shutting down during\n+        // this test method.\n+        fail(\""TabletServer unexpectedly shut down\"");\n+        return false;\n+      }\n+    }\n+\n+  }\n+\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5143"", ""pr_id"": 5143, ""issue_id"": 5047, ""repo"": ""apache/accumulo"", ""problem_statement"": ""The handling of children for non-existent nodes in ZooCache may not be correct.\n**Describe the bug**\r\n\r\nZooCache attempts to cache nonexistence.  This may not be done correctly for the case of getChildren.\r\n\r\nThe ZooCache code expects that ZooKeeper.getChildren() may return null [here](https://github.com/apache/accumulo/blob/cfae5e9abc7447aacc7136777f853fc0842c49de/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java#L316-L318).  However it will probably never return null.  When a node in zookeeper exists and has no children an empty list is returned.  When a node does not exist a NONODE KeeperException is thrown.  The code zoocache code is handling this, however it does not cache this.   The code behaves correctly, this is only a possible performance bug.\r\n\r\n**Expected behavior**\r\n\r\n * Remove handling for null in code as its not expected to ever happen.\r\n * Determine if its safe to cache non-existence for getChildren, which depends on ZK watcher behavior.  If it is safe then look into changing the code to cache this.  If not then only the null handling needs to be removed for this issue.\r\n"", ""issue_word_count"": 180, ""test_files_count"": 2, ""non_test_files_count"": 3, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZcNode.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java"", ""core/src/main/java/org/apache/accumulo/core/util/cache/Caches.java"", ""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java"", ""test/src/main/java/org/apache/accumulo/test/zookeeper/ZooCacheIT.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java"", ""test/src/main/java/org/apache/accumulo/test/zookeeper/ZooCacheIT.java""], ""base_commit"": ""a4ab7a48dd9e097288e776992d218ca84ef3634d"", ""head_commit"": ""8088c67dbf6d051941f9d9839928893a9287ae29"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5143"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5143"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-09T12:31:06.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZcNode.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZcNode.java\nnew file mode 100644\nindex 00000000000..bcd2b93872d\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZcNode.java\n@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.fate.zookeeper;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * Immutable data class used by zoo cache to hold what it is caching for single zookeeper node. Data\n+ * and children are obtained from zookeeper at different times. This class is structured so that\n+ * data can be obtained first and then children added later or visa veras.\n+ *\n+ * <p>\n+ * Four distinct states can be cached for a zookeeper node.\n+ * <ul>\n+ * <li>Can cache that a node does not exist in zookeeper. This state is represented by data, state,\n+ * and children all being null.</li>\n+ * <li>Can cache only the data for a zookeeper node. For this state data and stat are non-null while\n+ * children is null. Calling getChildren on node in this state will throw an exception.</li>\n+ * <li>Can cache only the children for a zookeeper node. For this state children is non-null while\n+ * data and stat are null. Calling getData or getStat on node in this state will throw an\n+ * exception.</li>\n+ * <li>Can cache the children and data for a zookeeper node. For this state data,stat, and children\n+ * are non-null.</li>\n+ * </ul>\n+ * <p>\n+ *\n+ */\n+class ZcNode {\n+\n+  private final byte[] data;\n+  private final ZooCache.ZcStat stat;\n+  private final List<String> children;\n+\n+  static final ZcNode NON_EXISTENT = new ZcNode();\n+\n+  private ZcNode() {\n+    this.data = null;\n+    this.stat = null;\n+    this.children = null;\n+  }\n+\n+  /**\n+   * Creates a new ZcNode that combines the data and stat from an existing ZcNode and sets the\n+   * children.\n+   */\n+  ZcNode(List<String> children, ZcNode existing) {\n+    Objects.requireNonNull(children);\n+    if (existing == null) {\n+      this.data = null;\n+      this.stat = null;\n+    } else {\n+      this.data = existing.data;\n+      this.stat = existing.stat;\n+    }\n+\n+    this.children = List.copyOf(children);\n+  }\n+\n+  /**\n+   * Creates a new ZcNode that combines the children from an existing ZcNode and sets the data and\n+   * stat.\n+   */\n+  ZcNode(byte[] data, ZooCache.ZcStat zstat, ZcNode existing) {\n+    this.data = Objects.requireNonNull(data);\n+    this.stat = Objects.requireNonNull(zstat);\n+    if (existing == null) {\n+      this.children = null;\n+    } else {\n+      this.children = existing.children;\n+    }\n+  }\n+\n+  /**\n+   * @return the data if the node exists and the data was set OR return null when the node does not\n+   *         exist\n+   * @throws IllegalStateException in the case where the node exists and the data was never set\n+   */\n+  byte[] getData() {\n+    Preconditions.checkState(cachedData());\n+    return data;\n+  }\n+\n+  /**\n+   * @return the stat if the node exists and the stat was set OR return null when the node does not\n+   *         exist\n+   * @throws IllegalStateException in the case where the node exists and the data was never set\n+   */\n+  ZooCache.ZcStat getStat() {\n+    Preconditions.checkState(cachedData());\n+    return stat;\n+  }\n+\n+  /**\n+   * @return the children if the node exists and the children were set OR return null when the node\n+   *         does not exist exists\n+   * @throws IllegalStateException in the case where the node exists and the children were never set\n+   */\n+  List<String> getChildren() {\n+    Preconditions.checkState(cachedChildren());\n+    return children;\n+  }\n+\n+  /**\n+   * @return true if the node does not exists or it exists and children are cached.\n+   */\n+  boolean cachedChildren() {\n+    return children != null || notExists();\n+  }\n+\n+  /**\n+   * @return true if the node does not exists or it exists and data and stat cached.\n+   */\n+  boolean cachedData() {\n+    return data != null || notExists();\n+  }\n+\n+  /**\n+   * @return true if the node does not exists in zookeeper\n+   */\n+  boolean notExists() {\n+    return stat == null && data == null && children == null;\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java\nindex 86b869fa152..ae8e752b20a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/ZooCache.java\n@@ -21,10 +21,11 @@\n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.accumulo.core.util.LazySingletons.RANDOM;\n \n+import java.time.Duration;\n import java.util.ConcurrentModificationException;\n import java.util.List;\n import java.util.Optional;\n-import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.locks.LockSupport;\n import java.util.function.Predicate;\n@@ -32,6 +33,7 @@\n import org.apache.accumulo.core.lock.ServiceLock;\n import org.apache.accumulo.core.lock.ServiceLockData;\n import org.apache.accumulo.core.lock.ServiceLockPaths.ServiceLockPath;\n+import org.apache.accumulo.core.util.cache.Caches;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.KeeperException.Code;\n import org.apache.zookeeper.WatchedEvent;\n@@ -41,6 +43,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.RemovalListener;\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n \n@@ -56,49 +60,10 @@ public class ZooCache {\n   private static final AtomicLong nextCacheId = new AtomicLong(0);\n   private final String cacheId = \""ZC\"" + nextCacheId.incrementAndGet();\n \n-  private static class ZcNode {\n-    final byte[] data;\n-    final ZcStat stat;\n-    final boolean dataSet;\n-    final List<String> children;\n-    final boolean childrenSet;\n-\n-    private ZcNode(ZcNode other, List<String> children) {\n-      this.data = other != null ? other.data : null;\n-      this.stat = other != null ? other.stat : null;\n-      this.dataSet = other != null ? other.dataSet : false;\n-      this.children = children;\n-      this.childrenSet = true;\n-    }\n-\n-    public ZcNode(byte[] data, ZcStat zstat, ZcNode zcn) {\n-      this.data = data;\n-      this.stat = zstat;\n-      this.dataSet = true;\n-      this.children = zcn != null ? zcn.children : null;\n-      this.childrenSet = zcn != null ? zcn.childrenSet : false;\n-    }\n-\n-    byte[] getData() {\n-      Preconditions.checkState(dataSet);\n-      return data;\n-    }\n-\n-    ZcStat getStat() {\n-      Preconditions.checkState(dataSet);\n-      return stat;\n-    }\n-\n-    List<String> getChildren() {\n-      Preconditions.checkState(childrenSet);\n-      return children;\n-    }\n-  }\n-\n-  // ConcurrentHashMap will only allow one thread to run at a time for a given key and this\n-  // implementation relies on that. Not all concurrent map implementations have this behavior for\n+  // The concurrent map returned by Caffiene will only allow one thread to run at a time for a given\n+  // key and ZooCache relies on that. Not all concurrent map implementations have this behavior for\n   // their compute functions.\n-  private final ConcurrentHashMap<String,ZcNode> nodeCache;\n+  private final ConcurrentMap<String,ZcNode> nodeCache;\n \n   private final ZooReader zReader;\n \n@@ -161,6 +126,8 @@ public void process(WatchedEvent event) {\n         case NodeChildrenChanged:\n         case NodeCreated:\n         case NodeDeleted:\n+        case ChildWatchRemoved:\n+        case DataWatchRemoved:\n           remove(event.getPath());\n           break;\n         case None:\n@@ -206,9 +173,30 @@ public void process(WatchedEvent event) {\n    * @param watcher watcher object\n    */\n   public ZooCache(ZooReader reader, Watcher watcher) {\n+    this(reader, watcher, Duration.ofMinutes(3));\n+  }\n+\n+  public ZooCache(ZooReader reader, Watcher watcher, Duration timeout) {\n     this.zReader = reader;\n-    nodeCache = new ConcurrentHashMap<>();\n     this.externalWatcher = watcher;\n+    RemovalListener<String,ZcNode> removalListerner = (path, zcNode, reason) -> {\n+      try {\n+        log.trace(\""{} removing watches for {} because {}\"", cacheId, path, reason);\n+        reader.getZooKeeper().removeWatches(path, ZooCache.this.watcher, Watcher.WatcherType.Any,\n+            false);\n+      } catch (InterruptedException | KeeperException | RuntimeException e) {\n+        log.warn(\""{} failed to remove watches on path {} in zookeeper\"", cacheId, path, e);\n+      }\n+    };\n+    // Must register the removal listener using evictionListener inorder for removal to be mutually\n+    // exclusive with any other operations on the same path. This is important for watcher\n+    // consistency, concurrently adding and removing watches for the same path would leave zoocache\n+    // in a really bad state. The cache builder has another way to register a removal listener that\n+    // is not mutually exclusive.\n+    Cache<String,ZcNode> cache =\n+        Caches.getInstance().createNewBuilder(Caches.CacheName.ZOO_CACHE, false)\n+            .expireAfterAccess(timeout).evictionListener(removalListerner).build();\n+    nodeCache = cache.asMap();\n     log.trace(\""{} created new cache\"", cacheId, new Exception());\n   }\n \n@@ -316,43 +304,46 @@ public List<String> getChildren(final String zPath) {\n       public List<String> run() throws KeeperException, InterruptedException {\n \n         var zcNode = nodeCache.get(zPath);\n-        if (zcNode != null && zcNode.childrenSet) {\n+        if (zcNode != null && zcNode.cachedChildren()) {\n           return zcNode.getChildren();\n         }\n \n         log.trace(\""{} {} was not in children cache, looking up in zookeeper\"", cacheId, zPath);\n \n-        try {\n-          zcNode = nodeCache.compute(zPath, (zp, zcn) -> {\n-            // recheck the children now that lock is held on key\n-            if (zcn != null && zcn.childrenSet) {\n-              return zcn;\n-            }\n+        zcNode = nodeCache.compute(zPath, (zp, zcn) -> {\n+          // recheck the children now that lock is held on key\n+          if (zcn != null && zcn.cachedChildren()) {\n+            return zcn;\n+          }\n \n-            try {\n-              final ZooKeeper zooKeeper = getZooKeeper();\n-              List<String> children;\n-              children = zooKeeper.getChildren(zPath, watcher);\n-              if (children != null) {\n-                children = List.copyOf(children);\n-              }\n-              return new ZcNode(zcn, children);\n-            } catch (KeeperException e) {\n-              throw new ZcException(e);\n-            } catch (InterruptedException e) {\n-              throw new ZcInterruptedException(e);\n+          try {\n+            final ZooKeeper zooKeeper = getZooKeeper();\n+            // Register a watcher on the node to monitor creation/deletion events for the node. It\n+            // is possible that an event from this watch could trigger prior to calling getChildren.\n+            // That is ok because the compute() call on the map has a lock and processing the event\n+            // will block until compute() returns. After compute() returns the event processing\n+            // would clear the map entry.\n+            Stat stat = zooKeeper.exists(zPath, watcher);\n+            if (stat == null) {\n+              log.trace(\""{} getChildren saw that {} does not exists\"", cacheId, zPath);\n+              return ZcNode.NON_EXISTENT;\n             }\n-          });\n-          // increment this after compute call completes when the change is visible\n-          updateCount.incrementAndGet();\n-          return zcNode.getChildren();\n-        } catch (ZcException zce) {\n-          if (zce.getZKException().code() == Code.NONODE) {\n-            return null;\n-          } else {\n-            throw zce;\n+            List<String> children = zooKeeper.getChildren(zPath, watcher);\n+            log.trace(\""{} adding {} children of {} to cache\"", cacheId, children.size(), zPath);\n+            return new ZcNode(children, zcn);\n+          } catch (KeeperException.NoNodeException nne) {\n+            log.trace(\""{} get children saw race condition for {}, node deleted after exists call\"",\n+                cacheId, zPath);\n+            throw new ConcurrentModificationException(nne);\n+          } catch (KeeperException e) {\n+            throw new ZcException(e);\n+          } catch (InterruptedException e) {\n+            throw new ZcInterruptedException(e);\n           }\n-        }\n+        });\n+        // increment this after compute call completes when the change is visible\n+        updateCount.incrementAndGet();\n+        return zcNode.getChildren();\n       }\n     };\n \n@@ -386,7 +377,7 @@ public byte[] get(final String zPath, final ZcStat status) {\n       public byte[] run() throws KeeperException, InterruptedException {\n \n         var zcNode = nodeCache.get(zPath);\n-        if (zcNode != null && zcNode.dataSet) {\n+        if (zcNode != null && zcNode.cachedData()) {\n           if (status != null) {\n             copyStats(status, zcNode.getStat());\n           }\n@@ -398,7 +389,7 @@ public byte[] run() throws KeeperException, InterruptedException {\n         zcNode = nodeCache.compute(zPath, (zp, zcn) -> {\n           // recheck the now that lock is held on key, it may be present now. Could have been\n           // computed while waiting for lock.\n-          if (zcn != null && zcn.dataSet) {\n+          if (zcn != null && zcn.cachedData()) {\n             return zcn;\n           }\n           /*\n@@ -412,18 +403,19 @@ public byte[] run() throws KeeperException, InterruptedException {\n           try {\n             final ZooKeeper zooKeeper = getZooKeeper();\n             Stat stat = zooKeeper.exists(zPath, watcher);\n-            byte[] data = null;\n-            ZcStat zstat = null;\n             if (stat == null) {\n               if (log.isTraceEnabled()) {\n                 log.trace(\""{} zookeeper did not contain {}\"", cacheId, zPath);\n               }\n+              return ZcNode.NON_EXISTENT;\n             } else {\n+              byte[] data = null;\n+              ZcStat zstat = null;\n               try {\n                 data = zooKeeper.getData(zPath, watcher, stat);\n                 zstat = new ZcStat(stat);\n               } catch (KeeperException.BadVersionException | KeeperException.NoNodeException e1) {\n-                throw new ConcurrentModificationException();\n+                throw new ConcurrentModificationException(e1);\n               } catch (InterruptedException e) {\n                 throw new ZcInterruptedException(e);\n               }\n@@ -431,8 +423,8 @@ public byte[] run() throws KeeperException, InterruptedException {\n                 log.trace(\""{} zookeeper contained {} {}\"", cacheId, zPath,\n                     (data == null ? null : new String(data, UTF_8)));\n               }\n+              return new ZcNode(data, zstat, zcn);\n             }\n-            return new ZcNode(data, zstat, zcn);\n           } catch (KeeperException ke) {\n             throw new ZcException(ke);\n           } catch (InterruptedException e) {\n@@ -502,9 +494,9 @@ public long getUpdateCount() {\n    * @return true if data value is cached\n    */\n   @VisibleForTesting\n-  boolean dataCached(String zPath) {\n+  public boolean dataCached(String zPath) {\n     var zcn = nodeCache.get(zPath);\n-    return zcn != null && zcn.dataSet;\n+    return zcn != null && zcn.cachedData();\n   }\n \n   /**\n@@ -514,9 +506,9 @@ boolean dataCached(String zPath) {\n    * @return true if children are cached\n    */\n   @VisibleForTesting\n-  boolean childrenCached(String zPath) {\n+  public boolean childrenCached(String zPath) {\n     var zcn = nodeCache.get(zPath);\n-    return zcn != null && zcn.childrenSet;\n+    return zcn != null && zcn.cachedChildren();\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/cache/Caches.java b/core/src/main/java/org/apache/accumulo/core/util/cache/Caches.java\nindex 67dc8f5bb5a..8724f87e04f 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/cache/Caches.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/cache/Caches.java\n@@ -65,7 +65,8 @@ public enum CacheName {\n     TSRM_FILE_LENGTHS,\n     TINYLFU_BLOCK_CACHE,\n     VOLUME_HDFS_CONFIGS,\n-    MINC_AGE\n+    MINC_AGE,\n+    ZOO_CACHE\n   }\n \n   private static final Logger LOG = LoggerFactory.getLogger(Caches.class);\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java\nindex 39a21845682..3aced2e103e 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/zookeeper/ZooCacheTest.java\n@@ -176,6 +176,8 @@ private void testGet_Retry2(Exception e) throws Exception {\n \n   @Test\n   public void testGetChildren() throws Exception {\n+    Stat existsStat = new Stat();\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n     expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(CHILDREN);\n     replay(zk);\n \n@@ -190,34 +192,61 @@ public void testGetChildren() throws Exception {\n \n   @Test\n   public void testGetChildren_NoKids() throws Exception {\n-    expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(null);\n+    Stat existsStat = new Stat();\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n+    expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(List.of());\n     replay(zk);\n \n-    assertNull(zc.getChildren(ZPATH));\n+    assertEquals(List.of(), zc.getChildren(ZPATH));\n     verify(zk);\n \n-    assertNull(zc.getChildren(ZPATH)); // cache hit\n+    assertEquals(List.of(), zc.getChildren(ZPATH)); // cache hit\n+  }\n+\n+  @Test\n+  public void testGetChildren_RaceCondition() throws Exception {\n+    // simulate the node being deleted between calling zookeeper.exists and zookeeper.getChildren\n+    Stat existsStat = new Stat();\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n+    expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class)))\n+        .andThrow(new KeeperException.NoNodeException(ZPATH));\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(null);\n+    replay(zk);\n+    assertNull(zc.getChildren(ZPATH));\n+    verify(zk);\n+    assertNull(zc.getChildren(ZPATH));\n   }\n \n   @Test\n   public void testGetChildren_Retry() throws Exception {\n+    Stat existsStat = new Stat();\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n     expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class)))\n         .andThrow(new KeeperException.BadVersionException(ZPATH));\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n     expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(CHILDREN);\n     replay(zk);\n \n     assertEquals(CHILDREN, zc.getChildren(ZPATH));\n     verify(zk);\n+    assertEquals(CHILDREN, zc.getChildren(ZPATH));\n   }\n \n   @Test\n-  public void testGetChildren_EatNoNode() throws Exception {\n-    expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class)))\n-        .andThrow(new KeeperException.NoNodeException(ZPATH));\n+  public void testGetChildren_NoNode() throws Exception {\n+    assertFalse(zc.childrenCached(ZPATH));\n+    assertFalse(zc.dataCached(ZPATH));\n+    expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(null);\n     replay(zk);\n \n     assertNull(zc.getChildren(ZPATH));\n     verify(zk);\n+    assertNull(zc.getChildren(ZPATH));\n+    // when its discovered a node does not exists in getChildren then its also known it does not\n+    // exists for getData\n+    assertNull(zc.get(ZPATH));\n+    assertTrue(zc.childrenCached(ZPATH));\n+    assertTrue(zc.dataCached(ZPATH));\n   }\n \n   private static class TestWatcher implements Watcher {\n@@ -300,6 +329,82 @@ public void testWatchDataNode_Expired() throws Exception {\n     testWatchDataNode_Clear(Watcher.Event.KeeperState.Expired);\n   }\n \n+  @Test\n+  public void testGetDataThenChildren() throws Exception {\n+    testGetBoth(true);\n+  }\n+\n+  @Test\n+  public void testGetChildrenThenDate() throws Exception {\n+    testGetBoth(false);\n+  }\n+\n+  private void testGetBoth(boolean getDataFirst) throws Exception {\n+    assertFalse(zc.childrenCached(ZPATH));\n+    assertFalse(zc.dataCached(ZPATH));\n+\n+    var uc1 = zc.getUpdateCount();\n+\n+    final long ephemeralOwner1 = 123456789L;\n+    Stat existsStat1 = new Stat();\n+    existsStat1.setEphemeralOwner(ephemeralOwner1);\n+\n+    final long ephemeralOwner2 = 987654321L;\n+    Stat existsStat2 = new Stat();\n+    existsStat2.setEphemeralOwner(ephemeralOwner2);\n+\n+    if (getDataFirst) {\n+      expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat1);\n+      expect(zk.getData(eq(ZPATH), anyObject(Watcher.class), eq(existsStat1))).andReturn(DATA);\n+      expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat2);\n+      expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(CHILDREN);\n+    } else {\n+      expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat2);\n+      expect(zk.getChildren(eq(ZPATH), anyObject(Watcher.class))).andReturn(CHILDREN);\n+      expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat1);\n+      expect(zk.getData(eq(ZPATH), anyObject(Watcher.class), eq(existsStat1))).andReturn(DATA);\n+    }\n+\n+    replay(zk);\n+\n+    if (getDataFirst) {\n+      var zcStat = new ZcStat();\n+      var data = zc.get(ZPATH, zcStat);\n+      assertEquals(ephemeralOwner1, zcStat.getEphemeralOwner());\n+      assertArrayEquals(DATA, data);\n+    } else {\n+      var children = zc.getChildren(ZPATH);\n+      assertEquals(CHILDREN, children);\n+    }\n+    var uc2 = zc.getUpdateCount();\n+    assertTrue(uc1 < uc2);\n+\n+    if (getDataFirst) {\n+      var children = zc.getChildren(ZPATH);\n+      assertEquals(CHILDREN, children);\n+    } else {\n+      var zcStat = new ZcStat();\n+      var data = zc.get(ZPATH, zcStat);\n+      assertEquals(ephemeralOwner1, zcStat.getEphemeralOwner());\n+      assertArrayEquals(DATA, data);\n+    }\n+    var uc3 = zc.getUpdateCount();\n+    assertTrue(uc2 < uc3);\n+\n+    verify(zk);\n+\n+    var zcStat = new ZcStat();\n+    var data = zc.get(ZPATH, zcStat);\n+    // the stat is associated with the data so should aways see the one returned by the call to get\n+    // data and not get children\n+    assertEquals(ephemeralOwner1, zcStat.getEphemeralOwner());\n+    assertArrayEquals(DATA, data);\n+    var children = zc.getChildren(ZPATH);\n+    assertEquals(CHILDREN, children);\n+    // everything is cached so the get calls on the cache should not change the update count\n+    assertEquals(uc3, zc.getUpdateCount());\n+  }\n+\n   private void testWatchDataNode_Clear(Watcher.Event.KeeperState state) throws Exception {\n     WatchedEvent event = new WatchedEvent(Watcher.Event.EventType.None, state, null);\n     TestWatcher exw = new TestWatcher(event);\n@@ -347,7 +452,13 @@ private void testWatchChildrenNode(List<String> initialChildren,\n \n   private Watcher watchChildren(List<String> initialChildren) throws Exception {\n     Capture<Watcher> cw = EasyMock.newCapture();\n-    expect(zk.getChildren(eq(ZPATH), capture(cw))).andReturn(initialChildren);\n+    if (initialChildren == null) {\n+      expect(zk.exists(eq(ZPATH), capture(cw))).andReturn(null);\n+    } else {\n+      Stat existsStat = new Stat();\n+      expect(zk.exists(eq(ZPATH), anyObject(Watcher.class))).andReturn(existsStat);\n+      expect(zk.getChildren(eq(ZPATH), capture(cw))).andReturn(initialChildren);\n+    }\n     replay(zk);\n     zc.getChildren(ZPATH);\n     assertTrue(zc.childrenCached(ZPATH));\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/zookeeper/ZooCacheIT.java b/test/src/main/java/org/apache/accumulo/test/zookeeper/ZooCacheIT.java\nnew file mode 100644\nindex 00000000000..645b7c70d2f\n--- /dev/null\n+++ b/test/src/main/java/org/apache/accumulo/test/zookeeper/ZooCacheIT.java\n@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.test.zookeeper;\n+\n+import static org.apache.accumulo.harness.AccumuloITBase.ZOOKEEPER_TESTING_SERVER;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.io.File;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.fate.zookeeper.ZooCache;\n+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n+import org.apache.accumulo.test.util.Wait;\n+import org.apache.zookeeper.Watcher;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@Tag(ZOOKEEPER_TESTING_SERVER)\n+public class ZooCacheIT {\n+\n+  private ZooKeeperTestingServer szk = null;\n+  private ZooReaderWriter zk = null;\n+\n+  @TempDir\n+  private File tempDir;\n+\n+  @BeforeEach\n+  public void setup() throws Exception {\n+    szk = new ZooKeeperTestingServer(tempDir);\n+    zk = szk.getZooReaderWriter();\n+  }\n+\n+  @AfterEach\n+  public void teardown() throws Exception {\n+    szk.close();\n+  }\n+\n+  @Test\n+  public void testGetChildren() throws Exception {\n+\n+    Set<String> watchesRemoved = Collections.synchronizedSet(new HashSet<>());\n+    Watcher watcher = event -> {\n+      if (event.getType() == Watcher.Event.EventType.ChildWatchRemoved\n+          || event.getType() == Watcher.Event.EventType.DataWatchRemoved) {\n+        watchesRemoved.add(event.getPath());\n+      }\n+    };\n+    ZooCache zooCache = new ZooCache(zk, watcher, Duration.ofSeconds(3));\n+\n+    zk.mkdirs(\""/test2\"");\n+    zk.mkdirs(\""/test3/c1\"");\n+    zk.mkdirs(\""/test3/c2\"");\n+\n+    // cache non-existence of /test1 and existence of /test2 and /test3\n+    long uc1 = zooCache.getUpdateCount();\n+    assertNull(zooCache.getChildren(\""/test1\""));\n+    long uc2 = zooCache.getUpdateCount();\n+    assertTrue(uc1 < uc2);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    long uc3 = zooCache.getUpdateCount();\n+    assertTrue(uc2 < uc3);\n+    assertEquals(Set.of(\""c1\"", \""c2\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    long uc4 = zooCache.getUpdateCount();\n+    assertTrue(uc3 < uc4);\n+\n+    // The cache should be stable now and new accesses should not change the update count\n+    assertNull(zooCache.getChildren(\""/test1\""));\n+    // once getChildren discovers that a node does not exists, then get data will also know this\n+    assertNull(zooCache.get(\""/test1\""));\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c2\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc4, zooCache.getUpdateCount());\n+\n+    // Had cached non-existence of \""/test1\"", should get a notification that it was created\n+    zk.mkdirs(\""/test1\"");\n+\n+    Wait.waitFor(() -> {\n+      var children = zooCache.getChildren(\""/test1\"");\n+      return children != null && children.isEmpty();\n+    });\n+\n+    long uc5 = zooCache.getUpdateCount();\n+    assertTrue(uc4 < uc5);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test1\""));\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c2\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc5, zooCache.getUpdateCount());\n+\n+    // add a child to /test3, should get a notification of the change\n+    zk.mkdirs(\""/test3/c3\"");\n+    Wait.waitFor(() -> {\n+      var children = zooCache.getChildren(\""/test3\"");\n+      return children != null && children.size() == 3;\n+    });\n+    long uc6 = zooCache.getUpdateCount();\n+    assertTrue(uc5 < uc6);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test1\""));\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c2\"", \""c3\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc6, zooCache.getUpdateCount());\n+\n+    // remove a child from /test3\n+    zk.delete(\""/test3/c2\"");\n+    Wait.waitFor(() -> {\n+      var children = zooCache.getChildren(\""/test3\"");\n+      return children != null && children.size() == 2;\n+    });\n+    long uc7 = zooCache.getUpdateCount();\n+    assertTrue(uc6 < uc7);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test1\""));\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c3\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc7, zooCache.getUpdateCount());\n+\n+    // remove /test2, should start caching that it does not exist\n+    zk.delete(\""/test2\"");\n+    Wait.waitFor(() -> zooCache.getChildren(\""/test2\"") == null);\n+    long uc8 = zooCache.getUpdateCount();\n+    assertTrue(uc7 < uc8);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test1\""));\n+    assertNull(zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c3\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc8, zooCache.getUpdateCount());\n+\n+    // add /test2 back, should update\n+    zk.mkdirs(\""/test2\"");\n+    Wait.waitFor(() -> zooCache.getChildren(\""/test2\"") != null);\n+    long uc9 = zooCache.getUpdateCount();\n+    assertTrue(uc8 < uc9);\n+    assertEquals(List.of(), zooCache.getChildren(\""/test1\""));\n+    assertEquals(List.of(), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c1\"", \""c3\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc9, zooCache.getUpdateCount());\n+\n+    // make multiple changes. the cache should see all of these\n+    zk.delete(\""/test1\"");\n+    zk.mkdirs(\""/test2/ca\"");\n+    zk.delete(\""/test3/c1\"");\n+    zk.mkdirs(\""/test3/c4\"");\n+    zk.delete(\""/test3/c4\"");\n+    zk.mkdirs(\""/test3/c5\"");\n+\n+    Wait.waitFor(() -> {\n+      var children1 = zooCache.getChildren(\""/test1\"");\n+      var children2 = zooCache.getChildren(\""/test2\"");\n+      var children3 = zooCache.getChildren(\""/test3\"");\n+      return children1 == null && children2 != null && children2.size() == 1 && children3 != null\n+          && Set.copyOf(children3).equals(Set.of(\""c3\"", \""c5\""));\n+    });\n+    long uc10 = zooCache.getUpdateCount();\n+    assertTrue(uc9 < uc10);\n+    assertNull(zooCache.getChildren(\""/test1\""));\n+    assertEquals(List.of(\""ca\""), zooCache.getChildren(\""/test2\""));\n+    assertEquals(Set.of(\""c3\"", \""c5\""), Set.copyOf(zooCache.getChildren(\""/test3\"")));\n+    assertEquals(uc10, zooCache.getUpdateCount());\n+\n+    // wait for the cache to evict and clear watches\n+    Wait.waitFor(() -> {\n+      // the cache will not run its eviction handler unless accessed, so access something that is\n+      // not expected to be evicted\n+      zooCache.getChildren(\""/test4\"");\n+      return watchesRemoved.equals(Set.of(\""/test1\"", \""/test2\"", \""/test3\""));\n+    });\n+\n+    assertFalse(zooCache.childrenCached(\""/test1\""));\n+    assertFalse(zooCache.childrenCached(\""/test2\""));\n+    assertFalse(zooCache.childrenCached(\""/test3\""));\n+  }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5122"", ""pr_id"": 5122, ""issue_id"": 5097, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Seeding compaction commit and split fate transactions is slow\n**Describe the bug**\r\n\r\nWhile testing splitting lots of tablets it was observed that seeding the split fate transactions was the bottleneck and not running the actual fate operation.  Looking at [the code](https://github.com/apache/accumulo/blob/bbfd250d8694c210faf31116d199b56570c46f38/core/src/main/java/org/apache/accumulo/core/fate/Fate.java#L421-L455) there is a lot of back and forth to the seed a fate transaction\r\n\r\n 1. Do conditional mutation reserve\r\n 2. Do a scan to see if there is a top on the stack\r\n 3. Do a conditional mutation to set the transaction info\r\n 4. Do a conditional mutation to set the transaction info\r\n 5. Do a conditional mutation to set the status\r\n\r\nAll of this could likely be done in a single conditional mutation.  This would avoid the scan and doing multiple conditional mutations.  Observed this while looking into splits, but improving this would speed up compaction commit and split.\r\n\r\n**Expected behavior**\r\nIf possible a single conditional mutation is run to seed a fate transaction.\r\n\r\n**Additional context**\r\n\r\nThere may be other places in the code that are submitting multiple conditional mutations for fate operations when one would suffice.  This is caused by the code evolving from initially having a zookeeper persistent store where data was stored in multiple zk nodes for a single fate operation. Multiple ZK nodes require multiple write.  Implementing #4905 could allow zookeeper and accumuo table impls to have less writes for fate updates by improving the code for both to support doing more single writes.\r\n"", ""issue_word_count"": 260, ""test_files_count"": 8, ""non_test_files_count"": 8, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/Fate.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/RowExistsIterator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/SetEncodingIterator.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/FateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java"", ""core/src/test/java/org/apache/accumulo/core/fate/TestStore.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateIT.java"", ""test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java""], ""base_commit"": ""40dc910caa840e7aa94f6f17b0521aa80a270050"", ""head_commit"": ""51ad6542f651851900f44845a5e52e244af57ad4"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5122"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5122"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-10T00:21:28.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\nindex 1350cce6524..16fb8e5f931 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/Fate.java\n@@ -62,8 +62,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import com.google.common.base.Preconditions;\n-\n /**\n  * Fault tolerant executor\n  */\n@@ -401,57 +399,16 @@ public FateId startTransaction() {\n     return store.create();\n   }\n \n-  public Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<T> repo,\n-      boolean autoCleanUp, String goalMessage) {\n-\n-    Optional<FateTxStore<T>> optTxStore = store.createAndReserve(fateKey);\n-\n-    return optTxStore.map(txStore -> {\n-      var fateId = txStore.getID();\n-      try {\n-        Preconditions.checkState(txStore.getStatus() == NEW);\n-        seedTransaction(txName, fateId, repo, autoCleanUp, goalMessage, txStore);\n-      } finally {\n-        txStore.unreserve(Duration.ZERO);\n-      }\n-      return fateId;\n-    });\n-  }\n-\n-  private void seedTransaction(String txName, FateId fateId, Repo<T> repo, boolean autoCleanUp,\n-      String goalMessage, FateTxStore<T> txStore) {\n-    if (txStore.top() == null) {\n-      try {\n-        log.info(\""Seeding {} {}\"", fateId, goalMessage);\n-        txStore.push(repo);\n-      } catch (StackOverflowException e) {\n-        // this should not happen\n-        throw new IllegalStateException(e);\n-      }\n-    }\n-\n-    if (autoCleanUp) {\n-      txStore.setTransactionInfo(TxInfo.AUTO_CLEAN, autoCleanUp);\n-    }\n-\n-    txStore.setTransactionInfo(TxInfo.TX_NAME, txName);\n-\n-    txStore.setStatus(SUBMITTED);\n+  public void seedTransaction(String txName, FateKey fateKey, Repo<T> repo, boolean autoCleanUp) {\n+    store.seedTransaction(txName, fateKey, repo, autoCleanUp);\n   }\n \n   // start work in the transaction.. it is safe to call this\n   // multiple times for a transaction... but it will only seed once\n   public void seedTransaction(String txName, FateId fateId, Repo<T> repo, boolean autoCleanUp,\n       String goalMessage) {\n-    FateTxStore<T> txStore = store.reserve(fateId);\n-    try {\n-      if (txStore.getStatus() == NEW) {\n-        seedTransaction(txName, fateId, repo, autoCleanUp, goalMessage, txStore);\n-      }\n-    } finally {\n-      txStore.unreserve(Duration.ZERO);\n-    }\n-\n+    log.info(\""Seeding {} {}\"", fateId, goalMessage);\n+    store.seedTransaction(txName, fateId, repo, autoCleanUp);\n   }\n \n   // check on the transaction\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\nindex d199a7463e4..ac675d7fb95 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n@@ -33,6 +33,29 @@ public interface FateMutator<T> {\n \n   FateMutator<T> putCreateTime(long ctime);\n \n+  /**\n+   * Requires that nothing exists for this fate mutation.\n+   */\n+  FateMutator<T> requireAbsent();\n+\n+  /**\n+   * Require that the transaction status is one of the given statuses. If no statuses are provided,\n+   * require that the status column is absent.\n+   *\n+   * @param statuses The statuses to check against.\n+   */\n+  FateMutator<T> requireStatus(TStatus... statuses);\n+\n+  /**\n+   * Require the transaction has no reservation.\n+   */\n+  FateMutator<T> requireUnreserved();\n+\n+  /**\n+   * Require the transaction has no fate key set.\n+   */\n+  FateMutator<T> requireAbsentKey();\n+\n   /**\n    * Add a conditional mutation to {@link FateSchema.TxColumnFamily#RESERVATION_COLUMN} that will\n    * put the reservation if there is not already a reservation present\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\nindex 5d99a8df3a3..ea7dd85c571 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n@@ -29,6 +29,7 @@\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.BatchWriter;\n import org.apache.accumulo.core.client.ConditionalWriter;\n+import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.MutationsRejectedException;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n@@ -48,12 +49,16 @@\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.hadoop.io.Text;\n \n+import com.google.common.base.Preconditions;\n+\n public class FateMutatorImpl<T> implements FateMutator<T> {\n \n   private final ClientContext context;\n   private final String tableName;\n   private final FateId fateId;\n   private final ConditionalMutation mutation;\n+  private boolean requiredUnreserved = false;\n+  public static final int INITIAL_ITERATOR_PRIO = 1000000;\n \n   public FateMutatorImpl(ClientContext context, String tableName, FateId fateId) {\n     this.context = Objects.requireNonNull(context);\n@@ -81,10 +86,34 @@ public FateMutator<T> putCreateTime(long ctime) {\n   }\n \n   @Override\n-  public FateMutator<T> putReservedTx(FateStore.FateReservation reservation) {\n+  public FateMutator<T> requireAbsent() {\n+    IteratorSetting is = new IteratorSetting(INITIAL_ITERATOR_PRIO, RowExistsIterator.class);\n+    Condition c = new Condition(\""\"", \""\"").setIterators(is);\n+    mutation.addCondition(c);\n+    return this;\n+  }\n+\n+  @Override\n+  public FateMutator<T> requireUnreserved() {\n+    Preconditions.checkState(!requiredUnreserved);\n     Condition condition = new Condition(TxColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n         TxColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n     mutation.addCondition(condition);\n+    requiredUnreserved = true;\n+    return this;\n+  }\n+\n+  @Override\n+  public FateMutator<T> requireAbsentKey() {\n+    Condition condition = new Condition(TxColumnFamily.TX_KEY_COLUMN.getColumnFamily(),\n+        TxColumnFamily.TX_KEY_COLUMN.getColumnQualifier());\n+    mutation.addCondition(condition);\n+    return this;\n+  }\n+\n+  @Override\n+  public FateMutator<T> putReservedTx(FateStore.FateReservation reservation) {\n+    requireUnreserved();\n     TxColumnFamily.RESERVATION_COLUMN.put(mutation, new Value(reservation.getSerialized()));\n     return this;\n   }\n@@ -179,12 +208,7 @@ public FateMutator<T> delete() {\n     return this;\n   }\n \n-  /**\n-   * Require that the transaction status is one of the given statuses. If no statuses are provided,\n-   * require that the status column is absent.\n-   *\n-   * @param statuses The statuses to check against.\n-   */\n+  @Override\n   public FateMutator<T> requireStatus(TStatus... statuses) {\n     Condition condition = StatusMappingIterator.createCondition(statuses);\n     mutation.addCondition(condition);\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/RowExistsIterator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/RowExistsIterator.java\nnew file mode 100644\nindex 00000000000..6095546b0da\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/RowExistsIterator.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.fate.user;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.data.ByteSequence;\n+import org.apache.accumulo.core.data.Range;\n+import org.apache.accumulo.core.iterators.WrappingIterator;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * Iterator is used by conditional mutations to check if row exists.\n+ */\n+public class RowExistsIterator extends WrappingIterator {\n+\n+  @Override\n+  public void seek(Range range, Collection<ByteSequence> columnFamilies, boolean inclusive)\n+      throws IOException {\n+    Preconditions.checkState(range.getStartKey() != null && range.getEndKey() != null);\n+    var startRow = range.getStartKey().getRow();\n+    var endRow = range.getEndKey().getRow();\n+    Preconditions.checkState(startRow.equals(endRow));\n+    Range r = new Range(startRow);\n+    super.seek(r, Set.of(), false);\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\nindex 4a9f2517c01..9a5984f4ed6 100644\n--- a/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n+++ b/core/src/main/java/org/apache/accumulo/core/logging/FateLogger.java\n@@ -149,6 +149,33 @@ public FateId create() {\n         return fateId;\n       }\n \n+      @Override\n+      public Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<T> repo,\n+          boolean autoCleanUp) {\n+        var optional = store.seedTransaction(txName, fateKey, repo, autoCleanUp);\n+        if (storeLog.isTraceEnabled()) {\n+          optional.ifPresentOrElse(fateId -> {\n+            storeLog.trace(\""{} seeded {} {} {}\"", fateId, fateKey, toLogString.apply(repo),\n+                autoCleanUp);\n+          }, () -> {\n+            storeLog.trace(\""Possibly unable to seed {} {} {}\"", fateKey, toLogString.apply(repo),\n+                autoCleanUp);\n+          });\n+        }\n+        return optional;\n+      }\n+\n+      @Override\n+      public boolean seedTransaction(String txName, FateId fateId, Repo<T> repo,\n+          boolean autoCleanUp) {\n+        boolean seeded = store.seedTransaction(txName, fateId, repo, autoCleanUp);\n+        if (storeLog.isTraceEnabled()) {\n+          storeLog.trace(\""{} {} {} {}\"", fateId, seeded ? \""seeded\"" : \""unable to seed\"",\n+              toLogString.apply(repo), autoCleanUp);\n+        }\n+        return seeded;\n+      }\n+\n       @Override\n       public int getDeferredCount() {\n         return store.getDeferredCount();\n@@ -164,22 +191,6 @@ public boolean isDeferredOverflow() {\n         return store.isDeferredOverflow();\n       }\n \n-      @Override\n-      public Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n-        Optional<FateTxStore<T>> txStore = store.createAndReserve(fateKey);\n-        if (storeLog.isTraceEnabled()) {\n-          if (txStore.isPresent()) {\n-            storeLog.trace(\""{} created and reserved fate transaction using key : {}\"",\n-                txStore.orElseThrow().getID(), fateKey);\n-          } else {\n-            storeLog.trace(\n-                \""fate transaction was not created using key : {}, existing transaction exists\"",\n-                fateKey);\n-          }\n-        }\n-        return txStore;\n-      }\n-\n       @Override\n       public Map<FateId,FateReservation> getActiveReservations() {\n         return store.getActiveReservations();\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/SetEncodingIterator.java b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/SetEncodingIterator.java\nindex b0456afd32a..ebe732049f1 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/SetEncodingIterator.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/metadata/iterators/SetEncodingIterator.java\n@@ -79,7 +79,7 @@ static Text getTabletRow(Range range) {\n     // expecting this range to cover a single metadata row, so validate the range meets expectations\n     MetadataSchema.TabletsSection.validateRow(row);\n     Preconditions.checkArgument(row.equals(range.getEndKey().getRow()));\n-    return range.getStartKey().getRow();\n+    return row;\n   }\n \n   @Override\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\nindex e06229c21b2..772676a491b 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/compaction/coordinator/CompactionCoordinator.java\n@@ -747,11 +747,8 @@ public void compactionCompleted(TInfo tinfo, TCredentials credentials,\n     // Start a fate transaction to commit the compaction.\n     CompactionMetadata ecm = tabletMeta.getExternalCompactions().get(ecid);\n     var renameOp = new RenameCompactionFile(new CompactionCommitData(ecid, extent, ecm, stats));\n-    var txid = localFate.seedTransaction(\""COMMIT_COMPACTION\"", FateKey.forCompactionCommit(ecid),\n-        renameOp, true, \""Commit compaction \"" + ecid);\n-\n-    txid.ifPresentOrElse(fateId -> LOG.debug(\""initiated compaction commit {} {}\"", ecid, fateId),\n-        () -> LOG.debug(\""compaction commit already initiated for {}\"", ecid));\n+    localFate.seedTransaction(\""COMMIT_COMPACTION\"", FateKey.forCompactionCommit(ecid), renameOp,\n+        true);\n   }\n \n   @Override\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java b/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java\nindex 78f08a9471a..7b56ea43880 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/split/SeedSplitTask.java\n@@ -18,10 +18,7 @@\n  */\n package org.apache.accumulo.manager.split;\n \n-import java.util.Optional;\n-\n import org.apache.accumulo.core.dataImpl.KeyExtent;\n-import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.manager.Manager;\n@@ -44,17 +41,8 @@ public SeedSplitTask(Manager manager, KeyExtent extent) {\n   public void run() {\n     try {\n       var fateInstanceType = FateInstanceType.fromTableId((extent.tableId()));\n-\n-      Optional<FateId> optFateId =\n-          manager.fate(fateInstanceType).seedTransaction(\""SYSTEM_SPLIT\"", FateKey.forSplit(extent),\n-              new FindSplits(extent), true, \""System initiated split of tablet \"" + extent);\n-\n-      optFateId.ifPresentOrElse(fateId -> {\n-        log.trace(\""System initiated a split for : {} {}\"", extent, fateId);\n-      }, () -> {\n-        log.trace(\""System attempted to initiate a split but one was in progress : {}\"", extent);\n-      });\n-\n+      manager.fate(fateInstanceType).seedTransaction(\""SYSTEM_SPLIT\"", FateKey.forSplit(extent),\n+          new FindSplits(extent), true);\n     } catch (Exception e) {\n       log.error(\""Failed to split {}\"", extent, e);\n     }\n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\nindex ff5e45d3103..3bc322c3c21 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/AbstractFateStore.java\n@@ -69,6 +69,11 @@ public FateId fromTypeAndKey(FateInstanceType instanceType, FateKey fateKey) {\n       UUID txUUID = UUID.nameUUIDFromBytes(fateKey.getSerialized());\n       return FateId.from(instanceType, txUUID);\n     }\n+\n+    @Override\n+    public FateId newRandomId(FateInstanceType instanceType) {\n+      return FateId.from(instanceType, UUID.randomUUID());\n+    }\n   };\n \n   // The ZooKeeper lock for the process that's running this store instance\n@@ -402,6 +407,12 @@ public FateId getID() {\n \n   public interface FateIdGenerator {\n     FateId fromTypeAndKey(FateInstanceType instanceType, FateKey fateKey);\n+\n+    FateId newRandomId(FateInstanceType instanceType);\n+  }\n+\n+  protected void seededTx() {\n+    unreservedRunnableCount.increment();\n   }\n \n   protected byte[] serializeTxInfo(Serializable so) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\nindex 09ee12dd94e..d434770461e 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/FateStore.java\n@@ -50,19 +50,41 @@ public interface FateStore<T> extends ReadOnlyFateStore<T> {\n   FateId create();\n \n   /**\n-   * Creates and reserves a transaction using the given key. If something is already running for the\n-   * given key, then Optional.empty() will be returned. When this returns a non-empty id, it will be\n-   * in the new state.\n+   * Seeds a transaction with the given repo if it does not exists. A fateId will be derived from\n+   * the fateKey. If seeded, sets the following data for the fateId in the store.\n    *\n-   * <p>\n-   * In the case where a process dies in the middle of a call to this. If later, another call is\n-   * made with the same key and its in the new state then the FateId for that key will be returned.\n-   * </p>\n+   * <ul>\n+   * <li>Set the tx name</li>\n+   * <li>Set the status to SUBMITTED</li>\n+   * <li>Set the fate key</li>\n+   * <li>Sets autocleanup only if true</li>\n+   * <li>Sets the creation time</li>\n+   * </ul>\n    *\n-   * @throws IllegalStateException when there is an unexpected collision. This can occur if two key\n-   *         hash to the same FateId or if a random FateId already exists.\n+   * @return The return type is only intended for testing it may not be correct in the face of\n+   *         failures. When there are no failures returns optional w/ the fate id set if seeded and\n+   *         empty optional otherwise. If there was a failure this could return an empty optional\n+   *         when it actually succeeded.\n    */\n-  Optional<FateTxStore<T>> createAndReserve(FateKey fateKey);\n+  Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<T> repo,\n+      boolean autoCleanUp);\n+\n+  /**\n+   * Seeds a transaction with the given repo if its current status is NEW and it is currently\n+   * unreserved. If seeded, sets the following data for the fateId in the store.\n+   *\n+   * <ul>\n+   * <li>Set the tx name</li>\n+   * <li>Set the status to SUBMITTED</li>\n+   * <li>Sets autocleanup only if true</li>\n+   * <li>Sets the creation time</li>\n+   * </ul>\n+   *\n+   * @return The return type is only intended for testing it may not be correct in the face of\n+   *         failures. When there are no failures returns true if seeded and false otherwise. If\n+   *         there was a failure this could return false when it actually succeeded.\n+   */\n+  boolean seedTransaction(String txName, FateId fateId, Repo<T> repo, boolean autoCleanUp);\n \n   /**\n    * An interface that allows read/write access to the data related to a single fate operation.\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex efd0cbc62f4..c134db18405 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -20,7 +20,6 @@\n \n import java.io.IOException;\n import java.io.Serializable;\n-import java.time.Duration;\n import java.util.EnumSet;\n import java.util.List;\n import java.util.Map.Entry;\n@@ -30,6 +29,7 @@\n import java.util.UUID;\n import java.util.function.Function;\n import java.util.function.Predicate;\n+import java.util.function.Supplier;\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n@@ -106,7 +106,7 @@ public FateId create() {\n         UtilWaitThread.sleep(100);\n       }\n \n-      var status = newMutator(fateId).requireStatus().putStatus(TStatus.NEW)\n+      var status = newMutator(fateId).requireAbsent().putStatus(TStatus.NEW)\n           .putCreateTime(System.currentTimeMillis()).tryMutate();\n \n       switch (status) {\n@@ -123,104 +123,62 @@ public FateId create() {\n   }\n \n   public FateId getFateId() {\n-    return FateId.from(fateInstanceType, UUID.randomUUID());\n+    return fateIdGenerator.newRandomId(type());\n   }\n \n   @Override\n-  public Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n-    final var reservation = FateReservation.from(lockID, UUID.randomUUID());\n+  public Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<T> repo,\n+      boolean autoCleanUp) {\n     final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n-    Optional<FateTxStore<T>> txStore = Optional.empty();\n-    int maxAttempts = 5;\n-    FateMutator.Status status = null;\n-\n-    // Only need to retry if it is UNKNOWN\n-    for (int attempt = 0; attempt < maxAttempts; attempt++) {\n-      status = newMutator(fateId).requireStatus().putStatus(TStatus.NEW).putKey(fateKey)\n-          .putReservedTx(reservation).putCreateTime(System.currentTimeMillis()).tryMutate();\n-      if (status != FateMutator.Status.UNKNOWN) {\n-        break;\n-      }\n-      UtilWaitThread.sleep(100);\n+    Supplier<FateMutator<T>> mutatorFactory = () -> newMutator(fateId).requireAbsent()\n+        .putKey(fateKey).putCreateTime(System.currentTimeMillis());\n+    if (seedTransaction(mutatorFactory, fateKey + \"" \"" + fateId, txName, repo, autoCleanUp)) {\n+      return Optional.of(fateId);\n+    } else {\n+      return Optional.empty();\n     }\n+  }\n \n-    switch (status) {\n-      case ACCEPTED:\n-        txStore = Optional.of(new FateTxStoreImpl(fateId, reservation));\n-        break;\n-      case REJECTED:\n-        // If the status is REJECTED, we need to check what about the mutation was REJECTED:\n-        // 1) Possible something like the following occurred:\n-        // the first attempt was UNKNOWN but written, the next attempt would be rejected\n-        // We return the FateTxStore in this case.\n-        // 2) If there is a collision with existing fate id, throw error\n-        // 3) If the fate id is already reserved, return an empty optional\n-        // 4) If the fate id is still NEW/unseeded and unreserved, we can try to reserve it\n-        try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {\n-          scanner.setRange(getRow(fateId));\n-          scanner.fetchColumn(TxColumnFamily.STATUS_COLUMN.getColumnFamily(),\n-              TxColumnFamily.STATUS_COLUMN.getColumnQualifier());\n-          scanner.fetchColumn(TxColumnFamily.TX_KEY_COLUMN.getColumnFamily(),\n-              TxColumnFamily.TX_KEY_COLUMN.getColumnQualifier());\n-          scanner.fetchColumn(TxColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n-              TxColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n-          TStatus statusSeen = TStatus.UNKNOWN;\n-          Optional<FateKey> fateKeySeen = Optional.empty();\n-          Optional<FateReservation> reservationSeen = Optional.empty();\n-\n-          for (Entry<Key,Value> entry : scanner) {\n-            Text colf = entry.getKey().getColumnFamily();\n-            Text colq = entry.getKey().getColumnQualifier();\n-            Value val = entry.getValue();\n-\n-            switch (colq.toString()) {\n-              case TxColumnFamily.STATUS:\n-                statusSeen = TStatus.valueOf(val.toString());\n-                break;\n-              case TxColumnFamily.TX_KEY:\n-                fateKeySeen = Optional.of(FateKey.deserialize(val.get()));\n-                break;\n-              case TxColumnFamily.RESERVATION:\n-                reservationSeen = Optional.of(FateReservation.deserialize(val.get()));\n-                break;\n-              default:\n-                throw new IllegalStateException(\""Unexpected column seen: \"" + colf + \"":\"" + colq);\n-            }\n-          }\n+  @Override\n+  public boolean seedTransaction(String txName, FateId fateId, Repo<T> repo, boolean autoCleanUp) {\n+    Supplier<FateMutator<T>> mutatorFactory =\n+        () -> newMutator(fateId).requireStatus(TStatus.NEW).requireUnreserved().requireAbsentKey();\n+    return seedTransaction(mutatorFactory, fateId.canonical(), txName, repo, autoCleanUp);\n+  }\n \n-          if (statusSeen == TStatus.NEW) {\n-            verifyFateKey(fateId, fateKeySeen, fateKey);\n-            // This will be the case if the mutation status is REJECTED but the mutation was written\n-            if (reservationSeen.isPresent() && reservationSeen.orElseThrow().equals(reservation)) {\n-              txStore = Optional.of(new FateTxStoreImpl(fateId, reservation));\n-            } else if (reservationSeen.isEmpty()) {\n-              // NEW/unseeded transaction and not reserved, so we can allow it to be reserved\n-              // we tryReserve() since another thread may have reserved it since the scan\n-              txStore = tryReserve(fateId);\n-              // the status was known before reserving to be NEW,\n-              // however it could change so check after reserving to avoid race conditions.\n-              var statusAfterReserve =\n-                  txStore.map(ReadOnlyFateTxStore::getStatus).orElse(TStatus.UNKNOWN);\n-              if (statusAfterReserve != TStatus.NEW) {\n-                txStore.ifPresent(txs -> txs.unreserve(Duration.ZERO));\n-                txStore = Optional.empty();\n-              }\n-            }\n-          } else {\n-            log.trace(\n-                \""fate id {} tstatus {} fate key {} is reserved {} \""\n-                    + \""has already been seeded with work (non-NEW status)\"",\n-                fateId, statusSeen, fateKeySeen.orElse(null), reservationSeen.isPresent());\n-          }\n-        } catch (TableNotFoundException e) {\n-          throw new IllegalStateException(tableName + \"" not found!\"", e);\n-        }\n-        break;\n-      default:\n-        throw new IllegalStateException(\""Unknown or unexpected status \"" + status);\n+  private boolean seedTransaction(Supplier<FateMutator<T>> mutatorFactory, String logId,\n+      String txName, Repo<T> repo, boolean autoCleanUp) {\n+    int maxAttempts = 5;\n+    for (int attempt = 0; attempt < maxAttempts; attempt++) {\n+      var mutator = mutatorFactory.get();\n+      mutator =\n+          mutator.putName(serializeTxInfo(txName)).putRepo(1, repo).putStatus(TStatus.SUBMITTED);\n+      if (autoCleanUp) {\n+        mutator = mutator.putAutoClean(serializeTxInfo(autoCleanUp));\n+      }\n+      var status = mutator.tryMutate();\n+      if (status == FateMutator.Status.ACCEPTED) {\n+        // signal to the super class that a new fate transaction was seeded and is ready to run\n+        seededTx();\n+        log.trace(\""Attempt to seed {} returned {}\"", logId, status);\n+        return true;\n+      } else if (status == FateMutator.Status.REJECTED) {\n+        log.debug(\""Attempt to seed {} returned {}\"", logId, status);\n+        return false;\n+      } else if (status == FateMutator.Status.UNKNOWN) {\n+        // At this point can not reliably determine if the conditional mutation was successful or\n+        // not because no reservation was acquired. For example since no reservation was acquired it\n+        // is possible that seeding was a success and something immediately picked it up and started\n+        // operating on it and changing it. If scanning after that point can not conclude success or\n+        // failure. Another situation is that maybe the fateId already existed in a seeded form\n+        // prior to getting this unknown.\n+        log.debug(\""Attempt to seed {} returned {} status, retrying\"", logId, status);\n+        UtilWaitThread.sleep(250);\n+      }\n     }\n \n-    return txStore;\n+    log.warn(\""Repeatedly received unknown status when attempting to seed {}\"", logId);\n+    return false;\n   }\n \n   @Override\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\nindex d6da05e844f..28c0904ffa1 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/zookeeper/MetaFateStore.java\n@@ -20,12 +20,15 @@\n \n import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.NEW;\n+import static org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus.SUBMITTED;\n \n import java.io.ByteArrayOutputStream;\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.Serializable;\n import java.io.UncheckedIOException;\n+import java.time.Duration;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.EnumSet;\n@@ -100,7 +103,7 @@ public MetaFateStore(String path, ZooReaderWriter zk, ZooUtil.LockID lockID,\n   public FateId create() {\n     while (true) {\n       try {\n-        FateId fateId = FateId.from(fateInstanceType, UUID.randomUUID());\n+        FateId fateId = fateIdGenerator.newRandomId(fateInstanceType);\n         zk.putPersistentData(getTXPath(fateId), new NodeValue(TStatus.NEW, null).serialize(),\n             NodeExistsPolicy.FAIL);\n         return fateId;\n@@ -112,8 +115,7 @@ public FateId create() {\n     }\n   }\n \n-  @Override\n-  public Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n+  private Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n     final var reservation = FateReservation.from(lockID, UUID.randomUUID());\n     final var fateId = fateIdGenerator.fromTypeAndKey(type(), fateKey);\n \n@@ -161,6 +163,52 @@ public Optional<FateTxStore<T>> createAndReserve(FateKey fateKey) {\n     }\n   }\n \n+  @Override\n+  public Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<T> repo,\n+      boolean autoCleanUp) {\n+    return createAndReserve(fateKey).map(txStore -> {\n+      try {\n+        seedTransaction(txName, repo, autoCleanUp, txStore);\n+        return txStore.getID();\n+      } finally {\n+        txStore.unreserve(Duration.ZERO);\n+      }\n+    });\n+  }\n+\n+  @Override\n+  public boolean seedTransaction(String txName, FateId fateId, Repo<T> repo, boolean autoCleanUp) {\n+    return tryReserve(fateId).map(txStore -> {\n+      try {\n+        if (txStore.getStatus() == NEW) {\n+          seedTransaction(txName, repo, autoCleanUp, txStore);\n+          return true;\n+        }\n+        return false;\n+      } finally {\n+        txStore.unreserve(Duration.ZERO);\n+      }\n+    }).orElse(false);\n+  }\n+\n+  private void seedTransaction(String txName, Repo<T> repo, boolean autoCleanUp,\n+      FateTxStore<T> txStore) {\n+    if (txStore.top() == null) {\n+      try {\n+        txStore.push(repo);\n+      } catch (StackOverflowException e) {\n+        // this should not happen\n+        throw new IllegalStateException(e);\n+      }\n+    }\n+\n+    if (autoCleanUp) {\n+      txStore.setTransactionInfo(TxInfo.AUTO_CLEAN, autoCleanUp);\n+    }\n+    txStore.setTransactionInfo(TxInfo.TX_NAME, txName);\n+    txStore.setStatus(SUBMITTED);\n+  }\n+\n   @Override\n   public Optional<FateTxStore<T>> tryReserve(FateId fateId) {\n     // uniquely identify this attempt to reserve the fate operation data\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\nindex 2c54464663f..40d0d755b13 100644\n--- a/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n+++ b/core/src/test/java/org/apache/accumulo/core/fate/TestStore.java\n@@ -53,8 +53,15 @@ public FateId create() {\n   }\n \n   @Override\n-  public Optional<FateTxStore<String>> createAndReserve(FateKey key) {\n-    throw new UnsupportedOperationException();\n+  public Optional<FateId> seedTransaction(String txName, FateKey fateKey, Repo<String> repo,\n+      boolean autoCleanUp) {\n+    return Optional.empty();\n+  }\n+\n+  @Override\n+  public boolean seedTransaction(String txName, FateId fateId, Repo<String> repo,\n+      boolean autoCleanUp) {\n+    return false;\n   }\n \n   @Override\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\nindex e8955e465ab..c8905cd850a 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompaction_1_IT.java\n@@ -78,6 +78,7 @@\n import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.core.fate.FateStore;\n+import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n import org.apache.accumulo.core.fate.zookeeper.MetaFateStore;\n import org.apache.accumulo.core.iterators.DevNull;\n@@ -96,6 +97,7 @@\n import org.apache.accumulo.harness.MiniClusterConfigurationCallback;\n import org.apache.accumulo.harness.SharedMiniClusterBase;\n import org.apache.accumulo.manager.Manager;\n+import org.apache.accumulo.manager.tableOps.ManagerRepo;\n import org.apache.accumulo.minicluster.ServerType;\n import org.apache.accumulo.miniclusterImpl.MiniAccumuloConfigImpl;\n import org.apache.accumulo.server.util.FindCompactionTmpFiles;\n@@ -332,6 +334,21 @@ public void testCompactionCommitAndDeadDetectionAll() throws Exception {\n     }\n   }\n \n+  public static class FakeRepo extends ManagerRepo {\n+\n+    private static final long serialVersionUID = 1234L;\n+\n+    @Override\n+    public long isReady(FateId fateId, Manager environment) throws Exception {\n+      return 1000;\n+    }\n+\n+    @Override\n+    public Repo<Manager> call(FateId fateId, Manager environment) throws Exception {\n+      return null;\n+    }\n+  }\n+\n   private FateId createCompactionCommitAndDeadMetadata(AccumuloClient c,\n       FateStore<Manager> fateStore, String tableName,\n       Map<TableId,List<ExternalCompactionId>> allCids) throws Exception {\n@@ -345,10 +362,9 @@ private FateId createCompactionCommitAndDeadMetadata(AccumuloClient c,\n     // Create a fate transaction for one of the compaction ids that is in the new state, it\n     // should never run. Its purpose is to prevent the dead compaction detector\n     // from deleting the id.\n-    FateStore.FateTxStore<Manager> fateTx = fateStore\n-        .createAndReserve(FateKey.forCompactionCommit(allCids.get(tableId).get(0))).orElseThrow();\n-    var fateId = fateTx.getID();\n-    fateTx.unreserve(Duration.ZERO);\n+    Repo<Manager> repo = new FakeRepo();\n+    var fateId = fateStore.seedTransaction(\""COMPACTION_COMMIT\"",\n+        FateKey.forCompactionCommit(allCids.get(tableId).get(0)), repo, true).orElseThrow();\n \n     // Read the tablet metadata\n     var tabletsMeta = ctx.getAmple().readTablets().forTable(tableId).build().stream()\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\nindex d36e98bdecb..13e9e23abfb 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateIT.java\n@@ -74,6 +74,10 @@ public static class TestRepo implements Repo<TestEnv> {\n \n     private final String data;\n \n+    public TestRepo() {\n+      this(\""test\"");\n+    }\n+\n     public TestRepo(String data) {\n       this.data = data;\n     }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\nindex 64607cab7b6..1980dcf4eee 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/FateStoreIT.java\n@@ -25,10 +25,10 @@\n import static org.junit.jupiter.api.Assertions.assertNotEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.time.Duration;\n+import java.util.ArrayList;\n import java.util.EnumSet;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -40,6 +40,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.data.TableId;\n@@ -47,6 +48,7 @@\n import org.apache.accumulo.core.fate.AbstractFateStore;\n import org.apache.accumulo.core.fate.Fate.TxInfo;\n import org.apache.accumulo.core.fate.FateId;\n+import org.apache.accumulo.core.fate.FateInstanceType;\n import org.apache.accumulo.core.fate.FateKey;\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.FateStore.FateTxStore;\n@@ -296,18 +298,20 @@ protected void testCreateWithKey(FateStore<TestEnv> store, ServerContext sctx) {\n     FateKey fateKey2 =\n         FateKey.forCompactionCommit(ExternalCompactionId.generate(UUID.randomUUID()));\n \n-    FateTxStore<TestEnv> txStore1 = store.createAndReserve(fateKey1).orElseThrow();\n-    FateTxStore<TestEnv> txStore2 = store.createAndReserve(fateKey2).orElseThrow();\n+    var fateId1 = store.seedTransaction(\""TEST\"", fateKey1, new TestRepo(), true).orElseThrow();\n+    var fateId2 = store.seedTransaction(\""TEST\"", fateKey2, new TestRepo(), true).orElseThrow();\n \n-    assertNotEquals(txStore1.getID(), txStore2.getID());\n+    assertNotEquals(fateId1, fateId2);\n \n+    var txStore1 = store.reserve(fateId1);\n+    var txStore2 = store.reserve(fateId2);\n     try {\n       assertTrue(txStore1.timeCreated() > 0);\n-      assertEquals(TStatus.NEW, txStore1.getStatus());\n+      assertEquals(TStatus.SUBMITTED, txStore1.getStatus());\n       assertEquals(fateKey1, txStore1.getKey().orElseThrow());\n \n       assertTrue(txStore2.timeCreated() > 0);\n-      assertEquals(TStatus.NEW, txStore2.getStatus());\n+      assertEquals(TStatus.SUBMITTED, txStore2.getStatus());\n       assertEquals(fateKey2, txStore2.getKey().orElseThrow());\n \n       assertEquals(2, store.list().count());\n@@ -328,18 +332,17 @@ protected void testCreateWithKeyDuplicate(FateStore<TestEnv> store, ServerContex\n     KeyExtent ke =\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n \n-    // Creating with the same key should be fine if the status is NEW\n-    // A second call to createAndReserve() should just return an empty optional\n-    // since it's already in reserved and in progress\n     FateKey fateKey = FateKey.forSplit(ke);\n-    FateTxStore<TestEnv> txStore = store.createAndReserve(fateKey).orElseThrow();\n+    var fateId = store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).orElseThrow();\n \n     // second call is empty\n-    assertTrue(store.createAndReserve(fateKey).isEmpty());\n+    assertTrue(store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).isEmpty());\n+    assertFalse(store.seedTransaction(\""TEST\"", fateId, new TestRepo(), true));\n \n+    var txStore = store.reserve(fateId);\n     try {\n       assertTrue(txStore.timeCreated() > 0);\n-      assertEquals(TStatus.NEW, txStore.getStatus());\n+      assertEquals(TStatus.SUBMITTED, txStore.getStatus());\n       assertEquals(fateKey, txStore.getKey().orElseThrow());\n       assertEquals(1, store.list().count());\n     } finally {\n@@ -359,15 +362,16 @@ protected void testCreateWithKeyInProgress(FateStore<TestEnv> store, ServerConte\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n     FateKey fateKey = FateKey.forSplit(ke);\n \n-    FateTxStore<TestEnv> txStore = store.createAndReserve(fateKey).orElseThrow();\n+    var fateId = store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).orElseThrow();\n \n+    var txStore = store.reserve(fateId);\n     try {\n       assertTrue(txStore.timeCreated() > 0);\n       txStore.setStatus(TStatus.IN_PROGRESS);\n \n       // We have an existing transaction with the same key in progress\n       // so should return an empty Optional\n-      assertTrue(store.createAndReserve(fateKey).isEmpty());\n+      assertTrue(store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).isEmpty());\n       assertEquals(TStatus.IN_PROGRESS, txStore.getStatus());\n     } finally {\n       txStore.setStatus(TStatus.SUCCESSFUL);\n@@ -375,14 +379,20 @@ protected void testCreateWithKeyInProgress(FateStore<TestEnv> store, ServerConte\n       txStore.unreserve(Duration.ZERO);\n     }\n \n+    txStore = null;\n+\n     try {\n       // After deletion, make sure we can create again with the same key\n-      txStore = store.createAndReserve(fateKey).orElseThrow();\n+      var fateId2 = store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).orElseThrow();\n+      txStore = store.reserve(fateId);\n+      assertEquals(fateId, fateId2);\n       assertTrue(txStore.timeCreated() > 0);\n-      assertEquals(TStatus.NEW, txStore.getStatus());\n+      assertEquals(TStatus.SUBMITTED, txStore.getStatus());\n     } finally {\n-      txStore.delete();\n-      txStore.unreserve(Duration.ZERO);\n+      if (txStore != null) {\n+        txStore.delete();\n+        txStore.unreserve(Duration.ZERO);\n+      }\n     }\n \n   }\n@@ -392,8 +402,18 @@ public void testCreateWithKeyCollision() throws Exception {\n     // Replace the default hashing algorithm with one that always returns the same tid so\n     // we can check duplicate detection with different keys\n     executeTest(this::testCreateWithKeyCollision, AbstractFateStore.DEFAULT_MAX_DEFERRED,\n-        (instanceType, fateKey) -> FateId.from(instanceType,\n-            UUID.nameUUIDFromBytes(\""testing uuid\"".getBytes(UTF_8))));\n+        new AbstractFateStore.FateIdGenerator() {\n+          @Override\n+          public FateId fromTypeAndKey(FateInstanceType instanceType, FateKey fateKey) {\n+            return FateId.from(instanceType,\n+                UUID.nameUUIDFromBytes(\""testing uuid\"".getBytes(UTF_8)));\n+          }\n+\n+          @Override\n+          public FateId newRandomId(FateInstanceType instanceType) {\n+            return FateId.from(instanceType, UUID.randomUUID());\n+          }\n+        });\n   }\n \n   protected void testCreateWithKeyCollision(FateStore<TestEnv> store, ServerContext sctx) {\n@@ -404,13 +424,10 @@ protected void testCreateWithKeyCollision(FateStore<TestEnv> store, ServerContex\n     FateKey fateKey1 = FateKey.forSplit(ke1);\n     FateKey fateKey2 = FateKey.forSplit(ke2);\n \n-    FateTxStore<TestEnv> txStore = store.createAndReserve(fateKey1).orElseThrow();\n+    var fateId1 = store.seedTransaction(\""TEST\"", fateKey1, new TestRepo(), true).orElseThrow();\n+    var txStore = store.reserve(fateId1);\n     try {\n-      var e = assertThrows(IllegalStateException.class, () -> store.createAndReserve(fateKey2));\n-      assertEquals(\n-          \""Collision detected for fate id \""\n-              + FateId.from(store.type(), UUID.nameUUIDFromBytes(\""testing uuid\"".getBytes(UTF_8))),\n-          e.getMessage());\n+      assertTrue(store.seedTransaction(\""TEST\"", fateKey2, new TestRepo(), true).isEmpty());\n       assertEquals(fateKey1, txStore.getKey().orElseThrow());\n     } finally {\n       txStore.delete();\n@@ -430,26 +447,192 @@ protected void testCollisionWithRandomFateId(FateStore<TestEnv> store, ServerCon\n         new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n \n     FateKey fateKey = FateKey.forSplit(ke);\n-    FateTxStore<TestEnv> txStore = store.createAndReserve(fateKey).orElseThrow();\n-    FateId fateId = txStore.getID();\n+    var fateId = store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).orElseThrow();\n \n-    // After createAndReserve a fate transaction using a key we can simulate a collision with\n-    // a random FateId by deleting the key out of Fate and calling createAndReserve again to\n+    // After seeding a fate transaction using a key we can simulate a collision with\n+    // a random FateId by deleting the key out of Fate and calling seed again to\n     // verify it detects the key is missing. Then we can continue and see if we can still use\n     // the existing transaction.\n     deleteKey(fateId, sctx);\n-    var e = assertThrows(IllegalStateException.class, () -> store.createAndReserve(fateKey));\n-    assertEquals(\""fate key is missing from fate id \"" + fateId, e.getMessage());\n+    assertTrue(store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).isEmpty());\n \n+    var txStore = store.reserve(fateId);\n     // We should still be able to use the existing transaction\n     try {\n       assertTrue(txStore.timeCreated() > 0);\n+      assertEquals(TStatus.SUBMITTED, txStore.getStatus());\n+    } finally {\n+      txStore.delete();\n+      txStore.unreserve(Duration.ZERO);\n+    }\n+  }\n+\n+  public static final UUID DUPLICATE_UUID = UUID.randomUUID();\n+\n+  public static final List<UUID> UUIDS = List.of(DUPLICATE_UUID, DUPLICATE_UUID, UUID.randomUUID());\n+\n+  @Test\n+  public void testCreate() throws Exception {\n+    AtomicInteger index = new AtomicInteger(0);\n+    executeTest(this::testCreate, AbstractFateStore.DEFAULT_MAX_DEFERRED,\n+        new AbstractFateStore.FateIdGenerator() {\n+          @Override\n+          public FateId fromTypeAndKey(FateInstanceType instanceType, FateKey fateKey) {\n+            return FateId.from(instanceType,\n+                UUID.nameUUIDFromBytes(\""testing uuid\"".getBytes(UTF_8)));\n+          }\n+\n+          @Override\n+          public FateId newRandomId(FateInstanceType instanceType) {\n+            return FateId.from(instanceType, UUIDS.get(index.getAndIncrement() % UUIDS.size()));\n+          }\n+        });\n+  }\n+\n+  protected void testCreate(FateStore<TestEnv> store, ServerContext sctx) throws Exception {\n+\n+    var fateId1 = store.create();\n+    assertEquals(UUIDS.get(0), fateId1.getTxUUID());\n+\n+    // This UUIDS[1] should collide with UUIDS[0] and then the code should retry and end up UUIDS[2]\n+    var fateId2 = store.create();\n+    assertEquals(UUIDS.get(2), fateId2.getTxUUID());\n+\n+    for (var fateId : List.of(fateId1, fateId2)) {\n+      var txStore = store.reserve(fateId);\n+      try {\n+        assertEquals(TStatus.NEW, txStore.getStatus());\n+        assertTrue(txStore.timeCreated() > 0);\n+        assertNull(txStore.top());\n+        assertTrue(txStore.getKey().isEmpty());\n+        assertEquals(fateId, txStore.getID());\n+        assertTrue(txStore.getStack().isEmpty());\n+      } finally {\n+        txStore.unreserve(Duration.ZERO);\n+      }\n+    }\n+\n+    assertEquals(Set.of(fateId1, fateId2),\n+        store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+    var txStore = store.reserve(fateId2);\n+    try {\n+      txStore.delete();\n+    } finally {\n+      txStore.unreserve(Duration.ZERO);\n+    }\n+\n+    assertEquals(Set.of(fateId1),\n+        store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+    txStore = store.reserve(fateId1);\n+    try {\n+      txStore.setStatus(TStatus.SUBMITTED);\n+      txStore.setStatus(TStatus.IN_PROGRESS);\n+      txStore.push(new TestRepo());\n+    } finally {\n+      txStore.unreserve(Duration.ZERO);\n+    }\n+\n+    assertEquals(Set.of(fateId1),\n+        store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+    // should collide again with the first fate id and go to the second\n+    fateId2 = store.create();\n+    assertEquals(UUIDS.get(2), fateId2.getTxUUID());\n+\n+    assertEquals(Set.of(fateId1, fateId2),\n+        store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+    // ensure fateId1 was not altered in anyway by creating fateid2 when it collided\n+    txStore = store.reserve(fateId1);\n+    try {\n+      assertEquals(TStatus.IN_PROGRESS, txStore.getStatus());\n+      assertNotNull(txStore.top());\n+      txStore.forceDelete();\n+    } finally {\n+      txStore.unreserve(Duration.ZERO);\n+    }\n+\n+    assertEquals(Set.of(fateId2),\n+        store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+    txStore = store.reserve(fateId2);\n+    try {\n       assertEquals(TStatus.NEW, txStore.getStatus());\n+      txStore.delete();\n     } finally {\n+      txStore.unreserve(Duration.ZERO);\n+    }\n+\n+    // should be able to recreate something at the same id\n+    fateId1 = store.create();\n+    assertEquals(UUIDS.get(0), fateId1.getTxUUID());\n+    txStore = store.reserve(fateId1);\n+    try {\n+      assertEquals(TStatus.NEW, txStore.getStatus());\n+      assertTrue(txStore.timeCreated() > 0);\n+      assertNull(txStore.top());\n+      assertTrue(txStore.getKey().isEmpty());\n+      assertEquals(fateId1, txStore.getID());\n+      assertTrue(txStore.getStack().isEmpty());\n       txStore.delete();\n+    } finally {\n       txStore.unreserve(Duration.ZERO);\n     }\n \n+    assertEquals(Set.of(), store.list().map(FateIdStatus::getFateId).collect(Collectors.toSet()));\n+\n+  }\n+\n+  @Test\n+  public void testConcurrent() throws Exception {\n+    executeTest(this::testConcurrent);\n+  }\n+\n+  protected void testConcurrent(FateStore<TestEnv> store, ServerContext sctx) throws Exception {\n+    KeyExtent ke =\n+        new KeyExtent(TableId.of(getUniqueNames(1)[0]), new Text(\""zzz\""), new Text(\""aaa\""));\n+    FateKey fateKey = FateKey.forSplit(ke);\n+\n+    var executor = Executors.newFixedThreadPool(10);\n+    try {\n+      // have 10 threads all try to seed the same fate key, only one should succeed.\n+      List<Future<Optional<FateId>>> futures = new ArrayList<>(10);\n+      for (int i = 0; i < 10; i++) {\n+        futures.add(\n+            executor.submit(() -> store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true)));\n+      }\n+\n+      int idsSeen = 0;\n+      for (var future : futures) {\n+        if (future.get().isPresent()) {\n+          idsSeen++;\n+        }\n+      }\n+\n+      assertEquals(1, idsSeen);\n+      assertEquals(1, store.list(FateKey.FateKeyType.SPLIT).count());\n+      assertEquals(0, store.list(FateKey.FateKeyType.COMPACTION_COMMIT).count());\n+\n+      for (var future : futures) {\n+        if (future.get().isPresent()) {\n+          var txStore = store.reserve(future.get().orElseThrow());\n+          try {\n+            txStore.delete();\n+          } finally {\n+            txStore.unreserve(Duration.ZERO);\n+          }\n+        }\n+      }\n+\n+      assertEquals(0, store.list(FateKey.FateKeyType.SPLIT).count());\n+      assertEquals(0, store.list(FateKey.FateKeyType.COMPACTION_COMMIT).count());\n+\n+    } finally {\n+      executor.shutdown();\n+    }\n+\n   }\n \n   @Test\n@@ -500,9 +683,8 @@ protected void testListFateKeys(FateStore<TestEnv> store, ServerContext sctx) th\n \n     Map<FateKey,FateId> fateKeyIds = new HashMap<>();\n     for (FateKey fateKey : List.of(fateKey1, fateKey2, fateKey3, fateKey4)) {\n-      var fateTx = store.createAndReserve(fateKey).orElseThrow();\n-      fateKeyIds.put(fateKey, fateTx.getID());\n-      fateTx.unreserve(Duration.ZERO);\n+      var fateId = store.seedTransaction(\""TEST\"", fateKey, new TestRepo(), true).orElseThrow();\n+      fateKeyIds.put(fateKey, fateId);\n     }\n \n     HashSet<FateId> allIds = new HashSet<>();\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5102"", ""pr_id"": 5102, ""issue_id"": 5096, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Cleanup InstanceOperations API for retrieving active scans and compactions\nThis issue is to implement the suggestions mentioned in https://github.com/apache/accumulo/pull/5072#discussion_r1847008790\r\n\r\nThere's more details in that comment but the main goals are:\r\n\r\n1. Simplify the API for getting active scans and compactions by removing overlap of methods related to filtering servers and instead just creating methods that take a collection of servers to get scans/compactions from.\r\n2. Make getting  getActiveScans multi-threaded like getActiveCompactions is.\r\n"", ""issue_word_count"": 82, ""test_files_count"": 10, ""non_test_files_count"": 10, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java"", ""core/src/main/java/org/apache/accumulo/core/client/admin/ActiveScan.java"", ""core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveCompactionImpl.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveScanImpl.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java"", ""core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ActiveScanIterator.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java"", ""test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java"", ""test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java"", ""test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java""], ""pr_changed_test_files"": [""test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java"", ""test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java"", ""test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java"", ""test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java"", ""test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java"", ""test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java""], ""base_commit"": ""a4ab7a48dd9e097288e776992d218ca84ef3634d"", ""head_commit"": ""69a60d02f22da389fe2622cad4c61156115f1178"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5102"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5102"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-06T18:04:10.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java b/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java\nindex c3b7831162c..3b461e0758b 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveCompaction.java\n@@ -134,9 +134,9 @@ public enum CompactionReason {\n   public abstract List<IteratorSetting> getIterators();\n \n   /**\n-   * Return the host where the compaction is running.\n+   * Return the server where the compaction is running.\n    *\n-   * @since 2.1.0\n+   * @since 4.0.0\n    */\n-  public abstract ServerId getHost();\n+  public abstract ServerId getServerId();\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveScan.java b/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveScan.java\nindex d294acf6bf9..d5659f80a18 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveScan.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/ActiveScan.java\n@@ -21,6 +21,7 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.accumulo.core.client.admin.servers.ServerId;\n import org.apache.accumulo.core.data.Column;\n import org.apache.accumulo.core.data.TabletId;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -96,4 +97,11 @@ public abstract class ActiveScan {\n    * @since 1.5.0\n    */\n   public abstract long getIdleTime();\n+\n+  /**\n+   * Return the server where the scan is running.\n+   *\n+   * @since 4.0.0\n+   */\n+  public abstract ServerId getServerId();\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\nindex b325eaf52b3..8545cd38982 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n@@ -259,22 +259,22 @@ Set<ServerId> getServers(ServerId.Type type, Predicate<String> resourceGroupPred\n    * @param tserver The tablet server address. This should be of the form\n    *        {@code <ip address>:<port>}\n    * @return A list of active scans on tablet server.\n-   * @deprecated see {@link #getActiveScans(ServerId)}\n+   * @deprecated see {@link #getActiveScans(Collection)}\n    */\n   @Deprecated(since = \""4.0.0\"")\n   List<ActiveScan> getActiveScans(String tserver)\n       throws AccumuloException, AccumuloSecurityException;\n \n   /**\n-   * List the active scans on a server.\n+   * List the active scans on a collection of servers.\n    *\n-   * @param server server type and address\n-   * @return A stream of active scans on server.\n+   * @param servers Collection of server types and addresses\n+   * @return A list of active scans on the given servers.\n    * @throws IllegalArgumentException when the type of the server is not TABLET_SERVER or\n    *         SCAN_SERVER\n    * @since 4.0.0\n    */\n-  List<ActiveScan> getActiveScans(ServerId server)\n+  List<ActiveScan> getActiveScans(Collection<ServerId> servers)\n       throws AccumuloException, AccumuloSecurityException;\n \n   /**\n@@ -286,32 +286,20 @@ List<ActiveScan> getActiveScans(ServerId server)\n    * @param tserver The server address. This should be of the form {@code <ip address>:<port>}\n    * @return the list of active compactions\n    * @since 1.5.0\n-   * @deprecated see {@link #getActiveCompactions(ServerId server)}\n+   * @deprecated see {@link #getActiveCompactions(Collection)}\n    */\n   @Deprecated(since = \""4.0.0\"")\n   List<ActiveCompaction> getActiveCompactions(String tserver)\n       throws AccumuloException, AccumuloSecurityException;\n \n-  /**\n-   * List the active compaction running on a TabletServer or Compactor. The server address can be\n-   * retrieved using {@link #getCompactors()} or {@link #getTabletServers()}. Use\n-   * {@link #getActiveCompactions()} to get a list of all compactions running on tservers and\n-   * compactors.\n-   *\n-   * @param server The ServerId object\n-   * @return the list of active compactions\n-   * @throws IllegalArgumentException when the type of the server is not TABLET_SERVER or COMPACTOR\n-   * @since 4.0.0\n-   */\n-  List<ActiveCompaction> getActiveCompactions(ServerId server)\n-      throws AccumuloException, AccumuloSecurityException;\n-\n   /**\n    * List all internal and external compactions running in Accumulo.\n    *\n    * @return the list of active compactions\n    * @since 2.1.0\n+   * @deprecated see {@link #getActiveCompactions(Collection)}\n    */\n+  @Deprecated(since = \""4.0.0\"")\n   List<ActiveCompaction> getActiveCompactions() throws AccumuloException, AccumuloSecurityException;\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveCompactionImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveCompactionImpl.java\nindex a041220fd89..f27aba8add2 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveCompactionImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveCompactionImpl.java\n@@ -119,7 +119,7 @@ public List<IteratorSetting> getIterators() {\n   }\n \n   @Override\n-  public ServerId getHost() {\n+  public ServerId getServerId() {\n     return server;\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveScanImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveScanImpl.java\nindex a3b177c4a88..cf8c845f5f6 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveScanImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ActiveScanImpl.java\n@@ -21,11 +21,13 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n \n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.ActiveScan;\n import org.apache.accumulo.core.client.admin.ScanState;\n import org.apache.accumulo.core.client.admin.ScanType;\n+import org.apache.accumulo.core.client.admin.servers.ServerId;\n import org.apache.accumulo.core.data.Column;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.data.TabletId;\n@@ -44,20 +46,21 @@ public class ActiveScanImpl extends ActiveScan {\n \n   private final long scanId;\n   private final String client;\n-  private String tableName;\n+  private final String tableName;\n   private final long age;\n   private final long idle;\n-  private ScanType type;\n-  private ScanState state;\n-  private KeyExtent extent;\n-  private List<Column> columns;\n-  private List<String> ssiList;\n-  private Map<String,Map<String,String>> ssio;\n+  private final ScanType type;\n+  private final ScanState state;\n+  private final KeyExtent extent;\n+  private final List<Column> columns;\n+  private final List<String> ssiList;\n+  private final Map<String,Map<String,String>> ssio;\n   private final String user;\n-  private Authorizations authorizations;\n+  private final Authorizations authorizations;\n+  private final ServerId server;\n \n   ActiveScanImpl(ClientContext context,\n-      org.apache.accumulo.core.tabletscan.thrift.ActiveScan activeScan)\n+      org.apache.accumulo.core.tabletscan.thrift.ActiveScan activeScan, ServerId server)\n       throws TableNotFoundException {\n     this.scanId = activeScan.scanId;\n     this.client = activeScan.client;\n@@ -81,6 +84,7 @@ public class ActiveScanImpl extends ActiveScan {\n       this.ssiList.add(ii.iterName + \""=\"" + ii.priority + \"",\"" + ii.className);\n     }\n     this.ssio = activeScan.ssio;\n+    this.server = Objects.requireNonNull(server);\n   }\n \n   @Override\n@@ -152,4 +156,9 @@ public Authorizations getAuthorizations() {\n   public long getIdleTime() {\n     return idle;\n   }\n+\n+  @Override\n+  public ServerId getServerId() {\n+    return server;\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\nindex 7886b9739b8..e17ca1f0a42 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n@@ -25,6 +25,7 @@\n import static org.apache.accumulo.core.rpc.ThriftUtil.getClient;\n import static org.apache.accumulo.core.rpc.ThriftUtil.returnClient;\n import static org.apache.accumulo.core.util.threads.ThreadPoolNames.INSTANCE_OPS_COMPACTIONS_FINDER_POOL;\n+import static org.apache.accumulo.core.util.threads.ThreadPoolNames.INSTANCE_OPS_SCANS_FINDER_POOL;\n \n import java.time.Duration;\n import java.util.ArrayList;\n@@ -38,6 +39,7 @@\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Future;\n import java.util.function.BiPredicate;\n import java.util.function.Consumer;\n@@ -51,6 +53,7 @@\n import org.apache.accumulo.core.client.admin.ActiveScan;\n import org.apache.accumulo.core.client.admin.InstanceOperations;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n+import org.apache.accumulo.core.client.admin.servers.ServerId.Type;\n import org.apache.accumulo.core.clientImpl.thrift.ClientService;\n import org.apache.accumulo.core.clientImpl.thrift.ConfigurationType;\n import org.apache.accumulo.core.clientImpl.thrift.TVersionedProperties;\n@@ -72,12 +75,14 @@\n import org.apache.accumulo.core.util.LocalityGroupUtil.LocalityGroupConfigurationError;\n import org.apache.accumulo.core.util.Retry;\n import org.apache.accumulo.core.util.compaction.ExternalCompactionUtil;\n+import org.apache.accumulo.core.util.threads.ThreadPoolNames;\n import org.apache.thrift.TException;\n import org.apache.thrift.transport.TTransport;\n import org.slf4j.LoggerFactory;\n \n import com.google.common.base.Preconditions;\n import com.google.common.net.HostAndPort;\n+import com.google.common.util.concurrent.MoreExecutors;\n \n /**\n  * Provides a class for administering the accumulo instance\n@@ -265,33 +270,18 @@ public List<String> getTabletServers() {\n   @Deprecated(since = \""4.0.0\"")\n   public List<ActiveScan> getActiveScans(String tserver)\n       throws AccumuloException, AccumuloSecurityException {\n-    final var parsedTserver = HostAndPort.fromString(tserver);\n-    TabletScanClientService.Client client = null;\n-    try {\n-      client = getClient(ThriftClientTypes.TABLET_SCAN, parsedTserver, context);\n-\n-      List<ActiveScan> as = new ArrayList<>();\n-      for (var activeScan : client.getActiveScans(TraceUtil.traceInfo(), context.rpcCreds())) {\n-        try {\n-          as.add(new ActiveScanImpl(context, activeScan));\n-        } catch (TableNotFoundException e) {\n-          throw new AccumuloException(e);\n-        }\n-      }\n-      return as;\n-    } catch (ThriftSecurityException e) {\n-      throw new AccumuloSecurityException(e.user, e.code, e);\n-    } catch (TException e) {\n-      throw new AccumuloException(e);\n-    } finally {\n-      if (client != null) {\n-        returnClient(client, context);\n-      }\n-    }\n+    var si = getServerId(tserver, List.of(Type.TABLET_SERVER, Type.SCAN_SERVER));\n+    // getActiveScans throws exceptions so we can't use Optional.map() here\n+    return si.isPresent() ? getActiveScans(si.orElseThrow()) : List.of();\n   }\n \n   @Override\n-  public List<ActiveScan> getActiveScans(ServerId server)\n+  public List<ActiveScan> getActiveScans(Collection<ServerId> servers)\n+      throws AccumuloException, AccumuloSecurityException {\n+    return queryServers(servers, this::getActiveScans, INSTANCE_OPS_SCANS_FINDER_POOL);\n+  }\n+\n+  private List<ActiveScan> getActiveScans(ServerId server)\n       throws AccumuloException, AccumuloSecurityException {\n \n     Objects.requireNonNull(server);\n@@ -309,7 +299,7 @@ public List<ActiveScan> getActiveScans(ServerId server)\n       List<ActiveScan> as = new ArrayList<>();\n       for (var activeScan : rpcClient.getActiveScans(TraceUtil.traceInfo(), context.rpcCreds())) {\n         try {\n-          as.add(new ActiveScanImpl(context, activeScan));\n+          as.add(new ActiveScanImpl(context, activeScan, server));\n         } catch (TableNotFoundException e) {\n           throw new AccumuloException(e);\n         }\n@@ -337,21 +327,12 @@ public boolean testClassLoad(final String className, final String asTypeName)\n   @Deprecated\n   public List<ActiveCompaction> getActiveCompactions(String server)\n       throws AccumuloException, AccumuloSecurityException {\n-\n-    HostAndPort hp = HostAndPort.fromString(server);\n-\n-    ServerId si = getServer(ServerId.Type.COMPACTOR, null, hp.getHost(), hp.getPort());\n-    if (si == null) {\n-      si = getServer(ServerId.Type.TABLET_SERVER, null, hp.getHost(), hp.getPort());\n-    }\n-    if (si == null) {\n-      return List.of();\n-    }\n-    return getActiveCompactions(si);\n+    var si = getServerId(server, List.of(Type.COMPACTOR, Type.TABLET_SERVER));\n+    // getActiveCompactions throws exceptions so we can't use Optional.map() here\n+    return si.isPresent() ? getActiveCompactions(si.orElseThrow()) : List.of();\n   }\n \n-  @Override\n-  public List<ActiveCompaction> getActiveCompactions(ServerId server)\n+  private List<ActiveCompaction> getActiveCompactions(ServerId server)\n       throws AccumuloException, AccumuloSecurityException {\n \n     Objects.requireNonNull(server);\n@@ -391,6 +372,7 @@ public List<ActiveCompaction> getActiveCompactions(ServerId server)\n   }\n \n   @Override\n+  @Deprecated\n   public List<ActiveCompaction> getActiveCompactions()\n       throws AccumuloException, AccumuloSecurityException {\n \n@@ -404,19 +386,34 @@ public List<ActiveCompaction> getActiveCompactions()\n   @Override\n   public List<ActiveCompaction> getActiveCompactions(Collection<ServerId> compactionServers)\n       throws AccumuloException, AccumuloSecurityException {\n+    return queryServers(compactionServers, this::getActiveCompactions,\n+        INSTANCE_OPS_COMPACTIONS_FINDER_POOL);\n+  }\n+\n+  private <T> List<T> queryServers(Collection<ServerId> servers, ServerQuery<List<T>> serverQuery,\n+      ThreadPoolNames pool) throws AccumuloException, AccumuloSecurityException {\n+\n+    final ExecutorService executorService;\n+    // If size 0 or 1 there's no need to create a thread pool\n+    if (servers.isEmpty()) {\n+      return List.of();\n+    } else if (servers.size() == 1) {\n+      executorService = MoreExecutors.newDirectExecutorService();\n+    } else {\n+      int numThreads = Math.max(4, Math.min((servers.size()) / 10, 256));\n+      executorService =\n+          context.threadPools().getPoolBuilder(pool).numCoreThreads(numThreads).build();\n+    }\n \n-    int numThreads = Math.max(4, Math.min((compactionServers.size()) / 10, 256));\n-    var executorService = context.threadPools().getPoolBuilder(INSTANCE_OPS_COMPACTIONS_FINDER_POOL)\n-        .numCoreThreads(numThreads).build();\n     try {\n-      List<Future<List<ActiveCompaction>>> futures = new ArrayList<>();\n+      List<Future<List<T>>> futures = new ArrayList<>();\n \n-      for (ServerId server : compactionServers) {\n-        futures.add(executorService.submit(() -> getActiveCompactions(server)));\n+      for (ServerId server : servers) {\n+        futures.add(executorService.submit(() -> serverQuery.execute(server)));\n       }\n \n-      List<ActiveCompaction> ret = new ArrayList<>();\n-      for (Future<List<ActiveCompaction>> future : futures) {\n+      List<T> ret = new ArrayList<>();\n+      for (Future<List<T>> future : futures) {\n         try {\n           ret.addAll(future.get());\n         } catch (InterruptedException | ExecutionException e) {\n@@ -635,4 +632,13 @@ private ServerId createServerId(ServerId.Type type, ServiceLockPath slp) {\n     return new ServerId(type, resourceGroup, host, port);\n   }\n \n+  private Optional<ServerId> getServerId(String server, List<Type> types) {\n+    HostAndPort hp = HostAndPort.fromString(server);\n+    return types.stream().map(type -> getServer(type, null, hp.getHost(), hp.getPort()))\n+        .findFirst();\n+  }\n+\n+  interface ServerQuery<T> {\n+    T execute(ServerId server) throws AccumuloException, AccumuloSecurityException;\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\nindex 87fab3bc913..b57baf0d5d5 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/threads/ThreadPoolNames.java\n@@ -39,6 +39,7 @@ public enum ThreadPoolNames {\n   SERVICE_LOCK_POOL(\""accumulo.pool.service.lock\""),\n   IMPORT_TABLE_RENAME_POOL(\""accumulo.pool.import.table.rename\""),\n   INSTANCE_OPS_COMPACTIONS_FINDER_POOL(\""accumulo.pool.instance.ops.active.compactions.finder\""),\n+  INSTANCE_OPS_SCANS_FINDER_POOL(\""accumulo.pool.instance.ops.active.scans.finder\""),\n   MANAGER_FATE_POOL(\""accumulo.pool.manager.fate\""),\n   MANAGER_STATUS_POOL(\""accumulo.pool.manager.status\""),\n   MANAGER_UPGRADE_COORDINATOR_METADATA_POOL(\""accumulo.pool.manager.upgrade.metadata\""),\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\nindex 732c68db573..87815512a4f 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\n@@ -86,7 +86,7 @@ private static String formatActiveCompactionLine(ActiveCompaction ac) {\n     }\n \n     String hostSuffix;\n-    switch (ac.getHost().getType()) {\n+    switch (ac.getServerId().getType()) {\n       case TABLET_SERVER:\n         hostSuffix = \""\"";\n         break;\n@@ -94,17 +94,17 @@ private static String formatActiveCompactionLine(ActiveCompaction ac) {\n         hostSuffix = \"" (ext)\"";\n         break;\n       default:\n-        hostSuffix = ac.getHost().getType().name();\n+        hostSuffix = ac.getServerId().getType().name();\n         break;\n     }\n \n-    String host = ac.getHost().toHostPortString() + hostSuffix;\n+    String host = ac.getServerId().toHostPortString() + hostSuffix;\n \n     try {\n       var dur = new DurationFormat(ac.getAge(), \""\"");\n       return String.format(\n           \""%21s | %21s | %9s | %5s | %6s | %5s | %5s | %15s | %-40s | %5s | %35s | %9s | %s\"",\n-          ac.getHost().getResourceGroup(), host, dur, ac.getType(), ac.getReason(),\n+          ac.getServerId().getResourceGroup(), host, dur, ac.getType(), ac.getReason(),\n           shortenCount(ac.getEntriesRead()), shortenCount(ac.getEntriesWritten()), ac.getTable(),\n           ac.getTablet(), ac.getInputFiles().size(), output, iterList, iterOpts);\n     } catch (TableNotFoundException e) {\n@@ -132,7 +132,8 @@ public static Stream<String> activeCompactionsForServer(String tserver,\n       return Stream.of();\n     } else {\n       try {\n-        return instanceOps.getActiveCompactions(server).stream().sorted(COMPACTION_AGE_DESCENDING)\n+        return instanceOps.getActiveCompactions(List.of(server)).stream()\n+            .sorted(COMPACTION_AGE_DESCENDING)\n             .map(ActiveCompactionHelper::formatActiveCompactionLine);\n       } catch (Exception e) {\n         LOG.debug(\""Failed to list active compactions for server {}\"", tserver, e);\n@@ -160,16 +161,19 @@ public static Stream<String> activeCompactions(InstanceOperations instanceOps,\n \n   public static Stream<String> activeCompactions(InstanceOperations instanceOps) {\n     try {\n-      return sortActiveCompactions(instanceOps.getActiveCompactions());\n+      Set<ServerId> compactionServers = new HashSet<>();\n+      compactionServers.addAll(instanceOps.getServers(ServerId.Type.COMPACTOR));\n+      compactionServers.addAll(instanceOps.getServers(ServerId.Type.TABLET_SERVER));\n+      return sortActiveCompactions(instanceOps.getActiveCompactions(compactionServers));\n     } catch (AccumuloException | AccumuloSecurityException e) {\n       return Stream.of(\""ERROR \"" + e.getMessage());\n     }\n   }\n \n   private static Stream<String> sortActiveCompactions(List<ActiveCompaction> activeCompactions) {\n-    Comparator<ActiveCompaction> comparator =\n-        Comparator.comparing((ActiveCompaction ac) -> ac.getHost().getHost())\n-            .thenComparing(ac -> ac.getHost().getPort()).thenComparing(COMPACTION_AGE_DESCENDING);\n+    Comparator<ActiveCompaction> comparator = Comparator\n+        .comparing((ActiveCompaction ac) -> ac.getServerId().getHost())\n+        .thenComparing(ac -> ac.getServerId().getPort()).thenComparing(COMPACTION_AGE_DESCENDING);\n     return activeCompactions.stream().sorted(comparator)\n         .map(ActiveCompactionHelper::formatActiveCompactionLine);\n   }\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveScanIterator.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveScanIterator.java\ndeleted file mode 100644\nindex 6eb6a076d8d..00000000000\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveScanIterator.java\n+++ /dev/null\n@@ -1,104 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   https://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.accumulo.shell.commands;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Set;\n-\n-import org.apache.accumulo.core.client.admin.ActiveScan;\n-import org.apache.accumulo.core.client.admin.InstanceOperations;\n-import org.apache.accumulo.core.client.admin.ScanType;\n-import org.apache.accumulo.core.client.admin.servers.ServerId;\n-import org.apache.accumulo.core.util.DurationFormat;\n-\n-class ActiveScanIterator implements Iterator<String> {\n-\n-  private InstanceOperations instanceOps;\n-  private Iterator<ServerId> tsIter;\n-  private Iterator<String> scansIter;\n-\n-  private void readNext() {\n-    final List<String> scans = new ArrayList<>();\n-\n-    while (tsIter.hasNext()) {\n-\n-      final ServerId server = tsIter.next();\n-      try {\n-        final List<ActiveScan> asl = instanceOps.getActiveScans(server);\n-\n-        for (ActiveScan as : asl) {\n-          var dur = new DurationFormat(as.getAge(), \""\"");\n-          var dur2 = new DurationFormat(as.getLastContactTime(), \""\"");\n-          scans.add(String.format(\n-              \""%21s |%21s |%21s |%9s |%9s |%7s |%6s |%8s |%8s |%10s |%20s |%10s |%20s |%10s | %s\"",\n-              server.getResourceGroup(), server.toHostPortString(), as.getClient(), dur, dur2,\n-              as.getState(), as.getType(), as.getUser(), as.getTable(), as.getColumns(),\n-              as.getAuthorizations(), (as.getType() == ScanType.SINGLE ? as.getTablet() : \""N/A\""),\n-              as.getScanid(), as.getSsiList(), as.getSsio()));\n-        }\n-      } catch (Exception e) {\n-        scans.add(server + \"" ERROR \"" + e.getMessage());\n-      }\n-\n-      if (!scans.isEmpty()) {\n-        break;\n-      }\n-    }\n-\n-    scansIter = scans.iterator();\n-  }\n-\n-  ActiveScanIterator(Set<ServerId> tservers, InstanceOperations instanceOps) {\n-    this.instanceOps = instanceOps;\n-    this.tsIter = tservers.iterator();\n-\n-    final String header = String.format(\n-        \"" %-21s| %-21s| %-21s| %-9s| %-9s| %-7s| %-6s|\""\n-            + \"" %-8s| %-8s| %-10s| %-20s| %-10s| %-10s | %-20s | %s\"",\n-        \""GROUP\"", \""SERVER\"", \""CLIENT\"", \""AGE\"", \""LAST\"", \""STATE\"", \""TYPE\"", \""USER\"", \""TABLE\"", \""COLUMNS\"",\n-        \""AUTHORIZATIONS\"", \""TABLET\"", \""SCAN ID\"", \""ITERATORS\"", \""ITERATOR OPTIONS\"");\n-\n-    scansIter = Collections.singletonList(header).iterator();\n-  }\n-\n-  @Override\n-  public boolean hasNext() {\n-    return scansIter.hasNext();\n-  }\n-\n-  @Override\n-  public String next() {\n-    final String next = scansIter.next();\n-\n-    if (!scansIter.hasNext()) {\n-      readNext();\n-    }\n-\n-    return next;\n-  }\n-\n-  @Override\n-  public void remove() {\n-    throw new UnsupportedOperationException();\n-  }\n-\n-}\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\nindex c8283cfb22e..dbda36a8c0b 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n@@ -18,15 +18,20 @@\n  */\n package org.apache.accumulo.shell.commands;\n \n-import java.util.HashSet;\n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.Optional;\n-import java.util.Set;\n import java.util.function.BiPredicate;\n import java.util.function.Predicate;\n import java.util.regex.Pattern;\n+import java.util.stream.Stream;\n \n+import org.apache.accumulo.core.client.AccumuloException;\n+import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.admin.InstanceOperations;\n+import org.apache.accumulo.core.client.admin.ScanType;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n+import org.apache.accumulo.core.util.DurationFormat;\n import org.apache.accumulo.shell.Shell;\n import org.apache.accumulo.shell.Shell.Command;\n import org.apache.commons.cli.CommandLine;\n@@ -34,6 +39,7 @@\n import org.apache.commons.cli.Options;\n \n import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n \n public class ListScansCommand extends Command {\n \n@@ -51,7 +57,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n \n     final InstanceOperations instanceOps = shellState.getAccumuloClient().instanceOperations();\n     final boolean paginate = !cl.hasOption(disablePaginationOpt.getOpt());\n-    final Set<ServerId> servers = new HashSet<>();\n+    final List<ServerId> servers = new ArrayList<>();\n \n     String serverValue = getServerOptValue(cl, serverOpt, tserverOption);\n     if (serverValue != null || cl.hasOption(rgOpt)) {\n@@ -66,11 +72,34 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n       servers.addAll(instanceOps.getServers(ServerId.Type.TABLET_SERVER));\n     }\n \n-    shellState.printLines(new ActiveScanIterator(servers, instanceOps), paginate);\n+    Stream<String> activeScans = getActiveScans(instanceOps, servers);\n+    activeScans = appendHeader(activeScans);\n+    shellState.printLines(activeScans.iterator(), paginate);\n \n     return 0;\n   }\n \n+  private Stream<String> getActiveScans(InstanceOperations instanceOps, List<ServerId> servers) {\n+    List<List<ServerId>> partServerIds = Lists.partition(servers, 100);\n+    return partServerIds.stream().flatMap(ids -> {\n+      try {\n+        return instanceOps.getActiveScans(ids).stream().map(as -> {\n+          var dur = new DurationFormat(as.getAge(), \""\"");\n+          var dur2 = new DurationFormat(as.getLastContactTime(), \""\"");\n+          var server = as.getServerId();\n+          return (String.format(\n+              \""%21s |%21s |%21s |%9s |%9s |%7s |%6s |%8s |%8s |%10s |%20s |%10s |%20s |%10s | %s\"",\n+              server.getResourceGroup(), server.toHostPortString(), as.getClient(), dur, dur2,\n+              as.getState(), as.getType(), as.getUser(), as.getTable(), as.getColumns(),\n+              as.getAuthorizations(), (as.getType() == ScanType.SINGLE ? as.getTablet() : \""N/A\""),\n+              as.getScanid(), as.getSsiList(), as.getSsio()));\n+        });\n+      } catch (AccumuloException | AccumuloSecurityException e) {\n+        return Stream.of(\""ERROR \"" + e.getMessage());\n+      }\n+    });\n+  }\n+\n   @Override\n   public int numArgs() {\n     return 0;\n@@ -120,4 +149,12 @@ static Predicate<String> rgRegexPredicate(String rgRegex) {\n         .orElse(rg -> true);\n   }\n \n+  private static Stream<String> appendHeader(Stream<String> stream) {\n+    Stream<String> header = Stream.of(String.format(\n+        \"" %-21s| %-21s| %-21s| %-9s| %-9s| %-7s| %-6s|\""\n+            + \"" %-8s| %-8s| %-10s| %-20s| %-10s| %-10s | %-20s | %s\"",\n+        \""GROUP\"", \""SERVER\"", \""CLIENT\"", \""AGE\"", \""LAST\"", \""STATE\"", \""TYPE\"", \""USER\"", \""TABLE\"", \""COLUMNS\"",\n+        \""AUTHORIZATIONS\"", \""TABLET\"", \""SCAN ID\"", \""ITERATORS\"", \""ITERATOR OPTIONS\""));\n+    return Stream.concat(header, stream);\n+  }\n }\n"", ""test_patch"": ""diff --git a/test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java b/test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java\nindex 5fd3df9ab41..17ec90fa688 100644\n--- a/test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/InstanceOperationsIT.java\n@@ -154,18 +154,19 @@ public void testGetServers() throws AccumuloException, AccumuloSecurityException\n       assertEquals(0, iops.getServers(ServerId.Type.MONITOR).size());\n \n       for (ServerId compactor : iops.getServers(ServerId.Type.COMPACTOR)) {\n-        assertNotNull(iops.getActiveCompactions(compactor));\n-        assertThrows(IllegalArgumentException.class, () -> iops.getActiveScans(compactor));\n+        assertNotNull(iops.getActiveCompactions(List.of(compactor)));\n+        assertThrows(IllegalArgumentException.class, () -> iops.getActiveScans(List.of(compactor)));\n       }\n \n       for (ServerId tserver : iops.getServers(ServerId.Type.TABLET_SERVER)) {\n-        assertNotNull(iops.getActiveCompactions(tserver));\n-        assertNotNull(iops.getActiveScans(tserver));\n+        assertNotNull(iops.getActiveCompactions(List.of(tserver)));\n+        assertNotNull(iops.getActiveScans(List.of(tserver)));\n       }\n \n       for (ServerId sserver : iops.getServers(ServerId.Type.SCAN_SERVER)) {\n-        assertThrows(IllegalArgumentException.class, () -> iops.getActiveCompactions(sserver));\n-        assertNotNull(iops.getActiveScans(sserver));\n+        assertThrows(IllegalArgumentException.class,\n+            () -> iops.getActiveCompactions(List.of(sserver)));\n+        assertNotNull(iops.getActiveScans(List.of(sserver)));\n       }\n \n     }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java b/test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java\nindex eced2eb8b41..9705a69f7d7 100644\n--- a/test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/InterruptibleScannersIT.java\n@@ -23,6 +23,7 @@\n \n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.List;\n \n import org.apache.accumulo.core.client.Accumulo;\n import org.apache.accumulo.core.client.AccumuloClient;\n@@ -74,7 +75,7 @@ public void test() throws Exception {\n                 .iterator().next();\n             do {\n               ArrayList<ActiveScan> scans =\n-                  new ArrayList<>(client.instanceOperations().getActiveScans(tserver));\n+                  new ArrayList<>(client.instanceOperations().getActiveScans(List.of(tserver)));\n               // Remove scans not against our table and not owned by us\n               scans.removeIf(scan -> !getAdminPrincipal().equals(scan.getUser())\n                   || !tableName.equals(scan.getTable()));\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java b/test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java\nindex 7fc9d5c7975..210cd673730 100644\n--- a/test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/ZombieScanIT.java\n@@ -46,6 +46,7 @@\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.ScannerBase.ConsistencyLevel;\n+import org.apache.accumulo.core.client.admin.ActiveScan;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n import org.apache.accumulo.core.client.admin.ScanType;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n@@ -238,7 +239,7 @@ public void testZombieScan() throws Exception {\n       // system.\n       Set<ServerId> tabletServersWithZombieScans = new HashSet<>();\n       for (ServerId tserver : c.instanceOperations().getServers(ServerId.Type.TABLET_SERVER)) {\n-        if (c.instanceOperations().getActiveScans(tserver).stream()\n+        if (c.instanceOperations().getActiveScans(List.of(tserver)).stream()\n             .flatMap(activeScan -> activeScan.getSsiList().stream())\n             .anyMatch(scanIters -> scanIters.contains(ZombieIterator.class.getName()))) {\n           tabletServersWithZombieScans.add(tserver);\n@@ -355,13 +356,9 @@ private static long countLocations(String table, AccumuloClient client) throws E\n   private static long countDistinctTabletsScans(String table, AccumuloClient client)\n       throws Exception {\n     Set<ServerId> tservers = client.instanceOperations().getServers(ServerId.Type.TABLET_SERVER);\n-    long count = 0;\n-    for (ServerId tserver : tservers) {\n-      count += client.instanceOperations().getActiveScans(tserver).stream()\n-          .filter(activeScan -> activeScan.getTable().equals(table))\n-          .map(activeScan -> activeScan.getTablet()).distinct().count();\n-    }\n-    return count;\n+    return client.instanceOperations().getActiveScans(tservers).stream()\n+        .filter(activeScan -> activeScan.getTable().equals(table)).map(ActiveScan::getTablet)\n+        .distinct().count();\n   }\n \n   private Future<String> startStuckScan(AccumuloClient c, String table, ExecutorService executor,\n@@ -420,12 +417,11 @@ private void assertValidScanIds(AccumuloClient c)\n       throws AccumuloException, AccumuloSecurityException {\n     Set<Long> scanIds = new HashSet<>();\n     Set<ScanType> scanTypes = new HashSet<>();\n-    for (ServerId tserver : c.instanceOperations().getServers(ServerId.Type.TABLET_SERVER)) {\n-      c.instanceOperations().getActiveScans(tserver).forEach(activeScan -> {\n-        scanIds.add(activeScan.getScanid());\n-        scanTypes.add(activeScan.getType());\n-      });\n-    }\n+    var tservers = c.instanceOperations().getServers(ServerId.Type.TABLET_SERVER);\n+    c.instanceOperations().getActiveScans(tservers).forEach(activeScan -> {\n+      scanIds.add(activeScan.getScanid());\n+      scanTypes.add(activeScan.getType());\n+    });\n     assertNotEquals(0, scanIds.size());\n     scanIds.forEach(id -> assertTrue(id != 0L && id != -1L));\n     // ensure coverage of both batch and single scans\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\nindex c3e24401a2b..b3ed3c26fa2 100644\n--- a/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n+++ b/test/src/main/java/org/apache/accumulo/test/compaction/ExternalCompactionTestUtils.java\n@@ -43,8 +43,11 @@\n import org.apache.accumulo.core.client.IteratorSetting;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n+import org.apache.accumulo.core.client.admin.ActiveCompaction;\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n+import org.apache.accumulo.core.client.admin.InstanceOperations;\n import org.apache.accumulo.core.client.admin.NewTableConfiguration;\n+import org.apache.accumulo.core.client.admin.servers.ServerId;\n import org.apache.accumulo.core.clientImpl.ClientContext;\n import org.apache.accumulo.core.compaction.thrift.CompactionCoordinatorService;\n import org.apache.accumulo.core.compaction.thrift.TCompactionState;\n@@ -389,4 +392,12 @@ public static void assertNoCompactionMetadata(TabletMetadata tabletMetadata) {\n     assertEquals(Set.of(), tabletMetadata.getExternalCompactions().keySet());\n     assertEquals(Set.of(), tabletMetadata.getUserCompactionsRequested());\n   }\n+\n+  public static List<ActiveCompaction> getActiveCompactions(InstanceOperations instanceOps)\n+      throws AccumuloException, AccumuloSecurityException {\n+    Set<ServerId> compactionServers = new HashSet<>();\n+    compactionServers.addAll(instanceOps.getServers(ServerId.Type.COMPACTOR));\n+    compactionServers.addAll(instanceOps.getServers(ServerId.Type.TABLET_SERVER));\n+    return instanceOps.getActiveCompactions(compactionServers);\n+  }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java b/test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java\nindex 0f7acde3b3a..b7cf0f4bc87 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/CompactionIT.java\n@@ -21,6 +21,7 @@\n import static java.util.concurrent.TimeUnit.SECONDS;\n import static java.util.stream.Collectors.toList;\n import static java.util.stream.Collectors.toSet;\n+import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.getActiveCompactions;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n@@ -1158,7 +1159,7 @@ public void testGetActiveCompactions() throws Exception {\n \n       List<ActiveCompaction> compactions = new ArrayList<>();\n       do {\n-        client.instanceOperations().getActiveCompactions().forEach((ac) -> {\n+        getActiveCompactions(client.instanceOperations()).forEach((ac) -> {\n           try {\n             if (ac.getTable().equals(table1)) {\n               compactions.add(ac);\n@@ -1171,12 +1172,12 @@ public void testGetActiveCompactions() throws Exception {\n       } while (compactions.isEmpty());\n \n       ActiveCompaction running1 = compactions.get(0);\n-      ServerId host = running1.getHost();\n+      ServerId host = running1.getServerId();\n       assertTrue(host.getType() == ServerId.Type.COMPACTOR);\n \n       compactions.clear();\n       do {\n-        client.instanceOperations().getActiveCompactions(host).forEach((ac) -> {\n+        client.instanceOperations().getActiveCompactions(List.of(host)).forEach((ac) -> {\n           try {\n             if (ac.getTable().equals(table1)) {\n               compactions.add(ac);\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java b/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java\nindex 87cf984f90e..48b2bdb632c 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMajCIT.java\n@@ -19,6 +19,7 @@\n package org.apache.accumulo.test.functional;\n \n import static org.apache.accumulo.core.metrics.Metric.MAJC_PAUSED;\n+import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.getActiveCompactions;\n import static org.apache.accumulo.test.util.Wait.waitFor;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNull;\n@@ -188,7 +189,7 @@ public void testMajCPauses() throws Exception {\n       compactionThread.interrupt();\n       compactionThread.join();\n       assertNull(error.get());\n-      assertTrue(client.instanceOperations().getActiveCompactions().stream()\n+      assertTrue(getActiveCompactions(client.instanceOperations()).stream()\n           .anyMatch(ac -> ac.getPausedCount() > 0));\n     }\n \n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java b/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java\nindex f3d957f4377..5ceee6c6013 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/MemoryStarvedMinCIT.java\n@@ -19,6 +19,7 @@\n package org.apache.accumulo.test.functional;\n \n import static org.apache.accumulo.core.metrics.Metric.MINC_PAUSED;\n+import static org.apache.accumulo.test.compaction.ExternalCompactionTestUtils.getActiveCompactions;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNull;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n@@ -155,7 +156,7 @@ public void testMinCPauses() throws Exception {\n         ingestThread.interrupt();\n         ingestThread.join();\n         assertNull(error.get());\n-        assertTrue(client.instanceOperations().getActiveCompactions().stream()\n+        assertTrue(getActiveCompactions(client.instanceOperations()).stream()\n             .anyMatch(ac -> ac.getPausedCount() > 0));\n       }\n     }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java b/test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java\nindex 02519e10af7..5b974944d89 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/ScanIdIT.java\n@@ -189,7 +189,7 @@ private Set<Long> getScanIds(AccumuloClient client)\n       List<ActiveScan> activeScans = null;\n       for (int i = 0; i < 10; i++) {\n         try {\n-          activeScans = client.instanceOperations().getActiveScans(tserver);\n+          activeScans = client.instanceOperations().getActiveScans(List.of(tserver));\n           break;\n         } catch (AccumuloException e) {\n           if (e.getCause() instanceof TableNotFoundException) {\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java b/test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java\nindex 1a19e0cae61..3458ceb9b3e 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/ScannerIT.java\n@@ -226,11 +226,7 @@ public static long countActiveScans(AccumuloClient c, ServerType serverType, Str\n       throw new IllegalArgumentException(\""Unsupported server type \"" + serverType);\n     }\n \n-    long count = 0;\n-    for (ServerId tserver : servers) {\n-      count += c.instanceOperations().getActiveScans(tserver).stream()\n-          .filter(activeScan -> activeScan.getTable().equals(tableName)).count();\n-    }\n-    return count;\n+    return c.instanceOperations().getActiveScans(servers).stream()\n+        .filter(activeScan -> activeScan.getTable().equals(tableName)).count();\n   }\n }\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java b/test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java\nindex 2f3288433b0..4754e418852 100644\n--- a/test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/functional/SessionBlockVerifyIT.java\n@@ -145,11 +145,8 @@ public void run() throws Exception {\n         }\n \n         int sessionsFound = 0;\n-        // we have configured 1 tserver, so we can grab the one and only\n-        ServerId tserver =\n-            getOnlyElement(c.instanceOperations().getServers(ServerId.Type.TABLET_SERVER));\n-\n-        final List<ActiveScan> scans = c.instanceOperations().getActiveScans(tserver);\n+        var tservers = c.instanceOperations().getServers(ServerId.Type.TABLET_SERVER);\n+        final List<ActiveScan> scans = c.instanceOperations().getActiveScans(tservers);\n \n         for (ActiveScan scan : scans) {\n           // only here to minimize chance of seeing meta extent scans\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5072"", ""pr_id"": 5072, ""issue_id"": 5069, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Add server and resource group pattern options to listscans and listcompactions\n**Is your feature request related to a problem? Please describe.**\r\n\r\nThe listscans and listcompactions command contact every server to gather information unless the tserver option is given in which case a single tablet server, scan server, or compactor is contacted.  This option name no longer makes sense. Also it would be really useful to be able to quickly list compactions of scans for a subset of servers instead of a single server or all servers.\r\n\r\n**Describe the solution you'd like**\r\n\r\nRemove/deprecate the tablet server option and replace it with the following options that take a reg ex.\r\n\r\n  * `-rg/--regource-group <regex>`\r\n  * `-s/--server <regex>`\r\n\r\nThe implementation of these options could call this [function](https://github.com/apache/accumulo/blob/78fd622c022b7186761aabfcb687609d900569db/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java#L251) passing predicates that execute the regex.  This approach will offer the following benefits for filtering servers.\r\n\r\n * Getting the list of servers requires a lot of back and forth RPCs to zookeeper.  Passing in regex predicates to the InstanceOperations.getServers function will reduce the amount of zookeeper RPCs and the overall latency of the command.\r\n * Once a list of servers has been efficiently obtained an RPC to each one must be made to obtain the scan or compaction information.  Operating on a subset of servers for these RPCs will reduce the overall latency of the command.\r\n\r\nWith these options it would be possible to efficiently only list scans on a resource group, subset of servers in a resource group, severs on a rack (if the server naming scheme supports this), etc.\r\n"", ""issue_word_count"": 276, ""test_files_count"": 3, ""non_test_files_count"": 5, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java"", ""shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java"", ""shell/src/test/java/org/apache/accumulo/shell/commands/ListCompactionsCommandTest.java"", ""shell/src/test/java/org/apache/accumulo/shell/commands/ListScansCommandTest.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java""], ""pr_changed_test_files"": [""shell/src/test/java/org/apache/accumulo/shell/commands/ListCompactionsCommandTest.java"", ""shell/src/test/java/org/apache/accumulo/shell/commands/ListScansCommandTest.java"", ""test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java""], ""base_commit"": ""907f4a35dfe094d0d61ca4a2a013845ded4e5623"", ""head_commit"": ""fdb1ff8c51e9cd2dad24f0da415dadaa4fc002ac"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5072"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5072"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-23T18:09:10.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\nindex 30eb8e76ef1..b325eaf52b3 100644\n--- a/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n+++ b/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.accumulo.core.client.admin;\n \n+import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -28,6 +29,7 @@\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n+import org.apache.accumulo.core.client.admin.servers.ServerId.Type;\n import org.apache.accumulo.core.data.InstanceId;\n \n public interface InstanceOperations {\n@@ -312,6 +314,19 @@ List<ActiveCompaction> getActiveCompactions(ServerId server)\n    */\n   List<ActiveCompaction> getActiveCompactions() throws AccumuloException, AccumuloSecurityException;\n \n+  /**\n+   * List the active compaction running on a collection of TabletServers or Compactors. The server\n+   * addresses can be retrieved using {@link #getServers(Type)}. Use {@link #getActiveCompactions()}\n+   * to get a list of all compactions running on tservers and compactors.\n+   *\n+   * @param servers The collection of servers\n+   * @return the list of active compactions\n+   * @throws IllegalArgumentException if a type of server is not TABLET_SERVER or COMPACTOR\n+   * @since 4.0.0\n+   */\n+  List<ActiveCompaction> getActiveCompactions(Collection<ServerId> servers)\n+      throws AccumuloException, AccumuloSecurityException;\n+\n   /**\n    * Check to see if a server process at the host and port is up and responding to RPC requests.\n    *\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\nindex 9eb7123d9af..7886b9739b8 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/InstanceOperationsImpl.java\n@@ -28,6 +28,7 @@\n \n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.ConcurrentModificationException;\n import java.util.HashSet;\n@@ -397,6 +398,13 @@ public List<ActiveCompaction> getActiveCompactions()\n     compactionServers.addAll(getServers(ServerId.Type.COMPACTOR));\n     compactionServers.addAll(getServers(ServerId.Type.TABLET_SERVER));\n \n+    return getActiveCompactions(compactionServers);\n+  }\n+\n+  @Override\n+  public List<ActiveCompaction> getActiveCompactions(Collection<ServerId> compactionServers)\n+      throws AccumuloException, AccumuloSecurityException {\n+\n     int numThreads = Math.max(4, Math.min((compactionServers.size()) / 10, 256));\n     var executorService = context.threadPools().getPoolBuilder(INSTANCE_OPS_COMPACTIONS_FINDER_POOL)\n         .numCoreThreads(numThreads).build();\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\nindex 81d9e35cd11..732c68db573 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ActiveCompactionHelper.java\n@@ -21,8 +21,12 @@\n import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n+import java.util.function.BiPredicate;\n+import java.util.function.Predicate;\n import java.util.stream.Stream;\n \n import org.apache.accumulo.core.client.AccumuloException;\n@@ -137,16 +141,37 @@ public static Stream<String> activeCompactionsForServer(String tserver,\n     }\n   }\n \n+  public static Stream<String> activeCompactions(InstanceOperations instanceOps,\n+      Predicate<String> resourceGroupPredicate, BiPredicate<String,Integer> hostPortPredicate) {\n+\n+    try {\n+      final Set<ServerId> compactionServers = new HashSet<>();\n+      compactionServers.addAll(instanceOps.getServers(ServerId.Type.COMPACTOR,\n+          resourceGroupPredicate, hostPortPredicate));\n+      compactionServers.addAll(instanceOps.getServers(ServerId.Type.TABLET_SERVER,\n+          resourceGroupPredicate, hostPortPredicate));\n+\n+      return sortActiveCompactions(instanceOps.getActiveCompactions(compactionServers));\n+    } catch (AccumuloException | AccumuloSecurityException e) {\n+      LOG.debug(\""Failed to list active compactions with resource group and server predicates\"", e);\n+      return Stream.of(\""ERROR \"" + e.getMessage());\n+    }\n+  }\n+\n   public static Stream<String> activeCompactions(InstanceOperations instanceOps) {\n-    Comparator<ActiveCompaction> comparator =\n-        Comparator.comparing((ActiveCompaction ac) -> ac.getHost().getHost())\n-            .thenComparing(ac -> ac.getHost().getPort()).thenComparing(COMPACTION_AGE_DESCENDING);\n     try {\n-      return instanceOps.getActiveCompactions().stream().sorted(comparator)\n-          .map(ActiveCompactionHelper::formatActiveCompactionLine);\n+      return sortActiveCompactions(instanceOps.getActiveCompactions());\n     } catch (AccumuloException | AccumuloSecurityException e) {\n       return Stream.of(\""ERROR \"" + e.getMessage());\n     }\n   }\n \n+  private static Stream<String> sortActiveCompactions(List<ActiveCompaction> activeCompactions) {\n+    Comparator<ActiveCompaction> comparator =\n+        Comparator.comparing((ActiveCompaction ac) -> ac.getHost().getHost())\n+            .thenComparing(ac -> ac.getHost().getPort()).thenComparing(COMPACTION_AGE_DESCENDING);\n+    return activeCompactions.stream().sorted(comparator)\n+        .map(ActiveCompactionHelper::formatActiveCompactionLine);\n+  }\n+\n }\n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\nindex 7c5f32cc483..2aec7dfce68 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListCompactionsCommand.java\n@@ -18,6 +18,10 @@\n  */\n package org.apache.accumulo.shell.commands;\n \n+import static org.apache.accumulo.shell.commands.ListScansCommand.getServerOptValue;\n+import static org.apache.accumulo.shell.commands.ListScansCommand.rgRegexPredicate;\n+import static org.apache.accumulo.shell.commands.ListScansCommand.serverRegexPredicate;\n+\n import java.util.regex.Pattern;\n import java.util.stream.Stream;\n \n@@ -30,7 +34,7 @@\n \n public class ListCompactionsCommand extends Command {\n \n-  private Option tserverOption, disablePaginationOpt, filterOption;\n+  private Option serverOpt, tserverOption, rgOpt, disablePaginationOpt, filterOption;\n \n   @Override\n   public String description() {\n@@ -51,9 +55,12 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n \n     Stream<String> activeCompactionStream;\n \n-    if (cl.hasOption(tserverOption.getOpt())) {\n-      activeCompactionStream = ActiveCompactionHelper\n-          .activeCompactionsForServer(cl.getOptionValue(tserverOption.getOpt()), instanceOps);\n+    String serverValue = getServerOptValue(cl, serverOpt, tserverOption);\n+    if (serverValue != null || cl.hasOption(rgOpt)) {\n+      final var serverPredicate = serverRegexPredicate(serverValue);\n+      final var rgPredicate = rgRegexPredicate(cl.getOptionValue(rgOpt));\n+      activeCompactionStream =\n+          ActiveCompactionHelper.activeCompactions(instanceOps, rgPredicate, serverPredicate);\n     } else {\n       activeCompactionStream = ActiveCompactionHelper.activeCompactions(instanceOps);\n     }\n@@ -85,11 +92,22 @@ public Options getOptions() {\n     filterOption = new Option(\""f\"", \""filter\"", true, \""show only compactions that match the regex\"");\n     opts.addOption(filterOption);\n \n+    serverOpt = new Option(\""s\"", \""server\"", true,\n+        \""tablet/compactor server regex to list compactions for. Regex will match against strings like <host>:<port>\"");\n+    serverOpt.setArgName(\""tablet/compactor server regex\"");\n+    opts.addOption(serverOpt);\n+\n+    // Leaving here for backwards compatibility, same as serverOpt\n     tserverOption = new Option(\""ts\"", \""tabletServer\"", true,\n-        \""tablet server or compactor to list compactions for\"");\n+        \""tablet/compactor server regex to list compactions for\"");\n     tserverOption.setArgName(\""tablet server\"");\n     opts.addOption(tserverOption);\n \n+    rgOpt = new Option(\""rg\"", \""resourceGroup\"", true,\n+        \""tablet/compactor server resource group regex to list compactions for\"");\n+    rgOpt.setArgName(\""resource group\"");\n+    opts.addOption(rgOpt);\n+\n     disablePaginationOpt = new Option(\""np\"", \""no-pagination\"", false, \""disable pagination of output\"");\n     opts.addOption(disablePaginationOpt);\n \n\ndiff --git a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\nindex 54ec60055bf..c8283cfb22e 100644\n--- a/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n+++ b/shell/src/main/java/org/apache/accumulo/shell/commands/ListScansCommand.java\n@@ -19,7 +19,11 @@\n package org.apache.accumulo.shell.commands;\n \n import java.util.HashSet;\n+import java.util.Optional;\n import java.util.Set;\n+import java.util.function.BiPredicate;\n+import java.util.function.Predicate;\n+import java.util.regex.Pattern;\n \n import org.apache.accumulo.core.client.admin.InstanceOperations;\n import org.apache.accumulo.core.client.admin.servers.ServerId;\n@@ -29,11 +33,11 @@\n import org.apache.commons.cli.Option;\n import org.apache.commons.cli.Options;\n \n-import com.google.common.net.HostAndPort;\n+import com.google.common.base.Preconditions;\n \n public class ListScansCommand extends Command {\n \n-  private Option tserverOption, disablePaginationOpt;\n+  private Option serverOpt, tserverOption, rgOpt, disablePaginationOpt;\n \n   @Override\n   public String description() {\n@@ -49,13 +53,14 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n     final boolean paginate = !cl.hasOption(disablePaginationOpt.getOpt());\n     final Set<ServerId> servers = new HashSet<>();\n \n-    if (cl.hasOption(tserverOption.getOpt())) {\n-      String serverAddress = cl.getOptionValue(tserverOption.getOpt());\n-      final HostAndPort hp = HostAndPort.fromString(serverAddress);\n+    String serverValue = getServerOptValue(cl, serverOpt, tserverOption);\n+    if (serverValue != null || cl.hasOption(rgOpt)) {\n+      final var serverPredicate = serverRegexPredicate(serverValue);\n+      final var rgPredicate = rgRegexPredicate(cl.getOptionValue(rgOpt));\n       servers\n-          .add(instanceOps.getServer(ServerId.Type.SCAN_SERVER, null, hp.getHost(), hp.getPort()));\n-      servers.add(\n-          instanceOps.getServer(ServerId.Type.TABLET_SERVER, null, hp.getHost(), hp.getPort()));\n+          .addAll(instanceOps.getServers(ServerId.Type.SCAN_SERVER, rgPredicate, serverPredicate));\n+      servers.addAll(\n+          instanceOps.getServers(ServerId.Type.TABLET_SERVER, rgPredicate, serverPredicate));\n     } else {\n       servers.addAll(instanceOps.getServers(ServerId.Type.SCAN_SERVER));\n       servers.addAll(instanceOps.getServers(ServerId.Type.TABLET_SERVER));\n@@ -75,14 +80,44 @@ public int numArgs() {\n   public Options getOptions() {\n     final Options opts = new Options();\n \n-    tserverOption = new Option(\""ts\"", \""tabletServer\"", true, \""tablet server to list scans for\"");\n+    serverOpt = new Option(\""s\"", \""server\"", true,\n+        \""tablet/scan server regex to list scans for. Regex will match against strings like <host>:<port>\"");\n+    serverOpt.setArgName(\""tablet/scan server regex\"");\n+    opts.addOption(serverOpt);\n+\n+    // Leaving here for backwards compatibility, same as serverOpt\n+    tserverOption = new Option(\""ts\"", \""tabletServer\"", true, \""tablet/scan server to list scans for\"");\n     tserverOption.setArgName(\""tablet server\"");\n     opts.addOption(tserverOption);\n \n+    rgOpt = new Option(\""rg\"", \""resourceGroup\"", true,\n+        \""tablet/scan server resource group regex to list scans for\"");\n+    rgOpt.setArgName(\""resource group\"");\n+    opts.addOption(rgOpt);\n+\n     disablePaginationOpt = new Option(\""np\"", \""no-pagination\"", false, \""disable pagination of output\"");\n     opts.addOption(disablePaginationOpt);\n \n     return opts;\n   }\n \n+  static String getServerOptValue(CommandLine cl, Option serverOpt, Option tserverOption) {\n+    Preconditions.checkArgument(!(cl.hasOption(serverOpt) && cl.hasOption(tserverOption)),\n+        \""serverOpt and tserverOption may not be both set at the same time.\"");\n+    return cl.hasOption(serverOpt) ? cl.getOptionValue(serverOpt)\n+        : cl.getOptionValue(tserverOption);\n+  }\n+\n+  static BiPredicate<String,Integer> serverRegexPredicate(String serverRegex) {\n+    return Optional.ofNullable(serverRegex).map(regex -> Pattern.compile(regex).asMatchPredicate())\n+        .map(matcherPredicate -> (BiPredicate<String,\n+            Integer>) (h, p) -> matcherPredicate.test(h + \"":\"" + p))\n+        .orElse((h, p) -> true);\n+  }\n+\n+  static Predicate<String> rgRegexPredicate(String rgRegex) {\n+    return Optional.ofNullable(rgRegex).map(regex -> Pattern.compile(regex).asMatchPredicate())\n+        .orElse(rg -> true);\n+  }\n+\n }\n"", ""test_patch"": ""diff --git a/shell/src/test/java/org/apache/accumulo/shell/commands/ListCompactionsCommandTest.java b/shell/src/test/java/org/apache/accumulo/shell/commands/ListCompactionsCommandTest.java\nnew file mode 100644\nindex 00000000000..71457f84b2a\n--- /dev/null\n+++ b/shell/src/test/java/org/apache/accumulo/shell/commands/ListCompactionsCommandTest.java\n@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.shell.commands;\n+\n+import org.apache.commons.cli.ParseException;\n+import org.junit.jupiter.api.Test;\n+\n+public class ListCompactionsCommandTest {\n+\n+  @Test\n+  public void testServerRegexPredicate() throws ParseException {\n+    ListScansCommandTest.testServerRegexPredicate(new ListCompactionsCommand());\n+  }\n+\n+  @Test\n+  public void testResourceGroupRegexPredicate() throws ParseException {\n+    ListScansCommandTest.testResourceGroupRegexPredicate(new ListCompactionsCommand());\n+  }\n+\n+}\n\ndiff --git a/shell/src/test/java/org/apache/accumulo/shell/commands/ListScansCommandTest.java b/shell/src/test/java/org/apache/accumulo/shell/commands/ListScansCommandTest.java\nnew file mode 100644\nindex 00000000000..d95a2af3b99\n--- /dev/null\n+++ b/shell/src/test/java/org/apache/accumulo/shell/commands/ListScansCommandTest.java\n@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.shell.commands;\n+\n+import static org.apache.accumulo.shell.commands.ListScansCommand.getServerOptValue;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.util.List;\n+import java.util.function.BiPredicate;\n+import java.util.function.Predicate;\n+\n+import org.apache.accumulo.shell.Shell.Command;\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.DefaultParser;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.junit.jupiter.api.Test;\n+\n+public class ListScansCommandTest {\n+\n+  @Test\n+  public void testTetServerOptValue() throws ParseException {\n+    var cmd = new ListScansCommand();\n+    CommandLineParser parser = new DefaultParser();\n+    Options opts = cmd.getOptions();\n+    Option serverOpt = opts.getOption(\""-s\"");\n+    Option tserverOpt = opts.getOption(\""-ts\"");\n+\n+    assertThrows(IllegalArgumentException.class,\n+        () -> getServerOptValue(\n+            parser.parse(opts, new String[] {\""-s\"", \""server:1\"", \""-ts\"", \""server:2\""}), serverOpt,\n+            tserverOpt));\n+    assertEquals(\""server:1\"", getServerOptValue(parser.parse(opts, new String[] {\""-s\"", \""server:1\""}),\n+        serverOpt, tserverOpt));\n+    assertEquals(\""server:2\"", getServerOptValue(parser.parse(opts, new String[] {\""-ts\"", \""server:2\""}),\n+        serverOpt, tserverOpt));\n+  }\n+\n+  @Test\n+  public void testServerRegexPredicate() throws ParseException {\n+    testServerRegexPredicate(new ListScansCommand());\n+  }\n+\n+  @Test\n+  public void testResourceGroupRegexPredicate() throws ParseException {\n+    testResourceGroupRegexPredicate(new ListScansCommand());\n+  }\n+\n+  static void testServerRegexPredicate(Command cmd) throws ParseException {\n+    Options opts = cmd.getOptions();\n+    CommandLineParser parser = new DefaultParser();\n+\n+    List<String> matching =\n+        List.of(\"".*:[0-9]*\"", \""local.*:2000\"", \""localhost:2000\"", \""l.*:200[0-9].*\"");\n+    for (String serverRegex : matching) {\n+      for (String opt : List.of(\""-s\"", \""-ts\"")) {\n+        var predicate = buildServerPredicate(opts, parser, opt, serverRegex);\n+        assertTrue(predicate.test(\""localhost\"", 2000));\n+      }\n+    }\n+\n+    List<String> nonMatching = List.of(\"".*:[0-1]*\"", \""local.*:2100\"", \""localhost:3000\"", \""localhost\"");\n+    for (String serverRegex : nonMatching) {\n+      for (String opt : List.of(\""-s\"", \""-ts\"")) {\n+        var predicate = buildServerPredicate(opts, parser, opt, serverRegex);\n+        assertFalse(predicate.test(\""localhost\"", 2000));\n+      }\n+    }\n+\n+  }\n+\n+  static void testResourceGroupRegexPredicate(Command cmd) throws ParseException {\n+    Options opts = cmd.getOptions();\n+    CommandLineParser parser = new DefaultParser();\n+\n+    List<String> matching = List.of(\"".*\"", \""test.*\"", \"".*group\"", \""testgroup\"");\n+    for (String rgRegex : matching) {\n+      var predicate = buildResourceGroupPredicate(opts, parser, rgRegex);\n+      assertTrue(predicate.test(\""testgroup\""));\n+    }\n+\n+    List<String> nonMatching = List.of(\"".*gro\"", \""test.*gr\"", \""testgroup1\"", \""tg.*\"");\n+    for (String rgRegex : nonMatching) {\n+      var predicate = buildResourceGroupPredicate(opts, parser, rgRegex);\n+      assertFalse(predicate.test(\""testgroup\""));\n+    }\n+  }\n+\n+  static BiPredicate<String,Integer> buildServerPredicate(Options opts, CommandLineParser parser,\n+      String opt, String serverRegex) throws ParseException {\n+\n+    // Test flags for server regex\n+    String[] args = {opt, serverRegex};\n+    Option serverOpt = opts.getOption(opt);\n+    CommandLine cli = parser.parse(opts, args);\n+    return ListScansCommand.serverRegexPredicate(cli.getOptionValue(serverOpt));\n+  }\n+\n+  static Predicate<String> buildResourceGroupPredicate(Options opts, CommandLineParser parser,\n+      String rgRegex) throws ParseException {\n+\n+    // Test flag works for resource group regex\n+    String[] args = {\""-rg\"", rgRegex};\n+    Option serverOpt = opts.getOption(\""-rg\"");\n+    CommandLine cli = parser.parse(opts, args);\n+    return ListScansCommand.rgRegexPredicate(cli.getOptionValue(serverOpt));\n+  }\n+}\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java b/test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java\nindex dfbea21b04b..791be6ff837 100644\n--- a/test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/shell/ShellServerIT.java\n@@ -48,6 +48,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n+import java.util.Optional;\n import java.util.Set;\n import java.util.SortedSet;\n import java.util.regex.Pattern;\n@@ -1528,12 +1529,23 @@ public void listcompactions() throws Exception {\n     ts.exec(\""insert d cf cq value\"", true);\n     ts.exec(\""flush -t \"" + table, true);\n     ts.exec(\""sleep 0.2\"", true);\n-    ts.exec(\""listcompactions\"", true, \""default_tablet\"");\n+    verifyListCompactions(\""listcompactions\"", \""default_tablet\"");\n+    // basic regex filtering test, more tests are in ListCompactionsCommandTest\n+    verifyListCompactions(\""listcompactions -s .*:[0-9]*\"", \""default_tablet\"");\n+    verifyListCompactions(\""listcompactions -rg def.*\"", \""default_tablet\"");\n+    verifyListCompactions(\""listcompactions -s .*:[0-9]* -rg def.*\"", \""default_tablet\"");\n+    // non matching\n+    assertFalse(ts.exec(\""listcompactions -s bad.*\"", true).contains(\""default_tablet\""));\n+    assertFalse(ts.exec(\""listcompactions -rg bad.*\"", true).contains(\""default_tablet\""));\n+    ts.exec(\""deletetable -f \"" + table, true);\n+  }\n+\n+  private void verifyListCompactions(String cmd, String expected) throws IOException {\n+    ts.exec(cmd, true, expected);\n     String[] lines = ts.output.get().split(\""\\n\"");\n     String last = lines[lines.length - 1];\n     String[] parts = last.split(\""\\\\|\"");\n     assertEquals(13, parts.length);\n-    ts.exec(\""deletetable -f \"" + table, true);\n   }\n \n   @Test\n@@ -1655,6 +1667,26 @@ public void listscans() throws Exception {\n       ts.exec(\""insert \"" + i + \"" cf cq value\"", true);\n     }\n \n+    // Sanity checks that the regex will match\n+    // Full regex tests are done in ListScansCommandTest\n+    listscans(table, null, null, true);\n+    listscans(table, \"".*:[0-9]*\"", null, true);\n+    listscans(table, null, \""def.*\"", true);\n+    listscans(table, \"".*:[0-9]*\"", \""def.*\"", true);\n+\n+    // check not matching\n+    listscans(table, null, \""bad.*\"", false);\n+    listscans(table, \""bad.*\"", null, false);\n+\n+    ts.exec(\""deletetable -f \"" + table, true);\n+  }\n+\n+  private void listscans(String table, String serverRegex, String rgRegex, boolean match)\n+      throws Exception {\n+    final StringBuilder cmd = new StringBuilder(\""listscans\"");\n+    Optional.ofNullable(serverRegex).ifPresent(sr -> cmd.append(\"" -s \"").append(sr));\n+    Optional.ofNullable(rgRegex).ifPresent(rgr -> cmd.append(\"" -rg \"").append(rgr));\n+\n     try (AccumuloClient accumuloClient = Accumulo.newClient().from(getClientProps()).build();\n         Scanner s = accumuloClient.createScanner(table, Authorizations.EMPTY)) {\n       IteratorSetting cfg = new IteratorSetting(30, SlowIterator.class);\n@@ -1667,9 +1699,12 @@ public void listscans() throws Exception {\n       thread.start();\n \n       List<String> scans = new ArrayList<>();\n-      // Try to find the active scan for about 15seconds\n-      for (int i = 0; i < 50 && scans.isEmpty(); i++) {\n-        String currentScans = ts.exec(\""listscans\"", true);\n+      // Try to find the active scan for about 15 seconds when should match\n+      // else just 1 second to speed up test as the tests for the unmatching case\n+      // come after the matching so the scan should list quickly if they will match\n+      int attempts = match ? 50 : 3;\n+      for (int i = 0; i < attempts && scans.isEmpty(); i++) {\n+        String currentScans = ts.exec(cmd.toString(), true);\n         log.info(\""Got output from listscans:\\n{}\"", currentScans);\n         String[] lines = currentScans.split(\""\\n\"");\n         for (int scanOffset = 2; scanOffset < lines.length; scanOffset++) {\n@@ -1685,31 +1720,34 @@ public void listscans() throws Exception {\n       }\n       thread.join();\n \n-      assertFalse(scans.isEmpty(), \""Could not find any active scans over table \"" + table);\n+      if (match) {\n+        assertFalse(scans.isEmpty(), \""Could not find any active scans over table \"" + table);\n \n-      for (String scan : scans) {\n-        if (!scan.contains(\""RUNNING\"")) {\n-          log.info(\""Ignoring scan because it doesn't contain 'RUNNING': {}\"", scan);\n-          continue;\n+        for (String scan : scans) {\n+          if (!scan.contains(\""RUNNING\"")) {\n+            log.info(\""Ignoring scan because it doesn't contain 'RUNNING': {}\"", scan);\n+            continue;\n+          }\n+          String[] parts = scan.split(\""\\\\|\"");\n+          assertEquals(15, parts.length, \""Expected 15 colums, but found \"" + parts.length\n+              + \"" instead for '\"" + Arrays.toString(parts) + \""'\"");\n+          String tserver = parts[1].trim();\n+          // TODO: any way to tell if the client address is accurate? could be local IP, host,\n+          // loopback...?\n+          String hostPortPattern = \"".+:\\\\d+\"";\n+          assertMatches(tserver, hostPortPattern);\n+          assertTrue(accumuloClient.instanceOperations().getServers(ServerId.Type.TABLET_SERVER)\n+              .stream().anyMatch((srv) -> srv.toHostPortString().equals(tserver)));\n+          String client = parts[1].trim();\n+          assertMatches(client, hostPortPattern);\n+          // Scan ID should be a long (throwing an exception if it fails to parse)\n+          Long r = Long.parseLong(parts[12].trim());\n+          assertNotNull(r);\n         }\n-        String[] parts = scan.split(\""\\\\|\"");\n-        assertEquals(15, parts.length, \""Expected 15 colums, but found \"" + parts.length\n-            + \"" instead for '\"" + Arrays.toString(parts) + \""'\"");\n-        String tserver = parts[1].trim();\n-        // TODO: any way to tell if the client address is accurate? could be local IP, host,\n-        // loopback...?\n-        String hostPortPattern = \"".+:\\\\d+\"";\n-        assertMatches(tserver, hostPortPattern);\n-        assertTrue(accumuloClient.instanceOperations().getServers(ServerId.Type.TABLET_SERVER)\n-            .stream().anyMatch((srv) -> srv.toHostPortString().equals(tserver)));\n-        String client = parts[1].trim();\n-        assertMatches(client, hostPortPattern);\n-        // Scan ID should be a long (throwing an exception if it fails to parse)\n-        Long r = Long.parseLong(parts[12].trim());\n-        assertNotNull(r);\n+      } else {\n+        assertTrue(scans.isEmpty(), \""Should not find any active scans over table \"" + table);\n       }\n     }\n-    ts.exec(\""deletetable -f \"" + table, true);\n   }\n \n   @Test\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5031"", ""pr_id"": 5031, ""issue_id"": 4977, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Store columns Fate needs to find work in a locality group\n**Is your feature request related to a problem? Please describe.**\r\n\r\nFate continually scans the accumulo.fate table looking for new work.  This scan only reads a small amount of data per fate transaction, but needs to filter out all of the other data.  If the data needed to find work  was stored in its own locality group then this would avoid filtering out data.  Using a locality group would also help with cache utilization in the tablet server as only the small amount of data is read when fate does a full table scan looking for work.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nThese [two columns](https://github.com/apache/accumulo/blob/b0b9555b1314e2707d8c36a34320c7b38361c046/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java#L294-L295) are read when fate looks for work.  Need to refactor these columns to be in their own column family and then configure the accumulo initialization code to setup the locality group config when creating the table.\r\n"", ""issue_word_count"": 174, ""test_files_count"": 3, ""non_test_files_count"": 5, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/StatusMappingIterator.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/InitialConfiguration.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java""], ""pr_changed_test_files"": [""core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java"", ""core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java"", ""test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java""], ""base_commit"": ""21a74537b6b45dc2ab955b807e4d0b6a050ad1cc"", ""head_commit"": ""ffb6a12105bedf831e9f72966b1e11f23f9e5cc2"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5031"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5031"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-12T14:38:41.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\nindex 0280dbf7498..f8a65011473 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutator.java\n@@ -24,6 +24,7 @@\n import org.apache.accumulo.core.fate.FateStore;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.fate.Repo;\n+import org.apache.accumulo.core.fate.user.schema.FateSchema;\n \n public interface FateMutator<T> {\n \n@@ -57,9 +58,8 @@ public interface FateMutator<T> {\n   FateMutator<T> requireAbsentKey();\n \n   /**\n-   * Add a conditional mutation to\n-   * {@link org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily#RESERVATION_COLUMN}\n-   * that will put the reservation if there is not already a reservation present\n+   * Add a conditional mutation to {@link FateSchema.TxAdminColumnFamily#RESERVATION_COLUMN} that\n+   * will put the reservation if there is not already a reservation present\n    *\n    * @param reservation the reservation to attempt to put\n    * @return the FateMutator with this added mutation\n@@ -67,9 +67,8 @@ public interface FateMutator<T> {\n   FateMutator<T> putReservedTx(FateStore.FateReservation reservation);\n \n   /**\n-   * Add a conditional mutation to\n-   * {@link org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily#RESERVATION_COLUMN}\n-   * that will delete the column if the column value matches the given reservation\n+   * Add a conditional mutation to {@link FateSchema.TxAdminColumnFamily#RESERVATION_COLUMN} that\n+   * will delete the column if the column value matches the given reservation\n    *\n    * @param reservation the reservation to attempt to remove\n    * @return the FateMutator with this added mutation\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\nindex bb33f6ea818..264198cf93a 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/FateMutatorImpl.java\n@@ -44,6 +44,7 @@\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.RepoColumnFamily;\n+import org.apache.accumulo.core.fate.user.schema.FateSchema.TxAdminColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxInfoColumnFamily;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -69,7 +70,7 @@ public FateMutatorImpl(ClientContext context, String tableName, FateId fateId) {\n \n   @Override\n   public FateMutator<T> putStatus(TStatus status) {\n-    TxColumnFamily.STATUS_COLUMN.put(mutation, new Value(status.name()));\n+    TxAdminColumnFamily.STATUS_COLUMN.put(mutation, new Value(status.name()));\n     return this;\n   }\n \n@@ -96,8 +97,8 @@ public FateMutator<T> requireAbsent() {\n   @Override\n   public FateMutator<T> requireUnreserved() {\n     Preconditions.checkState(!requiredUnreserved);\n-    Condition condition = new Condition(TxColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n-        TxColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n+    Condition condition = new Condition(TxAdminColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n+        TxAdminColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n     mutation.addCondition(condition);\n     requiredUnreserved = true;\n     return this;\n@@ -114,23 +115,23 @@ public FateMutator<T> requireAbsentKey() {\n   @Override\n   public FateMutator<T> putReservedTx(FateStore.FateReservation reservation) {\n     requireUnreserved();\n-    TxColumnFamily.RESERVATION_COLUMN.put(mutation, new Value(reservation.getSerialized()));\n+    TxAdminColumnFamily.RESERVATION_COLUMN.put(mutation, new Value(reservation.getSerialized()));\n     return this;\n   }\n \n   @Override\n   public FateMutator<T> putUnreserveTx(FateStore.FateReservation reservation) {\n-    Condition condition = new Condition(TxColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n-        TxColumnFamily.RESERVATION_COLUMN.getColumnQualifier())\n+    Condition condition = new Condition(TxAdminColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n+        TxAdminColumnFamily.RESERVATION_COLUMN.getColumnQualifier())\n         .setValue(reservation.getSerialized());\n     mutation.addCondition(condition);\n-    TxColumnFamily.RESERVATION_COLUMN.putDelete(mutation);\n+    TxAdminColumnFamily.RESERVATION_COLUMN.putDelete(mutation);\n     return this;\n   }\n \n   @Override\n   public FateMutator<T> putFateOp(byte[] data) {\n-    TxInfoColumnFamily.FATE_OP_COLUMN.put(mutation, new Value(data));\n+    TxAdminColumnFamily.FATE_OP_COLUMN.put(mutation, new Value(data));\n     return this;\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/StatusMappingIterator.java b/core/src/main/java/org/apache/accumulo/core/fate/user/StatusMappingIterator.java\nindex 1a0fae5aa3e..83538cbb1ce 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/StatusMappingIterator.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/StatusMappingIterator.java\n@@ -18,7 +18,7 @@\n  */\n package org.apache.accumulo.core.fate.user;\n \n-import static org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily.STATUS_COLUMN;\n+import static org.apache.accumulo.core.fate.user.schema.FateSchema.TxAdminColumnFamily.STATUS_COLUMN;\n \n import java.io.IOException;\n import java.util.Arrays;\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java b/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\nindex 889ceac1e55..7377b5fc995 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/schema/FateSchema.java\n@@ -27,26 +27,18 @@ public static class TxColumnFamily {\n     public static final String STR_NAME = \""tx\"";\n     public static final Text NAME = new Text(STR_NAME);\n \n-    public static final String STATUS = \""status\"";\n-    public static final ColumnFQ STATUS_COLUMN = new ColumnFQ(NAME, new Text(STATUS));\n-\n     public static final String TX_KEY = \""txkey\"";\n     public static final ColumnFQ TX_KEY_COLUMN = new ColumnFQ(NAME, new Text(TX_KEY));\n \n     public static final String CREATE_TIME = \""ctime\"";\n     public static final ColumnFQ CREATE_TIME_COLUMN = new ColumnFQ(NAME, new Text(CREATE_TIME));\n \n-    public static final String RESERVATION = \""reservation\"";\n-    public static final ColumnFQ RESERVATION_COLUMN = new ColumnFQ(NAME, new Text(RESERVATION));\n   }\n \n   public static class TxInfoColumnFamily {\n     public static final String STR_NAME = \""txinfo\"";\n     public static final Text NAME = new Text(STR_NAME);\n \n-    public static final String FATE_OP = \""fateop\"";\n-    public static final ColumnFQ FATE_OP_COLUMN = new ColumnFQ(NAME, new Text(FATE_OP));\n-\n     public static final String AUTO_CLEAN = \""autoclean\"";\n     public static final ColumnFQ AUTO_CLEAN_COLUMN = new ColumnFQ(NAME, new Text(AUTO_CLEAN));\n \n@@ -65,4 +57,19 @@ public static class RepoColumnFamily {\n     public static final Text NAME = new Text(STR_NAME);\n   }\n \n+  // when FATE looks for work, this is the family scanned\n+  public static class TxAdminColumnFamily {\n+    public static final String STR_NAME = \""txadmin\"";\n+    public static final Text NAME = new Text(STR_NAME);\n+\n+    public static final String STATUS = \""status\"";\n+    public static final ColumnFQ STATUS_COLUMN = new ColumnFQ(NAME, new Text(STATUS));\n+\n+    public static final String RESERVATION = \""reservation\"";\n+    public static final ColumnFQ RESERVATION_COLUMN = new ColumnFQ(NAME, new Text(RESERVATION));\n+\n+    public static final String FATE_OP = \""fateop\"";\n+    public static final ColumnFQ FATE_OP_COLUMN = new ColumnFQ(NAME, new Text(FATE_OP));\n+  }\n+\n }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/InitialConfiguration.java b/server/base/src/main/java/org/apache/accumulo/server/init/InitialConfiguration.java\nindex 0f8a382fa2d..688a76336e7 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/InitialConfiguration.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/InitialConfiguration.java\n@@ -88,11 +88,11 @@ public InitialConfiguration(Configuration hadoopConf, SiteConfiguration siteConf\n \n     initialFateTableConf.putAll(commonConfig);\n     initialFateTableConf.put(Property.TABLE_SPLIT_THRESHOLD.getKey(), \""256M\"");\n-    // Create a locality group that contains status so its fast to scan. When fate looks for work is\n-    // scans this family.\n-    initialFateTableConf.put(Property.TABLE_LOCALITY_GROUP_PREFIX.getKey() + \""status\"",\n-        FateSchema.TxColumnFamily.STR_NAME);\n-    initialFateTableConf.put(Property.TABLE_LOCALITY_GROUPS.getKey(), \""status\"");\n+    // Create a locality group that contains tx admin columns so its fast to scan. When fate\n+    // looks for work it scans this family.\n+    initialFateTableConf.put(Property.TABLE_LOCALITY_GROUP_PREFIX.getKey() + \""txAdmin\"",\n+        FateSchema.TxAdminColumnFamily.STR_NAME);\n+    initialFateTableConf.put(Property.TABLE_LOCALITY_GROUPS.getKey(), \""txAdmin\"");\n \n     initialScanRefTableConf.putAll(commonConfig);\n \n"", ""test_patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java b/core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java\nindex f6a6b3bef59..a0e19e1fafb 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/RowFateStatusFilter.java\n@@ -58,7 +58,7 @@ public void init(SortedKeyValueIterator<Key,Value> source, Map<String,String> op\n   protected boolean filter(Text currentRow, List<Key> keys, List<Value> values) {\n     for (int i = 0; i < keys.size(); i++) {\n       Key key = keys.get(i);\n-      if (FateSchema.TxColumnFamily.STATUS_COLUMN.hasColumns(key)\n+      if (FateSchema.TxAdminColumnFamily.STATUS_COLUMN.hasColumns(key)\n           && valuesToAccept.contains(ReadOnlyFateStore.TStatus.valueOf(values.get(i).toString()))) {\n         return true;\n       }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\nindex 311c2890dd7..d3d117c9fa4 100644\n--- a/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n+++ b/core/src/main/java/org/apache/accumulo/core/fate/user/UserFateStore.java\n@@ -58,6 +58,7 @@\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.StackOverflowException;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.RepoColumnFamily;\n+import org.apache.accumulo.core.fate.user.schema.FateSchema.TxAdminColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n import org.apache.accumulo.core.fate.user.schema.FateSchema.TxInfoColumnFamily;\n import org.apache.accumulo.core.fate.zookeeper.ZooUtil;\n@@ -224,8 +225,8 @@ public Optional<FateTxStore<T>> tryReserve(FateId fateId) {\n       // attempt or was not written at all).\n       try (Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY)) {\n         scanner.setRange(getRow(fateId));\n-        scanner.fetchColumn(TxColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n-            TxColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n+        scanner.fetchColumn(TxAdminColumnFamily.RESERVATION_COLUMN.getColumnFamily(),\n+            TxAdminColumnFamily.RESERVATION_COLUMN.getColumnQualifier());\n         FateReservation persistedRes =\n             scanner.stream().map(entry -> FateReservation.deserialize(entry.getValue().get()))\n                 .findFirst().orElse(null);\n@@ -270,9 +271,10 @@ protected Stream<FateIdStatus> getTransactions(EnumSet<TStatus> statuses) {\n       Scanner scanner = context.createScanner(tableName, Authorizations.EMPTY);\n       scanner.setRange(new Range());\n       RowFateStatusFilter.configureScanner(scanner, statuses);\n-      TxColumnFamily.STATUS_COLUMN.fetch(scanner);\n-      TxColumnFamily.RESERVATION_COLUMN.fetch(scanner);\n-      TxInfoColumnFamily.FATE_OP_COLUMN.fetch(scanner);\n+      // columns fetched here must be in/added to TxAdminColumnFamily for locality group benefits\n+      TxAdminColumnFamily.STATUS_COLUMN.fetch(scanner);\n+      TxAdminColumnFamily.RESERVATION_COLUMN.fetch(scanner);\n+      TxAdminColumnFamily.FATE_OP_COLUMN.fetch(scanner);\n       return scanner.stream().onClose(scanner::close).map(e -> {\n         String txUUIDStr = e.getKey().getRow().toString();\n         FateId fateId = FateId.from(fateInstanceType, txUUIDStr);\n@@ -296,13 +298,13 @@ protected Stream<FateIdStatus> getTransactions(EnumSet<TStatus> statuses) {\n           Text colq = entry.getKey().getColumnQualifier();\n           Value val = entry.getValue();\n           switch (colq.toString()) {\n-            case TxColumnFamily.STATUS:\n+            case TxAdminColumnFamily.STATUS:\n               status = TStatus.valueOf(val.toString());\n               break;\n-            case TxColumnFamily.RESERVATION:\n+            case TxAdminColumnFamily.RESERVATION:\n               reservation = FateReservation.deserialize(val.get());\n               break;\n-            case TxInfoColumnFamily.FATE_OP:\n+            case TxAdminColumnFamily.FATE_OP:\n               fateOp = (Fate.FateOperation) deserializeTxInfo(TxInfo.FATE_OP, val.get());\n               break;\n             default:\n@@ -352,7 +354,7 @@ public Stream<FateKey> list(FateKey.FateKeyType type) {\n   protected TStatus _getStatus(FateId fateId) {\n     return scanTx(scanner -> {\n       scanner.setRange(getRow(fateId));\n-      TxColumnFamily.STATUS_COLUMN.fetch(scanner);\n+      TxAdminColumnFamily.STATUS_COLUMN.fetch(scanner);\n       return scanner.stream().map(e -> TStatus.valueOf(e.getValue().toString())).findFirst()\n           .orElse(TStatus.UNKNOWN);\n     });\n@@ -555,7 +557,7 @@ public Serializable getTransactionInfo(TxInfo txInfo) {\n         final ColumnFQ cq;\n         switch (txInfo) {\n           case FATE_OP:\n-            cq = TxInfoColumnFamily.FATE_OP_COLUMN;\n+            cq = TxAdminColumnFamily.FATE_OP_COLUMN;\n             break;\n           case AUTO_CLEAN:\n             cq = TxInfoColumnFamily.AUTO_CLEAN_COLUMN;\n\ndiff --git a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\nindex 0c5ba1cd6bf..50ea3159f7f 100644\n--- a/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n+++ b/test/src/main/java/org/apache/accumulo/test/fate/user/UserFateIT.java\n@@ -36,7 +36,7 @@\n import org.apache.accumulo.core.fate.FateId;\n import org.apache.accumulo.core.fate.ReadOnlyFateStore.TStatus;\n import org.apache.accumulo.core.fate.user.UserFateStore;\n-import org.apache.accumulo.core.fate.user.schema.FateSchema.TxColumnFamily;\n+import org.apache.accumulo.core.fate.user.schema.FateSchema;\n import org.apache.accumulo.core.iterators.user.VersioningIterator;\n import org.apache.accumulo.core.metadata.AccumuloTable;\n import org.apache.accumulo.core.security.Authorizations;\n@@ -124,7 +124,7 @@ public void testFateInitialConfigCorrectness() throws Exception {\n   protected TStatus getTxStatus(ServerContext context, FateId fateId) {\n     try (Scanner scanner = context.createScanner(table, Authorizations.EMPTY)) {\n       scanner.setRange(getRow(fateId));\n-      TxColumnFamily.STATUS_COLUMN.fetch(scanner);\n+      FateSchema.TxAdminColumnFamily.STATUS_COLUMN.fetch(scanner);\n       return StreamSupport.stream(scanner.spliterator(), false)\n           .map(e -> TStatus.valueOf(e.getValue().toString())).findFirst().orElse(TStatus.UNKNOWN);\n     } catch (TableNotFoundException e) {\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-5026"", ""pr_id"": 5026, ""issue_id"": 4610, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Tablet with lots of file may not be readable on scan servers for long periods of time.\n**Describe the bug**\r\n\r\nTablets can only be scanned when they have less than a configurable number of files.   Scan servers remember a tablets files for a configurable time period.  If a tablet has too many files to scan then it may be unreadable on a scan server for scan servers configured period to remember tablets files even if the tablet is compacted.    \r\n\r\nThis is a suspected bug, need to write a test to confirm.  \r\n\r\n**To Reproduce**\r\n\r\nThe following outlines a possible test to confirm this problem.\r\n\r\n * Configure max files for scan to 20\r\n * Somehow temporarily disable compactions on a test table\r\n * Create greater than 20 files on tablet in the test table\r\n * Set the scan server tablet metadata cache timeout to 10 mins\r\n * Attempt to scan the tablet w/ >20 files on a scan server\r\n * Wait a little bit\r\n * enable compactions to reduce the number of files.\r\n * See what happens with the scan started prior to the compaction\r\n\r\n**Expected behavior**\r\n\r\nIdeally soon after a tablet is compacted to have less than the max files for scan it would be readable on a scan server.\r\n"", ""issue_word_count"": 201, ""test_files_count"": 2, ""non_test_files_count"": 4, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java"", ""core/src/main/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlanner.java"", ""core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionJobPrioritizer.java"", ""core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java"", ""server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionService.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java"", ""core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java""], ""base_commit"": ""d888cd809e7f99e1810111eddedaeb417526c990"", ""head_commit"": ""4c2426235dc24a4111a0f042b276bb41780d23c2"", ""repo_url"": ""https://github.com/apache/accumulo/pull/5026"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/5026"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-08T19:55:50.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java b/core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java\nindex 4b933fc744e..9f4946b3575 100644\n--- a/core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java\n+++ b/core/src/main/java/org/apache/accumulo/core/spi/compaction/CompactionPlanner.java\n@@ -21,8 +21,10 @@\n import java.util.Collection;\n import java.util.Map;\n \n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.CompactionConfig;\n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n+import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n \n@@ -79,6 +81,13 @@ public interface InitParameters {\n    */\n   public interface PlanningParameters {\n \n+    /**\n+     * @return The id of the namespace that the table is assigned to\n+     * @throws TableNotFoundException thrown when the namespace for a table cannot be calculated\n+     * @since 2.1.4\n+     */\n+    NamespaceId getNamespaceId() throws TableNotFoundException;\n+\n     /**\n      * @return The id of the table that compactions are being planned for.\n      * @see ServiceEnvironment#getTableName(TableId)\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlanner.java b/core/src/main/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlanner.java\nindex 73952b79006..6c60d5095cf 100644\n--- a/core/src/main/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlanner.java\n+++ b/core/src/main/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlanner.java\n@@ -29,6 +29,7 @@\n import java.util.Objects;\n import java.util.Set;\n \n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n import org.apache.accumulo.core.conf.ConfigurationTypeHelper;\n import org.apache.accumulo.core.conf.Property;\n@@ -250,6 +251,7 @@ private void determineMaxFilesToCompact(InitParameters params) {\n \n   @Override\n   public CompactionPlan makePlan(PlanningParameters params) {\n+    int maxTabletFiles = 0;\n     try {\n \n       if (params.getCandidates().isEmpty()) {\n@@ -322,7 +324,7 @@ public CompactionPlan makePlan(PlanningParameters params) {\n         } else if (params.getKind() == CompactionKind.SYSTEM\n             && params.getRunningCompactions().isEmpty()\n             && params.getAll().size() == params.getCandidates().size()) {\n-          int maxTabletFiles = getMaxTabletFiles(\n+          maxTabletFiles = getMaxTabletFiles(\n               params.getServiceEnvironment().getConfiguration(params.getTableId()));\n           if (params.getAll().size() > maxTabletFiles) {\n             // The tablet is above its max files, there are no compactions running, all files are\n@@ -339,11 +341,13 @@ public CompactionPlan makePlan(PlanningParameters params) {\n         // determine which executor to use based on the size of the files\n         var ceid = getExecutor(group);\n \n-        return params.createPlanBuilder().addJob(createPriority(params, group), ceid, group)\n-            .build();\n+        return params.createPlanBuilder()\n+            .addJob(createPriority(params, group, maxTabletFiles), ceid, group).build();\n       }\n     } catch (RuntimeException e) {\n       throw e;\n+    } catch (TableNotFoundException e) {\n+      throw new RuntimeException(\""Error getting namespace for table: \"" + params.getTableId(), e);\n     }\n   }\n \n@@ -415,10 +419,10 @@ private Collection<CompactableFile> findFilesToCompactWithLowerRatio(PlanningPar\n     return found;\n   }\n \n-  private static short createPriority(PlanningParameters params,\n-      Collection<CompactableFile> group) {\n-    return CompactionJobPrioritizer.createPriority(params.getKind(), params.getAll().size(),\n-        group.size());\n+  private static short createPriority(PlanningParameters params, Collection<CompactableFile> group,\n+      int maxTabletFiles) throws TableNotFoundException {\n+    return CompactionJobPrioritizer.createPriority(params.getNamespaceId(), params.getTableId(),\n+        params.getKind(), params.getAll().size(), group.size(), maxTabletFiles);\n   }\n \n   private long getMaxSizeToCompact(CompactionKind kind) {\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionJobPrioritizer.java b/core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionJobPrioritizer.java\nindex 1f87459f116..686e589f5c9 100644\n--- a/core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionJobPrioritizer.java\n+++ b/core/src/main/java/org/apache/accumulo/core/util/compaction/CompactionJobPrioritizer.java\n@@ -19,9 +19,22 @@\n package org.apache.accumulo.core.util.compaction;\n \n import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n \n+import org.apache.accumulo.core.clientImpl.Namespace;\n+import org.apache.accumulo.core.data.NamespaceId;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.metadata.MetadataTable;\n+import org.apache.accumulo.core.metadata.RootTable;\n import org.apache.accumulo.core.spi.compaction.CompactionJob;\n import org.apache.accumulo.core.spi.compaction.CompactionKind;\n+import org.apache.accumulo.core.util.Pair;\n+import org.apache.commons.lang3.Range;\n+\n+import com.google.common.base.Preconditions;\n \n public class CompactionJobPrioritizer {\n \n@@ -29,32 +42,113 @@ public class CompactionJobPrioritizer {\n       Comparator.comparingInt(CompactionJob::getPriority)\n           .thenComparingInt(job -> job.getFiles().size()).reversed();\n \n-  public static short createPriority(CompactionKind kind, int totalFiles, int compactingFiles) {\n-\n-    int prio = totalFiles + compactingFiles;\n-\n-    switch (kind) {\n-      case USER:\n-      case CHOP:\n-        // user-initiated compactions will have a positive priority\n-        // based on number of files\n-        if (prio > Short.MAX_VALUE) {\n-          return Short.MAX_VALUE;\n-        }\n-        return (short) prio;\n-      case SELECTOR:\n-      case SYSTEM:\n-        // system-initiated compactions will have a negative priority\n-        // starting at -32768 and increasing based on number of files\n-        // maxing out at -1\n-        if (prio > Short.MAX_VALUE) {\n-          return -1;\n-        } else {\n-          return (short) (Short.MIN_VALUE + prio);\n-        }\n-      default:\n-        throw new AssertionError(\""Unknown kind \"" + kind);\n+  private static final Map<Pair<TableId,CompactionKind>,Range<Short>> SYSTEM_TABLE_RANGES =\n+      new HashMap<>();\n+  private static final Map<Pair<NamespaceId,CompactionKind>,\n+      Range<Short>> ACCUMULO_NAMESPACE_RANGES = new HashMap<>();\n+\n+  // Create ranges of possible priority values where each range has\n+  // 2000 possible values. Priority order is:\n+  // root table user initiated\n+  // root table system initiated\n+  // metadata table user initiated\n+  // metadata table system initiated\n+  // other tables in accumulo namespace user initiated\n+  // other tables in accumulo namespace system initiated\n+  // user tables that have more files that configured system initiated\n+  // user tables user initiated\n+  // user tables system initiated\n+  static final Range<Short> ROOT_TABLE_USER = Range.of((short) 30768, (short) 32767);\n+  static final Range<Short> ROOT_TABLE_SYSTEM = Range.of((short) 28768, (short) 30767);\n+\n+  static final Range<Short> METADATA_TABLE_USER = Range.of((short) 26768, (short) 28767);\n+  static final Range<Short> METADATA_TABLE_SYSTEM = Range.of((short) 24768, (short) 26767);\n+\n+  static final Range<Short> SYSTEM_NS_USER = Range.of((short) 22768, (short) 24767);\n+  static final Range<Short> SYSTEM_NS_SYSTEM = Range.of((short) 20768, (short) 22767);\n+\n+  static final Range<Short> TABLE_OVER_SIZE = Range.of((short) 18768, (short) 20767);\n+\n+  static final Range<Short> USER_TABLE_USER = Range.of((short) 1, (short) 18767);\n+  static final Range<Short> USER_TABLE_SYSTEM = Range.of((short) -32768, (short) 0);\n+\n+  static {\n+    // root table\n+    SYSTEM_TABLE_RANGES.put(new Pair<>(RootTable.ID, CompactionKind.USER), ROOT_TABLE_USER);\n+    SYSTEM_TABLE_RANGES.put(new Pair<>(RootTable.ID, CompactionKind.SYSTEM), ROOT_TABLE_SYSTEM);\n+\n+    // metadata table\n+    SYSTEM_TABLE_RANGES.put(new Pair<>(MetadataTable.ID, CompactionKind.USER), METADATA_TABLE_USER);\n+    SYSTEM_TABLE_RANGES.put(new Pair<>(MetadataTable.ID, CompactionKind.SYSTEM),\n+        METADATA_TABLE_SYSTEM);\n+\n+    // metadata table\n+    ACCUMULO_NAMESPACE_RANGES.put(new Pair<>(Namespace.ACCUMULO.id(), CompactionKind.USER),\n+        SYSTEM_NS_USER);\n+    ACCUMULO_NAMESPACE_RANGES.put(new Pair<>(Namespace.ACCUMULO.id(), CompactionKind.SYSTEM),\n+        SYSTEM_NS_SYSTEM);\n+  }\n+\n+  public static short createPriority(final NamespaceId nsId, final TableId tableId,\n+      final CompactionKind kind, final int totalFiles, final int compactingFiles,\n+      final int maxFilesPerTablet) {\n+\n+    Objects.requireNonNull(nsId, \""nsId cannot be null\"");\n+    Objects.requireNonNull(tableId, \""tableId cannot be null\"");\n+    Preconditions.checkArgument(totalFiles >= 0, \""totalFiles is negative %s\"", totalFiles);\n+    Preconditions.checkArgument(compactingFiles >= 0, \""compactingFiles is negative %s\"",\n+        compactingFiles);\n+\n+    final Function<Range<Short>,Short> normalPriorityFunction = new Function<>() {\n+      @Override\n+      public Short apply(Range<Short> f) {\n+        return (short) Math.min(f.getMaximum(), f.getMinimum() + totalFiles + compactingFiles);\n+      }\n+    };\n+\n+    final Function<Range<Short>,Short> tabletOverSizeFunction = new Function<>() {\n+      @Override\n+      public Short apply(Range<Short> f) {\n+        return (short) Math.min(f.getMaximum(),\n+            f.getMinimum() + compactingFiles + (totalFiles - maxFilesPerTablet));\n+      }\n+    };\n+\n+    // Handle the case of a CHOP compaction. For the purposes of determining\n+    // a priority, treat them as a USER compaction.\n+    CompactionKind calculationKind = kind;\n+    if (kind == CompactionKind.CHOP) {\n+      calculationKind = CompactionKind.USER;\n+    } else if (kind == CompactionKind.SELECTOR) {\n+      calculationKind = CompactionKind.SYSTEM;\n     }\n+\n+    Range<Short> range = null;\n+    Function<Range<Short>,Short> func = normalPriorityFunction;\n+    if (Namespace.ACCUMULO.id() == nsId) {\n+      // Handle system tables\n+      range = SYSTEM_TABLE_RANGES.get(new Pair<>(tableId, calculationKind));\n+      if (range == null) {\n+        range = ACCUMULO_NAMESPACE_RANGES.get(new Pair<>(nsId, calculationKind));\n+      }\n+    } else {\n+      // Handle user tables\n+      if (totalFiles > maxFilesPerTablet && calculationKind == CompactionKind.SYSTEM) {\n+        range = TABLE_OVER_SIZE;\n+        func = tabletOverSizeFunction;\n+      } else if (calculationKind == CompactionKind.SYSTEM) {\n+        range = USER_TABLE_SYSTEM;\n+      } else {\n+        range = USER_TABLE_USER;\n+      }\n+    }\n+\n+    if (range == null) {\n+      throw new IllegalStateException(\n+          \""Error calculating compaction priority for table: \"" + tableId);\n+    }\n+    return func.apply(range);\n+\n   }\n \n }\n\ndiff --git a/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionService.java b/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionService.java\nindex 593be6b67c9..52107f19d05 100644\n--- a/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionService.java\n+++ b/server/tserver/src/main/java/org/apache/accumulo/tserver/compactions/CompactionService.java\n@@ -41,9 +41,11 @@\n import java.util.function.Consumer;\n import java.util.function.Function;\n \n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n import org.apache.accumulo.core.conf.ConfigurationTypeHelper;\n import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.dataImpl.KeyExtent;\n import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n@@ -235,6 +237,11 @@ public CpPlanParams(CompactionKind kind, Compactable comp, Compactable.Files fil\n \n     private final ServiceEnvironment senv = new ServiceEnvironmentImpl(context);\n \n+    @Override\n+    public NamespaceId getNamespaceId() throws TableNotFoundException {\n+      return context.getNamespaceId(comp.getTableId());\n+    }\n+\n     @Override\n     public TableId getTableId() {\n       return comp.getTableId();\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java b/core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java\nindex 8423ee86e61..f3186701ad5 100644\n--- a/core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/spi/compaction/DefaultCompactionPlannerTest.java\n@@ -35,12 +35,15 @@\n import java.util.Set;\n import java.util.stream.Collectors;\n \n+import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n+import org.apache.accumulo.core.clientImpl.Namespace;\n import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.conf.ConfigurationTypeHelper;\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.conf.SiteConfiguration;\n+import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n import org.apache.accumulo.core.spi.common.ServiceEnvironment;\n import org.apache.accumulo.core.spi.common.ServiceEnvironment.Configuration;\n@@ -741,6 +744,11 @@ private static CompactionPlanner.PlanningParameters createPlanningParams(Set<Com\n       CompactionKind kind, Configuration conf) {\n     return new CompactionPlanner.PlanningParameters() {\n \n+      @Override\n+      public NamespaceId getNamespaceId() throws TableNotFoundException {\n+        return Namespace.ACCUMULO.id();\n+      }\n+\n       @Override\n       public TableId getTableId() {\n         return TableId.of(\""42\"");\n\ndiff --git a/core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java b/core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java\nindex 70f18be1cd7..1aa62c4f6ba 100644\n--- a/core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java\n+++ b/core/src/test/java/org/apache/accumulo/core/util/compaction/CompactionPrioritizerTest.java\n@@ -18,22 +18,41 @@\n  */\n package org.apache.accumulo.core.util.compaction;\n \n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.METADATA_TABLE_SYSTEM;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.METADATA_TABLE_USER;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.ROOT_TABLE_SYSTEM;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.ROOT_TABLE_USER;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.SYSTEM_NS_SYSTEM;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.SYSTEM_NS_USER;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.TABLE_OVER_SIZE;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.USER_TABLE_SYSTEM;\n+import static org.apache.accumulo.core.util.compaction.CompactionJobPrioritizer.USER_TABLE_USER;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n import java.net.URI;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Optional;\n \n import org.apache.accumulo.core.client.admin.compaction.CompactableFile;\n+import org.apache.accumulo.core.clientImpl.Namespace;\n+import org.apache.accumulo.core.data.TableId;\n+import org.apache.accumulo.core.metadata.MetadataTable;\n+import org.apache.accumulo.core.metadata.RootTable;\n import org.apache.accumulo.core.spi.compaction.CompactionJob;\n import org.apache.accumulo.core.spi.compaction.CompactionKind;\n+import org.apache.commons.lang3.Range;\n import org.junit.jupiter.api.Test;\n \n public class CompactionPrioritizerTest {\n \n+  private static final int TABLET_FILE_MAX = 3001;\n+\n   public CompactionJob createJob(CompactionKind kind, String tablet, int numFiles, int totalFiles) {\n \n     Collection<CompactableFile> files = new ArrayList<>();\n@@ -41,43 +60,181 @@ public CompactionJob createJob(CompactionKind kind, String tablet, int numFiles,\n       files.add(CompactableFile\n           .create(URI.create(\""hdfs://foonn/accumulo/tables/5/\"" + tablet + \""/\"" + i + \"".rf\""), 4, 4));\n     }\n-    // TODO pass numFiles\n     return new CompactionJobImpl(\n-        CompactionJobPrioritizer.createPriority(kind, totalFiles, numFiles),\n+        CompactionJobPrioritizer.createPriority(Namespace.DEFAULT.id(), TableId.of(\""5\""), kind,\n+            totalFiles, numFiles, totalFiles * 2),\n         CompactionExecutorIdImpl.externalId(\""test\""), files, kind, Optional.of(false));\n   }\n \n   @Test\n-  public void testPrioritizer() throws Exception {\n-    assertEquals((short) 0, CompactionJobPrioritizer.createPriority(CompactionKind.USER, 0, 0));\n-    assertEquals((short) 10000,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.USER, 10000, 0));\n-    assertEquals((short) 32767,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.USER, 32767, 0));\n-    assertEquals((short) 32767,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.USER, Integer.MAX_VALUE, 0));\n-\n-    assertEquals((short) -32768,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.SYSTEM, 0, 0));\n-    assertEquals((short) -22768,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.SYSTEM, 10000, 0));\n-    assertEquals((short) -1,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.SYSTEM, 32767, 0));\n-    assertEquals((short) -1,\n-        CompactionJobPrioritizer.createPriority(CompactionKind.SYSTEM, Integer.MAX_VALUE, 0));\n+  public void testNonOverlappingRanges() {\n+    List<Range<Short>> ranges = new ArrayList<>();\n+    ranges.add(ROOT_TABLE_USER);\n+    ranges.add(ROOT_TABLE_SYSTEM);\n+    ranges.add(METADATA_TABLE_USER);\n+    ranges.add(METADATA_TABLE_SYSTEM);\n+    ranges.add(SYSTEM_NS_USER);\n+    ranges.add(SYSTEM_NS_SYSTEM);\n+    ranges.add(TABLE_OVER_SIZE);\n+    ranges.add(USER_TABLE_USER);\n+    ranges.add(USER_TABLE_SYSTEM);\n+\n+    for (Range<Short> r1 : ranges) {\n+      for (Range<Short> r2 : ranges) {\n+        if (r1 == r2) {\n+          continue;\n+        }\n+        assertFalse(r1.isOverlappedBy(r2), r1.toString() + \"" is overlapped by \"" + r2.toString());\n+      }\n+    }\n+\n+    Collections.sort(ranges, new Comparator<Range<Short>>() {\n+      @Override\n+      public int compare(Range<Short> r1, Range<Short> r2) {\n+        return Short.compare(r1.getMinimum(), r2.getMinimum());\n+      }\n+    });\n+    assertEquals(Short.MIN_VALUE, ranges.get(0).getMinimum());\n+    assertEquals(Short.MAX_VALUE, ranges.get(ranges.size() - 1).getMaximum());\n+    // check that the max of the previous range is one less than the\n+    // minimum of the current range to make sure there are no holes.\n+    short lastMax = Short.MIN_VALUE;\n+    for (Range<Short> r : ranges) {\n+      if (lastMax != Short.MIN_VALUE) {\n+        assertTrue(r.getMinimum() - lastMax == 1);\n+      }\n+      lastMax = r.getMaximum();\n+    }\n+  }\n+\n+  @Test\n+  public void testRootTablePriorities() {\n+    assertEquals(ROOT_TABLE_USER.getMinimum() + 1, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.USER, 0, 1, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_USER.getMinimum() + 1010, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.USER, 1000, 10, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_USER.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.USER, 3000, 100, TABLET_FILE_MAX));\n+\n+    assertEquals(ROOT_TABLE_USER.getMinimum() + 2, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.CHOP, 0, 2, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_USER.getMinimum() + 1020, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.CHOP, 1000, 20, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_USER.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.CHOP, 3000, 200, TABLET_FILE_MAX));\n+\n+    assertEquals(ROOT_TABLE_SYSTEM.getMinimum() + 3, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.SYSTEM, 0, 3, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_SYSTEM.getMinimum() + 1030, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.SYSTEM, 1000, 30, TABLET_FILE_MAX));\n+    assertEquals(ROOT_TABLE_SYSTEM.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.SYSTEM, 3000, 300, TABLET_FILE_MAX));\n+  }\n+\n+  @Test\n+  public void testMetaTablePriorities() {\n+    assertEquals(METADATA_TABLE_USER.getMinimum() + 4, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.USER, 0, 4, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_USER.getMinimum() + 1040, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.USER, 1000, 40, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_USER.getMaximum(),\n+        CompactionJobPrioritizer.createPriority(Namespace.ACCUMULO.id(), MetadataTable.ID,\n+            CompactionKind.USER, 3000, 400, TABLET_FILE_MAX));\n+\n+    assertEquals(METADATA_TABLE_USER.getMinimum() + 5, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.CHOP, 0, 5, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_USER.getMinimum() + 1050, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.CHOP, 1000, 50, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_USER.getMaximum(),\n+        CompactionJobPrioritizer.createPriority(Namespace.ACCUMULO.id(), MetadataTable.ID,\n+            CompactionKind.CHOP, 3000, 500, TABLET_FILE_MAX));\n+\n+    assertEquals(METADATA_TABLE_SYSTEM.getMinimum() + 6, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.SYSTEM, 0, 6, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_SYSTEM.getMinimum() + 1060,\n+        CompactionJobPrioritizer.createPriority(Namespace.ACCUMULO.id(), MetadataTable.ID,\n+            CompactionKind.SYSTEM, 1000, 60, TABLET_FILE_MAX));\n+    assertEquals(METADATA_TABLE_SYSTEM.getMaximum(),\n+        CompactionJobPrioritizer.createPriority(Namespace.ACCUMULO.id(), MetadataTable.ID,\n+            CompactionKind.SYSTEM, 3000, 600, TABLET_FILE_MAX));\n+  }\n+\n+  @Test\n+  public void testSystemNamespacePriorities() {\n+    TableId tid = TableId.of(\""someOtherSystemTable\"");\n+    assertEquals(SYSTEM_NS_USER.getMinimum() + 7, CompactionJobPrioritizer\n+        .createPriority(Namespace.ACCUMULO.id(), tid, CompactionKind.USER, 0, 7, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_USER.getMinimum() + 1070, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.USER, 1000, 70, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_USER.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.USER, 3000, 700, TABLET_FILE_MAX));\n+\n+    assertEquals(SYSTEM_NS_USER.getMinimum() + 8, CompactionJobPrioritizer\n+        .createPriority(Namespace.ACCUMULO.id(), tid, CompactionKind.CHOP, 0, 8, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_USER.getMinimum() + 1080, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.CHOP, 1000, 80, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_USER.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.CHOP, 3000, 800, TABLET_FILE_MAX));\n+\n+    assertEquals(SYSTEM_NS_SYSTEM.getMinimum() + 9, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.SYSTEM, 0, 9, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_SYSTEM.getMinimum() + 1090, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.SYSTEM, 1000, 90, TABLET_FILE_MAX));\n+    assertEquals(SYSTEM_NS_SYSTEM.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.SYSTEM, 3000, 900, TABLET_FILE_MAX));\n+  }\n+\n+  @Test\n+  public void testUserTablePriorities() {\n+    TableId tid = TableId.of(\""someUserTable\"");\n+    assertEquals(USER_TABLE_USER.getMinimum() + 10, CompactionJobPrioritizer\n+        .createPriority(Namespace.DEFAULT.id(), tid, CompactionKind.USER, 0, 10, TABLET_FILE_MAX));\n+    assertEquals(USER_TABLE_USER.getMinimum() + 1100, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.USER, 1000, 100, TABLET_FILE_MAX));\n+    assertEquals(USER_TABLE_USER.getMinimum() + 4000, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.USER, 3000, 1000, TABLET_FILE_MAX));\n+\n+    assertEquals(USER_TABLE_SYSTEM.getMinimum() + 11, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.SYSTEM, 0, 11, TABLET_FILE_MAX));\n+    assertEquals(USER_TABLE_SYSTEM.getMinimum() + 1110, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.SYSTEM, 1000, 110, TABLET_FILE_MAX));\n+    assertEquals(USER_TABLE_SYSTEM.getMinimum() + 4100, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.SYSTEM, 3000, 1100, TABLET_FILE_MAX));\n+  }\n+\n+  @Test\n+  public void testTableOverSize() {\n+    final int tabletFileMax = 30;\n+    final TableId tid = TableId.of(\""someTable\"");\n+    assertEquals(ROOT_TABLE_SYSTEM.getMinimum() + 150, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.SYSTEM, 100, 50, tabletFileMax));\n+    assertEquals(METADATA_TABLE_SYSTEM.getMinimum() + 150, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.SYSTEM, 100, 50, tabletFileMax));\n+    assertEquals(SYSTEM_NS_SYSTEM.getMinimum() + 150, CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.SYSTEM, 100, 50, tabletFileMax));\n+    assertEquals(TABLE_OVER_SIZE.getMinimum() + 120, CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.SYSTEM, 100, 50, tabletFileMax));\n+    assertEquals(ROOT_TABLE_SYSTEM.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), RootTable.ID, CompactionKind.SYSTEM, 3000, 50, tabletFileMax));\n+    assertEquals(METADATA_TABLE_SYSTEM.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), MetadataTable.ID, CompactionKind.SYSTEM, 3000, 50, tabletFileMax));\n+    assertEquals(SYSTEM_NS_SYSTEM.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.ACCUMULO.id(), tid, CompactionKind.SYSTEM, 3000, 50, tabletFileMax));\n+    assertEquals(TABLE_OVER_SIZE.getMaximum(), CompactionJobPrioritizer.createPriority(\n+        Namespace.DEFAULT.id(), tid, CompactionKind.SYSTEM, 3000, 50, tabletFileMax));\n   }\n \n   @Test\n   public void testCompactionJobComparator() {\n-    var j1 = createJob(CompactionKind.USER, \""t-009\"", 10, 20);\n-    var j2 = createJob(CompactionKind.USER, \""t-010\"", 11, 25);\n-    var j3 = createJob(CompactionKind.USER, \""t-011\"", 11, 20);\n-    var j4 = createJob(CompactionKind.SYSTEM, \""t-012\"", 11, 30);\n-    var j5 = createJob(CompactionKind.SYSTEM, \""t-013\"", 5, 10);\n-    var j6 = createJob(CompactionKind.CHOP, \""t-014\"", 5, 40);\n-    var j7 = createJob(CompactionKind.CHOP, \""t-015\"", 5, 7);\n-    var j8 = createJob(CompactionKind.SELECTOR, \""t-014\"", 5, 21);\n-    var j9 = createJob(CompactionKind.SELECTOR, \""t-015\"", 7, 20);\n+    var j1 = createJob(CompactionKind.USER, \""t-009\"", 10, 20); // 30\n+    var j2 = createJob(CompactionKind.USER, \""t-010\"", 11, 25); // 36\n+    var j3 = createJob(CompactionKind.USER, \""t-011\"", 11, 20); // 31\n+    var j4 = createJob(CompactionKind.SYSTEM, \""t-012\"", 11, 30); // 40\n+    var j5 = createJob(CompactionKind.SYSTEM, \""t-013\"", 5, 10); // 15\n+    var j6 = createJob(CompactionKind.CHOP, \""t-014\"", 5, 40); // 45\n+    var j7 = createJob(CompactionKind.CHOP, \""t-015\"", 5, 7); // 12\n+    var j8 = createJob(CompactionKind.SELECTOR, \""t-014\"", 5, 21); // 26\n+    var j9 = createJob(CompactionKind.SELECTOR, \""t-015\"", 7, 20); // 27\n \n     var expected = List.of(j6, j2, j3, j1, j7, j4, j9, j8, j5);\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__accumulo-4996"", ""pr_id"": 4996, ""issue_id"": 4698, ""repo"": ""apache/accumulo"", ""problem_statement"": ""Create table name to table id map in zookeeper\nTables in zookeeper are conceptually organized as a `Map<TableId, TableName>`.  A lot of code in the Accumulo client needs the inverse `Map< TableName, TableId>`.  There is complex code in the Accumulo client related to inverting the map in zookeeper and keeping it up to date.  If an index were built in zookeeper of the form `Map< TableName, TableId>` then the complex code in the client could possibly be removed.  Also mapping a table name to table id could be more efficient, avoiding always reading all tables to map a single table name to a table id. This would reduce the load on zookeeper server for the case of lots of client and lots of tables.  For more details see the comments on #4684.  "", ""issue_word_count"": 133, ""test_files_count"": 2, ""non_test_files_count"": 10, ""pr_changed_files"": [""core/src/main/java/org/apache/accumulo/core/Constants.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java"", ""core/src/main/java/org/apache/accumulo/core/clientImpl/Namespaces.java"", ""core/src/test/java/org/apache/accumulo/core/clientImpl/NamespaceMappingTest.java"", ""server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java"", ""server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/create/PopulateZookeeperWithNamespace.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/rename/RenameNamespace.java"", ""server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/accumulo/core/clientImpl/NamespaceMappingTest.java"", ""server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java""], ""base_commit"": ""0ad4d3e7b0876e9a5e9bb99304ab505004606501"", ""head_commit"": ""d6738a11ad0c3e5ba9c14600ef30ba192b6f6d1b"", ""repo_url"": ""https://github.com/apache/accumulo/pull/4996"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__accumulo/4996"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-15T06:01:59.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/accumulo/core/Constants.java b/core/src/main/java/org/apache/accumulo/core/Constants.java\nindex 411af753cfe..ca9e0b62ff1 100644\n--- a/core/src/main/java/org/apache/accumulo/core/Constants.java\n+++ b/core/src/main/java/org/apache/accumulo/core/Constants.java\n@@ -48,7 +48,6 @@ public class Constants {\n   public static final String ZTABLE_NAMESPACE = \""/namespace\"";\n \n   public static final String ZNAMESPACES = \""/namespaces\"";\n-  public static final String ZNAMESPACE_NAME = \""/name\"";\n \n   public static final String ZMANAGERS = \""/managers\"";\n   public static final String ZMANAGER_LOCK = ZMANAGERS + \""/lock\"";\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\nindex 899bad6c10b..298a14329d4 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/ClientContext.java\n@@ -142,6 +142,7 @@ public class ClientContext implements AccumuloClient {\n   private final Supplier<SaslConnectionParams> saslSupplier;\n   private final Supplier<SslConnectionParams> sslSupplier;\n   private final Supplier<ScanServerSelector> scanServerSelectorSupplier;\n+  private final NamespaceMapping namespaces;\n   private TCredentials rpcCreds;\n   private ThriftTransportPool thriftTransportPool;\n   private ZookeeperLockChecker zkLockChecker;\n@@ -249,6 +250,7 @@ public ClientContext(SingletonReservation reservation, ClientInfo info,\n         clientThreadPools = ThreadPools.getClientThreadPools(ueh);\n       }\n     }\n+    this.namespaces = new NamespaceMapping(this);\n   }\n \n   public Ample getAmple() {\n@@ -1114,4 +1116,8 @@ public synchronized ZookeeperLockChecker getTServerLockChecker() {\n     return this.zkLockChecker;\n   }\n \n+  public NamespaceMapping getNamespaces() {\n+    return namespaces;\n+  }\n+\n }\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java\nnew file mode 100644\nindex 00000000000..7a6ce7ab6b8\n--- /dev/null\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/NamespaceMapping.java\n@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.clientImpl;\n+\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Collections.emptySortedMap;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.accumulo.core.util.LazySingletons.GSON;\n+\n+import java.lang.reflect.Type;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n+import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n+import org.apache.accumulo.core.data.NamespaceId;\n+import org.apache.accumulo.core.fate.zookeeper.ZooCache;\n+import org.apache.accumulo.core.fate.zookeeper.ZooReaderWriter;\n+import org.apache.zookeeper.KeeperException;\n+\n+import com.google.common.collect.ImmutableSortedMap;\n+import com.google.gson.reflect.TypeToken;\n+\n+public class NamespaceMapping {\n+  // type token must represent a mutable type, so it can be altered in the mutateExisting methods\n+  // without needing to make a copy\n+  private static final Type MAP_TYPE = new TypeToken<TreeMap<String,String>>() {}.getType();\n+  private final ClientContext context;\n+  private volatile SortedMap<NamespaceId,String> currentNamespaceMap = emptySortedMap();\n+  private volatile SortedMap<String,NamespaceId> currentNamespaceReverseMap = emptySortedMap();\n+  private volatile long lastMzxid;\n+\n+  public NamespaceMapping(ClientContext context) {\n+    this.context = context;\n+  }\n+\n+  public static void put(ZooReaderWriter zoo, String zPath, NamespaceId namespaceId,\n+      String namespaceName)\n+      throws InterruptedException, KeeperException, AcceptableThriftTableOperationException {\n+    if (Namespace.DEFAULT.id().equals(namespaceId) || Namespace.ACCUMULO.id().equals(namespaceId)) {\n+      throw new AssertionError(\n+          \""Putting built-in namespaces in map should not be possible after init\"");\n+    }\n+    zoo.mutateExisting(zPath, data -> {\n+      var namespaces = deserialize(data);\n+      if (namespaces.containsKey(namespaceId.canonical())) {\n+        throw new AcceptableThriftTableOperationException(null, namespaceId.canonical(),\n+            TableOperation.CREATE, TableOperationExceptionType.NAMESPACE_EXISTS,\n+            \""Namespace Id already exists\"");\n+      }\n+      namespaces.put(namespaceId.canonical(), namespaceName);\n+      return serialize(namespaces);\n+    });\n+  }\n+\n+  public static void remove(ZooReaderWriter zoo, String zPath, NamespaceId namespaceId)\n+      throws InterruptedException, KeeperException, AcceptableThriftTableOperationException {\n+    if (Namespace.DEFAULT.id().equals(namespaceId) || Namespace.ACCUMULO.id().equals(namespaceId)) {\n+      throw new AssertionError(\""Removing built-in namespaces in map should not be possible\"");\n+    }\n+    zoo.mutateExisting(zPath, data -> {\n+      var namespaces = deserialize(data);\n+      if (!namespaces.containsKey(namespaceId.canonical())) {\n+        throw new AcceptableThriftTableOperationException(null, namespaceId.canonical(),\n+            TableOperation.DELETE, TableOperationExceptionType.NAMESPACE_NOTFOUND,\n+            \""Namespace already removed while processing\"");\n+      }\n+      namespaces.remove(namespaceId.canonical());\n+      return serialize(namespaces);\n+    });\n+  }\n+\n+  public static void rename(ZooReaderWriter zoo, String zPath, NamespaceId namespaceId,\n+      String oldName, String newName)\n+      throws InterruptedException, KeeperException, AcceptableThriftTableOperationException {\n+    if (Namespace.DEFAULT.id().equals(namespaceId) || Namespace.ACCUMULO.id().equals(namespaceId)) {\n+      throw new AssertionError(\""Renaming built-in namespaces in map should not be possible\"");\n+    }\n+    zoo.mutateExisting(zPath, current -> {\n+      var currentNamespaceMap = deserialize(current);\n+      final String currentName = currentNamespaceMap.get(namespaceId.canonical());\n+      if (currentName.equals(newName)) {\n+        return null; // assume in this case the operation is running again, so we are done\n+      }\n+      if (!currentName.equals(oldName)) {\n+        throw new AcceptableThriftTableOperationException(null, oldName, TableOperation.RENAME,\n+            TableOperationExceptionType.NAMESPACE_NOTFOUND, \""Name changed while processing\"");\n+      }\n+      if (currentNamespaceMap.containsValue(newName)) {\n+        throw new AcceptableThriftTableOperationException(null, newName, TableOperation.RENAME,\n+            TableOperationExceptionType.NAMESPACE_EXISTS, \""Namespace name already exists\"");\n+      }\n+      currentNamespaceMap.put(namespaceId.canonical(), newName);\n+      return serialize(currentNamespaceMap);\n+    });\n+  }\n+\n+  public static byte[] serialize(Map<String,String> map) {\n+    return GSON.get().toJson(new TreeMap<>(map), MAP_TYPE).getBytes(UTF_8);\n+  }\n+\n+  public static Map<String,String> deserialize(byte[] data) {\n+    requireNonNull(data, \""/namespaces node should not be null\"");\n+    return GSON.get().fromJson(new String(data, UTF_8), MAP_TYPE);\n+  }\n+\n+  private synchronized void update() {\n+    final ZooCache zc = context.getZooCache();\n+    final String zPath = context.getZooKeeperRoot() + Constants.ZNAMESPACES;\n+    final ZooCache.ZcStat stat = new ZooCache.ZcStat();\n+\n+    byte[] data = zc.get(zPath, stat);\n+    if (stat.getMzxid() > lastMzxid) {\n+      if (data == null) {\n+        throw new IllegalStateException(\""namespaces node should not be null\"");\n+      } else {\n+        Map<String,String> idToName = deserialize(data);\n+        if (!idToName.containsKey(Namespace.DEFAULT.id().canonical())\n+            || !idToName.containsKey(Namespace.ACCUMULO.id().canonical())) {\n+          throw new IllegalStateException(\""Built-in namespace is not present in map\"");\n+        }\n+        var converted = ImmutableSortedMap.<NamespaceId,String>naturalOrder();\n+        var convertedReverse = ImmutableSortedMap.<String,NamespaceId>naturalOrder();\n+        idToName.forEach((idString, name) -> {\n+          var id = NamespaceId.of(idString);\n+          converted.put(id, name);\n+          convertedReverse.put(name, id);\n+        });\n+        currentNamespaceMap = converted.build();\n+        currentNamespaceReverseMap = convertedReverse.build();\n+      }\n+      lastMzxid = stat.getMzxid();\n+    }\n+  }\n+\n+  public SortedMap<NamespaceId,String> getIdToNameMap() {\n+    update();\n+    return currentNamespaceMap;\n+  }\n+\n+  public SortedMap<String,NamespaceId> getNameToIdMap() {\n+    update();\n+    return currentNamespaceReverseMap;\n+  }\n+\n+}\n\ndiff --git a/core/src/main/java/org/apache/accumulo/core/clientImpl/Namespaces.java b/core/src/main/java/org/apache/accumulo/core/clientImpl/Namespaces.java\nindex 4368b318966..366375f82f8 100644\n--- a/core/src/main/java/org/apache/accumulo/core/clientImpl/Namespaces.java\n+++ b/core/src/main/java/org/apache/accumulo/core/clientImpl/Namespaces.java\n@@ -18,21 +18,14 @@\n  */\n package org.apache.accumulo.core.clientImpl;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n-import java.util.ArrayList;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map.Entry;\n import java.util.SortedMap;\n-import java.util.TreeMap;\n-import java.util.function.BiConsumer;\n \n-import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.NamespaceNotFoundException;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n-import org.apache.accumulo.core.fate.zookeeper.ZooCache;\n import org.apache.accumulo.core.util.tables.TableNameUtil;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -41,9 +34,7 @@ public class Namespaces {\n   private static final Logger log = LoggerFactory.getLogger(Namespaces.class);\n \n   public static boolean exists(ClientContext context, NamespaceId namespaceId) {\n-    ZooCache zc = context.getZooCache();\n-    List<String> namespaceIds = zc.getChildren(context.getZooKeeperRoot() + Constants.ZNAMESPACES);\n-    return namespaceIds.contains(namespaceId.canonical());\n+    return context.getNamespaces().getIdToNameMap().containsKey(namespaceId);\n   }\n \n   public static List<TableId> getTableIds(ClientContext context, NamespaceId namespaceId)\n@@ -70,39 +61,18 @@ public static List<String> getTableNames(ClientContext context, NamespaceId name\n     return names;\n   }\n \n-  /**\n-   * Gets all the namespaces from ZK. The first arg (t) the BiConsumer accepts is the ID and the\n-   * second (u) is the namespaceName.\n-   */\n-  private static void getAllNamespaces(ClientContext context,\n-      BiConsumer<String,String> biConsumer) {\n-    final ZooCache zc = context.getZooCache();\n-    List<String> namespaceIds = zc.getChildren(context.getZooKeeperRoot() + Constants.ZNAMESPACES);\n-    for (String id : namespaceIds) {\n-      byte[] path = zc.get(context.getZooKeeperRoot() + Constants.ZNAMESPACES + \""/\"" + id\n-          + Constants.ZNAMESPACE_NAME);\n-      if (path != null) {\n-        biConsumer.accept(id, new String(path, UTF_8));\n-      }\n-    }\n-  }\n-\n   /**\n    * Return sorted map with key = ID, value = namespaceName\n    */\n   public static SortedMap<NamespaceId,String> getIdToNameMap(ClientContext context) {\n-    SortedMap<NamespaceId,String> idMap = new TreeMap<>();\n-    getAllNamespaces(context, (id, name) -> idMap.put(NamespaceId.of(id), name));\n-    return idMap;\n+    return context.getNamespaces().getIdToNameMap();\n   }\n \n   /**\n    * Return sorted map with key = namespaceName, value = ID\n    */\n   public static SortedMap<String,NamespaceId> getNameToIdMap(ClientContext context) {\n-    SortedMap<String,NamespaceId> nameMap = new TreeMap<>();\n-    getAllNamespaces(context, (id, name) -> nameMap.put(name, NamespaceId.of(id)));\n-    return nameMap;\n+    return context.getNamespaces().getNameToIdMap();\n   }\n \n   /**\n@@ -110,17 +80,12 @@ public static SortedMap<String,NamespaceId> getNameToIdMap(ClientContext context\n    */\n   public static NamespaceId getNamespaceId(ClientContext context, String namespaceName)\n       throws NamespaceNotFoundException {\n-    final ArrayList<NamespaceId> singleId = new ArrayList<>(1);\n-    getAllNamespaces(context, (id, name) -> {\n-      if (name.equals(namespaceName)) {\n-        singleId.add(NamespaceId.of(id));\n-      }\n-    });\n-    if (singleId.isEmpty()) {\n+    var id = context.getNamespaces().getNameToIdMap().get(namespaceName);\n+    if (id == null) {\n       throw new NamespaceNotFoundException(null, namespaceName,\n           \""getNamespaceId() failed to find namespace\"");\n     }\n-    return singleId.get(0);\n+    return id;\n   }\n \n   /**\n@@ -150,13 +115,8 @@ public static boolean namespaceNameExists(ClientContext context, String namespac\n    */\n   public static String getNamespaceName(ClientContext context, NamespaceId namespaceId)\n       throws NamespaceNotFoundException {\n-    String name;\n-    ZooCache zc = context.getZooCache();\n-    byte[] path = zc.get(context.getZooKeeperRoot() + Constants.ZNAMESPACES + \""/\""\n-        + namespaceId.canonical() + Constants.ZNAMESPACE_NAME);\n-    if (path != null) {\n-      name = new String(path, UTF_8);\n-    } else {\n+    String name = getIdToNameMap(context).get(namespaceId);\n+    if (name == null) {\n       throw new NamespaceNotFoundException(namespaceId.canonical(), null,\n           \""getNamespaceName() failed to find namespace\"");\n     }\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java b/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\nindex 50dfc76b23d..1fc771dc097 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/init/ZooKeeperInitializer.java\n@@ -21,10 +21,12 @@\n import static java.nio.charset.StandardCharsets.UTF_8;\n \n import java.io.IOException;\n+import java.util.Map;\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.admin.TimeType;\n import org.apache.accumulo.core.clientImpl.Namespace;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Value;\n@@ -110,7 +112,10 @@ void initialize(final ServerContext context, final boolean clearInstanceName,\n     String zkInstanceRoot = Constants.ZROOT + \""/\"" + instanceId;\n     zoo.putPersistentData(zkInstanceRoot + Constants.ZTABLES, Constants.ZTABLES_INITIAL_ID,\n         ZooUtil.NodeExistsPolicy.FAIL);\n-    zoo.putPersistentData(zkInstanceRoot + Constants.ZNAMESPACES, new byte[0],\n+    zoo.putPersistentData(zkInstanceRoot + Constants.ZNAMESPACES,\n+        NamespaceMapping\n+            .serialize(Map.of(Namespace.DEFAULT.id().canonical(), Namespace.DEFAULT.name(),\n+                Namespace.ACCUMULO.id().canonical(), Namespace.ACCUMULO.name())),\n         ZooUtil.NodeExistsPolicy.FAIL);\n \n     TableManager.prepareNewNamespaceState(context, Namespace.DEFAULT.id(), Namespace.DEFAULT.name(),\n\ndiff --git a/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java b/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\nindex 380942b8aa1..15ce8d59419 100644\n--- a/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\n+++ b/server/base/src/main/java/org/apache/accumulo/server/tables/TableManager.java\n@@ -29,6 +29,9 @@\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.NamespaceNotFoundException;\n+import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n+import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.data.TableId;\n@@ -67,14 +70,14 @@ public class TableManager {\n   private final ZooReaderWriter zoo;\n   private final ZooCache zooStateCache;\n \n-  public static void prepareNewNamespaceState(ZooReaderWriter zoo, final PropStore propStore,\n-      InstanceId instanceId, NamespaceId namespaceId, String namespace,\n-      NodeExistsPolicy existsPolicy) throws KeeperException, InterruptedException {\n+  public static void prepareNewNamespaceState(final ServerContext context, NamespaceId namespaceId,\n+      String namespace, NodeExistsPolicy existsPolicy)\n+      throws KeeperException, InterruptedException {\n+    final PropStore propStore = context.getPropStore();\n+    final InstanceId instanceId = context.getInstanceID();\n     log.debug(\""Creating ZooKeeper entries for new namespace {} (ID: {})\"", namespace, namespaceId);\n-    String zPath = Constants.ZROOT + \""/\"" + instanceId + Constants.ZNAMESPACES + \""/\"" + namespaceId;\n-\n-    zoo.putPersistentData(zPath, new byte[0], existsPolicy);\n-    zoo.putPersistentData(zPath + Constants.ZNAMESPACE_NAME, namespace.getBytes(UTF_8),\n+    context.getZooReaderWriter().putPersistentData(\n+        Constants.ZROOT + \""/\"" + instanceId + Constants.ZNAMESPACES + \""/\"" + namespaceId, new byte[0],\n         existsPolicy);\n     var propKey = NamespacePropKey.of(instanceId, namespaceId);\n     if (!propStore.exists(propKey)) {\n@@ -82,13 +85,6 @@ public static void prepareNewNamespaceState(ZooReaderWriter zoo, final PropStore\n     }\n   }\n \n-  public static void prepareNewNamespaceState(final ServerContext context, NamespaceId namespaceId,\n-      String namespace, NodeExistsPolicy existsPolicy)\n-      throws KeeperException, InterruptedException {\n-    prepareNewNamespaceState(context.getZooReaderWriter(), context.getPropStore(),\n-        context.getInstanceID(), namespaceId, namespace, existsPolicy);\n-  }\n-\n   public static void prepareNewTableState(ZooReaderWriter zoo, PropStore propStore,\n       InstanceId instanceId, TableId tableId, NamespaceId namespaceId, String tableName,\n       TableState state, NodeExistsPolicy existsPolicy)\n@@ -329,7 +325,15 @@ public void process(WatchedEvent event) {\n   }\n \n   public void removeNamespace(NamespaceId namespaceId)\n-      throws KeeperException, InterruptedException {\n+      throws KeeperException, InterruptedException, AcceptableThriftTableOperationException {\n+    try {\n+      NamespaceMapping.remove(zoo, zkRoot + Constants.ZNAMESPACES, namespaceId);\n+    } catch (AcceptableThriftTableOperationException e) {\n+      // ignore not found, because that's what we're trying to do anyway\n+      if (e.getType() != TableOperationExceptionType.NAMESPACE_NOTFOUND) {\n+        throw e;\n+      }\n+    }\n     zoo.recursiveDelete(zkRoot + Constants.ZNAMESPACES + \""/\"" + namespaceId, NodeMissingPolicy.SKIP);\n   }\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java\nindex be051943a33..8fc09eac2ca 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/Utils.java\n@@ -258,18 +258,6 @@ public static Lock getReadLock(Manager env, AbstractId<?> id, long tid) {\n     return Utils.getLock(env.getContext(), id, tid, LockType.READ);\n   }\n \n-  public static void checkNamespaceDoesNotExist(ServerContext context, String namespace,\n-      NamespaceId namespaceId, TableOperation operation)\n-      throws AcceptableThriftTableOperationException {\n-\n-    NamespaceId n = Namespaces.lookupNamespaceId(context, namespace);\n-\n-    if (n != null && !n.equals(namespaceId)) {\n-      throw new AcceptableThriftTableOperationException(null, namespace, operation,\n-          TableOperationExceptionType.NAMESPACE_EXISTS, null);\n-    }\n-  }\n-\n   /**\n    * Given a fully-qualified Path and a flag indicating if the file info is base64 encoded or not,\n    * retrieve the data from a file on the file system. It is assumed that the file is textual and\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/create/PopulateZookeeperWithNamespace.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/create/PopulateZookeeperWithNamespace.java\nindex 939b6f0916d..b47223f1519 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/create/PopulateZookeeperWithNamespace.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/create/PopulateZookeeperWithNamespace.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.accumulo.manager.tableOps.namespace.create;\n \n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.zookeeper.DistributedReadWriteLock.LockType;\n@@ -50,17 +52,17 @@ public Repo<Manager> call(long tid, Manager manager) throws Exception {\n \n     Utils.getTableNameLock().lock();\n     try {\n-      Utils.checkNamespaceDoesNotExist(manager.getContext(), namespaceInfo.namespaceName,\n-          namespaceInfo.namespaceId, TableOperation.CREATE);\n-\n-      TableManager.prepareNewNamespaceState(manager.getContext(), namespaceInfo.namespaceId,\n+      var context = manager.getContext();\n+      NamespaceMapping.put(context.getZooReaderWriter(),\n+          Constants.ZROOT + \""/\"" + context.getInstanceID() + Constants.ZNAMESPACES,\n+          namespaceInfo.namespaceId, namespaceInfo.namespaceName);\n+      TableManager.prepareNewNamespaceState(context, namespaceInfo.namespaceId,\n           namespaceInfo.namespaceName, NodeExistsPolicy.OVERWRITE);\n \n-      PropUtil.setProperties(manager.getContext(),\n-          NamespacePropKey.of(manager.getContext(), namespaceInfo.namespaceId),\n+      PropUtil.setProperties(context, NamespacePropKey.of(context, namespaceInfo.namespaceId),\n           namespaceInfo.props);\n \n-      manager.getContext().clearTableListCache();\n+      context.clearTableListCache();\n \n       return new FinishCreateNamespace(namespaceInfo);\n     } finally {\n@@ -70,8 +72,9 @@ public Repo<Manager> call(long tid, Manager manager) throws Exception {\n \n   @Override\n   public void undo(long tid, Manager manager) throws Exception {\n+    var context = manager.getContext();\n     manager.getTableManager().removeNamespace(namespaceInfo.namespaceId);\n-    manager.getContext().clearTableListCache();\n+    context.clearTableListCache();\n     Utils.unreserveNamespace(manager, namespaceInfo.namespaceId, tid, LockType.WRITE);\n   }\n \n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/rename/RenameNamespace.java b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/rename/RenameNamespace.java\nindex 3fdddf07585..b1fcb9f30b9 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/rename/RenameNamespace.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/tableOps/namespace/rename/RenameNamespace.java\n@@ -18,12 +18,9 @@\n  */\n package org.apache.accumulo.manager.tableOps.namespace.rename;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import org.apache.accumulo.core.Constants;\n-import org.apache.accumulo.core.clientImpl.AcceptableThriftTableOperationException;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.clientImpl.thrift.TableOperation;\n-import org.apache.accumulo.core.clientImpl.thrift.TableOperationExceptionType;\n import org.apache.accumulo.core.data.NamespaceId;\n import org.apache.accumulo.core.fate.Repo;\n import org.apache.accumulo.core.fate.zookeeper.DistributedReadWriteLock.LockType;\n@@ -59,23 +56,9 @@ public Repo<Manager> call(long id, Manager manager) throws Exception {\n \n     Utils.getTableNameLock().lock();\n     try {\n-      Utils.checkNamespaceDoesNotExist(manager.getContext(), newName, namespaceId,\n-          TableOperation.RENAME);\n-\n-      final String tap = manager.getZooKeeperRoot() + Constants.ZNAMESPACES + \""/\"" + namespaceId\n-          + Constants.ZNAMESPACE_NAME;\n+      NamespaceMapping.rename(zoo, manager.getZooKeeperRoot() + Constants.ZNAMESPACES, namespaceId,\n+          oldName, newName);\n \n-      zoo.mutateExisting(tap, current -> {\n-        final String currentName = new String(current, UTF_8);\n-        if (currentName.equals(newName)) {\n-          return null; // assume in this case the operation is running again, so we are done\n-        }\n-        if (!currentName.equals(oldName)) {\n-          throw new AcceptableThriftTableOperationException(null, oldName, TableOperation.RENAME,\n-              TableOperationExceptionType.NAMESPACE_NOTFOUND, \""Name changed while processing\"");\n-        }\n-        return newName.getBytes(UTF_8);\n-      });\n       manager.getContext().clearTableListCache();\n     } finally {\n       Utils.getTableNameLock().unlock();\n\ndiff --git a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\nindex a130d114459..60d80aaa448 100644\n--- a/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\n+++ b/server/manager/src/main/java/org/apache/accumulo/manager/upgrade/Upgrader11to12.java\n@@ -26,17 +26,20 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.TreeMap;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.BatchDeleter;\n import org.apache.accumulo.core.client.BatchWriter;\n import org.apache.accumulo.core.client.IsolatedScanner;\n import org.apache.accumulo.core.client.MutationsRejectedException;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Range;\n@@ -81,7 +84,10 @@ public class Upgrader11to12 implements Upgrader {\n   static final Set<Text> UPGRADE_FAMILIES =\n       Set.of(DataFileColumnFamily.NAME, CHOPPED, ExternalCompactionColumnFamily.NAME);\n \n-  public static final String ZTRACERS = \""/tracers\"";\n+  private static final String ZTRACERS = \""/tracers\"";\n+\n+  @VisibleForTesting\n+  static final String ZNAMESPACE_NAME = \""/name\"";\n \n   @Override\n   public void upgradeZookeeper(@NonNull ServerContext context) {\n@@ -117,6 +123,27 @@ public void upgradeZookeeper(@NonNull ServerContext context) {\n         zrw.overwritePersistentData(rootBase, rtm.toJson().getBytes(UTF_8), stat.getVersion());\n         log.info(\""Root metadata in ZooKeeper after upgrade: {}\"", rtm.toJson());\n       }\n+\n+      String zPath = Constants.ZROOT + \""/\"" + context.getInstanceID() + Constants.ZNAMESPACES;\n+      byte[] namespacesData = zrw.getData(zPath);\n+      if (namespacesData.length != 0) {\n+        throw new IllegalStateException(\n+            \""Unexpected data found under namespaces node: \"" + new String(namespacesData, UTF_8));\n+      }\n+      List<String> namespaceIdList = zrw.getChildren(zPath);\n+      Map<String,String> namespaceMap = new HashMap<>();\n+      for (String namespaceId : namespaceIdList) {\n+        String namespaceNamePath = zPath + \""/\"" + namespaceId + ZNAMESPACE_NAME;\n+        namespaceMap.put(namespaceId, new String(zrw.getData(namespaceNamePath), UTF_8));\n+      }\n+      byte[] mapping = NamespaceMapping.serialize(namespaceMap);\n+      zrw.putPersistentData(zPath, mapping, ZooUtil.NodeExistsPolicy.OVERWRITE);\n+\n+      for (String namespaceId : namespaceIdList) {\n+        String namespaceNamePath = zPath + \""/\"" + namespaceId + ZNAMESPACE_NAME;\n+        zrw.delete(namespaceNamePath);\n+      }\n+\n     } catch (InterruptedException ex) {\n       Thread.currentThread().interrupt();\n       throw new IllegalStateException(\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/accumulo/core/clientImpl/NamespaceMappingTest.java b/core/src/test/java/org/apache/accumulo/core/clientImpl/NamespaceMappingTest.java\nnew file mode 100644\nindex 00000000000..833f17ca4a0\n--- /dev/null\n+++ b/core/src/test/java/org/apache/accumulo/core/clientImpl/NamespaceMappingTest.java\n@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   https://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.accumulo.core.clientImpl;\n+\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import com.google.gson.JsonSyntaxException;\n+\n+class NamespaceMappingTest {\n+\n+  @Test\n+  void testSerialize() {\n+    assertThrows(NullPointerException.class, () -> NamespaceMapping.serialize(null));\n+    assertEquals(\""{}\"", new String(NamespaceMapping.serialize(Map.of()), UTF_8));\n+    assertEquals(\""{\\\""a\\\"":\\\""b\\\""}\"", new String(NamespaceMapping.serialize(Map.of(\""a\"", \""b\"")), UTF_8));\n+    assertEquals(\""{\\\""a\\\"":\\\""b\\\"",\\\""c\\\"":\\\""d\\\""}\"",\n+        new String(NamespaceMapping.serialize(Map.of(\""a\"", \""b\"", \""c\"", \""d\"")), UTF_8));\n+  }\n+\n+  @Test\n+  void testDeserialize() {\n+    assertThrows(NullPointerException.class, () -> NamespaceMapping.deserialize(null));\n+    assertEquals(null, NamespaceMapping.deserialize(new byte[0]));\n+    assertTrue(NamespaceMapping.deserialize(\""{}\"".getBytes(UTF_8)) instanceof TreeMap);\n+    assertEquals(Map.of(), NamespaceMapping.deserialize(\""{}\"".getBytes(UTF_8)));\n+    assertEquals(Map.of(\""a\"", \""b\""), NamespaceMapping.deserialize(\""{\\\""a\\\"":\\\""b\\\""}\"".getBytes(UTF_8)));\n+    assertEquals(Map.of(\""a\"", \""b\"", \""c\"", \""d\""),\n+        NamespaceMapping.deserialize(\""{\\\""a\\\"":\\\""b\\\"",\\\""c\\\"":\\\""d\\\""}\"".getBytes(UTF_8)));\n+\n+    // check malformed json\n+    assertThrows(JsonSyntaxException.class,\n+        () -> NamespaceMapping.deserialize(\""-\"".getBytes(UTF_8)));\n+    // check incorrect json type for string value\n+    assertThrows(JsonSyntaxException.class,\n+        () -> NamespaceMapping.deserialize(\""{\\\""a\\\"":{}}\"".getBytes(UTF_8)));\n+    // check valid json, but not a map\n+    assertThrows(JsonSyntaxException.class,\n+        () -> NamespaceMapping.deserialize(\""\\\""[\\\""a\\\""]\\\""\"".getBytes(UTF_8)));\n+\n+    // strange edge case because empty json array can be converted into an empty map; we don't ever\n+    // expect this to be found, but there's no easy way to check for this edge case that is worth\n+    // the effort\n+    assertTrue(NamespaceMapping.deserialize(\""[]\"".getBytes(UTF_8)) instanceof SortedMap);\n+    assertEquals(Map.of(), NamespaceMapping.deserialize(\""[]\"".getBytes(UTF_8)));\n+  }\n+\n+}\n\ndiff --git a/server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java b/server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java\nindex d176a4c56c8..bb21b4efec4 100644\n--- a/server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java\n+++ b/server/manager/src/test/java/org/apache/accumulo/manager/upgrade/Upgrader11to12Test.java\n@@ -20,8 +20,11 @@\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.accumulo.manager.upgrade.Upgrader11to12.UPGRADE_FAMILIES;\n+import static org.apache.accumulo.manager.upgrade.Upgrader11to12.ZNAMESPACE_NAME;\n+import static org.easymock.EasyMock.aryEq;\n import static org.easymock.EasyMock.capture;\n import static org.easymock.EasyMock.createMock;\n+import static org.easymock.EasyMock.createStrictMock;\n import static org.easymock.EasyMock.eq;\n import static org.easymock.EasyMock.expect;\n import static org.easymock.EasyMock.expectLastCall;\n@@ -40,10 +43,13 @@\n import java.util.Map;\n import java.util.TreeMap;\n import java.util.UUID;\n+import java.util.function.Supplier;\n import java.util.stream.Collectors;\n \n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.BatchWriter;\n import org.apache.accumulo.core.client.MutationsRejectedException;\n+import org.apache.accumulo.core.clientImpl.NamespaceMapping;\n import org.apache.accumulo.core.data.ColumnUpdate;\n import org.apache.accumulo.core.data.InstanceId;\n import org.apache.accumulo.core.data.Key;\n@@ -346,14 +352,18 @@ public void upgradeZooKeeperTest() throws Exception {\n     Upgrader11to12 upgrader = new Upgrader11to12();\n \n     ServerContext context = createMock(ServerContext.class);\n-    ZooReaderWriter zrw = createMock(ZooReaderWriter.class);\n+    ZooReaderWriter zrw = createStrictMock(ZooReaderWriter.class);\n \n     expect(context.getInstanceID()).andReturn(iid).anyTimes();\n     expect(context.getZooReaderWriter()).andReturn(zrw).anyTimes();\n \n+    zrw.recursiveDelete(Constants.ZROOT + \""/\"" + iid.canonical() + \""/tracers\"",\n+        ZooUtil.NodeMissingPolicy.SKIP);\n+    expectLastCall().once();\n+\n     Capture<Stat> statCapture = newCapture();\n-    expect(zrw.getData(eq(\""/accumulo/\"" + iid.canonical() + \""/root_tablet\""), capture(statCapture)))\n-        .andAnswer(() -> {\n+    expect(zrw.getData(eq(Constants.ZROOT + \""/\"" + iid.canonical() + \""/root_tablet\""),\n+        capture(statCapture))).andAnswer(() -> {\n           Stat stat = statCapture.getValue();\n           stat.setCtime(System.currentTimeMillis());\n           stat.setMtime(System.currentTimeMillis());\n@@ -364,12 +374,31 @@ public void upgradeZooKeeperTest() throws Exception {\n         }).once();\n \n     Capture<byte[]> byteCapture = newCapture();\n-    expect(zrw.overwritePersistentData(eq(\""/accumulo/\"" + iid.canonical() + \""/root_tablet\""),\n+    expect(zrw.overwritePersistentData(eq(Constants.ZROOT + \""/\"" + iid.canonical() + \""/root_tablet\""),\n         capture(byteCapture), eq(123))).andReturn(true).once();\n \n-    zrw.recursiveDelete(\""/accumulo/\"" + iid.canonical() + \""/tracers\"",\n-        ZooUtil.NodeMissingPolicy.SKIP);\n-    expectLastCall().once();\n+    expect(zrw.getData(eq(Constants.ZROOT + \""/\"" + iid.canonical() + Constants.ZNAMESPACES)))\n+        .andReturn(new byte[0]).once();\n+    Map<String,String> mockNamespaces = Map.of(\""ns1\"", \""ns1name\"", \""ns2\"", \""ns2name\"");\n+    expect(zrw.getChildren(eq(Constants.ZROOT + \""/\"" + iid.canonical() + Constants.ZNAMESPACES)))\n+        .andReturn(List.copyOf(mockNamespaces.keySet())).once();\n+    for (String ns : mockNamespaces.keySet()) {\n+      Supplier<String> pathMatcher = () -> eq(Constants.ZROOT + \""/\"" + iid.canonical()\n+          + Constants.ZNAMESPACES + \""/\"" + ns + ZNAMESPACE_NAME);\n+      expect(zrw.getData(pathMatcher.get())).andReturn(mockNamespaces.get(ns).getBytes(UTF_8))\n+          .once();\n+    }\n+    byte[] mapping = NamespaceMapping.serialize(mockNamespaces);\n+    expect(\n+        zrw.putPersistentData(eq(Constants.ZROOT + \""/\"" + iid.canonical() + Constants.ZNAMESPACES),\n+            aryEq(mapping), eq(ZooUtil.NodeExistsPolicy.OVERWRITE)))\n+        .andReturn(true).once();\n+    for (String ns : mockNamespaces.keySet()) {\n+      Supplier<String> pathMatcher = () -> eq(Constants.ZROOT + \""/\"" + iid.canonical()\n+          + Constants.ZNAMESPACES + \""/\"" + ns + ZNAMESPACE_NAME);\n+      zrw.delete(pathMatcher.get());\n+      expectLastCall().once();\n+    }\n \n     replay(context, zrw);\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"

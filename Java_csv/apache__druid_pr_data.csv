metadata
"{""instance_id"": ""apache__druid-17937"", ""pr_id"": 17937, ""issue_id"": 17769, ""repo"": ""apache/druid"", ""problem_statement"": ""Enhancing Query Context Handling: Introducing SQL-Level SETTINGS, Raw SQL Support, System Table and Context Validation\n## Related PRs\n\n-  Support of `SET` statement: #17894\n-  Raw SQL on HTTP endpoints: #17937\n- a sys.settings system table and validation of query parameters: #18087\n- [ ] TO DO: SETTINGs subclause on SQL\n\n## Motivation\n\nQuery context is part of query request. Under current implementation, query context and SQL are seperated. It makes sense for native query, where query and query context are kept in separated fields. \nHowever, for SQL, such design imposes complexity of SQL request -- we have to write SQL in JSON way. \n```json\n{\n  \""query\"":\""SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \\\""__time\\\"" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10\"",\n  \""context\"":  {\n    \""enableParallelMerge\"": false\n  }\n}\n```\n\nThis is NOT the straightforward way to use SQL.\n\nUnder the web-cosole, the console helps us encapsulate the query into the JSON, however, there're still problems:\n1. After writting SQL, we have to use 'Edit Context' feature on web-console to customize settings. \n2. query context is query level instead of client side application level. web-console remembers the edited query context for furture queries, which might not be what we expect. Sometimes we forget to reset the query context, or we have to delete the query context manually. \n\nAnother probloem is that, there's no validation of the query context items. \nWe can put ANYTHING in the query context, if there's typo of query context attribute name, Druid DOES NOT tell us about it.\n\nLast but not the least, we DON'T know which query context properties are supported by Druid, we have to read through different documents to know what query context properties are supported, such as:\n- https://druid.apache.org/docs/latest/querying/searchquery#query-context\n- https://druid.apache.org/docs/latest/querying/query-context\n- https://druid.apache.org/docs/latest/querying/groupbyquery#advanced-configurations\n\n## Proposal\n\nLet's solve these problem together. \n\n### Firstly, let's introduce a `SETTINGS` subclause in the SQL statement. \nThis subclause accepts a list of key-value pair, where each key is the support query context property while the value is the corresponding value of that property. For example:\n\n```sql\nSELECT * FROM wikipedia\nSETTINGS enableParallelMerge = false, sqlOuterLimit = 10\n```\n\nSince query context now is part of SQL, it's naturally for users to add/append query context properties per query as they want.\n\nSome other databases solves this problem in different ways.\n\n- For OLTP database like MySQL, it provides `SET` statement to allow users to change session level variables. Since Druid has no 'session' concept because queries are executed on HTTP connection, such alternative is NOT applicable for Druid\n- Some databases like StarRocks, allows users customize variables in SQL hint, like:\n```sql\nSELECT /*+ SET_VAR(query_mem_limit = 8589934592) */ name FROM people ORDER BY name;\n```\nThis does not require changes of SQL parser, but the biggest disadvantage is it's not user friendly.\n- SQL Server provides a [`OPTION` subclause](https://learn.microsoft.com/en-us/sql/t-sql/queries/option-clause-transact-sql) as query hint, which is similar to the proposal\n```sql\nSELECT * FROM FactResellerSales\nOPTION (LABEL = 'q17');\n```\n\nThe proposed change is not easy in Druid as it requires us to customize Calcite by editing the file `sql/src/main/codegen/config.fmpp`\nWhat the parser does is converting the settings clause into a `QueryContext` object internally.\n\n### Secondly, let's improve the `/druid/v2/sql` endpoint by allowing Druid accept raw text SQL instead of only JSON format.\n\nIf the Content-Type is given as `application/json`, which is current behaviour, Druid treats the input as JSON, or it treats the entire input as raw SQL text.\n\nUnder this mode, we can send SQLs to Druid in much simpler way:\n\n```text\ncurl -X 'POST' -d 'SELECT * FROM wikipedia SETTINGS enableParallelMerge = false, sqlOuterLimit = 10'  http://localhost:8888/druid/v2/sql \n```\n\n### Thirdly, inside the Druid, let's define a `sys.settings` system table to hold all query context properties.\n\nWe should put all query context properties together and register them into this table so that query context properties can be managed in a single place.\n\nThe schema of this should be sth as follows:\n\n|  Column Name | Type | Description |\n|----------------|------|------------|\n| name | String | query context property name |\n| type | String | type of this property |\n| default_value | String | The default value of this property is it's not given in user's query |\n| description | String | The description of this property |\n\nWith this table:\n- it's very easy for users to know how many properties/what kind of properties are supported in the query context. No need to check documents as the default document pages matches the latest the version which might be different from the version users are using. Querying from sys.settings table always tell them which properties are supported\n- web-console can also use this system table for better code completion and user experience\n\n### Forthly, Druid MUST verify if query context properties given by user queries are valid\n\nWhen a query comes into Druid, it should verifies if given query context properties are pre-defined and valid. It MUST reject any queries with bad query context settings.\n\n\nThe above changes 1,2,3 are independent(so they can be done separately) while the validation of query context attributes might share the same internal data structure of `sys.settings` table.\n\n"", ""issue_word_count"": 896, ""test_files_count"": 3, ""non_test_files_count"": 10, ""pr_changed_files"": [""docs/api-reference/sql-api.md"", ""extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/dart/controller/http/DartSqlResource.java"", ""extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/sql/resources/SqlStatementResource.java"", ""integration-tests/pom.xml"", ""integration-tests/src/test/java/org/apache/druid/tests/query/ITSqlQueryTest.java"", ""server/src/main/java/org/apache/druid/server/initialization/jetty/HttpException.java"", ""server/src/main/java/org/apache/druid/server/initialization/jetty/HttpExceptionMapper.java"", ""server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java"", ""services/src/main/java/org/apache/druid/server/AsyncQueryForwardingServlet.java"", ""services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java"", ""sql/pom.xml"", ""sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java"", ""sql/src/main/java/org/apache/druid/sql/http/SqlResource.java""], ""pr_changed_test_files"": [""integration-tests/pom.xml"", ""integration-tests/src/test/java/org/apache/druid/tests/query/ITSqlQueryTest.java"", ""services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java""], ""base_commit"": ""a85a3e58570d2eec62a5be312fb9e7c86fe37be7"", ""head_commit"": ""f5a74e521e8fef6aa1b89a4e5b42408509f9b2ea"", ""repo_url"": ""https://github.com/apache/druid/pull/17937"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17937"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-30T23:03:09.000Z"", ""patch"": ""diff --git a/docs/api-reference/sql-api.md b/docs/api-reference/sql-api.md\nindex ccba5b154355..7ec029038c68 100644\n--- a/docs/api-reference/sql-api.md\n+++ b/docs/api-reference/sql-api.md\n@@ -38,7 +38,8 @@ In this topic, `http://ROUTER_IP:ROUTER_PORT` is a placeholder for your Router s\n \n ### Submit a query\n \n-Submits a SQL-based query in the JSON request body. Returns a JSON object with the query results and optional metadata for the results. You can also use this endpoint to query [metadata tables](../querying/sql-metadata-tables.md).\n+Submits a SQL-based query in the JSON or text format request body. \n+Returns a JSON object with the query results and optional metadata for the results. You can also use this endpoint to query [metadata tables](../querying/sql-metadata-tables.md).\n \n Each query has an associated SQL query ID. You can set this ID manually using the SQL context parameter `sqlQueryId`. If not set, Druid automatically generates `sqlQueryId` and returns it in the response header for `X-Druid-SQL-Query-Id`. Note that you need the `sqlQueryId` to [cancel a query](#cancel-a-query).\n \n@@ -46,7 +47,10 @@ Each query has an associated SQL query ID. You can set this ID manually using th\n \n `POST` `/druid/v2/sql`\n \n-#### Request body\n+#### JSON Format Request body\n+\n+To send queries in JSON format, the `Content-Type` in the HTTP request MUST be `application/json`.\n+If there are multiple `Content-Type` headers, the **first** one is used.\n \n The request body takes the following properties:\n \n@@ -99,6 +103,36 @@ The request body takes the following properties:\n     }\n     ```\n \n+##### Text Format Request body\n+\n+Druid also allows you to submit SQL queries in text format which is simpler than above JSON format. \n+To do this, just set the `Content-Type` request header to `text/plain` or `application/x-www-form-urlencoded`, and pass SQL via the HTTP Body. \n+\n+If `application/x-www-form-urlencoded` is used, make sure the SQL query is URL-encoded.\n+\n+If there are multiple `Content-Type` headers, the **first** one is used.\n+\n+For response, the `resultFormat` is always `object` with the HTTP response header `Content-Type: application/json`.\n+If you want more control over the query context or response format, use the above JSON format request body instead.\n+\n+The following example demonstrates how to submit a SQL query in text format:\n+\n+```commandline\n+echo 'SELECT 1' | curl -H 'Content-Type: text/plain' http://ROUTER_IP:ROUTER_PORT/druid/v2/sql --data @- \n+```\n+\n+We can also use `application/x-www-form-urlencoded` to submit URL-encoded SQL queries as shown by the following examples:\n+\n+```commandline\n+echo 'SELECT%20%31' | curl http://ROUTER_IP:ROUTER_PORT/druid/v2/sql --data @-\n+echo 'SELECT 1' | curl http://ROUTER_IP:ROUTER_PORT/druid/v2/sql --data-urlencode @-\n+```\n+\n+The `curl` tool uses `application/x-www-form-urlencoded` as Content-Type header if the header is not given.\n+\n+The first example pass the URL-encoded query `SELECT%20%31`, which is `SELECT 1`, to the `curl` and `curl` will directly sends it to the server.\n+While the second example passes the raw query `SELECT 1` to `curl` and the `curl` encodes the query to `SELECT%20%31` because of `--data-urlencode` option and sends the encoded text to the server.\n+\n #### Responses\n \n <Tabs>\n\ndiff --git a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/dart/controller/http/DartSqlResource.java b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/dart/controller/http/DartSqlResource.java\nindex a277d7d126ff..7d1f0bce0f08 100644\n--- a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/dart/controller/http/DartSqlResource.java\n+++ b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/dart/controller/http/DartSqlResource.java\n@@ -24,6 +24,7 @@\n import com.google.common.collect.Iterables;\n import com.google.common.util.concurrent.Futures;\n import com.google.inject.Inject;\n+import com.sun.jersey.api.core.HttpContext;\n import org.apache.druid.common.guava.FutureUtils;\n import org.apache.druid.guice.annotations.Self;\n import org.apache.druid.java.util.common.logger.Logger;\n@@ -50,7 +51,6 @@\n import org.apache.druid.sql.http.SqlResource;\n \n import javax.servlet.http.HttpServletRequest;\n-import javax.ws.rs.Consumes;\n import javax.ws.rs.DELETE;\n import javax.ws.rs.GET;\n import javax.ws.rs.POST;\n@@ -202,11 +202,19 @@ public GetQueriesResponse doGetRunningQueries(\n    */\n   @POST\n   @Produces(MediaType.APPLICATION_JSON)\n-  @Consumes(MediaType.APPLICATION_JSON)\n+  @Override\n+  public Response doPost(\n+      @Context final HttpServletRequest req,\n+      @Context final HttpContext httpContext\n+  )\n+  {\n+    return this.doPost(SqlQuery.from(httpContext), req);\n+  }\n+\n   @Override\n   public Response doPost(\n       final SqlQuery sqlQuery,\n-      @Context final HttpServletRequest req\n+      final HttpServletRequest req\n   )\n   {\n     final Map<String, Object> context = new HashMap<>(sqlQuery.getContext());\n\ndiff --git a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/sql/resources/SqlStatementResource.java b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/sql/resources/SqlStatementResource.java\nindex f0a800c246ab..a45d77949228 100644\n--- a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/sql/resources/SqlStatementResource.java\n+++ b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/sql/resources/SqlStatementResource.java\n@@ -26,6 +26,7 @@\n import com.google.common.io.CountingOutputStream;\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.inject.Inject;\n+import com.sun.jersey.api.core.HttpContext;\n import org.apache.druid.client.indexing.TaskPayloadResponse;\n import org.apache.druid.client.indexing.TaskStatusResponse;\n import org.apache.druid.common.guava.FutureUtils;\n@@ -96,7 +97,6 @@\n \n import javax.servlet.http.HttpServletRequest;\n import javax.validation.constraints.NotNull;\n-import javax.ws.rs.Consumes;\n import javax.ws.rs.DELETE;\n import javax.ws.rs.GET;\n import javax.ws.rs.POST;\n@@ -167,8 +167,15 @@ public Response isEnabled(@Context final HttpServletRequest request)\n \n   @POST\n   @Produces(MediaType.APPLICATION_JSON)\n-  @Consumes(MediaType.APPLICATION_JSON)\n-  public Response doPost(final SqlQuery sqlQuery, @Context final HttpServletRequest req)\n+  public Response doPost(@Context final HttpServletRequest req,\n+                         @Context final HttpContext httpContext)\n+  {\n+    return doPost(SqlQuery.from(httpContext), req);\n+  }\n+\n+  @VisibleForTesting\n+  Response doPost(final SqlQuery sqlQuery,\n+                  final HttpServletRequest req)\n   {\n     SqlQuery modifiedQuery = createModifiedSqlQuery(sqlQuery);\n \n\ndiff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpException.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpException.java\nnew file mode 100644\nindex 000000000000..9d779f16d9ac\n--- /dev/null\n+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpException.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.initialization.jetty;\n+\n+\n+import javax.ws.rs.core.Response;\n+\n+public class HttpException extends RuntimeException\n+{\n+  private final Response.Status statusCode;\n+  private final String message;\n+\n+  public HttpException(Response.Status statusCode, String message)\n+  {\n+    this.statusCode = statusCode;\n+    this.message = message;\n+  }\n+\n+  public Response.Status getStatusCode()\n+  {\n+    return statusCode;\n+  }\n+\n+  @Override\n+  public String getMessage()\n+  {\n+    return message;\n+  }\n+}\n\ndiff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpExceptionMapper.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpExceptionMapper.java\nnew file mode 100644\nindex 000000000000..710576859bc5\n--- /dev/null\n+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/HttpExceptionMapper.java\n@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.initialization.jetty;\n+\n+\n+import com.google.common.collect.ImmutableMap;\n+\n+import javax.ws.rs.core.MediaType;\n+import javax.ws.rs.core.Response;\n+import javax.ws.rs.ext.ExceptionMapper;\n+import javax.ws.rs.ext.Provider;\n+\n+@Provider\n+public class HttpExceptionMapper implements ExceptionMapper<HttpException>\n+{\n+  @Override\n+  public Response toResponse(HttpException exception)\n+  {\n+    return Response.status(exception.getStatusCode())\n+                   .type(MediaType.APPLICATION_JSON)\n+                   .entity(ImmutableMap.of(\n+                       \""error\"", exception.getMessage()\n+                   ))\n+                   .build();\n+  }\n+}\n\ndiff --git a/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java b/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java\nindex 8ce48871c155..a662ca00541f 100644\n--- a/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java\n+++ b/server/src/main/java/org/apache/druid/server/initialization/jetty/JettyServerModule.java\n@@ -127,6 +127,7 @@ protected void configureServlets()\n     binder.bind(ForbiddenExceptionMapper.class).in(LazySingleton.class);\n     binder.bind(BadRequestExceptionMapper.class).in(LazySingleton.class);\n     binder.bind(ServiceUnavailableExceptionMapper.class).in(LazySingleton.class);\n+    binder.bind(HttpExceptionMapper.class).in(LazySingleton.class);\n \n     serve(\""/*\"").with(GuiceContainer.class);\n \n\ndiff --git a/services/src/main/java/org/apache/druid/server/AsyncQueryForwardingServlet.java b/services/src/main/java/org/apache/druid/server/AsyncQueryForwardingServlet.java\nindex f626b45d97f3..b1f4153cf740 100644\n--- a/services/src/main/java/org/apache/druid/server/AsyncQueryForwardingServlet.java\n+++ b/services/src/main/java/org/apache/druid/server/AsyncQueryForwardingServlet.java\n@@ -47,6 +47,7 @@\n import org.apache.druid.query.QueryMetrics;\n import org.apache.druid.query.QueryToolChestWarehouse;\n import org.apache.druid.server.initialization.ServerConfig;\n+import org.apache.druid.server.initialization.jetty.HttpException;\n import org.apache.druid.server.initialization.jetty.StandardResponseHeaderFilterHolder;\n import org.apache.druid.server.log.RequestLogger;\n import org.apache.druid.server.metrics.QueryCountStatsProvider;\n@@ -273,7 +274,7 @@ protected void service(HttpServletRequest request, HttpServletResponse response)\n       }\n     } else if (isSqlQueryEndpoint && HttpMethod.POST.is(method)) {\n       try {\n-        SqlQuery inputSqlQuery = objectMapper.readValue(request.getInputStream(), SqlQuery.class);\n+        SqlQuery inputSqlQuery = SqlQuery.from(request, objectMapper);\n         inputSqlQuery = buildSqlQueryWithId(inputSqlQuery);\n         request.setAttribute(SQL_QUERY_ATTRIBUTE, inputSqlQuery);\n         if (routeSqlByStrategy) {\n@@ -283,8 +284,8 @@ protected void service(HttpServletRequest request, HttpServletResponse response)\n         }\n         LOG.debug(\""Forwarding SQL query to broker [%s]\"", targetServer.getHost());\n       }\n-      catch (IOException e) {\n-        handleQueryParseException(request, response, objectMapper, e, false);\n+      catch (HttpException e) {\n+        handleQueryParseException(request, response, e.getStatusCode().getStatusCode(), objectMapper, e, false);\n         return;\n       }\n       catch (Exception e) {\n@@ -359,7 +360,20 @@ void handleQueryParseException(\n       HttpServletRequest request,\n       HttpServletResponse response,\n       ObjectMapper objectMapper,\n-      IOException parseException,\n+      Throwable parseException,\n+      boolean isNativeQuery\n+  ) throws IOException\n+  {\n+    handleQueryParseException(request, response, HttpServletResponse.SC_BAD_REQUEST, objectMapper, parseException, isNativeQuery);\n+  }\n+\n+  private void handleQueryParseException(\n+      HttpServletRequest request,\n+      HttpServletResponse response,\n+      int httpStatusCode,\n+      ObjectMapper objectMapper,\n+      Throwable parseException,\n+\n       boolean isNativeQuery\n   ) throws IOException\n   {\n@@ -394,7 +408,7 @@ void handleQueryParseException(\n     }\n \n     // Write to the response\n-    response.setStatus(HttpServletResponse.SC_BAD_REQUEST);\n+    response.setStatus(httpStatusCode);\n     response.setContentType(MediaType.APPLICATION_JSON);\n     objectMapper.writeValue(\n         response.getOutputStream(),\n@@ -470,6 +484,7 @@ private void setProxyRequestContent(Request proxyRequest, HttpServletRequest cli\n       byte[] bytes = objectMapper.writeValueAsBytes(content);\n       proxyRequest.content(new BytesContentProvider(bytes));\n       proxyRequest.getHeaders().put(HttpHeader.CONTENT_LENGTH, String.valueOf(bytes.length));\n+      proxyRequest.getHeaders().put(HttpHeader.CONTENT_TYPE, MediaType.APPLICATION_JSON);\n     }\n     catch (JsonProcessingException e) {\n       throw new RuntimeException(e);\n\ndiff --git a/sql/pom.xml b/sql/pom.xml\nindex 5afdc8b26888..e6e463231f10 100644\n--- a/sql/pom.xml\n+++ b/sql/pom.xml\n@@ -165,6 +165,10 @@\n       <groupId>org.apache.commons</groupId>\n       <artifactId>commons-lang3</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>commons-io</groupId>\n+      <artifactId>commons-io</artifactId>\n+    </dependency>\n     <dependency>\n       <groupId>org.checkerframework</groupId>\n       <artifactId>checker-qual</artifactId>\n@@ -186,6 +190,10 @@\n       <artifactId>value-annotations</artifactId>\n       <scope>provided</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>com.sun.jersey</groupId>\n+      <artifactId>jersey-server</artifactId>\n+    </dependency>\n \n     <!-- Tests -->\n     <dependency>\n@@ -264,11 +272,6 @@\n       <artifactId>jackson-dataformat-yaml</artifactId>\n       <scope>test</scope>\n     </dependency>\n-    <dependency>\n-      <groupId>commons-io</groupId>\n-      <artifactId>commons-io</artifactId>\n-      <scope>test</scope>\n-    </dependency>\n     <dependency>\n       <groupId>org.apache.druid</groupId>\n       <artifactId>druid-processing</artifactId>\n\ndiff --git a/sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java b/sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java\nindex 623570f3df23..71f9c14563ce 100644\n--- a/sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java\n+++ b/sql/src/main/java/org/apache/druid/sql/http/SqlQuery.java\n@@ -22,15 +22,30 @@\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonInclude;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonParseException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.exc.MismatchedInputException;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.sun.jersey.api.container.ContainerException;\n+import com.sun.jersey.api.core.HttpContext;\n import org.apache.calcite.avatica.remote.TypedValue;\n+import org.apache.commons.io.IOUtils;\n import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.query.QueryContext;\n import org.apache.druid.query.http.ClientSqlQuery;\n+import org.apache.druid.server.initialization.jetty.HttpException;\n \n import javax.annotation.Nullable;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.ws.rs.core.HttpHeaders;\n+import javax.ws.rs.core.MediaType;\n+import javax.ws.rs.core.Response;\n+import java.io.IOException;\n+import java.net.URLDecoder;\n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n@@ -200,4 +215,133 @@ public SqlQuery withQueryContext(Map<String, Object> newContext)\n   {\n     return new SqlQuery(query, resultFormat, header, typesHeader, sqlTypesHeader, newContext, parameters);\n   }\n+\n+  /**\n+   * Extract SQL query object or SQL text from an HTTP Request\n+   */\n+  @FunctionalInterface\n+  interface ISqlQueryExtractor<T>\n+  {\n+    T extract() throws IOException;\n+  }\n+\n+  /**\n+   * For BROKERs to use.\n+   * <p>\n+   * Brokers use com.sun.jersey upon Jetty for RESTful API, however jersey int\u00d8ernally has special handling for x-www-form-urlencoded,\n+   * it's not able to get the data from the stream of HttpServletRequest for such content type.\n+   * So we use HttpContext to get the request entity/string instead of using HttpServletRequest.\n+   *\n+   * @throws HttpException if the content type is not supported or the SQL query is malformed\n+   */\n+  public static SqlQuery from(HttpContext httpContext) throws HttpException\n+  {\n+    return from(\n+        httpContext.getRequest().getRequestHeaders().getFirst(HttpHeaders.CONTENT_TYPE),\n+        () -> {\n+          try {\n+            return httpContext.getRequest().getEntity(SqlQuery.class);\n+          }\n+          catch (ContainerException e) {\n+            if (e.getCause() instanceof JsonParseException) {\n+              throw new HttpException(\n+                  Response.Status.BAD_REQUEST,\n+                  StringUtils.format(\""Malformed SQL query wrapped in JSON: %s\"", e.getCause().getMessage())\n+              );\n+            } else {\n+              throw e;\n+            }\n+          }\n+        },\n+        () -> httpContext.getRequest().getEntity(String.class)\n+    );\n+  }\n+\n+  /**\n+   * For Router to use\n+   *\n+   * @throws HttpException if the content type is not supported or the SQL query is malformed\n+   */\n+  public static SqlQuery from(HttpServletRequest request, ObjectMapper objectMapper) throws HttpException\n+  {\n+    return from(\n+        request.getContentType(),\n+        () -> objectMapper.readValue(request.getInputStream(), SqlQuery.class),\n+        () -> new String(IOUtils.toByteArray(request.getInputStream()), StandardCharsets.UTF_8)\n+    );\n+  }\n+\n+  private static SqlQuery from(\n+      String contentType,\n+      ISqlQueryExtractor<SqlQuery> jsonQueryExtractor,\n+      ISqlQueryExtractor<String> rawQueryExtractor\n+  ) throws HttpException\n+  {\n+    try {\n+      if (MediaType.APPLICATION_JSON.equals(contentType)) {\n+\n+        SqlQuery sqlQuery = jsonQueryExtractor.extract();\n+        if (sqlQuery == null) {\n+          throw new HttpException(Response.Status.BAD_REQUEST, \""Empty query\"");\n+        }\n+        return sqlQuery;\n+\n+      } else if (MediaType.TEXT_PLAIN.equals(contentType)) {\n+\n+        String sql = rawQueryExtractor.extract().trim();\n+        if (sql.isEmpty()) {\n+          throw new HttpException(Response.Status.BAD_REQUEST, \""Empty query\"");\n+        }\n+\n+        return new SqlQuery(sql, null, false, false, false, null, null);\n+\n+      } else if (MediaType.APPLICATION_FORM_URLENCODED.equals(contentType)) {\n+\n+        String sql = rawQueryExtractor.extract().trim();\n+        if (sql.isEmpty()) {\n+          throw new HttpException(Response.Status.BAD_REQUEST, \""Empty query\"");\n+        }\n+\n+        try {\n+          sql = URLDecoder.decode(sql, StandardCharsets.UTF_8);\n+        }\n+        catch (IllegalArgumentException e) {\n+          throw new HttpException(\n+              Response.Status.BAD_REQUEST,\n+              \""Unable to decoded URL-Encoded SQL query: \"" + e.getMessage()\n+          );\n+        }\n+\n+        return new SqlQuery(sql, null, false, false, false, null, null);\n+\n+      } else {\n+        throw new HttpException(\n+            Response.Status.UNSUPPORTED_MEDIA_TYPE,\n+            StringUtils.format(\n+                \""Unsupported Content-Type: %s. Only application/json, text/plain or application/x-www-form-urlencoded is supported.\"",\n+                contentType\n+            )\n+        );\n+      }\n+    }\n+    catch (MismatchedInputException e) {\n+      if (e.getOriginalMessage().endsWith(\""end-of-input\"")) {\n+        throw new HttpException(Response.Status.BAD_REQUEST, \""Empty query\"");\n+      } else {\n+        throw new HttpException(\n+            Response.Status.BAD_REQUEST,\n+            StringUtils.format(\""Malformed SQL query wrapped in JSON: %s\"", e.getMessage())\n+        );\n+      }\n+    }\n+    catch (JsonParseException e) {\n+      throw new HttpException(\n+          Response.Status.BAD_REQUEST,\n+          StringUtils.format(\""Malformed SQL query wrapped in JSON: %s\"", e.getMessage())\n+      );\n+    }\n+    catch (IOException e) {\n+      throw new HttpException(Response.Status.BAD_REQUEST, \""Unable to read query from request: \"" + e.getMessage());\n+    }\n+  }\n }\n\ndiff --git a/sql/src/main/java/org/apache/druid/sql/http/SqlResource.java b/sql/src/main/java/org/apache/druid/sql/http/SqlResource.java\nindex c966b4ec8bd2..5199500bd494 100644\n--- a/sql/src/main/java/org/apache/druid/sql/http/SqlResource.java\n+++ b/sql/src/main/java/org/apache/druid/sql/http/SqlResource.java\n@@ -22,6 +22,7 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Preconditions;\n import com.google.inject.Inject;\n+import com.sun.jersey.api.core.HttpContext;\n import org.apache.druid.common.exception.SanitizableException;\n import org.apache.druid.guice.annotations.NativeQuery;\n import org.apache.druid.guice.annotations.Self;\n@@ -47,7 +48,6 @@\n \n import javax.annotation.Nullable;\n import javax.servlet.http.HttpServletRequest;\n-import javax.ws.rs.Consumes;\n import javax.ws.rs.DELETE;\n import javax.ws.rs.POST;\n import javax.ws.rs.Path;\n@@ -104,11 +104,21 @@ protected SqlResource(\n \n   @POST\n   @Produces(MediaType.APPLICATION_JSON)\n-  @Consumes(MediaType.APPLICATION_JSON)\n   @Nullable\n+  public Response doPost(\n+      @Context final HttpServletRequest req,\n+      @Context final HttpContext httpContext\n+  )\n+  {\n+    return doPost(SqlQuery.from(httpContext), req);\n+  }\n+\n+  /**\n+   * This method is defined as public so that subclasses like Dart or test can access it\n+   */\n   public Response doPost(\n       final SqlQuery sqlQuery,\n-      @Context final HttpServletRequest req\n+      final HttpServletRequest req\n   )\n   {\n     final HttpStatement stmt = sqlStatementFactory.httpStatement(sqlQuery, req);\n"", ""test_patch"": ""diff --git a/integration-tests/pom.xml b/integration-tests/pom.xml\nindex f54e88c93044..60cbe4edcb4d 100644\n--- a/integration-tests/pom.xml\n+++ b/integration-tests/pom.xml\n@@ -418,6 +418,14 @@\n             <groupId>com.google.protobuf</groupId>\n             <artifactId>protobuf-java</artifactId>\n         </dependency>\n+        <dependency>\n+            <groupId>org.apache.httpcomponents</groupId>\n+            <artifactId>httpcore</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.httpcomponents</groupId>\n+            <artifactId>httpclient</artifactId>\n+        </dependency>\n \n         <!-- Tests -->\n         <dependency>\n\ndiff --git a/integration-tests/src/test/java/org/apache/druid/tests/query/ITSqlQueryTest.java b/integration-tests/src/test/java/org/apache/druid/tests/query/ITSqlQueryTest.java\nnew file mode 100644\nindex 000000000000..46d6a938321e\n--- /dev/null\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/query/ITSqlQueryTest.java\n@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.query;\n+\n+import com.google.inject.Inject;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.IntegrationTestingConfig;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.apache.http.HttpEntity;\n+import org.apache.http.HttpStatus;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpPost;\n+import org.apache.http.entity.StringEntity;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.apache.http.util.EntityUtils;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import javax.ws.rs.core.MediaType;\n+import java.io.IOException;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.function.Function;\n+\n+/**\n+ * Test the SQL endpoint with different Content-Type\n+ */\n+@Test(groups = {TestNGGroup.QUERY, TestNGGroup.CENTRALIZED_DATASOURCE_SCHEMA})\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITSqlQueryTest\n+{\n+  private static final Logger LOG = new Logger(ITSqlQueryTest.class);\n+\n+  @Inject\n+  IntegrationTestingConfig config;\n+\n+  interface IExecutable\n+  {\n+    void execute(String endpoint) throws IOException;\n+  }\n+\n+  interface OnRequest\n+  {\n+    void on(HttpPost request) throws IOException;\n+  }\n+\n+  interface OnResponse\n+  {\n+    void on(int statusCode, HttpEntity response) throws IOException;\n+  }\n+\n+  private void executeWithRetry(String endpoint, String contentType, IExecutable executable)\n+  {\n+    Throwable lastException = null;\n+    for (int i = 1; i <= 5; i++) {\n+      LOG.info(\""Query to %s with Content-Type = %s, tries = %s\"", endpoint, contentType, i);\n+      try {\n+        executable.execute(endpoint);\n+        return;\n+      }\n+      catch (IOException e) {\n+        // Only catch IOException\n+        lastException = e;\n+      }\n+      try {\n+        Thread.sleep(200);\n+      }\n+      catch (InterruptedException ignored) {\n+        break;\n+      }\n+    }\n+    throw new ISE(contentType + \"" failed after 5 tries, last exception: \"" + lastException);\n+  }\n+\n+  private void executeQuery(\n+      String contentType,\n+      OnRequest onRequest,\n+      OnResponse onResponse\n+  )\n+  {\n+    IExecutable executable = (endpoint) -> {\n+      try (CloseableHttpClient client = HttpClientBuilder.create().build()) {\n+        HttpPost request = new HttpPost(endpoint);\n+        if (contentType != null) {\n+          request.addHeader(\""Content-Type\"", contentType);\n+        }\n+        onRequest.on(request);\n+\n+        try (CloseableHttpResponse response = client.execute(request)) {\n+          HttpEntity responseEntity = response.getEntity();\n+          assertNotNull(responseEntity);\n+\n+          onResponse.on(\n+              response.getStatusLine().getStatusCode(),\n+              responseEntity\n+          );\n+        }\n+      }\n+    };\n+\n+    // Send query to broker to exeucte\n+    executeWithRetry(StringUtils.format(\""%s/druid/v2/sql/\"", config.getBrokerUrl()), contentType, executable);\n+\n+    // Send query to router to execute\n+    executeWithRetry(StringUtils.format(\""%s/druid/v2/sql/\"", config.getRouterUrl()), contentType, executable);\n+  }\n+\n+  private void assertEquals(String expected, String actual)\n+  {\n+    if (!expected.equals(actual)) {\n+      throw new ISE(\""Expected [%s] but got [%s]\"", expected, actual);\n+    }\n+  }\n+\n+  private void assertEquals(int expected, int actual)\n+  {\n+    if (expected != actual) {\n+      throw new ISE(\""Expected [%d] but got [%d]\"", expected, actual);\n+    }\n+  }\n+\n+  private void assertNotNull(Object object)\n+  {\n+    if (object == null) {\n+      throw new ISE(\""Expected not null\"");\n+    }\n+  }\n+\n+  private void assertStringCompare(String expected, String actual, Function<String, Boolean> predicate)\n+  {\n+    if (!predicate.apply(expected)) {\n+      throw new ISE(\""Expected: [%s] but got [%s]\"", expected, actual);\n+    }\n+  }\n+\n+  @Test\n+  public void testNullContentType()\n+  {\n+    executeQuery(\n+        null,\n+        (request) -> {\n+          request.setEntity(new StringEntity(\""select 1\""));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(HttpStatus.SC_UNSUPPORTED_MEDIA_TYPE, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Unsupported Content-Type:\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testUnsupportedContentType()\n+  {\n+    executeQuery(\n+        \""application/xml\"",\n+        (request) -> {\n+          request.setEntity(new StringEntity(\""select 1\""));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(HttpStatus.SC_UNSUPPORTED_MEDIA_TYPE, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Unsupported Content-Type:\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testTextPlain()\n+  {\n+    executeQuery(\n+        MediaType.TEXT_PLAIN,\n+        (request) -> {\n+          request.setEntity(new StringEntity(\""select \\n1\""));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(200, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertEquals(\""[{\\\""EXPR$0\\\"":1}]\"", responseBody);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testFormURLEncoded()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_FORM_URLENCODED,\n+        (request) -> {\n+          request.setEntity(new StringEntity(URLEncoder.encode(\""select 'x % y'\"", StandardCharsets.UTF_8)));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(200, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertEquals(\""[{\\\""EXPR$0\\\"":\\\""x % y\\\""}]\"", responseBody);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testFormURLEncoded_InvalidEncoding()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_FORM_URLENCODED,\n+        (request) -> {\n+          request.setEntity(new StringEntity(\""select 'x % y'\""));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Unable to decoded\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testJSON()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_JSON,\n+        (request) -> {\n+          request.setEntity(new StringEntity(StringUtils.format(\""{\\\""query\\\"":\\\""select 567\\\""}\"")));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(200, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertEquals(\""[{\\\""EXPR$0\\\"":567}]\"", responseBody);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testInvalidJSONFormat()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_JSON,\n+        (request) -> {\n+          request.setEntity(new StringEntity(StringUtils.format(\""{\\\""query\\\"":select 567}\"")));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Malformed SQL query\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testEmptyQuery_TextPlain()\n+  {\n+    executeQuery(\n+        MediaType.TEXT_PLAIN,\n+        (request) -> {\n+          // Empty query, DO NOTHING\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Empty query\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testEmptyQuery_UrlEncoded()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_FORM_URLENCODED,\n+        (request) -> {\n+          // Empty query, DO NOTHING\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Empty query\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testBlankQuery_TextPlain()\n+  {\n+    executeQuery(\n+        MediaType.TEXT_PLAIN,\n+        (request) -> {\n+          // an query with blank characters\n+          request.setEntity(new StringEntity(\""     \""));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Empty query\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  @Test\n+  public void testEmptyQuery_JSON()\n+  {\n+    executeQuery(\n+        MediaType.APPLICATION_JSON,\n+        (request) -> {\n+          // Empty query, DO NOTHING\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(400, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertStringCompare(\""Empty query\"", responseBody, responseBody::contains);\n+        }\n+    );\n+  }\n+\n+  /**\n+   * When multiple Content-Type headers are set, the first one(in this case it's the text format) should be used.\n+   */\n+  @Test\n+  public void testMultipleContentType()\n+  {\n+    executeQuery(\n+        MediaType.TEXT_PLAIN,\n+        (request) -> {\n+          // Add one more Content-Type header\n+          request.addHeader(\""Content-Type\"", MediaType.APPLICATION_JSON);\n+          request.setEntity(new StringEntity(StringUtils.format(\""SELECT 1\"")));\n+        },\n+        (statusCode, responseEntity) -> {\n+          assertEquals(200, statusCode);\n+\n+          String responseBody = EntityUtils.toString(responseEntity).trim();\n+          assertEquals(\""[{\\\""EXPR$0\\\"":1}]\"", responseBody);\n+        }\n+    );\n+  }\n+}\n\ndiff --git a/services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java b/services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java\nindex 11d734bec291..591186069bba 100644\n--- a/services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java\n+++ b/services/src/test/java/org/apache/druid/server/AsyncQueryForwardingServletTest.java\n@@ -590,7 +590,7 @@ public int read()\n       }\n     };\n     final HttpServletRequest requestMock = EasyMock.createMock(HttpServletRequest.class);\n-    EasyMock.expect(requestMock.getContentType()).andReturn(\""application/json\"").times(2);\n+    EasyMock.expect(requestMock.getContentType()).andReturn(\""application/json\"").anyTimes();\n     requestMock.setAttribute(\""org.apache.druid.proxy.objectMapper\"", jsonMapper);\n     EasyMock.expectLastCall();\n     EasyMock.expect(requestMock.getRequestURI())\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__druid-17846"", ""pr_id"": 17846, ""issue_id"": 17435, ""repo"": ""apache/druid"", ""problem_statement"": ""Remove `org.apache.druid.discovery.BrokerClient` by switching to `org.apache.druid.sql.client.BrokerClient`\n`org.apache.druid.discovery.BrokerClient` was deprecated in favor of `org.apache.druid.sql.client.BrokerClient`  in https://github.com/apache/druid/pull/17382.\r\n\r\nCurrently, there's only one usage of `org.apache.druid.discovery.BrokerClient` in `SegmentLoadStatusFetcher` in the MSQ [code](https://github.com/apache/druid/blob/master/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java#L241). We can remove that instance altogether and migrate to using the new `org.apache.druid.sql.client.BrokerClient` which is a more robust client."", ""issue_word_count"": 104, ""test_files_count"": 3, ""non_test_files_count"": 5, ""pr_changed_files"": [""extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/ControllerImpl.java"", ""extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java"", ""extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java"", ""extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java"", ""server/src/main/java/org/apache/druid/client/broker/BrokerClient.java"", ""server/src/main/java/org/apache/druid/client/broker/BrokerClientImpl.java"", ""server/src/main/java/org/apache/druid/discovery/BrokerClient.java"", ""server/src/test/java/org/apache/druid/discovery/BrokerClientTest.java""], ""pr_changed_test_files"": [""extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java"", ""extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java"", ""server/src/test/java/org/apache/druid/discovery/BrokerClientTest.java""], ""base_commit"": ""43b16b433b5b9eafd817a820b214dab1156ed25b"", ""head_commit"": ""92b64af0fa4e0995dde00ebd60239e0e14157599"", ""repo_url"": ""https://github.com/apache/druid/pull/17846"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17846"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-04T02:56:37.000Z"", ""patch"": ""diff --git a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/ControllerImpl.java b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/ControllerImpl.java\nindex cc73ea42c1a4..f890a9e9ac0f 100644\n--- a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/ControllerImpl.java\n+++ b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/ControllerImpl.java\n@@ -34,10 +34,10 @@\n import it.unimi.dsi.fastutil.ints.IntArraySet;\n import it.unimi.dsi.fastutil.ints.IntList;\n import it.unimi.dsi.fastutil.ints.IntSet;\n+import org.apache.druid.client.broker.BrokerClient;\n import org.apache.druid.common.guava.FutureUtils;\n import org.apache.druid.data.input.StringTuple;\n import org.apache.druid.data.input.impl.DimensionsSpec;\n-import org.apache.druid.discovery.BrokerClient;\n import org.apache.druid.error.DruidException;\n import org.apache.druid.frame.allocation.ArenaMemoryAllocator;\n import org.apache.druid.frame.channel.ReadableConcatFrameChannel;\n\ndiff --git a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java\nindex d4eaef600125..814f4d8a63db 100644\n--- a/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java\n+++ b/extensions-core/multi-stage-query/src/main/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcher.java\n@@ -26,23 +26,20 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.util.concurrent.ListeningExecutorService;\n import com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.druid.client.broker.BrokerClient;\n import org.apache.druid.common.guava.FutureUtils;\n-import org.apache.druid.discovery.BrokerClient;\n import org.apache.druid.java.util.common.DateTimes;\n import org.apache.druid.java.util.common.Pair;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.concurrent.Execs;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.java.util.http.client.Request;\n+import org.apache.druid.query.http.ClientSqlQuery;\n import org.apache.druid.sql.http.ResultFormat;\n-import org.apache.druid.sql.http.SqlQuery;\n import org.apache.druid.timeline.DataSegment;\n-import org.jboss.netty.handler.codec.http.HttpMethod;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n import javax.annotation.Nullable;\n-import javax.ws.rs.core.MediaType;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n@@ -238,13 +235,12 @@ private void updateStatus(State state, DateTime startTime)\n    */\n   private VersionLoadStatus fetchLoadStatusFromBroker() throws Exception\n   {\n-    Request request = brokerClient.makeRequest(HttpMethod.POST, \""/druid/v2/sql/\"");\n-    SqlQuery sqlQuery = new SqlQuery(StringUtils.format(LOAD_QUERY, datasource, versionsConditionString),\n-                                     ResultFormat.OBJECTLINES,\n-                                     false, false, false, null, null\n+    ClientSqlQuery clientSqlQuery = new ClientSqlQuery(\n+        StringUtils.format(LOAD_QUERY, datasource, versionsConditionString),\n+        ResultFormat.OBJECTLINES.contentType(),\n+        false, false, false, null, null\n     );\n-    request.setContent(MediaType.APPLICATION_JSON, objectMapper.writeValueAsBytes(sqlQuery));\n-    String response = brokerClient.sendQuery(request);\n+    final String response = FutureUtils.get(brokerClient.submitSqlQuery(clientSqlQuery), true);\n \n     if (response == null) {\n       // Unable to query broker\n\ndiff --git a/server/src/main/java/org/apache/druid/client/broker/BrokerClient.java b/server/src/main/java/org/apache/druid/client/broker/BrokerClient.java\nindex 611e6399ee69..ea370572c447 100644\n--- a/server/src/main/java/org/apache/druid/client/broker/BrokerClient.java\n+++ b/server/src/main/java/org/apache/druid/client/broker/BrokerClient.java\n@@ -37,6 +37,11 @@\n  */\n public interface BrokerClient\n {\n+  /**\n+   * Submit the given {@code sqlQuery} to the Broker's SQL query endpoint.\n+   */\n+  ListenableFuture<String> submitSqlQuery(ClientSqlQuery sqlQuery);\n+\n   /**\n    * Submit the given {@code sqlQuery} to the Broker's SQL task endpoint.\n    */\n\ndiff --git a/server/src/main/java/org/apache/druid/client/broker/BrokerClientImpl.java b/server/src/main/java/org/apache/druid/client/broker/BrokerClientImpl.java\nindex 728a67401b2e..5ad609147429 100644\n--- a/server/src/main/java/org/apache/druid/client/broker/BrokerClientImpl.java\n+++ b/server/src/main/java/org/apache/druid/client/broker/BrokerClientImpl.java\n@@ -26,6 +26,8 @@\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.jackson.JacksonUtils;\n import org.apache.druid.java.util.http.client.response.BytesFullResponseHandler;\n+import org.apache.druid.java.util.http.client.response.FullResponseHolder;\n+import org.apache.druid.java.util.http.client.response.StringFullResponseHandler;\n import org.apache.druid.query.explain.ExplainPlan;\n import org.apache.druid.query.http.ClientSqlQuery;\n import org.apache.druid.query.http.SqlTaskStatus;\n@@ -33,6 +35,7 @@\n import org.apache.druid.rpc.ServiceClient;\n import org.jboss.netty.handler.codec.http.HttpMethod;\n \n+import java.nio.charset.StandardCharsets;\n import java.util.List;\n \n public class BrokerClientImpl implements BrokerClient\n@@ -46,6 +49,19 @@ public BrokerClientImpl(final ServiceClient client, final ObjectMapper jsonMappe\n     this.jsonMapper = jsonMapper;\n   }\n \n+  @Override\n+  public ListenableFuture<String> submitSqlQuery(final ClientSqlQuery sqlQuery)\n+  {\n+    return FutureUtils.transform(\n+        client.asyncRequest(\n+            new RequestBuilder(HttpMethod.POST, \""/druid/v2/sql/\"")\n+                    .jsonContent(jsonMapper, sqlQuery),\n+            new StringFullResponseHandler(StandardCharsets.UTF_8)\n+        ),\n+        FullResponseHolder::getContent\n+    );\n+  }\n+\n   @Override\n   public ListenableFuture<SqlTaskStatus> submitSqlTask(final ClientSqlQuery sqlQuery)\n   {\n\ndiff --git a/server/src/main/java/org/apache/druid/discovery/BrokerClient.java b/server/src/main/java/org/apache/druid/discovery/BrokerClient.java\ndeleted file mode 100644\nindex a0ddbf42bed8..000000000000\n--- a/server/src/main/java/org/apache/druid/discovery/BrokerClient.java\n+++ /dev/null\n@@ -1,127 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.druid.discovery;\n-\n-import com.google.inject.Inject;\n-import org.apache.druid.error.DruidException;\n-import org.apache.druid.guice.annotations.EscalatedGlobal;\n-import org.apache.druid.java.util.common.IOE;\n-import org.apache.druid.java.util.common.RetryUtils;\n-import org.apache.druid.java.util.common.StringUtils;\n-import org.apache.druid.java.util.http.client.HttpClient;\n-import org.apache.druid.java.util.http.client.Request;\n-import org.apache.druid.java.util.http.client.response.StringFullResponseHandler;\n-import org.apache.druid.java.util.http.client.response.StringFullResponseHolder;\n-import org.apache.druid.rpc.ServiceClient;\n-import org.jboss.netty.channel.ChannelException;\n-import org.jboss.netty.handler.codec.http.HttpMethod;\n-import org.jboss.netty.handler.codec.http.HttpResponseStatus;\n-\n-import java.io.IOException;\n-import java.net.MalformedURLException;\n-import java.net.URL;\n-import java.nio.charset.StandardCharsets;\n-import java.util.concurrent.ExecutionException;\n-\n-/**\n- * This class facilitates interaction with Broker.\n- * Note that this should be removed and reconciled with org.apache.druid.sql.client.BrokerClient, which has the\n- * built-in functionality of {@link ServiceClient}, and proper Guice and service discovery wired in.\n- */\n-@Deprecated\n-public class BrokerClient\n-{\n-  private static final int MAX_RETRIES = 5;\n-\n-  private final HttpClient brokerHttpClient;\n-  private final DruidNodeDiscovery druidNodeDiscovery;\n-\n-  @Inject\n-  public BrokerClient(\n-      @EscalatedGlobal HttpClient brokerHttpClient,\n-      DruidNodeDiscoveryProvider druidNodeDiscoveryProvider\n-  )\n-  {\n-    this.brokerHttpClient = brokerHttpClient;\n-    this.druidNodeDiscovery = druidNodeDiscoveryProvider.getForNodeRole(NodeRole.BROKER);\n-  }\n-\n-  /**\n-   * Creates and returns a {@link Request} after choosing a broker.\n-   */\n-  public Request makeRequest(HttpMethod httpMethod, String urlPath) throws IOException\n-  {\n-    String host = ClientUtils.pickOneHost(druidNodeDiscovery);\n-\n-    if (host == null) {\n-      throw DruidException.forPersona(DruidException.Persona.ADMIN)\n-                          .ofCategory(DruidException.Category.NOT_FOUND)\n-                          .build(\""A leader node could not be found for [%s] service. Check the logs to validate that service is healthy.\"", NodeRole.BROKER);\n-    }\n-    return new Request(httpMethod, new URL(StringUtils.format(\""%s%s\"", host, urlPath)));\n-  }\n-\n-  public String sendQuery(final Request request) throws Exception\n-  {\n-    return RetryUtils.retry(\n-        () -> {\n-          Request newRequestUrl = getNewRequestUrl(request);\n-          final StringFullResponseHolder fullResponseHolder = brokerHttpClient.go(newRequestUrl, new StringFullResponseHandler(StandardCharsets.UTF_8)).get();\n-\n-          HttpResponseStatus responseStatus = fullResponseHolder.getResponse().getStatus();\n-          if (HttpResponseStatus.SERVICE_UNAVAILABLE.equals(responseStatus)\n-              || HttpResponseStatus.GATEWAY_TIMEOUT.equals(responseStatus)) {\n-            throw DruidException.forPersona(DruidException.Persona.OPERATOR)\n-                                .ofCategory(DruidException.Category.RUNTIME_FAILURE)\n-                                .build(\""Request to broker failed due to failed response status: [%s]\"", responseStatus);\n-          }\n-          return fullResponseHolder.getContent();\n-        },\n-        (throwable) -> {\n-          if (throwable instanceof ExecutionException) {\n-            return throwable.getCause() instanceof IOException || throwable.getCause() instanceof ChannelException;\n-          }\n-          if (throwable instanceof DruidException) {\n-            return ((DruidException) throwable).getCategory() == DruidException.Category.RUNTIME_FAILURE;\n-          }\n-          return throwable instanceof IOE;\n-        },\n-        MAX_RETRIES\n-    );\n-  }\n-\n-  private Request getNewRequestUrl(Request oldRequest)\n-  {\n-    try {\n-      return ClientUtils.withUrl(\n-          oldRequest,\n-          new URL(StringUtils.format(\""%s%s\"", ClientUtils.pickOneHost(druidNodeDiscovery), oldRequest.getUrl().getPath()))\n-      );\n-    }\n-    catch (MalformedURLException e) {\n-      // Not an IOException; this is our own fault.\n-      throw DruidException.defensive(\n-          \""Failed to build url with path[%s] and query string [%s].\"",\n-          oldRequest.getUrl().getPath(),\n-          oldRequest.getUrl().getQuery()\n-      );\n-    }\n-  }\n-}\n"", ""test_patch"": ""diff --git a/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java b/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java\nindex 548a7ac473e9..fdebcc7a655f 100644\n--- a/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java\n+++ b/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/exec/SegmentLoadStatusFetcherTest.java\n@@ -20,9 +20,11 @@\n package org.apache.druid.msq.exec;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n-import org.apache.druid.discovery.BrokerClient;\n+import com.google.common.util.concurrent.Futures;\n+import com.google.common.util.concurrent.ListenableFuture;\n+import org.apache.druid.client.broker.BrokerClient;\n import org.apache.druid.java.util.common.Intervals;\n-import org.apache.druid.java.util.http.client.Request;\n+import org.apache.druid.query.http.ClientSqlQuery;\n import org.apache.druid.timeline.DataSegment;\n import org.apache.druid.timeline.partition.NumberedShardSpec;\n import org.junit.Assert;\n@@ -34,17 +36,18 @@\n import java.util.stream.IntStream;\n \n import static org.mockito.ArgumentMatchers.any;\n-import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.doAnswer;\n-import static org.mockito.Mockito.doReturn;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n public class SegmentLoadStatusFetcherTest\n {\n   private static final String TEST_DATASOURCE = \""testDatasource\"";\n \n+  private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n+\n   private SegmentLoadStatusFetcher segmentLoadWaiter;\n \n   private BrokerClient brokerClient;\n@@ -57,13 +60,14 @@ public void testSingleVersionWaitsForLoadCorrectly() throws Exception\n   {\n     brokerClient = mock(BrokerClient.class);\n \n-    doReturn(mock(Request.class)).when(brokerClient).makeRequest(any(), anyString());\n-    doAnswer(new Answer<String>()\n+    String dummyString = \""\"";\n+    when(brokerClient.submitSqlQuery(any(ClientSqlQuery.class))).thenReturn(Futures.immediateFuture(dummyString));\n+    doAnswer(new Answer<ListenableFuture<String>>()\n     {\n       int timesInvoked = 0;\n \n       @Override\n-      public String answer(InvocationOnMock invocation) throws Throwable\n+      public ListenableFuture<String> answer(InvocationOnMock invocation) throws Throwable\n       {\n         timesInvoked += 1;\n         SegmentLoadStatusFetcher.VersionLoadStatus loadStatus = new SegmentLoadStatusFetcher.VersionLoadStatus(\n@@ -73,12 +77,13 @@ public String answer(InvocationOnMock invocation) throws Throwable\n             5 - timesInvoked,\n             0\n         );\n-        return new ObjectMapper().writeValueAsString(loadStatus);\n+        String jsonResponse = OBJECT_MAPPER.writeValueAsString(loadStatus);\n+        return Futures.immediateFuture(jsonResponse);\n       }\n-    }).when(brokerClient).sendQuery(any());\n+    }).when(brokerClient).submitSqlQuery(any(ClientSqlQuery.class));\n     segmentLoadWaiter = new SegmentLoadStatusFetcher(\n         brokerClient,\n-        new ObjectMapper(),\n+        OBJECT_MAPPER,\n         \""id\"",\n         TEST_DATASOURCE,\n         IntStream.range(0, 5).boxed().map(partitionNum -> createTestDataSegment(\""version1\"", partitionNum)).collect(Collectors.toSet()),\n@@ -86,7 +91,7 @@ public String answer(InvocationOnMock invocation) throws Throwable\n     );\n     segmentLoadWaiter.waitForSegmentsToLoad();\n \n-    verify(brokerClient, times(5)).sendQuery(any());\n+    verify(brokerClient, times(5)).submitSqlQuery(any(ClientSqlQuery.class));\n   }\n \n   @Test\n@@ -94,13 +99,14 @@ public void testMultipleVersionWaitsForLoadCorrectly() throws Exception\n   {\n     brokerClient = mock(BrokerClient.class);\n \n-    doReturn(mock(Request.class)).when(brokerClient).makeRequest(any(), anyString());\n-    doAnswer(new Answer<String>()\n+    String dummyString = \""\"";\n+    when(brokerClient.submitSqlQuery(any(ClientSqlQuery.class))).thenReturn(Futures.immediateFuture(dummyString));\n+    when(brokerClient.submitSqlQuery(any(ClientSqlQuery.class))).thenAnswer(new Answer<ListenableFuture<String>>()\n     {\n       int timesInvoked = 0;\n \n       @Override\n-      public String answer(InvocationOnMock invocation) throws Throwable\n+      public ListenableFuture<String> answer(InvocationOnMock invocation) throws Throwable\n       {\n         timesInvoked += 1;\n         SegmentLoadStatusFetcher.VersionLoadStatus loadStatus = new SegmentLoadStatusFetcher.VersionLoadStatus(\n@@ -110,12 +116,13 @@ public String answer(InvocationOnMock invocation) throws Throwable\n             5 - timesInvoked,\n             0\n         );\n-        return new ObjectMapper().writeValueAsString(loadStatus);\n+        String jsonResponse = OBJECT_MAPPER.writeValueAsString(loadStatus);\n+        return Futures.immediateFuture(jsonResponse);\n       }\n-    }).when(brokerClient).sendQuery(any());\n+    });\n     segmentLoadWaiter = new SegmentLoadStatusFetcher(\n         brokerClient,\n-        new ObjectMapper(),\n+        OBJECT_MAPPER,\n         \""id\"",\n         TEST_DATASOURCE,\n         IntStream.range(0, 5).boxed().map(partitionNum -> createTestDataSegment(\""version1\"", partitionNum)).collect(Collectors.toSet()),\n@@ -123,22 +130,25 @@ public String answer(InvocationOnMock invocation) throws Throwable\n     );\n     segmentLoadWaiter.waitForSegmentsToLoad();\n \n-    verify(brokerClient, times(5)).sendQuery(any());\n+    verify(brokerClient, times(5)).submitSqlQuery(any(ClientSqlQuery.class));\n   }\n \n   @Test\n   public void triggerCancellationFromAnotherThread() throws Exception\n   {\n     brokerClient = mock(BrokerClient.class);\n-    doReturn(mock(Request.class)).when(brokerClient).makeRequest(any(), anyString());\n-    doAnswer(new Answer<String>()\n+\n+    String dummyString = \""\"";\n+    when(brokerClient.submitSqlQuery(any(ClientSqlQuery.class))).thenReturn(Futures.immediateFuture(dummyString));\n+\n+    doAnswer(new Answer<ListenableFuture<String>>()\n     {\n       int timesInvoked = 0;\n \n       @Override\n-      public String answer(InvocationOnMock invocation) throws Throwable\n+      public ListenableFuture<String> answer(InvocationOnMock invocation) throws Throwable\n       {\n-        // sleeping broker call to simulate a long running query\n+        // sleeping broker call to simulate a long-running query\n         Thread.sleep(1000);\n         timesInvoked++;\n         SegmentLoadStatusFetcher.VersionLoadStatus loadStatus = new SegmentLoadStatusFetcher.VersionLoadStatus(\n@@ -148,12 +158,13 @@ public String answer(InvocationOnMock invocation) throws Throwable\n             5 - timesInvoked,\n             0\n         );\n-        return new ObjectMapper().writeValueAsString(loadStatus);\n+        String jsonResponse = OBJECT_MAPPER.writeValueAsString(loadStatus);\n+        return Futures.immediateFuture(jsonResponse);\n       }\n-    }).when(brokerClient).sendQuery(any());\n+    }).when(brokerClient).submitSqlQuery(any(ClientSqlQuery.class));\n     segmentLoadWaiter = new SegmentLoadStatusFetcher(\n         brokerClient,\n-        new ObjectMapper(),\n+        OBJECT_MAPPER,\n         \""id\"",\n         TEST_DATASOURCE,\n         IntStream.range(0, 5).boxed().map(partitionNum -> createTestDataSegment(\""version1\"", partitionNum)).collect(Collectors.toSet()),\n\ndiff --git a/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java b/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java\nindex 951e08d35a3d..08efce8e9106 100644\n--- a/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java\n+++ b/extensions-core/multi-stage-query/src/test/java/org/apache/druid/msq/test/MSQTestBase.java\n@@ -43,7 +43,6 @@\n import org.apache.druid.data.input.impl.DimensionsSpec;\n import org.apache.druid.data.input.impl.LongDimensionSchema;\n import org.apache.druid.data.input.impl.StringDimensionSchema;\n-import org.apache.druid.discovery.BrokerClient;\n import org.apache.druid.discovery.NodeRole;\n import org.apache.druid.frame.channel.FrameChannelSequence;\n import org.apache.druid.frame.processor.Bouncer;\n@@ -75,7 +74,6 @@\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.java.util.emitter.EmittingLogger;\n-import org.apache.druid.java.util.http.client.Request;\n import org.apache.druid.metadata.input.InputSourceModule;\n import org.apache.druid.msq.counters.CounterNames;\n import org.apache.druid.msq.counters.CounterSnapshots;\n@@ -247,7 +245,7 @@\n import static org.mockito.Mockito.mock;\n \n /**\n- * Base test runner for running MSQ unit tests. It sets up multi stage query execution environment\n+ * Base test runner for running MSQ unit tests. It sets up multi-stage query execution environment\n  * and populates data for the datasources. The runner does not go via the HTTP layer for communication between the\n  * various MSQ processes.\n  * <p>\n@@ -437,7 +435,6 @@ public void setUp2() throws Exception\n \n     segmentManager = new MSQTestSegmentManager(segmentCacheManager);\n \n-    BrokerClient brokerClient = mock(BrokerClient.class);\n     List<Module> modules = ImmutableList.of(\n         binder -> {\n           DruidProcessingConfig druidProcessingConfig = new DruidProcessingConfig()\n@@ -537,7 +534,6 @@ public String getFormatString()\n         new LookylooModule(),\n         new SegmentWranglerModule(),\n         new HllSketchModule(),\n-        binder -> binder.bind(BrokerClient.class).toInstance(brokerClient),\n         binder -> binder.bind(Bouncer.class).toInstance(new Bouncer(1))\n     );\n     // adding node role injection to the modules, since CliPeon would also do that through run method\n@@ -551,8 +547,6 @@ public String getFormatString()\n     objectMapper.registerModules(sqlModule.getJacksonModules());\n     objectMapper.registerModules(BuiltInTypesModule.getJacksonModulesList());\n \n-    doReturn(mock(Request.class)).when(brokerClient).makeRequest(any(), anyString());\n-\n     testTaskActionClient = Mockito.spy(new MSQTestTaskActionClient(objectMapper, injector));\n     indexingServiceClient = new MSQTestOverlordServiceClient(\n         objectMapper,\n\ndiff --git a/server/src/test/java/org/apache/druid/discovery/BrokerClientTest.java b/server/src/test/java/org/apache/druid/discovery/BrokerClientTest.java\ndeleted file mode 100644\nindex de03877a9b0c..000000000000\n--- a/server/src/test/java/org/apache/druid/discovery/BrokerClientTest.java\n+++ /dev/null\n@@ -1,182 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.druid.discovery;\n-\n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.ImmutableMap;\n-import com.google.inject.Injector;\n-import com.google.inject.Key;\n-import com.google.inject.name.Names;\n-import org.apache.druid.guice.GuiceInjectors;\n-import org.apache.druid.guice.Jerseys;\n-import org.apache.druid.guice.JsonConfigProvider;\n-import org.apache.druid.guice.LazySingleton;\n-import org.apache.druid.guice.LifecycleModule;\n-import org.apache.druid.guice.annotations.Self;\n-import org.apache.druid.initialization.Initialization;\n-import org.apache.druid.java.util.http.client.HttpClient;\n-import org.apache.druid.java.util.http.client.Request;\n-import org.apache.druid.server.DruidNode;\n-import org.apache.druid.server.initialization.BaseJettyTest;\n-import org.apache.druid.server.initialization.jetty.JettyServerInitializer;\n-import org.easymock.EasyMock;\n-import org.eclipse.jetty.server.Server;\n-import org.jboss.netty.handler.codec.http.HttpMethod;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-import javax.ws.rs.POST;\n-import javax.ws.rs.Path;\n-import javax.ws.rs.Produces;\n-import javax.ws.rs.core.MediaType;\n-import javax.ws.rs.core.Response;\n-import java.nio.charset.StandardCharsets;\n-\n-public class BrokerClientTest extends BaseJettyTest\n-{\n-  private DiscoveryDruidNode discoveryDruidNode;\n-  private HttpClient httpClient;\n-\n-  @Override\n-  protected Injector setupInjector()\n-  {\n-    final DruidNode node = new DruidNode(\""test\"", \""localhost\"", false, null, null, true, false);\n-    discoveryDruidNode = new DiscoveryDruidNode(node, NodeRole.BROKER, ImmutableMap.of());\n-\n-    Injector injector = Initialization.makeInjectorWithModules(\n-        GuiceInjectors.makeStartupInjector(), ImmutableList.of(\n-            binder -> {\n-              JsonConfigProvider.bindInstance(\n-                  binder,\n-                  Key.get(DruidNode.class, Self.class),\n-                  node\n-              );\n-              binder.bind(Integer.class).annotatedWith(Names.named(\""port\"")).toInstance(node.getPlaintextPort());\n-              binder.bind(JettyServerInitializer.class).to(DruidLeaderClientTest.TestJettyServerInitializer.class).in(LazySingleton.class);\n-              Jerseys.addResource(binder, SimpleResource.class);\n-              LifecycleModule.register(binder, Server.class);\n-            }\n-        )\n-    );\n-    httpClient = injector.getInstance(BaseJettyTest.ClientHolder.class).getClient();\n-    return injector;\n-  }\n-\n-  @Test\n-  public void testSimple() throws Exception\n-  {\n-    DruidNodeDiscovery druidNodeDiscovery = EasyMock.createMock(DruidNodeDiscovery.class);\n-    EasyMock.expect(druidNodeDiscovery.getAllNodes()).andReturn(ImmutableList.of(discoveryDruidNode)).anyTimes();\n-\n-    DruidNodeDiscoveryProvider druidNodeDiscoveryProvider = EasyMock.createMock(DruidNodeDiscoveryProvider.class);\n-    EasyMock.expect(druidNodeDiscoveryProvider.getForNodeRole(NodeRole.BROKER)).andReturn(druidNodeDiscovery);\n-\n-    EasyMock.replay(druidNodeDiscovery, druidNodeDiscoveryProvider);\n-\n-    BrokerClient brokerClient = new BrokerClient(\n-        httpClient,\n-        druidNodeDiscoveryProvider\n-    );\n-\n-    Request request = brokerClient.makeRequest(HttpMethod.POST, \""/simple/direct\"");\n-    request.setContent(\""hello\"".getBytes(StandardCharsets.UTF_8));\n-    Assert.assertEquals(\""hello\"", brokerClient.sendQuery(request));\n-  }\n-\n-  @Test\n-  public void testRetryableError() throws Exception\n-  {\n-    DruidNodeDiscovery druidNodeDiscovery = EasyMock.createMock(DruidNodeDiscovery.class);\n-    EasyMock.expect(druidNodeDiscovery.getAllNodes()).andReturn(ImmutableList.of(discoveryDruidNode)).anyTimes();\n-\n-    DruidNodeDiscoveryProvider druidNodeDiscoveryProvider = EasyMock.createMock(DruidNodeDiscoveryProvider.class);\n-    EasyMock.expect(druidNodeDiscoveryProvider.getForNodeRole(NodeRole.BROKER)).andReturn(druidNodeDiscovery);\n-\n-    EasyMock.replay(druidNodeDiscovery, druidNodeDiscoveryProvider);\n-\n-    BrokerClient brokerClient = new BrokerClient(\n-        httpClient,\n-        druidNodeDiscoveryProvider\n-    );\n-\n-    Request request = brokerClient.makeRequest(HttpMethod.POST, \""/simple/flakey\"");\n-    request.setContent(\""hello\"".getBytes(StandardCharsets.UTF_8));\n-    Assert.assertEquals(\""hello\"", brokerClient.sendQuery(request));\n-  }\n-\n-  @Test\n-  public void testNonRetryableError() throws Exception\n-  {\n-    DruidNodeDiscovery druidNodeDiscovery = EasyMock.createMock(DruidNodeDiscovery.class);\n-    EasyMock.expect(druidNodeDiscovery.getAllNodes()).andReturn(ImmutableList.of(discoveryDruidNode)).anyTimes();\n-\n-    DruidNodeDiscoveryProvider druidNodeDiscoveryProvider = EasyMock.createMock(DruidNodeDiscoveryProvider.class);\n-    EasyMock.expect(druidNodeDiscoveryProvider.getForNodeRole(NodeRole.BROKER)).andReturn(druidNodeDiscovery);\n-\n-    EasyMock.replay(druidNodeDiscovery, druidNodeDiscoveryProvider);\n-\n-    BrokerClient brokerClient = new BrokerClient(\n-        httpClient,\n-        druidNodeDiscoveryProvider\n-    );\n-\n-    Request request = brokerClient.makeRequest(HttpMethod.POST, \""/simple/error\"");\n-    Assert.assertEquals(\""\"", brokerClient.sendQuery(request));\n-  }\n-\n-  @Path(\""/simple\"")\n-  public static class SimpleResource\n-  {\n-    private static int attempt = 0;\n-\n-    @POST\n-    @Path(\""/direct\"")\n-    @Produces(MediaType.APPLICATION_JSON)\n-    public Response direct(String input)\n-    {\n-      if (\""hello\"".equals(input)) {\n-        return Response.ok(\""hello\"").build();\n-      } else {\n-        return Response.serverError().build();\n-      }\n-    }\n-\n-    @POST\n-    @Path(\""/flakey\"")\n-    @Produces(MediaType.APPLICATION_JSON)\n-    public Response redirecting()\n-    {\n-      if (attempt > 2) {\n-        return Response.ok(\""hello\"").build();\n-      } else {\n-        attempt += 1;\n-        return Response.status(504).build();\n-      }\n-    }\n-\n-    @POST\n-    @Path(\""/error\"")\n-    @Produces(MediaType.APPLICATION_JSON)\n-    public Response error()\n-    {\n-      return Response.status(404).build();\n-    }\n-  }\n-}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__druid-17691"", ""pr_id"": 17691, ""issue_id"": 7943, ""repo"": ""apache/druid"", ""problem_statement"": ""Possible bug when loading multivalue+multipart String columns\n### Affected Version\r\n\r\n0.13.0 and likely later versions, not sure what the earliest affected version is\r\n\r\n### Description\r\n\r\nA user reported errors loading certain segments after upgrading from 0.11.0 -> 0.13.0: https://groups.google.com/forum/?pli=1#!topic/druid-user/m6IAMFLRrQM\r\n\r\nThe error and stack trace:\r\n\r\n```\r\n2019-06-12T17:42:46,230 ERROR [ZkCoordinator] org.apache.druid.server.coordination.SegmentLoadDropHandler - Failed to load segment for dataSource: {class=org.apache.druid.server.coordination.SegmentLoadDropHandler, exceptionType=class org.apache.druid.segment.loading.SegmentLoadingException, exceptionMessage=Exception loading segment[sapphire-stage-druid-metrics_2019-05-21T14:00:00.000Z_2019-05-21T15:00:00.000Z_2019-05-21T14:00:14.673Z], segment=DataSegment{size=7112133889, shardSpec=NumberedShardSpec{partitionNum=0, partitions=0}, metrics=[count, value_sum, value_min, value_max], dimensions=[feed, service, host, version, metric, dataSource, duration, hasFilters, id, interval, segment, type, clusterName, memKind, poolKind, poolName, bufferpoolName, gcGen, gcName, gcGenSpaceName, context, remoteAddress, success, server, taskId, taskType, tier, priority, taskStatus], version='2019-05-21T14:00:14.673Z', loadSpec={type=>hdfs, path=>hdfs://xxxxx/druid/sapphire-stage/data/sapphire-stage-druid-metrics/20190521T140000.000Z_20190521T150000.000Z/2019-05-21T14_00_14.673Z/0_index.zip}, interval=2019-05-21T14:00:00.000Z/2019-05-21T15:00:00.000Z, dataSource='sapphire-stage-druid-metrics', binaryVersion='9'}}\r\norg.apache.druid.segment.loading.SegmentLoadingException: Exception loading segment[sapphire-stage-druid-metrics_2019-05-21T14:00:00.000Z_2019-05-21T15:00:00.000Z_2019-05-21T14:00:14.673Z]\r\n       at org.apache.druid.server.coordination.SegmentLoadDropHandler.loadSegment(SegmentLoadDropHandler.java:265) ~[druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.coordination.SegmentLoadDropHandler.addSegment(SegmentLoadDropHandler.java:307) [druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.coordination.SegmentChangeRequestLoad.go(SegmentChangeRequestLoad.java:47) [druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.coordination.ZkCoordinator$1.childEvent(ZkCoordinator.java:118) [druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:520) [curator-recipes-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:514) [curator-recipes-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [curator-framework-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:296) [curator-client-4.0.0.jar:?]\r\n       at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [curator-framework-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:512) [curator-recipes-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-4.0.0.jar:4.0.0]\r\n       at org.apache.curator.framework.recipes.cache.PathChildrenCache$9.run(PathChildrenCache.java:771) [curator-recipes-4.0.0.jar:4.0.0]\r\n       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_73]\r\n       at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_73]\r\n       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_73]\r\n       at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_73]\r\n       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_73]\r\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_73]\r\n       at java.lang.Thread.run(Thread.java:745) [?:1.8.0_73]\r\nCaused by: org.apache.druid.java.util.common.IAE: use read(ByteBuffer buffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper) to read version 2 indexed.\r\n       at org.apache.druid.segment.data.GenericIndexed.read(GenericIndexed.java:131) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.data.CompressedVSizeColumnarIntsSupplier.fromByteBuffer(CompressedVSizeColumnarIntsSupplier.java:161) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.data.V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(V3CompressedVSizeColumnarMultiIntsSupplier.java:67) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.serde.DictionaryEncodedColumnPartSerde$1.readMultiValuedColumn(DictionaryEncodedColumnPartSerde.java:381) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.serde.DictionaryEncodedColumnPartSerde$1.read(DictionaryEncodedColumnPartSerde.java:309) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.column.ColumnDescriptor.read(ColumnDescriptor.java:106) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.IndexIO$V9IndexLoader.deserializeColumn(IndexIO.java:618) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.IndexIO$V9IndexLoader.load(IndexIO.java:593) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.IndexIO.loadIndex(IndexIO.java:187) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.loading.MMappedQueryableSegmentizerFactory.factorize(MMappedQueryableSegmentizerFactory.java:48) ~[druid-processing-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.segment.loading.SegmentLoaderLocalCacheManager.getSegment(SegmentLoaderLocalCacheManager.java:123) ~[druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.SegmentManager.getAdapter(SegmentManager.java:196) ~[druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.SegmentManager.loadSegment(SegmentManager.java:157) ~[druid-server-0.13.0.jar:0.13.0]\r\n       at org.apache.druid.server.coordination.SegmentLoadDropHandler.loadSegment(SegmentLoadDropHandler.java:261) ~[druid-server-0.13.0.jar:0.13.0]\r\n       ... 18 more\r\n```\r\n\r\nThe segment in question is quite large (7GB+): `DataSegment{size=7112133889,`\r\n\r\nFrom that, it looks like `CompressedVSizeColumnarIntsSupplier.fromByteBuffer` may need to handle the multipart column case and sometimes call `public static <T> GenericIndexed<T> read(ByteBuffer buffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper)` with a `SmooshedFileMapper`.\r\n\r\n```\r\n  public static CompressedVSizeColumnarIntsSupplier fromByteBuffer(\r\n      ByteBuffer buffer,\r\n      ByteOrder order\r\n  )\r\n  {\r\n    byte versionFromBuffer = buffer.get();\r\n\r\n    if (versionFromBuffer == VERSION) {\r\n      final int numBytes = buffer.get();\r\n      final int totalSize = buffer.getInt();\r\n      final int sizePer = buffer.getInt();\r\n\r\n      final CompressionStrategy compression = CompressionStrategy.forId(buffer.get());\r\n\r\n      return new CompressedVSizeColumnarIntsSupplier(\r\n          totalSize,\r\n          sizePer,\r\n          numBytes,\r\n          GenericIndexed.read(buffer, new DecompressingByteBufferObjectStrategy(order, compression)),\r\n          compression\r\n      );\r\n\r\n    }\r\n\r\n    throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\r\n  }\r\n```\r\n"", ""issue_word_count"": 984, ""test_files_count"": 21, ""non_test_files_count"": 38, ""pr_changed_files"": [""benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java"", ""extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalColumnPartSupplier.java"", ""extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalLongColumnSerializer.java"", ""extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalMetricSerde.java"", ""processing/src/main/java/org/apache/druid/segment/DictionaryEncodedColumnMerger.java"", ""processing/src/main/java/org/apache/druid/segment/IndexIO.java"", ""processing/src/main/java/org/apache/druid/segment/MetricHolder.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarDoublesSuppliers.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarFloatsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarLongsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/data/CompressionFactory.java"", ""processing/src/main/java/org/apache/druid/segment/data/GenericIndexed.java"", ""processing/src/main/java/org/apache/druid/segment/data/GenericIndexedWriter.java"", ""processing/src/main/java/org/apache/druid/segment/data/IntermediateColumnarLongsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializer.java"", ""processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/nested/CompressedNestedDataComplexColumn.java"", ""processing/src/main/java/org/apache/druid/segment/nested/ScalarDoubleColumnAndIndexSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/nested/ScalarLongColumnAndIndexSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/nested/ScalarStringColumnAndIndexSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/nested/VariantColumnAndIndexSupplier.java"", ""processing/src/main/java/org/apache/druid/segment/serde/DictionaryEncodedColumnPartSerde.java"", ""processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerde.java"", ""processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerdeV2.java"", ""processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerde.java"", ""processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerdeV2.java"", ""processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerde.java"", ""processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerdeV2.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java"", ""processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java""], ""pr_changed_test_files"": [""benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java"", ""benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java"", ""processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java"", ""processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java"", ""processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java""], ""base_commit"": ""f4912d1c6620caa40cbd02cbd8ef91fd68187a1b"", ""head_commit"": ""b23ed5081b3fe23aa8dc2a018d43187b91912841"", ""repo_url"": ""https://github.com/apache/druid/pull/17691"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17691"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-03T18:12:32.000Z"", ""patch"": ""diff --git a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalColumnPartSupplier.java b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalColumnPartSupplier.java\nindex c51fbc3384e2..66e5e9afac42 100644\n--- a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalColumnPartSupplier.java\n+++ b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalColumnPartSupplier.java\n@@ -22,6 +22,7 @@\n \n import com.google.common.base.Supplier;\n import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.IndexIO;\n import org.apache.druid.segment.column.ComplexColumn;\n import org.apache.druid.segment.data.CompressedVSizeColumnarIntsSupplier;\n@@ -40,10 +41,12 @@ public class CompressedBigDecimalColumnPartSupplier implements Supplier<ComplexC\n    * Compressed.\n    *\n    * @param buffer Byte buffer\n+   * @param smooshMapper mapper for secondary files, in case of large columns\n    * @return new instance of CompressedBigDecimalColumnPartSupplier\n    */\n   public static CompressedBigDecimalColumnPartSupplier fromByteBuffer(\n-      ByteBuffer buffer\n+      ByteBuffer buffer,\n+      SmooshedFileMapper smooshMapper\n   )\n   {\n     byte versionFromBuffer = buffer.get();\n@@ -53,11 +56,12 @@ public static CompressedBigDecimalColumnPartSupplier fromByteBuffer(\n \n       CompressedVSizeColumnarIntsSupplier scaleSupplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n           buffer,\n-          IndexIO.BYTE_ORDER\n+          IndexIO.BYTE_ORDER,\n+          smooshMapper\n       );\n \n       V3CompressedVSizeColumnarMultiIntsSupplier magnitudeSupplier =\n-          V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, IndexIO.BYTE_ORDER);\n+          V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, IndexIO.BYTE_ORDER, smooshMapper);\n \n       return new CompressedBigDecimalColumnPartSupplier(\n           buffer.position() - positionStart,\n\ndiff --git a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalLongColumnSerializer.java b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalLongColumnSerializer.java\nindex ef899c2f4557..f25c028d9047 100644\n--- a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalLongColumnSerializer.java\n+++ b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalLongColumnSerializer.java\n@@ -25,6 +25,7 @@\n import org.apache.druid.segment.data.ArrayBasedIndexedInts;\n import org.apache.druid.segment.data.CompressedVSizeColumnarIntsSerializer;\n import org.apache.druid.segment.data.CompressionStrategy;\n+import org.apache.druid.segment.data.GenericIndexedWriter;\n import org.apache.druid.segment.data.V3CompressedVSizeColumnarMultiIntsSerializer;\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n \n@@ -66,7 +67,8 @@ public static CompressedBigDecimalLongColumnSerializer create(\n             segmentWriteOutMedium,\n             String.format(Locale.ROOT, \""%s.magnitude\"", filenameBase),\n             Integer.MAX_VALUE,\n-            CompressionStrategy.LZ4\n+            CompressionStrategy.LZ4,\n+            GenericIndexedWriter.MAX_FILE_SIZE\n         )\n     );\n   }\n\ndiff --git a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalMetricSerde.java b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalMetricSerde.java\nindex d76896c83e09..749af8ff3803 100644\n--- a/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalMetricSerde.java\n+++ b/extensions-contrib/compressed-bigdecimal/src/main/java/org/apache/druid/compressedbigdecimal/CompressedBigDecimalMetricSerde.java\n@@ -74,7 +74,7 @@ public CompressedBigDecimal extractValue(InputRow inputRow, String metricName)\n   public void deserializeColumn(ByteBuffer buffer, ColumnBuilder builder)\n   {\n     builder.setComplexColumnSupplier(\n-        CompressedBigDecimalColumnPartSupplier.fromByteBuffer(buffer)\n+        CompressedBigDecimalColumnPartSupplier.fromByteBuffer(buffer, builder.getFileMapper())\n     );\n   }\n \n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/DictionaryEncodedColumnMerger.java b/processing/src/main/java/org/apache/druid/segment/DictionaryEncodedColumnMerger.java\nindex 6859715cf387..763d163d33fc 100644\n--- a/processing/src/main/java/org/apache/druid/segment/DictionaryEncodedColumnMerger.java\n+++ b/processing/src/main/java/org/apache/druid/segment/DictionaryEncodedColumnMerger.java\n@@ -461,7 +461,8 @@ protected void setupEncodedValueWriter() throws IOException\n             segmentWriteOutMedium,\n             filenameBase,\n             cardinality,\n-            compressionStrategy\n+            compressionStrategy,\n+            GenericIndexedWriter.MAX_FILE_SIZE\n         );\n       } else {\n         encodedValueSerializer =\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/IndexIO.java b/processing/src/main/java/org/apache/druid/segment/IndexIO.java\nindex a6ddad614064..93dae9179dc4 100644\n--- a/processing/src/main/java/org/apache/druid/segment/IndexIO.java\n+++ b/processing/src/main/java/org/apache/druid/segment/IndexIO.java\n@@ -357,7 +357,8 @@ public MMappedIndex mapDir(File inDir) throws IOException\n \n       CompressedColumnarLongsSupplier timestamps = CompressedColumnarLongsSupplier.fromByteBuffer(\n           smooshedFiles.mapFile(makeTimeFile(inDir, BYTE_ORDER).getName()),\n-          BYTE_ORDER\n+          BYTE_ORDER,\n+          smooshedFiles\n       );\n \n       Map<String, MetricHolder> metrics = Maps.newLinkedHashMap();\n@@ -385,7 +386,10 @@ public MMappedIndex mapDir(File inDir) throws IOException\n             fileDimensionName\n         );\n \n-        dimValueUtf8Lookups.put(dimension, GenericIndexed.read(dimBuffer, GenericIndexed.UTF8_STRATEGY));\n+        dimValueUtf8Lookups.put(\n+            dimension,\n+            GenericIndexed.read(dimBuffer, GenericIndexed.UTF8_STRATEGY, smooshedFiles)\n+        );\n         dimColumns.put(dimension, VSizeColumnarMultiInts.readFromByteBuffer(dimBuffer));\n       }\n \n@@ -393,7 +397,7 @@ public MMappedIndex mapDir(File inDir) throws IOException\n       for (int i = 0; i < availableDimensions.size(); ++i) {\n         bitmaps.put(\n             SERIALIZER_UTILS.readString(invertedBuffer),\n-            GenericIndexed.read(invertedBuffer, bitmapSerdeFactory.getObjectStrategy())\n+            GenericIndexed.read(invertedBuffer, bitmapSerdeFactory.getObjectStrategy(), smooshedFiles)\n         );\n       }\n \n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/MetricHolder.java b/processing/src/main/java/org/apache/druid/segment/MetricHolder.java\nindex 11d4b688712d..bcf6753fde52 100644\n--- a/processing/src/main/java/org/apache/druid/segment/MetricHolder.java\n+++ b/processing/src/main/java/org/apache/druid/segment/MetricHolder.java\n@@ -38,6 +38,9 @@ public class MetricHolder\n   private static final byte[] VERSION = new byte[]{0x0};\n   private static final SerializerUtils SERIALIZER_UTILS = new SerializerUtils();\n \n+  /**\n+   * Read a metric column from a legacy (v8) segment.\n+   */\n   public static MetricHolder fromByteBuffer(ByteBuffer buf)\n   {\n     final byte ver = buf.get();\n@@ -51,7 +54,11 @@ public static MetricHolder fromByteBuffer(ByteBuffer buf)\n \n     switch (holder.type) {\n       case FLOAT:\n-        holder.floatType = CompressedColumnarFloatsSupplier.fromByteBuffer(buf, ByteOrder.nativeOrder());\n+        holder.floatType = CompressedColumnarFloatsSupplier.fromByteBuffer(\n+            buf,\n+            ByteOrder.nativeOrder(),\n+            null // OK since this method is only used for legacy segments, which always use version 1 indexed\n+        );\n         break;\n       case COMPLEX:\n         final ComplexMetricSerde serdeForType = ComplexMetrics.getSerdeForType(holder.getTypeName());\n@@ -72,7 +79,7 @@ public static MetricHolder fromByteBuffer(ByteBuffer buf)\n \n   private static <T> GenericIndexed<T> read(ByteBuffer buf, ComplexMetricSerde serde)\n   {\n-    return GenericIndexed.read(buf, serde.getObjectStrategy());\n+    return GenericIndexed.read(buf, serde.getObjectStrategy(), null);\n   }\n \n   public enum MetricType\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSerializer.java\nindex 8c2dfb9c028b..6b323bf4b86a 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSerializer.java\n@@ -26,7 +26,6 @@\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n \n import javax.annotation.Nullable;\n-\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n@@ -57,6 +56,7 @@ public class BlockLayoutColumnarDoublesSerializer implements ColumnarDoublesSeri\n       String filenameBase,\n       ByteOrder byteOrder,\n       CompressionStrategy compression,\n+      int fileSizeLimit,\n       Closer closer\n   )\n   {\n@@ -66,6 +66,7 @@ public class BlockLayoutColumnarDoublesSerializer implements ColumnarDoublesSeri\n         filenameBase,\n         compression,\n         CompressedPools.BUFFER_SIZE,\n+        fileSizeLimit,\n         closer\n     );\n     this.compression = compression;\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSupplier.java\nindex 010e0b698577..4e97bc29497c 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarDoublesSupplier.java\n@@ -21,6 +21,7 @@\n \n import com.google.common.base.Supplier;\n import org.apache.druid.collections.ResourceHolder;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n \n import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n@@ -43,11 +44,16 @@ public BlockLayoutColumnarDoublesSupplier(\n       int sizePer,\n       ByteBuffer fromBuffer,\n       ByteOrder byteOrder,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      SmooshedFileMapper smooshMapper\n   )\n   {\n     this.strategy = strategy;\n-    this.baseDoubleBuffers = GenericIndexed.read(fromBuffer, DecompressingByteBufferObjectStrategy.of(byteOrder, strategy));\n+    this.baseDoubleBuffers = GenericIndexed.read(\n+        fromBuffer,\n+        DecompressingByteBufferObjectStrategy.of(byteOrder, strategy),\n+        smooshMapper\n+    );\n     this.totalSize = totalSize;\n     this.sizePer = sizePer;\n   }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSerializer.java\nindex 5640339a316e..c9c65b751f9e 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSerializer.java\n@@ -57,6 +57,7 @@ public class BlockLayoutColumnarFloatsSerializer implements ColumnarFloatsSerial\n       String filenameBase,\n       ByteOrder byteOrder,\n       CompressionStrategy compression,\n+      int fileSizeLimit,\n       Closer closer\n   )\n   {\n@@ -66,6 +67,7 @@ public class BlockLayoutColumnarFloatsSerializer implements ColumnarFloatsSerial\n         filenameBase,\n         compression,\n         CompressedPools.BUFFER_SIZE,\n+        fileSizeLimit,\n         closer\n     );\n     this.compression = compression;\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSupplier.java\nindex 383a99b3f473..1ce18fdbcd95 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarFloatsSupplier.java\n@@ -21,6 +21,7 @@\n \n import com.google.common.base.Supplier;\n import org.apache.druid.collections.ResourceHolder;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n \n import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n@@ -42,10 +43,15 @@ public BlockLayoutColumnarFloatsSupplier(\n       int sizePer,\n       ByteBuffer fromBuffer,\n       ByteOrder byteOrder,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      @Nullable SmooshedFileMapper smooshMapper\n   )\n   {\n-    baseFloatBuffers = GenericIndexed.read(fromBuffer, DecompressingByteBufferObjectStrategy.of(byteOrder, strategy));\n+    baseFloatBuffers = GenericIndexed.read(\n+        fromBuffer,\n+        DecompressingByteBufferObjectStrategy.of(byteOrder, strategy),\n+        smooshMapper\n+    );\n     this.totalSize = totalSize;\n     this.sizePer = sizePer;\n   }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java\nindex 37d468d62e49..a0a65343b75d 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSerializer.java\n@@ -60,6 +60,7 @@ public class BlockLayoutColumnarLongsSerializer implements ColumnarLongsSerializ\n       ByteOrder byteOrder,\n       CompressionFactory.LongEncodingWriter writer,\n       CompressionStrategy compression,\n+      int fileSizeLimit,\n       Closer closer\n   )\n   {\n@@ -71,6 +72,7 @@ public class BlockLayoutColumnarLongsSerializer implements ColumnarLongsSerializ\n         filenameBase,\n         compression,\n         bufferSize,\n+        fileSizeLimit,\n         closer\n     );\n     this.writer = writer;\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSupplier.java\nindex aa0346c6e34e..77714e18103e 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/BlockLayoutColumnarLongsSupplier.java\n@@ -22,6 +22,7 @@\n import com.google.common.base.Supplier;\n import org.apache.druid.collections.ResourceHolder;\n import org.apache.druid.common.semantic.SemanticUtils;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n \n import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n@@ -51,11 +52,16 @@ public BlockLayoutColumnarLongsSupplier(\n       ByteBuffer fromBuffer,\n       ByteOrder order,\n       CompressionFactory.LongEncodingReader reader,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      SmooshedFileMapper smooshMapper\n   )\n   {\n     this.strategy = strategy;\n-    this.baseLongBuffers = GenericIndexed.read(fromBuffer, DecompressingByteBufferObjectStrategy.of(order, strategy));\n+    this.baseLongBuffers = GenericIndexed.read(\n+        fromBuffer,\n+        DecompressingByteBufferObjectStrategy.of(order, strategy),\n+        smooshMapper\n+    );\n     this.totalSize = totalSize;\n     this.sizePer = sizePer;\n     this.baseReader = reader;\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarDoublesSuppliers.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarDoublesSuppliers.java\nindex 86443f942cea..17d4fdf034bf 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarDoublesSuppliers.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarDoublesSuppliers.java\n@@ -21,6 +21,7 @@\n \n import com.google.common.base.Supplier;\n import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n \n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n@@ -34,9 +35,19 @@ private CompressedColumnarDoublesSuppliers()\n   {\n   }\n \n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param buffer       primary buffer to read from\n+   * @param order        byte order\n+   * @param smooshMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                     a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n   public static Supplier<ColumnarDoubles> fromByteBuffer(\n       ByteBuffer buffer,\n-      ByteOrder order\n+      ByteOrder order,\n+      SmooshedFileMapper smooshMapper\n   )\n   {\n     byte versionFromBuffer = buffer.get();\n@@ -54,7 +65,8 @@ public static Supplier<ColumnarDoubles> fromByteBuffer(\n           sizePer,\n           buffer.asReadOnlyBuffer(),\n           order,\n-          compression\n+          compression,\n+          smooshMapper\n       );\n     }\n     throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarFloatsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarFloatsSupplier.java\nindex 64b77f07aed8..282dd7b68a5c 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarFloatsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarFloatsSupplier.java\n@@ -23,9 +23,11 @@\n import org.apache.druid.io.Channels;\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.serde.MetaSerdeHelper;\n import org.apache.druid.segment.serde.Serializer;\n \n+import javax.annotation.Nullable;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n@@ -82,7 +84,20 @@ public void writeTo(WritableByteChannel channel, FileSmoosher smoosher) throws I\n     Channels.writeFully(channel, buffer.asReadOnlyBuffer());\n   }\n \n-  public static CompressedColumnarFloatsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order)\n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param buffer       primary buffer to read from\n+   * @param order        byte order\n+   * @param smooshMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                     a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n+  public static CompressedColumnarFloatsSupplier fromByteBuffer(\n+      ByteBuffer buffer,\n+      ByteOrder order,\n+      @Nullable SmooshedFileMapper smooshMapper\n+  )\n   {\n     byte versionFromBuffer = buffer.get();\n \n@@ -99,7 +114,8 @@ public static CompressedColumnarFloatsSupplier fromByteBuffer(ByteBuffer buffer,\n           sizePer,\n           buffer.asReadOnlyBuffer(),\n           order,\n-          compression\n+          compression,\n+          smooshMapper\n       );\n       return new CompressedColumnarFloatsSupplier(\n           totalSize,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializer.java\nindex cc724ba1cc7a..519360cf3bf2 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializer.java\n@@ -59,6 +59,7 @@ public class CompressedColumnarIntsSerializer extends SingleValueColumnarIntsSer\n       final int chunkFactor,\n       final ByteOrder byteOrder,\n       final CompressionStrategy compression,\n+      final int fileSizeLimit,\n       final Closer closer\n   )\n   {\n@@ -72,6 +73,7 @@ public class CompressedColumnarIntsSerializer extends SingleValueColumnarIntsSer\n             filenameBase,\n             compression,\n             chunkFactor * Integer.BYTES,\n+            fileSizeLimit,\n             closer\n         ),\n         closer\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplier.java\nindex 22b477c019cc..a481e897d9e1 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplier.java\n@@ -114,25 +114,6 @@ GenericIndexed<ResourceHolder<ByteBuffer>> getBaseIntBuffers()\n     return baseIntBuffers;\n   }\n \n-  public static CompressedColumnarIntsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order)\n-  {\n-    byte versionFromBuffer = buffer.get();\n-\n-    if (versionFromBuffer == VERSION) {\n-      final int totalSize = buffer.getInt();\n-      final int sizePer = buffer.getInt();\n-      final CompressionStrategy compression = CompressionStrategy.forId(buffer.get());\n-      return new CompressedColumnarIntsSupplier(\n-          totalSize,\n-          sizePer,\n-          GenericIndexed.read(buffer, DecompressingByteBufferObjectStrategy.of(order, compression)),\n-          compression\n-      );\n-    }\n-\n-    throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\n-  }\n-\n   public static CompressedColumnarIntsSupplier fromByteBuffer(\n       ByteBuffer buffer,\n       ByteOrder order,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarLongsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarLongsSupplier.java\nindex 869e4495a32d..939b4e483925 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarLongsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedColumnarLongsSupplier.java\n@@ -23,9 +23,11 @@\n import org.apache.druid.io.Channels;\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.serde.MetaSerdeHelper;\n import org.apache.druid.segment.serde.Serializer;\n \n+import javax.annotation.Nullable;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n@@ -97,7 +99,20 @@ public void writeTo(WritableByteChannel channel, FileSmoosher smoosher) throws I\n     Channels.writeFully(channel, buffer.asReadOnlyBuffer());\n   }\n \n-  public static CompressedColumnarLongsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order)\n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param buffer       primary buffer to read from\n+   * @param order        byte order\n+   * @param smooshMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                     a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n+  public static CompressedColumnarLongsSupplier fromByteBuffer(\n+      ByteBuffer buffer,\n+      ByteOrder order,\n+      @Nullable SmooshedFileMapper smooshMapper\n+  )\n   {\n     byte versionFromBuffer = buffer.get();\n \n@@ -120,7 +135,8 @@ public static CompressedColumnarLongsSupplier fromByteBuffer(ByteBuffer buffer,\n           buffer.asReadOnlyBuffer(),\n           order,\n           encoding,\n-          compression\n+          compression,\n+          smooshMapper\n       );\n       return new CompressedColumnarLongsSupplier(\n           totalSize,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializer.java\nindex 84f4799e6d26..0bf216e1cb6e 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializer.java\n@@ -63,6 +63,7 @@ public static CompressedVSizeColumnarIntsSerializer create(\n         CompressedVSizeColumnarIntsSupplier.maxIntsInBufferForValue(maxValue),\n         IndexIO.BYTE_ORDER,\n         compression,\n+        GenericIndexedWriter.MAX_FILE_SIZE,\n         closer\n     );\n   }\n@@ -87,6 +88,7 @@ public static CompressedVSizeColumnarIntsSerializer create(\n       final int chunkFactor,\n       final ByteOrder byteOrder,\n       final CompressionStrategy compression,\n+      final int fileSizeLimit,\n       final Closer closer\n   )\n   {\n@@ -101,6 +103,7 @@ public static CompressedVSizeColumnarIntsSerializer create(\n             filenameBase,\n             compression,\n             sizePer(maxValue, chunkFactor),\n+            fileSizeLimit,\n             closer\n         ),\n         closer\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplier.java\nindex b02f4fd6c88b..bd541ea3aff7 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplier.java\n@@ -140,33 +140,6 @@ GenericIndexed<ResourceHolder<ByteBuffer>> getBaseBuffers()\n     return baseBuffers;\n   }\n \n-  public static CompressedVSizeColumnarIntsSupplier fromByteBuffer(\n-      ByteBuffer buffer,\n-      ByteOrder order\n-  )\n-  {\n-    byte versionFromBuffer = buffer.get();\n-\n-    if (versionFromBuffer == VERSION) {\n-      final int numBytes = buffer.get();\n-      final int totalSize = buffer.getInt();\n-      final int sizePer = buffer.getInt();\n-\n-      final CompressionStrategy compression = CompressionStrategy.forId(buffer.get());\n-\n-      return new CompressedVSizeColumnarIntsSupplier(\n-          totalSize,\n-          sizePer,\n-          numBytes,\n-          GenericIndexed.read(buffer, DecompressingByteBufferObjectStrategy.of(order, compression)),\n-          compression\n-      );\n-\n-    }\n-\n-    throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\n-  }\n-\n   public static CompressedVSizeColumnarIntsSupplier fromByteBuffer(\n       ByteBuffer buffer,\n       ByteOrder order,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplier.java\nindex bd4ab0694016..9ca4eb0cd8cc 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplier.java\n@@ -26,6 +26,7 @@\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.query.monomorphicprocessing.RuntimeShapeInspector;\n \n import java.io.IOException;\n@@ -78,18 +79,24 @@ public void writeTo(WritableByteChannel channel, FileSmoosher smoosher) throws I\n     valueSupplier.writeTo(channel, smoosher);\n   }\n \n-  public static CompressedVSizeColumnarMultiIntsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order)\n+  public static CompressedVSizeColumnarMultiIntsSupplier fromByteBuffer(\n+      ByteBuffer buffer,\n+      ByteOrder order,\n+      SmooshedFileMapper smooshMapper\n+  )\n   {\n     byte versionFromBuffer = buffer.get();\n \n     if (versionFromBuffer == VERSION) {\n       CompressedVSizeColumnarIntsSupplier offsetSupplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n           buffer,\n-          order\n+          order,\n+          smooshMapper\n       );\n       CompressedVSizeColumnarIntsSupplier valueSupplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n           buffer,\n-          order\n+          order,\n+          smooshMapper\n       );\n       return new CompressedVSizeColumnarMultiIntsSupplier(offsetSupplier, valueSupplier);\n     }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/CompressionFactory.java b/processing/src/main/java/org/apache/druid/segment/data/CompressionFactory.java\nindex 55d4c2d1f883..57dcea0db635 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/CompressionFactory.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/CompressionFactory.java\n@@ -25,10 +25,12 @@\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.serde.MetaSerdeHelper;\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.WriteOutBytes;\n \n+import javax.annotation.Nullable;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n@@ -306,13 +308,27 @@ public interface LongEncodingReader\n     LongEncodingStrategy getStrategy();\n   }\n \n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param totalSize      number of rows in the column\n+   * @param sizePer        number of values per compression buffer, for compressed columns\n+   * @param fromBuffer     primary buffer to read from\n+   * @param order          byte order\n+   * @param encodingFormat encoding of each long value\n+   * @param strategy       compression strategy, for compressed columns\n+   * @param smooshMapper   required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                       a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n   public static Supplier<ColumnarLongs> getLongSupplier(\n       int totalSize,\n       int sizePer,\n       ByteBuffer fromBuffer,\n       ByteOrder order,\n       LongEncodingFormat encodingFormat,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      @Nullable SmooshedFileMapper smooshMapper\n   )\n   {\n     if (strategy == CompressionStrategy.NONE) {\n@@ -324,7 +340,8 @@ public static Supplier<ColumnarLongs> getLongSupplier(\n           fromBuffer,\n           order,\n           encodingFormat.getReader(fromBuffer, order),\n-          strategy\n+          strategy,\n+          smooshMapper\n       );\n     }\n   }\n@@ -363,6 +380,7 @@ public static ColumnarLongsSerializer getLongSerializer(\n             order,\n             new LongsLongEncodingWriter(order),\n             compressionStrategy,\n+            GenericIndexedWriter.MAX_FILE_SIZE,\n             closer\n         );\n       }\n@@ -373,18 +391,31 @@ public static ColumnarLongsSerializer getLongSerializer(\n \n   // Float currently does not support any encoding types, and stores values as 4 byte float\n \n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param totalSize    number of rows in the column\n+   * @param sizePer      number of values per compression buffer, for compressed columns\n+   * @param fromBuffer   primary buffer to read from\n+   * @param order        byte order\n+   * @param strategy     compression strategy, for compressed columns\n+   * @param smooshMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                     a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n   public static Supplier<ColumnarFloats> getFloatSupplier(\n       int totalSize,\n       int sizePer,\n       ByteBuffer fromBuffer,\n       ByteOrder order,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      @Nullable SmooshedFileMapper smooshMapper\n   )\n   {\n     if (strategy == CompressionStrategy.NONE) {\n       return new EntireLayoutColumnarFloatsSupplier(totalSize, fromBuffer, order);\n     } else {\n-      return new BlockLayoutColumnarFloatsSupplier(totalSize, sizePer, fromBuffer, order, strategy);\n+      return new BlockLayoutColumnarFloatsSupplier(totalSize, sizePer, fromBuffer, order, strategy, smooshMapper);\n     }\n   }\n \n@@ -406,26 +437,45 @@ public static ColumnarFloatsSerializer getFloatSerializer(\n           filenameBase,\n           order,\n           compressionStrategy,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           closer\n       );\n     }\n   }\n \n+  /**\n+   * Reads a column from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param totalSize    number of rows in the column\n+   * @param sizePer      number of values per compression buffer, for compressed columns\n+   * @param fromBuffer   primary buffer to read from\n+   * @param byteOrder    byte order\n+   * @param strategy     compression strategy, for compressed columns\n+   * @param smooshMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                     a single-file column. Generally, this should only be null in tests, not production code.\n+   */\n   public static Supplier<ColumnarDoubles> getDoubleSupplier(\n       int totalSize,\n       int sizePer,\n       ByteBuffer fromBuffer,\n       ByteOrder byteOrder,\n-      CompressionStrategy strategy\n+      CompressionStrategy strategy,\n+      SmooshedFileMapper smooshMapper\n   )\n   {\n-    switch (strategy) {\n-      case NONE:\n-        return new EntireLayoutColumnarDoublesSupplier(totalSize, fromBuffer, byteOrder);\n-      default:\n-        return new BlockLayoutColumnarDoublesSupplier(totalSize, sizePer, fromBuffer, byteOrder, strategy);\n+    if (strategy == CompressionStrategy.NONE) {\n+      return new EntireLayoutColumnarDoublesSupplier(totalSize, fromBuffer, byteOrder);\n+    } else {\n+      return new BlockLayoutColumnarDoublesSupplier(\n+          totalSize,\n+          sizePer,\n+          fromBuffer,\n+          byteOrder,\n+          strategy,\n+          smooshMapper\n+      );\n     }\n-\n   }\n \n   public static ColumnarDoublesSerializer getDoubleSerializer(\n@@ -446,6 +496,7 @@ public static ColumnarDoublesSerializer getDoubleSerializer(\n           filenameBase,\n           byteOrder,\n           compression,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           closer\n       );\n     }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/GenericIndexed.java b/processing/src/main/java/org/apache/druid/segment/data/GenericIndexed.java\nindex 2c61d85a2ed1..419f24a0922a 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/GenericIndexed.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/GenericIndexed.java\n@@ -22,6 +22,7 @@\n import com.google.common.primitives.Ints;\n import org.apache.druid.collections.ResourceHolder;\n import org.apache.druid.common.utils.SerializerUtils;\n+import org.apache.druid.error.DruidException;\n import org.apache.druid.io.Channels;\n import org.apache.druid.java.util.common.ByteBufferUtils;\n import org.apache.druid.java.util.common.IAE;\n@@ -174,32 +175,36 @@ public boolean readRetainsBufferReference()\n     }\n   };\n \n-  public static <T> GenericIndexed<T> read(ByteBuffer buffer, ObjectStrategy<T> strategy)\n-  {\n-    byte versionFromBuffer = buffer.get();\n-\n-    if (VERSION_ONE == versionFromBuffer) {\n-      return createGenericIndexedVersionOne(buffer, strategy);\n-    } else if (VERSION_TWO == versionFromBuffer) {\n-      throw new IAE(\n-          \""use read(ByteBuffer buffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper)\""\n-          + \"" to read version 2 indexed.\""\n-      );\n-    }\n-    throw new IAE(\""Unknown version[%d]\"", (int) versionFromBuffer);\n-  }\n-\n-  public static <T> GenericIndexed<T> read(ByteBuffer buffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper)\n+  /**\n+   * Reads a GenericIndexed from a {@link ByteBuffer}, possibly using additional secondary files from a\n+   * {@link SmooshedFileMapper}.\n+   *\n+   * @param buffer     primary buffer to read from\n+   * @param strategy   deserialization strategy\n+   * @param fileMapper required for reading version 2 (multi-file) indexed. May be null if you know you are reading\n+   *                   a version 1 indexed.\n+   */\n+  public static <T> GenericIndexed<T> read(\n+      ByteBuffer buffer,\n+      ObjectStrategy<T> strategy,\n+      @Nullable SmooshedFileMapper fileMapper\n+  )\n   {\n     byte versionFromBuffer = buffer.get();\n \n     if (VERSION_ONE == versionFromBuffer) {\n       return createGenericIndexedVersionOne(buffer, strategy);\n     } else if (VERSION_TWO == versionFromBuffer) {\n+      if (fileMapper == null) {\n+        throw DruidException.defensive(\n+            \""use read(ByteBuffer buffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper)\""\n+            + \"" with non-null fileMapper to read version 2 indexed.\""\n+        );\n+      }\n       return createGenericIndexedVersionTwo(buffer, strategy, fileMapper);\n     }\n \n-    throw new IAE(\""Unknown version [%s]\"", versionFromBuffer);\n+    throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\n   }\n \n   public static <T> GenericIndexed<T> fromArray(T[] objects, ObjectStrategy<T> strategy)\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/GenericIndexedWriter.java b/processing/src/main/java/org/apache/druid/segment/data/GenericIndexedWriter.java\nindex ddc6bbe88767..524eab0b515e 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/GenericIndexedWriter.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/GenericIndexedWriter.java\n@@ -52,6 +52,7 @@\n public class GenericIndexedWriter<T> implements DictionaryWriter<T>\n {\n   private static final int PAGE_SIZE = 4096;\n+  public static final int MAX_FILE_SIZE = Integer.MAX_VALUE - PAGE_SIZE;\n \n   private static final MetaSerdeHelper<GenericIndexedWriter> SINGLE_FILE_META_SERDE_HELPER = MetaSerdeHelper\n       .firstWriteByte((GenericIndexedWriter x) -> GenericIndexed.VERSION_ONE)\n@@ -72,18 +73,31 @@ public class GenericIndexedWriter<T> implements DictionaryWriter<T>\n       .writeByteArray(x -> x.fileNameByteArray);\n \n \n+  /**\n+   * Creates a new writer that accepts byte buffers and compresses them.\n+   *\n+   * @param segmentWriteOutMedium supplier of temporary files\n+   * @param filenameBase          base filename to be used for secondary files, if multiple files are needed\n+   * @param compressionStrategy   compression strategy to apply\n+   * @param bufferSize            size of the buffers that will be passed in\n+   * @param fileSizeLimit         limit for files created by the writer. In production code, this should always be\n+   *                              {@link GenericIndexedWriter#MAX_FILE_SIZE}. The parameter is exposed only for testing.\n+   * @param closer                closer to attach temporary compression buffers to\n+   */\n   public static GenericIndexedWriter<ByteBuffer> ofCompressedByteBuffers(\n       final SegmentWriteOutMedium segmentWriteOutMedium,\n       final String filenameBase,\n       final CompressionStrategy compressionStrategy,\n       final int bufferSize,\n+      final int fileSizeLimit,\n       final Closer closer\n   )\n   {\n     GenericIndexedWriter<ByteBuffer> writer = new GenericIndexedWriter<>(\n         segmentWriteOutMedium,\n         filenameBase,\n-        compressedByteBuffersWriteObjectStrategy(compressionStrategy, bufferSize, closer)\n+        compressedByteBuffersWriteObjectStrategy(compressionStrategy, bufferSize, closer),\n+        fileSizeLimit\n     );\n     writer.objectsSorted = false;\n     return writer;\n@@ -169,7 +183,7 @@ public GenericIndexedWriter(\n       ObjectStrategy<T> strategy\n   )\n   {\n-    this(segmentWriteOutMedium, filenameBase, strategy, Integer.MAX_VALUE & ~PAGE_SIZE);\n+    this(segmentWriteOutMedium, filenameBase, strategy, MAX_FILE_SIZE);\n   }\n \n   public GenericIndexedWriter(\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/IntermediateColumnarLongsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/IntermediateColumnarLongsSerializer.java\nindex 7403f8dfd20b..ae8cb15cee80 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/IntermediateColumnarLongsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/IntermediateColumnarLongsSerializer.java\n@@ -145,6 +145,7 @@ private void makeDelegate() throws IOException\n           order,\n           writer,\n           compression,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           closer\n       );\n     }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializer.java b/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializer.java\nindex 0fac36399d1d..4c882d88f889 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializer.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializer.java\n@@ -35,12 +35,24 @@ public class V3CompressedVSizeColumnarMultiIntsSerializer extends ColumnarMultiI\n {\n   private static final byte VERSION = V3CompressedVSizeColumnarMultiIntsSupplier.VERSION;\n \n+  /**\n+   * Creates a new serializer.\n+   *\n+   * @param columnName            name of the column to write\n+   * @param segmentWriteOutMedium supplier of temporary files\n+   * @param filenameBase          base filename to be used for secondary files, if multiple files are needed\n+   * @param maxValue              maximum integer value that will be written to the column\n+   * @param compression           compression strategy to apply\n+   * @param fileSizeLimit         limit for files created by the writer. In production code, this should always be\n+   *                              {@link GenericIndexedWriter#MAX_FILE_SIZE}. The parameter is exposed only for testing.\n+   */\n   public static V3CompressedVSizeColumnarMultiIntsSerializer create(\n       final String columnName,\n       final SegmentWriteOutMedium segmentWriteOutMedium,\n       final String filenameBase,\n       final int maxValue,\n-      final CompressionStrategy compression\n+      final CompressionStrategy compression,\n+      final int fileSizeLimit\n   )\n   {\n     return new V3CompressedVSizeColumnarMultiIntsSerializer(\n@@ -48,20 +60,22 @@ public static V3CompressedVSizeColumnarMultiIntsSerializer create(\n         new CompressedColumnarIntsSerializer(\n             columnName,\n             segmentWriteOutMedium,\n-            filenameBase,\n+            filenameBase + \"".offsets\"",\n             CompressedColumnarIntsSupplier.MAX_INTS_IN_BUFFER,\n             IndexIO.BYTE_ORDER,\n             compression,\n+            fileSizeLimit,\n             segmentWriteOutMedium.getCloser()\n         ),\n         new CompressedVSizeColumnarIntsSerializer(\n             columnName,\n             segmentWriteOutMedium,\n-            filenameBase,\n+            filenameBase + \"".values\"",\n             maxValue,\n             CompressedVSizeColumnarIntsSupplier.maxIntsInBufferForValue(maxValue),\n             IndexIO.BYTE_ORDER,\n             compression,\n+            fileSizeLimit,\n             segmentWriteOutMedium.getCloser()\n         )\n     );\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplier.java b/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplier.java\nindex 3bb934cd296c..c1d6d82f407a 100644\n--- a/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplier.java\n@@ -56,24 +56,6 @@ private V3CompressedVSizeColumnarMultiIntsSupplier(\n     this.valueSupplier = valueSupplier;\n   }\n \n-  public static V3CompressedVSizeColumnarMultiIntsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order)\n-  {\n-    byte versionFromBuffer = buffer.get();\n-\n-    if (versionFromBuffer == VERSION) {\n-      CompressedColumnarIntsSupplier offsetSupplier = CompressedColumnarIntsSupplier.fromByteBuffer(\n-          buffer,\n-          order\n-      );\n-      CompressedVSizeColumnarIntsSupplier valueSupplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n-          buffer,\n-          order\n-      );\n-      return new V3CompressedVSizeColumnarMultiIntsSupplier(offsetSupplier, valueSupplier);\n-    }\n-    throw new IAE(\""Unknown version[%s]\"", versionFromBuffer);\n-  }\n-\n   public static V3CompressedVSizeColumnarMultiIntsSupplier fromByteBuffer(ByteBuffer buffer, ByteOrder order, SmooshedFileMapper mapper)\n   {\n     byte versionFromBuffer = buffer.get();\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/nested/CompressedNestedDataComplexColumn.java b/processing/src/main/java/org/apache/druid/segment/nested/CompressedNestedDataComplexColumn.java\nindex 311e72cdcfe7..5913425cc1a4 100644\n--- a/processing/src/main/java/org/apache/druid/segment/nested/CompressedNestedDataComplexColumn.java\n+++ b/processing/src/main/java/org/apache/druid/segment/nested/CompressedNestedDataComplexColumn.java\n@@ -968,18 +968,20 @@ private ColumnHolder readNestedFieldColumn(String field, int fieldIndex)\n       int pos = dataBuffer.position();\n       final Supplier<ColumnarLongs> longs = longsLength > 0 ? CompressedColumnarLongsSupplier.fromByteBuffer(\n           dataBuffer,\n-          byteOrder\n+          byteOrder,\n+          columnBuilder.getFileMapper()\n       ) : () -> null;\n       dataBuffer.position(pos + longsLength);\n       pos = dataBuffer.position();\n       final Supplier<ColumnarDoubles> doubles = doublesLength > 0 ? CompressedColumnarDoublesSuppliers.fromByteBuffer(\n           dataBuffer,\n-          byteOrder\n+          byteOrder,\n+          columnBuilder.getFileMapper()\n       ) : () -> null;\n       dataBuffer.position(pos + doublesLength);\n       final WritableSupplier<ColumnarInts> ints;\n       if (version == DictionaryEncodedColumnPartSerde.VERSION.COMPRESSED) {\n-        ints = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(dataBuffer, byteOrder);\n+        ints = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(dataBuffer, byteOrder, columnBuilder.getFileMapper());\n       } else {\n         ints = VSizeColumnarInts.readFromByteBuffer(dataBuffer);\n       }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/nested/ScalarDoubleColumnAndIndexSupplier.java b/processing/src/main/java/org/apache/druid/segment/nested/ScalarDoubleColumnAndIndexSupplier.java\nindex 6f7e2f8a2905..2723effb6ae6 100644\n--- a/processing/src/main/java/org/apache/druid/segment/nested/ScalarDoubleColumnAndIndexSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/nested/ScalarDoubleColumnAndIndexSupplier.java\n@@ -142,7 +142,8 @@ public static ScalarDoubleColumnAndIndexSupplier read(\n \n         final Supplier<ColumnarDoubles> doubles = CompressedColumnarDoublesSuppliers.fromByteBuffer(\n             doublesValueColumn,\n-            byteOrder\n+            byteOrder,\n+            columnBuilder.getFileMapper()\n         );\n         final ByteBuffer valueIndexBuffer = NestedCommonFormatColumnPartSerde.loadInternalFile(\n             mapper,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/nested/ScalarLongColumnAndIndexSupplier.java b/processing/src/main/java/org/apache/druid/segment/nested/ScalarLongColumnAndIndexSupplier.java\nindex 063dc2bfb9c0..5cca18e4323a 100644\n--- a/processing/src/main/java/org/apache/druid/segment/nested/ScalarLongColumnAndIndexSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/nested/ScalarLongColumnAndIndexSupplier.java\n@@ -151,7 +151,8 @@ public static ScalarLongColumnAndIndexSupplier read(\n \n         final Supplier<ColumnarLongs> longs = CompressedColumnarLongsSupplier.fromByteBuffer(\n             longsValueColumn,\n-            byteOrder\n+            byteOrder,\n+            columnBuilder.getFileMapper()\n         );\n         return new ScalarLongColumnAndIndexSupplier(\n             longDictionarySupplier,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/nested/ScalarStringColumnAndIndexSupplier.java b/processing/src/main/java/org/apache/druid/segment/nested/ScalarStringColumnAndIndexSupplier.java\nindex ee7fe1475b73..386c6fba7852 100644\n--- a/processing/src/main/java/org/apache/druid/segment/nested/ScalarStringColumnAndIndexSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/nested/ScalarStringColumnAndIndexSupplier.java\n@@ -85,7 +85,8 @@ public static ScalarStringColumnAndIndexSupplier read(\n         );\n         final CompressedVSizeColumnarIntsSupplier ints = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n             encodedValueColumn,\n-            byteOrder\n+            byteOrder,\n+            columnBuilder.getFileMapper()\n         );\n         final ByteBuffer valueIndexBuffer = NestedCommonFormatColumnPartSerde.loadInternalFile(\n             mapper,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/nested/VariantColumnAndIndexSupplier.java b/processing/src/main/java/org/apache/druid/segment/nested/VariantColumnAndIndexSupplier.java\nindex 6a2ec4769762..d3254c536f4a 100644\n--- a/processing/src/main/java/org/apache/druid/segment/nested/VariantColumnAndIndexSupplier.java\n+++ b/processing/src/main/java/org/apache/druid/segment/nested/VariantColumnAndIndexSupplier.java\n@@ -163,7 +163,8 @@ public static VariantColumnAndIndexSupplier read(\n         );\n         final CompressedVSizeColumnarIntsSupplier ints = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n             encodedValueColumn,\n-            byteOrder\n+            byteOrder,\n+            fileMapper\n         );\n         final ByteBuffer valueIndexBuffer = NestedCommonFormatColumnPartSerde.loadInternalFile(\n             fileMapper,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/DictionaryEncodedColumnPartSerde.java b/processing/src/main/java/org/apache/druid/segment/serde/DictionaryEncodedColumnPartSerde.java\nindex 02e7f5b4d397..4dd4de0e42af 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/DictionaryEncodedColumnPartSerde.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/DictionaryEncodedColumnPartSerde.java\n@@ -29,6 +29,7 @@\n import org.apache.druid.io.Channels;\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n import org.apache.druid.segment.column.BaseColumn;\n import org.apache.druid.segment.column.ColumnBuilder;\n import org.apache.druid.segment.column.ColumnConfig;\n@@ -332,10 +333,10 @@ public void read(\n         final WritableSupplier<ColumnarMultiInts> rMultiValuedColumn;\n \n         if (hasMultipleValues) {\n-          rMultiValuedColumn = readMultiValuedColumn(rVersion, buffer, rFlags);\n+          rMultiValuedColumn = readMultiValuedColumn(rVersion, buffer, rFlags, builder.getFileMapper());\n           rSingleValuedColumn = null;\n         } else {\n-          rSingleValuedColumn = readSingleValuedColumn(rVersion, buffer);\n+          rSingleValuedColumn = readSingleValuedColumn(rVersion, buffer, builder.getFileMapper());\n           rMultiValuedColumn = null;\n         }\n \n@@ -381,20 +382,29 @@ public void read(\n         }\n       }\n \n-      private WritableSupplier<ColumnarInts> readSingleValuedColumn(VERSION version, ByteBuffer buffer)\n+      private WritableSupplier<ColumnarInts> readSingleValuedColumn(\n+          VERSION version,\n+          ByteBuffer buffer,\n+          SmooshedFileMapper smooshReader\n+      )\n       {\n         switch (version) {\n           case UNCOMPRESSED_SINGLE_VALUE:\n           case UNCOMPRESSED_WITH_FLAGS:\n             return VSizeColumnarInts.readFromByteBuffer(buffer);\n           case COMPRESSED:\n-            return CompressedVSizeColumnarIntsSupplier.fromByteBuffer(buffer, byteOrder);\n+            return CompressedVSizeColumnarIntsSupplier.fromByteBuffer(buffer, byteOrder, smooshReader);\n           default:\n             throw new IAE(\""Unsupported single-value version[%s]\"", version);\n         }\n       }\n \n-      private WritableSupplier<ColumnarMultiInts> readMultiValuedColumn(VERSION version, ByteBuffer buffer, int flags)\n+      private WritableSupplier<ColumnarMultiInts> readMultiValuedColumn(\n+          VERSION version,\n+          ByteBuffer buffer,\n+          int flags,\n+          SmooshedFileMapper smooshReader\n+      )\n       {\n         switch (version) {\n           case UNCOMPRESSED_MULTI_VALUE: {\n@@ -409,9 +419,9 @@ private WritableSupplier<ColumnarMultiInts> readMultiValuedColumn(VERSION versio\n           }\n           case COMPRESSED: {\n             if (Feature.MULTI_VALUE.isSet(flags)) {\n-              return CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, byteOrder);\n+              return CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, byteOrder, smooshReader);\n             } else if (Feature.MULTI_VALUE_V3.isSet(flags)) {\n-              return V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, byteOrder);\n+              return V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(buffer, byteOrder, smooshReader);\n             } else {\n               throw new IAE(\""Unrecognized multi-value flag[%d] for version[%s]\"", flags, version);\n             }\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerde.java b/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerde.java\nindex 012b3ed77b05..a1611e4a73c3 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerde.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerde.java\n@@ -99,7 +99,8 @@ public Deserializer getDeserializer()\n     return (buffer, builder, columnConfig, parent) -> {\n       final Supplier<ColumnarDoubles> column = CompressedColumnarDoublesSuppliers.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n       DoubleNumericColumnSupplier columnSupplier = new DoubleNumericColumnSupplier(\n           column,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerdeV2.java b/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerdeV2.java\nindex 520249b923e8..dd3dbdf37b18 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerdeV2.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/DoubleNumericColumnPartSerdeV2.java\n@@ -147,7 +147,8 @@ public Deserializer getDeserializer()\n       int initialPos = buffer.position();\n       final Supplier<ColumnarDoubles> column = CompressedColumnarDoublesSuppliers.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n \n       buffer.position(initialPos + offset);\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerde.java b/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerde.java\nindex 441f774c7d17..e8f1e6c73dac 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerde.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerde.java\n@@ -99,7 +99,8 @@ public Deserializer getDeserializer()\n     return (buffer, builder, columnConfig, parent) -> {\n       final CompressedColumnarFloatsSupplier column = CompressedColumnarFloatsSupplier.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n       FloatNumericColumnSupplier columnSupplier = new FloatNumericColumnSupplier(\n           column,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerdeV2.java b/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerdeV2.java\nindex 116d2dec9b09..d79f0b1d11cf 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerdeV2.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/FloatNumericColumnPartSerdeV2.java\n@@ -145,7 +145,8 @@ public Deserializer getDeserializer()\n       int initialPos = buffer.position();\n       final CompressedColumnarFloatsSupplier column = CompressedColumnarFloatsSupplier.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n       buffer.position(initialPos + offset);\n       final ImmutableBitmap bitmap;\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerde.java b/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerde.java\nindex fa94b91ecd4e..be1050abc014 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerde.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerde.java\n@@ -99,7 +99,8 @@ public Deserializer getDeserializer()\n     return (buffer, builder, columnConfig, parent) -> {\n       final CompressedColumnarLongsSupplier column = CompressedColumnarLongsSupplier.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n       LongNumericColumnSupplier columnSupplier = new LongNumericColumnSupplier(\n           column,\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerdeV2.java b/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerdeV2.java\nindex 272670e88ef8..7c2e65478cf8 100644\n--- a/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerdeV2.java\n+++ b/processing/src/main/java/org/apache/druid/segment/serde/LongNumericColumnPartSerdeV2.java\n@@ -147,7 +147,8 @@ public Deserializer getDeserializer()\n       int initialPos = buffer.position();\n       final CompressedColumnarLongsSupplier column = CompressedColumnarLongsSupplier.fromByteBuffer(\n           buffer,\n-          byteOrder\n+          byteOrder,\n+          builder.getFileMapper()\n       );\n       buffer.position(initialPos + offset);\n       final ImmutableBitmap bitmap;\n"", ""test_patch"": ""diff --git a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java\nindex 1a6fc81e4eb8..bd203ea6fcb1 100644\n--- a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java\n+++ b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/BaseColumnarLongsBenchmark.java\n@@ -318,7 +318,7 @@ static ColumnarLongs createColumnarLongs(String encoding, ByteBuffer buffer)\n       case \""none-longs\"":\n       case \""zstd-auto\"":\n       case \""zstd-longs\"":\n-        return CompressedColumnarLongsSupplier.fromByteBuffer(buffer, ByteOrder.LITTLE_ENDIAN).get();\n+        return CompressedColumnarLongsSupplier.fromByteBuffer(buffer, ByteOrder.LITTLE_ENDIAN, null).get();\n     }\n \n     throw new IllegalArgumentException(\""unknown encoding\"");\n\ndiff --git a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java\nindex 5db092ba02ab..d132d781a966 100644\n--- a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java\n+++ b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedColumnarIntsBenchmark.java\n@@ -82,7 +82,8 @@ public void setup() throws IOException\n     );\n     this.compressed = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n         bufferCompressed,\n-        ByteOrder.nativeOrder()\n+        ByteOrder.nativeOrder(),\n+        null\n     ).get();\n \n     final ByteBuffer bufferUncompressed = serialize(VSizeColumnarInts.fromArray(vals));\n\ndiff --git a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java\nindex 87665ab9597b..e77c1f8fc7a2 100644\n--- a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java\n+++ b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/CompressedVSizeColumnarMultiIntsBenchmark.java\n@@ -95,7 +95,8 @@ public void setup() throws IOException\n     );\n     this.compressed = CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n         bufferCompressed,\n-        ByteOrder.nativeOrder()\n+        ByteOrder.nativeOrder(),\n+        null\n     ).get();\n \n     final ByteBuffer bufferUncompressed = serialize(\n\ndiff --git a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java\nindex c74415c5a43d..45f61e8d959b 100644\n--- a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java\n+++ b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/FloatCompressionBenchmark.java\n@@ -75,7 +75,7 @@ public void setup() throws Exception\n     File compFile = new File(dir, file + \""-\"" + strategy);\n     bufferHandler = FileUtils.map(compFile);\n     ByteBuffer buffer = bufferHandler.get();\n-    supplier = CompressedColumnarFloatsSupplier.fromByteBuffer(buffer, ByteOrder.nativeOrder());\n+    supplier = CompressedColumnarFloatsSupplier.fromByteBuffer(buffer, ByteOrder.nativeOrder(), null);\n   }\n \n   @TearDown\n\ndiff --git a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java\nindex d327eaf91a05..d846c13b209c 100644\n--- a/benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java\n+++ b/benchmarks/src/test/java/org/apache/druid/benchmark/compression/LongCompressionBenchmark.java\n@@ -79,7 +79,7 @@ public void setup() throws Exception\n     File compFile = new File(dir, file + \""-\"" + strategy + \""-\"" + format);\n     bufferHandler = FileUtils.map(compFile);\n     ByteBuffer buffer = bufferHandler.get();\n-    supplier = CompressedColumnarLongsSupplier.fromByteBuffer(buffer, ByteOrder.nativeOrder());\n+    supplier = CompressedColumnarLongsSupplier.fromByteBuffer(buffer, ByteOrder.nativeOrder(), null);\n   }\n \n   @TearDown\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java\nindex 2d213eddb5f8..950c007e5c28 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSerializerTest.java\n@@ -20,6 +20,7 @@\n package org.apache.druid.segment.data;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Supplier;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import it.unimi.dsi.fastutil.ints.IntArrayList;\n@@ -35,6 +36,8 @@\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.segment.writeout.WriteOutBytes;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -150,6 +153,60 @@ public void testMultiValueFileLargeData() throws Exception\n     }\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final CompressedColumnarIntsSerializer serializer = new CompressedColumnarIntsSerializer(\n+          columnName,\n+          segmentWriteOutMedium,\n+          columnName,\n+          CompressedColumnarIntsSupplier.MAX_INTS_IN_BUFFER,\n+          byteOrder,\n+          compressionStrategy,\n+          fileSizeLimit,\n+          segmentWriteOutMedium.getCloser()\n+      );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.addValue(random.nextInt() ^ Integer.MIN_VALUE);\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the column actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final Supplier<ColumnarInts> columnSupplier = CompressedColumnarIntsSupplier.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          byteOrder,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarInts column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n+\n   // this test takes ~30 minutes to run\n   @Ignore\n   @Test\n@@ -168,6 +225,7 @@ public void testTooManyValues() throws IOException\n           CompressedColumnarIntsSupplier.MAX_INTS_IN_BUFFER,\n           byteOrder,\n           compressionStrategy,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       serializer.open();\n@@ -198,6 +256,7 @@ private void checkSerializedSizeAndData(int chunkFactor) throws Exception\n         chunkFactor,\n         byteOrder,\n         compressionStrategy,\n+        GenericIndexedWriter.MAX_FILE_SIZE,\n         segmentWriteOutMedium.getCloser()\n     );\n     CompressedColumnarIntsSupplier supplierFromList = CompressedColumnarIntsSupplier.fromList(\n@@ -221,7 +280,8 @@ private void checkSerializedSizeAndData(int chunkFactor) throws Exception\n     // read from ByteBuffer and check values\n     CompressedColumnarIntsSupplier supplierFromByteBuffer = CompressedColumnarIntsSupplier.fromByteBuffer(\n         ByteBuffer.wrap(IOUtils.toByteArray(writeOutBytes.asInputStream())),\n-        byteOrder\n+        byteOrder,\n+        null\n     );\n     ColumnarInts columnarInts = supplierFromByteBuffer.get();\n     Assert.assertEquals(vals.length, columnarInts.size());\n@@ -247,6 +307,7 @@ private void checkV2SerializedSizeAndData(int chunkFactor) throws Exception\n             \""test\"",\n             compressionStrategy,\n             Long.BYTES * 10000,\n+            GenericIndexedWriter.MAX_FILE_SIZE,\n             segmentWriteOutMedium.getCloser()\n         ),\n         segmentWriteOutMedium.getCloser()\n@@ -264,7 +325,8 @@ private void checkV2SerializedSizeAndData(int chunkFactor) throws Exception\n     // read from ByteBuffer and check values\n     CompressedColumnarIntsSupplier supplierFromByteBuffer = CompressedColumnarIntsSupplier.fromByteBuffer(\n         mapper.mapFile(\""test\""),\n-        byteOrder\n+        byteOrder,\n+        null\n     );\n     ColumnarInts columnarInts = supplierFromByteBuffer.get();\n     Assert.assertEquals(vals.length, columnarInts.size());\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java\nindex 214da00365bc..25ad99dbd12a 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedColumnarIntsSupplierTest.java\n@@ -109,7 +109,7 @@ private void makeWithSerde(final int chunkSize) throws IOException\n     final byte[] bytes = baos.toByteArray();\n     Assert.assertEquals(theSupplier.getSerializedSize(), bytes.length);\n \n-    supplier = CompressedColumnarIntsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), ByteOrder.nativeOrder());\n+    supplier = CompressedColumnarIntsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), ByteOrder.nativeOrder(), null);\n     columnarInts = supplier.get();\n   }\n \n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java\nindex e4f416c31ee4..51b0c2a63e17 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedDoublesSerdeTest.java\n@@ -23,11 +23,18 @@\n import com.google.common.primitives.Doubles;\n import it.unimi.dsi.fastutil.ints.IntArrays;\n import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedWriter;\n import org.apache.druid.segment.writeout.OffHeapMemorySegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.Assert;\n+import org.junit.Assume;\n import org.junit.Ignore;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -37,12 +44,14 @@\n import org.junit.runners.Parameterized;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.File;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n import java.nio.channels.Channels;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -131,6 +140,63 @@ public void testChunkSerde() throws Exception\n     testWithValues(chunk);\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    // This test only makes sense if we can use BlockLayoutColumnarDoubleSerializer directly.\n+    // Exclude incompatible compressionStrategy.\n+    Assume.assumeThat(compressionStrategy, CoreMatchers.not(CoreMatchers.equalTo(CompressionStrategy.NONE)));\n+\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final ColumnarDoublesSerializer serializer = new BlockLayoutColumnarDoublesSerializer(\n+          columnName,\n+          segmentWriteOutMedium,\n+          columnName,\n+          order,\n+          compressionStrategy,\n+          fileSizeLimit,\n+          segmentWriteOutMedium.getCloser()\n+      );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.add(random.nextLong());\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the column actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final Supplier<ColumnarDoubles> columnSupplier = CompressedColumnarDoublesSuppliers.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          order,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarDoubles column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n+\n   // this test takes ~45 minutes to run\n   @Ignore\n   @Test\n@@ -179,7 +245,7 @@ public void testWithValues(double[] values) throws Exception\n     serializer.writeTo(Channels.newChannel(baos), null);\n     Assert.assertEquals(baos.size(), serializer.getSerializedSize());\n     Supplier<ColumnarDoubles> supplier = CompressedColumnarDoublesSuppliers\n-        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order);\n+        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order, null);\n     try (ColumnarDoubles doubles = supplier.get()) {\n       assertIndexMatchesVals(doubles, values);\n       for (int i = 0; i < 10; i++) {\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java\nindex 1994e316de49..20a82cecbc6c 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedFloatsSerdeTest.java\n@@ -23,11 +23,18 @@\n import com.google.common.primitives.Floats;\n import it.unimi.dsi.fastutil.ints.IntArrays;\n import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedWriter;\n import org.apache.druid.segment.writeout.OffHeapMemorySegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.Assert;\n+import org.junit.Assume;\n import org.junit.Ignore;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -37,12 +44,14 @@\n import org.junit.runners.Parameterized;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.File;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n import java.nio.channels.Channels;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -138,6 +147,63 @@ public void testChunkSerde() throws Exception\n     testWithValues(chunk);\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    // This test only makes sense if we can use BlockLayoutColumnarFloatSerializer directly.\n+    // Exclude incompatible compressionStrategy.\n+    Assume.assumeThat(compressionStrategy, CoreMatchers.not(CoreMatchers.equalTo(CompressionStrategy.NONE)));\n+\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final ColumnarFloatsSerializer serializer = new BlockLayoutColumnarFloatsSerializer(\n+          columnName,\n+          segmentWriteOutMedium,\n+          columnName,\n+          order,\n+          compressionStrategy,\n+          fileSizeLimit,\n+          segmentWriteOutMedium.getCloser()\n+      );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.add(random.nextLong());\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the column actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final Supplier<ColumnarFloats> columnSupplier = CompressedColumnarFloatsSupplier.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          order,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarFloats column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n+\n   // this test takes ~30 minutes to run\n   @Ignore\n   @Test\n@@ -188,7 +254,7 @@ public void testWithValues(float[] values) throws Exception\n     serializer.writeTo(Channels.newChannel(baos), null);\n     Assert.assertEquals(baos.size(), serializer.getSerializedSize());\n     CompressedColumnarFloatsSupplier supplier = CompressedColumnarFloatsSupplier\n-        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order);\n+        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order, null);\n     try (ColumnarFloats floats = supplier.get()) {\n \n       assertIndexMatchesVals(floats, values);\n@@ -241,9 +307,8 @@ private void testSupplierSerde(CompressedColumnarFloatsSupplier supplier, float[\n \n     final byte[] bytes = baos.toByteArray();\n     Assert.assertEquals(supplier.getSerializedSize(), bytes.length);\n-    CompressedColumnarFloatsSupplier anotherSupplier = CompressedColumnarFloatsSupplier.fromByteBuffer(\n-        ByteBuffer.wrap(bytes), order\n-    );\n+    CompressedColumnarFloatsSupplier anotherSupplier =\n+        CompressedColumnarFloatsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), order, null);\n     try (ColumnarFloats indexed = anotherSupplier.get()) {\n       assertIndexMatchesVals(indexed, vals);\n     }\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java\nindex 4876a347fb21..db8babedd269 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsAutoEncodingSerdeTest.java\n@@ -115,7 +115,7 @@ public void testValues(long[] values) throws Exception\n     serializer.writeTo(Channels.newChannel(baos), null);\n     Assert.assertEquals(baos.size(), serializer.getSerializedSize());\n     CompressedColumnarLongsSupplier supplier =\n-        CompressedColumnarLongsSupplier.fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order);\n+        CompressedColumnarLongsSupplier.fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order, null);\n     ColumnarLongs longs = supplier.get();\n \n     assertIndexMatchesVals(longs, values);\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java\nindex dfc457fe8b9e..0e55a0ca29fd 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedLongsSerdeTest.java\n@@ -23,11 +23,18 @@\n import com.google.common.primitives.Longs;\n import it.unimi.dsi.fastutil.ints.IntArrays;\n import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.io.smoosh.FileSmoosher;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedFileMapper;\n+import org.apache.druid.java.util.common.io.smoosh.SmooshedWriter;\n import org.apache.druid.segment.writeout.OffHeapMemorySegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.SegmentWriteOutMedium;\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.Assert;\n+import org.junit.Assume;\n import org.junit.Ignore;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -37,12 +44,14 @@\n import org.junit.runners.Parameterized;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.File;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n import java.nio.channels.Channels;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -166,6 +175,65 @@ public void testTooManyValues() throws IOException\n     }\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    // This test only makes sense if we can use BlockLayoutColumnarLongsSerializer directly. Exclude incompatible\n+    // combinations of compressionStrategy, encodingStrategy.\n+    Assume.assumeThat(compressionStrategy, CoreMatchers.not(CoreMatchers.equalTo(CompressionStrategy.NONE)));\n+    Assume.assumeThat(encodingStrategy, CoreMatchers.equalTo(CompressionFactory.LongEncodingStrategy.LONGS));\n+\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final ColumnarLongsSerializer serializer = new BlockLayoutColumnarLongsSerializer(\n+          columnName,\n+          segmentWriteOutMedium,\n+          columnName,\n+          order,\n+          new LongsLongEncodingWriter(order),\n+          compressionStrategy,\n+          fileSizeLimit,\n+          segmentWriteOutMedium.getCloser()\n+      );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.add(random.nextLong());\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the column actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final CompressedColumnarLongsSupplier columnSupplier = CompressedColumnarLongsSupplier.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          order,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarLongs column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n+\n   public void testWithValues(long[] values) throws Exception\n   {\n     testValues(values);\n@@ -193,7 +261,7 @@ public void testValues(long[] values) throws Exception\n     serializer.writeTo(Channels.newChannel(baos), null);\n     Assert.assertEquals(baos.size(), serializer.getSerializedSize());\n     CompressedColumnarLongsSupplier supplier = CompressedColumnarLongsSupplier\n-        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order);\n+        .fromByteBuffer(ByteBuffer.wrap(baos.toByteArray()), order, null);\n     try (ColumnarLongs longs = supplier.get()) {\n \n       assertIndexMatchesVals(longs, values);\n@@ -255,10 +323,8 @@ private void testSupplierSerde(CompressedColumnarLongsSupplier supplier, long[]\n \n     final byte[] bytes = baos.toByteArray();\n     Assert.assertEquals(supplier.getSerializedSize(), bytes.length);\n-    CompressedColumnarLongsSupplier anotherSupplier = CompressedColumnarLongsSupplier.fromByteBuffer(\n-        ByteBuffer.wrap(bytes),\n-        order\n-    );\n+    CompressedColumnarLongsSupplier anotherSupplier =\n+        CompressedColumnarLongsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), order, null);\n     try (ColumnarLongs indexed = anotherSupplier.get()) {\n       assertIndexMatchesVals(indexed, vals);\n     }\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java\nindex c06e11c90d94..f0208d791b30 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSerializerTest.java\n@@ -20,6 +20,7 @@\n package org.apache.druid.segment.data;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Supplier;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import com.google.common.primitives.Ints;\n@@ -34,6 +35,8 @@\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.segment.writeout.WriteOutBytes;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -125,6 +128,7 @@ private void checkSerializedSizeAndData(int chunkSize) throws Exception\n         chunkSize,\n         byteOrder,\n         compressionStrategy,\n+        GenericIndexedWriter.MAX_FILE_SIZE,\n         segmentWriteOutMedium.getCloser()\n     );\n     CompressedVSizeColumnarIntsSupplier supplierFromList = CompressedVSizeColumnarIntsSupplier.fromList(\n@@ -149,7 +153,8 @@ private void checkSerializedSizeAndData(int chunkSize) throws Exception\n     // read from ByteBuffer and check values\n     CompressedVSizeColumnarIntsSupplier supplierFromByteBuffer = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n         ByteBuffer.wrap(IOUtils.toByteArray(writeOutBytes.asInputStream())),\n-        byteOrder\n+        byteOrder,\n+        null\n     );\n     ColumnarInts columnarInts = supplierFromByteBuffer.get();\n     for (int i = 0; i < vals.length; ++i) {\n@@ -199,6 +204,7 @@ public void testTooManyValues() throws IOException\n           \""test\"",\n           compressionStrategy,\n           Long.BYTES * 10000,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       CompressedVSizeColumnarIntsSerializer serializer = new CompressedVSizeColumnarIntsSerializer(\n@@ -236,6 +242,7 @@ private void checkV2SerializedSizeAndData(int chunkSize) throws Exception\n         \""test\"",\n         compressionStrategy,\n         Long.BYTES * 10000,\n+        GenericIndexedWriter.MAX_FILE_SIZE,\n         segmentWriteOutMedium.getCloser()\n     );\n     CompressedVSizeColumnarIntsSerializer writer = new CompressedVSizeColumnarIntsSerializer(\n@@ -264,7 +271,8 @@ private void checkV2SerializedSizeAndData(int chunkSize) throws Exception\n \n     CompressedVSizeColumnarIntsSupplier supplierFromByteBuffer = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n         mapper.mapFile(\""test\""),\n-        byteOrder\n+        byteOrder,\n+        null\n     );\n \n     ColumnarInts columnarInts = supplierFromByteBuffer.get();\n@@ -284,4 +292,59 @@ public void testMultiValueFileLargeData() throws Exception\n     }\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final int maxValue = Integer.MAX_VALUE;\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final CompressedVSizeColumnarIntsSerializer serializer = new CompressedVSizeColumnarIntsSerializer(\n+          columnName,\n+          segmentWriteOutMedium,\n+          columnName,\n+          maxValue,\n+          CompressedVSizeColumnarIntsSupplier.maxIntsInBufferForValue(maxValue),\n+          byteOrder,\n+          compressionStrategy,\n+          fileSizeLimit,\n+          segmentWriteOutMedium.getCloser()\n+      );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.addValue(random.nextInt() ^ Integer.MIN_VALUE);\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the column actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final Supplier<ColumnarInts> columnSupplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          byteOrder,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarInts column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n }\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java\nindex 48a443ef6610..6231d73a14f0 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarIntsSupplierTest.java\n@@ -136,7 +136,7 @@ private void makeWithSerde(final int chunkSize) throws IOException\n     final byte[] bytes = baos.toByteArray();\n     Assert.assertEquals(theSupplier.getSerializedSize(), bytes.length);\n \n-    supplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), byteOrder);\n+    supplier = CompressedVSizeColumnarIntsSupplier.fromByteBuffer(ByteBuffer.wrap(bytes), byteOrder, null);\n     columnarInts = supplier.get();\n   }\n \n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java\nindex 60de4d78ed55..2798cf7a5d32 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/CompressedVSizeColumnarMultiIntsSupplierTest.java\n@@ -87,7 +87,8 @@ public WritableSupplier<ColumnarMultiInts> fromByteBuffer(ByteBuffer buffer)\n     return wrapSupplier(\n         CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n             buffer,\n-            ByteOrder.nativeOrder()\n+            ByteOrder.nativeOrder(),\n+            null\n         ),\n         closer\n     );\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java b/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java\nindex 52e5087f0f37..ad0c39f98a88 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/GenericIndexedTest.java\n@@ -126,7 +126,7 @@ private GenericIndexed<String> serializeAndDeserialize(GenericIndexed<String> in\n \n     final ByteBuffer byteBuffer = ByteBuffer.wrap(baos.toByteArray());\n     Assert.assertEquals(indexed.getSerializedSize(), byteBuffer.remaining());\n-    GenericIndexed<String> deserialized = GenericIndexed.read(byteBuffer, GenericIndexed.STRING_STRATEGY);\n+    GenericIndexed<String> deserialized = GenericIndexed.read(byteBuffer, GenericIndexed.STRING_STRATEGY, null);\n     Assert.assertEquals(0, byteBuffer.remaining());\n     return deserialized;\n   }\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java b/processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java\nindex ec745ffa2301..6458db738f9b 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/TestColumnCompression.java\n@@ -92,7 +92,8 @@ public void setUp() throws Exception\n     );\n     this.compressed = CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n         buffer,\n-        ByteOrder.nativeOrder()\n+        ByteOrder.nativeOrder(),\n+        null\n     ).get();\n \n     filter = new BitSet();\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java b/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java\nindex 29ba49913c44..246b32b2ac00 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSerializerTest.java\n@@ -20,6 +20,7 @@\n package org.apache.druid.segment.data;\n \n import com.google.common.base.Function;\n+import com.google.common.base.Supplier;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import org.apache.commons.io.IOUtils;\n@@ -34,6 +35,8 @@\n import org.apache.druid.segment.writeout.TmpFileSegmentWriteOutMediumFactory;\n import org.apache.druid.segment.writeout.WriteOutBytes;\n import org.apache.druid.utils.CloseableUtils;\n+import org.hamcrest.MatcherAssert;\n+import org.hamcrest.Matchers;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Ignore;\n@@ -45,6 +48,7 @@\n import org.junit.runners.Parameterized;\n \n import java.io.File;\n+import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n import java.util.ArrayList;\n@@ -151,6 +155,65 @@ public void testMultiValueFileLargeData() throws Exception\n     }\n   }\n \n+  @Test\n+  public void testLargeColumn() throws IOException\n+  {\n+    final File columnDir = temporaryFolder.newFolder();\n+    final String columnName = \""column\"";\n+    final long numRows = 500_000; // enough values that we expect to switch into large-column mode\n+\n+    try (\n+        SegmentWriteOutMedium segmentWriteOutMedium =\n+            TmpFileSegmentWriteOutMediumFactory.instance().makeSegmentWriteOutMedium(temporaryFolder.newFolder());\n+        FileSmoosher smoosher = new FileSmoosher(columnDir)\n+    ) {\n+      final Random random = new Random(0);\n+      final int fileSizeLimit = 128_000; // limit to 128KB so we switch to large-column mode sooner\n+      final V3CompressedVSizeColumnarMultiIntsSerializer serializer =\n+          V3CompressedVSizeColumnarMultiIntsSerializer.create(\n+              columnName,\n+              segmentWriteOutMedium,\n+              columnName,\n+              Integer.MAX_VALUE,\n+              compressionStrategy,\n+              fileSizeLimit\n+          );\n+      serializer.open();\n+\n+      for (int i = 0; i < numRows; i++) {\n+        serializer.addValues(new ArrayBasedIndexedInts(new int[]{random.nextInt() ^ Integer.MIN_VALUE}));\n+      }\n+\n+      try (SmooshedWriter primaryWriter = smoosher.addWithSmooshedWriter(columnName, serializer.getSerializedSize())) {\n+        serializer.writeTo(primaryWriter, smoosher);\n+      }\n+    }\n+\n+    try (SmooshedFileMapper smooshMapper = SmooshedFileMapper.load(columnDir)) {\n+      MatcherAssert.assertThat(\n+          \""Number of offset parts written\"", // ensure the offsets subcolumn actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column.offsets_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      MatcherAssert.assertThat(\n+          \""Number of value parts written\"", // ensure the values subcolumn actually ended up multi-part\n+          smooshMapper.getInternalFilenames().stream().filter(s -> s.startsWith(\""column.values_value_\"")).count(),\n+          Matchers.greaterThan(1L)\n+      );\n+\n+      final Supplier<ColumnarMultiInts> columnSupplier = V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n+          smooshMapper.mapFile(columnName),\n+          byteOrder,\n+          smooshMapper\n+      );\n+\n+      try (final ColumnarMultiInts column = columnSupplier.get()) {\n+        Assert.assertEquals(numRows, column.size());\n+      }\n+    }\n+  }\n+\n   // this test takes ~30 minutes to run\n   @Ignore\n   @Test\n@@ -207,6 +270,7 @@ private void checkSerializedSizeAndData(int offsetChunkFactor, int valueChunkFac\n           offsetChunkFactor,\n           byteOrder,\n           compressionStrategy,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       CompressedVSizeColumnarIntsSerializer valueWriter = new CompressedVSizeColumnarIntsSerializer(\n@@ -217,6 +281,7 @@ private void checkSerializedSizeAndData(int offsetChunkFactor, int valueChunkFac\n           valueChunkFactor,\n           byteOrder,\n           compressionStrategy,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       V3CompressedVSizeColumnarMultiIntsSerializer writer =\n@@ -244,7 +309,8 @@ private void checkSerializedSizeAndData(int offsetChunkFactor, int valueChunkFac\n       // read from ByteBuffer and check values\n       V3CompressedVSizeColumnarMultiIntsSupplier supplierFromByteBuffer = V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n           ByteBuffer.wrap(IOUtils.toByteArray(writeOutBytes.asInputStream())),\n-          byteOrder\n+          byteOrder,\n+          null\n       );\n \n       try (final ColumnarMultiInts columnarMultiInts = supplierFromByteBuffer.get()) {\n@@ -281,6 +347,7 @@ private void checkV2SerializedSizeAndData(int offsetChunkFactor, int valueChunkF\n               \""offset\"",\n               compressionStrategy,\n               Long.BYTES * 250000,\n+              GenericIndexedWriter.MAX_FILE_SIZE,\n               segmentWriteOutMedium.getCloser()\n           ),\n           segmentWriteOutMedium.getCloser()\n@@ -291,6 +358,7 @@ private void checkV2SerializedSizeAndData(int offsetChunkFactor, int valueChunkF\n           \""value\"",\n           compressionStrategy,\n           Long.BYTES * 250000,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       CompressedVSizeColumnarIntsSerializer valueWriter = new CompressedVSizeColumnarIntsSerializer(\n@@ -316,7 +384,7 @@ private void checkV2SerializedSizeAndData(int offsetChunkFactor, int valueChunkF\n       SmooshedFileMapper mapper = Smoosh.map(tmpDirectory);\n \n       V3CompressedVSizeColumnarMultiIntsSupplier supplierFromByteBuffer =\n-          V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(mapper.mapFile(\""test\""), byteOrder);\n+          V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(mapper.mapFile(\""test\""), byteOrder, null);\n       ColumnarMultiInts columnarMultiInts = supplierFromByteBuffer.get();\n       Assert.assertEquals(columnarMultiInts.size(), vals.size());\n       for (int i = 0; i < vals.size(); ++i) {\n@@ -359,6 +427,7 @@ private void generateV2SerializedSizeAndData(\n               \""offset\"",\n               compressionStrategy,\n               Long.BYTES * 250000,\n+              GenericIndexedWriter.MAX_FILE_SIZE,\n               segmentWriteOutMedium.getCloser()\n           ),\n           segmentWriteOutMedium.getCloser()\n@@ -369,6 +438,7 @@ private void generateV2SerializedSizeAndData(\n           \""value\"",\n           compressionStrategy,\n           Long.BYTES * 250000,\n+          GenericIndexedWriter.MAX_FILE_SIZE,\n           segmentWriteOutMedium.getCloser()\n       );\n       CompressedVSizeColumnarIntsSerializer valueWriter = new CompressedVSizeColumnarIntsSerializer(\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java\nindex ad529e10bdbb..f6a0ccbaeb76 100644\n--- a/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/data/V3CompressedVSizeColumnarMultiIntsSupplierTest.java\n@@ -85,7 +85,8 @@ public WritableSupplier<ColumnarMultiInts> fromByteBuffer(ByteBuffer buffer)\n     return wrapSupplier(\n         V3CompressedVSizeColumnarMultiIntsSupplier.fromByteBuffer(\n             buffer,\n-            ByteOrder.nativeOrder()\n+            ByteOrder.nativeOrder(),\n+            null\n         ),\n         closer\n     );\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java\nindex c9d7e05c622d..e661b747a108 100644\n--- a/processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/nested/NestedFieldColumnIndexSupplierTest.java\n@@ -146,7 +146,7 @@ public void setup() throws IOException\n     arrayWriter.open();\n     writeToBuffer(arrayBuffer, arrayWriter);\n \n-    GenericIndexed<ByteBuffer> strings = GenericIndexed.read(stringBuffer, GenericIndexed.UTF8_STRATEGY);\n+    GenericIndexed<ByteBuffer> strings = GenericIndexed.read(stringBuffer, GenericIndexed.UTF8_STRATEGY, null);\n     globalStrings = () -> strings.singleThreaded();\n     globalLongs = FixedIndexed.read(longBuffer, TypeStrategies.LONG, ByteOrder.nativeOrder(), Long.BYTES);\n     globalDoubles = FixedIndexed.read(doubleBuffer, TypeStrategies.DOUBLE, ByteOrder.nativeOrder(), Double.BYTES);\n@@ -1241,7 +1241,7 @@ public void testEnsureNoImproperSelectionFromAdjustedGlobals() throws IOExceptio\n     doubleWriter.open();\n     writeToBuffer(doubleBuffer, doubleWriter);\n \n-    GenericIndexed<ByteBuffer> strings = GenericIndexed.read(stringBuffer, GenericIndexed.UTF8_STRATEGY);\n+    GenericIndexed<ByteBuffer> strings = GenericIndexed.read(stringBuffer, GenericIndexed.UTF8_STRATEGY, null);\n     Supplier<Indexed<ByteBuffer>> stringIndexed = () -> strings.singleThreaded();\n     Supplier<FixedIndexed<Long>> longIndexed = FixedIndexed.read(longBuffer, TypeStrategies.LONG, ByteOrder.nativeOrder(), Long.BYTES);\n     Supplier<FixedIndexed<Double>> doubleIndexed = FixedIndexed.read(doubleBuffer, TypeStrategies.DOUBLE, ByteOrder.nativeOrder(), Double.BYTES);\n@@ -1293,7 +1293,8 @@ public void testEnsureNoImproperSelectionFromAdjustedGlobals() throws IOExceptio\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     NestedFieldColumnIndexSupplier<?> indexSupplier = new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1397,7 +1398,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeStringSupplier(ColumnCon\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1481,7 +1483,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeStringWithNullsSupplier(\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1561,7 +1564,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeLongSupplier(ColumnConfi\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1646,7 +1650,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeLongSupplierWithNull(Col\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1726,7 +1731,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeDoubleSupplier(ColumnCon\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1811,7 +1817,8 @@ private NestedFieldColumnIndexSupplier<?> makeSingleTypeDoubleSupplierWithNull(C\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n@@ -1903,7 +1910,8 @@ private NestedFieldColumnIndexSupplier<?> makeVariantSupplierWithNull(ColumnConf\n         Integer.BYTES\n     );\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n \n     return new NestedFieldColumnIndexSupplier<>(\n         new FieldTypeInfo.TypeSet(\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java b/processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java\nindex 263b4132dd7d..74319c884a83 100644\n--- a/processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/serde/DictionaryEncodedStringIndexSupplierTest.java\n@@ -157,10 +157,11 @@ private StringUtf8ColumnIndexSupplier<?> makeStringWithNullsSupplier() throws IO\n     writeToBuffer(byteBuffer, stringWriter);\n     writeToBuffer(bitmapsBuffer, bitmapWriter);\n \n-    GenericIndexed<ImmutableBitmap> bitmaps = GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy());\n+    GenericIndexed<ImmutableBitmap> bitmaps =\n+        GenericIndexed.read(bitmapsBuffer, roaringFactory.getObjectStrategy(), null);\n     return new StringUtf8ColumnIndexSupplier<>(\n         roaringFactory.getBitmapFactory(),\n-        GenericIndexed.read(byteBuffer, GenericIndexed.UTF8_STRATEGY)::singleThreaded,\n+        GenericIndexed.read(byteBuffer, GenericIndexed.UTF8_STRATEGY, null)::singleThreaded,\n         bitmaps,\n         null\n     );\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java b/processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java\nindex cdc502c6c193..99f3c24a54a5 100644\n--- a/processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/serde/HyperUniquesSerdeForTest.java\n@@ -93,7 +93,7 @@ public void deserializeColumn(ByteBuffer byteBuffer, ColumnBuilder columnBuilder\n   {\n     final GenericIndexed column;\n     if (columnBuilder.getFileMapper() == null) {\n-      column = GenericIndexed.read(byteBuffer, getObjectStrategy());\n+      column = GenericIndexed.read(byteBuffer, getObjectStrategy(), null);\n     } else {\n       column = GenericIndexed.read(byteBuffer, getObjectStrategy(), columnBuilder.getFileMapper());\n     }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__druid-17674"", ""pr_id"": 17674, ""issue_id"": 17669, ""repo"": ""apache/druid"", ""problem_statement"": ""Add the capability to speed up S3 uploads using AWS transfer manager\n### Description\n\n\n\n* Use [AWS Transfer Manager](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/transfer-manager.html) to speed up uploads to S3.\n\n\n### Motivation\n\n* To improve speed of uploads to S3.\n"", ""issue_word_count"": 45, ""test_files_count"": 11, ""non_test_files_count"": 5, ""pr_changed_files"": [""docs/development/extensions-core/s3.md"", ""extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3StorageConfig.java"", ""extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TransferConfig.java"", ""extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java"", ""extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TransferConfigTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3Test.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java""], ""pr_changed_test_files"": [""extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TransferConfigTest.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3Test.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java"", ""extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java""], ""base_commit"": ""e36e187a632ef1b3784a1c0f8b8048d2101070db"", ""head_commit"": ""284b95c0f29e7beba9454c8c8a9cab3ddfcf5371"", ""repo_url"": ""https://github.com/apache/druid/pull/17674"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17674"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-01T09:27:27.000Z"", ""patch"": ""diff --git a/docs/development/extensions-core/s3.md b/docs/development/extensions-core/s3.md\nindex 2565451c2725..be6cd8546493 100644\n--- a/docs/development/extensions-core/s3.md\n+++ b/docs/development/extensions-core/s3.md\n@@ -57,6 +57,9 @@ To use S3 for Deep Storage, you must supply [connection information](#configurat\n |`druid.storage.type`|Global deep storage provider. Must be set to `s3` to make use of this extension.|Must be set (likely `s3`).|\n |`druid.storage.disableAcl`|Boolean flag for how object permissions are handled. To use ACLs, set this property to `false`. To use Object Ownership, set it to `true`. The permission requirements for ACLs and Object Ownership are different. For more information, see [S3 permissions settings](#s3-permissions-settings).|false|\n |`druid.storage.useS3aSchema`|If true, use the \""s3a\"" filesystem when using Hadoop-based ingestion. If false, the \""s3n\"" filesystem will be used. Only affects Hadoop-based ingestion.|false|\n+|`druid.storage.transfer.useTransferManager`| If true, use AWS S3 Transfer Manager to upload segments to S3.|true|\n+|`druid.storage.transfer.minimumUploadPartSize`| Minimum size (in bytes) of each part in a multipart upload.|20971520 (20 MB)|\n+|`druid.storage.transfer.multipartUploadThreshold`| The file size threshold (in bytes) above which a file upload is converted into a multipart upload instead of a single PUT request.| 20971520 (20 MB)|\n \n ## Configuration\n \n\ndiff --git a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3StorageConfig.java b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3StorageConfig.java\nindex cfae0eb084b7..b52d13cd518e 100644\n--- a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3StorageConfig.java\n+++ b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3StorageConfig.java\n@@ -36,12 +36,22 @@ public class S3StorageConfig\n   @JsonProperty(\""sse\"")\n   private final ServerSideEncryption serverSideEncryption;\n \n+  /**\n+   * S3 transfer config.\n+   *\n+   * @see S3StorageDruidModule#configure\n+   */\n+  @JsonProperty(\""transfer\"")\n+  private final S3TransferConfig s3TransferConfig;\n+\n   @JsonCreator\n   public S3StorageConfig(\n-      @JsonProperty(\""sse\"") ServerSideEncryption serverSideEncryption\n+      @JsonProperty(\""sse\"") ServerSideEncryption serverSideEncryption,\n+      @JsonProperty(\""transfer\"") S3TransferConfig s3TransferConfig\n   )\n   {\n     this.serverSideEncryption = serverSideEncryption == null ? new NoopServerSideEncryption() : serverSideEncryption;\n+    this.s3TransferConfig = s3TransferConfig == null ? new S3TransferConfig() : s3TransferConfig;\n   }\n \n   @JsonProperty(\""sse\"")\n@@ -49,4 +59,10 @@ public ServerSideEncryption getServerSideEncryption()\n   {\n     return serverSideEncryption;\n   }\n+\n+  @JsonProperty(\""transfer\"")\n+  public S3TransferConfig getS3TransferConfig()\n+  {\n+    return s3TransferConfig;\n+  }\n }\n\ndiff --git a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TransferConfig.java b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TransferConfig.java\nnew file mode 100644\nindex 000000000000..4df62b833423\n--- /dev/null\n+++ b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3TransferConfig.java\n@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.storage.s3;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.validation.constraints.Min;\n+\n+/**\n+ */\n+public class S3TransferConfig\n+{\n+  @JsonProperty\n+  private boolean useTransferManager = true;\n+\n+  @JsonProperty\n+  @Min(1)\n+  private long minimumUploadPartSize = 20 * 1024 * 1024L;\n+\n+  @JsonProperty\n+  @Min(1)\n+  private long multipartUploadThreshold = 20 * 1024 * 1024L;\n+\n+  public void setUseTransferManager(boolean useTransferManager)\n+  {\n+    this.useTransferManager = useTransferManager;\n+  }\n+\n+  public void setMinimumUploadPartSize(long minimumUploadPartSize)\n+  {\n+    this.minimumUploadPartSize = minimumUploadPartSize;\n+  }\n+\n+  public void setMultipartUploadThreshold(long multipartUploadThreshold)\n+  {\n+    this.multipartUploadThreshold = multipartUploadThreshold;\n+  }\n+\n+  public boolean isUseTransferManager()\n+  {\n+    return useTransferManager;\n+  }\n+\n+  public long getMinimumUploadPartSize()\n+  {\n+    return minimumUploadPartSize;\n+  }\n+\n+  public long getMultipartUploadThreshold()\n+  {\n+    return multipartUploadThreshold;\n+  }\n+\n+}\n\ndiff --git a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java\nindex b299d4f9dd80..1eba9907ab36 100644\n--- a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java\n+++ b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/S3Utils.java\n@@ -96,6 +96,9 @@ public boolean apply(Throwable e)\n         // This can happen sometimes when AWS isn't able to obtain the credentials for some service:\n         // https://github.com/aws/aws-sdk-java/issues/2285\n         return true;\n+      } else if (e instanceof InterruptedException) {\n+        Thread.interrupted(); // Clear interrupted state and not retry\n+        return false;\n       } else if (e instanceof AmazonClientException) {\n         return AWSClientUtil.isClientExceptionRecoverable((AmazonClientException) e);\n       } else {\n@@ -348,7 +351,7 @@ static void uploadFileIfPossible(\n       String bucket,\n       String key,\n       File file\n-  )\n+  ) throws InterruptedException\n   {\n     final PutObjectRequest putObjectRequest = new PutObjectRequest(bucket, key, file);\n \n@@ -356,7 +359,7 @@ static void uploadFileIfPossible(\n       putObjectRequest.setAccessControlList(S3Utils.grantFullControlToBucketOwner(service, bucket));\n     }\n     log.info(\""Pushing [%s] to bucket[%s] and key[%s].\"", file, bucket, key);\n-    service.putObject(putObjectRequest);\n+    service.upload(putObjectRequest);\n   }\n \n   @Nullable\n\ndiff --git a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3.java b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3.java\nindex 31120ba883c4..e747ddf9f4e1 100644\n--- a/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3.java\n+++ b/extensions-core/s3-extensions/src/main/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3.java\n@@ -43,6 +43,9 @@\n import com.amazonaws.services.s3.model.S3Object;\n import com.amazonaws.services.s3.model.UploadPartRequest;\n import com.amazonaws.services.s3.model.UploadPartResult;\n+import com.amazonaws.services.s3.transfer.TransferManager;\n+import com.amazonaws.services.s3.transfer.TransferManagerBuilder;\n+import com.amazonaws.services.s3.transfer.Upload;\n import org.apache.druid.java.util.common.ISE;\n \n import java.io.File;\n@@ -65,11 +68,21 @@ public static Builder builder()\n \n   private final AmazonS3 amazonS3;\n   private final ServerSideEncryption serverSideEncryption;\n+  private final TransferManager transferManager;\n \n-  public ServerSideEncryptingAmazonS3(AmazonS3 amazonS3, ServerSideEncryption serverSideEncryption)\n+  public ServerSideEncryptingAmazonS3(AmazonS3 amazonS3, ServerSideEncryption serverSideEncryption, S3TransferConfig transferConfig)\n   {\n     this.amazonS3 = amazonS3;\n     this.serverSideEncryption = serverSideEncryption;\n+    if (transferConfig.isUseTransferManager()) {\n+      this.transferManager = TransferManagerBuilder.standard()\n+          .withS3Client(amazonS3)\n+          .withMinimumUploadPartSize(transferConfig.getMinimumUploadPartSize())\n+          .withMultipartUploadThreshold(transferConfig.getMultipartUploadThreshold())\n+          .build();\n+    } else {\n+      this.transferManager = null;\n+    }\n   }\n \n   public AmazonS3 getAmazonS3()\n@@ -173,10 +186,20 @@ public CompleteMultipartUploadResult completeMultipartUpload(CompleteMultipartUp\n     return amazonS3.completeMultipartUpload(request);\n   }\n \n+  public void upload(PutObjectRequest request) throws InterruptedException\n+  {\n+    if (transferManager == null) {\n+      putObject(request);\n+    } else {\n+      Upload transfer = transferManager.upload(serverSideEncryption.decorate(request));\n+      transfer.waitForCompletion();\n+    }\n+  }\n+\n   public static class Builder\n   {\n     private AmazonS3ClientBuilder amazonS3ClientBuilder = AmazonS3Client.builder();\n-    private S3StorageConfig s3StorageConfig = new S3StorageConfig(new NoopServerSideEncryption());\n+    private S3StorageConfig s3StorageConfig = new S3StorageConfig(new NoopServerSideEncryption(), null);\n \n     public Builder setAmazonS3ClientBuilder(AmazonS3ClientBuilder amazonS3ClientBuilder)\n     {\n@@ -217,7 +240,7 @@ public ServerSideEncryptingAmazonS3 build()\n         throw new RuntimeException(e);\n       }\n \n-      return new ServerSideEncryptingAmazonS3(amazonS3Client, s3StorageConfig.getServerSideEncryption());\n+      return new ServerSideEncryptingAmazonS3(amazonS3Client, s3StorageConfig.getServerSideEncryption(), s3StorageConfig.getS3TransferConfig());\n     }\n   }\n }\n"", ""test_patch"": ""diff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java\nindex 4f14364e7222..d2c2a33293f9 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/data/input/s3/S3InputSourceTest.java\n@@ -74,6 +74,7 @@\n import org.apache.druid.metadata.DefaultPasswordProvider;\n import org.apache.druid.storage.s3.NoopServerSideEncryption;\n import org.apache.druid.storage.s3.S3InputDataConfig;\n+import org.apache.druid.storage.s3.S3TransferConfig;\n import org.apache.druid.storage.s3.S3Utils;\n import org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3;\n import org.apache.druid.testing.InitializedNullHandlingTest;\n@@ -113,7 +114,8 @@ public class S3InputSourceTest extends InitializedNullHandlingTest\n   public static final AmazonS3ClientBuilder AMAZON_S3_CLIENT_BUILDER = AmazonS3Client.builder();\n   public static final ServerSideEncryptingAmazonS3 SERVICE = new ServerSideEncryptingAmazonS3(\n       S3_CLIENT,\n-      new NoopServerSideEncryption()\n+      new NoopServerSideEncryption(),\n+      new S3TransferConfig()\n   );\n   public static final S3InputDataConfig INPUT_DATA_CONFIG;\n   private static final int MAX_LISTING_LENGTH = 10;\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java\nindex ea2ca4af26c1..8ee6c826718d 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ObjectSummaryIteratorTest.java\n@@ -195,7 +195,7 @@ private static ServerSideEncryptingAmazonS3 makeMockClient(\n       final List<S3ObjectSummary> objects\n   )\n   {\n-    return new ServerSideEncryptingAmazonS3(null, null)\n+    return new ServerSideEncryptingAmazonS3(null, null, new S3TransferConfig())\n     {\n       @Override\n       public ListObjectsV2Result listObjectsV2(final ListObjectsV2Request request)\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java\nindex f5005c706e01..4acf553fae50 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentArchiverTest.java\n@@ -76,7 +76,8 @@ public String getArchiveBaseKey()\n   private static final Supplier<ServerSideEncryptingAmazonS3> S3_SERVICE = Suppliers.ofInstance(\n       new ServerSideEncryptingAmazonS3(\n           EasyMock.createStrictMock(AmazonS3Client.class),\n-          new NoopServerSideEncryption()\n+          new NoopServerSideEncryption(),\n+          new S3TransferConfig()\n       )\n   );\n   private static final S3DataSegmentPuller PULLER = new S3DataSegmentPuller(S3_SERVICE.get());\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java\nindex 550a72cef43c..8f653e956a83 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentMoverTest.java\n@@ -201,7 +201,7 @@ private static class MockAmazonS3Client extends ServerSideEncryptingAmazonS3\n \n     private MockAmazonS3Client()\n     {\n-      super(new AmazonS3Client(), new NoopServerSideEncryption());\n+      super(new AmazonS3Client(), new NoopServerSideEncryption(), new S3TransferConfig());\n     }\n \n     public boolean didMove()\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java\nindex ba1aba1305ee..698f9d6e63f4 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3DataSegmentPusherTest.java\n@@ -25,7 +25,7 @@\n import com.amazonaws.services.s3.model.Grant;\n import com.amazonaws.services.s3.model.Owner;\n import com.amazonaws.services.s3.model.Permission;\n-import com.amazonaws.services.s3.model.PutObjectResult;\n+import com.amazonaws.services.s3.model.PutObjectRequest;\n import com.google.common.io.Files;\n import org.apache.druid.error.DruidException;\n import org.apache.druid.java.util.common.Intervals;\n@@ -41,9 +41,9 @@\n import org.junit.rules.TemporaryFolder;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.function.Consumer;\n import java.util.regex.Pattern;\n \n /**\n@@ -58,12 +58,8 @@ public class S3DataSegmentPusherTest\n   public void testPush() throws Exception\n   {\n     testPushInternal(\n-        false,\n-        \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/index\\\\.zip\"",\n-        client ->\n-            EasyMock.expect(client.putObject(EasyMock.anyObject()))\n-                    .andReturn(new PutObjectResult())\n-                    .once()\n+            false,\n+            \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/index\\\\.zip\""\n     );\n   }\n \n@@ -71,12 +67,8 @@ public void testPush() throws Exception\n   public void testPushUseUniquePath() throws Exception\n   {\n     testPushInternal(\n-        true,\n-        \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/[A-Za-z0-9-]{36}/index\\\\.zip\"",\n-        client ->\n-            EasyMock.expect(client.putObject(EasyMock.anyObject()))\n-                    .andReturn(new PutObjectResult())\n-                    .once()\n+            true,\n+            \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/[A-Za-z0-9-]{36}/index\\\\.zip\""\n     );\n   }\n \n@@ -86,30 +78,19 @@ public void testEntityTooLarge()\n     final DruidException exception = Assert.assertThrows(\n         DruidException.class,\n         () ->\n-            testPushInternal(\n+        testPushInternalForEntityTooLarge(\n                 false,\n-                \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/index\\\\.zip\"",\n-                client -> {\n-                  final AmazonS3Exception e = new AmazonS3Exception(\""whoa too many bytes\"");\n-                  e.setErrorCode(S3Utils.ERROR_ENTITY_TOO_LARGE);\n-                  EasyMock.expect(client.putObject(EasyMock.anyObject()))\n-                          .andThrow(e)\n-                          .once();\n-                }\n-            )\n+                \""key/foo/2015-01-01T00:00:00\\\\.000Z_2016-01-01T00:00:00\\\\.000Z/0/0/index\\\\.zip\""\n+        )\n     );\n \n     MatcherAssert.assertThat(\n-        exception,\n-        ThrowableMessageMatcher.hasMessage(CoreMatchers.startsWith(\""Got error[EntityTooLarge] from S3\""))\n+            exception,\n+            ThrowableMessageMatcher.hasMessage(CoreMatchers.startsWith(\""Got error[EntityTooLarge] from S3\""))\n     );\n   }\n \n-  private void testPushInternal(\n-      boolean useUniquePath,\n-      String matcher,\n-      Consumer<ServerSideEncryptingAmazonS3> clientDecorator\n-  ) throws Exception\n+  private void testPushInternal(boolean useUniquePath, String matcher) throws Exception\n   {\n     ServerSideEncryptingAmazonS3 s3Client = EasyMock.createStrictMock(ServerSideEncryptingAmazonS3.class);\n \n@@ -118,10 +99,36 @@ private void testPushInternal(\n     acl.grantAllPermissions(new Grant(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl));\n     EasyMock.expect(s3Client.getBucketAcl(EasyMock.eq(\""bucket\""))).andReturn(acl).once();\n \n-    clientDecorator.accept(s3Client);\n+    s3Client.upload(EasyMock.anyObject(PutObjectRequest.class));\n+    EasyMock.expectLastCall().once();\n \n     EasyMock.replay(s3Client);\n \n+    validate(useUniquePath, matcher, s3Client);\n+  }\n+\n+  private void testPushInternalForEntityTooLarge(boolean useUniquePath, String matcher) throws Exception\n+  {\n+    ServerSideEncryptingAmazonS3 s3Client = EasyMock.createStrictMock(ServerSideEncryptingAmazonS3.class);\n+    final AmazonS3Exception e = new AmazonS3Exception(\""whoa too many bytes\"");\n+    e.setErrorCode(S3Utils.ERROR_ENTITY_TOO_LARGE);\n+\n+\n+    final AccessControlList acl = new AccessControlList();\n+    acl.setOwner(new Owner(\""ownerId\"", \""owner\""));\n+    acl.grantAllPermissions(new Grant(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl));\n+    EasyMock.expect(s3Client.getBucketAcl(EasyMock.eq(\""bucket\""))).andReturn(acl).once();\n+\n+    s3Client.upload(EasyMock.anyObject(PutObjectRequest.class));\n+    EasyMock.expectLastCall().andThrow(e).once();\n+\n+    EasyMock.replay(s3Client);\n+\n+    validate(useUniquePath, matcher, s3Client);\n+  }\n+\n+  private void validate(boolean useUniquePath, String matcher, ServerSideEncryptingAmazonS3 s3Client) throws IOException\n+  {\n     S3DataSegmentPusherConfig config = new S3DataSegmentPusherConfig();\n     config.setBucket(\""bucket\"");\n     config.setBaseKey(\""key\"");\n@@ -136,15 +143,15 @@ private void testPushInternal(\n     final long size = data.length;\n \n     DataSegment segmentToPush = new DataSegment(\n-        \""foo\"",\n-        Intervals.of(\""2015/2016\""),\n-        \""0\"",\n-        new HashMap<>(),\n-        new ArrayList<>(),\n-        new ArrayList<>(),\n-        NoneShardSpec.instance(),\n-        0,\n-        size\n+            \""foo\"",\n+            Intervals.of(\""2015/2016\""),\n+            \""0\"",\n+            new HashMap<>(),\n+            new ArrayList<>(),\n+            new ArrayList<>(),\n+            NoneShardSpec.instance(),\n+            0,\n+            size\n     );\n \n     DataSegment segment = pusher.push(tempFolder.getRoot(), segmentToPush, useUniquePath);\n@@ -153,8 +160,8 @@ private void testPushInternal(\n     Assert.assertEquals(1, (int) segment.getBinaryVersion());\n     Assert.assertEquals(\""bucket\"", segment.getLoadSpec().get(\""bucket\""));\n     Assert.assertTrue(\n-        segment.getLoadSpec().get(\""key\"").toString(),\n-        Pattern.compile(matcher).matcher(segment.getLoadSpec().get(\""key\"").toString()).matches()\n+            segment.getLoadSpec().get(\""key\"").toString(),\n+            Pattern.compile(matcher).matcher(segment.getLoadSpec().get(\""key\"").toString()).matches()\n     );\n     Assert.assertEquals(\""s3_zip\"", segment.getLoadSpec().get(\""type\""));\n \n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java\nindex 3210a26cc584..8f898848c87d 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3StorageConnectorProviderTest.java\n@@ -148,7 +148,7 @@ public void configure(Binder binder)\n         new InjectableValues.Std()\n             .addValue(\n                 ServerSideEncryptingAmazonS3.class,\n-                new ServerSideEncryptingAmazonS3(null, new NoopServerSideEncryption())\n+                new ServerSideEncryptingAmazonS3(null, new NoopServerSideEncryption(), new S3TransferConfig())\n             )\n             .addValue(\n                 S3UploadManager.class,\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java\nindex 011dc4888456..4fa8cf7e044f 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TaskLogsTest.java\n@@ -29,7 +29,6 @@\n import com.amazonaws.services.s3.model.Owner;\n import com.amazonaws.services.s3.model.Permission;\n import com.amazonaws.services.s3.model.PutObjectRequest;\n-import com.amazonaws.services.s3.model.PutObjectResult;\n import com.amazonaws.services.s3.model.S3Object;\n import com.amazonaws.services.s3.model.S3ObjectSummary;\n import com.google.common.base.Optional;\n@@ -123,11 +122,9 @@ public void testTaskLogsPushWithAclEnabled() throws Exception\n   }\n   \n   @Test\n-  public void test_pushTaskStatus() throws IOException \n+  public void test_pushTaskStatus() throws IOException, InterruptedException\n   {\n-    EasyMock.expect(s3Client.putObject(EasyMock.anyObject(PutObjectRequest.class)))\n-        .andReturn(new PutObjectResult())\n-        .once();\n+    s3Client.upload(EasyMock.anyObject(PutObjectRequest.class));\n \n     EasyMock.replay(s3Client);\n \n@@ -148,12 +145,11 @@ public void test_pushTaskStatus() throws IOException\n   }\n \n   @Test\n-  public void test_pushTaskPayload() throws IOException\n+  public void test_pushTaskPayload() throws IOException, InterruptedException\n   {\n     Capture<PutObjectRequest> putObjectRequestCapture = Capture.newInstance(CaptureType.FIRST);\n-    EasyMock.expect(s3Client.putObject(EasyMock.capture(putObjectRequestCapture)))\n-        .andReturn(new PutObjectResult())\n-        .once();\n+    s3Client.upload(EasyMock.capture(putObjectRequestCapture));\n+    EasyMock.expectLastCall().once();\n \n     EasyMock.replay(s3Client);\n \n@@ -617,9 +613,8 @@ private S3TaskLogs getS3TaskLogs()\n \n   private List<Grant> testPushInternal(boolean disableAcl, String ownerId, String ownerDisplayName) throws Exception\n   {\n-    EasyMock.expect(s3Client.putObject(EasyMock.anyObject()))\n-            .andReturn(new PutObjectResult())\n-            .once();\n+    s3Client.upload(EasyMock.anyObject(PutObjectRequest.class));\n+    EasyMock.expectLastCall().once();\n \n     AccessControlList aclExpected = new AccessControlList();\n     aclExpected.setOwner(new Owner(ownerId, ownerDisplayName));\n@@ -628,9 +623,8 @@ private List<Grant> testPushInternal(boolean disableAcl, String ownerId, String\n             .andReturn(aclExpected)\n             .once();\n \n-    EasyMock.expect(s3Client.putObject(EasyMock.anyObject(PutObjectRequest.class)))\n-            .andReturn(new PutObjectResult())\n-            .once();\n+    s3Client.upload(EasyMock.anyObject(PutObjectRequest.class));\n+    EasyMock.expectLastCall().once();\n \n     EasyMock.replay(s3Client);\n \n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TransferConfigTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TransferConfigTest.java\nnew file mode 100644\nindex 000000000000..3cbace59475c\n--- /dev/null\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/S3TransferConfigTest.java\n@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.storage.s3;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class S3TransferConfigTest\n+{\n+  @Test\n+  public void testDefaultValues()\n+  {\n+    S3TransferConfig config = new S3TransferConfig();\n+    Assert.assertTrue(config.isUseTransferManager());\n+    Assert.assertEquals(20 * 1024 * 1024L, config.getMinimumUploadPartSize());\n+    Assert.assertEquals(20 * 1024 * 1024L, config.getMultipartUploadThreshold());\n+  }\n+\n+  @Test\n+  public void testSetUseTransferManager()\n+  {\n+    S3TransferConfig config = new S3TransferConfig();\n+    config.setUseTransferManager(true);\n+    Assert.assertTrue(config.isUseTransferManager());\n+  }\n+\n+  @Test\n+  public void testSetMinimumUploadPartSize()\n+  {\n+    S3TransferConfig config = new S3TransferConfig();\n+    config.setMinimumUploadPartSize(10 * 1024 * 1024L);\n+    Assert.assertEquals(10 * 1024 * 1024L, config.getMinimumUploadPartSize());\n+  }\n+\n+  @Test\n+  public void testSetMultipartUploadThreshold()\n+  {\n+    S3TransferConfig config = new S3TransferConfig();\n+    config.setMultipartUploadThreshold(10 * 1024 * 1024L);\n+    Assert.assertEquals(10 * 1024 * 1024L, config.getMultipartUploadThreshold());\n+  }\n+}\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3Test.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3Test.java\nnew file mode 100644\nindex 000000000000..75e1a72da0d2\n--- /dev/null\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/ServerSideEncryptingAmazonS3Test.java\n@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.storage.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.model.PutObjectRequest;\n+import com.amazonaws.services.s3.model.PutObjectResult;\n+import com.amazonaws.services.s3.transfer.TransferManager;\n+import com.amazonaws.services.s3.transfer.Upload;\n+import org.easymock.EasyMock;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+import java.lang.reflect.Field;\n+\n+\n+\n+public class ServerSideEncryptingAmazonS3Test\n+{\n+  private AmazonS3 mockAmazonS3;\n+  private ServerSideEncryption mockServerSideEncryption;\n+  private S3TransferConfig mockTransferConfig;\n+  private TransferManager mockTransferManager;\n+\n+  @Before\n+  public void setup()\n+  {\n+    mockAmazonS3 = EasyMock.createMock(AmazonS3.class);\n+    mockServerSideEncryption = EasyMock.createMock(ServerSideEncryption.class);\n+    mockTransferConfig = EasyMock.createMock(S3TransferConfig.class);\n+    mockTransferManager = EasyMock.createMock(TransferManager.class);\n+  }\n+\n+  @Test\n+  public void testConstructor_WithTransferManager() throws NoSuchFieldException, IllegalAccessException\n+  {\n+    EasyMock.expect(mockTransferConfig.isUseTransferManager()).andReturn(true);\n+    EasyMock.expect(mockTransferConfig.getMinimumUploadPartSize()).andReturn(5L);\n+    EasyMock.expect(mockTransferConfig.getMultipartUploadThreshold()).andReturn(10L);\n+    EasyMock.replay(mockTransferConfig);\n+\n+    ServerSideEncryptingAmazonS3 s3 = new ServerSideEncryptingAmazonS3(mockAmazonS3, mockServerSideEncryption, mockTransferConfig);\n+\n+    Field transferManagerField = ServerSideEncryptingAmazonS3.class.getDeclaredField(\""transferManager\"");\n+    transferManagerField.setAccessible(true);\n+    Object transferManager = transferManagerField.get(s3);\n+\n+    Assert.assertNotNull(\""TransferManager should be initialized\"", transferManager);\n+    Assert.assertNotNull(s3);\n+    EasyMock.verify(mockTransferConfig);\n+  }\n+\n+  @Test\n+  public void testConstructor_WithoutTransferManager() throws NoSuchFieldException, IllegalAccessException\n+  {\n+\n+    EasyMock.expect(mockTransferConfig.isUseTransferManager()).andReturn(false);\n+    EasyMock.replay(mockTransferConfig);\n+\n+    ServerSideEncryptingAmazonS3 s3 = new ServerSideEncryptingAmazonS3(mockAmazonS3, mockServerSideEncryption, mockTransferConfig);\n+\n+    Field transferManagerField = ServerSideEncryptingAmazonS3.class.getDeclaredField(\""transferManager\"");\n+    transferManagerField.setAccessible(true);\n+    Object transferManager = transferManagerField.get(s3);\n+\n+    Assert.assertNull(\""TransferManager should not be initialized\"", transferManager);\n+    Assert.assertNotNull(s3);\n+    EasyMock.verify(mockTransferConfig);\n+  }\n+\n+  @Test\n+  public void testUpload_WithoutTransferManager() throws InterruptedException\n+  {\n+    PutObjectRequest originalRequest = new PutObjectRequest(\""bucket\"", \""key\"", \""file\"");\n+    PutObjectRequest decoratedRequest = new PutObjectRequest(\""bucket\"", \""key\"", \""file-encrypted\"");\n+    PutObjectResult mockResult = new PutObjectResult();\n+\n+    EasyMock.expect(mockTransferConfig.isUseTransferManager()).andReturn(false);\n+    EasyMock.replay(mockTransferConfig);\n+\n+    EasyMock.expect(mockServerSideEncryption.decorate(originalRequest)).andReturn(decoratedRequest);\n+    EasyMock.replay(mockServerSideEncryption);\n+\n+    EasyMock.expect(mockAmazonS3.putObject(decoratedRequest)).andReturn(mockResult).once();\n+    EasyMock.replay(mockAmazonS3);\n+\n+    ServerSideEncryptingAmazonS3 s3 = new ServerSideEncryptingAmazonS3(mockAmazonS3, mockServerSideEncryption, mockTransferConfig);\n+    s3.upload(originalRequest);\n+\n+    EasyMock.verify(mockServerSideEncryption);\n+    EasyMock.verify(mockAmazonS3);\n+    EasyMock.verify(mockTransferConfig);\n+  }\n+\n+  @Test\n+  public void testUpload_WithTransferManager() throws InterruptedException, NoSuchFieldException, IllegalAccessException\n+  {\n+    PutObjectRequest originalRequest = new PutObjectRequest(\""bucket\"", \""key\"", \""file\"");\n+    PutObjectRequest decoratedRequest = new PutObjectRequest(\""bucket\"", \""key\"", \""file-encrypted\"");\n+    Upload mockUpload = EasyMock.createMock(Upload.class);\n+\n+    EasyMock.expect(mockTransferConfig.isUseTransferManager()).andReturn(true).once();\n+    EasyMock.expect(mockTransferConfig.getMinimumUploadPartSize()).andReturn(5242880L).once(); // 5 MB\n+    EasyMock.expect(mockTransferConfig.getMultipartUploadThreshold()).andReturn(10485760L).once(); // 10 MB\n+    EasyMock.replay(mockTransferConfig);\n+\n+    EasyMock.expect(mockServerSideEncryption.decorate(originalRequest)).andReturn(decoratedRequest);\n+    EasyMock.replay(mockServerSideEncryption);\n+\n+    EasyMock.expect(mockTransferManager.upload(decoratedRequest)).andReturn(mockUpload);\n+    EasyMock.replay(mockTransferManager);\n+\n+    mockUpload.waitForCompletion();\n+    EasyMock.expectLastCall();\n+    EasyMock.replay(mockUpload);\n+\n+    ServerSideEncryptingAmazonS3 s3 = new ServerSideEncryptingAmazonS3(mockAmazonS3, mockServerSideEncryption, mockTransferConfig);\n+\n+    Field transferManagerField = ServerSideEncryptingAmazonS3.class.getDeclaredField(\""transferManager\"");\n+    transferManagerField.setAccessible(true);\n+    transferManagerField.set(s3, mockTransferManager);\n+\n+    s3.upload(originalRequest);\n+\n+    EasyMock.verify(mockServerSideEncryption);\n+    EasyMock.verify(mockTransferManager);\n+    EasyMock.verify(mockUpload);\n+  }\n+}\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java\nindex 3685fc6fa19b..fefcb8c3c38b 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/TestAWSCredentialsProvider.java\n@@ -67,7 +67,7 @@ public void testWithFixedAWSKeys()\n         new AWSProxyConfig(),\n         new AWSEndpointConfig(),\n         new AWSClientConfig(),\n-        new S3StorageConfig(new NoopServerSideEncryption())\n+        new S3StorageConfig(new NoopServerSideEncryption(), null)\n     );\n \n     s3Module.getAmazonS3Client(\n@@ -102,7 +102,7 @@ public void testWithFileSessionCredentials() throws IOException\n         new AWSProxyConfig(),\n         new AWSEndpointConfig(),\n         new AWSClientConfig(),\n-        new S3StorageConfig(new NoopServerSideEncryption())\n+        new S3StorageConfig(new NoopServerSideEncryption(), null)\n     );\n \n     s3Module.getAmazonS3Client(\n\ndiff --git a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java\nindex ead65a89f771..437f635433bd 100644\n--- a/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java\n+++ b/extensions-core/s3-extensions/src/test/java/org/apache/druid/storage/s3/output/RetryableS3OutputStreamTest.java\n@@ -36,6 +36,7 @@\n import org.apache.druid.java.util.metrics.StubServiceEmitter;\n import org.apache.druid.query.DruidProcessingConfigTest;\n import org.apache.druid.storage.s3.NoopServerSideEncryption;\n+import org.apache.druid.storage.s3.S3TransferConfig;\n import org.apache.druid.storage.s3.ServerSideEncryptingAmazonS3;\n import org.easymock.EasyMock;\n import org.junit.Assert;\n@@ -230,7 +231,7 @@ private static class TestAmazonS3 extends ServerSideEncryptingAmazonS3\n \n     private TestAmazonS3(int totalUploadFailures)\n     {\n-      super(EasyMock.createMock(AmazonS3.class), new NoopServerSideEncryption());\n+      super(EasyMock.createMock(AmazonS3.class), new NoopServerSideEncryption(), new S3TransferConfig());\n       this.uploadFailuresLeft = totalUploadFailures;\n     }\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__druid-17652"", ""pr_id"": 17652, ""issue_id"": 17651, ""repo"": ""apache/druid"", ""problem_statement"": ""Query Failure due to ResultLevelCache Population OOM\nPlease provide a detailed title (e.g. \""Broker crashes when using TopN query with Bound filter\"" instead of just \""Broker crashes\"").\n\n### Affected Version\n\n31.0.1\n\n### Description\n\nCurrently, if a query's result set size is large enough, the result set serializer can OOM [here](https://github.com/apache/druid/blob/master/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java#L294) when allocating large buffers. Not only will this affect one-off queries, but this causes a crash-loop on Broker nodes if they are attempting to make/cache large SegmentMetadata queries on boot."", ""issue_word_count"": 103, ""test_files_count"": 2, ""non_test_files_count"": 2, ""pr_changed_files"": [""processing/src/main/java/org/apache/druid/io/LimitedOutputStream.java"", ""processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java"", ""server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java"", ""server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java""], ""pr_changed_test_files"": [""processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java"", ""server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java""], ""base_commit"": ""8f6566285ffbc64d1d7237de0a29a6b6980c8628"", ""head_commit"": ""39089bbace3cef181cb83e72a58d27f5cd5c04ca"", ""repo_url"": ""https://github.com/apache/druid/pull/17652"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17652"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-27T18:52:09.000Z"", ""patch"": ""diff --git a/processing/src/main/java/org/apache/druid/io/LimitedOutputStream.java b/processing/src/main/java/org/apache/druid/io/LimitedOutputStream.java\nindex 6d27abb42739..043bd53d5267 100644\n--- a/processing/src/main/java/org/apache/druid/io/LimitedOutputStream.java\n+++ b/processing/src/main/java/org/apache/druid/io/LimitedOutputStream.java\n@@ -22,13 +22,14 @@\n import org.apache.druid.error.DruidException;\n import org.apache.druid.java.util.common.IOE;\n \n+import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n import java.io.OutputStream;\n import java.util.function.Function;\n \n /**\n  * An {@link OutputStream} that limits how many bytes can be written. Throws {@link IOException} if the limit\n- * is exceeded.\n+ * is exceeded. *Not* thread-safe.\n  */\n public class LimitedOutputStream extends OutputStream\n {\n@@ -88,6 +89,14 @@ public void close() throws IOException\n     out.close();\n   }\n \n+  public byte[] toByteArray()\n+  {\n+    if (!(out instanceof ByteArrayOutputStream)) {\n+      throw new UnsupportedOperationException(out.getClass().getName() + \""does not implement toByteArray()\"");\n+    }\n+    return ((ByteArrayOutputStream) out).toByteArray();\n+  }\n+\n   private void plus(final int n) throws IOException\n   {\n     written += n;\n\ndiff --git a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\nindex dedfb0028b77..8cc6348a7c8a 100644\n--- a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\n+++ b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\n@@ -30,6 +30,7 @@\n import org.apache.druid.client.cache.Cache;\n import org.apache.druid.client.cache.Cache.NamedKey;\n import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.io.LimitedOutputStream;\n import org.apache.druid.java.util.common.RE;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.guava.Sequence;\n@@ -152,6 +153,8 @@ public void after(boolean isDone, Throwable thrown)\n                   // The resultset identifier and its length is cached along with the resultset\n                   resultLevelCachePopulator.populateResults();\n                   log.debug(\""Cache population complete for query %s\"", query.getId());\n+                } else { // thrown == null && !resultLevelCachePopulator.isShouldPopulate()\n+                  log.error(\""Failed (gracefully) to populate result level cache for query %s\"", query.getId());\n                 }\n                 resultLevelCachePopulator.stopPopulating();\n               }\n@@ -233,8 +236,8 @@ private ResultLevelCachePopulator createResultLevelCachePopulator(\n       try {\n         //   Save the resultSetId and its length\n         resultLevelCachePopulator.cacheObjectStream.write(ByteBuffer.allocate(Integer.BYTES)\n-                                                                    .putInt(resultSetId.length())\n-                                                                    .array());\n+                                                              .putInt(resultSetId.length())\n+                                                              .array());\n         resultLevelCachePopulator.cacheObjectStream.write(StringUtils.toUtf8(resultSetId));\n       }\n       catch (IOException ioe) {\n@@ -255,7 +258,7 @@ private class ResultLevelCachePopulator\n     private final Cache.NamedKey key;\n     private final CacheConfig cacheConfig;\n     @Nullable\n-    private ByteArrayOutputStream cacheObjectStream;\n+    private LimitedOutputStream cacheObjectStream;\n \n     private ResultLevelCachePopulator(\n         Cache cache,\n@@ -270,7 +273,14 @@ private ResultLevelCachePopulator(\n       this.serialiers = mapper.getSerializerProviderInstance();\n       this.key = key;\n       this.cacheConfig = cacheConfig;\n-      this.cacheObjectStream = shouldPopulate ? new ByteArrayOutputStream() : null;\n+      this.cacheObjectStream = shouldPopulate ? new LimitedOutputStream(\n+          new ByteArrayOutputStream(),\n+          cacheConfig.getResultLevelCacheLimit(), limit -> StringUtils.format(\n+          \""resultLevelCacheLimit[%,d] exceeded. \""\n+          + \""Max ResultLevelCacheLimit for cache exceeded. Result caching failed.\"",\n+          limit\n+      )\n+      ) : null;\n     }\n \n     boolean isShouldPopulate()\n@@ -289,12 +299,8 @@ private void cacheResultEntry(\n     )\n     {\n       Preconditions.checkNotNull(cacheObjectStream, \""cacheObjectStream\"");\n-      int cacheLimit = cacheConfig.getResultLevelCacheLimit();\n       try (JsonGenerator gen = mapper.getFactory().createGenerator(cacheObjectStream)) {\n         JacksonUtils.writeObjectUsingSerializerProvider(gen, serialiers, cacheFn.apply(resultEntry));\n-        if (cacheLimit > 0 && cacheObjectStream.size() > cacheLimit) {\n-          stopPopulating();\n-        }\n       }\n       catch (IOException ex) {\n         log.error(ex, \""Failed to retrieve entry to be cached. Result Level caching will not be performed!\"");\n"", ""test_patch"": ""diff --git a/processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java b/processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java\nindex a11b63149710..54757570d6f6 100644\n--- a/processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java\n+++ b/processing/src/test/java/org/apache/druid/io/LimitedOutputStreamTest.java\n@@ -27,6 +27,7 @@\n import org.junit.internal.matchers.ThrowableMessageMatcher;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.OutputStream;\n \n@@ -65,6 +66,26 @@ public void test_limitThree() throws IOException\n     }\n   }\n \n+  @Test\n+  public void test_toByteArray() throws IOException\n+  {\n+    try (final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+         final LimitedOutputStream stream =\n+             new LimitedOutputStream(baos, 3, LimitedOutputStreamTest::makeErrorMessage)) {\n+      stream.write('a');\n+      stream.write(new byte[]{'b'});\n+      stream.write(new byte[]{'c'}, 0, 1);\n+\n+      MatcherAssert.assertThat(stream.toByteArray(), CoreMatchers.equalTo(new byte[]{'a', 'b', 'c'}));\n+    }\n+\n+    try (final DataOutputStream dos = new DataOutputStream(new ByteArrayOutputStream());\n+         final LimitedOutputStream stream =\n+             new LimitedOutputStream(dos, 3, LimitedOutputStreamTest::makeErrorMessage)) {\n+      Assert.assertThrows(UnsupportedOperationException.class, stream::toByteArray);\n+    }\n+  }\n+\n   private static String makeErrorMessage(final long limit)\n   {\n     return StringUtils.format(\""Limit[%d] exceeded\"", limit);\n\ndiff --git a/server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java b/server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java\nindex 6245509465c1..3cb4ae528e67 100644\n--- a/server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java\n+++ b/server/src/test/java/org/apache/druid/query/ResultLevelCachingQueryRunnerTest.java\n@@ -39,6 +39,7 @@\n public class ResultLevelCachingQueryRunnerTest extends QueryRunnerBasedOnClusteredClientTestBase\n {\n   private Cache cache;\n+  private static final int DEFAULT_CACHE_ENTRY_MAX_SIZE = Integer.MAX_VALUE;\n \n   @Before\n   public void setup()\n@@ -58,7 +59,7 @@ public void testNotPopulateAndNotUse()\n     prepareCluster(10);\n     final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner1 = createQueryRunner(\n-        newCacheConfig(false, false),\n+        newCacheConfig(false, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -72,7 +73,7 @@ public void testNotPopulateAndNotUse()\n     Assert.assertEquals(0, cache.getStats().getNumMisses());\n \n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner2 = createQueryRunner(\n-        newCacheConfig(false, false),\n+        newCacheConfig(false, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -93,7 +94,7 @@ public void testPopulateAndNotUse()\n     prepareCluster(10);\n     final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner1 = createQueryRunner(\n-        newCacheConfig(true, false),\n+        newCacheConfig(true, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -107,7 +108,7 @@ public void testPopulateAndNotUse()\n     Assert.assertEquals(0, cache.getStats().getNumMisses());\n \n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner2 = createQueryRunner(\n-        newCacheConfig(true, false),\n+        newCacheConfig(true, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -128,7 +129,7 @@ public void testNotPopulateAndUse()\n     prepareCluster(10);\n     final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner1 = createQueryRunner(\n-        newCacheConfig(false, false),\n+        newCacheConfig(false, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -142,7 +143,7 @@ public void testNotPopulateAndUse()\n     Assert.assertEquals(0, cache.getStats().getNumMisses());\n \n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner2 = createQueryRunner(\n-        newCacheConfig(false, true),\n+        newCacheConfig(false, true, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -163,7 +164,7 @@ public void testPopulateAndUse()\n     prepareCluster(10);\n     final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner1 = createQueryRunner(\n-        newCacheConfig(true, true),\n+        newCacheConfig(true, true, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -177,7 +178,7 @@ public void testPopulateAndUse()\n     Assert.assertEquals(1, cache.getStats().getNumMisses());\n \n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner2 = createQueryRunner(\n-        newCacheConfig(true, true),\n+        newCacheConfig(true, true, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -192,6 +193,41 @@ public void testPopulateAndUse()\n     Assert.assertEquals(1, cache.getStats().getNumMisses());\n   }\n \n+  @Test\n+  public void testNoPopulateIfEntrySizeExceedsMaximum()\n+  {\n+    prepareCluster(10);\n+    final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n+    final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner1 = createQueryRunner(\n+        newCacheConfig(true, true, 128),\n+        query\n+    );\n+\n+    final Sequence<Result<TimeseriesResultValue>> sequence1 = queryRunner1.run(\n+        QueryPlus.wrap(query),\n+        responseContext()\n+    );\n+    final List<Result<TimeseriesResultValue>> results1 = sequence1.toList();\n+    Assert.assertEquals(0, cache.getStats().getNumHits());\n+    Assert.assertEquals(0, cache.getStats().getNumEntries());\n+    Assert.assertEquals(1, cache.getStats().getNumMisses());\n+\n+    final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner2 = createQueryRunner(\n+        newCacheConfig(true, true, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n+        query\n+    );\n+\n+    final Sequence<Result<TimeseriesResultValue>> sequence2 = queryRunner2.run(\n+        QueryPlus.wrap(query),\n+        responseContext()\n+    );\n+    final List<Result<TimeseriesResultValue>> results2 = sequence2.toList();\n+    Assert.assertEquals(results1, results2);\n+    Assert.assertEquals(0, cache.getStats().getNumHits());\n+    Assert.assertEquals(1, cache.getStats().getNumEntries());\n+    Assert.assertEquals(2, cache.getStats().getNumMisses());\n+  }\n+\n   @Test\n   public void testPopulateCacheWhenQueryThrowExceptionShouldNotCache()\n   {\n@@ -206,7 +242,7 @@ public void testPopulateCacheWhenQueryThrowExceptionShouldNotCache()\n \n     final Query<Result<TimeseriesResultValue>> query = timeseriesQuery(BASE_SCHEMA_INFO.getDataInterval());\n     final ResultLevelCachingQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n-        newCacheConfig(true, false),\n+        newCacheConfig(true, false, DEFAULT_CACHE_ENTRY_MAX_SIZE),\n         query\n     );\n \n@@ -249,7 +285,11 @@ private <T> ResultLevelCachingQueryRunner<T> createQueryRunner(\n     );\n   }\n \n-  private CacheConfig newCacheConfig(boolean populateResultLevelCache, boolean useResultLevelCache)\n+  private CacheConfig newCacheConfig(\n+      boolean populateResultLevelCache,\n+      boolean useResultLevelCache,\n+      int resultLevelCacheLimit\n+  )\n   {\n     return new CacheConfig()\n     {\n@@ -264,6 +304,12 @@ public boolean isUseResultLevelCache()\n       {\n         return useResultLevelCache;\n       }\n+\n+      @Override\n+      public int getResultLevelCacheLimit()\n+      {\n+        return resultLevelCacheLimit;\n+      }\n     };\n   }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__druid-17568"", ""pr_id"": 17568, ""issue_id"": 17575, ""repo"": ""apache/druid"", ""problem_statement"": ""Remove SQL 'incompatible' modes\nIssue for tracking removal of SQL incompatible configs from Druid, including `druid.generic.useDefaultValueForNull`, `druid.generic.useThreeValueLogicForNativeFilters`, and `druid.expressions.useStrictBooleans`.\r\n\r\nPrevious issue about making SQL compatible modes the default in Druid 28: #14154\r\nPR deprecating modes in docs: #15713\r\ndev list thread (though missing at least one reply for some reason): https://lists.apache.org/thread/016nqtdbjlkfy3r5bnxc2d4rmt79237j"", ""issue_word_count"": 62, ""test_files_count"": 8, ""non_test_files_count"": 11, ""pr_changed_files"": [""indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java"", ""processing/src/main/java/org/apache/druid/common/config/NullHandling.java"", ""processing/src/main/java/org/apache/druid/math/expr/BinaryEvalOpExprBase.java"", ""processing/src/main/java/org/apache/druid/math/expr/BinaryLogicalOperatorExpr.java"", ""processing/src/main/java/org/apache/druid/math/expr/ExprEval.java"", ""processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessing.java"", ""processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessingConfig.java"", ""processing/src/main/java/org/apache/druid/math/expr/UnaryOperatorExpr.java"", ""processing/src/main/java/org/apache/druid/math/expr/vector/VectorComparisonProcessors.java"", ""processing/src/main/java/org/apache/druid/math/expr/vector/VectorProcessors.java"", ""processing/src/main/java/org/apache/druid/segment/filter/NotFilter.java"", ""processing/src/test/java/org/apache/druid/math/expr/EvalTest.java"", ""processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java"", ""processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java"", ""processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterNonStrictBooleansTest.java"", ""processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java"", ""processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java"", ""sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRule.java"", ""sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java""], ""pr_changed_test_files"": [""indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java"", ""processing/src/test/java/org/apache/druid/math/expr/EvalTest.java"", ""processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java"", ""processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java"", ""processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterNonStrictBooleansTest.java"", ""processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java"", ""processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java"", ""sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java""], ""base_commit"": ""bb4416a17b7ace1eaa3b59ac61607700e7e600c5"", ""head_commit"": ""8739074b99bc043f99d6ca855ea090fbde69b6f7"", ""repo_url"": ""https://github.com/apache/druid/pull/17568"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__druid/17568"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-18T02:49:16.000Z"", ""patch"": ""diff --git a/processing/src/main/java/org/apache/druid/common/config/NullHandling.java b/processing/src/main/java/org/apache/druid/common/config/NullHandling.java\nindex b98d81421d25..578c64acbc56 100644\n--- a/processing/src/main/java/org/apache/druid/common/config/NullHandling.java\n+++ b/processing/src/main/java/org/apache/druid/common/config/NullHandling.java\n@@ -22,7 +22,6 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Strings;\n import com.google.inject.Inject;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.BitmapResultFactory;\n import org.apache.druid.query.filter.DimFilter;\n import org.apache.druid.query.filter.ValueMatcher;\n@@ -130,8 +129,7 @@ public static boolean sqlCompatible()\n   public static boolean useThreeValueLogic()\n   {\n     return sqlCompatible() &&\n-           INSTANCE.isUseThreeValueLogicForNativeFilters() &&\n-           ExpressionProcessing.useStrictBooleans();\n+           INSTANCE.isUseThreeValueLogicForNativeFilters();\n   }\n \n   @Nullable\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/BinaryEvalOpExprBase.java b/processing/src/main/java/org/apache/druid/math/expr/BinaryEvalOpExprBase.java\nindex 2104dcc45db7..6b7bb21a1f7e 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/BinaryEvalOpExprBase.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/BinaryEvalOpExprBase.java\n@@ -23,7 +23,6 @@\n import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.IAE;\n import org.apache.druid.java.util.common.StringUtils;\n-import org.apache.druid.segment.column.Types;\n \n import javax.annotation.Nullable;\n import java.util.Objects;\n@@ -206,9 +205,6 @@ public ExprEval eval(ObjectBinding bindings)\n         result = evalDouble(leftVal.asDouble(), rightVal.asDouble());\n         break;\n     }\n-    if (!ExpressionProcessing.useStrictBooleans() && !type.is(ExprType.STRING) && !type.isArray()) {\n-      return ExprEval.ofBoolean(result, type);\n-    }\n     return ExprEval.ofLongBoolean(result);\n   }\n \n@@ -224,11 +220,7 @@ public ExprEval eval(ObjectBinding bindings)\n   @Override\n   public ExpressionType getOutputType(InputBindingInspector inspector)\n   {\n-    ExpressionType implicitCast = super.getOutputType(inspector);\n-    if (ExpressionProcessing.useStrictBooleans() || Types.isNullOr(implicitCast, ExprType.STRING)) {\n-      return ExpressionType.LONG;\n-    }\n-    return implicitCast;\n+    return ExpressionType.LONG;\n   }\n \n   @Override\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/BinaryLogicalOperatorExpr.java b/processing/src/main/java/org/apache/druid/math/expr/BinaryLogicalOperatorExpr.java\nindex 13bb4e7f52f1..880271ff1de1 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/BinaryLogicalOperatorExpr.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/BinaryLogicalOperatorExpr.java\n@@ -340,9 +340,6 @@ protected BinaryOpExprBase copy(Expr left, Expr right)\n   public ExprEval eval(ObjectBinding bindings)\n   {\n     ExprEval leftVal = left.eval(bindings);\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return leftVal.asBoolean() ? right.eval(bindings) : leftVal;\n-    }\n \n     // if left is false, always false\n     if (leftVal.value() != null && !leftVal.asBoolean()) {\n@@ -376,9 +373,7 @@ public ExprEval eval(ObjectBinding bindings)\n   @Override\n   public boolean canVectorize(InputBindingInspector inspector)\n   {\n-    return ExpressionProcessing.useStrictBooleans() &&\n-           inspector.areSameTypes(left, right) &&\n-           inspector.canVectorize(left, right);\n+    return inspector.areSameTypes(left, right) && inspector.canVectorize(left, right);\n   }\n \n   @Override\n@@ -391,9 +386,6 @@ public <T> ExprVectorProcessor<T> asVectorProcessor(VectorInputBindingInspector\n   @Override\n   public ExpressionType getOutputType(InputBindingInspector inspector)\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return super.getOutputType(inspector);\n-    }\n     return ExpressionType.LONG;\n   }\n }\n@@ -415,9 +407,6 @@ protected BinaryOpExprBase copy(Expr left, Expr right)\n   public ExprEval eval(ObjectBinding bindings)\n   {\n     ExprEval leftVal = left.eval(bindings);\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return leftVal.asBoolean() ? leftVal : right.eval(bindings);\n-    }\n \n     // if left is true, always true\n     if (leftVal.value() != null && leftVal.asBoolean()) {\n@@ -454,9 +443,7 @@ public ExprEval eval(ObjectBinding bindings)\n   public boolean canVectorize(InputBindingInspector inspector)\n   {\n \n-    return ExpressionProcessing.useStrictBooleans() &&\n-           inspector.areSameTypes(left, right) &&\n-           inspector.canVectorize(left, right);\n+    return inspector.areSameTypes(left, right) && inspector.canVectorize(left, right);\n   }\n \n   @Override\n@@ -469,9 +456,6 @@ public <T> ExprVectorProcessor<T> asVectorProcessor(VectorInputBindingInspector\n   @Override\n   public ExpressionType getOutputType(InputBindingInspector inspector)\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return super.getOutputType(inspector);\n-    }\n     return ExpressionType.LONG;\n   }\n }\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/ExprEval.java b/processing/src/main/java/org/apache/druid/math/expr/ExprEval.java\nindex a18ca8b61e2a..3c2e5630df47 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/ExprEval.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/ExprEval.java\n@@ -242,11 +242,7 @@ private static Class convertType(@Nullable Class existing, Class next)\n     if (Number.class.isAssignableFrom(next) || next == String.class || next == Boolean.class) {\n       // coerce booleans\n       if (next == Boolean.class) {\n-        if (ExpressionProcessing.useStrictBooleans()) {\n-          next = Long.class;\n-        } else {\n-          next = String.class;\n-        }\n+        next = Long.class;\n       }\n       if (existing == null) {\n         return next;\n@@ -350,28 +346,6 @@ public static ExprEval ofArray(ExpressionType outputType, @Nullable Object[] val\n     return new ArrayExprEval(outputType, value);\n   }\n \n-  /**\n-   * Convert a boolean back into native expression type\n-   *\n-   * Do not use this method unless {@link ExpressionProcessing#useStrictBooleans()} is set to false.\n-   * {@link ExpressionType#LONG} is the Druid boolean unless this mode is enabled, so use {@link #ofLongBoolean}\n-   * instead.\n-   */\n-  @Deprecated\n-  public static ExprEval ofBoolean(boolean value, ExpressionType type)\n-  {\n-    switch (type.getType()) {\n-      case DOUBLE:\n-        return of(Evals.asDouble(value));\n-      case LONG:\n-        return ofLongBoolean(value);\n-      case STRING:\n-        return of(String.valueOf(value));\n-      default:\n-        throw new Types.InvalidCastBooleanException(type);\n-    }\n-  }\n-\n   /**\n    * Convert a boolean into a long expression type\n    */\n@@ -421,10 +395,7 @@ public static ExprEval bestEffortOf(@Nullable Object val)\n       return new LongExprEval((Number) val);\n     }\n     if (val instanceof Boolean) {\n-      if (ExpressionProcessing.useStrictBooleans()) {\n-        return ofLongBoolean((Boolean) val);\n-      }\n-      return new StringExprEval(String.valueOf(val));\n+      return ofLongBoolean((Boolean) val);\n     }\n     if (val instanceof Long[]) {\n       final Long[] inputArray = (Long[]) val;\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessing.java b/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessing.java\nindex 9a4d2ef46942..2387cea909f1 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessing.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessing.java\n@@ -48,12 +48,6 @@ public static void initializeForTests()\n     INSTANCE = new ExpressionProcessingConfig(null, null, null, null);\n   }\n \n-  @VisibleForTesting\n-  public static void initializeForStrictBooleansTests(boolean useStrict)\n-  {\n-    INSTANCE = new ExpressionProcessingConfig(useStrict, null, null, null);\n-  }\n-\n   @VisibleForTesting\n   public static void initializeForHomogenizeNullMultiValueStrings()\n   {\n@@ -66,15 +60,6 @@ public static void initializeForFallback()\n     INSTANCE = new ExpressionProcessingConfig(null, null, null, true);\n   }\n \n-  /**\n-   * All boolean expressions are {@link ExpressionType#LONG}\n-   */\n-  public static boolean useStrictBooleans()\n-  {\n-    checkInitialized();\n-    return INSTANCE.isUseStrictBooleans();\n-  }\n-\n   /**\n    * All {@link ExprType#ARRAY} values will be converted to {@link ExpressionType#STRING} by their column selectors\n    * (not within expression processing) to be treated as multi-value strings instead of native arrays.\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessingConfig.java b/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessingConfig.java\nindex 3d235fe3ddaa..a0d58eb48064 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessingConfig.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/ExpressionProcessingConfig.java\n@@ -30,6 +30,7 @@ public class ExpressionProcessingConfig\n {\n   private static final Logger LOG = new Logger(ExpressionProcessingConfig.class);\n \n+  @Deprecated\n   public static final String NULL_HANDLING_LEGACY_LOGICAL_OPS_STRING = \""druid.expressions.useStrictBooleans\"";\n   // Coerce arrays to multi value strings\n   public static final String PROCESS_ARRAYS_AS_MULTIVALUE_STRINGS_CONFIG_STRING =\n@@ -39,9 +40,6 @@ public class ExpressionProcessingConfig\n       \""druid.expressions.homogenizeNullMultiValueStringArrays\"";\n   public static final String ALLOW_VECTORIZE_FALLBACK = \""druid.expressions.allowVectorizeFallback\"";\n \n-  @JsonProperty(\""useStrictBooleans\"")\n-  private final boolean useStrictBooleans;\n-\n   @JsonProperty(\""processArraysAsMultiValueStrings\"")\n   private final boolean processArraysAsMultiValueStrings;\n \n@@ -51,9 +49,13 @@ public class ExpressionProcessingConfig\n   @JsonProperty(\""allowVectorizeFallback\"")\n   private final boolean allowVectorizeFallback;\n \n+  @Deprecated\n+  @JsonProperty(\""useStrictBooleans\"")\n+  private final boolean useStrictBooleans;\n+\n   @JsonCreator\n   public ExpressionProcessingConfig(\n-      @JsonProperty(\""useStrictBooleans\"") @Nullable Boolean useStrictBooleans,\n+      @Deprecated @JsonProperty(\""useStrictBooleans\"") @Nullable Boolean useStrictBooleans,\n       @JsonProperty(\""processArraysAsMultiValueStrings\"") @Nullable Boolean processArraysAsMultiValueStrings,\n       @JsonProperty(\""homogenizeNullMultiValueStringArrays\"") @Nullable Boolean homogenizeNullMultiValueStringArrays,\n       @JsonProperty(\""allowVectorizeFallback\"") @Nullable Boolean allowVectorizeFallback\n@@ -83,17 +85,12 @@ public ExpressionProcessingConfig(\n     final String docsBaseFormat = \""https://druid.apache.org/docs/%s/querying/sql-data-types#%s\"";\n     if (!this.useStrictBooleans) {\n       LOG.warn(\n-          \""druid.expressions.useStrictBooleans set to 'false', we recommend using 'true' if using SQL to query Druid for the most SQL compliant behavior, see %s for details\"",\n+          \""druid.expressions.useStrictBooleans set to 'false', but has been removed from Druid and is always 'true' now for the most SQL compliant behavior, see %s for details\"",\n           StringUtils.format(docsBaseFormat, version, \""boolean-logic\"")\n       );\n     }\n   }\n \n-  public boolean isUseStrictBooleans()\n-  {\n-    return useStrictBooleans;\n-  }\n-\n   public boolean processArraysAsMultiValueStrings()\n   {\n     return processArraysAsMultiValueStrings;\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/UnaryOperatorExpr.java b/processing/src/main/java/org/apache/druid/math/expr/UnaryOperatorExpr.java\nindex f9f2c5bbcc28..30b3f9f4fc7a 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/UnaryOperatorExpr.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/UnaryOperatorExpr.java\n@@ -26,7 +26,6 @@\n import org.apache.druid.math.expr.vector.ExprVectorProcessor;\n import org.apache.druid.math.expr.vector.VectorMathProcessors;\n import org.apache.druid.math.expr.vector.VectorProcessors;\n-import org.apache.druid.segment.column.Types;\n \n import javax.annotation.Nullable;\n import java.math.BigInteger;\n@@ -181,11 +180,6 @@ public ExprEval eval(ObjectBinding bindings)\n     if (NullHandling.sqlCompatible() && (ret.value() == null)) {\n       return ExprEval.of(null);\n     }\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      // conforming to other boolean-returning binary operators\n-      ExpressionType retType = ret.type().is(ExprType.DOUBLE) ? ExpressionType.DOUBLE : ExpressionType.LONG;\n-      return ExprEval.ofBoolean(!ret.asBoolean(), retType);\n-    }\n     return ExprEval.ofLongBoolean(!ret.asBoolean());\n   }\n \n@@ -193,13 +187,6 @@ public ExprEval eval(ObjectBinding bindings)\n   @Override\n   public ExpressionType getOutputType(InputBindingInspector inspector)\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      ExpressionType implicitCast = super.getOutputType(inspector);\n-      if (Types.is(implicitCast, ExprType.STRING)) {\n-        return ExpressionType.LONG;\n-      }\n-      return implicitCast;\n-    }\n     return ExpressionType.LONG;\n   }\n \n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/vector/VectorComparisonProcessors.java b/processing/src/main/java/org/apache/druid/math/expr/vector/VectorComparisonProcessors.java\nindex a132c0ee6725..1df76f2bed21 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/vector/VectorComparisonProcessors.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/vector/VectorComparisonProcessors.java\n@@ -23,7 +23,6 @@\n import org.apache.druid.math.expr.Evals;\n import org.apache.druid.math.expr.Expr;\n import org.apache.druid.math.expr.ExprType;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.math.expr.ExpressionType;\n import org.apache.druid.segment.column.Types;\n \n@@ -33,50 +32,6 @@\n \n public class VectorComparisonProcessors\n {\n-  @Deprecated\n-  public static <T> ExprVectorProcessor<T> makeComparisonProcessor(\n-      Expr.VectorInputBindingInspector inspector,\n-      Expr left,\n-      Expr right,\n-      Supplier<LongOutObjectsInFunctionVectorProcessor> longOutStringsInFunctionVectorProcessor,\n-      Supplier<LongOutLongsInFunctionVectorValueProcessor> longOutLongsInProcessor,\n-      Supplier<DoubleOutLongDoubleInFunctionVectorValueProcessor> doubleOutLongDoubleInProcessor,\n-      Supplier<DoubleOutDoubleLongInFunctionVectorValueProcessor> doubleOutDoubleLongInProcessor,\n-      Supplier<DoubleOutDoublesInFunctionVectorValueProcessor> doubleOutDoublesInProcessor\n-  )\n-  {\n-    assert !ExpressionProcessing.useStrictBooleans();\n-    final ExpressionType leftType = left.getOutputType(inspector);\n-    final ExpressionType rightType = right.getOutputType(inspector);\n-    ExprVectorProcessor<?> processor = null;\n-    if (Types.is(leftType, ExprType.STRING)) {\n-      if (Types.isNullOr(rightType, ExprType.STRING)) {\n-        processor = longOutStringsInFunctionVectorProcessor.get();\n-      } else {\n-        processor = doubleOutDoublesInProcessor.get();\n-      }\n-    } else if (leftType == null) {\n-      if (Types.isNullOr(rightType, ExprType.STRING)) {\n-        processor = longOutStringsInFunctionVectorProcessor.get();\n-      }\n-    } else if (leftType.is(ExprType.DOUBLE) || Types.is(rightType, ExprType.DOUBLE)) {\n-      processor = doubleOutDoublesInProcessor.get();\n-    }\n-    if (processor != null) {\n-      return (ExprVectorProcessor<T>) processor;\n-    }\n-    // fall through to normal math processor logic\n-    return VectorMathProcessors.makeMathProcessor(\n-        inspector,\n-        left,\n-        right,\n-        longOutLongsInProcessor,\n-        doubleOutLongDoubleInProcessor,\n-        doubleOutDoubleLongInProcessor,\n-        doubleOutDoublesInProcessor\n-    );\n-  }\n-\n   public static <T> ExprVectorProcessor<T> makeBooleanProcessor(\n       Expr.VectorInputBindingInspector inspector,\n       Expr left,\n@@ -131,75 +86,6 @@ public static <T> ExprVectorProcessor<T> equal(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(Objects.equals(leftVal, rightVal));\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left == right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(left == right);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(left == right);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(left == right);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n@@ -275,75 +161,6 @@ public static <T> ExprVectorProcessor<T> notEqual(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(!Objects.equals(leftVal, rightVal));\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left != right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(left != right);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(left != right);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(left != right);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n@@ -419,77 +236,6 @@ public static <T> ExprVectorProcessor<T> greaterThanOrEqual(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(\n-                  Comparators.<String>naturalNullsFirst().compare((String) leftVal, (String) rightVal) >= 0\n-              );\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left >= right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) >= 0);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) >= 0);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) >= 0);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n@@ -567,77 +313,6 @@ public static <T> ExprVectorProcessor<T> greaterThan(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(\n-                  Comparators.<String>naturalNullsFirst().compare((String) leftVal, (String) rightVal) > 0\n-              );\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left > right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) > 0);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) > 0);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) > 0);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n@@ -715,77 +390,6 @@ public static <T> ExprVectorProcessor<T> lessThanOrEqual(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(\n-                  Comparators.<String>naturalNullsFirst().compare((String) leftVal, (String) rightVal) <= 0\n-              );\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left <= right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) <= 0);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) <= 0);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) <= 0);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n@@ -863,77 +467,6 @@ public static <T> ExprVectorProcessor<T> lessThan(\n       Expr right\n   )\n   {\n-    if (!ExpressionProcessing.useStrictBooleans()) {\n-      return makeComparisonProcessor(\n-          inspector,\n-          left,\n-          right,\n-          () -> new LongOutObjectsInFunctionVectorProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize(),\n-              ExpressionType.STRING\n-          )\n-          {\n-            @Nullable\n-            @Override\n-            Long processValue(@Nullable Object leftVal, @Nullable Object rightVal)\n-            {\n-              return Evals.asLong(\n-                  Comparators.<String>naturalNullsFirst().compare((String) leftVal, (String) rightVal) < 0\n-              );\n-            }\n-          },\n-          () -> new LongOutLongsInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public long apply(long left, long right)\n-            {\n-              return Evals.asLong(left < right);\n-            }\n-          },\n-          () -> new DoubleOutLongDoubleInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(long left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) < 0);\n-            }\n-          },\n-          () -> new DoubleOutDoubleLongInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, long right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) < 0);\n-            }\n-          },\n-          () -> new DoubleOutDoublesInFunctionVectorValueProcessor(\n-              left.asVectorProcessor(inspector),\n-              right.asVectorProcessor(inspector),\n-              inspector.getMaxVectorSize()\n-          )\n-          {\n-            @Override\n-            public double apply(double left, double right)\n-            {\n-              return Evals.asDouble(Double.compare(left, right) < 0);\n-            }\n-          }\n-      );\n-    }\n     return makeBooleanProcessor(\n         inspector,\n         left,\n\ndiff --git a/processing/src/main/java/org/apache/druid/math/expr/vector/VectorProcessors.java b/processing/src/main/java/org/apache/druid/math/expr/vector/VectorProcessors.java\nindex d5d3933b860b..1f5727b83616 100644\n--- a/processing/src/main/java/org/apache/druid/math/expr/vector/VectorProcessors.java\n+++ b/processing/src/main/java/org/apache/druid/math/expr/vector/VectorProcessors.java\n@@ -25,7 +25,6 @@\n import org.apache.druid.math.expr.Evals;\n import org.apache.druid.math.expr.Expr;\n import org.apache.druid.math.expr.ExprType;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.math.expr.ExpressionType;\n import org.apache.druid.math.expr.Exprs;\n import org.apache.druid.segment.column.Types;\n@@ -658,25 +657,14 @@ public long apply(long input)\n         }\n       };\n     } else if (Types.is(inputType, ExprType.DOUBLE)) {\n-      if (!ExpressionProcessing.useStrictBooleans()) {\n-        processor = new DoubleOutDoubleInFunctionVectorValueProcessor(expr.asVectorProcessor(inspector), maxVectorSize)\n-        {\n-          @Override\n-          public double apply(double input)\n-          {\n-            return Evals.asDouble(!Evals.asBoolean(input));\n-          }\n-        };\n-      } else {\n-        processor = new LongOutDoubleInFunctionVectorValueProcessor(expr.asVectorProcessor(inspector), maxVectorSize)\n+      processor = new LongOutDoubleInFunctionVectorValueProcessor(expr.asVectorProcessor(inspector), maxVectorSize)\n+      {\n+        @Override\n+        public long apply(double input)\n         {\n-          @Override\n-          public long apply(double input)\n-          {\n-            return Evals.asLong(!Evals.asBoolean(input));\n-          }\n-        };\n-      }\n+          return Evals.asLong(!Evals.asBoolean(input));\n+        }\n+      };\n     }\n     if (processor == null) {\n       throw Exprs.cannotVectorize();\n\ndiff --git a/processing/src/main/java/org/apache/druid/segment/filter/NotFilter.java b/processing/src/main/java/org/apache/druid/segment/filter/NotFilter.java\nindex 3e89148e2ccc..e44cf2b57e44 100644\n--- a/processing/src/main/java/org/apache/druid/segment/filter/NotFilter.java\n+++ b/processing/src/main/java/org/apache/druid/segment/filter/NotFilter.java\n@@ -21,7 +21,6 @@\n \n import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.java.util.common.StringUtils;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.BitmapResultFactory;\n import org.apache.druid.query.filter.ColumnIndexSelector;\n import org.apache.druid.query.filter.Filter;\n@@ -45,11 +44,10 @@\n /**\n  * Nice filter you have there... NOT!\n  *\n- * If {@link ExpressionProcessing#useStrictBooleans()} and {@link NullHandling#sqlCompatible()} are both true, this\n- * filter inverts the {@code includeUnknown} flag to properly map Druids native two-valued logic (true, false) to SQL\n- * three-valued logic (true, false, unknown). At the top level, this flag is always passed in as 'false', and is only\n- * flipped by this filter. Other logical filters ({@link AndFilter} and {@link OrFilter}) propagate the value of\n- * {@code includeUnknown} to their children.\n+ * If {@link NullHandling#sqlCompatible()} is true, this filter inverts the {@code includeUnknown} flag to properly\n+ * map Druids native two-valued logic (true, false) to SQL three-valued logic (true, false, unknown). At the top level,\n+ * this flag is always passed in as 'false', and is only flipped by this filter. Other logical filters\n+ * ({@link AndFilter} and {@link OrFilter}) propagate the value of {@code includeUnknown} to their children.\n  *\n  * For example, if the base filter is equality, by default value matchers and indexes only return true for the rows\n  * that are equal to the value. When wrapped in a not filter, the not filter indicates that the equality matchers and\n\ndiff --git a/sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRule.java b/sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRule.java\nindex 97fa5b86a6a1..c17e339e7854 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRule.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRule.java\n@@ -27,7 +27,6 @@\n import org.apache.calcite.rex.RexLiteral;\n import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.error.InvalidSqlInput;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.InlineDataSource;\n import org.apache.druid.segment.column.RowSignature;\n import org.apache.druid.sql.calcite.planner.Calcites;\n@@ -122,7 +121,7 @@ public static Object getValueFromLiteral(RexLiteral literal, PlannerContext plan\n         }\n         return ((Number) RexLiteral.value(literal)).longValue();\n       case BOOLEAN:\n-        if (ExpressionProcessing.useStrictBooleans() && NullHandling.sqlCompatible() && literal.isNull()) {\n+        if (NullHandling.sqlCompatible() && literal.isNull()) {\n           return null;\n         }\n         return literal.isAlwaysTrue() ? 1L : 0L;\n"", ""test_patch"": ""diff --git a/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java b/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java\nindex 0220aacd8922..463416fc33e6 100644\n--- a/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java\n+++ b/indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerDiscoveryTest.java\n@@ -31,7 +31,6 @@\n import org.apache.druid.data.input.impl.StringDimensionSchema;\n import org.apache.druid.data.input.impl.TimestampSpec;\n import org.apache.druid.jackson.DefaultObjectMapper;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.segment.AutoTypeColumnSchema;\n import org.apache.druid.segment.column.ColumnType;\n import org.apache.druid.segment.column.RowSignature;\n@@ -56,71 +55,6 @@ public class InputSourceSamplerDiscoveryTest extends InitializedNullHandlingTest\n   );\n   private InputSourceSampler inputSourceSampler = new InputSourceSampler(OBJECT_MAPPER);\n \n-  @Test\n-  public void testDiscoveredTypesNonStrictBooleans()\n-  {\n-\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      final InputSource inputSource = new InlineInputSource(Strings.join(STR_JSON_ROWS, '\\n'));\n-      final SamplerResponse response = inputSourceSampler.sample(\n-          inputSource,\n-          new JsonInputFormat(null, null, null, null, null),\n-          DataSchema.builder()\n-                    .withDataSource(\""test\"")\n-                    .withTimestamp(new TimestampSpec(\""t\"", null, null))\n-                    .withDimensions(DimensionsSpec.builder().useSchemaDiscovery(true).build())\n-                    .build(),\n-          null\n-      );\n-\n-      Assert.assertEquals(6, response.getNumRowsRead());\n-      Assert.assertEquals(5, response.getNumRowsIndexed());\n-      Assert.assertEquals(6, response.getData().size());\n-      Assert.assertEquals(\n-          ImmutableList.of(\n-              new StringDimensionSchema(\""string\""),\n-              new LongDimensionSchema(\""long\""),\n-              new DoubleDimensionSchema(\""double\""),\n-              new StringDimensionSchema(\""bool\""),\n-              new StringDimensionSchema(\""variant\""),\n-              new AutoTypeColumnSchema(\""array\"", null),\n-              new AutoTypeColumnSchema(\""nested\"", null)\n-          ),\n-          response.getLogicalDimensions()\n-      );\n-\n-      Assert.assertEquals(\n-          ImmutableList.of(\n-              new AutoTypeColumnSchema(\""string\"", null),\n-              new AutoTypeColumnSchema(\""long\"", null),\n-              new AutoTypeColumnSchema(\""double\"", null),\n-              new AutoTypeColumnSchema(\""bool\"", null),\n-              new AutoTypeColumnSchema(\""variant\"", null),\n-              new AutoTypeColumnSchema(\""array\"", null),\n-              new AutoTypeColumnSchema(\""nested\"", null)\n-          ),\n-          response.getPhysicalDimensions()\n-      );\n-      Assert.assertEquals(\n-          RowSignature.builder()\n-                      .addTimeColumn()\n-                      .add(\""string\"", ColumnType.STRING)\n-                      .add(\""long\"", ColumnType.LONG)\n-                      .add(\""double\"", ColumnType.DOUBLE)\n-                      .add(\""bool\"", ColumnType.STRING)\n-                      .add(\""variant\"", ColumnType.STRING)\n-                      .add(\""array\"", ColumnType.LONG_ARRAY)\n-                      .add(\""nested\"", ColumnType.NESTED_DATA)\n-                      .build(),\n-          response.getLogicalSegmentSchema()\n-      );\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-  }\n-\n   @Test\n   public void testDiscoveredTypesStrictBooleans()\n   {\n\ndiff --git a/processing/src/test/java/org/apache/druid/math/expr/EvalTest.java b/processing/src/test/java/org/apache/druid/math/expr/EvalTest.java\nindex 2f68840955fa..ae515d13abfc 100644\n--- a/processing/src/test/java/org/apache/druid/math/expr/EvalTest.java\n+++ b/processing/src/test/java/org/apache/druid/math/expr/EvalTest.java\n@@ -41,7 +41,6 @@\n import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNull;\n \n /**\n  */\n@@ -84,93 +83,50 @@ public void testDoubleEval()\n     assertEquals(2.0, evalDouble(\""\\\""x\\\""\"", bindings), 0.0001);\n     assertEquals(304.0, evalDouble(\""300 + \\\""x\\\"" * 2\"", bindings), 0.0001);\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      Assert.assertFalse(evalDouble(\""1.0 && 0.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""1.0 && 2.0\"", bindings) > 0.0);\n-\n-      Assert.assertTrue(evalDouble(\""1.0 || 0.0\"", bindings) > 0.0);\n-      Assert.assertFalse(evalDouble(\""0.0 || 0.0\"", bindings) > 0.0);\n-\n-      Assert.assertTrue(evalDouble(\""2.0 > 1.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""2.0 >= 2.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""1.0 < 2.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""2.0 <= 2.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""2.0 == 2.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""2.0 != 1.0\"", bindings) > 0.0);\n-\n-      Assert.assertEquals(1L, evalLong(\""notdistinctfrom(2.0, 2.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""isdistinctfrom(2.0, 1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""notdistinctfrom(2.0, 1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""isdistinctfrom(2.0, 2.0)\"", bindings));\n-\n-      Assert.assertEquals(0L, evalLong(\""istrue(0.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""isfalse(0.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""nottrue(0.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""notfalse(0.0)\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""istrue(1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""isfalse(1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""nottrue(1.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""notfalse(1.0)\"", bindings));\n-\n-      Assert.assertTrue(evalDouble(\""!-1.0\"", bindings) > 0.0);\n-      Assert.assertTrue(evalDouble(\""!0.0\"", bindings) > 0.0);\n-      Assert.assertFalse(evalDouble(\""!2.0\"", bindings) > 0.0);\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      Assert.assertEquals(0L, evalLong(\""1.0 && 0.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""1.0 && 2.0\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""1.0 || 0.0\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""0.0 || 0.0\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""2.0 > 1.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""2.0 >= 2.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""1.0 < 2.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""2.0 <= 2.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""2.0 == 2.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""2.0 != 1.0\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""notdistinctfrom(2.0, 2.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""isdistinctfrom(2.0, 1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""notdistinctfrom(2.0, 1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""isdistinctfrom(2.0, 2.0)\"", bindings));\n-\n-      Assert.assertEquals(0L, evalLong(\""istrue(0.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""isfalse(0.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""nottrue(0.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""notfalse(0.0)\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""istrue(1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""isfalse(1.0)\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""nottrue(1.0)\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""notfalse(1.0)\"", bindings));\n-\n-      Assert.assertEquals(1L, evalLong(\""!-1.0\"", bindings));\n-      Assert.assertEquals(1L, evalLong(\""!0.0\"", bindings));\n-      Assert.assertEquals(0L, evalLong(\""!2.0\"", bindings));\n-\n-      assertEquals(3.5, evalDouble(\""2.0 + 1.5\"", bindings), 0.0001);\n-      assertEquals(0.5, evalDouble(\""2.0 - 1.5\"", bindings), 0.0001);\n-      assertEquals(3.0, evalDouble(\""2.0 * 1.5\"", bindings), 0.0001);\n-      assertEquals(4.0, evalDouble(\""2.0 / 0.5\"", bindings), 0.0001);\n-      assertEquals(0.2, evalDouble(\""2.0 % 0.3\"", bindings), 0.0001);\n-      assertEquals(8.0, evalDouble(\""2.0 ^ 3.0\"", bindings), 0.0001);\n-      assertEquals(-1.5, evalDouble(\""-1.5\"", bindings), 0.0001);\n-\n-\n-      assertEquals(2.0, evalDouble(\""sqrt(4.0)\"", bindings), 0.0001);\n-      assertEquals(2.0, evalDouble(\""if(1.0, 2.0, 3.0)\"", bindings), 0.0001);\n-      assertEquals(3.0, evalDouble(\""if(0.0, 2.0, 3.0)\"", bindings), 0.0001);\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    Assert.assertEquals(0L, evalLong(\""1.0 && 0.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""1.0 && 2.0\"", bindings));\n+\n+    Assert.assertEquals(1L, evalLong(\""1.0 || 0.0\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""0.0 || 0.0\"", bindings));\n+\n+    Assert.assertEquals(1L, evalLong(\""2.0 > 1.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""2.0 >= 2.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""1.0 < 2.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""2.0 <= 2.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""2.0 == 2.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""2.0 != 1.0\"", bindings));\n+\n+    Assert.assertEquals(1L, evalLong(\""notdistinctfrom(2.0, 2.0)\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""isdistinctfrom(2.0, 1.0)\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""notdistinctfrom(2.0, 1.0)\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""isdistinctfrom(2.0, 2.0)\"", bindings));\n+\n+    Assert.assertEquals(0L, evalLong(\""istrue(0.0)\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""isfalse(0.0)\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""nottrue(0.0)\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""notfalse(0.0)\"", bindings));\n+\n+    Assert.assertEquals(1L, evalLong(\""istrue(1.0)\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""isfalse(1.0)\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""nottrue(1.0)\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""notfalse(1.0)\"", bindings));\n+\n+    Assert.assertEquals(1L, evalLong(\""!-1.0\"", bindings));\n+    Assert.assertEquals(1L, evalLong(\""!0.0\"", bindings));\n+    Assert.assertEquals(0L, evalLong(\""!2.0\"", bindings));\n+\n+    assertEquals(3.5, evalDouble(\""2.0 + 1.5\"", bindings), 0.0001);\n+    assertEquals(0.5, evalDouble(\""2.0 - 1.5\"", bindings), 0.0001);\n+    assertEquals(3.0, evalDouble(\""2.0 * 1.5\"", bindings), 0.0001);\n+    assertEquals(4.0, evalDouble(\""2.0 / 0.5\"", bindings), 0.0001);\n+    assertEquals(0.2, evalDouble(\""2.0 % 0.3\"", bindings), 0.0001);\n+    assertEquals(8.0, evalDouble(\""2.0 ^ 3.0\"", bindings), 0.0001);\n+    assertEquals(-1.5, evalDouble(\""-1.5\"", bindings), 0.0001);\n+\n+\n+    assertEquals(2.0, evalDouble(\""sqrt(4.0)\"", bindings), 0.0001);\n+    assertEquals(2.0, evalDouble(\""if(1.0, 2.0, 3.0)\"", bindings), 0.0001);\n+    assertEquals(3.0, evalDouble(\""if(0.0, 2.0, 3.0)\"", bindings), 0.0001);\n   }\n \n   @Test\n@@ -934,56 +890,29 @@ public void testBooleanReturn()\n         ImmutableMap.of(\""x\"", 100L, \""y\"", 100L, \""z\"", 100D, \""w\"", 100D)\n     );\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      ExprEval eval = Parser.parse(\""x==z\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertTrue(eval.asBoolean());\n-      assertEquals(ExpressionType.DOUBLE, eval.type());\n+    ExprEval eval = Parser.parse(\""x==y\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertTrue(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n \n-      eval = Parser.parse(\""x!=z\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertFalse(eval.asBoolean());\n-      assertEquals(ExpressionType.DOUBLE, eval.type());\n+    eval = Parser.parse(\""x!=y\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertFalse(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n \n-      eval = Parser.parse(\""z==w\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertTrue(eval.asBoolean());\n-      assertEquals(ExpressionType.DOUBLE, eval.type());\n+    eval = Parser.parse(\""x==z\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertTrue(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n \n-      eval = Parser.parse(\""z!=w\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertFalse(eval.asBoolean());\n-      assertEquals(ExpressionType.DOUBLE, eval.type());\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      ExprEval eval = Parser.parse(\""x==y\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertTrue(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-\n-      eval = Parser.parse(\""x!=y\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertFalse(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-\n-      eval = Parser.parse(\""x==z\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertTrue(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-\n-      eval = Parser.parse(\""x!=z\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertFalse(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-\n-      eval = Parser.parse(\""z==w\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertTrue(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-\n-      eval = Parser.parse(\""z!=w\"", ExprMacroTable.nil()).eval(bindings);\n-      Assert.assertFalse(eval.asBoolean());\n-      assertEquals(ExpressionType.LONG, eval.type());\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    eval = Parser.parse(\""x!=z\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertFalse(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n+\n+    eval = Parser.parse(\""z==w\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertTrue(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n+\n+    eval = Parser.parse(\""z!=w\"", ExprMacroTable.nil()).eval(bindings);\n+    Assert.assertFalse(eval.asBoolean());\n+    assertEquals(ExpressionType.LONG, eval.type());\n   }\n \n   @Test\n@@ -991,142 +920,60 @@ public void testLogicalOperators()\n   {\n     Expr.ObjectBinding bindings = InputBindings.nilBindings();\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      assertEquals(1L, eval(\""'true' && 'true'\"", bindings).value());\n-      assertEquals(0L, eval(\""'true' && 'false'\"", bindings).value());\n-      assertEquals(0L, eval(\""'false' && 'true'\"", bindings).value());\n-      assertEquals(0L, eval(\""'troo' && 'true'\"", bindings).value());\n-      assertEquals(0L, eval(\""'false' && 'false'\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""'true' || 'true'\"", bindings).value());\n-      assertEquals(1L, eval(\""'true' || 'false'\"", bindings).value());\n-      assertEquals(1L, eval(\""'false' || 'true'\"", bindings).value());\n-      assertEquals(1L, eval(\""'troo' || 'true'\"", bindings).value());\n-      assertEquals(0L, eval(\""'false' || 'false'\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1 && 1\"", bindings).value());\n-      assertEquals(1L, eval(\""100 && 11\"", bindings).value());\n-      assertEquals(0L, eval(\""1 && 0\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && 1\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && 0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1 || 1\"", bindings).value());\n-      assertEquals(1L, eval(\""100 || 11\"", bindings).value());\n-      assertEquals(1L, eval(\""1 || 0\"", bindings).value());\n-      assertEquals(1L, eval(\""0 || 1\"", bindings).value());\n-      assertEquals(1L, eval(\""111 || 0\"", bindings).value());\n-      assertEquals(1L, eval(\""0 || 111\"", bindings).value());\n-      assertEquals(0L, eval(\""0 || 0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1.0 && 1.0\"", bindings).value());\n-      assertEquals(1L, eval(\""0.100 && 1.1\"", bindings).value());\n-      assertEquals(0L, eval(\""1.0 && 0.0\"", bindings).value());\n-      assertEquals(0L, eval(\""0.0 && 1.0\"", bindings).value());\n-      assertEquals(0L, eval(\""0.0 && 0.0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1.0 || 1.0\"", bindings).value());\n-      assertEquals(1L, eval(\""0.2 || 0.3\"", bindings).value());\n-      assertEquals(1L, eval(\""1.0 || 0.0\"", bindings).value());\n-      assertEquals(1L, eval(\""0.0 || 1.0\"", bindings).value());\n-      assertEquals(1L, eval(\""1.11 || 0.0\"", bindings).value());\n-      assertEquals(1L, eval(\""0.0 || 0.111\"", bindings).value());\n-      assertEquals(0L, eval(\""0.0 || 0.0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""null || 1\"", bindings).value());\n-      assertEquals(1L, eval(\""1 || null\"", bindings).value());\n-      // in sql incompatible mode, null is false, so we return 0\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""null || 0\"", bindings).valueOrDefault());\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""0 || null\"", bindings).valueOrDefault());\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""null || null\"", bindings).valueOrDefault());\n-\n-      // in sql incompatible mode, null is false, so we return 0\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""null && 1\"", bindings).valueOrDefault());\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""1 && null\"", bindings).valueOrDefault());\n-      assertEquals(NullHandling.defaultLongValue(), eval(\""null && null\"", bindings).valueOrDefault());\n-      // if either side is false, output is false in both modes\n-      assertEquals(0L, eval(\""null && 0\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && null\"", bindings).value());\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n-\n-    try {\n-      // turn on legacy insanity mode\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-\n-      assertEquals(\""true\"", eval(\""'true' && 'true'\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""'true' && 'false'\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""'false' && 'true'\"", bindings).value());\n-      assertEquals(\""troo\"", eval(\""'troo' && 'true'\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""'false' && 'false'\"", bindings).value());\n-\n-      assertEquals(\""true\"", eval(\""'true' || 'true'\"", bindings).value());\n-      assertEquals(\""true\"", eval(\""'true' || 'false'\"", bindings).value());\n-      assertEquals(\""true\"", eval(\""'false' || 'true'\"", bindings).value());\n-      assertEquals(\""true\"", eval(\""'troo' || 'true'\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""'false' || 'false'\"", bindings).value());\n-\n-      assertEquals(1.0, eval(\""1.0 && 1.0\"", bindings).value());\n-      assertEquals(1.1, eval(\""0.100 && 1.1\"", bindings).value());\n-      assertEquals(0.0, eval(\""1.0 && 0.0\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 && 1.0\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 && 0.0\"", bindings).value());\n-\n-      assertEquals(1.0, eval(\""1.0 || 1.0\"", bindings).value());\n-      assertEquals(0.2, eval(\""0.2 || 0.3\"", bindings).value());\n-      assertEquals(1.0, eval(\""1.0 || 0.0\"", bindings).value());\n-      assertEquals(1.0, eval(\""0.0 || 1.0\"", bindings).value());\n-      assertEquals(1.11, eval(\""1.11 || 0.0\"", bindings).value());\n-      assertEquals(0.111, eval(\""0.0 || 0.111\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 || 0.0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1 && 1\"", bindings).value());\n-      assertEquals(11L, eval(\""100 && 11\"", bindings).value());\n-      assertEquals(0L, eval(\""1 && 0\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && 1\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && 0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""1 || 1\"", bindings).value());\n-      assertEquals(100L, eval(\""100 || 11\"", bindings).value());\n-      assertEquals(1L, eval(\""1 || 0\"", bindings).value());\n-      assertEquals(1L, eval(\""0 || 1\"", bindings).value());\n-      assertEquals(111L, eval(\""111 || 0\"", bindings).value());\n-      assertEquals(111L, eval(\""0 || 111\"", bindings).value());\n-      assertEquals(0L, eval(\""0 || 0\"", bindings).value());\n-\n-      assertEquals(1.0, eval(\""1.0 && 1.0\"", bindings).value());\n-      assertEquals(1.1, eval(\""0.100 && 1.1\"", bindings).value());\n-      assertEquals(0.0, eval(\""1.0 && 0.0\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 && 1.0\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 && 0.0\"", bindings).value());\n-\n-      assertEquals(1.0, eval(\""1.0 || 1.0\"", bindings).value());\n-      assertEquals(0.2, eval(\""0.2 || 0.3\"", bindings).value());\n-      assertEquals(1.0, eval(\""1.0 || 0.0\"", bindings).value());\n-      assertEquals(1.0, eval(\""0.0 || 1.0\"", bindings).value());\n-      assertEquals(1.11, eval(\""1.11 || 0.0\"", bindings).value());\n-      assertEquals(0.111, eval(\""0.0 || 0.111\"", bindings).value());\n-      assertEquals(0.0, eval(\""0.0 || 0.0\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""null || 1\"", bindings).value());\n-      assertEquals(1L, eval(\""1 || null\"", bindings).value());\n-      assertEquals(0L, eval(\""null || 0\"", bindings).value());\n-      Assert.assertNull(eval(\""0 || null\"", bindings).value());\n-      Assert.assertNull(eval(\""null || null\"", bindings).value());\n-\n-      Assert.assertNull(eval(\""null && 1\"", bindings).value());\n-      Assert.assertNull(eval(\""1 && null\"", bindings).value());\n-      Assert.assertNull(eval(\""null && 0\"", bindings).value());\n-      assertEquals(0L, eval(\""0 && null\"", bindings).value());\n-      assertNull(eval(\""null && null\"", bindings).value());\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    assertEquals(1L, eval(\""'true' && 'true'\"", bindings).value());\n+    assertEquals(0L, eval(\""'true' && 'false'\"", bindings).value());\n+    assertEquals(0L, eval(\""'false' && 'true'\"", bindings).value());\n+    assertEquals(0L, eval(\""'troo' && 'true'\"", bindings).value());\n+    assertEquals(0L, eval(\""'false' && 'false'\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""'true' || 'true'\"", bindings).value());\n+    assertEquals(1L, eval(\""'true' || 'false'\"", bindings).value());\n+    assertEquals(1L, eval(\""'false' || 'true'\"", bindings).value());\n+    assertEquals(1L, eval(\""'troo' || 'true'\"", bindings).value());\n+    assertEquals(0L, eval(\""'false' || 'false'\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""1 && 1\"", bindings).value());\n+    assertEquals(1L, eval(\""100 && 11\"", bindings).value());\n+    assertEquals(0L, eval(\""1 && 0\"", bindings).value());\n+    assertEquals(0L, eval(\""0 && 1\"", bindings).value());\n+    assertEquals(0L, eval(\""0 && 0\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""1 || 1\"", bindings).value());\n+    assertEquals(1L, eval(\""100 || 11\"", bindings).value());\n+    assertEquals(1L, eval(\""1 || 0\"", bindings).value());\n+    assertEquals(1L, eval(\""0 || 1\"", bindings).value());\n+    assertEquals(1L, eval(\""111 || 0\"", bindings).value());\n+    assertEquals(1L, eval(\""0 || 111\"", bindings).value());\n+    assertEquals(0L, eval(\""0 || 0\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""1.0 && 1.0\"", bindings).value());\n+    assertEquals(1L, eval(\""0.100 && 1.1\"", bindings).value());\n+    assertEquals(0L, eval(\""1.0 && 0.0\"", bindings).value());\n+    assertEquals(0L, eval(\""0.0 && 1.0\"", bindings).value());\n+    assertEquals(0L, eval(\""0.0 && 0.0\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""1.0 || 1.0\"", bindings).value());\n+    assertEquals(1L, eval(\""0.2 || 0.3\"", bindings).value());\n+    assertEquals(1L, eval(\""1.0 || 0.0\"", bindings).value());\n+    assertEquals(1L, eval(\""0.0 || 1.0\"", bindings).value());\n+    assertEquals(1L, eval(\""1.11 || 0.0\"", bindings).value());\n+    assertEquals(1L, eval(\""0.0 || 0.111\"", bindings).value());\n+    assertEquals(0L, eval(\""0.0 || 0.0\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""null || 1\"", bindings).value());\n+    assertEquals(1L, eval(\""1 || null\"", bindings).value());\n+    // in sql incompatible mode, null is false, so we return 0\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""null || 0\"", bindings).valueOrDefault());\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""0 || null\"", bindings).valueOrDefault());\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""null || null\"", bindings).valueOrDefault());\n+\n+    // in sql incompatible mode, null is false, so we return 0\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""null && 1\"", bindings).valueOrDefault());\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""1 && null\"", bindings).valueOrDefault());\n+    assertEquals(NullHandling.defaultLongValue(), eval(\""null && null\"", bindings).valueOrDefault());\n+    // if either side is false, output is false in both modes\n+    assertEquals(0L, eval(\""null && 0\"", bindings).value());\n+    assertEquals(0L, eval(\""0 && null\"", bindings).value());\n   }\n \n   @Test\n@@ -1143,86 +990,40 @@ public void testBooleanInputs()\n     bindingsMap.put(\""b2\"", false);\n     Expr.ObjectBinding bindings = InputBindings.forMap(bindingsMap);\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      assertEquals(1L, eval(\""s1 && s1\"", bindings).value());\n-      assertEquals(0L, eval(\""s1 && s2\"", bindings).value());\n-      assertEquals(0L, eval(\""s2 && s1\"", bindings).value());\n-      assertEquals(0L, eval(\""s2 && s2\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""s1 || s1\"", bindings).value());\n-      assertEquals(1L, eval(\""s1 || s2\"", bindings).value());\n-      assertEquals(1L, eval(\""s2 || s1\"", bindings).value());\n-      assertEquals(0L, eval(\""s2 || s2\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""l1 && l1\"", bindings).value());\n-      assertEquals(0L, eval(\""l1 && l2\"", bindings).value());\n-      assertEquals(0L, eval(\""l2 && l1\"", bindings).value());\n-      assertEquals(0L, eval(\""l2 && l2\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""b1 && b1\"", bindings).value());\n-      assertEquals(0L, eval(\""b1 && b2\"", bindings).value());\n-      assertEquals(0L, eval(\""b2 && b1\"", bindings).value());\n-      assertEquals(0L, eval(\""b2 && b2\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""d1 && d1\"", bindings).value());\n-      assertEquals(0L, eval(\""d1 && d2\"", bindings).value());\n-      assertEquals(0L, eval(\""d2 && d1\"", bindings).value());\n-      assertEquals(0L, eval(\""d2 && d2\"", bindings).value());\n-\n-      assertEquals(1L, eval(\""b1\"", bindings).value());\n-      assertEquals(1L, eval(\""if(b1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(l1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(d1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(s1,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(b2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(l2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(d2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(s2,1,0)\"", bindings).value());\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n-\n-    try {\n-      // turn on legacy insanity mode\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-\n-      assertEquals(\""true\"", eval(\""s1 && s1\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""s1 && s2\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""s2 && s1\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""s2 && s2\"", bindings).value());\n-\n-      assertEquals(\""true\"", eval(\""b1 && b1\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""b1 && b2\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""b2 && b1\"", bindings).value());\n-      assertEquals(\""false\"", eval(\""b2 && b2\"", bindings).value());\n-\n-      assertEquals(100L, eval(\""l1 && l1\"", bindings).value());\n-      assertEquals(0L, eval(\""l1 && l2\"", bindings).value());\n-      assertEquals(0L, eval(\""l2 && l1\"", bindings).value());\n-      assertEquals(0L, eval(\""l2 && l2\"", bindings).value());\n-\n-      assertEquals(1.1, eval(\""d1 && d1\"", bindings).value());\n-      assertEquals(0.0, eval(\""d1 && d2\"", bindings).value());\n-      assertEquals(0.0, eval(\""d2 && d1\"", bindings).value());\n-      assertEquals(0.0, eval(\""d2 && d2\"", bindings).value());\n-\n-      assertEquals(\""true\"", eval(\""b1\"", bindings).value());\n-      assertEquals(1L, eval(\""if(b1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(l1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(d1,1,0)\"", bindings).value());\n-      assertEquals(1L, eval(\""if(s1,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(b2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(l2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(d2,1,0)\"", bindings).value());\n-      assertEquals(0L, eval(\""if(s2,1,0)\"", bindings).value());\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    assertEquals(1L, eval(\""s1 && s1\"", bindings).value());\n+    assertEquals(0L, eval(\""s1 && s2\"", bindings).value());\n+    assertEquals(0L, eval(\""s2 && s1\"", bindings).value());\n+    assertEquals(0L, eval(\""s2 && s2\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""s1 || s1\"", bindings).value());\n+    assertEquals(1L, eval(\""s1 || s2\"", bindings).value());\n+    assertEquals(1L, eval(\""s2 || s1\"", bindings).value());\n+    assertEquals(0L, eval(\""s2 || s2\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""l1 && l1\"", bindings).value());\n+    assertEquals(0L, eval(\""l1 && l2\"", bindings).value());\n+    assertEquals(0L, eval(\""l2 && l1\"", bindings).value());\n+    assertEquals(0L, eval(\""l2 && l2\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""b1 && b1\"", bindings).value());\n+    assertEquals(0L, eval(\""b1 && b2\"", bindings).value());\n+    assertEquals(0L, eval(\""b2 && b1\"", bindings).value());\n+    assertEquals(0L, eval(\""b2 && b2\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""d1 && d1\"", bindings).value());\n+    assertEquals(0L, eval(\""d1 && d2\"", bindings).value());\n+    assertEquals(0L, eval(\""d2 && d1\"", bindings).value());\n+    assertEquals(0L, eval(\""d2 && d2\"", bindings).value());\n+\n+    assertEquals(1L, eval(\""b1\"", bindings).value());\n+    assertEquals(1L, eval(\""if(b1,1,0)\"", bindings).value());\n+    assertEquals(1L, eval(\""if(l1,1,0)\"", bindings).value());\n+    assertEquals(1L, eval(\""if(d1,1,0)\"", bindings).value());\n+    assertEquals(1L, eval(\""if(s1,1,0)\"", bindings).value());\n+    assertEquals(0L, eval(\""if(b2,1,0)\"", bindings).value());\n+    assertEquals(0L, eval(\""if(l2,1,0)\"", bindings).value());\n+    assertEquals(0L, eval(\""if(d2,1,0)\"", bindings).value());\n+    assertEquals(0L, eval(\""if(s2,1,0)\"", bindings).value());\n   }\n \n   @Test\n@@ -1621,17 +1422,6 @@ public void testBestEffortOf()\n     assertBestEffortOf(true, ExpressionType.LONG, 1L);\n     assertBestEffortOf(Arrays.asList(true, false), ExpressionType.LONG_ARRAY, new Object[]{1L, 0L});\n \n-    try {\n-      // in non-strict boolean mode, they are strings\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      assertBestEffortOf(true, ExpressionType.STRING, \""true\"");\n-      assertBestEffortOf(Arrays.asList(true, false), ExpressionType.STRING_ARRAY, new Object[]{\""true\"", \""false\""});\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n-\n     // doubles\n     assertBestEffortOf(1.0, ExpressionType.DOUBLE, 1.0);\n     assertBestEffortOf(1.0f, ExpressionType.DOUBLE, 1.0);\n\ndiff --git a/processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java b/processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java\nindex 4ab7c7be0ed3..088df4c9ef58 100644\n--- a/processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java\n+++ b/processing/src/test/java/org/apache/druid/math/expr/OutputTypeTest.java\n@@ -70,29 +70,12 @@ public void testUnaryOperators()\n     assertOutputType(\""-y\"", inspector, ExpressionType.LONG);\n     assertOutputType(\""-z\"", inspector, ExpressionType.DOUBLE);\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      assertOutputType(\""!'true'\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""!1\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""!x\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""!y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""!1.1\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""!z\"", inspector, ExpressionType.LONG);\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n-\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      assertOutputType(\""!1.1\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""!z\"", inspector, ExpressionType.DOUBLE);\n-    }\n-    finally {\n-      // reset\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    assertOutputType(\""!'true'\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""!1\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""!x\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""!y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""!1.1\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""!z\"", inspector, ExpressionType.LONG);\n   }\n \n   @Test\n@@ -126,61 +109,32 @@ public void testBinaryMathOperators()\n     assertOutputType(\""z^z_\"", inspector, ExpressionType.DOUBLE);\n     assertOutputType(\""z%z_\"", inspector, ExpressionType.DOUBLE);\n \n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      assertOutputType(\""y>y_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_<y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_<=y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_>=y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_==y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_!=y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_ && y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y_ || y\"", inspector, ExpressionType.LONG);\n-\n-      assertOutputType(\""z>y_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z<y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z<=y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y>=z\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z==y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z!=y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z && y\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""y || z\"", inspector, ExpressionType.LONG);\n-\n-      assertOutputType(\""z>z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z<z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z<=z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z_>=z\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z==z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z!=z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z && z_\"", inspector, ExpressionType.LONG);\n-      assertOutputType(\""z_ || z\"", inspector, ExpressionType.LONG);\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      assertOutputType(\""z>y_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z<y\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z<=y\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""y>=z\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z==y\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z!=y\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z && y\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""y || z\"", inspector, ExpressionType.DOUBLE);\n-\n-      assertOutputType(\""z>z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z<z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z<=z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z_>=z\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z==z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z!=z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z && z_\"", inspector, ExpressionType.DOUBLE);\n-      assertOutputType(\""z_ || z\"", inspector, ExpressionType.DOUBLE);\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    assertOutputType(\""y>y_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_<y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_<=y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_>=y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_==y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_!=y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_ && y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y_ || y\"", inspector, ExpressionType.LONG);\n+\n+    assertOutputType(\""z>y_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z<y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z<=y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y>=z\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z==y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z!=y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z && y\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""y || z\"", inspector, ExpressionType.LONG);\n+\n+    assertOutputType(\""z>z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z<z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z<=z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z_>=z\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z==z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z!=z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z && z_\"", inspector, ExpressionType.LONG);\n+    assertOutputType(\""z_ || z\"", inspector, ExpressionType.LONG);\n     assertOutputType(\""1*(2 + 3.0)\"", inspector, ExpressionType.DOUBLE);\n   }\n \n\ndiff --git a/processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java b/processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java\nindex b2c3d37d5892..ba1c1c5b9486 100644\n--- a/processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java\n+++ b/processing/src/test/java/org/apache/druid/query/scan/NestedDataScanQueryTest.java\n@@ -30,7 +30,6 @@\n import org.apache.druid.java.util.common.guava.Sequence;\n import org.apache.druid.java.util.common.io.Closer;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.Druids;\n import org.apache.druid.query.NestedDataTestUtils;\n import org.apache.druid.query.Query;\n@@ -668,76 +667,6 @@ public void testIngestAndScanSegmentsRealtimeSchemaDiscoveryArrayTypes() throws\n     Assert.assertEquals(resultsSegments.get(0).getEvents().toString(), resultsRealtime.get(0).getEvents().toString());\n   }\n \n-  @Test\n-  public void testIngestAndScanSegmentsRealtimeSchemaDiscoveryMoreArrayTypesNonStrictBooleans() throws Exception\n-  {\n-\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      Druids.ScanQueryBuilder builder = Druids.newScanQueryBuilder()\n-                                              .dataSource(\""test_datasource\"")\n-                                              .intervals(\n-                                                  new MultipleIntervalSegmentSpec(\n-                                                      Collections.singletonList(Intervals.ETERNITY)\n-                                                  )\n-                                              )\n-                                              .resultFormat(ScanQuery.ResultFormat.RESULT_FORMAT_COMPACTED_LIST)\n-                                              .limit(100)\n-                                              .context(ImmutableMap.of());\n-      Query<ScanResultValue> scanQuery = builder.build();\n-      final AggregatorFactory[] aggs = new AggregatorFactory[]{new CountAggregatorFactory(\""count\"")};\n-      List<Segment> realtimeSegs = ImmutableList.of(\n-          NestedDataTestUtils.createIncrementalIndex(\n-              tempFolder,\n-              NestedDataTestUtils.ARRAY_TYPES_DATA_FILE_2,\n-              TestIndex.DEFAULT_JSON_INPUT_FORMAT,\n-              NestedDataTestUtils.TIMESTAMP_SPEC,\n-              NestedDataTestUtils.AUTO_DISCOVERY,\n-              TransformSpec.NONE,\n-              aggs,\n-              Granularities.NONE,\n-              true\n-          )\n-      );\n-      List<Segment> segs = NestedDataTestUtils.createSegments(\n-          tempFolder,\n-          closer,\n-          NestedDataTestUtils.ARRAY_TYPES_DATA_FILE_2,\n-          TestIndex.DEFAULT_JSON_INPUT_FORMAT,\n-          NestedDataTestUtils.TIMESTAMP_SPEC,\n-          NestedDataTestUtils.AUTO_DISCOVERY,\n-          TransformSpec.NONE,\n-          aggs,\n-          Granularities.NONE,\n-          true,\n-          IndexSpec.DEFAULT\n-      );\n-\n-\n-      final Sequence<ScanResultValue> seq = helper.runQueryOnSegmentsObjs(realtimeSegs, scanQuery);\n-      final Sequence<ScanResultValue> seq2 = helper.runQueryOnSegmentsObjs(segs, scanQuery);\n-\n-      List<ScanResultValue> resultsRealtime = seq.toList();\n-      List<ScanResultValue> resultsSegments = seq2.toList();\n-      logResults(resultsSegments);\n-      logResults(resultsRealtime);\n-      Assert.assertEquals(1, resultsRealtime.size());\n-      Assert.assertEquals(resultsRealtime.size(), resultsSegments.size());\n-      Assert.assertEquals(\n-          \""[\""\n-          + \""[978652800000, [A, A], [null, null], [1, 1], [0.1, 0.1], [true, true], [null, null], {s_str1=[A, A], s_str2=[null, null], s_num_int=[1, 1], s_num_float=[0.1, 0.1], s_bool=[true, true], s_null=[null, null]}, 1], \""\n-          + \""[978739200000, [A, A], [null, null], [1, 1], [0.1, 0.1], [true, true], [null, null], {s_str1=[A, A], s_str2=[null, null], s_num_int=[1, 1], s_num_float=[0.1, 0.1], s_bool=[true, true], s_null=[null, null]}, 1], \""\n-          + \""[978825600000, [A, A], [null, null], [1, 1], [0.1, 0.1], [true, true], [null, null], {s_str1=[A, A], s_str2=[null, null], s_num_int=[1, 1], s_num_float=[0.1, 0.1], s_bool=[true, true], s_null=[null, null]}, 1], \""\n-          + \""[978912000000, [A, A], [null, null], [1, 1], [0.1, 0.1], [true, true], [null, null], {s_str1=[A, A], s_str2=[null, null], s_num_int=[1, 1], s_num_float=[0.1, 0.1], s_bool=[true, true], s_null=[null, null]}, 1]]\"",\n-          resultsSegments.get(0).getEvents().toString()\n-      );\n-      Assert.assertEquals(resultsSegments.get(0).getEvents().toString(), resultsRealtime.get(0).getEvents().toString());\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-  }\n-\n   @Test\n   public void testIngestAndScanSegmentsRealtimeSchemaDiscoveryMoreArrayTypesStrictBooleans() throws Exception\n   {\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterNonStrictBooleansTest.java b/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterNonStrictBooleansTest.java\ndeleted file mode 100644\nindex 8b60360b6c16..000000000000\n--- a/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterNonStrictBooleansTest.java\n+++ /dev/null\n@@ -1,143 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.druid.segment.filter;\n-\n-import com.google.common.base.Function;\n-import com.google.common.collect.ImmutableList;\n-import org.apache.druid.common.config.NullHandling;\n-import org.apache.druid.java.util.common.Pair;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n-import org.apache.druid.query.filter.NotDimFilter;\n-import org.apache.druid.segment.CursorFactory;\n-import org.apache.druid.segment.IndexBuilder;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-import java.io.Closeable;\n-\n-@RunWith(Parameterized.class)\n-public class ExpressionFilterNonStrictBooleansTest extends ExpressionFilterTest\n-{\n-  public ExpressionFilterNonStrictBooleansTest(\n-      String testName,\n-      IndexBuilder indexBuilder,\n-      Function<IndexBuilder, Pair<CursorFactory, Closeable>> finisher,\n-      boolean cnf,\n-      boolean optimize\n-  )\n-  {\n-    super(testName, indexBuilder, finisher, cnf, optimize);\n-  }\n-\n-  @Before\n-  @Override\n-  public void setup()\n-  {\n-    ExpressionProcessing.initializeForStrictBooleansTests(false);\n-  }\n-\n-  @Override\n-  @Test\n-  public void testComplement()\n-  {\n-    if (NullHandling.sqlCompatible()) {\n-      assertFilterMatches(edf(\""dim5 == 'a'\""), ImmutableList.of(\""0\""));\n-      // non-strict mode is wild\n-      assertFilterMatches(\n-          NotDimFilter.of(edf(\""dim5 == 'a'\"")),\n-          ImmutableList.of(\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-      assertFilterMatches(\n-          edf(\""dim5 == ''\""), ImmutableList.of(\""4\"")\n-      );\n-      // non-strict mode!\n-      assertFilterMatches(\n-          NotDimFilter.of(edf(\""dim5 == ''\"")), ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-    } else {\n-      assertFilterMatches(edf(\""dim5 == 'a'\""), ImmutableList.of(\""0\""));\n-      assertFilterMatches(\n-          NotDimFilter.of(edf(\""dim5 == 'a'\"")),\n-          ImmutableList.of(\""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-    }\n-  }\n-\n-  @Override\n-  @Test\n-  public void testMissingColumn()\n-  {\n-    if (NullHandling.replaceWithDefault()) {\n-      assertFilterMatches(\n-          edf(\""missing == ''\""),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-      assertFilterMatches(\n-          edf(\""missing == otherMissing\""),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-    } else {\n-      // AS per SQL standard null == null returns false.\n-      assertFilterMatches(edf(\""missing == null\""), ImmutableList.of());\n-      // in non-strict mode, madness happens\n-      assertFilterMatches(\n-          NotDimFilter.of(edf(\""missing == null\"")),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-      // also this madness doesn't do madness\n-      assertFilterMatches(\n-          edf(\""missing == otherMissing\""),\n-          ImmutableList.of()\n-      );\n-      assertFilterMatches(\n-          NotDimFilter.of(edf(\""missing == otherMissing\"")),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-    }\n-    assertFilterMatches(edf(\""missing == '1'\""), ImmutableList.of());\n-    assertFilterMatches(edf(\""missing == 2\""), ImmutableList.of());\n-    if (NullHandling.replaceWithDefault()) {\n-      // missing equivaluent to 0\n-      assertFilterMatches(\n-          edf(\""missing < '2'\""),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-      assertFilterMatches(\n-          edf(\""missing < 2\""),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-      assertFilterMatches(\n-          edf(\""missing < 2.0\""),\n-          ImmutableList.of(\""0\"", \""1\"", \""2\"", \""3\"", \""4\"", \""5\"", \""6\"", \""7\"", \""8\"", \""9\"")\n-      );\n-    } else {\n-      // missing equivalent to null\n-      assertFilterMatches(edf(\""missing < '2'\""), ImmutableList.of());\n-      assertFilterMatches(edf(\""missing < 2\""), ImmutableList.of());\n-      assertFilterMatches(edf(\""missing < 2.0\""), ImmutableList.of());\n-    }\n-    assertFilterMatches(edf(\""missing > '2'\""), ImmutableList.of());\n-    assertFilterMatches(edf(\""missing > 2\""), ImmutableList.of());\n-    assertFilterMatches(edf(\""missing > 2.0\""), ImmutableList.of());\n-    assertFilterMatchesSkipVectorize(edf(\""like(missing, '1%')\""), ImmutableList.of());\n-  }\n-}\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java b/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java\nindex aa2d21ef9d33..9311a90b4e47 100644\n--- a/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/filter/ExpressionFilterTest.java\n@@ -36,7 +36,6 @@\n import org.apache.druid.data.input.impl.TimestampSpec;\n import org.apache.druid.java.util.common.DateTimes;\n import org.apache.druid.java.util.common.Pair;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.expression.TestExprMacroTable;\n import org.apache.druid.query.filter.ExpressionDimFilter;\n import org.apache.druid.query.filter.Filter;\n@@ -46,10 +45,8 @@\n import org.apache.druid.segment.column.ColumnType;\n import org.apache.druid.segment.column.RowSignature;\n import org.apache.druid.segment.incremental.IncrementalIndexSchema;\n-import org.junit.After;\n import org.junit.AfterClass;\n import org.junit.Assert;\n-import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -126,22 +123,10 @@ public ExpressionFilterTest(\n     );\n   }\n \n-  @Before\n-  public void setup()\n-  {\n-    ExpressionProcessing.initializeForStrictBooleansTests(true);\n-  }\n-\n-  @After\n-  public void teardown()\n-  {\n-    ExpressionProcessing.initializeForTests();\n-  }\n-\n   @AfterClass\n   public static void tearDown() throws Exception\n   {\n-    BaseFilterTest.tearDown(ColumnComparisonFilterTest.class.getName());\n+    BaseFilterTest.tearDown(ExpressionFilterTest.class.getName());\n   }\n \n   @Test\n\ndiff --git a/processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java b/processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java\nindex 3ed0a51bf039..90413b6ab023 100644\n--- a/processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/transform/TransformSpecTest.java\n@@ -30,7 +30,6 @@\n import org.apache.druid.data.input.impl.TimeAndDimsParseSpec;\n import org.apache.druid.data.input.impl.TimestampSpec;\n import org.apache.druid.java.util.common.DateTimes;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.query.expression.TestExprMacroTable;\n import org.apache.druid.query.filter.AndDimFilter;\n import org.apache.druid.query.filter.SelectorDimFilter;\n@@ -208,68 +207,31 @@ public void testTransformTimeFromTime()\n   @Test\n   public void testBoolTransforms()\n   {\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(true);\n-      final TransformSpec transformSpec = new TransformSpec(\n-          null,\n-          ImmutableList.of(\n-              new ExpressionTransform(\""truthy1\"", \""bool\"", TestExprMacroTable.INSTANCE),\n-              new ExpressionTransform(\""truthy2\"", \""if(bool,1,0)\"", TestExprMacroTable.INSTANCE)\n-          )\n-      );\n-\n-      Assert.assertEquals(\n-          ImmutableSet.of(\""bool\""),\n-          transformSpec.getRequiredColumns()\n-      );\n-\n-      final InputRowParser<Map<String, Object>> parser = transformSpec.decorate(PARSER);\n-      final InputRow row = parser.parseBatch(ROW1).get(0);\n-\n-      Assert.assertNotNull(row);\n-      Assert.assertEquals(1L, row.getRaw(\""truthy1\""));\n-      Assert.assertEquals(1L, row.getRaw(\""truthy2\""));\n-\n-      final InputRow row2 = parser.parseBatch(ROW2).get(0);\n-\n-      Assert.assertNotNull(row2);\n-      Assert.assertEquals(0L, row2.getRaw(\""truthy1\""));\n-      Assert.assertEquals(0L, row2.getRaw(\""truthy2\""));\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n-    try {\n-      ExpressionProcessing.initializeForStrictBooleansTests(false);\n-      final TransformSpec transformSpec = new TransformSpec(\n-          null,\n-          ImmutableList.of(\n-              new ExpressionTransform(\""truthy1\"", \""bool\"", TestExprMacroTable.INSTANCE),\n-              new ExpressionTransform(\""truthy2\"", \""if(bool,1,0)\"", TestExprMacroTable.INSTANCE)\n-              )\n-      );\n-\n-      Assert.assertEquals(\n-          ImmutableSet.of(\""bool\""),\n-          transformSpec.getRequiredColumns()\n-      );\n-\n-      final InputRowParser<Map<String, Object>> parser = transformSpec.decorate(PARSER);\n-      final InputRow row = parser.parseBatch(ROW1).get(0);\n-\n-      Assert.assertNotNull(row);\n-      Assert.assertEquals(\""true\"", row.getRaw(\""truthy1\""));\n-      Assert.assertEquals(1L, row.getRaw(\""truthy2\""));\n-\n-      final InputRow row2 = parser.parseBatch(ROW2).get(0);\n-\n-      Assert.assertNotNull(row2);\n-      Assert.assertEquals(\""false\"", row2.getRaw(\""truthy1\""));\n-      Assert.assertEquals(0L, row2.getRaw(\""truthy2\""));\n-    }\n-    finally {\n-      ExpressionProcessing.initializeForTests();\n-    }\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(\n+            new ExpressionTransform(\""truthy1\"", \""bool\"", TestExprMacroTable.INSTANCE),\n+            new ExpressionTransform(\""truthy2\"", \""if(bool,1,0)\"", TestExprMacroTable.INSTANCE)\n+        )\n+    );\n+\n+    Assert.assertEquals(\n+        ImmutableSet.of(\""bool\""),\n+        transformSpec.getRequiredColumns()\n+    );\n+\n+    final InputRowParser<Map<String, Object>> parser = transformSpec.decorate(PARSER);\n+    final InputRow row = parser.parseBatch(ROW1).get(0);\n+\n+    Assert.assertNotNull(row);\n+    Assert.assertEquals(1L, row.getRaw(\""truthy1\""));\n+    Assert.assertEquals(1L, row.getRaw(\""truthy2\""));\n+\n+    final InputRow row2 = parser.parseBatch(ROW2).get(0);\n+\n+    Assert.assertNotNull(row2);\n+    Assert.assertEquals(0L, row2.getRaw(\""truthy1\""));\n+    Assert.assertEquals(0L, row2.getRaw(\""truthy2\""));\n   }\n \n   @Test\n\ndiff --git a/sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java b/sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java\nindex 5a6db426d29b..f84c8455861f 100644\n--- a/sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java\n+++ b/sql/src/test/java/org/apache/druid/sql/calcite/rule/DruidLogicalValuesRuleTest.java\n@@ -31,7 +31,6 @@\n import org.apache.druid.common.config.NullHandling;\n import org.apache.druid.error.DruidExceptionMatcher;\n import org.apache.druid.java.util.common.DateTimes;\n-import org.apache.druid.math.expr.ExpressionProcessing;\n import org.apache.druid.sql.calcite.planner.DruidTypeSystem;\n import org.apache.druid.sql.calcite.planner.PlannerContext;\n import org.apache.druid.testing.InitializedNullHandlingTest;\n@@ -144,7 +143,7 @@ public void testGetValueFromNullBooleanLiteral()\n     {\n       RexLiteral literal = REX_BUILDER.makeLiteral(null, REX_BUILDER.getTypeFactory().createSqlType(SqlTypeName.BOOLEAN));\n \n-      if (NullHandling.sqlCompatible() && ExpressionProcessing.useStrictBooleans()) {\n+      if (NullHandling.sqlCompatible()) {\n         final Object fromLiteral = DruidLogicalValuesRule.getValueFromLiteral(literal, DEFAULT_CONTEXT);\n         Assert.assertNull(fromLiteral);\n       } else {\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"

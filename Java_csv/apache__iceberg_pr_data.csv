metadata
"{""instance_id"": ""apache__iceberg-12637"", ""pr_id"": 12637, ""issue_id"": 11814, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Table corruption using lock-free Hive commits\n### Apache Iceberg version\n\n1.6.1\n\n### Query engine\n\nSpark\n\n### Please describe the bug \ud83d\udc1e\n\nWe observed the following situation happen a few times now when using lock-free Hive catalog commits introduced in https://github.com/apache/iceberg/pull/6570:\r\n\r\n We run an `ALTER TABLE table SET TBLPROPERTIES ('key' = 'value')` or any other operation that results in an Iceberg commit, either Spark or any other engine. For whatever reason the connection to the Hive metastore is broken and the HMS operation fails during the first attempt:\r\n```\r\nWARN org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. alter_table_with_environmentContext\r\norg.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset\r\n<...>\r\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1693)\r\n<...>\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)\r\n<...>\r\nat org.apache.iceberg.hive.MetastoreUtil.alterTable(MetastoreUtil.java:78)\r\nat org.apache.iceberg.hive.HiveOperationsBase.lambda$persistTable$0(HiveOperationsBase.java:112)\r\n<...>\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:239)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:135)\r\n<...>\r\nat org.apache.iceberg.spark.SparkCatalog.alterTable(SparkCatalog.java:345)\r\n<...>\r\n```\r\nbut the operation actually succeeds and updates the metadata location, which means that when the `RetryingMetaStoreClient` attempts resubmitting the operation, it fails with:\r\n```\r\nMetaException(message:The table has been modified. The parameter value for key 'metadata_location' is '<new>'. The expected was value was '<previous>')\r\n```\r\nThe Iceberg commit is then considered failed and the new metadata file is cleaned up in the `finally` block [here](https://github.com/apache/iceberg/blob/b428fbc59bd1579f4dc918a5cd48fce667d81ce1/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L320) before retrying the commit. But the problem is that the Hive table has the new metadata location set, so when Iceberg tries refreshing the table it fails, because the new metadata file no longer exists, leaving the table in a corrupted state.\r\n\r\nI suppose a fix could be checking the exception and ignoring the case when the already set location is equal to the new metadata location, but parsing the error message sounds very hacky.\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time"", ""issue_word_count"": 395, ""test_files_count"": 4, ""non_test_files_count"": 3, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java"", ""core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java"", ""hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java"", ""hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java"", ""hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java"", ""hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java"", ""hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql""], ""pr_changed_test_files"": [""hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java"", ""hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java"", ""hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java"", ""hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql""], ""base_commit"": ""03ff41c189c7420992be0e4a4ddc63f005e2e0d5"", ""head_commit"": ""5c5f6724ac4c8434925623a74e7ec61cc12c44d3"", ""repo_url"": ""https://github.com/apache/iceberg/pull/12637"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/12637"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-02T12:34:32.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\nindex 09c2249046f4..0635b56a7fba 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\n@@ -49,20 +49,58 @@ public enum CommitStatus {\n    * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n    * may indicate the commit has failed but don't have proof that this is the case. Note that all\n    * the previous locations must also be searched on the chance that a second committer was able to\n-   * successfully commit on top of our commit.\n+   * successfully commit on top of our commit. When the {@code newMetadataLocation} is not in the\n+   * history or the {@code commitStatusSupplier} fails repeatedly the method returns {@link\n+   * CommitStatus#UNKNOWN}, because possible pending retries might still commit the change.\n    *\n    * @param tableOrViewName full name of the Table/View\n    * @param newMetadataLocation the path of the new commit file\n    * @param properties properties for retry\n    * @param commitStatusSupplier check if the latest metadata presents or not using metadata\n    *     location for table.\n-   * @return Commit Status of Success, Failure or Unknown\n+   * @return Commit Status of Success or Unknown\n    */\n   protected CommitStatus checkCommitStatus(\n       String tableOrViewName,\n       String newMetadataLocation,\n       Map<String, String> properties,\n       Supplier<Boolean> commitStatusSupplier) {\n+    CommitStatus strictStatus =\n+        checkCommitStatusStrict(\n+            tableOrViewName, newMetadataLocation, properties, commitStatusSupplier);\n+    if (strictStatus == CommitStatus.FAILURE) {\n+      LOG.warn(\n+          \""Commit status check: Commit to {} of {} unknown, new metadata location is not current \""\n+              + \""or in history\"",\n+          tableOrViewName,\n+          newMetadataLocation);\n+      return CommitStatus.UNKNOWN;\n+    }\n+    return strictStatus;\n+  }\n+\n+  /**\n+   * Attempt to load the content and see if any current or past metadata location matches the one we\n+   * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n+   * may indicate the commit has failed and don't have proof that this is the case, but we can be\n+   * sure that no retry attempts for the commit will be successful later. Note that all the previous\n+   * locations must also be searched on the chance that a second committer was able to successfully\n+   * commit on top of our commit. When the {@code newMetadataLocation} is not in the history the\n+   * method returns {@link CommitStatus#FAILURE}, when the {@code commitStatusSupplier} fails\n+   * repeatedly the method returns {@link CommitStatus#UNKNOWN}.\n+   *\n+   * @param tableOrViewName full name of the Table/View\n+   * @param newMetadataLocation the path of the new commit file\n+   * @param properties properties for retry\n+   * @param commitStatusSupplier check if the latest metadata presents or not using metadata\n+   *     location for table.\n+   * @return Commit Status of Success, Failure or Unknown\n+   */\n+  protected CommitStatus checkCommitStatusStrict(\n+      String tableOrViewName,\n+      String newMetadataLocation,\n+      Map<String, String> properties,\n+      Supplier<Boolean> commitStatusSupplier) {\n     int maxAttempts =\n         PropertyUtil.propertyAsInt(\n             properties, COMMIT_NUM_STATUS_CHECKS, COMMIT_NUM_STATUS_CHECKS_DEFAULT);\n@@ -98,11 +136,7 @@ protected CommitStatus checkCommitStatus(\n                     newMetadataLocation);\n                 status.set(CommitStatus.SUCCESS);\n               } else {\n-                LOG.warn(\n-                    \""Commit status check: Commit to {} of {} unknown, new metadata location is not current \""\n-                        + \""or in history\"",\n-                    tableOrViewName,\n-                    newMetadataLocation);\n+                status.set(CommitStatus.FAILURE);\n               }\n             });\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\nindex dbab9e813966..9fa52d52ea5d 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.function.Function;\n import java.util.function.Predicate;\n+import org.apache.iceberg.BaseMetastoreOperations.CommitStatus;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n import org.apache.iceberg.exceptions.CommitFailedException;\n@@ -286,20 +287,39 @@ public long newSnapshotId() {\n    * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n    * may indicate the commit has failed but are not proof that this is the case. Past locations must\n    * also be searched on the chance that a second committer was able to successfully commit on top\n-   * of our commit.\n+   * of our commit. When the {@code newMetadataLocation} is not found, the method returns {@link\n+   * CommitStatus#UNKNOWN}.\n    *\n    * @param newMetadataLocation the path of the new commit file\n    * @param config metadata to use for configuration\n-   * @return Commit Status of Success, Failure or Unknown\n+   * @return Commit Status of Success, Unknown\n    */\n   protected CommitStatus checkCommitStatus(String newMetadataLocation, TableMetadata config) {\n-    return CommitStatus.valueOf(\n-        checkCommitStatus(\n-                tableName(),\n-                newMetadataLocation,\n-                config.properties(),\n-                () -> checkCurrentMetadataLocation(newMetadataLocation))\n-            .name());\n+    return checkCommitStatus(\n+        tableName(),\n+        newMetadataLocation,\n+        config.properties(),\n+        () -> checkCurrentMetadataLocation(newMetadataLocation));\n+  }\n+\n+  /**\n+   * Attempt to load the table and see if any current or past metadata location matches the one we\n+   * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n+   * may indicate the commit has failed but are not proof that this is the case. Past locations must\n+   * also be searched on the chance that a second committer was able to successfully commit on top\n+   * of our commit. When the {@code newMetadataLocation} is not found, the method returns {@link\n+   * CommitStatus#FAILURE}.\n+   *\n+   * @param newMetadataLocation the path of the new commit file\n+   * @param config metadata to use for configuration\n+   * @return Commit Status of Success, Failure or Unknown\n+   */\n+  protected CommitStatus checkCommitStatusStrict(String newMetadataLocation, TableMetadata config) {\n+    return checkCommitStatusStrict(\n+        tableName(),\n+        newMetadataLocation,\n+        config.properties(),\n+        () -> checkCurrentMetadataLocation(newMetadataLocation));\n   }\n \n   /**\n\ndiff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\nindex 619f20ab87a3..0e801b57e5eb 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n@@ -268,16 +268,6 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n         throw e;\n \n       } catch (Throwable e) {\n-        if (e.getMessage() != null\n-            && e.getMessage()\n-                .contains(\n-                    \""The table has been modified. The parameter value for key '\""\n-                        + HiveTableOperations.METADATA_LOCATION_PROP\n-                        + \""' is\"")) {\n-          throw new CommitFailedException(\n-              e, \""The table %s.%s has been modified concurrently\"", database, tableName);\n-        }\n-\n         if (e.getMessage() != null\n             && e.getMessage().contains(\""Table/View 'HIVE_LOCKS' does not exist\"")) {\n           throw new RuntimeException(\n@@ -287,15 +277,31 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n               e);\n         }\n \n-        LOG.error(\n-            \""Cannot tell if commit to {}.{} succeeded, attempting to reconnect and check.\"",\n-            database,\n-            tableName,\n-            e);\n         commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n-        commitStatus =\n-            BaseMetastoreOperations.CommitStatus.valueOf(\n-                checkCommitStatus(newMetadataLocation, metadata).name());\n+        if (e.getMessage() != null\n+            && e.getMessage()\n+                .contains(\n+                    \""The table has been modified. The parameter value for key '\""\n+                        + HiveTableOperations.METADATA_LOCATION_PROP\n+                        + \""' is\"")) {\n+          // It's possible the HMS client incorrectly retries a successful operation, due to network\n+          // issue for example, and triggers this exception. So we need double-check to make sure\n+          // this is really a concurrent modification. Hitting this exception means no pending\n+          // requests, if any, can succeed later, so it's safe to check status in strict mode\n+          commitStatus = checkCommitStatusStrict(newMetadataLocation, metadata);\n+          if (commitStatus == BaseMetastoreOperations.CommitStatus.FAILURE) {\n+            throw new CommitFailedException(\n+                e, \""The table %s.%s has been modified concurrently\"", database, tableName);\n+          }\n+        } else {\n+          LOG.error(\n+              \""Cannot tell if commit to {}.{} succeeded, attempting to reconnect and check.\"",\n+              database,\n+              tableName,\n+              e);\n+          commitStatus = checkCommitStatus(newMetadataLocation, metadata);\n+        }\n+\n         switch (commitStatus) {\n           case SUCCESS:\n             break;\n"", ""test_patch"": ""diff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\nindex c750ff4de62e..fe37223423fa 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\n@@ -48,7 +48,7 @@ public void beforeAll(ExtensionContext extensionContext) throws Exception {\n       }\n     }\n \n-    metastore.start(hiveConfWithOverrides);\n+    metastore.start(hiveConfWithOverrides, 5, true);\n     metastoreClient = new HiveMetaStoreClient(hiveConfWithOverrides);\n     if (null != databaseName) {\n       String dbPath = metastore.getDatabasePath(databaseName);\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\nindex d12a8503313b..0ffcb057095f 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\n@@ -22,6 +22,7 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.any;\n import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.doAnswer;\n@@ -63,6 +64,7 @@\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.CommitFailedException;\n import org.apache.iceberg.hadoop.ConfigProperties;\n@@ -205,6 +207,37 @@ public static void cleanup() {\n     }\n   }\n \n+  @Test\n+  public void testMultipleAlterTableForNoLock() throws Exception {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    table.updateProperties().set(TableProperties.HIVE_LOCK_ENABLED, \""false\"").commit();\n+    spyOps.refresh();\n+    TableMetadata metadataV3 = spyOps.current();\n+    AtomicReference<Throwable> alterTableException = new AtomicReference<>(null);\n+    doAnswer(\n+            i -> {\n+              try {\n+                // mock a situation where alter table is unexpectedly invoked more than once\n+                i.callRealMethod();\n+                return i.callRealMethod();\n+              } catch (Throwable e) {\n+                alterTableException.compareAndSet(null, e);\n+                throw e;\n+              }\n+            })\n+        .when(spyClient)\n+        .alter_table_with_environmentContext(anyString(), anyString(), any(), any());\n+    spyOps.commit(metadataV3, metadataV1);\n+    verify(spyClient, times(1))\n+        .alter_table_with_environmentContext(anyString(), anyString(), any(), any());\n+    assertThat(alterTableException)\n+        .as(\""Expecting to trigger an exception indicating table has been modified\"")\n+        .hasValueMatching(\n+            t ->\n+                t.getMessage()\n+                    .contains(\""The table has been modified. The parameter value for key '\""));\n+  }\n+\n   @Test\n   public void testLockAcquisitionAtFirstTime() throws TException, InterruptedException {\n     doReturn(acquiredLockResponse).when(spyClient).lock(any());\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\nindex c141f0cced02..9736b32e8727 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n@@ -153,10 +153,21 @@ public void start(HiveConf conf) {\n    * @param poolSize The number of threads in the executor pool\n    */\n   public void start(HiveConf conf, int poolSize) {\n+    start(conf, poolSize, false);\n+  }\n+\n+  /**\n+   * Starts a TestHiveMetastore with a provided connection pool size and HiveConf.\n+   *\n+   * @param conf The hive configuration to use\n+   * @param poolSize The number of threads in the executor pool\n+   * @param directSql Used to turn on directSql\n+   */\n+  public void start(HiveConf conf, int poolSize, boolean directSql) {\n     try {\n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      initConf(conf, port);\n+      initConf(conf, port, directSql);\n \n       this.hiveConf = conf;\n       this.server = newThriftServer(socket, poolSize, hiveConf);\n@@ -261,11 +272,11 @@ private TServer newThriftServer(TServerSocket socket, int poolSize, HiveConf con\n     return new TThreadPoolServer(args);\n   }\n \n-  private void initConf(HiveConf conf, int port) {\n+  private void initConf(HiveConf conf, int port, boolean directSql) {\n     conf.set(HiveConf.ConfVars.METASTOREURIS.varname, \""thrift://localhost:\"" + port);\n     conf.set(\n         HiveConf.ConfVars.METASTOREWAREHOUSE.varname, \""file:\"" + HIVE_LOCAL_DIR.getAbsolutePath());\n-    conf.set(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL.varname, \""false\"");\n+    conf.set(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL.varname, String.valueOf(directSql));\n     conf.set(HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES.varname, \""false\"");\n     conf.set(\""iceberg.hive.client-pool-size\"", \""2\"");\n     // Setting this to avoid thrift exception during running Iceberg tests outside Iceberg.\n\ndiff --git a/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql b/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\nindex 55097d6639f2..b7b095c81ac1 100644\n--- a/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\n+++ b/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\n@@ -52,9 +52,9 @@ CREATE TABLE \""APP\"".\""DATABASE_PARAMS\"" (\""DB_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCH\n \n CREATE TABLE \""APP\"".\""TBL_COL_PRIVS\"" (\""TBL_COLUMN_GRANT_ID\"" BIGINT NOT NULL, \""COLUMN_NAME\"" VARCHAR(767), \""CREATE_TIME\"" INTEGER NOT NULL, \""GRANT_OPTION\"" SMALLINT NOT NULL, \""GRANTOR\"" VARCHAR(128), \""GRANTOR_TYPE\"" VARCHAR(128), \""PRINCIPAL_NAME\"" VARCHAR(128), \""PRINCIPAL_TYPE\"" VARCHAR(128), \""TBL_COL_PRIV\"" VARCHAR(128), \""TBL_ID\"" BIGINT, \""AUTHORIZER\"" VARCHAR(128));\n \n-CREATE TABLE \""APP\"".\""SERDE_PARAMS\"" (\""SERDE_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" CLOB);\n+CREATE TABLE \""APP\"".\""SERDE_PARAMS\"" (\""SERDE_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" VARCHAR(32672));\n \n-CREATE TABLE \""APP\"".\""COLUMNS_V2\"" (\""CD_ID\"" BIGINT NOT NULL, \""COMMENT\"" VARCHAR(4000), \""COLUMN_NAME\"" VARCHAR(767) NOT NULL, \""TYPE_NAME\"" CLOB, \""INTEGER_IDX\"" INTEGER NOT NULL);\n+CREATE TABLE \""APP\"".\""COLUMNS_V2\"" (\""CD_ID\"" BIGINT NOT NULL, \""COMMENT\"" VARCHAR(4000), \""COLUMN_NAME\"" VARCHAR(767) NOT NULL, \""TYPE_NAME\"" VARCHAR(32672), \""INTEGER_IDX\"" INTEGER NOT NULL);\n \n CREATE TABLE \""APP\"".\""SORT_COLS\"" (\""SD_ID\"" BIGINT NOT NULL, \""COLUMN_NAME\"" VARCHAR(767), \""ORDER\"" INTEGER NOT NULL, \""INTEGER_IDX\"" INTEGER NOT NULL);\n \n@@ -130,7 +130,7 @@ CREATE TABLE \""APP\"".\""TAB_COL_STATS\""(\n     \""BIT_VECTOR\"" BLOB\n );\n \n-CREATE TABLE \""APP\"".\""TABLE_PARAMS\"" (\""TBL_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" CLOB);\n+CREATE TABLE \""APP\"".\""TABLE_PARAMS\"" (\""TBL_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" VARCHAR(32672));\n \n CREATE TABLE \""APP\"".\""BUCKETING_COLS\"" (\""SD_ID\"" BIGINT NOT NULL, \""BUCKET_COL_NAME\"" VARCHAR(256), \""INTEGER_IDX\"" INTEGER NOT NULL);\n \n@@ -138,7 +138,7 @@ CREATE TABLE \""APP\"".\""TYPE_FIELDS\"" (\""TYPE_NAME\"" BIGINT NOT NULL, \""COMMENT\"" VARCHAR\n \n CREATE TABLE \""APP\"".\""NUCLEUS_TABLES\"" (\""CLASS_NAME\"" VARCHAR(128) NOT NULL, \""TABLE_NAME\"" VARCHAR(128) NOT NULL, \""TYPE\"" VARCHAR(4) NOT NULL, \""OWNER\"" VARCHAR(2) NOT NULL, \""VERSION\"" VARCHAR(20) NOT NULL, \""INTERFACE_NAME\"" VARCHAR(256) DEFAULT NULL);\n \n-CREATE TABLE \""APP\"".\""SD_PARAMS\"" (\""SD_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" CLOB);\n+CREATE TABLE \""APP\"".\""SD_PARAMS\"" (\""SD_ID\"" BIGINT NOT NULL, \""PARAM_KEY\"" VARCHAR(256) NOT NULL, \""PARAM_VALUE\"" VARCHAR(32672));\n \n CREATE TABLE \""APP\"".\""SKEWED_STRING_LIST\"" (\""STRING_LIST_ID\"" BIGINT NOT NULL);\n \n@@ -218,7 +218,7 @@ CREATE TABLE \""APP\"".\""MV_CREATION_METADATA\"" (\n   \""CAT_NAME\"" VARCHAR(256) NOT NULL,\n   \""DB_NAME\"" VARCHAR(128) NOT NULL,\n   \""TBL_NAME\"" VARCHAR(256) NOT NULL,\n-  \""TXN_LIST\"" CLOB,\n+  \""TXN_LIST\"" VARCHAR(32672),\n   \""MATERIALIZATION_TIME\"" BIGINT NOT NULL\n );\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-12520"", ""pr_id"": 12520, ""issue_id"": 12495, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Unable to set `write-default` for a column\n### Apache Iceberg version\n\n1.8.1 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug \ud83d\udc1e\n\n- I am attempting to create a table via the  Iceberg Java API with the `write-default` values set on some columns. After creating the table and reading the schema back, I expected the schema to include the default value definitions but they are missing.\n\n```\n// To create a new teable\nfinal TableIdentifier tableIdentifier = TableIdentifier.parse(\""MyNamespace.MyTable\"");\nfinal Schema schema = new Schema(List.of(\n        Types.NestedField.required(1, \""intCol\"", Types.IntegerType.get()),\n        Types.NestedField.required(2, \""doubleCol\"", Types.DoubleType.get()),\n        Types.NestedField.required(3, \""longCol\"", Types.LongType.get()),\n        Types.NestedField.required(\""newIntCol\"")\n                .withId(4)\n                .ofType(Types.IntegerType.get())\n                .withWriteDefault(Integer.valueOf(\""10\""))\n                .build()\n));\n\ncatalog.createTable(tableIdentifier, schema);\n\n\n// To read back the schema\ncatalog.loadTable.loadTable(tableIdentifier).schema();\n// Schema doesn't have write default set for `newIntCol`\n```\nSame issue also exist for `initial-default` too.\n\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [x] I cannot contribute a fix for this bug at this time"", ""issue_word_count"": 197, ""test_files_count"": 4, ""non_test_files_count"": 3, ""pr_changed_files"": [""api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java"", ""api/src/main/java/org/apache/iceberg/types/AssignIds.java"", ""api/src/main/java/org/apache/iceberg/types/ReassignDoc.java"", ""api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java"", ""core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"", ""spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java""], ""pr_changed_test_files"": [""api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java"", ""core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"", ""spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java""], ""base_commit"": ""c02ebe4740b22d6f5a78b636aea2d918037b2751"", ""head_commit"": ""6eb83f886a634148c5aa9a4e258d8cb5e5f5ee23"", ""repo_url"": ""https://github.com/apache/iceberg/pull/12520"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/12520"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-14T13:20:20.000Z"", ""patch"": ""diff --git a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\nindex 75055cddc197..f3759f1d72f3 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n@@ -88,11 +88,7 @@ public Type struct(Types.StructType struct, Iterable<Type> futures) {\n     for (int i = 0; i < length; i += 1) {\n       Types.NestedField field = fields.get(i);\n       Type type = types.next();\n-      if (field.isOptional()) {\n-        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type, field.doc()));\n-      } else {\n-        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type, field.doc()));\n-      }\n+      newFields.add(Types.NestedField.from(field).withId(newIds.get(i)).ofType(type).build());\n     }\n \n     return Types.StructType.of(newFields);\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignIds.java b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\nindex b2f72751eb89..fd5ac7ff67b9 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n@@ -56,11 +56,7 @@ public Type struct(Types.StructType struct, Iterable<Type> futures) {\n     for (int i = 0; i < length; i += 1) {\n       Types.NestedField field = fields.get(i);\n       Type type = types.next();\n-      if (field.isOptional()) {\n-        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type, field.doc()));\n-      } else {\n-        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type, field.doc()));\n-      }\n+      newFields.add(Types.NestedField.from(field).withId(newIds.get(i)).ofType(type).build());\n     }\n \n     return Types.StructType.of(newFields);\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\nindex 328d81c42885..86527fb3897f 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n@@ -50,13 +50,8 @@ public Type struct(Types.StructType struct, Iterable<Type> fieldTypes) {\n \n       Preconditions.checkNotNull(docField, \""Field \"" + fieldId + \"" not found in source schema\"");\n \n-      if (field.isRequired()) {\n-        newFields.add(\n-            Types.NestedField.required(fieldId, field.name(), types.get(i), docField.doc()));\n-      } else {\n-        newFields.add(\n-            Types.NestedField.optional(fieldId, field.name(), types.get(i), docField.doc()));\n-      }\n+      newFields.add(\n+          Types.NestedField.from(field).ofType(types.get(i)).withDoc(docField.doc()).build());\n     }\n \n     return Types.StructType.of(newFields);\n"", ""test_patch"": ""diff --git a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\nindex 078c0180b5e7..9501d4e2503a 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n@@ -653,6 +653,108 @@ public void testReassignOrRefreshIdsCaseInsensitive() {\n     assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n   }\n \n+  @Test\n+  public void testAssignIds() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \""a\"", Types.IntegerType.get()),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \""B\"", Types.IntegerType.get())));\n+\n+    Type actualSchema = TypeUtil.assignIds(schema.asStruct(), oldId -> oldId + 10);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(10, \""a\"", Types.IntegerType.get()),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(11)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(12, \""B\"", Types.IntegerType.get())));\n+\n+    assertThat(actualSchema).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @Test\n+  public void testAssignFreshIds() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \""a\"", Types.IntegerType.get()),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \""B\"", Types.IntegerType.get())));\n+\n+    Schema actualSchema = TypeUtil.assignFreshIds(schema, new AtomicInteger(10)::incrementAndGet);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(11, \""a\"", Types.IntegerType.get()),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(12)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(13, \""B\"", Types.IntegerType.get())));\n+\n+    assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @Test\n+  public void testReassignDoc() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \""a\"", Types.IntegerType.get()),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \""B\"", Types.IntegerType.get())));\n+\n+    Schema docSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \""a\"", Types.IntegerType.get(), \""a_doc\""),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withDoc(\""c_doc\"")\n+                    .build(),\n+                required(2, \""B\"", Types.IntegerType.get(), \""b_doc\"")));\n+\n+    Schema actualSchema = TypeUtil.reassignDoc(schema, docSchema);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \""a\"", Types.IntegerType.get(), \""a_doc\""),\n+                Types.NestedField.required(\""c\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .withDoc(\""c_doc\"")\n+                    .build(),\n+                required(2, \""B\"", Types.IntegerType.get(), \""b_doc\"")));\n+\n+    assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n   private static Stream<Arguments> testTypes() {\n     return Stream.of(\n         Arguments.of(Types.UnknownType.get()),\n\ndiff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex a8afcc83c6d0..b8aea5e2a167 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -62,6 +62,7 @@\n import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.metrics.CommitReport;\n import org.apache.iceberg.metrics.MetricsReport;\n@@ -684,6 +685,38 @@ public void testDefaultTableProperties() {\n     assertThat(catalog.dropTable(ident)).as(\""Should successfully drop table\"").isTrue();\n   }\n \n+  @Test\n+  public void testCreateTableWithDefaultColumnValue() {\n+    C catalog = catalog();\n+\n+    TableIdentifier ident = TableIdentifier.of(\""ns\"", \""table\"");\n+\n+    if (requiresNamespaceCreate()) {\n+      catalog.createNamespace(ident.namespace());\n+    }\n+\n+    assertThat(catalog.tableExists(ident)).as(\""Table should not exist\"").isFalse();\n+\n+    Schema schemaWithDefault =\n+        new Schema(\n+            List.of(\n+                Types.NestedField.required(\""colWithDefault\"")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withWriteDefault(Literal.of(10))\n+                    .withInitialDefault(Literal.of(12))\n+                    .build()));\n+\n+    catalog\n+        .buildTable(ident, schemaWithDefault)\n+        .withLocation(\""file:/tmp/ns/table\"")\n+        .withProperty(TableProperties.FORMAT_VERSION, \""3\"")\n+        .create();\n+    assertThat(catalog.tableExists(ident)).as(\""Table should exist\"").isTrue();\n+    assertThat(schemaWithDefault.asStruct())\n+        .isEqualTo(catalog.loadTable(ident).schema().asStruct());\n+  }\n+\n   @Test\n   public void testLoadTable() {\n     C catalog = catalog();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 3a269740b709..06d5e0c44fb3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -24,6 +24,7 @@\n import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n+import java.util.Map;\n import org.apache.avro.generic.GenericData;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.BaseTable;\n@@ -32,7 +33,9 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.data.AvroDataTest;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -84,7 +87,15 @@ protected void writeAndValidate(Schema writeSchema, Schema expectedSchema) throw\n     File location = new File(parent, \""test\"");\n \n     HadoopTables tables = new HadoopTables(CONF);\n-    Table table = tables.create(writeSchema, PartitionSpec.unpartitioned(), location.toString());\n+    // If V3 spec features are used, set the format version to 3\n+    Map<String, String> tableProperties =\n+        writeSchema.columns().stream()\n+                .anyMatch(f -> f.initialDefaultLiteral() != null || f.writeDefaultLiteral() != null)\n+            ? ImmutableMap.of(TableProperties.FORMAT_VERSION, \""3\"")\n+            : ImmutableMap.of();\n+    Table table =\n+        tables.create(\n+            writeSchema, PartitionSpec.unpartitioned(), tableProperties, location.toString());\n \n     // Important: use the table's schema for the rest of the test\n     // When tables are created, the column ids are reassigned.\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 3a269740b709..06d5e0c44fb3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -24,6 +24,7 @@\n import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n+import java.util.Map;\n import org.apache.avro.generic.GenericData;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.BaseTable;\n@@ -32,7 +33,9 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.data.AvroDataTest;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -84,7 +87,15 @@ protected void writeAndValidate(Schema writeSchema, Schema expectedSchema) throw\n     File location = new File(parent, \""test\"");\n \n     HadoopTables tables = new HadoopTables(CONF);\n-    Table table = tables.create(writeSchema, PartitionSpec.unpartitioned(), location.toString());\n+    // If V3 spec features are used, set the format version to 3\n+    Map<String, String> tableProperties =\n+        writeSchema.columns().stream()\n+                .anyMatch(f -> f.initialDefaultLiteral() != null || f.writeDefaultLiteral() != null)\n+            ? ImmutableMap.of(TableProperties.FORMAT_VERSION, \""3\"")\n+            : ImmutableMap.of();\n+    Table table =\n+        tables.create(\n+            writeSchema, PartitionSpec.unpartitioned(), tableProperties, location.toString());\n \n     // Important: use the table's schema for the rest of the test\n     // When tables are created, the column ids are reassigned.\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-12512"", ""pr_id"": 12512, ""issue_id"": 12472, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Parquet: Support Variant Array read and write\n### Feature Request / Improvement\n\n#12139 implements the Variant readers and #12323 implements the Variant writers. The arrays currently are not supported yet. We need to add reading and writing arrays in a Variant for non-shredded and shredded arrays. The shredded arrays are stored in the format listed in https://github.com/apache/parquet-format/blob/master/VariantShredding.md.\n \n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [x] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time"", ""issue_word_count"": 103, ""test_files_count"": 3, ""non_test_files_count"": 5, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/variants/ValueArray.java"", ""core/src/main/java/org/apache/iceberg/variants/Variants.java"", ""core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java"", ""core/src/test/java/org/apache/iceberg/variants/TestValueArray.java"", ""parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java"", ""parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java"", ""parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java"", ""parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java"", ""core/src/test/java/org/apache/iceberg/variants/TestValueArray.java"", ""parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java""], ""base_commit"": ""01fe380d455949abb49ebfecd9509afce8764fae"", ""head_commit"": ""df4e4d7f2267dc700e7a6c632eb4cd8844c7c5e5"", ""repo_url"": ""https://github.com/apache/iceberg/pull/12512"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/12512"", ""dockerfile"": """", ""pr_merged_at"": ""2025-04-25T23:17:10.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/variants/ValueArray.java b/core/src/main/java/org/apache/iceberg/variants/ValueArray.java\nnew file mode 100644\nindex 000000000000..3da79bcef106\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/variants/ValueArray.java\n@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.variants;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.List;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class ValueArray implements VariantArray {\n+  private SerializationState serializationState = null;\n+  private List<VariantValue> elements = Lists.newArrayList();\n+\n+  ValueArray() {}\n+\n+  @Override\n+  public VariantValue get(int index) {\n+    return elements.get(index);\n+  }\n+\n+  @Override\n+  public int numElements() {\n+    return elements.size();\n+  }\n+\n+  public void add(VariantValue value) {\n+    elements.add(value);\n+    this.serializationState = null;\n+  }\n+\n+  @Override\n+  public int sizeInBytes() {\n+    if (null == serializationState) {\n+      this.serializationState = new SerializationState(elements);\n+    }\n+\n+    return serializationState.size();\n+  }\n+\n+  @Override\n+  public int writeTo(ByteBuffer buffer, int offset) {\n+    Preconditions.checkArgument(\n+        buffer.order() == ByteOrder.LITTLE_ENDIAN, \""Invalid byte order: big endian\"");\n+\n+    if (null == serializationState) {\n+      this.serializationState = new SerializationState(elements);\n+    }\n+\n+    return serializationState.writeTo(buffer, offset);\n+  }\n+\n+  /** Common state for {@link #size()} and {@link #writeTo(ByteBuffer, int)} */\n+  private static class SerializationState {\n+    private final List<VariantValue> elements;\n+    private final int numElements;\n+    private final boolean isLarge;\n+    private final int dataSize;\n+    private final int offsetSize;\n+\n+    private SerializationState(List<VariantValue> elements) {\n+      this.elements = elements;\n+      this.numElements = elements.size();\n+      this.isLarge = numElements > 0xFF;\n+\n+      int totalDataSize = 0;\n+      for (VariantValue value : elements) {\n+        totalDataSize += value.sizeInBytes();\n+      }\n+\n+      this.dataSize = totalDataSize;\n+      this.offsetSize = VariantUtil.sizeOf(totalDataSize);\n+    }\n+\n+    private int size() {\n+      return 1 /* header */\n+          + (isLarge ? 4 : 1) /* num elements size */\n+          + (1 + numElements) * offsetSize /* offset list size */\n+          + dataSize;\n+    }\n+\n+    private int writeTo(ByteBuffer buffer, int offset) {\n+      int offsetListOffset =\n+          offset + 1 /* header size */ + (isLarge ? 4 : 1) /* num elements size */;\n+      int dataOffset = offsetListOffset + ((1 + numElements) * offsetSize);\n+      byte header = VariantUtil.arrayHeader(isLarge, offsetSize);\n+\n+      VariantUtil.writeByte(buffer, header, offset);\n+      VariantUtil.writeLittleEndianUnsigned(buffer, numElements, offset + 1, isLarge ? 4 : 1);\n+\n+      // Insert element offsets\n+      int nextValueOffset = 0;\n+      int index = 0;\n+      for (VariantValue element : elements) {\n+        // write the data offset\n+        VariantUtil.writeLittleEndianUnsigned(\n+            buffer, nextValueOffset, offsetListOffset + (index * offsetSize), offsetSize);\n+\n+        // write the data\n+        int valueSize = element.writeTo(buffer, dataOffset + nextValueOffset);\n+\n+        nextValueOffset += valueSize;\n+        index += 1;\n+      }\n+\n+      // write the final size of the data section\n+      VariantUtil.writeLittleEndianUnsigned(\n+          buffer, nextValueOffset, offsetListOffset + (index * offsetSize), offsetSize);\n+\n+      // return the total size\n+      return (dataOffset - offset) + dataSize;\n+    }\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/variants/Variants.java b/core/src/main/java/org/apache/iceberg/variants/Variants.java\nindex d5f8cb4ae67c..5591145ca603 100644\n--- a/core/src/main/java/org/apache/iceberg/variants/Variants.java\n+++ b/core/src/main/java/org/apache/iceberg/variants/Variants.java\n@@ -121,6 +121,10 @@ public static boolean isNull(ByteBuffer valueBuffer) {\n     return VariantUtil.readByte(valueBuffer, 0) == 0;\n   }\n \n+  public static ValueArray array() {\n+    return new ValueArray();\n+  }\n+\n   public static <T> VariantPrimitive<T> of(PhysicalType type, T value) {\n     return new PrimitiveWrapper<>(type, value);\n   }\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\nindex 3e5635958c0a..40b0aeecc3b5 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.variants.PhysicalType;\n import org.apache.iceberg.variants.ShreddedObject;\n+import org.apache.iceberg.variants.ValueArray;\n import org.apache.iceberg.variants.Variant;\n import org.apache.iceberg.variants.VariantMetadata;\n import org.apache.iceberg.variants.VariantObject;\n@@ -95,6 +96,14 @@ public static VariantValueReader objects(\n         fieldReaders);\n   }\n \n+  public static VariantValueReader array(\n+      int repeatedDefinitionLevel,\n+      int repeatedRepetitionLevel,\n+      ParquetValueReader<?> elementReader) {\n+    return new ArrayReader(\n+        repeatedDefinitionLevel, repeatedRepetitionLevel, (VariantValueReader) elementReader);\n+  }\n+\n   public static VariantValueReader asVariant(PhysicalType type, ParquetValueReader<?> reader) {\n     return new ValueAsVariantReader<>(type, reader);\n   }\n@@ -332,6 +341,58 @@ public void setPageSource(PageReadStore pageStore) {\n     }\n   }\n \n+  private static class ArrayReader implements VariantValueReader {\n+    private final int definitionLevel;\n+    private final int repetitionLevel;\n+    private final VariantValueReader reader;\n+    private final TripleIterator<?> column;\n+    private final List<TripleIterator<?>> children;\n+\n+    protected ArrayReader(int definitionLevel, int repetitionLevel, VariantValueReader reader) {\n+      this.definitionLevel = definitionLevel;\n+      this.repetitionLevel = repetitionLevel;\n+      this.reader = reader;\n+      this.column = reader.column();\n+      this.children = reader.columns();\n+    }\n+\n+    @Override\n+    public void setPageSource(PageReadStore pageStore) {\n+      reader.setPageSource(pageStore);\n+    }\n+\n+    @Override\n+    public ValueArray read(VariantMetadata metadata) {\n+      ValueArray arr = Variants.array();\n+      do {\n+        if (column.currentDefinitionLevel() > definitionLevel) {\n+          VariantValue value = reader.read(metadata);\n+          arr.add(value != null ? value : Variants.ofNull());\n+        } else {\n+          // consume the empty list triple\n+          for (TripleIterator<?> child : children) {\n+            child.nextNull();\n+          }\n+          // if the current definition level is equal to the definition level of this repeated type,\n+          // then the result is an empty list and the repetition level will always be <= rl.\n+          break;\n+        }\n+      } while (column.currentRepetitionLevel() > repetitionLevel);\n+\n+      return arr;\n+    }\n+\n+    @Override\n+    public TripleIterator<?> column() {\n+      return column;\n+    }\n+\n+    @Override\n+    public List<TripleIterator<?>> columns() {\n+      return children;\n+    }\n+  }\n+\n   private static class VariantReader implements ParquetValueReader<Variant> {\n     private final ParquetValueReader<VariantMetadata> metadataReader;\n     private final VariantValueReader valueReader;\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\nindex 71d2eb26627b..d0ca00b19313 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\n@@ -31,6 +31,7 @@ public abstract class ParquetVariantVisitor<R> {\n   static final String METADATA = \""metadata\"";\n   static final String VALUE = \""value\"";\n   static final String TYPED_VALUE = \""typed_value\"";\n+  static final String LIST = \""list\"";\n \n   /**\n    * Handles the root variant column group.\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java b/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\nindex df41c5aa6067..29ca90034623 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\n@@ -66,8 +66,8 @@ private String[] currentPath() {\n     return Streams.concat(Streams.stream(basePath), fieldNames.stream()).toArray(String[]::new);\n   }\n \n-  private String[] path(String name) {\n-    return Streams.concat(Streams.stream(basePath), fieldNames.stream(), Stream.of(name))\n+  private String[] path(String... names) {\n+    return Streams.concat(Streams.stream(basePath), fieldNames.stream(), Stream.of(names))\n         .toArray(String[]::new);\n   }\n \n@@ -162,8 +162,16 @@ public VariantValueReader object(\n \n   @Override\n   public VariantValueReader array(\n-      GroupType array, ParquetValueReader<?> valueResult, ParquetValueReader<?> elementResult) {\n-    throw new UnsupportedOperationException(\""Array is not yet supported\"");\n+      GroupType array, ParquetValueReader<?> valueReader, ParquetValueReader<?> elementReader) {\n+    int valueDL =\n+        valueReader != null ? schema.getMaxDefinitionLevel(path(VALUE)) - 1 : Integer.MAX_VALUE;\n+    int typedDL = schema.getMaxDefinitionLevel(path(TYPED_VALUE)) - 1;\n+    int repeatedDL = schema.getMaxDefinitionLevel(path(TYPED_VALUE, LIST)) - 1;\n+    int repeatedRL = schema.getMaxRepetitionLevel(path(TYPED_VALUE, LIST)) - 1;\n+    VariantValueReader typedReader =\n+        ParquetVariantReaders.array(repeatedDL, repeatedRL, elementReader);\n+\n+    return ParquetVariantReaders.shredded(valueDL, valueReader, typedDL, typedReader);\n   }\n \n   private static class LogicalTypeToVariantReader\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java b/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\nindex 6707ae6651a0..66d5c9911a79 100644\n--- a/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\n+++ b/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\n@@ -217,11 +217,12 @@ public void testPartiallyShreddedObjectSerializationLargeBuffer() {\n         .isEqualTo(DateTimeUtil.isoDateToDays(\""2024-10-12\""));\n   }\n \n-  @Test\n-  public void testTwoByteOffsets() {\n-    // a string larger than 255 bytes to push the value offset size above 1 byte\n-    String randomString = RandomUtil.generateString(300, random);\n-    SerializedPrimitive bigString = VariantTestUtil.createString(randomString);\n+  @ParameterizedTest\n+  @ValueSource(ints = {300, 70_000, 16_777_300})\n+  public void testMultiByteOffsets(int len) {\n+    // Use a string exceeding 255 bytes to test value offset sizes of 2, 3, and 4 bytes\n+    String randomString = RandomUtil.generateString(len, random);\n+    VariantPrimitive<String> bigString = Variants.of(randomString);\n \n     Map<String, VariantValue> data = Maps.newHashMap();\n     data.putAll(FIELDS);\n@@ -244,60 +245,6 @@ public void testTwoByteOffsets() {\n     assertThat(object.get(\""big\"").asPrimitive().get()).isEqualTo(randomString);\n   }\n \n-  @Test\n-  public void testThreeByteOffsets() {\n-    // a string larger than 65535 bytes to push the value offset size above 2 bytes\n-    String randomString = RandomUtil.generateString(70_000, random);\n-    SerializedPrimitive reallyBigString = VariantTestUtil.createString(randomString);\n-\n-    Map<String, VariantValue> data = Maps.newHashMap();\n-    data.putAll(FIELDS);\n-    data.put(\""really-big\"", reallyBigString);\n-\n-    ShreddedObject shredded = createShreddedObject(data);\n-    VariantValue value = roundTripLargeBuffer(shredded, shredded.metadata());\n-\n-    assertThat(value.type()).isEqualTo(PhysicalType.OBJECT);\n-    SerializedObject object = (SerializedObject) value;\n-    assertThat(object.numFields()).isEqualTo(4);\n-\n-    assertThat(object.get(\""a\"").type()).isEqualTo(PhysicalType.INT32);\n-    assertThat(object.get(\""a\"").asPrimitive().get()).isEqualTo(34);\n-    assertThat(object.get(\""b\"").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\""b\"").asPrimitive().get()).isEqualTo(\""iceberg\"");\n-    assertThat(object.get(\""c\"").type()).isEqualTo(PhysicalType.DECIMAL4);\n-    assertThat(object.get(\""c\"").asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n-    assertThat(object.get(\""really-big\"").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\""really-big\"").asPrimitive().get()).isEqualTo(randomString);\n-  }\n-\n-  @Test\n-  public void testFourByteOffsets() {\n-    // a string larger than 16777215 bytes to push the value offset size above 3 bytes\n-    String randomString = RandomUtil.generateString(16_777_300, random);\n-    SerializedPrimitive reallyBigString = VariantTestUtil.createString(randomString);\n-\n-    Map<String, VariantValue> data = Maps.newHashMap();\n-    data.putAll(FIELDS);\n-    data.put(\""really-big\"", reallyBigString);\n-\n-    ShreddedObject shredded = createShreddedObject(data);\n-    VariantValue value = roundTripLargeBuffer(shredded, shredded.metadata());\n-\n-    assertThat(value.type()).isEqualTo(PhysicalType.OBJECT);\n-    SerializedObject object = (SerializedObject) value;\n-    assertThat(object.numFields()).isEqualTo(4);\n-\n-    assertThat(object.get(\""a\"").type()).isEqualTo(PhysicalType.INT32);\n-    assertThat(object.get(\""a\"").asPrimitive().get()).isEqualTo(34);\n-    assertThat(object.get(\""b\"").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\""b\"").asPrimitive().get()).isEqualTo(\""iceberg\"");\n-    assertThat(object.get(\""c\"").type()).isEqualTo(PhysicalType.DECIMAL4);\n-    assertThat(object.get(\""c\"").asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n-    assertThat(object.get(\""really-big\"").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\""really-big\"").asPrimitive().get()).isEqualTo(randomString);\n-  }\n-\n   @ParameterizedTest\n   @ValueSource(booleans = {true, false})\n   @SuppressWarnings({\""unchecked\"", \""rawtypes\""})\n\ndiff --git a/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java b/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java\nnew file mode 100644\nindex 000000000000..f500f6106573\n--- /dev/null\n+++ b/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java\n@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.variants;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.List;\n+import java.util.Random;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.RandomUtil;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+public class TestValueArray {\n+  private static final VariantMetadata EMPTY_METADATA = Variants.emptyMetadata();\n+  private static final List<VariantValue> ELEMENTS =\n+      ImmutableList.of(\n+          Variants.of(34), Variants.of(\""iceberg\""), Variants.of(new BigDecimal(\""12.21\"")));\n+\n+  private final Random random = new Random(871925);\n+\n+  @Test\n+  public void testElementAccess() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    assertThat(arr.numElements()).isEqualTo(3);\n+    assertThat(arr.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(arr.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(1).asPrimitive().get()).isEqualTo(\""iceberg\"");\n+    assertThat(arr.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n+  }\n+\n+  @Test\n+  public void testSerializationMinimalBuffer() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    VariantValue value = roundTripMinimalBuffer(arr);\n+\n+    assertThat(value).isInstanceOf(SerializedArray.class);\n+    SerializedArray actual = (SerializedArray) value;\n+\n+    assertThat(actual.numElements()).isEqualTo(3);\n+    assertThat(actual.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actual.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(1).asPrimitive().get()).isEqualTo(\""iceberg\"");\n+    assertThat(actual.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n+  }\n+\n+  @Test\n+  public void testSerializationLargeBuffer() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    VariantValue value = roundTripLargeBuffer(arr);\n+\n+    assertThat(value).isInstanceOf(SerializedArray.class);\n+    SerializedArray actual = (SerializedArray) value;\n+\n+    assertThat(actual.numElements()).isEqualTo(3);\n+    assertThat(actual.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actual.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(1).asPrimitive().get()).isEqualTo(\""iceberg\"");\n+    assertThat(actual.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {300, 70_000, 16_777_300})\n+  public void testMultiByteOffsets(int len) {\n+    // Use a string exceeding 255 bytes to test value offset sizes of 2, 3, and 4 bytes\n+    String randomString = RandomUtil.generateString(len, random);\n+    VariantPrimitive<String> bigString = Variants.of(randomString);\n+\n+    List<VariantValue> data = Lists.newArrayList();\n+    data.addAll(ELEMENTS);\n+    data.add(bigString);\n+\n+    ValueArray shredded = createArray(data);\n+    VariantValue value = roundTripLargeBuffer(shredded);\n+\n+    assertThat(value.type()).isEqualTo(PhysicalType.ARRAY);\n+    SerializedArray actualArray = (SerializedArray) value;\n+    assertThat(actualArray.numElements()).isEqualTo(4);\n+\n+    assertThat(actualArray.get(0).type()).isEqualTo(PhysicalType.INT32);\n+    assertThat(actualArray.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actualArray.get(1).type()).isEqualTo(PhysicalType.STRING);\n+    assertThat(actualArray.get(1).asPrimitive().get()).isEqualTo(\""iceberg\"");\n+    assertThat(actualArray.get(2).type()).isEqualTo(PhysicalType.DECIMAL4);\n+    assertThat(actualArray.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\""12.21\""));\n+    assertThat(actualArray.get(3).type()).isEqualTo(PhysicalType.STRING);\n+    assertThat(actualArray.get(3).asPrimitive().get()).isEqualTo(randomString);\n+  }\n+\n+  @Test\n+  public void testLargeArray() {\n+    List<VariantValue> elements = Lists.newArrayList();\n+    for (int i = 0; i < 10_000; i += 1) {\n+      elements.add(Variants.of(RandomUtil.generateString(10, random)));\n+    }\n+\n+    ValueArray arr = createArray(elements);\n+    VariantValue value = roundTripLargeBuffer(arr);\n+\n+    assertThat(value.type()).isEqualTo(PhysicalType.ARRAY);\n+    SerializedArray actualArray = (SerializedArray) value;\n+    assertThat(actualArray.numElements()).isEqualTo(10_000);\n+\n+    for (int i = 0; i < 10_000; i++) {\n+      VariantTestUtil.assertEqual(elements.get(i), actualArray.get(i));\n+    }\n+  }\n+\n+  private static VariantValue roundTripMinimalBuffer(ValueArray arr) {\n+    ByteBuffer serialized = ByteBuffer.allocate(arr.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n+    arr.writeTo(serialized, 0);\n+\n+    return Variants.value(EMPTY_METADATA, serialized);\n+  }\n+\n+  private static VariantValue roundTripLargeBuffer(ValueArray arr) {\n+    ByteBuffer serialized =\n+        ByteBuffer.allocate(1000 + arr.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n+    arr.writeTo(serialized, 300);\n+\n+    ByteBuffer slice = serialized.duplicate().order(ByteOrder.LITTLE_ENDIAN);\n+    slice.position(300);\n+    slice.limit(300 + arr.sizeInBytes());\n+\n+    return Variants.value(EMPTY_METADATA, slice);\n+  }\n+\n+  private static ValueArray createArray(List<VariantValue> elements) {\n+    ValueArray arr = new ValueArray();\n+    for (VariantValue element : elements) {\n+      arr.add(element);\n+    }\n+\n+    return arr;\n+  }\n+}\n\ndiff --git a/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java b/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\nindex b0299762f7a2..23c6e9b3282c 100644\n--- a/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\n+++ b/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\n@@ -48,6 +48,7 @@\n import org.apache.iceberg.types.Types.VariantType;\n import org.apache.iceberg.variants.PhysicalType;\n import org.apache.iceberg.variants.ShreddedObject;\n+import org.apache.iceberg.variants.ValueArray;\n import org.apache.iceberg.variants.Variant;\n import org.apache.iceberg.variants.VariantMetadata;\n import org.apache.iceberg.variants.VariantObject;\n@@ -57,6 +58,8 @@\n import org.apache.iceberg.variants.Variants;\n import org.apache.parquet.avro.AvroSchemaConverter;\n import org.apache.parquet.avro.AvroWriteSupport;\n+import org.apache.parquet.conf.ParquetConfiguration;\n+import org.apache.parquet.conf.PlainParquetConfiguration;\n import org.apache.parquet.hadoop.ParquetWriter;\n import org.apache.parquet.hadoop.api.WriteSupport;\n import org.apache.parquet.schema.GroupType;\n@@ -135,6 +138,15 @@ public class TestVariantReaders {\n         Variants.ofUUID(\""f24f9b64-81fa-49d1-b74e-8c09a6e31c56\""),\n       };\n \n+  // Required configuration to convert between Avro and Parquet schemas with 3-level list structure\n+  private static final ParquetConfiguration CONF =\n+      new PlainParquetConfiguration(\n+          Map.of(\n+              AvroWriteSupport.WRITE_OLD_LIST_STRUCTURE,\n+              \""false\"",\n+              AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS,\n+              \""false\""));\n+\n   private static Stream<Arguments> metadataAndValues() {\n     Stream<Arguments> primitives =\n         Stream.of(PRIMITIVES).map(variant -> Arguments.of(EMPTY_METADATA, variant));\n@@ -255,7 +267,7 @@ public void testMissingValueColumn() throws IOException {\n   }\n \n   @Test\n-  public void testValueAndTypedValueConflict() throws IOException {\n+  public void testValueAndTypedValueConflict() {\n     GroupType variantType = variant(\""var\"", 2, shreddedPrimitive(PrimitiveTypeName.INT32));\n     MessageType parquetSchema = parquetSchema(variantType);\n \n@@ -885,6 +897,460 @@ public void testMixedRecords() throws IOException {\n     VariantTestUtil.assertEqual(expectedThree, actualThreeVariant.value());\n   }\n \n+  @Test\n+  public void testSimpleArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\""comedy\""));\n+    expectedArray.add(Variants.of(\""drama\""));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testNullArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType variantType = variant(\""var\"", 2, list(element(shreddedType)));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord var =\n+        record(\n+            variantType,\n+            Map.of(\n+                \""metadata\"",\n+                VariantTestUtil.emptyMetadata(),\n+                \""value\"",\n+                serialize(Variants.ofNull())));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(Variants.ofNull(), actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testEmptyArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType variantType = variant(\""var\"", 2, list(element(shreddedType)));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr = List.of();\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    assertThat(actualVariant.value().type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualVariant.value().asArray().numElements()).isEqualTo(0);\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+  }\n+\n+  @Test\n+  public void testArrayWithNull() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""value\"", serialize(Variants.ofNull()))),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\""comedy\""));\n+    expectedArray.add(Variants.ofNull());\n+    expectedArray.add(Variants.of(\""drama\""));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    assertThat(actualVariant.value().type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualVariant.value().asArray().numElements()).isEqualTo(3);\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testNestedArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType outerElementType = element(list(elementType));\n+    GroupType variantType = variant(\""var\"", 2, list(outerElementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> inner1 =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+    List<GenericRecord> outer1 =\n+        List.of(\n+            record(outerElementType, Map.of(\""typed_value\"", inner1)),\n+            record(outerElementType, Map.of(\""typed_value\"", List.of())));\n+    GenericRecord var =\n+        record(\n+            variantType,\n+            Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", outer1));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    ValueArray expectedInner1 = Variants.array();\n+    expectedInner1.add(Variants.of(\""comedy\""));\n+    expectedInner1.add(Variants.of(\""drama\""));\n+    ValueArray expectedInner2 = Variants.array();\n+    expectedArray.add(expectedInner1);\n+    expectedArray.add(expectedInner2);\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    // Verify\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithNestedObject() throws IOException {\n+    GroupType fieldA = field(\""a\"", shreddedPrimitive(PrimitiveTypeName.INT32));\n+    GroupType fieldB = field(\""b\"", shreddedPrimitive(PrimitiveTypeName.BINARY, STRING));\n+    GroupType shreddedFields = objectFields(fieldA, fieldB);\n+    GroupType elementType = element(shreddedFields);\n+    GroupType listType = list(elementType);\n+    GroupType variantType = variant(\""var\"", 2, listType);\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    // Row 1 with nested fully shredded object\n+    GenericRecord shredded1 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \""a\"",\n+                record(fieldA, Map.of(\""typed_value\"", 1)),\n+                \""b\"",\n+                record(fieldB, Map.of(\""typed_value\"", \""comedy\""))));\n+    GenericRecord shredded2 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \""a\"",\n+                record(fieldA, Map.of(\""typed_value\"", 2)),\n+                \""b\"",\n+                record(fieldB, Map.of(\""typed_value\"", \""drama\""))));\n+    List<GenericRecord> arr1 =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", shredded1)),\n+            record(elementType, Map.of(\""typed_value\"", shredded2)));\n+    GenericRecord var1 =\n+        record(variantType, Map.of(\""metadata\"", TEST_METADATA_BUFFER, \""typed_value\"", arr1));\n+    GenericRecord row1 = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var1));\n+\n+    ValueArray expected1 = Variants.array();\n+    ShreddedObject expectedElement1 = Variants.object(TEST_METADATA);\n+    expectedElement1.put(\""a\"", Variants.of(1));\n+    expectedElement1.put(\""b\"", Variants.of(\""comedy\""));\n+    expected1.add(expectedElement1);\n+    ShreddedObject expectedElement2 = Variants.object(TEST_METADATA);\n+    expectedElement2.put(\""a\"", Variants.of(2));\n+    expectedElement2.put(\""b\"", Variants.of(\""drama\""));\n+    expected1.add(expectedElement2);\n+\n+    // Row 2 with nested partially shredded object\n+    GenericRecord shredded3 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \""a\"",\n+                record(fieldA, Map.of(\""typed_value\"", 3)),\n+                \""b\"",\n+                record(fieldB, Map.of(\""typed_value\"", \""action\""))));\n+    ShreddedObject baseObject3 = Variants.object(TEST_METADATA);\n+    baseObject3.put(\""c\"", Variants.of(\""str\""));\n+\n+    GenericRecord shredded4 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \""a\"",\n+                record(fieldA, Map.of(\""typed_value\"", 4)),\n+                \""b\"",\n+                record(fieldB, Map.of(\""typed_value\"", \""horror\""))));\n+    ShreddedObject baseObject4 = Variants.object(TEST_METADATA);\n+    baseObject4.put(\""d\"", Variants.ofIsoDate(\""2024-01-30\""));\n+\n+    List<GenericRecord> arr2 =\n+        List.of(\n+            record(elementType, Map.of(\""value\"", serialize(baseObject3), \""typed_value\"", shredded3)),\n+            record(elementType, Map.of(\""value\"", serialize(baseObject4), \""typed_value\"", shredded4)));\n+    GenericRecord var2 =\n+        record(variantType, Map.of(\""metadata\"", TEST_METADATA_BUFFER, \""typed_value\"", arr2));\n+    GenericRecord row2 = record(parquetSchema, Map.of(\""id\"", 2, \""var\"", var2));\n+\n+    ValueArray expected2 = Variants.array();\n+    ShreddedObject expectedElement3 = Variants.object(TEST_METADATA);\n+    expectedElement3.put(\""a\"", Variants.of(3));\n+    expectedElement3.put(\""b\"", Variants.of(\""action\""));\n+    expectedElement3.put(\""c\"", Variants.of(\""str\""));\n+    expected2.add(expectedElement3);\n+    ShreddedObject expectedElement4 = Variants.object(TEST_METADATA);\n+    expectedElement4.put(\""a\"", Variants.of(4));\n+    expectedElement4.put(\""b\"", Variants.of(\""horror\""));\n+    expectedElement4.put(\""d\"", Variants.ofIsoDate(\""2024-01-30\""));\n+    expected2.add(expectedElement4);\n+\n+    // verify\n+    List<Record> actual = writeAndRead(parquetSchema, List.of(row1, row2));\n+    Record actual1 = actual.get(0);\n+    assertThat(actual1.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual1.getField(\""var\"")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant1 = (Variant) actual1.getField(\""var\"");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant1.metadata());\n+    VariantTestUtil.assertEqual(expected1, actualVariant1.value());\n+\n+    Record actual2 = actual.get(1);\n+    assertThat(actual2.getField(\""id\"")).isEqualTo(2);\n+    assertThat(actual2.getField(\""var\"")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant2 = (Variant) actual2.getField(\""var\"");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant2.metadata());\n+    VariantTestUtil.assertEqual(expected2, actualVariant2.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithNonArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr1 =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+    GenericRecord var1 =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr1));\n+    GenericRecord row1 = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var1));\n+\n+    ValueArray expectedArray1 = Variants.array();\n+    expectedArray1.add(Variants.of(\""comedy\""));\n+    expectedArray1.add(Variants.of(\""drama\""));\n+\n+    GenericRecord var2 =\n+        record(\n+            variantType,\n+            Map.of(\n+                \""metadata\"", VariantTestUtil.emptyMetadata(), \""value\"", serialize(Variants.of(34))));\n+    GenericRecord row2 = record(parquetSchema, Map.of(\""id\"", 2, \""var\"", var2));\n+\n+    VariantValue expectedValue2 = Variants.of(PhysicalType.INT32, 34);\n+\n+    GenericRecord var3 =\n+        record(variantType, Map.of(\""metadata\"", TEST_METADATA_BUFFER, \""value\"", TEST_OBJECT_BUFFER));\n+    GenericRecord row3 = record(parquetSchema, Map.of(\""id\"", 3, \""var\"", var3));\n+\n+    ShreddedObject expectedObject3 = Variants.object(TEST_METADATA);\n+    expectedObject3.put(\""a\"", Variants.ofNull());\n+    expectedObject3.put(\""d\"", Variants.of(\""iceberg\""));\n+\n+    // Test array is read properly after a non-array\n+    List<GenericRecord> arr4 =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""action\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""horror\"")));\n+    GenericRecord var4 =\n+        record(variantType, Map.of(\""metadata\"", TEST_METADATA_BUFFER, \""typed_value\"", arr4));\n+    GenericRecord row4 = record(parquetSchema, Map.of(\""id\"", 4, \""var\"", var4));\n+\n+    ValueArray expectedArray4 = Variants.array();\n+    expectedArray4.add(Variants.of(\""action\""));\n+    expectedArray4.add(Variants.of(\""horror\""));\n+\n+    List<Record> actual = writeAndRead(parquetSchema, List.of(row1, row2, row3, row4));\n+\n+    // Verify\n+    Record actual1 = actual.get(0);\n+    assertThat(actual1.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual1.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant1 = (Variant) actual1.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant1.metadata());\n+    VariantTestUtil.assertEqual(expectedArray1, actualVariant1.value());\n+\n+    Record actual2 = actual.get(1);\n+    assertThat(actual2.getField(\""id\"")).isEqualTo(2);\n+    assertThat(actual2.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant2 = (Variant) actual2.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant2.metadata());\n+    VariantTestUtil.assertEqual(expectedValue2, actualVariant2.value());\n+\n+    Record actual3 = actual.get(2);\n+    assertThat(actual3.getField(\""id\"")).isEqualTo(3);\n+    assertThat(actual3.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant3 = (Variant) actual3.getField(\""var\"");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant3.metadata());\n+    VariantTestUtil.assertEqual(expectedObject3, actualVariant3.value());\n+\n+    Record actual4 = actual.get(3);\n+    assertThat(actual4.getField(\""id\"")).isEqualTo(4);\n+    assertThat(actual4.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant4 = (Variant) actual4.getField(\""var\"");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant4.metadata());\n+    VariantTestUtil.assertEqual(expectedArray4, actualVariant4.value());\n+  }\n+\n+  @Test\n+  public void testArrayMissingValueColumn() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType =\n+        Types.buildGroup(Type.Repetition.OPTIONAL)\n+            .id(2)\n+            .required(PrimitiveTypeName.BINARY)\n+            .named(\""metadata\"")\n+            .addField(list(elementType))\n+            .named(\""var\"");\n+\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\""comedy\""));\n+    expectedArray.add(Variants.of(\""drama\""));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayMissingElementValueColumn() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType =\n+        Types.buildGroup(Type.Repetition.REQUIRED).addField(shreddedType).named(\""element\"");\n+\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\""typed_value\"", \""comedy\"")),\n+            record(elementType, Map.of(\""typed_value\"", \""drama\"")));\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\""comedy\""));\n+    expectedArray.add(Variants.of(\""drama\""));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithElementNullValueAndNullTypedValue() throws IOException {\n+    // Test the invalid case that both value and typed_value of an element are null\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord element = record(elementType, Map.of());\n+    GenericRecord variant =\n+        record(\n+            variantType,\n+            Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", List.of(element)));\n+    GenericRecord record = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", variant));\n+\n+    Record actual = writeAndRead(parquetSchema, record);\n+    assertThat(actual.getField(\""id\"")).isEqualTo(1);\n+    assertThat(actual.getField(\""var\"")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant = (Variant) actual.getField(\""var\"");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantValue actualValue = actualVariant.value();\n+    assertThat(actualValue.type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualValue.asArray().numElements()).isEqualTo(1);\n+    VariantTestUtil.assertEqual(Variants.ofNull(), actualValue.asArray().get(0));\n+  }\n+\n+  @Test\n+  public void testArrayWithElementValueTypedValueConflict() {\n+    // Test the invalid case that both value and typed_value of an element are not null\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\""var\"", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord element =\n+        record(elementType, Map.of(\""value\"", serialize(Variants.of(3)), \""typed_value\"", \""comedy\""));\n+    GenericRecord variant =\n+        record(\n+            variantType,\n+            Map.of(\""metadata\"", VariantTestUtil.emptyMetadata(), \""typed_value\"", List.of(element)));\n+    GenericRecord record = record(parquetSchema, Map.of(\""id\"", 1, \""var\"", variant));\n+\n+    assertThatThrownBy(() -> writeAndRead(parquetSchema, record))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\""Invalid variant, conflicting value and typed_value\"");\n+  }\n+\n   private static ByteBuffer serialize(VariantValue value) {\n     ByteBuffer buffer = ByteBuffer.allocate(value.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n     value.writeTo(buffer, 0);\n@@ -943,7 +1409,7 @@ static List<Record> writeAndRead(MessageType parquetSchema, List<GenericRecord>\n     OutputFile outputFile = new InMemoryOutputFile();\n \n     try (ParquetWriter<GenericRecord> writer =\n-        new TestWriterBuilder(outputFile).withFileType(parquetSchema).build()) {\n+        new TestWriterBuilder(outputFile).withFileType(parquetSchema).withConf(CONF).build()) {\n       for (GenericRecord record : records) {\n         writer.write(record);\n       }\n@@ -1104,14 +1570,38 @@ private static GroupType field(String name, Type shreddedType) {\n         .named(name);\n   }\n \n+  private static GroupType element(Type shreddedType) {\n+    return field(\""element\"", shreddedType);\n+  }\n+\n+  private static GroupType list(GroupType elementType) {\n+    return Types.optionalList().element(elementType).named(\""typed_value\"");\n+  }\n+\n+  private static void checkListType(GroupType listType) {\n+    // Check the list is a 3-level structure\n+    Preconditions.checkArgument(\n+        listType.getFieldCount() == 1\n+            && listType.getFields().get(0).isRepetition(Type.Repetition.REPEATED),\n+        \""Invalid list type: does not contain single repeated field: %s\"",\n+        listType);\n+\n+    GroupType repeated = listType.getFields().get(0).asGroupType();\n+    Preconditions.checkArgument(\n+        repeated.getFieldCount() == 1\n+            && repeated.getFields().get(0).isRepetition(Type.Repetition.REQUIRED),\n+        \""Invalid list type: does not contain single required subfield: %s\"",\n+        listType);\n+  }\n+\n   private static org.apache.avro.Schema avroSchema(GroupType schema) {\n     if (schema instanceof MessageType) {\n-      return new AvroSchemaConverter().convert((MessageType) schema);\n+      return new AvroSchemaConverter(CONF).convert((MessageType) schema);\n \n     } else {\n       MessageType wrapped = Types.buildMessage().addField(schema).named(\""table\"");\n       org.apache.avro.Schema avro =\n-          new AvroSchemaConverter().convert(wrapped).getFields().get(0).schema();\n+          new AvroSchemaConverter(CONF).convert(wrapped).getFields().get(0).schema();\n       switch (avro.getType()) {\n         case RECORD:\n           return avro;\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-12238"", ""pr_id"": 12238, ""issue_id"": 10392, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Variant Data Type Support\n### Proposed Change\r\n\r\nWe would like to propose to add Variant type to Iceberg data types. \r\n\r\nVariant data types allow for the efficient binary encoding of dynamic semi-structured data such as JSON, Avro,Parquet, etc. By encoding semi-structured data as a variant column, we retain the flexibility of the source data, while allowing query engines to more efficiently operate on the data.\r\n\r\nWith the support of Variant type, such data can be encoded in an efficient binary representation internally for better performance. Without that, we need to parse the data in its format inefficiently.\r\n\r\nThis will allow the following use cases:\r\n\r\n- Create an Iceberg table with a Variant column\r\n`CREATE OR REPLACE TABLE car_sales(record Variant);`\r\n- Insert semi-structured data into the Variant column\r\n`INSERT INTO car_sales SELECT PARSE_JSON(<json_string>)`\r\n- Query against the semi-structured data\r\n`SELECT VARIANT_GET(record, '$.dealer.ship', 'string') FROM car_sales`\r\n\r\n\r\n\r\n\r\n### Proposal document\r\nhttps://docs.google.com/document/d/1sq70XDiWJ2DemWyA5dVB80gKzwi0CWoM0LOWM7VJVd8/edit?tab=t.0\r\n\r\n### Specifications\r\n\r\n- [X] Table\r\n- [ ] View\r\n- [ ] REST\r\n- [ ] Puffin\r\n- [ ] Encryption\r\n- [ ] Other"", ""issue_word_count"": 170, ""test_files_count"": 6, ""non_test_files_count"": 14, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java"", ""core/src/main/java/org/apache/iceberg/avro/Avro.java"", ""core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java"", ""core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java"", ""core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java"", ""core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java"", ""core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java"", ""core/src/main/java/org/apache/iceberg/avro/HasIds.java"", ""core/src/main/java/org/apache/iceberg/avro/MissingIds.java"", ""core/src/main/java/org/apache/iceberg/avro/PruneColumns.java"", ""core/src/main/java/org/apache/iceberg/avro/RemoveIds.java"", ""core/src/main/java/org/apache/iceberg/avro/SchemaToType.java"", ""core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java"", ""core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java"", ""core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java"", ""core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java"", ""core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java"", ""core/src/test/java/org/apache/iceberg/avro/TestHasIds.java"", ""core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java"", ""core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java"", ""core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java"", ""core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java"", ""core/src/test/java/org/apache/iceberg/avro/TestHasIds.java"", ""core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java"", ""core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java""], ""base_commit"": ""a50ec923f3d928f67e2a4a361c0d1162341aa084"", ""head_commit"": ""dcc7f70b94d1b156f0bdf3d920b4b6b0f4b39ae1"", ""repo_url"": ""https://github.com/apache/iceberg/pull/12238"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/12238"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-04T21:03:41.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java b/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\nindex ce619c47fabe..de45a2bbaee1 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\n@@ -128,6 +128,11 @@ public Schema map(Schema map, Schema value) {\n     return map;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     return primitive;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/Avro.java b/core/src/main/java/org/apache/iceberg/avro/Avro.java\nindex 557a20daf303..2a3ea11590bb 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/Avro.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/Avro.java\n@@ -86,6 +86,7 @@ private enum Codec {\n \n   static {\n     LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+    LogicalTypes.register(VariantLogicalType.NAME, schema -> VariantLogicalType.get());\n     DEFAULT_MODEL.addLogicalTypeConversion(new Conversions.DecimalConversion());\n     DEFAULT_MODEL.addLogicalTypeConversion(new UUIDConversion());\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java b/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\nindex 69159b65be3e..f579381ac0c8 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\n@@ -27,6 +27,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n abstract class AvroCustomOrderSchemaVisitor<T, F> {\n+  private static final String METADATA = \""metadata\"";\n+  private static final String VALUE = \""value\"";\n+\n   public static <T, F> T visit(Schema schema, AvroCustomOrderSchemaVisitor<T, F> visitor) {\n     switch (schema.getType()) {\n       case RECORD:\n@@ -35,20 +38,29 @@ public static <T, F> T visit(Schema schema, AvroCustomOrderSchemaVisitor<T, F> v\n         Preconditions.checkState(\n             !visitor.recordLevels.contains(name), \""Cannot process recursive Avro record %s\"", name);\n \n-        visitor.recordLevels.push(name);\n-\n-        List<Schema.Field> fields = schema.getFields();\n-        List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n-        List<Supplier<F>> results = Lists.newArrayListWithExpectedSize(fields.size());\n-        for (Schema.Field field : schema.getFields()) {\n-          names.add(field.name());\n-          results.add(new VisitFieldFuture<>(field, visitor));\n+        if (schema.getLogicalType() instanceof VariantLogicalType) {\n+          Preconditions.checkArgument(\n+              AvroSchemaUtil.isVariantSchema(schema), \""Invalid variant record: %s\"", schema);\n+\n+          return visitor.variant(\n+              schema,\n+              new VisitFuture<>(schema.getField(METADATA).schema(), visitor),\n+              new VisitFuture<>(schema.getField(VALUE).schema(), visitor));\n+        } else {\n+          visitor.recordLevels.push(name);\n+\n+          List<Schema.Field> fields = schema.getFields();\n+          List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n+          List<Supplier<F>> results = Lists.newArrayListWithExpectedSize(fields.size());\n+          for (Schema.Field field : schema.getFields()) {\n+            names.add(field.name());\n+            results.add(new VisitFieldFuture<>(field, visitor));\n+          }\n+\n+          visitor.recordLevels.pop();\n+          return visitor.record(schema, names, Iterables.transform(results, Supplier::get));\n         }\n \n-        visitor.recordLevels.pop();\n-\n-        return visitor.record(schema, names, Iterables.transform(results, Supplier::get));\n-\n       case UNION:\n         List<Schema> types = schema.getTypes();\n         List<Supplier<T>> options = Lists.newArrayListWithExpectedSize(types.size());\n@@ -90,6 +102,10 @@ public T map(Schema map, Supplier<T> value) {\n     return null;\n   }\n \n+  public T variant(Schema variant, Supplier<T> metadataResult, Supplier<T> valueResult) {\n+    throw new UnsupportedOperationException(\""Unsupported type: variant\"");\n+  }\n+\n   public T primitive(Schema primitive) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java b/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\nindex ba3c6fece7f9..0db8d7dd5f9f 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\n@@ -39,6 +39,7 @@ private AvroEncoderUtil() {}\n \n   static {\n     LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+    LogicalTypes.register(VariantLogicalType.NAME, schema -> VariantLogicalType.get());\n   }\n \n   private static final byte[] MAGIC_BYTES = new byte[] {(byte) 0xC2, (byte) 0x01};\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\nindex 032d63105dfe..cdc50959ac8a 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\n@@ -219,6 +219,20 @@ public static boolean isKeyValueSchema(Schema schema) {\n     return schema.getType() == RECORD && schema.getFields().size() == 2;\n   }\n \n+  static boolean isVariantSchema(Schema schema) {\n+    if (schema.getType() != Schema.Type.RECORD || schema.getFields().size() != 2) {\n+      return false;\n+    }\n+\n+    Schema.Field metadataField = schema.getField(\""metadata\"");\n+    Schema.Field valueField = schema.getField(\""value\"");\n+\n+    return metadataField != null\n+        && metadataField.schema().getType() == Schema.Type.BYTES\n+        && valueField != null\n+        && valueField.schema().getType() == Schema.Type.BYTES;\n+  }\n+\n   static Schema createMap(int keyId, Schema keySchema, int valueId, Schema valueSchema) {\n     String keyValueName = \""k\"" + keyId + \""_v\"" + valueId;\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\nindex 9819924ffa99..df3ca72fa09f 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\n@@ -25,6 +25,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n public abstract class AvroSchemaVisitor<T> {\n+  private static final String METADATA = \""metadata\"";\n+  private static final String VALUE = \""value\"";\n+\n   public static <T> T visit(Schema schema, AvroSchemaVisitor<T> visitor) {\n     switch (schema.getType()) {\n       case RECORD:\n@@ -33,21 +36,30 @@ public static <T> T visit(Schema schema, AvroSchemaVisitor<T> visitor) {\n         Preconditions.checkState(\n             !visitor.recordLevels.contains(name), \""Cannot process recursive Avro record %s\"", name);\n \n-        visitor.recordLevels.push(name);\n+        if (schema.getLogicalType() instanceof VariantLogicalType) {\n+          Preconditions.checkArgument(\n+              AvroSchemaUtil.isVariantSchema(schema), \""Invalid variant record: %s\"", schema);\n \n-        List<Schema.Field> fields = schema.getFields();\n-        List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n-        List<T> results = Lists.newArrayListWithExpectedSize(fields.size());\n-        for (Schema.Field field : schema.getFields()) {\n-          names.add(field.name());\n-          T result = visitWithName(field.name(), field.schema(), visitor);\n-          results.add(result);\n+          return visitor.variant(\n+              schema,\n+              visit(schema.getField(METADATA).schema(), visitor),\n+              visit(schema.getField(VALUE).schema(), visitor));\n+        } else {\n+          visitor.recordLevels.push(name);\n+\n+          List<Schema.Field> fields = schema.getFields();\n+          List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n+          List<T> results = Lists.newArrayListWithExpectedSize(fields.size());\n+          for (Schema.Field field : schema.getFields()) {\n+            names.add(field.name());\n+            T result = visitWithName(field.name(), field.schema(), visitor);\n+            results.add(result);\n+          }\n+\n+          visitor.recordLevels.pop();\n+          return visitor.record(schema, names, results);\n         }\n \n-        visitor.recordLevels.pop();\n-\n-        return visitor.record(schema, names, results);\n-\n       case UNION:\n         List<Schema> types = schema.getTypes();\n         List<T> options = Lists.newArrayListWithExpectedSize(types.size());\n@@ -103,6 +115,10 @@ public T map(Schema map, T value) {\n     return null;\n   }\n \n+  public T variant(Schema variant, T metadataResult, T valueResult) {\n+    throw new UnsupportedOperationException(\""Unsupported type: variant\"");\n+  }\n+\n   public T primitive(Schema primitive) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java b/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\nindex c5c78dd1472a..f4dd2f41302d 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\n@@ -265,6 +265,11 @@ public Schema map(Schema map, Supplier<Schema> value) {\n     }\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Supplier<Schema> metadata, Supplier<Schema> value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     // check for type promotion\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/HasIds.java b/core/src/main/java/org/apache/iceberg/avro/HasIds.java\nindex 52ecfd01eaac..90e2094dfca8 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/HasIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/HasIds.java\n@@ -56,6 +56,11 @@ public Boolean union(Schema union, Iterable<Boolean> options) {\n     return Iterables.any(options, Boolean.TRUE::equals);\n   }\n \n+  @Override\n+  public Boolean variant(Schema variant, Supplier<Boolean> metadata, Supplier<Boolean> value) {\n+    return false;\n+  }\n+\n   @Override\n   public Boolean primitive(Schema primitive) {\n     return false;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/MissingIds.java b/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\nindex e47d012a36ee..4bb85f3d382c 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\n@@ -62,6 +62,11 @@ public Boolean union(Schema union, Iterable<Boolean> options) {\n     return Iterables.any(options, Boolean.TRUE::equals);\n   }\n \n+  @Override\n+  public Boolean variant(Schema variant, Supplier<Boolean> metadata, Supplier<Boolean> value) {\n+    return false;\n+  }\n+\n   @Override\n   public Boolean primitive(Schema primitive) {\n     // primitive node cannot be missing ID as Iceberg do not assign primitive node IDs in the first\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java b/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\nindex 2de2c0fe029d..2a31cd5304e7 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\n@@ -80,7 +80,7 @@ public Schema record(Schema record, List<String> names, List<Schema> fields) {\n       }\n \n       Schema fieldSchema = fields.get(field.pos());\n-      // All primitives are selected by selecting the field, but map and list\n+      // All primitives and variant are selected by selecting the field, but map and list\n       // types can be selected by projecting the keys, values, or elements. Empty\n       // Structs can be selected by selecting the record itself instead of its children.\n       // This creates two conditions where the field should be selected: if the\n@@ -259,6 +259,11 @@ private Schema mapWithIds(Schema map, Integer keyId, Integer valueId) {\n     return map;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return null;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     // primitives are not selected directly\n@@ -277,12 +282,11 @@ private static Schema copyRecord(Schema record, List<Schema.Field> newFields) {\n     return copy;\n   }\n \n+  /* Check the schema is a record but not a Variant type */\n   private boolean isRecord(Schema field) {\n-    if (AvroSchemaUtil.isOptionSchema(field)) {\n-      return AvroSchemaUtil.fromOption(field).getType().equals(Schema.Type.RECORD);\n-    } else {\n-      return field.getType().equals(Schema.Type.RECORD);\n-    }\n+    Schema schema = AvroSchemaUtil.isOptionSchema(field) ? AvroSchemaUtil.fromOption(field) : field;\n+    return schema.getType().equals(Schema.Type.RECORD)\n+        && !(schema.getLogicalType() instanceof VariantLogicalType);\n   }\n \n   private static Schema makeEmptyCopy(Schema field) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java b/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\nindex dccc8bf57e9d..cc01b107ba9c 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\n@@ -60,6 +60,11 @@ public Schema array(Schema array, Schema element) {\n     return result;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     return Schema.create(primitive.getType());\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java b/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\nindex 352fe22650ac..444acc670e51 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\n@@ -172,6 +172,11 @@ public Type map(Schema map, Type valueType) {\n     }\n   }\n \n+  @Override\n+  public Type variant(Schema variant, Type metadataType, Type valueType) {\n+    return Types.VariantType.get();\n+  }\n+\n   @Override\n   public Type primitive(Schema primitive) {\n     // first check supported logical types\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java b/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\nindex 05ce4e618662..4fcbcef16fd4 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\n@@ -187,6 +187,21 @@ public Schema map(Types.MapType map, Schema keySchema, Schema valueSchema) {\n     return mapSchema;\n   }\n \n+  @Override\n+  public Schema variant(Types.VariantType variant) {\n+    String recordName = fieldIds.peek() != null ? \""r\"" + fieldIds.peek() : \""variant\"";\n+    Schema schema =\n+        Schema.createRecord(\n+            recordName,\n+            null,\n+            null,\n+            false,\n+            List.of(\n+                new Schema.Field(\""metadata\"", BINARY_SCHEMA),\n+                new Schema.Field(\""value\"", BINARY_SCHEMA)));\n+    return VariantLogicalType.get().addToSchema(schema);\n+  }\n+\n   @Override\n   public Schema primitive(Type.PrimitiveType primitive) {\n     Schema primitiveSchema;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java b/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java\nnew file mode 100644\nindex 000000000000..e8d5db4fdca1\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java\n@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.avro;\n+\n+import org.apache.avro.LogicalType;\n+import org.apache.avro.Schema;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class VariantLogicalType extends LogicalType {\n+  static final String NAME = \""variant\"";\n+  private static final VariantLogicalType INSTANCE = new VariantLogicalType();\n+\n+  static VariantLogicalType get() {\n+    return INSTANCE;\n+  }\n+\n+  private VariantLogicalType() {\n+    super(NAME);\n+  }\n+\n+  @Override\n+  public void validate(Schema schema) {\n+    super.validate(schema);\n+    Preconditions.checkArgument(\n+        AvroSchemaUtil.isVariantSchema(schema), \""Invalid variant record: %s\"", schema);\n+  }\n+}\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java b/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\nindex 03108376eb4b..3b96f844b537 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\n@@ -46,6 +46,15 @@ static Schema record(String name, Schema.Field... fields) {\n     return Schema.createRecord(name, null, null, false, Arrays.asList(fields));\n   }\n \n+  static Schema variant(String name) {\n+    Schema schema =\n+        record(\n+            name,\n+            new Schema.Field(\""metadata\"", Schema.create(Schema.Type.BYTES), null, null),\n+            new Schema.Field(\""value\"", Schema.create(Schema.Type.BYTES), null, null));\n+    return VariantLogicalType.get().addToSchema(schema);\n+  }\n+\n   static Schema.Field addId(int id, Schema.Field field) {\n     field.addProp(AvroSchemaUtil.FIELD_ID_PROP, id);\n     return field;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java b/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\nindex cabc9f250c13..7f36e0de8fd3 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\n@@ -342,6 +342,23 @@ public void testInferredMapping() throws IOException {\n     assertThat(projected).isEqualTo(record);\n   }\n \n+  @Test\n+  public void testVariantNameMapping() {\n+    Schema icebergSchema =\n+        new Schema(\n+            Types.NestedField.required(0, \""id\"", Types.LongType.get()),\n+            Types.NestedField.required(1, \""var\"", Types.VariantType.get()));\n+\n+    org.apache.avro.Schema avroSchema = RemoveIds.removeIds(icebergSchema);\n+    assertThat(AvroSchemaUtil.hasIds(avroSchema)).as(\""Expect schema has no ids\"").isFalse();\n+\n+    NameMapping nameMapping =\n+        NameMapping.of(\n+            MappedField.of(0, ImmutableList.of(\""id\"")), MappedField.of(1, ImmutableList.of(\""var\"")));\n+    org.apache.avro.Schema mappedSchema = AvroSchemaUtil.applyNameMapping(avroSchema, nameMapping);\n+    assertThat(mappedSchema).isEqualTo(AvroSchemaUtil.convert(icebergSchema.asStruct(), \""table\""));\n+  }\n+\n   @Test\n   @Override\n   public void testAvroArrayAsLogicalMap() {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java b/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\nindex b71280daf8db..6632c04f3ff0 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\n@@ -23,6 +23,7 @@\n import java.util.Collections;\n import org.apache.avro.SchemaBuilder;\n import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n \n public class TestAvroSchemaProjection {\n@@ -150,4 +151,22 @@ public void projectWithMapSchemaChanged() {\n         .as(\""Result of buildAvroProjection is missing some IDs\"")\n         .isFalse();\n   }\n+\n+  @Test\n+  public void projectWithVariantType() {\n+    Schema icebergSchema =\n+        new Schema(\n+            Types.NestedField.required(0, \""id\"", Types.LongType.get()),\n+            Types.NestedField.required(1, \""data\"", Types.VariantType.get()));\n+\n+    org.apache.avro.Schema projectedSchema =\n+        AvroSchemaUtil.buildAvroProjection(\n+            AvroSchemaUtil.convert(icebergSchema.asStruct()),\n+            icebergSchema.select(\""data\""),\n+            Collections.emptyMap());\n+    assertThat(projectedSchema.getField(\""id\"")).isNull();\n+    org.apache.avro.Schema variantSchema = projectedSchema.getField(\""data\"").schema();\n+    assertThat(variantSchema.getLogicalType()).isEqualTo(VariantLogicalType.get());\n+    assertThat(AvroSchemaUtil.isVariantSchema(variantSchema)).isTrue();\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java b/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\nindex 4b69d9c879e9..fab520f109e1 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\n@@ -26,7 +26,7 @@\n \n public class TestHasIds {\n   @Test\n-  public void test() {\n+  public void testRemoveIdsHasIds() {\n     Schema schema =\n         new Schema(\n             Types.NestedField.required(0, \""id\"", Types.LongType.get()),\n@@ -39,7 +39,10 @@ public void test() {\n                     Types.StringType.get(),\n                     Types.StructType.of(\n                         Types.NestedField.required(1, \""lat\"", Types.FloatType.get()),\n-                        Types.NestedField.optional(2, \""long\"", Types.FloatType.get())))));\n+                        Types.NestedField.optional(2, \""long\"", Types.FloatType.get())))),\n+            Types.NestedField.required(\n+                8, \""types\"", Types.ListType.ofRequired(9, Types.StringType.get())),\n+            Types.NestedField.required(10, \""data\"", Types.VariantType.get()));\n \n     org.apache.avro.Schema avroSchema = RemoveIds.removeIds(schema);\n     assertThat(AvroSchemaUtil.hasIds(avroSchema)).as(\""Avro schema should not have IDs\"").isFalse();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java b/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java\nnew file mode 100644\nindex 000000000000..ecf92a00b35e\n--- /dev/null\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java\n@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.avro;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+public class TestPruneColumns {\n+  private static final org.apache.avro.Schema TEST_SCHEMA =\n+      AvroSchemaUtil.convert(\n+          new Schema(\n+                  Types.NestedField.required(0, \""id\"", Types.LongType.get()),\n+                  Types.NestedField.optional(\n+                      2,\n+                      \""properties\"",\n+                      Types.MapType.ofOptional(\n+                          3, 4, Types.StringType.get(), Types.IntegerType.get())),\n+                  Types.NestedField.required(\n+                      5,\n+                      \""location\"",\n+                      Types.StructType.of(\n+                          Types.NestedField.required(6, \""lat\"", Types.FloatType.get()),\n+                          Types.NestedField.optional(7, \""long\"", Types.FloatType.get()))),\n+                  Types.NestedField.required(\n+                      8, \""tags\"", Types.ListType.ofRequired(9, Types.StringType.get())),\n+                  Types.NestedField.optional(10, \""payload\"", Types.VariantType.get()))\n+              .asStruct());\n+\n+  @Test\n+  public void testSimple() {\n+    Schema expected = new Schema(Types.NestedField.required(0, \""id\"", Types.LongType.get()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(0));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {2, 3, 4})\n+  public void testSelectMap(int selectedId) {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.optional(\n+                2,\n+                \""properties\"",\n+                Types.MapType.ofOptional(3, 4, Types.StringType.get(), Types.IntegerType.get())));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(selectedId));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectEmptyStruct() {\n+    Schema expected = new Schema(Types.NestedField.required(5, \""location\"", Types.StructType.of()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(5));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectStructField() {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.required(\n+                5,\n+                \""location\"",\n+                Types.StructType.of(Types.NestedField.optional(7, \""long\"", Types.FloatType.get()))));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(7));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {8, 9})\n+  public void testSelectList(int selectedId) {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.required(\n+                8, \""tags\"", Types.ListType.ofRequired(9, Types.StringType.get())));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(selectedId));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectVariant() {\n+    Schema expected =\n+        new Schema(Types.NestedField.optional(10, \""payload\"", Types.VariantType.get()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(10));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+}\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java b/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\nindex d9dc49d17257..2c31fe92834b 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\n@@ -24,6 +24,7 @@\n import static org.apache.iceberg.avro.AvroTestHelpers.optionalField;\n import static org.apache.iceberg.avro.AvroTestHelpers.record;\n import static org.apache.iceberg.avro.AvroTestHelpers.requiredField;\n+import static org.apache.iceberg.avro.AvroTestHelpers.variant;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n@@ -57,7 +58,8 @@ public void testPrimitiveTypes() {\n             Types.UUIDType.get(),\n             Types.FixedType.ofLength(12),\n             Types.BinaryType.get(),\n-            Types.DecimalType.of(9, 4));\n+            Types.DecimalType.of(9, 4),\n+            Types.VariantType.get());\n \n     List<Schema> avroPrimitives =\n         Lists.newArrayList(\n@@ -77,7 +79,8 @@ public void testPrimitiveTypes() {\n             Schema.createFixed(\""fixed_12\"", null, null, 12),\n             Schema.create(Schema.Type.BYTES),\n             LogicalTypes.decimal(9, 4)\n-                .addToSchema(Schema.createFixed(\""decimal_9_4\"", null, null, 4)));\n+                .addToSchema(Schema.createFixed(\""decimal_9_4\"", null, null, 4)),\n+            variant(\""variant\""));\n \n     for (int i = 0; i < primitives.size(); i += 1) {\n       Type type = primitives.get(i);\n@@ -125,7 +128,8 @@ public void testStructAndPrimitiveTypes() {\n             optional(31, \""uuid\"", Types.UUIDType.get()),\n             optional(32, \""fixed\"", Types.FixedType.ofLength(16)),\n             optional(33, \""binary\"", Types.BinaryType.get()),\n-            optional(34, \""decimal\"", Types.DecimalType.of(14, 2)));\n+            optional(34, \""decimal\"", Types.DecimalType.of(14, 2)),\n+            optional(35, \""variant\"", Types.VariantType.get()));\n \n     Schema schema =\n         record(\n@@ -162,7 +166,8 @@ public void testStructAndPrimitiveTypes() {\n                 34,\n                 \""decimal\"",\n                 LogicalTypes.decimal(14, 2)\n-                    .addToSchema(Schema.createFixed(\""decimal_14_2\"", null, null, 6))));\n+                    .addToSchema(Schema.createFixed(\""decimal_14_2\"", null, null, 6))),\n+            optionalField(35, \""variant\"", variant(\""r35\"")));\n \n     assertThat(AvroSchemaUtil.convert(schema))\n         .as(\""Test conversion from Avro schema\"")\n@@ -370,4 +375,23 @@ public void testFieldDocsArePreserved() {\n         Lists.newArrayList(Iterables.transform(origSchema.columns(), Types.NestedField::doc));\n     assertThat(fieldDocs).isEqualTo(origFieldDocs);\n   }\n+\n+  @Test\n+  public void testVariantConversion() {\n+    org.apache.iceberg.Schema schema =\n+        new org.apache.iceberg.Schema(\n+            required(1, \""variantCol1\"", Types.VariantType.get()),\n+            required(2, \""variantCol2\"", Types.VariantType.get()));\n+    org.apache.avro.Schema avroSchema = AvroSchemaUtil.convert(schema.asStruct());\n+\n+    for (int id : Lists.newArrayList(1, 2)) {\n+      org.apache.avro.Schema variantSchema = avroSchema.getField(\""variantCol\"" + id).schema();\n+      assertThat(variantSchema.getName()).isEqualTo(\""r\"" + id);\n+      assertThat(variantSchema.getType()).isEqualTo(org.apache.avro.Schema.Type.RECORD);\n+      assertThat(variantSchema.getFields().size()).isEqualTo(2);\n+      assertThat(variantSchema.getField(\""metadata\"").schema().getType())\n+          .isEqualTo(Schema.Type.BYTES);\n+      assertThat(variantSchema.getField(\""value\"").schema().getType()).isEqualTo(Schema.Type.BYTES);\n+    }\n+  }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11831"", ""pr_id"": 11831, ""issue_id"": 10392, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Variant Data Type Support\n### Proposed Change\r\n\r\nWe would like to propose to add Variant type to Iceberg data types. \r\n\r\nVariant data types allow for the efficient binary encoding of dynamic semi-structured data such as JSON, Avro,Parquet, etc. By encoding semi-structured data as a variant column, we retain the flexibility of the source data, while allowing query engines to more efficiently operate on the data.\r\n\r\nWith the support of Variant type, such data can be encoded in an efficient binary representation internally for better performance. Without that, we need to parse the data in its format inefficiently.\r\n\r\nThis will allow the following use cases:\r\n\r\n- Create an Iceberg table with a Variant column\r\n`CREATE OR REPLACE TABLE car_sales(record Variant);`\r\n- Insert semi-structured data into the Variant column\r\n`INSERT INTO car_sales SELECT PARSE_JSON(<json_string>)`\r\n- Query against the semi-structured data\r\n`SELECT VARIANT_GET(record, '$.dealer.ship', 'string') FROM car_sales`\r\n\r\n\r\n\r\n\r\n### Proposal document\r\nhttps://docs.google.com/document/d/1sq70XDiWJ2DemWyA5dVB80gKzwi0CWoM0LOWM7VJVd8/edit?tab=t.0\r\n\r\n### Specifications\r\n\r\n- [X] Table\r\n- [ ] View\r\n- [ ] REST\r\n- [ ] Puffin\r\n- [ ] Encryption\r\n- [ ] Other"", ""issue_word_count"": 170, ""test_files_count"": 8, ""non_test_files_count"": 23, ""pr_changed_files"": [""api/src/main/java/org/apache/iceberg/Accessors.java"", ""api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java"", ""api/src/main/java/org/apache/iceberg/types/AssignIds.java"", ""api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java"", ""api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java"", ""api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java"", ""api/src/main/java/org/apache/iceberg/types/IndexById.java"", ""api/src/main/java/org/apache/iceberg/types/IndexByName.java"", ""api/src/main/java/org/apache/iceberg/types/IndexParents.java"", ""api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java"", ""api/src/main/java/org/apache/iceberg/types/PruneColumns.java"", ""api/src/main/java/org/apache/iceberg/types/ReassignDoc.java"", ""api/src/main/java/org/apache/iceberg/types/ReassignIds.java"", ""api/src/main/java/org/apache/iceberg/types/Type.java"", ""api/src/main/java/org/apache/iceberg/types/TypeUtil.java"", ""api/src/main/java/org/apache/iceberg/types/Types.java"", ""api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java"", ""api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java"", ""api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java"", ""api/src/test/java/org/apache/iceberg/types/TestTypes.java"", ""core/src/main/java/org/apache/iceberg/SchemaParser.java"", ""core/src/main/java/org/apache/iceberg/SchemaUpdate.java"", ""core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java"", ""core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java"", ""core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java"", ""core/src/main/java/org/apache/iceberg/types/FixupTypes.java"", ""core/src/test/java/org/apache/iceberg/TestSchemaParser.java"", ""core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java"", ""core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java"", ""spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java""], ""pr_changed_test_files"": [""api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java"", ""api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java"", ""api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java"", ""api/src/test/java/org/apache/iceberg/types/TestTypes.java"", ""core/src/test/java/org/apache/iceberg/TestSchemaParser.java"", ""core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java"", ""core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java""], ""base_commit"": ""bcbbd0344623ffea5b092e2de5debb0bc12892a1"", ""head_commit"": ""1472417cce3e2671de8197164078f6b056c0bd09"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11831"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11831"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-18T16:27:38.000Z"", ""patch"": ""diff --git a/api/src/main/java/org/apache/iceberg/Accessors.java b/api/src/main/java/org/apache/iceberg/Accessors.java\nindex 08233624f244..0b36730fbb4b 100644\n--- a/api/src/main/java/org/apache/iceberg/Accessors.java\n+++ b/api/src/main/java/org/apache/iceberg/Accessors.java\n@@ -232,6 +232,11 @@ public Map<Integer, Accessor<StructLike>> struct(\n       return accessors;\n     }\n \n+    @Override\n+    public Map<Integer, Accessor<StructLike>> variant(Types.VariantType variant) {\n+      return null;\n+    }\n+\n     @Override\n     public Map<Integer, Accessor<StructLike>> field(\n         Types.NestedField field, Map<Integer, Accessor<StructLike>> fieldResult) {\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\nindex e58f76a8de56..75055cddc197 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n@@ -124,6 +124,11 @@ public Type map(Types.MapType map, Supplier<Type> keyFuture, Supplier<Type> valu\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignIds.java b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\nindex 68588f581adc..b2f72751eb89 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n@@ -92,6 +92,11 @@ public Type map(Types.MapType map, Supplier<Type> keyFuture, Supplier<Type> valu\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java b/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\nindex 502e52c345e5..725f7f42562e 100644\n--- a/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\n+++ b/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\n@@ -250,6 +250,16 @@ public List<String> map(\n     }\n   }\n \n+  @Override\n+  public List<String> variant(Types.VariantType readVariant) {\n+    if (currentType.isVariantType()) {\n+      return NO_ERRORS;\n+    }\n+\n+    // Currently promotion is not allowed to variant type\n+    return ImmutableList.of(String.format(\"": %s cannot be read as a %s\"", currentType, readVariant));\n+  }\n+\n   @Override\n   public List<String> primitive(Type.PrimitiveType readPrimitive) {\n     if (currentType.equals(readPrimitive)) {\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java b/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\nindex f0750f337e2e..64faebb48243 100644\n--- a/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\n+++ b/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\n@@ -77,9 +77,9 @@ public Type map(Types.MapType map, Type keyResult, Type valueResult) {\n   }\n \n   @Override\n-  public Type variant() {\n-    if (predicate.test(Types.VariantType.get())) {\n-      return Types.VariantType.get();\n+  public Type variant(Types.VariantType variant) {\n+    if (predicate.test(variant)) {\n+      return variant;\n     }\n \n     return null;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java b/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\nindex a8a7de065ece..1ec70b8578bc 100644\n--- a/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\n@@ -47,7 +47,9 @@ public Set<Integer> struct(Types.StructType struct, List<Set<Integer>> fieldResu\n \n   @Override\n   public Set<Integer> field(Types.NestedField field, Set<Integer> fieldResult) {\n-    if ((includeStructIds && field.type().isStructType()) || field.type().isPrimitiveType()) {\n+    if ((includeStructIds && field.type().isStructType())\n+        || field.type().isPrimitiveType()\n+        || field.type().isVariantType()) {\n       fieldIds.add(field.fieldId());\n     }\n     return fieldIds;\n@@ -72,4 +74,9 @@ public Set<Integer> map(Types.MapType map, Set<Integer> keyResult, Set<Integer>\n     }\n     return fieldIds;\n   }\n+\n+  @Override\n+  public Set<Integer> variant(Types.VariantType variant) {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexById.java b/api/src/main/java/org/apache/iceberg/types/IndexById.java\nindex 40280c5ed9dd..a7b96eb381f7 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexById.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexById.java\n@@ -64,4 +64,9 @@ public Map<Integer, Types.NestedField> map(\n     }\n     return null;\n   }\n+\n+  @Override\n+  public Map<Integer, Types.NestedField> variant(Types.VariantType variant) {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexByName.java b/api/src/main/java/org/apache/iceberg/types/IndexByName.java\nindex 131434c9a156..60258f5c5c3e 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexByName.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexByName.java\n@@ -177,7 +177,7 @@ public Map<String, Integer> map(\n   }\n \n   @Override\n-  public Map<String, Integer> variant() {\n+  public Map<String, Integer> variant(Types.VariantType variant) {\n     return nameToId;\n   }\n \n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexParents.java b/api/src/main/java/org/apache/iceberg/types/IndexParents.java\nindex 952447ed2799..6e611d47e912 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexParents.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexParents.java\n@@ -77,7 +77,7 @@ public Map<Integer, Integer> map(\n   }\n \n   @Override\n-  public Map<Integer, Integer> variant() {\n+  public Map<Integer, Integer> variant(Types.VariantType variant) {\n     return idToParent;\n   }\n \n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java b/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\nindex 42f0da38167d..928a65878d3a 100644\n--- a/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\n+++ b/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\n@@ -33,6 +33,6 @@ class PrimitiveHolder implements Serializable {\n   }\n \n   Object readResolve() throws ObjectStreamException {\n-    return Types.fromPrimitiveString(typeAsString);\n+    return Types.fromTypeName(typeAsString);\n   }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/PruneColumns.java b/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\nindex daf2e6bbc0ca..56f01cf34bb5 100644\n--- a/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\n+++ b/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\n@@ -159,6 +159,11 @@ public Type map(Types.MapType map, Type ignored, Type valueResult) {\n     return null;\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return null;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return null;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\nindex 9ce04a7bd103..328d81c42885 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n@@ -96,6 +96,11 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignIds.java b/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\nindex 565ceee2a901..3d114f093f6b 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\n@@ -157,6 +157,11 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive; // nothing to reassign\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Type.java b/api/src/main/java/org/apache/iceberg/types/Type.java\nindex f4c6f22134a5..67e40df9e939 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Type.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Type.java\n@@ -82,6 +82,10 @@ default Types.MapType asMapType() {\n     throw new IllegalArgumentException(\""Not a map type: \"" + this);\n   }\n \n+  default Types.VariantType asVariantType() {\n+    throw new IllegalArgumentException(\""Not a variant type: \"" + this);\n+  }\n+\n   default boolean isNestedType() {\n     return false;\n   }\n@@ -98,6 +102,10 @@ default boolean isMapType() {\n     return false;\n   }\n \n+  default boolean isVariantType() {\n+    return false;\n+  }\n+\n   default NestedType asNestedType() {\n     throw new IllegalArgumentException(\""Not a nested type: \"" + this);\n   }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\nindex 39f2898757a6..4892696ab450 100644\n--- a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n+++ b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n@@ -616,8 +616,16 @@ public T map(Types.MapType map, T keyResult, T valueResult) {\n       return null;\n     }\n \n+    /**\n+     * @deprecated will be removed in 2.0.0; use {@link #variant(Types.VariantType)} instead.\n+     */\n+    @Deprecated\n     public T variant() {\n-      return null;\n+      return variant(Types.VariantType.get());\n+    }\n+\n+    public T variant(Types.VariantType variant) {\n+      throw new UnsupportedOperationException(\""Unsupported type: variant\"");\n     }\n \n     public T primitive(Type.PrimitiveType primitive) {\n@@ -684,7 +692,7 @@ public static <T> T visit(Type type, SchemaVisitor<T> visitor) {\n         return visitor.map(map, keyResult, valueResult);\n \n       case VARIANT:\n-        return visitor.variant();\n+        return visitor.variant(type.asVariantType());\n \n       default:\n         return visitor.primitive(type.asPrimitiveType());\n@@ -712,6 +720,10 @@ public T map(Types.MapType map, Supplier<T> keyResult, Supplier<T> valueResult)\n       return null;\n     }\n \n+    public T variant(Types.VariantType variant) {\n+      throw new UnsupportedOperationException(\""Unsupported type: variant\"");\n+    }\n+\n     public T primitive(Type.PrimitiveType primitive) {\n       return null;\n     }\n@@ -788,6 +800,9 @@ public static <T> T visit(Type type, CustomOrderSchemaVisitor<T> visitor) {\n             new VisitFuture<>(map.keyType(), visitor),\n             new VisitFuture<>(map.valueType(), visitor));\n \n+      case VARIANT:\n+        return visitor.variant(type.asVariantType());\n+\n       default:\n         return visitor.primitive(type.asPrimitiveType());\n     }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Types.java b/api/src/main/java/org/apache/iceberg/types/Types.java\nindex 6882f718508b..c1935d6980e9 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Types.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Types.java\n@@ -39,8 +39,8 @@ public class Types {\n \n   private Types() {}\n \n-  private static final ImmutableMap<String, PrimitiveType> TYPES =\n-      ImmutableMap.<String, PrimitiveType>builder()\n+  private static final ImmutableMap<String, Type> TYPES =\n+      ImmutableMap.<String, Type>builder()\n           .put(BooleanType.get().toString(), BooleanType.get())\n           .put(IntegerType.get().toString(), IntegerType.get())\n           .put(LongType.get().toString(), LongType.get())\n@@ -56,13 +56,14 @@ private Types() {}\n           .put(UUIDType.get().toString(), UUIDType.get())\n           .put(BinaryType.get().toString(), BinaryType.get())\n           .put(UnknownType.get().toString(), UnknownType.get())\n+          .put(VariantType.get().toString(), VariantType.get())\n           .buildOrThrow();\n \n   private static final Pattern FIXED = Pattern.compile(\""fixed\\\\[\\\\s*(\\\\d+)\\\\s*\\\\]\"");\n   private static final Pattern DECIMAL =\n       Pattern.compile(\""decimal\\\\(\\\\s*(\\\\d+)\\\\s*,\\\\s*(\\\\d+)\\\\s*\\\\)\"");\n \n-  public static PrimitiveType fromPrimitiveString(String typeString) {\n+  public static Type fromTypeName(String typeString) {\n     String lowerTypeString = typeString.toLowerCase(Locale.ROOT);\n     if (TYPES.containsKey(lowerTypeString)) {\n       return TYPES.get(lowerTypeString);\n@@ -81,6 +82,15 @@ public static PrimitiveType fromPrimitiveString(String typeString) {\n     throw new IllegalArgumentException(\""Cannot parse type string to primitive: \"" + typeString);\n   }\n \n+  public static PrimitiveType fromPrimitiveString(String typeString) {\n+    Type type = fromTypeName(typeString);\n+    if (type.isPrimitiveType()) {\n+      return type.asPrimitiveType();\n+    }\n+\n+    throw new IllegalArgumentException(\""Cannot parse type string: variant is not a primitive type\"");\n+  }\n+\n   public static class BooleanType extends PrimitiveType {\n     private static final BooleanType INSTANCE = new BooleanType();\n \n@@ -430,6 +440,16 @@ public String toString() {\n       return \""variant\"";\n     }\n \n+    @Override\n+    public boolean isVariantType() {\n+      return true;\n+    }\n+\n+    @Override\n+    public VariantType asVariantType() {\n+      return this;\n+    }\n+\n     @Override\n     public boolean equals(Object o) {\n       if (this == o) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SchemaParser.java b/core/src/main/java/org/apache/iceberg/SchemaParser.java\nindex 27e6ed048712..04655ce3f7d7 100644\n--- a/core/src/main/java/org/apache/iceberg/SchemaParser.java\n+++ b/core/src/main/java/org/apache/iceberg/SchemaParser.java\n@@ -143,8 +143,8 @@ static void toJson(Type.PrimitiveType primitive, JsonGenerator generator) throws\n   }\n \n   static void toJson(Type type, JsonGenerator generator) throws IOException {\n-    if (type.isPrimitiveType()) {\n-      toJson(type.asPrimitiveType(), generator);\n+    if (type.isPrimitiveType() || type.isVariantType()) {\n+      generator.writeString(type.toString());\n     } else {\n       Type.NestedType nested = type.asNestedType();\n       switch (type.typeId()) {\n@@ -179,7 +179,7 @@ public static String toJson(Schema schema, boolean pretty) {\n \n   private static Type typeFromJson(JsonNode json) {\n     if (json.isTextual()) {\n-      return Types.fromPrimitiveString(json.asText());\n+      return Types.fromTypeName(json.asText());\n     } else if (json.isObject()) {\n       JsonNode typeObj = json.get(TYPE);\n       if (typeObj != null) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SchemaUpdate.java b/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\nindex 2b541080ac72..7726c3a785d0 100644\n--- a/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\n@@ -722,6 +722,11 @@ public Type map(Types.MapType map, Type kResult, Type valueResult) {\n       }\n     }\n \n+    @Override\n+    public Type variant(Types.VariantType variant) {\n+      return variant;\n+    }\n+\n     @Override\n     public Type primitive(Type.PrimitiveType primitive) {\n       return primitive;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java b/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\nindex de6ce2ad0425..fc3d920a4069 100644\n--- a/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\n@@ -302,6 +302,11 @@ public MappedFields map(Types.MapType map, MappedFields keyResult, MappedFields\n           MappedField.of(map.valueId(), \""value\"", valueResult));\n     }\n \n+    @Override\n+    public MappedFields variant(Types.VariantType variant) {\n+      return null; // no mapping because variant has no nested fields with IDs\n+    }\n+\n     @Override\n     public MappedFields primitive(Type.PrimitiveType primitive) {\n       return null; // no mapping because primitives have no nested fields\n\ndiff --git a/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java b/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\nindex 9b2226f5714d..694bfb2f6242 100644\n--- a/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\n@@ -107,6 +107,9 @@ public static <P, T> T visit(\n \n         return visitor.map(map, partner, keyResult, valueResult);\n \n+      case VARIANT:\n+        return visitor.variant(type.asVariantType(), partner);\n+\n       default:\n         return visitor.primitive(type.asPrimitiveType(), partner);\n     }\n@@ -160,6 +163,10 @@ public R map(Types.MapType map, P partner, R keyResult, R valueResult) {\n     return null;\n   }\n \n+  public R variant(Types.VariantType variant, P partner) {\n+    throw new UnsupportedOperationException(\""Unsupported type: variant\"");\n+  }\n+\n   public R primitive(Type.PrimitiveType primitive, P partner) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\nindex 68172b7062a6..7c4dac9feff1 100644\n--- a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n@@ -142,6 +142,11 @@ public Boolean map(\n     return false;\n   }\n \n+  @Override\n+  public Boolean variant(Types.VariantType variant, Integer partnerId) {\n+    return partnerId == null;\n+  }\n+\n   @Override\n   public Boolean primitive(Type.PrimitiveType primitive, Integer partnerId) {\n     return partnerId == null;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/types/FixupTypes.java b/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\nindex 23fccddda3d9..1e4c0b597a6a 100644\n--- a/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\n+++ b/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\n@@ -147,6 +147,12 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    // nothing to fix up\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     if (sourceType.equals(primitive)) {\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex af0fa84f67a1..ad8a4beb55d0 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n@@ -565,6 +565,11 @@ public String map(Types.MapType map, String keyResult, String valueResult) {\n       return \""map<\"" + keyResult + \"", \"" + valueResult + \"">\"";\n     }\n \n+    @Override\n+    public String variant(Types.VariantType variant) {\n+      return \""variant\"";\n+    }\n+\n     @Override\n     public String primitive(Type.PrimitiveType primitive) {\n       switch (primitive.typeId()) {\n"", ""test_patch"": ""diff --git a/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java b/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\nindex 2d02da5346a7..debb9c9dc1d6 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\n@@ -22,10 +22,15 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n \n+import java.util.Arrays;\n import java.util.List;\n+import java.util.stream.Stream;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.types.Type.PrimitiveType;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestReadabilityChecks {\n   private static final Type.PrimitiveType[] PRIMITIVES =\n@@ -112,6 +117,40 @@ private void testDisallowPrimitiveToStruct(PrimitiveType from, Schema fromSchema\n         .contains(\""cannot be read as a struct\"");\n   }\n \n+  @Test\n+  public void testVariantToVariant() {\n+    Schema fromSchema = new Schema(required(1, \""from_field\"", Types.VariantType.get()));\n+    List<String> errors =\n+        CheckCompatibility.writeCompatibilityErrors(\n+            new Schema(required(1, \""to_field\"", Types.VariantType.get())), fromSchema);\n+    assertThat(errors).as(\""Should produce 0 error messages\"").isEmpty();\n+  }\n+\n+  private static Stream<Arguments> incompatibleTypesToVariant() {\n+    return Stream.of(\n+            Stream.of(\n+                Arguments.of(Types.StructType.of(required(1, \""from\"", Types.IntegerType.get()))),\n+                Arguments.of(\n+                    Types.MapType.ofRequired(\n+                        1, 2, Types.StringType.get(), Types.IntegerType.get())),\n+                Arguments.of(Types.ListType.ofRequired(1, Types.StringType.get()))),\n+            Arrays.stream(PRIMITIVES).map(type -> Arguments.of(type)))\n+        .flatMap(s -> s);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""incompatibleTypesToVariant\"")\n+  public void testIncompatibleTypesToVariant(Type from) {\n+    Schema fromSchema = new Schema(required(3, \""from_field\"", from));\n+    List<String> errors =\n+        CheckCompatibility.writeCompatibilityErrors(\n+            new Schema(required(3, \""to_field\"", Types.VariantType.get())), fromSchema);\n+    assertThat(errors).hasSize(1);\n+    assertThat(errors.get(0))\n+        .as(\""Should complain that other type to variant is not allowed\"")\n+        .contains(\""cannot be read as a variant\"");\n+  }\n+\n   @Test\n   public void testRequiredSchemaField() {\n     Schema write = new Schema(optional(1, \""from_field\"", Types.IntegerType.get()));\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\nindex a222e8e66b8e..790f59587c59 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n@@ -46,6 +46,7 @@ public void testIdentityTypes() throws Exception {\n           Types.StringType.get(),\n           Types.UUIDType.get(),\n           Types.BinaryType.get(),\n+          Types.UnknownType.get()\n         };\n \n     for (Type type : identityPrimitives) {\n@@ -136,15 +137,6 @@ public void testVariant() throws Exception {\n         .isEqualTo(variant);\n   }\n \n-  @Test\n-  public void testUnknown() throws Exception {\n-    Types.UnknownType unknown = Types.UnknownType.get();\n-    Type copy = TestHelpers.roundTripSerialize(unknown);\n-    assertThat(copy)\n-        .as(\""Unknown serialization should be equal to starting type\"")\n-        .isEqualTo(unknown);\n-  }\n-\n   @Test\n   public void testSchema() throws Exception {\n     Schema schema =\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\nindex 36384d232af3..b6556d70bd85 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n@@ -23,12 +23,18 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Types.IntegerType;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestTypeUtil {\n   @Test\n@@ -645,4 +651,95 @@ public void testReassignOrRefreshIdsCaseInsensitive() {\n                 required(2, \""FIELD2\"", Types.IntegerType.get())));\n     assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n   }\n+\n+  private static Stream<Arguments> testTypes() {\n+    return Stream.of(\n+        Arguments.of(Types.UnknownType.get()),\n+        Arguments.of(Types.VariantType.get()),\n+        Arguments.of(Types.TimestampNanoType.withoutZone()),\n+        Arguments.of(Types.TimestampNanoType.withZone()));\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testAssignIdsWithType(Type testType) {\n+    Types.StructType sourceType =\n+        Types.StructType.of(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+    Type expectedType =\n+        Types.StructType.of(required(10, \""id\"", IntegerType.get()), required(11, \""data\"", testType));\n+\n+    Type assignedType = TypeUtil.assignIds(sourceType, oldId -> oldId + 10);\n+    assertThat(assignedType).isEqualTo(expectedType);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testAssignFreshIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+\n+    Schema assignedSchema = TypeUtil.assignFreshIds(schema, new AtomicInteger(10)::incrementAndGet);\n+    Schema expectedSchema =\n+        new Schema(required(11, \""id\"", IntegerType.get()), required(12, \""data\"", testType));\n+    assertThat(assignedSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testReassignIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+    Schema sourceSchema =\n+        new Schema(required(1, \""id\"", IntegerType.get()), required(2, \""data\"", testType));\n+\n+    Schema reassignedSchema = TypeUtil.reassignIds(schema, sourceSchema);\n+    assertThat(reassignedSchema.asStruct()).isEqualTo(sourceSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testIndexByIdWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+\n+    Map<Integer, Types.NestedField> indexByIds = TypeUtil.indexById(schema.asStruct());\n+    assertThat(indexByIds.get(1).type()).isEqualTo(testType);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testIndexNameByIdWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+\n+    Map<Integer, String> indexNameByIds = TypeUtil.indexNameById(schema.asStruct());\n+    assertThat(indexNameByIds.get(1)).isEqualTo(\""data\"");\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testProjectWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+\n+    Schema expectedSchema = new Schema(required(1, \""data\"", testType));\n+    Schema projectedSchema = TypeUtil.project(schema, Sets.newHashSet(1));\n+    assertThat(projectedSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testGetProjectedIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+\n+    Set<Integer> projectedIds = TypeUtil.getProjectedIds(schema);\n+    assertThat(Set.of(0, 1)).isEqualTo(projectedIds);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\""testTypes\"")\n+  public void testReassignDocWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \""id\"", IntegerType.get()), required(1, \""data\"", testType));\n+    Schema docSourceSchema =\n+        new Schema(\n+            required(0, \""id\"", IntegerType.get(), \""id\""), required(1, \""data\"", testType, \""data\""));\n+\n+    Schema reassignedSchema = TypeUtil.reassignDoc(schema, docSourceSchema);\n+    assertThat(reassignedSchema.asStruct()).isEqualTo(docSourceSchema.asStruct());\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestTypes.java b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\nindex cbc37291375f..b3381d1ff440 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n@@ -25,6 +25,30 @@\n \n public class TestTypes {\n \n+  @Test\n+  public void fromTypeName() {\n+    assertThat(Types.fromTypeName(\""boolean\"")).isSameAs(Types.BooleanType.get());\n+    assertThat(Types.fromTypeName(\""BooLean\"")).isSameAs(Types.BooleanType.get());\n+\n+    assertThat(Types.fromTypeName(\""timestamp\"")).isSameAs(Types.TimestampType.withoutZone());\n+    assertThat(Types.fromTypeName(\""timestamptz\"")).isSameAs(Types.TimestampType.withZone());\n+    assertThat(Types.fromTypeName(\""timestamp_ns\"")).isSameAs(Types.TimestampNanoType.withoutZone());\n+    assertThat(Types.fromTypeName(\""timestamptz_ns\"")).isSameAs(Types.TimestampNanoType.withZone());\n+\n+    assertThat(Types.fromTypeName(\""Fixed[ 3 ]\"")).isEqualTo(Types.FixedType.ofLength(3));\n+\n+    assertThat(Types.fromTypeName(\""Decimal( 2 , 3 )\"")).isEqualTo(Types.DecimalType.of(2, 3));\n+\n+    assertThat(Types.fromTypeName(\""Decimal(2,3)\"")).isEqualTo(Types.DecimalType.of(2, 3));\n+\n+    assertThat(Types.fromTypeName(\""variant\"")).isSameAs(Types.VariantType.get());\n+    assertThat(Types.fromTypeName(\""Variant\"")).isSameAs(Types.VariantType.get());\n+\n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromTypeName(\""abcdefghij\""))\n+        .withMessage(\""Cannot parse type string to primitive: abcdefghij\"");\n+  }\n+\n   @Test\n   public void fromPrimitiveString() {\n     assertThat(Types.fromPrimitiveString(\""boolean\"")).isSameAs(Types.BooleanType.get());\n@@ -43,6 +67,13 @@ public void fromPrimitiveString() {\n \n     assertThat(Types.fromPrimitiveString(\""Decimal(2,3)\"")).isEqualTo(Types.DecimalType.of(2, 3));\n \n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromPrimitiveString(\""variant\""))\n+        .withMessage(\""Cannot parse type string: variant is not a primitive type\"");\n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromPrimitiveString(\""Variant\""))\n+        .withMessage(\""Cannot parse type string: variant is not a primitive type\"");\n+\n     assertThatExceptionOfType(IllegalArgumentException.class)\n         .isThrownBy(() -> Types.fromPrimitiveString(\""abcdefghij\""))\n         .withMessage(\""Cannot parse type string to primitive: abcdefghij\"");\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSchemaParser.java b/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\nindex ebd197a68af0..40db5cfee2cb 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\n@@ -123,4 +123,14 @@ public void testPrimitiveTypeDefaultValues(Type.PrimitiveType type, Object defau\n     assertThat(serialized.findField(\""col_with_default\"").initialDefault()).isEqualTo(defaultValue);\n     assertThat(serialized.findField(\""col_with_default\"").writeDefault()).isEqualTo(defaultValue);\n   }\n+\n+  @Test\n+  public void testVariantType() throws IOException {\n+    Schema schema =\n+        new Schema(\n+            Types.NestedField.required(1, \""id\"", Types.IntegerType.get()),\n+            Types.NestedField.optional(2, \""data\"", Types.VariantType.get()));\n+\n+    writeAndValidate(schema);\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\nindex 656e72a0c19c..8649ada99bef 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n@@ -26,7 +26,7 @@\n import java.util.List;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.types.Type.PrimitiveType;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.BinaryType;\n import org.apache.iceberg.types.Types.BooleanType;\n@@ -42,13 +42,16 @@\n import org.apache.iceberg.types.Types.StringType;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.types.Types.TimeType;\n+import org.apache.iceberg.types.Types.TimestampNanoType;\n import org.apache.iceberg.types.Types.TimestampType;\n import org.apache.iceberg.types.Types.UUIDType;\n+import org.apache.iceberg.types.Types.UnknownType;\n+import org.apache.iceberg.types.Types.VariantType;\n import org.junit.jupiter.api.Test;\n \n public class TestSchemaUnionByFieldName {\n \n-  private static List<? extends PrimitiveType> primitiveTypes() {\n+  private static List<? extends Type> primitiveTypes() {\n     return Lists.newArrayList(\n         StringType.get(),\n         TimeType.get(),\n@@ -63,11 +66,15 @@ private static List<? extends PrimitiveType> primitiveTypes() {\n         FixedType.ofLength(10),\n         DecimalType.of(10, 2),\n         LongType.get(),\n-        FloatType.get());\n+        FloatType.get(),\n+        VariantType.get(),\n+        UnknownType.get(),\n+        TimestampNanoType.withoutZone(),\n+        TimestampNanoType.withZone());\n   }\n \n   private static NestedField[] primitiveFields(\n-      Integer initialValue, List<? extends PrimitiveType> primitiveTypes) {\n+      Integer initialValue, List<? extends Type> primitiveTypes) {\n     AtomicInteger atomicInteger = new AtomicInteger(initialValue);\n     return primitiveTypes.stream()\n         .map(\n@@ -75,7 +82,7 @@ private static NestedField[] primitiveFields(\n                 optional(\n                     atomicInteger.incrementAndGet(),\n                     type.toString(),\n-                    Types.fromPrimitiveString(type.toString())))\n+                    Types.fromTypeName(type.toString())))\n         .toArray(NestedField[]::new);\n   }\n \n@@ -88,7 +95,7 @@ public void testAddTopLevelPrimitives() {\n \n   @Test\n   public void testAddTopLevelListOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema newSchema =\n           new Schema(optional(1, \""aList\"", Types.ListType.ofOptional(2, primitiveType)));\n       Schema applied = new SchemaUpdate(new Schema(), 0).unionByNameWith(newSchema).apply();\n@@ -98,7 +105,7 @@ public void testAddTopLevelListOfPrimitives() {\n \n   @Test\n   public void testAddTopLevelMapOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema newSchema =\n           new Schema(\n               optional(1, \""aMap\"", Types.MapType.ofOptional(2, 3, primitiveType, primitiveType)));\n@@ -109,7 +116,7 @@ public void testAddTopLevelMapOfPrimitives() {\n \n   @Test\n   public void testAddTopLevelStructOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema currentSchema =\n           new Schema(\n               optional(1, \""aStruct\"", Types.StructType.of(optional(2, \""primitive\"", primitiveType))));\n@@ -120,7 +127,7 @@ public void testAddTopLevelStructOfPrimitives() {\n \n   @Test\n   public void testAddNestedPrimitive() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema currentSchema = new Schema(optional(1, \""aStruct\"", Types.StructType.of()));\n       Schema newSchema =\n           new Schema(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java b/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\nindex d30a93d50d49..3ebf4d9242ab 100644\n--- a/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\n+++ b/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\n@@ -289,4 +289,16 @@ public void testMappingFindByName() {\n                 \""location\"",\n                 MappedFields.of(MappedField.of(11, \""latitude\""), MappedField.of(12, \""longitude\""))));\n   }\n+\n+  @Test\n+  public void testMappingVariantType() {\n+    Schema schema =\n+        new Schema(\n+            required(1, \""id\"", Types.LongType.get()), required(2, \""data\"", Types.VariantType.get()));\n+\n+    MappedFields expected = MappedFields.of(MappedField.of(1, \""id\""), MappedField.of(2, \""data\""));\n+\n+    NameMapping mapping = MappingUtil.create(schema);\n+    assertThat(mapping.asMappedFields()).isEqualTo(expected);\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\nindex 6f900ffebb10..e4e66abfefa0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n@@ -105,12 +105,13 @@ public void testDescribeSchema() {\n                 3,\n                 \""pairs\"",\n                 Types.MapType.ofOptional(4, 5, Types.StringType.get(), Types.LongType.get())),\n-            required(6, \""time\"", Types.TimestampType.withoutZone()));\n+            required(6, \""time\"", Types.TimestampType.withoutZone()),\n+            required(7, \""v\"", Types.VariantType.get()));\n \n     assertThat(Spark3Util.describe(schema))\n         .as(\""Schema description isn't correct.\"")\n         .isEqualTo(\n-            \""struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null>\"");\n+            \""struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null,v: variant not null>\"");\n   }\n \n   @Test\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11495"", ""pr_id"": 11495, ""issue_id"": 11122, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other"", ""issue_word_count"": 118, ""test_files_count"": 17, ""non_test_files_count"": 2, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/BaseRowDelta.java"", ""core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java"", ""core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java"", ""core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java"", ""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestBatchScans.java"", ""core/src/test/java/org/apache/iceberg/TestCommitReporting.java"", ""core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java"", ""core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java"", ""core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java"", ""core/src/test/java/org/apache/iceberg/TestRewriteFiles.java"", ""core/src/test/java/org/apache/iceberg/TestRewriteManifests.java"", ""core/src/test/java/org/apache/iceberg/TestRowDelta.java"", ""core/src/test/java/org/apache/iceberg/TestSnapshot.java"", ""core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java"", ""data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java"", ""core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java"", ""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestBatchScans.java"", ""core/src/test/java/org/apache/iceberg/TestCommitReporting.java"", ""core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java"", ""core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java"", ""core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java"", ""core/src/test/java/org/apache/iceberg/TestRewriteFiles.java"", ""core/src/test/java/org/apache/iceberg/TestRewriteManifests.java"", ""core/src/test/java/org/apache/iceberg/TestRowDelta.java"", ""core/src/test/java/org/apache/iceberg/TestSnapshot.java"", ""core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java"", ""data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java""], ""base_commit"": ""166edc7298825321f677e1e70cf88d7249e8035c"", ""head_commit"": ""a57f47c28ce8b7515596d9ece4aefc8d1fd8ee16"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11495"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11495"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-11T21:08:23.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\nindex 85c2269ee526..372fc5367f08 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n@@ -139,6 +139,8 @@ protected void validate(TableMetadata base, Snapshot parent) {\n       if (validateNewDeleteFiles) {\n         validateNoNewDeleteFiles(base, startingSnapshotId, conflictDetectionFilter, parent);\n       }\n+\n+      validateAddedDVs(base, startingSnapshotId, conflictDetectionFilter, parent);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\nindex 50885dbb06c7..6198ad00f680 100644\n--- a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\n@@ -48,11 +48,13 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.DataFileSet;\n import org.apache.iceberg.util.DeleteFileSet;\n import org.apache.iceberg.util.Pair;\n import org.apache.iceberg.util.PartitionSet;\n import org.apache.iceberg.util.SnapshotUtil;\n+import org.apache.iceberg.util.Tasks;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -70,6 +72,9 @@ abstract class MergingSnapshotProducer<ThisT> extends SnapshotProducer<ThisT> {\n   // delete files can be added in \""overwrite\"" or \""delete\"" operations\n   private static final Set<String> VALIDATE_ADDED_DELETE_FILES_OPERATIONS =\n       ImmutableSet.of(DataOperations.OVERWRITE, DataOperations.DELETE);\n+  // DVs can be added in \""overwrite\"", \""delete\"", and \""replace\"" operations\n+  private static final Set<String> VALIDATE_ADDED_DVS_OPERATIONS =\n+      ImmutableSet.of(DataOperations.OVERWRITE, DataOperations.DELETE, DataOperations.REPLACE);\n \n   private final String tableName;\n   private final TableOperations ops;\n@@ -83,6 +88,7 @@ abstract class MergingSnapshotProducer<ThisT> extends SnapshotProducer<ThisT> {\n   private final Map<Integer, DataFileSet> newDataFilesBySpec = Maps.newHashMap();\n   private Long newDataFilesDataSequenceNumber;\n   private final Map<Integer, DeleteFileSet> newDeleteFilesBySpec = Maps.newHashMap();\n+  private final Set<String> newDVRefs = Sets.newHashSet();\n   private final List<ManifestFile> appendManifests = Lists.newArrayList();\n   private final List<ManifestFile> rewrittenAppendManifests = Lists.newArrayList();\n   private final SnapshotSummary.Builder addedFilesSummary = SnapshotSummary.builder();\n@@ -245,13 +251,13 @@ private PartitionSpec spec(int specId) {\n \n   /** Add a delete file to the new snapshot. */\n   protected void add(DeleteFile file) {\n-    Preconditions.checkNotNull(file, \""Invalid delete file: null\"");\n+    validateNewDeleteFile(file);\n     add(new PendingDeleteFile(file));\n   }\n \n   /** Add a delete file to the new snapshot. */\n   protected void add(DeleteFile file, long dataSequenceNumber) {\n-    Preconditions.checkNotNull(file, \""Invalid delete file: null\"");\n+    validateNewDeleteFile(file);\n     add(new PendingDeleteFile(file, dataSequenceNumber));\n   }\n \n@@ -268,9 +274,39 @@ private void add(PendingDeleteFile file) {\n     if (deleteFiles.add(file)) {\n       addedFilesSummary.addedFile(spec, file);\n       hasNewDeleteFiles = true;\n+      if (ContentFileUtil.isDV(file)) {\n+        newDVRefs.add(file.referencedDataFile());\n+      }\n+    }\n+  }\n+\n+  protected void validateNewDeleteFile(DeleteFile file) {\n+    Preconditions.checkNotNull(file, \""Invalid delete file: null\"");\n+    switch (formatVersion()) {\n+      case 1:\n+        throw new IllegalArgumentException(\""Deletes are supported in V2 and above\"");\n+      case 2:\n+        Preconditions.checkArgument(\n+            file.content() == FileContent.EQUALITY_DELETES || !ContentFileUtil.isDV(file),\n+            \""Must not use DVs for position deletes in V2: %s\"",\n+            ContentFileUtil.dvDesc(file));\n+        break;\n+      case 3:\n+        Preconditions.checkArgument(\n+            file.content() == FileContent.EQUALITY_DELETES || ContentFileUtil.isDV(file),\n+            \""Must use DVs for position deletes in V%s: %s\"",\n+            formatVersion(),\n+            file.location());\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\""Unsupported format version: \"" + formatVersion());\n     }\n   }\n \n+  private int formatVersion() {\n+    return ops.current().formatVersion();\n+  }\n+\n   /** Add all files in a manifest to the new snapshot. */\n   protected void add(ManifestFile manifest) {\n     Preconditions.checkArgument(\n@@ -769,6 +805,58 @@ protected void validateDataFilesExist(\n     }\n   }\n \n+  // validates there are no concurrently added DVs for referenced data files\n+  protected void validateAddedDVs(\n+      TableMetadata base,\n+      Long startingSnapshotId,\n+      Expression conflictDetectionFilter,\n+      Snapshot parent) {\n+    // skip if there is no current table state or this operation doesn't add new DVs\n+    if (parent == null || newDVRefs.isEmpty()) {\n+      return;\n+    }\n+\n+    Pair<List<ManifestFile>, Set<Long>> history =\n+        validationHistory(\n+            base,\n+            startingSnapshotId,\n+            VALIDATE_ADDED_DVS_OPERATIONS,\n+            ManifestContent.DELETES,\n+            parent);\n+    List<ManifestFile> newDeleteManifests = history.first();\n+    Set<Long> newSnapshotIds = history.second();\n+\n+    Tasks.foreach(newDeleteManifests)\n+        .stopOnFailure()\n+        .throwFailureWhenFinished()\n+        .executeWith(workerPool())\n+        .run(manifest -> validateAddedDVs(manifest, conflictDetectionFilter, newSnapshotIds));\n+  }\n+\n+  private void validateAddedDVs(\n+      ManifestFile manifest, Expression conflictDetectionFilter, Set<Long> newSnapshotIds) {\n+    try (CloseableIterable<ManifestEntry<DeleteFile>> entries =\n+        ManifestFiles.readDeleteManifest(manifest, ops.io(), ops.current().specsById())\n+            .filterRows(conflictDetectionFilter)\n+            .caseSensitive(caseSensitive)\n+            .liveEntries()) {\n+\n+      for (ManifestEntry<DeleteFile> entry : entries) {\n+        DeleteFile file = entry.file();\n+        if (newSnapshotIds.contains(entry.snapshotId()) && ContentFileUtil.isDV(file)) {\n+          ValidationException.check(\n+              !newDVRefs.contains(file.referencedDataFile()),\n+              \""Found concurrently added DV for %s: %s\"",\n+              file.referencedDataFile(),\n+              ContentFileUtil.dvDesc(file));\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  // returns newly added manifests and snapshot IDs between the starting and parent snapshots\n   private Pair<List<ManifestFile>, Set<Long>> validationHistory(\n       TableMetadata base,\n       Long startingSnapshotId,\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\nindex 6ef28191e78e..481422457b73 100644\n--- a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n@@ -53,15 +53,6 @@ public static List<Object> parameters() {\n     return Arrays.asList(2, 3);\n   }\n \n-  static final DeleteFile FILE_A_POS_1 =\n-      FileMetadata.deleteFileBuilder(SPEC)\n-          .ofPositionDeletes()\n-          .withPath(\""/path/to/data-a-pos-deletes.parquet\"")\n-          .withFileSizeInBytes(10)\n-          .withPartition(FILE_A.partition())\n-          .withRecordCount(1)\n-          .build();\n-\n   static final DeleteFile FILE_A_EQ_1 =\n       FileMetadata.deleteFileBuilder(SPEC)\n           .ofEqualityDeletes()\n@@ -311,7 +302,7 @@ public void testUnpartitionedTableScan() throws IOException {\n   public void testPartitionedTableWithPartitionPosDeletes() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\""Should have one task\"").hasSize(1);\n@@ -323,7 +314,7 @@ public void testPartitionedTableWithPartitionPosDeletes() {\n     assertThat(task.deletes()).as(\""Should have one associated delete file\"").hasSize(1);\n     assertThat(task.deletes().get(0).path())\n         .as(\""Should have only pos delete file\"")\n-        .isEqualTo(FILE_A_POS_1.path());\n+        .isEqualTo(fileADeletes().path());\n   }\n \n   @TestTemplate\n@@ -349,7 +340,7 @@ public void testPartitionedTableWithPartitionEqDeletes() {\n   public void testPartitionedTableWithUnrelatedPartitionDeletes() {\n     table.newAppend().appendFile(FILE_B).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).addDeletes(FILE_A_EQ_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A_EQ_1).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\""Should have one task\"").hasSize(1);\n@@ -363,7 +354,9 @@ public void testPartitionedTableWithUnrelatedPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableWithOlderPartitionDeletes() {\n-    table.newRowDelta().addDeletes(FILE_A_POS_1).addDeletes(FILE_A_EQ_1).commit();\n+    assumeThat(formatVersion).as(\""DVs are not filtered using sequence numbers\"").isEqualTo(2);\n+\n+    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A_EQ_1).commit();\n \n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -379,6 +372,8 @@ public void testPartitionedTableWithOlderPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableScanWithGlobalDeletes() {\n+    assumeThat(formatVersion).as(\""Requires V2 position deletes\"").isEqualTo(2);\n+\n     table.newAppend().appendFile(FILE_A).commit();\n \n     TableMetadata base = table.ops().current();\n@@ -407,6 +402,8 @@ public void testPartitionedTableScanWithGlobalDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableScanWithGlobalAndPartitionDeletes() {\n+    assumeThat(formatVersion).as(\""Requires V2 position deletes\"").isEqualTo(2);\n+\n     table.newAppend().appendFile(FILE_A).commit();\n \n     table.newRowDelta().addDeletes(FILE_A_EQ_1).commit();\n@@ -437,7 +434,7 @@ public void testPartitionedTableScanWithGlobalAndPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableSequenceNumbers() {\n-    table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_EQ_1).addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_EQ_1).addDeletes(fileADeletes()).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\""Should have one task\"").hasSize(1);\n@@ -449,7 +446,7 @@ public void testPartitionedTableSequenceNumbers() {\n     assertThat(task.deletes()).as(\""Should have one associated delete file\"").hasSize(1);\n     assertThat(task.deletes().get(0).path())\n         .as(\""Should have only pos delete file\"")\n-        .isEqualTo(FILE_A_POS_1.path());\n+        .isEqualTo(fileADeletes().path());\n   }\n \n   @TestTemplate\n@@ -501,7 +498,7 @@ public void testPartitionedTableWithExistingDeleteFile() {\n \n     table.newRowDelta().addDeletes(FILE_A_EQ_1).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     table\n         .updateProperties()\n@@ -557,7 +554,7 @@ public void testPartitionedTableWithExistingDeleteFile() {\n     assertThat(task.deletes()).as(\""Should have two associated delete files\"").hasSize(2);\n     assertThat(Sets.newHashSet(Iterables.transform(task.deletes(), ContentFile::path)))\n         .as(\""Should have expected delete files\"")\n-        .isEqualTo(Sets.newHashSet(FILE_A_EQ_1.path(), FILE_A_POS_1.path()));\n+        .isEqualTo(Sets.newHashSet(FILE_A_EQ_1.path(), fileADeletes().path()));\n   }\n \n   @TestTemplate\n\ndiff --git a/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java b/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\nindex 13e96869b454..80551f0a2247 100644\n--- a/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\n@@ -186,7 +186,7 @@ public void scanningWithDeletes() throws IOException {\n             reporter);\n \n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_C).commit();\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(fileBDeletes()).commit();\n     ScanT tableScan = newScan(table);\n \n     try (CloseableIterable<T> fileScanTasks = tableScan.planFiles()) {\n@@ -208,12 +208,19 @@ public void scanningWithDeletes() throws IOException {\n     assertThat(result.totalDataManifests().value()).isEqualTo(1);\n     assertThat(result.totalDeleteManifests().value()).isEqualTo(1);\n     assertThat(result.totalFileSizeInBytes().value()).isEqualTo(30L);\n-    assertThat(result.totalDeleteFileSizeInBytes().value()).isEqualTo(20L);\n+    assertThat(result.totalDeleteFileSizeInBytes().value())\n+        .isEqualTo(contentSize(fileADeletes(), fileBDeletes()));\n     assertThat(result.skippedDataFiles().value()).isEqualTo(0);\n     assertThat(result.skippedDeleteFiles().value()).isEqualTo(0);\n     assertThat(result.indexedDeleteFiles().value()).isEqualTo(2);\n     assertThat(result.equalityDeleteFiles().value()).isEqualTo(0);\n-    assertThat(result.positionalDeleteFiles().value()).isEqualTo(2);\n+    if (formatVersion == 2) {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(2);\n+      assertThat(result.dvs().value()).isEqualTo(0);\n+    } else {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(0);\n+      assertThat(result.dvs().value()).isEqualTo(2);\n+    }\n   }\n \n   @TestTemplate\n@@ -264,8 +271,8 @@ public void scanningWithSkippedDeleteFiles() throws IOException {\n             tableDir, tableName, SCHEMA, SPEC, SortOrder.unsorted(), formatVersion, reporter);\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_D).commit();\n     table.newOverwrite().deleteFile(FILE_A).addFile(FILE_A2).commit();\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_D2_DELETES).commit();\n-    table.newRowDelta().addDeletes(FILE_B_DELETES).addDeletes(FILE_C2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_D2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileBDeletes()).addDeletes(FILE_C2_DELETES).commit();\n     ScanT tableScan = newScan(table);\n \n     List<FileScanTask> fileTasks = Lists.newArrayList();\n@@ -308,7 +315,7 @@ public void scanningWithEqualityAndPositionalDeleteFiles() throws IOException {\n             tableDir, tableName, SCHEMA, SPEC, SortOrder.unsorted(), formatVersion, reporter);\n     table.newAppend().appendFile(FILE_A).commit();\n     // FILE_A_DELETES = positionalDelete / FILE_A2_DELETES = equalityDelete\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n     ScanT tableScan = newScan(table);\n \n     try (CloseableIterable<T> fileScanTasks =\n@@ -321,7 +328,13 @@ public void scanningWithEqualityAndPositionalDeleteFiles() throws IOException {\n     ScanMetricsResult result = scanReport.scanMetrics();\n     assertThat(result.indexedDeleteFiles().value()).isEqualTo(2);\n     assertThat(result.equalityDeleteFiles().value()).isEqualTo(1);\n-    assertThat(result.positionalDeleteFiles().value()).isEqualTo(1);\n+    if (formatVersion == 2) {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(1);\n+      assertThat(result.dvs().value()).isEqualTo(0);\n+    } else {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(0);\n+      assertThat(result.dvs().value()).isEqualTo(1);\n+    }\n   }\n \n   static class TestMetricsReporter implements MetricsReporter {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex 9813d02910a6..46a1518e877f 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -45,6 +45,7 @@\n import org.apache.iceberg.relocated.com.google.common.io.Files;\n import org.apache.iceberg.types.Conversions;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ScanTaskUtil;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -85,6 +86,17 @@ public class TestBase {\n           .withPartitionPath(\""data_bucket=0\"") // easy way to set partition data for now\n           .withRecordCount(1)\n           .build();\n+  static final DeleteFile FILE_A_DV =\n+      FileMetadata.deleteFileBuilder(SPEC)\n+          .ofPositionDeletes()\n+          .withPath(\""/path/to/data-a-deletes.puffin\"")\n+          .withFileSizeInBytes(10)\n+          .withPartitionPath(\""data_bucket=0\"")\n+          .withRecordCount(1)\n+          .withReferencedDataFile(FILE_A.location())\n+          .withContentOffset(4)\n+          .withContentSizeInBytes(6)\n+          .build();\n   // Equality delete files.\n   static final DeleteFile FILE_A2_DELETES =\n       FileMetadata.deleteFileBuilder(SPEC)\n@@ -110,6 +122,17 @@ public class TestBase {\n           .withPartitionPath(\""data_bucket=1\"") // easy way to set partition data for now\n           .withRecordCount(1)\n           .build();\n+  static final DeleteFile FILE_B_DV =\n+      FileMetadata.deleteFileBuilder(SPEC)\n+          .ofPositionDeletes()\n+          .withPath(\""/path/to/data-b-deletes.puffin\"")\n+          .withFileSizeInBytes(10)\n+          .withPartitionPath(\""data_bucket=1\"")\n+          .withRecordCount(1)\n+          .withReferencedDataFile(FILE_B.location())\n+          .withContentOffset(4)\n+          .withContentSizeInBytes(6)\n+          .build();\n   static final DataFile FILE_C =\n       DataFiles.builder(SPEC)\n           .withPath(\""/path/to/data-c.parquet\"")\n@@ -643,6 +666,22 @@ protected DataFile newDataFile(String partitionPath) {\n         .build();\n   }\n \n+  protected DeleteFile fileADeletes() {\n+    return formatVersion >= 3 ? FILE_A_DV : FILE_A_DELETES;\n+  }\n+\n+  protected DeleteFile fileBDeletes() {\n+    return formatVersion >= 3 ? FILE_B_DV : FILE_B_DELETES;\n+  }\n+\n+  protected DeleteFile newDeletes(DataFile dataFile) {\n+    if (formatVersion >= 3) {\n+      return FileGenerationUtil.generateDV(table, dataFile);\n+    } else {\n+      return FileGenerationUtil.generatePositionDeleteFile(table, dataFile);\n+    }\n+  }\n+\n   protected DeleteFile newDeleteFile(int specId, String partitionPath) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n@@ -764,6 +803,14 @@ static Iterator<DataFile> files(ManifestFile manifest) {\n     return ManifestFiles.read(manifest, FILE_IO).iterator();\n   }\n \n+  static long recordCount(ContentFile<?>... files) {\n+    return Arrays.stream(files).mapToLong(ContentFile::recordCount).sum();\n+  }\n+\n+  static long contentSize(ContentFile<?>... files) {\n+    return ScanTaskUtil.contentSizeInBytes(Arrays.asList(files));\n+  }\n+\n   /** Used for assertions that only apply if the table version is v2. */\n   protected static class TableAssertions {\n     private boolean enabled;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBatchScans.java b/core/src/test/java/org/apache/iceberg/TestBatchScans.java\nindex 1597f44f6338..72cd00e0573d 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBatchScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBatchScans.java\n@@ -42,7 +42,7 @@ public void testDataTableScan() {\n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n     if (formatVersion > 1) {\n-      table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+      table.newRowDelta().addDeletes(fileADeletes()).commit();\n     }\n \n     BatchScan scan = table.newBatchScan();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestCommitReporting.java b/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\nindex 41b301668722..d333af98d623 100644\n--- a/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\n+++ b/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\n@@ -95,11 +95,13 @@ public void addAndDeleteDeleteFiles() {\n     // 2 positional + 1 equality\n     table\n         .newRowDelta()\n-        .addDeletes(FILE_A_DELETES)\n-        .addDeletes(FILE_B_DELETES)\n+        .addDeletes(fileADeletes())\n+        .addDeletes(fileBDeletes())\n         .addDeletes(FILE_C2_DELETES)\n         .commit();\n \n+    long totalDeleteContentSize = contentSize(fileADeletes(), fileBDeletes(), FILE_C2_DELETES);\n+\n     CommitReport report = reporter.lastCommitReport();\n     assertThat(report).isNotNull();\n     assertThat(report.operation()).isEqualTo(\""delete\"");\n@@ -110,7 +112,13 @@ public void addAndDeleteDeleteFiles() {\n     CommitMetricsResult metrics = report.commitMetrics();\n     assertThat(metrics.addedDeleteFiles().value()).isEqualTo(3L);\n     assertThat(metrics.totalDeleteFiles().value()).isEqualTo(3L);\n-    assertThat(metrics.addedPositionalDeleteFiles().value()).isEqualTo(2L);\n+    if (formatVersion == 2) {\n+      assertThat(metrics.addedPositionalDeleteFiles().value()).isEqualTo(2L);\n+      assertThat(metrics.addedDVs()).isNull();\n+    } else {\n+      assertThat(metrics.addedPositionalDeleteFiles()).isNull();\n+      assertThat(metrics.addedDVs().value()).isEqualTo(2L);\n+    }\n     assertThat(metrics.addedEqualityDeleteFiles().value()).isEqualTo(1L);\n \n     assertThat(metrics.addedPositionalDeletes().value()).isEqualTo(2L);\n@@ -119,15 +127,15 @@ public void addAndDeleteDeleteFiles() {\n     assertThat(metrics.addedEqualityDeletes().value()).isEqualTo(1L);\n     assertThat(metrics.totalEqualityDeletes().value()).isEqualTo(1L);\n \n-    assertThat(metrics.addedFilesSizeInBytes().value()).isEqualTo(30L);\n-    assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(30L);\n+    assertThat(metrics.addedFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n+    assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n \n     // now remove those 2 positional + 1 equality delete files\n     table\n         .newRewrite()\n         .rewriteFiles(\n             ImmutableSet.of(),\n-            ImmutableSet.of(FILE_A_DELETES, FILE_B_DELETES, FILE_C2_DELETES),\n+            ImmutableSet.of(fileADeletes(), fileBDeletes(), FILE_C2_DELETES),\n             ImmutableSet.of(),\n             ImmutableSet.of())\n         .commit();\n@@ -142,7 +150,13 @@ public void addAndDeleteDeleteFiles() {\n     metrics = report.commitMetrics();\n     assertThat(metrics.removedDeleteFiles().value()).isEqualTo(3L);\n     assertThat(metrics.totalDeleteFiles().value()).isEqualTo(0L);\n-    assertThat(metrics.removedPositionalDeleteFiles().value()).isEqualTo(2L);\n+    if (formatVersion == 2) {\n+      assertThat(metrics.removedPositionalDeleteFiles().value()).isEqualTo(2L);\n+      assertThat(metrics.removedDVs()).isNull();\n+    } else {\n+      assertThat(metrics.removedPositionalDeleteFiles()).isNull();\n+      assertThat(metrics.removedDVs().value()).isEqualTo(2L);\n+    }\n     assertThat(metrics.removedEqualityDeleteFiles().value()).isEqualTo(1L);\n \n     assertThat(metrics.removedPositionalDeletes().value()).isEqualTo(2L);\n@@ -151,7 +165,7 @@ public void addAndDeleteDeleteFiles() {\n     assertThat(metrics.removedEqualityDeletes().value()).isEqualTo(1L);\n     assertThat(metrics.totalEqualityDeletes().value()).isEqualTo(0L);\n \n-    assertThat(metrics.removedFilesSizeInBytes().value()).isEqualTo(30L);\n+    assertThat(metrics.removedFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n     assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(0L);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\nindex 9bce4e60a4f3..e061567e72a8 100644\n--- a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\n+++ b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\n@@ -131,7 +131,7 @@ public void testEntriesTableWithDeleteManifests() {\n     assumeThat(formatVersion).as(\""Only V2 Tables Support Deletes\"").isGreaterThanOrEqualTo(2);\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     Table entriesTable = new ManifestEntriesTable(table);\n     TableScan scan = entriesTable.newScan();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\nindex 30fdae01cd94..f811dac02043 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n@@ -58,14 +58,14 @@ private void preparePartitionedTable(boolean transactional) {\n       if (transactional) {\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n-            .addDeletes(FILE_B_DELETES)\n+            .addDeletes(fileADeletes())\n+            .addDeletes(fileBDeletes())\n             .addDeletes(FILE_C2_DELETES)\n             .addDeletes(FILE_D2_DELETES)\n             .commit();\n       } else {\n-        table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n-        table.newRowDelta().addDeletes(FILE_B_DELETES).commit();\n+        table.newRowDelta().addDeletes(fileADeletes()).commit();\n+        table.newRowDelta().addDeletes(fileBDeletes()).commit();\n         table.newRowDelta().addDeletes(FILE_C2_DELETES).commit();\n         table.newRowDelta().addDeletes(FILE_D2_DELETES).commit();\n       }\n@@ -721,7 +721,7 @@ public void testDeleteFilesTableSelection() throws IOException {\n     assumeThat(formatVersion).as(\""Position deletes are not supported by V1 Tables\"").isNotEqualTo(1);\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     Table deleteFilesTable = new DeleteFilesTable(table);\n \n@@ -1409,10 +1409,10 @@ public void testPositionDeletesWithFilter() {\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 0);\n     assertThat(posDeleteTask.file().path())\n         .as(\""Expected correct delete file on task\"")\n-        .isEqualTo(FILE_B_DELETES.path());\n+        .isEqualTo(fileBDeletes().path());\n     assertThat((Map<Integer, String>) constantsMap(posDeleteTask, partitionType))\n         .as(\""Expected correct delete file on constant column\"")\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), FILE_B_DELETES.path().toString());\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), fileBDeletes().path().toString());\n   }\n \n   @TestTemplate\n@@ -1479,17 +1479,16 @@ private void testPositionDeletesBaseTableFilter(boolean transactional) {\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 0);\n     assertThat(posDeleteTask.file().path())\n         .as(\""Expected correct delete file on task\"")\n-        .isEqualTo(FILE_A_DELETES.path());\n+        .isEqualTo(fileADeletes().path());\n     assertThat((Map<Integer, String>) constantsMap(posDeleteTask, partitionType))\n         .as(\""Expected correct delete file on constant column\"")\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), FILE_A_DELETES.path().toString());\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), fileADeletes().path().toString());\n   }\n \n   @TestTemplate\n   public void testPositionDeletesWithBaseTableFilterNot() {\n-    assumeThat(formatVersion)\n-        .as(\""Position deletes are not supported by V1 Tables\"")\n-        .isNotEqualTo(1); // use identity rather than bucket partition spec,\n+    assumeThat(formatVersion).as(\""Position deletes are not supported by V1 Tables\"").isEqualTo(2);\n+    // use identity rather than bucket partition spec,\n     // as bucket.project does not support projecting notEq\n     table.updateSpec().removeField(\""data_bucket\"").addField(\""id\"").commit();\n     PartitionSpec spec = table.spec();\n@@ -1619,20 +1618,8 @@ public void testPositionDeletesUnpartitioned() {\n             .build();\n     table.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n \n-    DeleteFile delete1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/delete1.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/delete2.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete1 = newDeletes(dataFile1);\n+    DeleteFile delete2 = newDeletes(dataFile2);\n     table.newRowDelta().addDeletes(delete1).addDeletes(delete2).commit();\n \n     PositionDeletesTable positionDeletesTable = new PositionDeletesTable(table);\n@@ -1655,16 +1642,16 @@ public void testPositionDeletesUnpartitioned() {\n         .isEqualTo(1);\n \n     assertThat(scanTasks).hasSize(2);\n-    scanTasks.sort(Comparator.comparing(f -> f.file().path().toString()));\n-    assertThat(scanTasks.get(0).file().path().toString()).isEqualTo(\""/path/to/delete1.parquet\"");\n-    assertThat(scanTasks.get(1).file().path().toString()).isEqualTo(\""/path/to/delete2.parquet\"");\n+    scanTasks.sort(Comparator.comparing(f -> f.file().pos()));\n+    assertThat(scanTasks.get(0).file().location()).isEqualTo(delete1.location());\n+    assertThat(scanTasks.get(1).file().location()).isEqualTo(delete2.location());\n \n     Types.StructType partitionType = Partitioning.partitionType(table);\n \n     assertThat((Map<Integer, String>) constantsMap(scanTasks.get(0), partitionType))\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), \""/path/to/delete1.parquet\"");\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), delete1.location());\n     assertThat((Map<Integer, String>) constantsMap(scanTasks.get(1), partitionType))\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), \""/path/to/delete2.parquet\"");\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), delete2.location());\n     assertThat((Map<Integer, Integer>) constantsMap(scanTasks.get(0), partitionType))\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 1);\n     assertThat((Map<Integer, Integer>) constantsMap(scanTasks.get(1), partitionType))\n@@ -1712,20 +1699,8 @@ public void testPositionDeletesManyColumns() {\n             .build();\n     table.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n \n-    DeleteFile delete1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/delete1.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/delete2.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete1 = newDeletes(dataFile1);\n+    DeleteFile delete2 = newDeletes(dataFile2);\n     table.newRowDelta().addDeletes(delete1).addDeletes(delete2).commit();\n \n     PositionDeletesTable positionDeletesTable = new PositionDeletesTable(table);\n@@ -1745,9 +1720,9 @@ public void testPositionDeletesManyColumns() {\n                   return (PositionDeletesScanTask) task;\n                 }));\n     assertThat(scanTasks).hasSize(2);\n-    scanTasks.sort(Comparator.comparing(f -> f.file().path().toString()));\n-    assertThat(scanTasks.get(0).file().path().toString()).isEqualTo(\""/path/to/delete1.parquet\"");\n-    assertThat(scanTasks.get(1).file().path().toString()).isEqualTo(\""/path/to/delete2.parquet\"");\n+    scanTasks.sort(Comparator.comparing(f -> f.file().pos()));\n+    assertThat(scanTasks.get(0).file().location()).isEqualTo(delete1.location());\n+    assertThat(scanTasks.get(1).file().location()).isEqualTo(delete2.location());\n   }\n \n   @TestTemplate\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\nindex 2de38541777b..84860d34bb31 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n@@ -159,7 +159,7 @@ public void testPartitionsTableScanWithAddPartitionOnNestedField() {\n \n   @TestTemplate\n   public void testPositionDeletesPartitionSpecRemoval() {\n-    assumeThat(formatVersion).as(\""Position deletes are not supported by V1 Tables\"").isNotEqualTo(1);\n+    assumeThat(formatVersion).as(\""Position deletes are not supported by V1 Tables\"").isEqualTo(2);\n     table.updateSpec().removeField(\""id\"").commit();\n \n     DeleteFile deleteFile = newDeleteFile(table.ops().current().spec().specId(), \""nested.id=1\"");\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\nindex 124cc2f28dd5..5b108e9ee565 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\n@@ -55,6 +55,7 @@ protected static List<Object> parameters() {\n \n   @TestTemplate\n   public void testEmptyTable() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     TableMetadata base = readMetadata();\n@@ -87,6 +88,7 @@ public void testEmptyTable() {\n \n   @TestTemplate\n   public void testAddOnly() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     assertThatThrownBy(\n@@ -130,6 +132,7 @@ public void testAddOnly() {\n \n   @TestTemplate\n   public void testDeleteOnly() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     assertThatThrownBy(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\nindex f1d23de32a42..72bb85c0446e 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\n@@ -1096,7 +1096,7 @@ public void testRewriteDataManifestsPreservesDeletes() {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1139,7 +1139,7 @@ public void testRewriteDataManifestsPreservesDeletes() {\n         dataSeqs(deleteSnapshotSeq, deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq, deleteSnapshotSeq),\n         ids(deleteSnapshotId, deleteSnapshotId),\n-        files(FILE_A_DELETES, FILE_A2_DELETES),\n+        files(fileADeletes(), FILE_A2_DELETES),\n         statuses(ManifestEntry.Status.ADDED, ManifestEntry.Status.ADDED));\n   }\n \n@@ -1158,7 +1158,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1179,7 +1179,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \""delete-manifest-file-2.avro\"",\n@@ -1218,7 +1218,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1244,7 +1244,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1287,7 +1287,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \""delete-manifest-file-2.avro\"",\n@@ -1337,7 +1337,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1361,7 +1361,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1379,7 +1379,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \""delete-manifest-file-2.avro\"",\n@@ -1440,7 +1440,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1464,7 +1464,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit the first set of delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the first delete snapshot info\n     Snapshot deleteSnapshot1 = table.currentSnapshot();\n@@ -1472,7 +1472,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     long deleteSnapshotSeq1 = deleteSnapshot1.sequenceNumber();\n \n     // commit the second set of delete files\n-    table.newRowDelta().addDeletes(FILE_B_DELETES).addDeletes(FILE_C2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileBDeletes()).addDeletes(FILE_C2_DELETES).commit();\n \n     // save the second delete snapshot info\n     Snapshot deleteSnapshot2 = table.currentSnapshot();\n@@ -1489,7 +1489,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n                 deleteSnapshotId1,\n                 deleteSnapshotSeq1,\n                 deleteSnapshotSeq1,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \""delete-manifest-file-2.avro\"",\n@@ -1507,7 +1507,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     rewriteManifests.addManifest(newDeleteManifest2);\n \n     // commit the third set of delete files concurrently\n-    table.newRewrite().deleteFile(FILE_B_DELETES).commit();\n+    table.newRewrite().deleteFile(fileBDeletes()).commit();\n \n     Snapshot concurrentSnapshot = table.currentSnapshot();\n     long concurrentSnapshotId = concurrentSnapshot.snapshotId();\n@@ -1541,7 +1541,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n         dataSeqs(deleteSnapshotSeq1),\n         fileSeqs(deleteSnapshotSeq1),\n         ids(deleteSnapshotId1),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1555,7 +1555,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n         dataSeqs(deleteSnapshotSeq2, deleteSnapshotSeq2),\n         fileSeqs(deleteSnapshotSeq2, deleteSnapshotSeq2),\n         ids(concurrentSnapshotId, deleteSnapshotId2),\n-        files(FILE_B_DELETES, FILE_C2_DELETES),\n+        files(fileBDeletes(), FILE_C2_DELETES),\n         statuses(ManifestEntry.Status.DELETED, ManifestEntry.Status.EXISTING));\n   }\n \n@@ -1567,7 +1567,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_C).commit();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1584,7 +1584,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \""delete-manifest-file-2.avro\"",\n@@ -1602,7 +1602,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n     rewriteManifests.addManifest(newDeleteManifest2);\n \n     // modify the original delete manifest concurrently\n-    table.newRewrite().deleteFile(FILE_A_DELETES).commit();\n+    table.newRewrite().deleteFile(fileADeletes()).commit();\n \n     // the rewrite must fail as the original delete manifest was replaced concurrently\n     assertThatThrownBy(rewriteManifests::commit)\n@@ -1621,7 +1621,7 @@ public void testDeleteManifestReplacementFailure() throws IOException {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n     // commit the first delete file\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     // save the first delete snapshot info\n     Snapshot deleteSnapshot1 = table.currentSnapshot();\n@@ -1648,7 +1648,7 @@ public void testDeleteManifestReplacementFailure() throws IOException {\n                 deleteSnapshotId1,\n                 deleteSnapshotSeq1,\n                 deleteSnapshotSeq1,\n-                FILE_A_DELETES),\n+                fileADeletes()),\n             manifestEntry(\n                 ManifestEntry.Status.EXISTING,\n                 deleteSnapshotId2,\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRowDelta.java b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\nindex 1d67e48a2ce2..b41be0c7a636 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n@@ -29,7 +29,9 @@\n import static org.apache.iceberg.util.SnapshotUtil.latestSnapshot;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n+import java.io.IOException;\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n@@ -38,8 +40,11 @@\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -52,13 +57,17 @@ public class TestRowDelta extends V2TableTestBase {\n \n   @Parameters(name = \""formatVersion = {0}, branch = {1}\"")\n   protected static List<Object> parameters() {\n-    return Arrays.asList(new Object[] {2, \""main\""}, new Object[] {2, \""testBranch\""});\n+    return Arrays.asList(\n+        new Object[] {2, \""main\""},\n+        new Object[] {2, \""testBranch\""},\n+        new Object[] {3, \""main\""},\n+        new Object[] {3, \""testBranch\""});\n   }\n \n   @TestTemplate\n   public void addOnlyDeleteFilesProducesDeleteOperation() {\n     SnapshotUpdate<?> rowDelta =\n-        table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES);\n+        table.newRowDelta().addDeletes(fileADeletes()).addDeletes(fileBDeletes());\n \n     commit(table, rowDelta, branch);\n     Snapshot snap = latestSnapshot(table, branch);\n@@ -70,7 +79,7 @@ public void addOnlyDeleteFilesProducesDeleteOperation() {\n   @TestTemplate\n   public void testAddDeleteFile() {\n     SnapshotUpdate<?> rowDelta =\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES);\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes());\n \n     commit(table, rowDelta, branch);\n     Snapshot snap = latestSnapshot(table, branch);\n@@ -95,7 +104,7 @@ public void testAddDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), snap.snapshotId()),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.ADDED, Status.ADDED));\n   }\n \n@@ -126,7 +135,7 @@ public void testValidateDataFilesExistDefaults() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -143,7 +152,7 @@ public void testValidateDataFilesExistDefaults() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_B_DELETES)\n+            .addDeletes(fileBDeletes())\n             .validateDataFilesExist(ImmutableList.of(FILE_B.path()))\n             .validateFromSnapshot(validateFromSnapshotId),\n         branch);\n@@ -155,7 +164,7 @@ public void testValidateDataFilesExistDefaults() {\n         dataSeqs(4L),\n         fileSeqs(4L),\n         ids(latestSnapshot(table, branch).snapshotId()),\n-        files(FILE_B_DELETES),\n+        files(fileBDeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -177,7 +186,7 @@ public void testValidateDataFilesExistOverwrite() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -209,7 +218,7 @@ public void testValidateDataFilesExistReplacePartitions() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -242,7 +251,7 @@ public void testValidateDataFilesExistFromSnapshot() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(validateFromSnapshotId)\n             .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n         branch);\n@@ -276,7 +285,7 @@ public void testValidateDataFilesExistFromSnapshot() {\n         dataSeqs(3L),\n         fileSeqs(3L),\n         ids(snap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -301,7 +310,7 @@ public void testValidateDataFilesExistRewrite() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -333,7 +342,7 @@ public void testValidateDataFilesExistValidateDeletes() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateDeletedFiles()\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n@@ -366,7 +375,7 @@ public void testValidateNoConflicts() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .conflictDetectionFilter(\n                             Expressions.equal(\""data\"", \""u\"")) // bucket16(\""u\"") -> 0\n@@ -399,7 +408,7 @@ public void testValidateNoConflictsFromSnapshot() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateDeletedFiles()\n             .validateFromSnapshot(validateFromSnapshotId)\n             .validateDataFilesExist(ImmutableList.of(FILE_A.path()))\n@@ -436,7 +445,7 @@ public void testValidateNoConflictsFromSnapshot() {\n         dataSeqs(3L),\n         fileSeqs(3L),\n         ids(snap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -444,7 +453,7 @@ public void testValidateNoConflictsFromSnapshot() {\n   public void testOverwriteWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -479,7 +488,7 @@ public void testOverwriteWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), deltaSnapshotId),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.EXISTING));\n   }\n \n@@ -487,7 +496,7 @@ public void testOverwriteWithDeleteFile() {\n   public void testReplacePartitionsWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -526,7 +535,7 @@ public void testReplacePartitionsWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), deltaSnapshotId),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.EXISTING));\n   }\n \n@@ -534,7 +543,7 @@ public void testReplacePartitionsWithDeleteFile() {\n   public void testDeleteByExpressionWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -564,13 +573,13 @@ public void testDeleteByExpressionWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), snap.snapshotId()),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.DELETED));\n   }\n \n   @TestTemplate\n   public void testDeleteDataFileWithDeleteFile() {\n-    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()), branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n     assertThat(latestSnapshot(table, branch).sequenceNumber()).isEqualTo(1);\n@@ -598,7 +607,7 @@ public void testDeleteDataFileWithDeleteFile() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n \n     // the manifest that removed FILE_A will be dropped next commit, causing the min sequence number\n@@ -619,13 +628,13 @@ public void testDeleteDataFileWithDeleteFile() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(nextSnap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.DELETED));\n   }\n \n   @TestTemplate\n   public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n-    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()), branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n     assertThat(latestSnapshot(table, branch).sequenceNumber()).isEqualTo(1);\n@@ -653,7 +662,7 @@ public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n \n     // the manifest that removed FILE_A will be dropped next merging commit, but FastAppend will not\n@@ -689,7 +698,7 @@ public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -728,14 +737,7 @@ public void testValidateDataFilesExistWithConflictDetectionFilter() {\n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/data-a-deletes.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\""data=a\"")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile = newDeletes(dataFile1);\n \n     Expression conflictDetectionFilter = Expressions.equal(\""data\"", \""a\"");\n     RowDelta rowDelta =\n@@ -789,14 +791,7 @@ public void testValidateDataFilesDoNotExistWithConflictDetectionFilter() {\n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/data-a-deletes.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\""data=a\"")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile = newDeletes(dataFile1);\n \n     Expression conflictDetectionFilter = Expressions.equal(\""data\"", \""a\"");\n     RowDelta rowDelta =\n@@ -847,9 +842,9 @@ public void testAddDeleteFilesMultipleSpecs() {\n     // commit a row delta with 1 data file and 3 delete files where delete files have different\n     // specs\n     DataFile dataFile = newDataFile(\""data=xyz\"");\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \""data_bucket=0\"");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \""\"");\n-    DeleteFile thirdDeleteFile = newDeleteFile(thirdSnapshotDataFile.specId(), \""data=abc\"");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n+    DeleteFile thirdDeleteFile = newDeletes(thirdSnapshotDataFile);\n \n     commit(\n         table,\n@@ -867,6 +862,7 @@ public void testAddDeleteFilesMultipleSpecs() {\n     assertThat(snapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     Map<String, String> summary = snapshot.summary();\n+    long posDeletesCount = recordCount(firstDeleteFile, secondDeleteFile, thirdDeleteFile);\n \n     assertThat(summary)\n         .containsEntry(CHANGED_PARTITION_COUNT_PROP, \""4\"")\n@@ -874,8 +870,8 @@ public void testAddDeleteFilesMultipleSpecs() {\n         .containsEntry(TOTAL_DATA_FILES_PROP, \""4\"")\n         .containsEntry(ADDED_DELETE_FILES_PROP, \""3\"")\n         .containsEntry(TOTAL_DELETE_FILES_PROP, \""3\"")\n-        .containsEntry(ADDED_POS_DELETES_PROP, \""3\"")\n-        .containsEntry(TOTAL_POS_DELETES_PROP, \""3\"")\n+        .containsEntry(ADDED_POS_DELETES_PROP, String.valueOf(posDeletesCount))\n+        .containsEntry(TOTAL_POS_DELETES_PROP, String.valueOf(posDeletesCount))\n         .hasEntrySatisfying(\n             CHANGED_PARTITION_PREFIX + \""data_bucket=0\"",\n             v -> assertThat(v).contains(ADDED_DELETE_FILES_PROP + \""=1\""))\n@@ -953,8 +949,8 @@ public void testManifestMergingMultipleSpecs() {\n     commit(table, table.newAppend().appendFile(secondSnapshotDataFile), branch);\n \n     // commit two delete files to two specs in a single operation\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \""data_bucket=0\"");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \""\"");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     commit(\n         table,\n@@ -968,12 +964,18 @@ public void testManifestMergingMultipleSpecs() {\n     assertThat(thirdSnapshot.deleteManifests(table.io())).hasSize(2);\n \n     // commit two more delete files to the same specs to trigger merging\n-    DeleteFile thirdDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \""data_bucket=0\"");\n-    DeleteFile fourthDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \""\"");\n+    DeleteFile thirdDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile fourthDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     commit(\n         table,\n-        table.newRowDelta().addDeletes(thirdDeleteFile).addDeletes(fourthDeleteFile),\n+        table\n+            .newRowDelta()\n+            .removeDeletes(firstDeleteFile)\n+            .addDeletes(thirdDeleteFile)\n+            .removeDeletes(secondDeleteFile)\n+            .addDeletes(fourthDeleteFile)\n+            .validateFromSnapshot(thirdSnapshot.snapshotId()),\n         branch);\n \n     Snapshot fourthSnapshot = latestSnapshot(table, branch);\n@@ -988,9 +990,9 @@ public void testManifestMergingMultipleSpecs() {\n         firstDeleteManifest,\n         dataSeqs(4L, 3L),\n         fileSeqs(4L, 3L),\n-        ids(fourthSnapshot.snapshotId(), thirdSnapshot.snapshotId()),\n+        ids(fourthSnapshot.snapshotId(), fourthSnapshot.snapshotId()),\n         files(thirdDeleteFile, firstDeleteFile),\n-        statuses(Status.ADDED, Status.EXISTING));\n+        statuses(Status.ADDED, Status.DELETED));\n \n     ManifestFile secondDeleteManifest = fourthSnapshot.deleteManifests(table.io()).get(0);\n     assertThat(secondDeleteManifest.partitionSpecId()).isEqualTo(secondSnapshotDataFile.specId());\n@@ -998,9 +1000,9 @@ public void testManifestMergingMultipleSpecs() {\n         secondDeleteManifest,\n         dataSeqs(4L, 3L),\n         fileSeqs(4L, 3L),\n-        ids(fourthSnapshot.snapshotId(), thirdSnapshot.snapshotId()),\n+        ids(fourthSnapshot.snapshotId(), fourthSnapshot.snapshotId()),\n         files(fourthDeleteFile, secondDeleteFile),\n-        statuses(Status.ADDED, Status.EXISTING));\n+        statuses(Status.ADDED, Status.DELETED));\n   }\n \n   @TestTemplate\n@@ -1019,8 +1021,8 @@ public void testAbortMultipleSpecs() {\n     commit(table, table.newAppend().appendFile(secondSnapshotDataFile), branch);\n \n     // prepare two delete files that belong to different specs\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \""data_bucket=0\"");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \""\"");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     // capture all deletes\n     Set<String> deletedFiles = Sets.newHashSet();\n@@ -1062,7 +1064,7 @@ public void testConcurrentConflictingRowDelta() {\n             .newRowDelta()\n             .toBranch(branch)\n             .addRows(FILE_B)\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(firstSnapshot.snapshotId())\n             .conflictDetectionFilter(conflictDetectionFilter)\n             .validateNoConflictingDataFiles()\n@@ -1071,7 +1073,7 @@ public void testConcurrentConflictingRowDelta() {\n     table\n         .newRowDelta()\n         .toBranch(branch)\n-        .addDeletes(FILE_A_DELETES)\n+        .addDeletes(fileADeletes())\n         .validateFromSnapshot(firstSnapshot.snapshotId())\n         .conflictDetectionFilter(conflictDetectionFilter)\n         .validateNoConflictingDataFiles()\n@@ -1094,7 +1096,7 @@ public void testConcurrentConflictingRowDeltaWithoutAppendValidation() {\n     RowDelta rowDelta =\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(firstSnapshot.snapshotId())\n             .conflictDetectionFilter(conflictDetectionFilter)\n             .validateNoConflictingDeleteFiles();\n@@ -1102,7 +1104,7 @@ public void testConcurrentConflictingRowDeltaWithoutAppendValidation() {\n     table\n         .newRowDelta()\n         .toBranch(branch)\n-        .addDeletes(FILE_A_DELETES)\n+        .addDeletes(fileADeletes())\n         .validateFromSnapshot(firstSnapshot.snapshotId())\n         .conflictDetectionFilter(conflictDetectionFilter)\n         .validateNoConflictingDataFiles()\n@@ -1149,14 +1151,7 @@ public void testConcurrentNonConflictingRowDelta() {\n     Expression conflictDetectionFilter = Expressions.equal(\""data\"", \""a\"");\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/data-a-deletes.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\""data=a\"")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile1 = newDeletes(dataFile1);\n \n     // mock a DELETE operation with serializable isolation\n     RowDelta rowDelta =\n@@ -1170,14 +1165,7 @@ public void testConcurrentNonConflictingRowDelta() {\n             .validateNoConflictingDeleteFiles();\n \n     // add a delete file for partition B\n-    DeleteFile deleteFile2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\""/path/to/data-b-deletes.parquet\"")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\""data=b\"")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile2 = newDeletes(dataFile2);\n \n     table\n         .newRowDelta()\n@@ -1320,8 +1308,8 @@ public void testConcurrentConflictingRowDeltaAndRewriteFilesWithSequenceNumber()\n \n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n-    // add an position delete file\n-    DeleteFile deleteFile1 = newDeleteFile(table.spec().specId(), \""data=a\"");\n+    // add position deletes\n+    DeleteFile deleteFile1 = newDeletes(dataFile1);\n \n     // mock a DELETE operation with serializable isolation\n     RowDelta rowDelta =\n@@ -1357,7 +1345,7 @@ public void testRowDeltaCaseSensitivity() {\n \n     Snapshot firstSnapshot = latestSnapshot(table, branch);\n \n-    commit(table, table.newRowDelta().addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addDeletes(fileADeletes()), branch);\n \n     Expression conflictDetectionFilter = Expressions.equal(Expressions.bucket(\""dAtA\"", 16), 0);\n \n@@ -1413,12 +1401,12 @@ public void testRowDeltaCaseSensitivity() {\n   @TestTemplate\n   public void testRewrittenDeleteFiles() {\n     DataFile dataFile = newDataFile(\""data_bucket=0\"");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n-    DeleteFile newDeleteFile = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile newDeleteFile = newDeletes(dataFile);\n     RowDelta rowDelta =\n         table\n             .newRowDelta()\n@@ -1458,14 +1446,16 @@ public void testRewrittenDeleteFiles() {\n \n   @TestTemplate\n   public void testConcurrentDeletesRewriteSameDeleteFile() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n     DataFile dataFile = newDataFile(\""data_bucket=0\"");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     // commit the first DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile1 = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile newDeleteFile1 = newDeletes(dataFile);\n     RowDelta delete1 =\n         table\n             .newRowDelta()\n@@ -1478,7 +1468,7 @@ public void testConcurrentDeletesRewriteSameDeleteFile() {\n     assertThat(snapshot1.sequenceNumber()).isEqualTo(2L);\n \n     // commit the second DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile2 = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile newDeleteFile2 = newDeletes(dataFile);\n     RowDelta delete2 =\n         table\n             .newRowDelta()\n@@ -1522,13 +1512,13 @@ public void testConcurrentDeletesRewriteSameDeleteFile() {\n   @TestTemplate\n   public void testConcurrentMergeRewriteSameDeleteFile() {\n     DataFile dataFile = newDataFile(\""data_bucket=0\"");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     // commit a DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile1 = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile newDeleteFile1 = newDeletes(dataFile);\n     RowDelta delete =\n         table\n             .newRowDelta()\n@@ -1540,7 +1530,7 @@ public void testConcurrentMergeRewriteSameDeleteFile() {\n \n     // attempt to commit a MERGE operation that replaces `deleteFile`\n     DataFile newDataFile2 = newDataFile(\""data_bucket=0\"");\n-    DeleteFile newDeleteFile2 = newDeleteFile(dataFile.specId(), \""data_bucket=0\"");\n+    DeleteFile newDeleteFile2 = newDeletes(dataFile);\n     RowDelta merge =\n         table\n             .newRowDelta()\n@@ -1556,4 +1546,102 @@ public void testConcurrentMergeRewriteSameDeleteFile() {\n         .isInstanceOf(ValidationException.class)\n         .hasMessageStartingWith(\""Found new conflicting delete files that can apply\"");\n   }\n+\n+  @TestTemplate\n+  public void testConcurrentDVsForSameDataFile() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    DataFile dataFile = newDataFile(\""data_bucket=0\"");\n+    commit(table, table.newRowDelta().addRows(dataFile), branch);\n+\n+    DeleteFile deleteFile1 = newDeletes(dataFile);\n+    RowDelta rowDelta1 = table.newRowDelta().addDeletes(deleteFile1);\n+\n+    DeleteFile deleteFile2 = newDeletes(dataFile);\n+    RowDelta rowDelta2 = table.newRowDelta().addDeletes(deleteFile2);\n+\n+    commit(table, rowDelta1, branch);\n+\n+    assertThatThrownBy(() -> commit(table, rowDelta2, branch))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\""Found concurrently added DV for %s\"", dataFile.location());\n+  }\n+\n+  @TestTemplate\n+  public void testManifestMergingAfterUpgradeToV3() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    // enable manifest merging\n+    table\n+        .updateProperties()\n+        .set(TableProperties.MANIFEST_MERGE_ENABLED, \""true\"")\n+        .set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \""2\"")\n+        .commit();\n+\n+    // add a data file\n+    DataFile dataFile = newDataFile(\""data_bucket=0\"");\n+    commit(table, table.newAppend().appendFile(dataFile), branch);\n+\n+    // commit a delete operation using a positional delete file\n+    DeleteFile deleteFile = newDeleteFileWithRef(dataFile);\n+    assertThat(deleteFile.format()).isEqualTo(FileFormat.PARQUET);\n+    RowDelta rowDelta1 = table.newRowDelta().addDeletes(deleteFile);\n+    Snapshot deleteFileSnapshot = commit(table, rowDelta1, branch);\n+\n+    // upgrade the table\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \""3\"").commit();\n+\n+    // commit a DV\n+    DeleteFile dv = newDV(dataFile);\n+    assertThat(dv.format()).isEqualTo(FileFormat.PUFFIN);\n+    RowDelta rowDelta2 = table.newRowDelta().addDeletes(dv);\n+    Snapshot dvSnapshot = commit(table, rowDelta2, branch);\n+\n+    // both must be part of the table and merged into one manifest\n+    ManifestFile deleteManifest = Iterables.getOnlyElement(dvSnapshot.deleteManifests(table.io()));\n+    validateDeleteManifest(\n+        deleteManifest,\n+        dataSeqs(3L, 2L),\n+        fileSeqs(3L, 2L),\n+        ids(dvSnapshot.snapshotId(), deleteFileSnapshot.snapshotId()),\n+        files(dv, deleteFile),\n+        statuses(Status.ADDED, Status.EXISTING));\n+\n+    // only the DV must be assigned during planning\n+    List<ScanTask> tasks = planFiles();\n+    FileScanTask task = Iterables.getOnlyElement(tasks).asFileScanTask();\n+    assertThat(task.deletes()).hasSize(1);\n+    DeleteFile taskDV = Iterables.getOnlyElement(task.deletes());\n+    assertThat(taskDV.location()).isEqualTo(dv.location());\n+    assertThat(taskDV.referencedDataFile()).isEqualTo(dv.referencedDataFile());\n+    assertThat(taskDV.contentOffset()).isEqualTo(dv.contentOffset());\n+    assertThat(taskDV.contentSizeInBytes()).isEqualTo(dv.contentSizeInBytes());\n+  }\n+\n+  @TestTemplate\n+  public void testInabilityToAddPositionDeleteFilesInTablesWithDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+    DeleteFile deleteFile = newDeleteFile(table.spec().specId(), \""data_bucket=0\"");\n+    assertThatThrownBy(() -> table.newRowDelta().addDeletes(deleteFile))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\""Must use DVs for position deletes in V%s\"", formatVersion);\n+  }\n+\n+  @TestTemplate\n+  public void testInabilityToAddDVToV2Tables() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+    DataFile dataFile = newDataFile(\""data_bucket=0\"");\n+    DeleteFile dv = newDV(dataFile);\n+    assertThatThrownBy(() -> table.newRowDelta().addDeletes(dv))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\""Must not use DVs for position deletes in V2\"");\n+  }\n+\n+  private List<ScanTask> planFiles() {\n+    try (CloseableIterable<ScanTask> tasks = table.newBatchScan().useRef(branch).planFiles()) {\n+      return Lists.newArrayList(tasks);\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSnapshot.java b/core/src/test/java/org/apache/iceberg/TestSnapshot.java\nindex 8a30036f3242..bbe5e8f6cdd8 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshot.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshot.java\n@@ -123,7 +123,7 @@ public void testCachedDeleteFiles() {\n     int specId = table.spec().specId();\n \n     DataFile secondSnapshotDataFile = newDataFile(\""data_bucket=8/data_trunc_2=aa\"");\n-    DeleteFile secondSnapshotDeleteFile = newDeleteFile(specId, \""data_bucket=8/data_trunc_2=aa\"");\n+    DeleteFile secondSnapshotDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     table\n         .newRowDelta()\n@@ -131,7 +131,7 @@ public void testCachedDeleteFiles() {\n         .addDeletes(secondSnapshotDeleteFile)\n         .commit();\n \n-    DeleteFile thirdSnapshotDeleteFile = newDeleteFile(specId, \""data_bucket=8/data_trunc_2=aa\"");\n+    DeleteFile thirdSnapshotDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     ImmutableSet<DeleteFile> replacedDeleteFiles = ImmutableSet.of(secondSnapshotDeleteFile);\n     ImmutableSet<DeleteFile> newDeleteFiles = ImmutableSet.of(thirdSnapshotDeleteFile);\n@@ -248,11 +248,9 @@ public void testSequenceNumbersInAddedDeleteFiles() {\n \n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n-    int specId = table.spec().specId();\n-\n-    runAddedDeleteFileSequenceNumberTest(newDeleteFile(specId, \""data_bucket=8\""), 2);\n+    runAddedDeleteFileSequenceNumberTest(newDeletes(FILE_A), 2);\n \n-    runAddedDeleteFileSequenceNumberTest(newDeleteFile(specId, \""data_bucket=28\""), 3);\n+    runAddedDeleteFileSequenceNumberTest(newDeletes(FILE_B), 3);\n   }\n \n   private void runAddedDeleteFileSequenceNumberTest(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\nindex b0b9d003e35b..9c67e766a993 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n@@ -78,7 +78,7 @@ public void testFileSizeSummary() {\n \n   @TestTemplate\n   public void testFileSizeSummaryWithDeletes() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n \n@@ -260,7 +260,7 @@ public void rowDeltaWithDuplicates() {\n \n   @TestTemplate\n   public void rowDeltaWithDeletesAndDuplicates() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     table\n@@ -325,7 +325,7 @@ public void rewriteWithDuplicateFiles() {\n \n   @TestTemplate\n   public void rewriteWithDeletesAndDuplicates() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     table.newRowDelta().addRows(FILE_A2).addDeletes(FILE_A_DELETES).commit();\n\ndiff --git a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\nindex 23e0090ca49f..4e50ee57db41 100644\n--- a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n+++ b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n@@ -34,6 +34,7 @@\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.data.BaseDeleteLoader;\n@@ -295,9 +296,13 @@ public void testApplyPartitionScopedPositionDeletes() throws IOException {\n   }\n \n   private void commit(DeleteWriteResult result) {\n+    Snapshot startSnapshot = table.currentSnapshot();\n     RowDelta rowDelta = table.newRowDelta();\n     result.rewrittenDeleteFiles().forEach(rowDelta::removeDeletes);\n     result.deleteFiles().forEach(rowDelta::addDeletes);\n+    if (startSnapshot != null) {\n+      rowDelta.validateFromSnapshot(startSnapshot.snapshotId());\n+    }\n     rowDelta.commit();\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\nindex 9361c63176e0..659507e4c5e3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\n@@ -42,7 +42,11 @@ public static List<Object> parameters() {\n         new Object[] {2, LOCAL, LOCAL},\n         new Object[] {2, LOCAL, DISTRIBUTED},\n         new Object[] {2, DISTRIBUTED, LOCAL},\n-        new Object[] {2, LOCAL, DISTRIBUTED});\n+        new Object[] {2, LOCAL, DISTRIBUTED},\n+        new Object[] {3, LOCAL, LOCAL},\n+        new Object[] {3, LOCAL, DISTRIBUTED},\n+        new Object[] {3, DISTRIBUTED, LOCAL},\n+        new Object[] {3, DISTRIBUTED, DISTRIBUTED});\n   }\n \n   private static SparkSession spark = null;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\nindex acd4688440d1..2665d7ba8d3b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\n@@ -41,7 +41,11 @@ public static List<Object> parameters() {\n         new Object[] {2, LOCAL, LOCAL},\n         new Object[] {2, LOCAL, DISTRIBUTED},\n         new Object[] {2, DISTRIBUTED, LOCAL},\n-        new Object[] {2, DISTRIBUTED, DISTRIBUTED});\n+        new Object[] {2, DISTRIBUTED, DISTRIBUTED},\n+        new Object[] {3, LOCAL, LOCAL},\n+        new Object[] {3, LOCAL, DISTRIBUTED},\n+        new Object[] {3, DISTRIBUTED, LOCAL},\n+        new Object[] {3, DISTRIBUTED, DISTRIBUTED});\n   }\n \n   private static SparkSession spark = null;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex 79e48f47f241..11d61e599eba 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -719,7 +719,7 @@ public void testRewriteLargeManifestsEvolvedUnpartitionedV1Table() throws IOExce\n \n   @TestTemplate\n   public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -792,7 +792,7 @@ public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOExcept\n \n   @TestTemplate\n   public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\""c3\"").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -895,7 +895,7 @@ public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException\n \n   @TestTemplate\n   public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\""c3\"").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -956,6 +956,62 @@ public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException\n     assertThat(deleteManifests).hasSizeGreaterThanOrEqualTo(2);\n   }\n \n+  @TestTemplate\n+  public void testRewriteManifestsAfterUpgradeToV3() throws IOException {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\""c1\"").build();\n+    Map<String, String> options = ImmutableMap.of(TableProperties.FORMAT_VERSION, \""2\"");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    DataFile dataFile1 = newDataFile(table, \""c1=1\"");\n+    DeleteFile deleteFile1 = newDeletes(table, dataFile1);\n+    table.newRowDelta().addRows(dataFile1).addDeletes(deleteFile1).commit();\n+\n+    DataFile dataFile2 = newDataFile(table, \""c1=1\"");\n+    DeleteFile deleteFile2 = newDeletes(table, dataFile2);\n+    table.newRowDelta().addRows(dataFile2).addDeletes(deleteFile2).commit();\n+\n+    // upgrade the table to enable DVs\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \""3\"").commit();\n+\n+    DataFile dataFile3 = newDataFile(table, \""c1=1\"");\n+    DeleteFile dv3 = newDV(table, dataFile3);\n+    table.newRowDelta().addRows(dataFile3).addDeletes(dv3).commit();\n+\n+    SparkActions actions = SparkActions.get();\n+\n+    RewriteManifests.Result result =\n+        actions\n+            .rewriteManifests(table)\n+            .rewriteIf(manifest -> true)\n+            .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n+            .execute();\n+\n+    assertThat(result.rewrittenManifests()).as(\""Action should rewrite 6 manifests\"").hasSize(6);\n+    assertThat(result.addedManifests()).as(\""Action should add 2 manifests\"").hasSize(2);\n+    assertManifestsLocation(result.addedManifests());\n+\n+    table.refresh();\n+\n+    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n+      for (FileScanTask fileTask : tasks) {\n+        DataFile dataFile = fileTask.file();\n+        DeleteFile deleteFile = Iterables.getOnlyElement(fileTask.deletes());\n+        if (dataFile.location().equals(dataFile1.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile1.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile1);\n+        } else if (dataFile.location().equals(dataFile2.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile2.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile2);\n+        } else {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(dv3.referencedDataFile());\n+          assertEqual(deleteFile, dv3);\n+        }\n+      }\n+    }\n+  }\n+\n   private List<ThreeColumnRecord> actualRecords() {\n     return spark\n         .read()\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11476"", ""pr_id"": 11476, ""issue_id"": 11122, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other"", ""issue_word_count"": 118, ""test_files_count"": 2, ""non_test_files_count"": 11, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java"", ""core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java"", ""core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java"", ""core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java"", ""core/src/main/java/org/apache/iceberg/io/FanoutWriter.java"", ""core/src/main/java/org/apache/iceberg/io/StructCopy.java"", ""core/src/main/java/org/apache/iceberg/puffin/Puffin.java"", ""core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java"", ""core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java"", ""core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java"", ""data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"", ""spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java""], ""pr_changed_test_files"": [""data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java""], ""base_commit"": ""9be7f00dd6a9fb480a94c46d49473334908be859"", ""head_commit"": ""89b310799da8123c07470e5625f226f89f1c0259"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11476"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11476"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-06T20:32:07.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java b/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java\nnew file mode 100644\nindex 000000000000..ab47d8ad78d6\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java\n@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.deletes;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileMetadata;\n+import org.apache.iceberg.IcebergBuild;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.puffin.Blob;\n+import org.apache.iceberg.puffin.BlobMetadata;\n+import org.apache.iceberg.puffin.Puffin;\n+import org.apache.iceberg.puffin.PuffinWriter;\n+import org.apache.iceberg.puffin.StandardBlobTypes;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.ContentFileUtil;\n+import org.apache.iceberg.util.StructLikeUtil;\n+\n+public class BaseDVFileWriter implements DVFileWriter {\n+\n+  private static final String REFERENCED_DATA_FILE_KEY = \""referenced-data-file\"";\n+  private static final String CARDINALITY_KEY = \""cardinality\"";\n+\n+  private final OutputFileFactory fileFactory;\n+  private final Function<String, PositionDeleteIndex> loadPreviousDeletes;\n+  private final Map<String, Deletes> deletesByPath = Maps.newHashMap();\n+  private final Map<String, BlobMetadata> blobsByPath = Maps.newHashMap();\n+  private DeleteWriteResult result = null;\n+\n+  public BaseDVFileWriter(\n+      OutputFileFactory fileFactory, Function<String, PositionDeleteIndex> loadPreviousDeletes) {\n+    this.fileFactory = fileFactory;\n+    this.loadPreviousDeletes = loadPreviousDeletes;\n+  }\n+\n+  @Override\n+  public void delete(String path, long pos, PartitionSpec spec, StructLike partition) {\n+    Deletes deletes =\n+        deletesByPath.computeIfAbsent(path, key -> new Deletes(path, spec, partition));\n+    PositionDeleteIndex positions = deletes.positions();\n+    positions.delete(pos);\n+  }\n+\n+  @Override\n+  public DeleteWriteResult result() {\n+    Preconditions.checkState(result != null, \""Cannot get result from unclosed writer\"");\n+    return result;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (result == null) {\n+      List<DeleteFile> dvs = Lists.newArrayList();\n+      CharSequenceSet referencedDataFiles = CharSequenceSet.empty();\n+      List<DeleteFile> rewrittenDeleteFiles = Lists.newArrayList();\n+\n+      PuffinWriter writer = newWriter();\n+\n+      try (PuffinWriter closeableWriter = writer) {\n+        for (Deletes deletes : deletesByPath.values()) {\n+          String path = deletes.path();\n+          PositionDeleteIndex positions = deletes.positions();\n+          PositionDeleteIndex previousPositions = loadPreviousDeletes.apply(path);\n+          if (previousPositions != null) {\n+            positions.merge(previousPositions);\n+            for (DeleteFile previousDeleteFile : previousPositions.deleteFiles()) {\n+              // only DVs and file-scoped deletes can be discarded from the table state\n+              if (ContentFileUtil.isFileScoped(previousDeleteFile)) {\n+                rewrittenDeleteFiles.add(previousDeleteFile);\n+              }\n+            }\n+          }\n+          write(closeableWriter, deletes);\n+          referencedDataFiles.add(path);\n+        }\n+      }\n+\n+      // DVs share the Puffin path and file size but have different offsets\n+      String puffinPath = writer.location();\n+      long puffinFileSize = writer.fileSize();\n+\n+      for (String path : deletesByPath.keySet()) {\n+        DeleteFile dv = createDV(puffinPath, puffinFileSize, path);\n+        dvs.add(dv);\n+      }\n+\n+      this.result = new DeleteWriteResult(dvs, referencedDataFiles, rewrittenDeleteFiles);\n+    }\n+  }\n+\n+  private DeleteFile createDV(String path, long size, String referencedDataFile) {\n+    Deletes deletes = deletesByPath.get(referencedDataFile);\n+    BlobMetadata blobMetadata = blobsByPath.get(referencedDataFile);\n+    return FileMetadata.deleteFileBuilder(deletes.spec())\n+        .ofPositionDeletes()\n+        .withFormat(FileFormat.PUFFIN)\n+        .withPath(path)\n+        .withPartition(deletes.partition())\n+        .withFileSizeInBytes(size)\n+        .withReferencedDataFile(referencedDataFile)\n+        .withContentOffset(blobMetadata.offset())\n+        .withContentSizeInBytes(blobMetadata.length())\n+        .withRecordCount(deletes.positions().cardinality())\n+        .build();\n+  }\n+\n+  private void write(PuffinWriter writer, Deletes deletes) {\n+    String path = deletes.path();\n+    PositionDeleteIndex positions = deletes.positions();\n+    BlobMetadata blobMetadata = writer.write(toBlob(positions, path));\n+    blobsByPath.put(path, blobMetadata);\n+  }\n+\n+  private PuffinWriter newWriter() {\n+    EncryptedOutputFile outputFile = fileFactory.newOutputFile();\n+    String ident = \""Iceberg \"" + IcebergBuild.fullVersion();\n+    return Puffin.write(outputFile).createdBy(ident).build();\n+  }\n+\n+  private Blob toBlob(PositionDeleteIndex positions, String path) {\n+    return new Blob(\n+        StandardBlobTypes.DV_V1,\n+        ImmutableList.of(MetadataColumns.ROW_POSITION.fieldId()),\n+        -1 /* snapshot ID is inherited */,\n+        -1 /* sequence number is inherited */,\n+        positions.serialize(),\n+        null /* uncompressed */,\n+        ImmutableMap.of(\n+            REFERENCED_DATA_FILE_KEY,\n+            path,\n+            CARDINALITY_KEY,\n+            String.valueOf(positions.cardinality())));\n+  }\n+\n+  private static class Deletes {\n+    private final String path;\n+    private final PositionDeleteIndex positions;\n+    private final PartitionSpec spec;\n+    private final StructLike partition;\n+\n+    private Deletes(String path, PartitionSpec spec, StructLike partition) {\n+      this.path = path;\n+      this.positions = new BitmapPositionDeleteIndex();\n+      this.spec = spec;\n+      this.partition = StructLikeUtil.copy(partition);\n+    }\n+\n+    public String path() {\n+      return path;\n+    }\n+\n+    public PositionDeleteIndex positions() {\n+      return positions;\n+    }\n+\n+    public PartitionSpec spec() {\n+      return spec;\n+    }\n+\n+    public StructLike partition() {\n+      return partition;\n+    }\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java b/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java\nnew file mode 100644\nindex 000000000000..2561f7be3d34\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.deletes;\n+\n+import java.io.Closeable;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+\n+/** A deletion vector file writer. */\n+public interface DVFileWriter extends Closeable {\n+  /**\n+   * Marks a position in a given data file as deleted.\n+   *\n+   * @param path the data file path\n+   * @param pos the data file position\n+   * @param spec the data file partition spec\n+   * @param partition the data file partition\n+   */\n+  void delete(String path, long pos, PartitionSpec spec, StructLike partition);\n+\n+  /**\n+   * Returns a result that contains information about written {@link DeleteFile}s. The result is\n+   * valid only after the writer is closed.\n+   *\n+   * @return the writer result\n+   */\n+  DeleteWriteResult result();\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java b/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\nindex 471dc3e56035..968db0ab538b 100644\n--- a/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\n@@ -40,6 +40,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.util.CharSequenceSet;\n import org.apache.iceberg.util.StructLikeMap;\n+import org.apache.iceberg.util.StructLikeUtil;\n import org.apache.iceberg.util.StructProjection;\n import org.apache.iceberg.util.Tasks;\n import org.apache.iceberg.util.ThreadPools;\n@@ -149,7 +150,7 @@ public void write(T row) throws IOException {\n       PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n \n       // Create a copied key from this row.\n-      StructLike copiedKey = StructCopy.copy(structProjection.wrap(asStructLike(row)));\n+      StructLike copiedKey = StructLikeUtil.copy(structProjection.wrap(asStructLike(row)));\n \n       // Adding a pos-delete to replace the old path-offset.\n       PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java b/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\nindex 1dc4871f0a12..06093ff2a943 100644\n--- a/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\n@@ -29,6 +29,7 @@\n import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructLikeUtil;\n \n /**\n  * A writer capable of writing to multiple specs and partitions that requires the incoming records\n@@ -81,7 +82,7 @@ public void write(T row, PartitionSpec spec, StructLike partition) {\n       this.partitionComparator = Comparators.forType(partitionType);\n       this.completedPartitions = StructLikeSet.create(partitionType);\n       // copy the partition key as the key object may be reused\n-      this.currentPartition = StructCopy.copy(partition);\n+      this.currentPartition = StructLikeUtil.copy(partition);\n       this.currentWriter = newWriter(currentSpec, currentPartition);\n \n     } else if (partition != currentPartition\n@@ -96,7 +97,7 @@ public void write(T row, PartitionSpec spec, StructLike partition) {\n       }\n \n       // copy the partition key as the key object may be reused\n-      this.currentPartition = StructCopy.copy(partition);\n+      this.currentPartition = StructLikeUtil.copy(partition);\n       this.currentWriter = newWriter(currentSpec, currentPartition);\n     }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java b/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\nindex 95d5cc1afcc9..340f4b3558d9 100644\n--- a/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.util.StructLikeMap;\n+import org.apache.iceberg.util.StructLikeUtil;\n \n /**\n  * A writer capable of writing to multiple specs and partitions that keeps files for each seen\n@@ -59,7 +60,7 @@ private FileWriter<T, R> writer(PartitionSpec spec, StructLike partition) {\n \n     if (writer == null) {\n       // copy the partition key as the key object may be reused\n-      StructLike copiedPartition = StructCopy.copy(partition);\n+      StructLike copiedPartition = StructLikeUtil.copy(partition);\n       writer = newWriter(spec, copiedPartition);\n       specWriters.put(copiedPartition, writer);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/StructCopy.java b/core/src/main/java/org/apache/iceberg/io/StructCopy.java\nindex 229dff371762..f8adbf62a1d6 100644\n--- a/core/src/main/java/org/apache/iceberg/io/StructCopy.java\n+++ b/core/src/main/java/org/apache/iceberg/io/StructCopy.java\n@@ -20,7 +20,13 @@\n \n import org.apache.iceberg.StructLike;\n \n-/** Copy the StructLike's values into a new one. It does not handle list or map values now. */\n+/**\n+ * Copy the StructLike's values into a new one. It does not handle list or map values now.\n+ *\n+ * @deprecated since 1.8.0, will be removed in 1.9.0; use {@link\n+ *     org.apache.iceberg.util.StructLikeUtil#copy(StructLike)} instead.\n+ */\n+@Deprecated\n class StructCopy implements StructLike {\n   static StructLike copy(StructLike struct) {\n     return struct != null ? new StructCopy(struct) : null;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/Puffin.java b/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\nindex 251486d01e76..d20a5596c4d4 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.puffin;\n \n import java.util.Map;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n@@ -31,6 +32,10 @@ public static WriteBuilder write(OutputFile outputFile) {\n     return new WriteBuilder(outputFile);\n   }\n \n+  public static WriteBuilder write(EncryptedOutputFile outputFile) {\n+    return new WriteBuilder(outputFile.encryptingOutputFile());\n+  }\n+\n   /** A builder for {@link PuffinWriter}. */\n   public static class WriteBuilder {\n     private final OutputFile outputFile;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java b/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\nindex 5728b7474885..cd44dab03b90 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\n@@ -44,6 +44,7 @@ public class PuffinWriter implements FileAppender<Blob> {\n   // Must not be modified\n   private static final byte[] MAGIC = PuffinFormat.getMagic();\n \n+  private final OutputFile outputFile;\n   private final PositionOutputStream outputStream;\n   private final Map<String, String> properties;\n   private final PuffinCompressionCodec footerCompression;\n@@ -63,6 +64,7 @@ public class PuffinWriter implements FileAppender<Blob> {\n     Preconditions.checkNotNull(outputFile, \""outputFile is null\"");\n     Preconditions.checkNotNull(properties, \""properties is null\"");\n     Preconditions.checkNotNull(defaultBlobCompression, \""defaultBlobCompression is null\"");\n+    this.outputFile = outputFile;\n     this.outputStream = outputFile.create();\n     this.properties = ImmutableMap.copyOf(properties);\n     this.footerCompression =\n@@ -70,8 +72,16 @@ public class PuffinWriter implements FileAppender<Blob> {\n     this.defaultBlobCompression = defaultBlobCompression;\n   }\n \n+  public String location() {\n+    return outputFile.location();\n+  }\n+\n   @Override\n   public void add(Blob blob) {\n+    write(blob);\n+  }\n+\n+  public BlobMetadata write(Blob blob) {\n     Preconditions.checkNotNull(blob, \""blob is null\"");\n     checkNotFinished();\n     try {\n@@ -82,7 +92,7 @@ public void add(Blob blob) {\n       ByteBuffer rawData = PuffinFormat.compress(codec, blob.blobData());\n       int length = rawData.remaining();\n       IOUtil.writeFully(outputStream, rawData);\n-      writtenBlobsMetadata.add(\n+      BlobMetadata blobMetadata =\n           new BlobMetadata(\n               blob.type(),\n               blob.inputFields(),\n@@ -91,7 +101,9 @@ public void add(Blob blob) {\n               fileOffset,\n               length,\n               codec.codecName(),\n-              blob.properties()));\n+              blob.properties());\n+      writtenBlobsMetadata.add(blobMetadata);\n+      return blobMetadata;\n     } catch (IOException e) {\n       throw new UncheckedIOException(e);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java b/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\nindex 2be5df5f88b9..ce78375c4b1a 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\n@@ -26,4 +26,7 @@ private StandardBlobTypes() {}\n    * href=\""https://datasketches.apache.org/\"">Apache DataSketches</a> library\n    */\n   public static final String APACHE_DATASKETCHES_THETA_V1 = \""apache-datasketches-theta-v1\"";\n+\n+  /** A serialized deletion vector according to the Iceberg spec */\n+  public static final String DV_V1 = \""deletion-vector-v1\"";\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java b/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java\nnew file mode 100644\nindex 000000000000..5285793a4aad\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java\n@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import org.apache.iceberg.StructLike;\n+\n+public class StructLikeUtil {\n+\n+  private StructLikeUtil() {}\n+\n+  public static StructLike copy(StructLike struct) {\n+    return StructCopy.copy(struct);\n+  }\n+\n+  private static class StructCopy implements StructLike {\n+    private static StructLike copy(StructLike struct) {\n+      return struct != null ? new StructCopy(struct) : null;\n+    }\n+\n+    private final Object[] values;\n+\n+    private StructCopy(StructLike toCopy) {\n+      this.values = new Object[toCopy.size()];\n+\n+      for (int i = 0; i < values.length; i += 1) {\n+        Object value = toCopy.get(i, Object.class);\n+\n+        if (value instanceof StructLike) {\n+          values[i] = copy((StructLike) value);\n+        } else {\n+          values[i] = value;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public int size() {\n+      return values.length;\n+    }\n+\n+    @Override\n+    public <T> T get(int pos, Class<T> javaClass) {\n+      return javaClass.cast(values[pos]);\n+    }\n+\n+    @Override\n+    public <T> void set(int pos, T value) {\n+      throw new UnsupportedOperationException(\""Struct copy cannot be modified\"");\n+    }\n+  }\n+}\n\ndiff --git a/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java b/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java\nnew file mode 100644\nindex 000000000000..ac74fb5a109c\n--- /dev/null\n+++ b/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java\n@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.errorprone.annotations.FormatMethod;\n+import com.google.errorprone.annotations.FormatString;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileGenerationUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n+import org.apache.iceberg.deletes.DeleteGranularity;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+import org.apache.iceberg.io.FanoutPositionOnlyDeleteWriter;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.TearDown;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Timeout;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+\n+/**\n+ * A benchmark that compares the performance of DV and position delete writers.\n+ *\n+ * <p>To run this benchmark for spark-3.5: <code>\n+ *   ./gradlew -DsparkVersions=3.5 :iceberg-spark:iceberg-spark-3.5_2.12:jmh\n+ *       -PjmhIncludeRegex=DVWriterBenchmark\n+ *       -PjmhOutputPath=benchmark/iceberg-dv-writer-benchmark-result.txt\n+ * </code>\n+ */\n+@Fork(1)\n+@State(Scope.Benchmark)\n+@Warmup(iterations = 3)\n+@Measurement(iterations = 10)\n+@Timeout(time = 20, timeUnit = TimeUnit.MINUTES)\n+@BenchmarkMode(Mode.SingleShotTime)\n+public class DVWriterBenchmark {\n+\n+  private static final String TABLE_NAME = \""test_table\"";\n+  private static final int DATA_FILE_RECORD_COUNT = 2_000_000;\n+  private static final long TARGET_FILE_SIZE = Long.MAX_VALUE;\n+\n+  @Param({\""5\"", \""10\""})\n+  private int referencedDataFileCount;\n+\n+  @Param({\""0.01\"", \""0.03\"", \""0.05\"", \""0.10\"", \""0.2\""})\n+  private double deletedRowsRatio;\n+\n+  private final Configuration hadoopConf = new Configuration();\n+  private final Random random = ThreadLocalRandom.current();\n+  private SparkSession spark;\n+  private Table table;\n+  private Iterable<InternalRow> positionDeletes;\n+\n+  @Setup\n+  public void setupBenchmark() throws NoSuchTableException, ParseException {\n+    setupSpark();\n+    initTable();\n+    generatePositionDeletes();\n+  }\n+\n+  @TearDown\n+  public void tearDownBenchmark() {\n+    dropTable();\n+    tearDownSpark();\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void dv(Blackhole blackhole) throws IOException {\n+    OutputFileFactory fileFactory = newFileFactory(FileFormat.PUFFIN);\n+    DVFileWriter writer = new BaseDVFileWriter(fileFactory, path -> null);\n+\n+    try (DVFileWriter closableWriter = writer) {\n+      for (InternalRow row : positionDeletes) {\n+        String path = row.getString(0);\n+        long pos = row.getLong(1);\n+        closableWriter.delete(path, pos, table.spec(), null);\n+      }\n+    }\n+\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void fileScopedParquetDeletes(Blackhole blackhole) throws IOException {\n+    FanoutPositionOnlyDeleteWriter<InternalRow> writer = newWriter(DeleteGranularity.FILE);\n+    write(writer, positionDeletes);\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void partitionScopedParquetDeletes(Blackhole blackhole) throws IOException {\n+    FanoutPositionOnlyDeleteWriter<InternalRow> writer = newWriter(DeleteGranularity.PARTITION);\n+    write(writer, positionDeletes);\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  private FanoutPositionOnlyDeleteWriter<InternalRow> newWriter(DeleteGranularity granularity) {\n+    return new FanoutPositionOnlyDeleteWriter<>(\n+        newWriterFactory(),\n+        newFileFactory(FileFormat.PARQUET),\n+        table.io(),\n+        TARGET_FILE_SIZE,\n+        granularity);\n+  }\n+\n+  private DeleteWriteResult write(\n+      FanoutPositionOnlyDeleteWriter<InternalRow> writer, Iterable<InternalRow> rows)\n+      throws IOException {\n+\n+    try (FanoutPositionOnlyDeleteWriter<InternalRow> closableWriter = writer) {\n+      PositionDelete<InternalRow> positionDelete = PositionDelete.create();\n+\n+      for (InternalRow row : rows) {\n+        String path = row.getString(0);\n+        long pos = row.getLong(1);\n+        positionDelete.set(path, pos, null /* no row */);\n+        closableWriter.write(positionDelete, table.spec(), null);\n+      }\n+    }\n+\n+    return writer.result();\n+  }\n+\n+  private SparkFileWriterFactory newWriterFactory() {\n+    return SparkFileWriterFactory.builderFor(table).dataFileFormat(FileFormat.PARQUET).build();\n+  }\n+\n+  private OutputFileFactory newFileFactory(FileFormat format) {\n+    return OutputFileFactory.builderFor(table, 1, 1).format(format).build();\n+  }\n+\n+  private void generatePositionDeletes() {\n+    int numDeletesPerFile = (int) (DATA_FILE_RECORD_COUNT * deletedRowsRatio);\n+    int numDeletes = referencedDataFileCount * numDeletesPerFile;\n+    List<InternalRow> deletes = Lists.newArrayListWithExpectedSize(numDeletes);\n+\n+    for (int pathIndex = 0; pathIndex < referencedDataFileCount; pathIndex++) {\n+      UTF8String dataFilePath = UTF8String.fromString(generateDataFilePath());\n+      Set<Long> positions = generatePositions(numDeletesPerFile);\n+      for (long pos : positions) {\n+        deletes.add(new GenericInternalRow(new Object[] {dataFilePath, pos}));\n+      }\n+    }\n+\n+    Collections.shuffle(deletes);\n+\n+    this.positionDeletes = deletes;\n+  }\n+\n+  public Set<Long> generatePositions(int numPositions) {\n+    Set<Long> positions = Sets.newHashSet();\n+\n+    while (positions.size() < numPositions) {\n+      long pos = random.nextInt(DATA_FILE_RECORD_COUNT);\n+      positions.add(pos);\n+    }\n+\n+    return positions;\n+  }\n+\n+  private String generateDataFilePath() {\n+    String fileName = FileGenerationUtil.generateFileName();\n+    return table.locationProvider().newDataLocation(table.spec(), null, fileName);\n+  }\n+\n+  private void setupSpark() {\n+    this.spark =\n+        SparkSession.builder()\n+            .config(\""spark.ui.enabled\"", false)\n+            .config(\""spark.serializer\"", \""org.apache.spark.serializer.KryoSerializer\"")\n+            .config(\""spark.sql.catalog.spark_catalog\"", SparkSessionCatalog.class.getName())\n+            .config(\""spark.sql.catalog.spark_catalog.type\"", \""hadoop\"")\n+            .config(\""spark.sql.catalog.spark_catalog.warehouse\"", newWarehouseDir())\n+            .master(\""local[*]\"")\n+            .getOrCreate();\n+  }\n+\n+  private void tearDownSpark() {\n+    spark.stop();\n+  }\n+\n+  private void initTable() throws NoSuchTableException, ParseException {\n+    sql(\""CREATE TABLE %s (c1 INT, c2 INT, c3 STRING) USING iceberg\"", TABLE_NAME);\n+    this.table = Spark3Util.loadIcebergTable(spark, TABLE_NAME);\n+  }\n+\n+  private void dropTable() {\n+    sql(\""DROP TABLE IF EXISTS %s PURGE\"", TABLE_NAME);\n+  }\n+\n+  private String newWarehouseDir() {\n+    return hadoopConf.get(\""hadoop.tmp.dir\"") + UUID.randomUUID();\n+  }\n+\n+  @FormatMethod\n+  private void sql(@FormatString String query, Object... args) {\n+    spark.sql(String.format(query, args));\n+  }\n+}\n"", ""test_patch"": ""diff --git a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\nnew file mode 100644\nindex 000000000000..ce742b1c4685\n--- /dev/null\n+++ b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.io;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.BaseDeleteLoader;\n+import org.apache.iceberg.data.DeleteLoader;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n+import org.apache.iceberg.deletes.PositionDeleteIndex;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class TestDVWriters<T> extends WriterTestBase<T> {\n+\n+  @Parameters(name = \""formatVersion = {0}\"")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(new Object[] {3});\n+  }\n+\n+  private OutputFileFactory fileFactory = null;\n+\n+  protected abstract StructLikeSet toSet(Iterable<T> records);\n+\n+  protected FileFormat dataFormat() {\n+    return FileFormat.PARQUET;\n+  }\n+\n+  @Override\n+  @BeforeEach\n+  public void setupTable() throws Exception {\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+  }\n+\n+  @TestTemplate\n+  public void testBasicDVs() throws IOException {\n+    FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n+\n+    // add the first data file\n+    List<T> rows1 = ImmutableList.of(toRow(1, \""aaa\""), toRow(2, \""aaa\""), toRow(11, \""aaa\""));\n+    DataFile dataFile1 = writeData(writerFactory, fileFactory, rows1, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile1).commit();\n+\n+    // add the second data file\n+    List<T> rows2 = ImmutableList.of(toRow(3, \""aaa\""), toRow(4, \""aaa\""), toRow(12, \""aaa\""));\n+    DataFile dataFile2 = writeData(writerFactory, fileFactory, rows2, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile2).commit();\n+\n+    // init the DV writer\n+    DVFileWriter dvWriter =\n+        new BaseDVFileWriter(fileFactory, new PreviousDeleteLoader(table, ImmutableMap.of()));\n+\n+    // write deletes for both data files (the order of records is mixed)\n+    dvWriter.delete(dataFile1.location(), 1L, table.spec(), null);\n+    dvWriter.delete(dataFile2.location(), 0L, table.spec(), null);\n+    dvWriter.delete(dataFile1.location(), 0L, table.spec(), null);\n+    dvWriter.delete(dataFile2.location(), 1L, table.spec(), null);\n+    dvWriter.close();\n+\n+    // verify the writer result\n+    DeleteWriteResult result = dvWriter.result();\n+    assertThat(result.deleteFiles()).hasSize(2);\n+    assertThat(result.referencedDataFiles())\n+        .hasSize(2)\n+        .contains(dataFile1.location())\n+        .contains(dataFile2.location());\n+    assertThat(result.referencesDataFiles()).isTrue();\n+  }\n+\n+  private static class PreviousDeleteLoader implements Function<String, PositionDeleteIndex> {\n+    private final Map<String, DeleteFile> deleteFiles;\n+    private final DeleteLoader deleteLoader;\n+\n+    PreviousDeleteLoader(Table table, Map<String, DeleteFile> deleteFiles) {\n+      this.deleteFiles = deleteFiles;\n+      this.deleteLoader = new BaseDeleteLoader(deleteFile -> table.io().newInputFile(deleteFile));\n+    }\n+\n+    @Override\n+    public PositionDeleteIndex apply(String path) {\n+      DeleteFile deleteFile = deleteFiles.get(path);\n+      if (deleteFile == null) {\n+        return null;\n+      }\n+      return deleteLoader.loadPositionDeletes(ImmutableList.of(deleteFile), path);\n+    }\n+  }\n+}\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java\nnew file mode 100644\nindex 000000000000..dfc693d3094d\n--- /dev/null\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java\n@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.List;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.FileWriterFactory;\n+import org.apache.iceberg.io.TestDVWriters;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class TestSparkDVWriters extends TestDVWriters<InternalRow> {\n+\n+  @Override\n+  protected FileWriterFactory<InternalRow> newWriterFactory(\n+      Schema dataSchema,\n+      List<Integer> equalityFieldIds,\n+      Schema equalityDeleteRowSchema,\n+      Schema positionDeleteRowSchema) {\n+    return SparkFileWriterFactory.builderFor(table)\n+        .dataSchema(table.schema())\n+        .dataFileFormat(dataFormat())\n+        .deleteFileFormat(dataFormat())\n+        .equalityFieldIds(ArrayUtil.toIntArray(equalityFieldIds))\n+        .equalityDeleteRowSchema(equalityDeleteRowSchema)\n+        .positionDeleteRowSchema(positionDeleteRowSchema)\n+        .build();\n+  }\n+\n+  @Override\n+  protected InternalRow toRow(Integer id, String data) {\n+    InternalRow row = new GenericInternalRow(2);\n+    row.update(0, id);\n+    row.update(1, UTF8String.fromString(data));\n+    return row;\n+  }\n+\n+  @Override\n+  protected StructLikeSet toSet(Iterable<InternalRow> rows) {\n+    StructLikeSet set = StructLikeSet.create(table.schema().asStruct());\n+    StructType sparkType = SparkSchemaUtil.convert(table.schema());\n+    for (InternalRow row : rows) {\n+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());\n+      set.add(wrapper.wrap(row));\n+    }\n+    return set;\n+  }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11464"", ""pr_id"": 11464, ""issue_id"": 11122, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other"", ""issue_word_count"": 118, ""test_files_count"": 4, ""non_test_files_count"": 7, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/SnapshotSummary.java"", ""core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java"", ""core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java"", ""core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java"", ""core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java"", ""core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java"", ""core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java"", ""core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java"", ""core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java""], ""base_commit"": ""5bd314bdf6c3b5e0e5346d0f7408353bdf31bc81"", ""head_commit"": ""c64b6593d9f89b4274193fdecba8038d72339b8b"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11464"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11464"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-05T16:02:00.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\nindex ad832a5e78e2..6043424cd7fc 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.ScanTaskUtil;\n \n public class SnapshotSummary {\n@@ -36,6 +37,8 @@ public class SnapshotSummary {\n   public static final String REMOVED_EQ_DELETE_FILES_PROP = \""removed-equality-delete-files\"";\n   public static final String ADD_POS_DELETE_FILES_PROP = \""added-position-delete-files\"";\n   public static final String REMOVED_POS_DELETE_FILES_PROP = \""removed-position-delete-files\"";\n+  public static final String ADDED_DVS_PROP = \""added-dvs\"";\n+  public static final String REMOVED_DVS_PROP = \""removed-dvs\"";\n   public static final String REMOVED_DELETE_FILES_PROP = \""removed-delete-files\"";\n   public static final String TOTAL_DELETE_FILES_PROP = \""total-delete-files\"";\n   public static final String ADDED_RECORDS_PROP = \""added-records\"";\n@@ -222,6 +225,8 @@ private static class UpdateMetrics {\n     private int removedEqDeleteFiles = 0;\n     private int addedPosDeleteFiles = 0;\n     private int removedPosDeleteFiles = 0;\n+    private int addedDVs = 0;\n+    private int removedDVs = 0;\n     private int addedDeleteFiles = 0;\n     private int removedDeleteFiles = 0;\n     private long addedRecords = 0L;\n@@ -243,6 +248,8 @@ void clear() {\n       this.removedPosDeleteFiles = 0;\n       this.addedDeleteFiles = 0;\n       this.removedDeleteFiles = 0;\n+      this.addedDVs = 0;\n+      this.removedDVs = 0;\n       this.addedRecords = 0L;\n       this.deletedRecords = 0L;\n       this.addedPosDeletes = 0L;\n@@ -262,6 +269,8 @@ void addTo(ImmutableMap.Builder<String, String> builder) {\n           removedPosDeleteFiles > 0, builder, REMOVED_POS_DELETE_FILES_PROP, removedPosDeleteFiles);\n       setIf(addedDeleteFiles > 0, builder, ADDED_DELETE_FILES_PROP, addedDeleteFiles);\n       setIf(removedDeleteFiles > 0, builder, REMOVED_DELETE_FILES_PROP, removedDeleteFiles);\n+      setIf(addedDVs > 0, builder, ADDED_DVS_PROP, addedDVs);\n+      setIf(removedDVs > 0, builder, REMOVED_DVS_PROP, removedDVs);\n       setIf(addedRecords > 0, builder, ADDED_RECORDS_PROP, addedRecords);\n       setIf(deletedRecords > 0, builder, DELETED_RECORDS_PROP, deletedRecords);\n \n@@ -283,8 +292,13 @@ void addedFile(ContentFile<?> file) {\n           this.addedRecords += file.recordCount();\n           break;\n         case POSITION_DELETES:\n+          DeleteFile deleteFile = (DeleteFile) file;\n+          if (ContentFileUtil.isDV(deleteFile)) {\n+            this.addedDVs += 1;\n+          } else {\n+            this.addedPosDeleteFiles += 1;\n+          }\n           this.addedDeleteFiles += 1;\n-          this.addedPosDeleteFiles += 1;\n           this.addedPosDeletes += file.recordCount();\n           break;\n         case EQUALITY_DELETES:\n@@ -306,8 +320,13 @@ void removedFile(ContentFile<?> file) {\n           this.deletedRecords += file.recordCount();\n           break;\n         case POSITION_DELETES:\n+          DeleteFile deleteFile = (DeleteFile) file;\n+          if (ContentFileUtil.isDV(deleteFile)) {\n+            this.removedDVs += 1;\n+          } else {\n+            this.removedPosDeleteFiles += 1;\n+          }\n           this.removedDeleteFiles += 1;\n-          this.removedPosDeleteFiles += 1;\n           this.removedPosDeletes += file.recordCount();\n           break;\n         case EQUALITY_DELETES:\n@@ -344,6 +363,8 @@ void merge(UpdateMetrics other) {\n       this.removedEqDeleteFiles += other.removedEqDeleteFiles;\n       this.addedPosDeleteFiles += other.addedPosDeleteFiles;\n       this.removedPosDeleteFiles += other.removedPosDeleteFiles;\n+      this.addedDVs += other.addedDVs;\n+      this.removedDVs += other.removedDVs;\n       this.addedDeleteFiles += other.addedDeleteFiles;\n       this.removedDeleteFiles += other.removedDeleteFiles;\n       this.addedSize += other.addedSize;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\nindex ad66e8d32408..7a87172708f6 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\n@@ -34,7 +34,9 @@ public interface CommitMetricsResult {\n   String ADDED_DELETE_FILES = \""added-delete-files\"";\n   String ADDED_EQ_DELETE_FILES = \""added-equality-delete-files\"";\n   String ADDED_POS_DELETE_FILES = \""added-positional-delete-files\"";\n+  String ADDED_DVS = \""added-dvs\"";\n   String REMOVED_POS_DELETE_FILES = \""removed-positional-delete-files\"";\n+  String REMOVED_DVS = \""removed-dvs\"";\n   String REMOVED_EQ_DELETE_FILES = \""removed-equality-delete-files\"";\n   String REMOVED_DELETE_FILES = \""removed-delete-files\"";\n   String TOTAL_DELETE_FILES = \""total-delete-files\"";\n@@ -75,6 +77,12 @@ public interface CommitMetricsResult {\n   @Nullable\n   CounterResult addedPositionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult addedDVs() {\n+    return null;\n+  }\n+\n   @Nullable\n   CounterResult removedDeleteFiles();\n \n@@ -84,6 +92,12 @@ public interface CommitMetricsResult {\n   @Nullable\n   CounterResult removedPositionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult removedDVs() {\n+    return null;\n+  }\n+\n   @Nullable\n   CounterResult totalDeleteFiles();\n \n@@ -136,6 +150,7 @@ static CommitMetricsResult from(\n         .addedDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.ADDED_DELETE_FILES_PROP))\n         .addedPositionalDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.ADD_POS_DELETE_FILES_PROP))\n+        .addedDVs(counterFrom(snapshotSummary, SnapshotSummary.ADDED_DVS_PROP))\n         .addedEqualityDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.ADD_EQ_DELETE_FILES_PROP))\n         .removedDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.REMOVED_DELETE_FILES_PROP))\n@@ -143,6 +158,7 @@ static CommitMetricsResult from(\n             counterFrom(snapshotSummary, SnapshotSummary.REMOVED_EQ_DELETE_FILES_PROP))\n         .removedPositionalDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP))\n+        .removedDVs(counterFrom(snapshotSummary, SnapshotSummary.REMOVED_DVS_PROP))\n         .totalDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.TOTAL_DELETE_FILES_PROP))\n         .addedRecords(counterFrom(snapshotSummary, SnapshotSummary.ADDED_RECORDS_PROP))\n         .removedRecords(counterFrom(snapshotSummary, SnapshotSummary.DELETED_RECORDS_PROP))\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\nindex d4fd883c4375..2c45581ba5d6 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\n@@ -81,6 +81,11 @@ static void toJson(CommitMetricsResult metrics, JsonGenerator gen) throws IOExce\n       CounterResultParser.toJson(metrics.addedPositionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.addedDVs()) {\n+      gen.writeFieldName(CommitMetricsResult.ADDED_DVS);\n+      CounterResultParser.toJson(metrics.addedDVs(), gen);\n+    }\n+\n     if (null != metrics.removedDeleteFiles()) {\n       gen.writeFieldName(CommitMetricsResult.REMOVED_DELETE_FILES);\n       CounterResultParser.toJson(metrics.removedDeleteFiles(), gen);\n@@ -91,6 +96,11 @@ static void toJson(CommitMetricsResult metrics, JsonGenerator gen) throws IOExce\n       CounterResultParser.toJson(metrics.removedPositionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.removedDVs()) {\n+      gen.writeFieldName(CommitMetricsResult.REMOVED_DVS);\n+      CounterResultParser.toJson(metrics.removedDVs(), gen);\n+    }\n+\n     if (null != metrics.removedEqualityDeleteFiles()) {\n       gen.writeFieldName(CommitMetricsResult.REMOVED_EQ_DELETE_FILES);\n       CounterResultParser.toJson(metrics.removedEqualityDeleteFiles(), gen);\n@@ -186,10 +196,12 @@ static CommitMetricsResult fromJson(JsonNode json) {\n             CounterResultParser.fromJson(CommitMetricsResult.ADDED_EQ_DELETE_FILES, json))\n         .addedPositionalDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.ADDED_POS_DELETE_FILES, json))\n+        .addedDVs(CounterResultParser.fromJson(CommitMetricsResult.ADDED_DVS, json))\n         .removedEqualityDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_EQ_DELETE_FILES, json))\n         .removedPositionalDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_POS_DELETE_FILES, json))\n+        .removedDVs(CounterResultParser.fromJson(CommitMetricsResult.REMOVED_DVS, json))\n         .removedDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_DELETE_FILES, json))\n         .totalDeleteFiles(\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\nindex 421466f0fa85..0f7def37638e 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\n@@ -40,6 +40,7 @@ public abstract class ScanMetrics {\n   public static final String INDEXED_DELETE_FILES = \""indexed-delete-files\"";\n   public static final String EQUALITY_DELETE_FILES = \""equality-delete-files\"";\n   public static final String POSITIONAL_DELETE_FILES = \""positional-delete-files\"";\n+  public static final String DVS = \""dvs\"";\n \n   public static ScanMetrics noop() {\n     return ScanMetrics.of(MetricsContext.nullMetrics());\n@@ -127,6 +128,11 @@ public Counter positionalDeleteFiles() {\n     return metricsContext().counter(POSITIONAL_DELETE_FILES);\n   }\n \n+  @Value.Derived\n+  public Counter dvs() {\n+    return metricsContext().counter(DVS);\n+  }\n+\n   public static ScanMetrics of(MetricsContext metricsContext) {\n     return ImmutableScanMetrics.builder().metricsContext(metricsContext).build();\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\nindex b930dd83adef..2137e52e0a89 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\n@@ -73,6 +73,12 @@ public interface ScanMetricsResult {\n   @Nullable\n   CounterResult positionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult dvs() {\n+    return null;\n+  }\n+\n   static ScanMetricsResult fromScanMetrics(ScanMetrics scanMetrics) {\n     Preconditions.checkArgument(null != scanMetrics, \""Invalid scan metrics: null\"");\n     return ImmutableScanMetricsResult.builder()\n@@ -93,6 +99,7 @@ static ScanMetricsResult fromScanMetrics(ScanMetrics scanMetrics) {\n         .indexedDeleteFiles(CounterResult.fromCounter(scanMetrics.indexedDeleteFiles()))\n         .equalityDeleteFiles(CounterResult.fromCounter(scanMetrics.equalityDeleteFiles()))\n         .positionalDeleteFiles(CounterResult.fromCounter(scanMetrics.positionalDeleteFiles()))\n+        .dvs(CounterResult.fromCounter(scanMetrics.dvs()))\n         .build();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\nindex 5cff1ae8e0db..f85c26753211 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\n@@ -121,6 +121,11 @@ static void toJson(ScanMetricsResult metrics, JsonGenerator gen) throws IOExcept\n       CounterResultParser.toJson(metrics.positionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.dvs()) {\n+      gen.writeFieldName(ScanMetrics.DVS);\n+      CounterResultParser.toJson(metrics.dvs(), gen);\n+    }\n+\n     gen.writeEndObject();\n   }\n \n@@ -159,6 +164,7 @@ static ScanMetricsResult fromJson(JsonNode json) {\n         .equalityDeleteFiles(CounterResultParser.fromJson(ScanMetrics.EQUALITY_DELETE_FILES, json))\n         .positionalDeleteFiles(\n             CounterResultParser.fromJson(ScanMetrics.POSITIONAL_DELETE_FILES, json))\n+        .dvs(CounterResultParser.fromJson(ScanMetrics.DVS, json))\n         .build();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\nindex 1ba891f58474..6e6aa25636bd 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n@@ -21,6 +21,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanMetricsUtil {\n@@ -31,7 +32,11 @@ public static void indexedDeleteFile(ScanMetrics metrics, DeleteFile deleteFile)\n     metrics.indexedDeleteFiles().increment();\n \n     if (deleteFile.content() == FileContent.POSITION_DELETES) {\n-      metrics.positionalDeleteFiles().increment();\n+      if (ContentFileUtil.isDV(deleteFile)) {\n+        metrics.dvs().increment();\n+      } else {\n+        metrics.positionalDeleteFiles().increment();\n+      }\n     } else if (deleteFile.content() == FileContent.EQUALITY_DELETES) {\n       metrics.equalityDeleteFiles().increment();\n     }\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\nindex 529e0cc614f6..b0b9d003e35b 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n@@ -358,4 +358,76 @@ public void rewriteWithDeletesAndDuplicates() {\n         .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, \""20\"")\n         .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \""1\"");\n   }\n+\n+  @TestTemplate\n+  public void testFileSizeSummaryWithDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    DeleteFile dv1 = newDV(FILE_A);\n+    table.newRowDelta().addDeletes(dv1).commit();\n+\n+    DeleteFile dv2 = newDV(FILE_B);\n+    table.newRowDelta().addDeletes(dv2).commit();\n+\n+    Map<String, String> summary1 = table.currentSnapshot().summary();\n+    long addedPosDeletes1 = dv2.recordCount();\n+    long addedFileSize1 = dv2.contentSizeInBytes();\n+    long totalPosDeletes1 = dv1.recordCount() + dv2.recordCount();\n+    long totalFileSize1 = dv1.contentSizeInBytes() + dv2.contentSizeInBytes();\n+    assertThat(summary1)\n+        .hasSize(12)\n+        .doesNotContainKey(SnapshotSummary.ADD_POS_DELETE_FILES_PROP)\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DELETE_FILES_PROP, \""1\"")\n+        .doesNotContainKey(SnapshotSummary.REMOVED_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DVS_PROP, \""1\"")\n+        .doesNotContainKey(SnapshotSummary.REMOVED_DVS_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_POS_DELETES_PROP, String.valueOf(addedPosDeletes1))\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_FILE_SIZE_PROP, String.valueOf(addedFileSize1))\n+        .doesNotContainKey(SnapshotSummary.REMOVED_FILE_SIZE_PROP)\n+        .containsEntry(SnapshotSummary.TOTAL_DELETE_FILES_PROP, \""2\"")\n+        .containsEntry(SnapshotSummary.TOTAL_POS_DELETES_PROP, String.valueOf(totalPosDeletes1))\n+        .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, String.valueOf(totalFileSize1))\n+        .containsEntry(SnapshotSummary.TOTAL_DATA_FILES_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.TOTAL_EQ_DELETES_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.CHANGED_PARTITION_COUNT_PROP, \""1\"");\n+\n+    DeleteFile dv3 = newDV(FILE_A);\n+    table\n+        .newRowDelta()\n+        .removeDeletes(dv1)\n+        .removeDeletes(dv2)\n+        .addDeletes(dv3)\n+        .validateFromSnapshot(table.currentSnapshot().snapshotId())\n+        .commit();\n+\n+    Map<String, String> summary2 = table.currentSnapshot().summary();\n+    long addedPosDeletes2 = dv3.recordCount();\n+    long removedPosDeletes2 = dv1.recordCount() + dv2.recordCount();\n+    long addedFileSize2 = dv3.contentSizeInBytes();\n+    long removedFileSize2 = dv1.contentSizeInBytes() + dv2.contentSizeInBytes();\n+    long totalPosDeletes2 = dv3.recordCount();\n+    long totalFileSize2 = dv3.contentSizeInBytes();\n+    assertThat(summary2)\n+        .hasSize(16)\n+        .doesNotContainKey(SnapshotSummary.ADD_POS_DELETE_FILES_PROP)\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DELETE_FILES_PROP, \""1\"")\n+        .containsEntry(SnapshotSummary.REMOVED_DELETE_FILES_PROP, \""2\"")\n+        .containsEntry(SnapshotSummary.ADDED_DVS_PROP, \""1\"")\n+        .containsEntry(SnapshotSummary.REMOVED_DVS_PROP, \""2\"")\n+        .containsEntry(SnapshotSummary.ADDED_POS_DELETES_PROP, String.valueOf(addedPosDeletes2))\n+        .containsEntry(SnapshotSummary.REMOVED_POS_DELETES_PROP, String.valueOf(removedPosDeletes2))\n+        .containsEntry(SnapshotSummary.ADDED_FILE_SIZE_PROP, String.valueOf(addedFileSize2))\n+        .containsEntry(SnapshotSummary.REMOVED_FILE_SIZE_PROP, String.valueOf(removedFileSize2))\n+        .containsEntry(SnapshotSummary.TOTAL_DELETE_FILES_PROP, \""1\"")\n+        .containsEntry(SnapshotSummary.TOTAL_POS_DELETES_PROP, String.valueOf(totalPosDeletes2))\n+        .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, String.valueOf(totalFileSize2))\n+        .containsEntry(SnapshotSummary.TOTAL_DATA_FILES_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.TOTAL_EQ_DELETES_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \""0\"")\n+        .containsEntry(SnapshotSummary.CHANGED_PARTITION_COUNT_PROP, \""2\"");\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\nindex 5aa2660143a4..1b51066cf15c 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\n@@ -74,6 +74,8 @@ public void roundTripSerde() {\n             .put(SnapshotSummary.ADDED_DELETE_FILES_PROP, \""4\"")\n             .put(SnapshotSummary.ADD_EQ_DELETE_FILES_PROP, \""5\"")\n             .put(SnapshotSummary.ADD_POS_DELETE_FILES_PROP, \""6\"")\n+            .put(SnapshotSummary.ADDED_DVS_PROP, \""1\"")\n+            .put(SnapshotSummary.REMOVED_DVS_PROP, \""4\"")\n             .put(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP, \""7\"")\n             .put(SnapshotSummary.REMOVED_EQ_DELETE_FILES_PROP, \""8\"")\n             .put(SnapshotSummary.REMOVED_DELETE_FILES_PROP, \""9\"")\n@@ -101,6 +103,8 @@ public void roundTripSerde() {\n     assertThat(result.addedDeleteFiles().value()).isEqualTo(4L);\n     assertThat(result.addedEqualityDeleteFiles().value()).isEqualTo(5L);\n     assertThat(result.addedPositionalDeleteFiles().value()).isEqualTo(6L);\n+    assertThat(result.addedDVs().value()).isEqualTo(1L);\n+    assertThat(result.removedDVs().value()).isEqualTo(4L);\n     assertThat(result.removedPositionalDeleteFiles().value()).isEqualTo(7L);\n     assertThat(result.removedEqualityDeleteFiles().value()).isEqualTo(8L);\n     assertThat(result.removedDeleteFiles().value()).isEqualTo(9L);\n@@ -153,6 +157,10 @@ public void roundTripSerde() {\n             + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""    \\\""value\\\"" : 6\\n\""\n             + \""  },\\n\""\n+            + \""  \\\""added-dvs\\\"" : {\\n\""\n+            + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n+            + \""    \\\""value\\\"" : 1\\n\""\n+            + \""  },\\n\""\n             + \""  \\\""removed-delete-files\\\"" : {\\n\""\n             + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""    \\\""value\\\"" : 9\\n\""\n@@ -161,6 +169,10 @@ public void roundTripSerde() {\n             + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""    \\\""value\\\"" : 7\\n\""\n             + \""  },\\n\""\n+            + \""  \\\""removed-dvs\\\"" : {\\n\""\n+            + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n+            + \""    \\\""value\\\"" : 4\\n\""\n+            + \""  },\\n\""\n             + \""  \\\""removed-equality-delete-files\\\"" : {\\n\""\n             + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""    \\\""value\\\"" : 8\\n\""\n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\nindex 44d5803c4a3a..f5cb1e237307 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\n@@ -178,6 +178,7 @@ public void extraFields() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment();\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     ScanMetricsResult scanMetricsResult = ScanMetricsResult.fromScanMetrics(scanMetrics);\n@@ -199,6 +200,7 @@ public void extraFields() {\n                     + \""\\\""indexed-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":10},\""\n                     + \""\\\""equality-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":4},\""\n                     + \""\\\""positional-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":6},\""\n+                    + \""\\\""dvs\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":1},\""\n                     + \""\\\""extra\\\"": \\\""value\\\"",\\\""extra2\\\"":23}\""))\n         .isEqualTo(scanMetricsResult);\n   }\n@@ -242,6 +244,7 @@ public void roundTripSerde() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment(3L);\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     ScanMetricsResult scanMetricsResult = ScanMetricsResult.fromScanMetrics(scanMetrics);\n@@ -312,6 +315,10 @@ public void roundTripSerde() {\n             + \""  \\\""positional-delete-files\\\"" : {\\n\""\n             + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""    \\\""value\\\"" : 6\\n\""\n+            + \""  },\\n\""\n+            + \""  \\\""dvs\\\"" : {\\n\""\n+            + \""    \\\""unit\\\"" : \\\""count\\\"",\\n\""\n+            + \""    \\\""value\\\"" : 3\\n\""\n             + \""  }\\n\""\n             + \""}\"";\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\nindex 51e21ad9bf01..caac0704cd3f 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\n@@ -84,6 +84,7 @@ public void extraFields() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment();\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     String tableName = \""roundTripTableName\"";\n@@ -118,6 +119,7 @@ public void extraFields() {\n                     + \""\\\""indexed-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":10},\""\n                     + \""\\\""equality-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":4},\""\n                     + \""\\\""positional-delete-files\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":6},\""\n+                    + \""\\\""dvs\\\"":{\\\""unit\\\"":\\\""count\\\"",\\\""value\\\"":1},\""\n                     + \""\\\""extra-metric\\\"":\\\""extra-val\\\""},\""\n                     + \""\\\""extra\\\"":\\\""extraVal\\\""}\""))\n         .isEqualTo(scanReport);\n@@ -279,6 +281,10 @@ public void roundTripSerde() {\n             + \""    \\\""positional-delete-files\\\"" : {\\n\""\n             + \""      \\\""unit\\\"" : \\\""count\\\"",\\n\""\n             + \""      \\\""value\\\"" : 6\\n\""\n+            + \""    },\\n\""\n+            + \""    \\\""dvs\\\"" : {\\n\""\n+            + \""      \\\""unit\\\"" : \\\""count\\\"",\\n\""\n+            + \""      \\\""value\\\"" : 0\\n\""\n             + \""    }\\n\""\n             + \""  }\\n\""\n             + \""}\"";\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11446"", ""pr_id"": 11446, ""issue_id"": 11122, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other"", ""issue_word_count"": 118, ""test_files_count"": 8, ""non_test_files_count"": 28, ""pr_changed_files"": [""api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java"", ""api/src/main/java/org/apache/iceberg/DataFile.java"", ""api/src/main/java/org/apache/iceberg/DeleteFile.java"", ""api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java"", ""api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java"", ""api/src/main/java/org/apache/iceberg/FileFormat.java"", ""api/src/main/java/org/apache/iceberg/FileScanTask.java"", ""api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java"", ""api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java"", ""api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java"", ""core/src/main/java/org/apache/iceberg/BaseFile.java"", ""core/src/main/java/org/apache/iceberg/BaseFileScanTask.java"", ""core/src/main/java/org/apache/iceberg/BaseScan.java"", ""core/src/main/java/org/apache/iceberg/ContentFileParser.java"", ""core/src/main/java/org/apache/iceberg/FileMetadata.java"", ""core/src/main/java/org/apache/iceberg/GenericDataFile.java"", ""core/src/main/java/org/apache/iceberg/GenericDeleteFile.java"", ""core/src/main/java/org/apache/iceberg/ScanSummary.java"", ""core/src/main/java/org/apache/iceberg/SnapshotProducer.java"", ""core/src/main/java/org/apache/iceberg/SnapshotSummary.java"", ""core/src/main/java/org/apache/iceberg/V3Metadata.java"", ""core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java"", ""core/src/main/java/org/apache/iceberg/util/TableScanUtil.java"", ""core/src/test/java/org/apache/iceberg/FileGenerationUtil.java"", ""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestContentFileParser.java"", ""core/src/test/java/org/apache/iceberg/TestManifestEncryption.java"", ""core/src/test/java/org/apache/iceberg/TestManifestReader.java"", ""core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java"", ""core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java"", ""flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java"", ""flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java"", ""flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java"", ""flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java"", ""flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java"", ""flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java""], ""pr_changed_test_files"": [""api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java"", ""core/src/test/java/org/apache/iceberg/FileGenerationUtil.java"", ""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestContentFileParser.java"", ""core/src/test/java/org/apache/iceberg/TestManifestEncryption.java"", ""core/src/test/java/org/apache/iceberg/TestManifestReader.java"", ""core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java"", ""core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java""], ""base_commit"": ""d9b9768766b359adf696f5dc9e321507bd0213d2"", ""head_commit"": ""a3617beedab69511aca4d342b3c3c9f8372f9a2c"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11446"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11446"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-04T18:35:08.000Z"", ""patch"": ""diff --git a/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java b/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\nindex d48b268287c3..506e344d3660 100644\n--- a/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for inserts generated by adding a data file to the table.\n@@ -55,7 +56,7 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(deletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DataFile.java b/api/src/main/java/org/apache/iceberg/DataFile.java\nindex 3c6d77f34d8f..ea6262afac85 100644\n--- a/api/src/main/java/org/apache/iceberg/DataFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DataFile.java\n@@ -104,12 +104,21 @@ public interface DataFile extends ContentFile<DataFile> {\n           \""referenced_data_file\"",\n           StringType.get(),\n           \""Fully qualified location (URI with FS scheme) of a data file that all deletes reference\"");\n+  Types.NestedField CONTENT_OFFSET =\n+      optional(\n+          144, \""content_offset\"", LongType.get(), \""The offset in the file where the content starts\"");\n+  Types.NestedField CONTENT_SIZE =\n+      optional(\n+          145,\n+          \""content_size_in_bytes\"",\n+          LongType.get(),\n+          \""The length of referenced content stored in the file\"");\n \n   int PARTITION_ID = 102;\n   String PARTITION_NAME = \""partition\"";\n   String PARTITION_DOC = \""Partition data tuple, schema based on the partition spec\"";\n \n-  // NEXT ID TO ASSIGN: 144\n+  // NEXT ID TO ASSIGN: 146\n \n   static StructType getType(StructType partitionType) {\n     // IDs start at 100 to leave room for changes to ManifestEntry\n@@ -131,7 +140,9 @@ static StructType getType(StructType partitionType) {\n         SPLIT_OFFSETS,\n         EQUALITY_IDS,\n         SORT_ORDER_ID,\n-        REFERENCED_DATA_FILE);\n+        REFERENCED_DATA_FILE,\n+        CONTENT_OFFSET,\n+        CONTENT_SIZE);\n   }\n \n   /**\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeleteFile.java b/api/src/main/java/org/apache/iceberg/DeleteFile.java\nindex 8e17e60fcccf..340a00e36b17 100644\n--- a/api/src/main/java/org/apache/iceberg/DeleteFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DeleteFile.java\n@@ -42,4 +42,26 @@ default List<Long> splitOffsets() {\n   default String referencedDataFile() {\n     return null;\n   }\n+\n+  /**\n+   * Returns the offset in the file where the content starts.\n+   *\n+   * <p>The content offset is required for deletion vectors and points to the start of the deletion\n+   * vector blob in the Puffin file, enabling direct access. This method always returns null for\n+   * equality and position delete files.\n+   */\n+  default Long contentOffset() {\n+    return null;\n+  }\n+\n+  /**\n+   * Returns the length of referenced content stored in the file.\n+   *\n+   * <p>The content size is required for deletion vectors and indicates the size of the deletion\n+   * vector blob in the Puffin file, enabling direct access. This method always returns null for\n+   * equality and position delete files.\n+   */\n+  default Long contentSizeInBytes() {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java b/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\nindex 9edd6afd0cea..4b9c1704b9d2 100644\n--- a/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for deletes generated by removing a data file from the table.\n@@ -54,7 +55,7 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + existingDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(existingDeletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java b/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\nindex 131edfddd349..1e0a52a53241 100644\n--- a/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for deletes generated by adding delete files to the table.\n@@ -63,9 +64,9 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length()\n-        + addedDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum()\n-        + existingDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    long addedDeletesSize = ScanTaskUtil.contentSizeInBytes(addedDeletes());\n+    long existingDeletesSize = ScanTaskUtil.contentSizeInBytes(existingDeletes());\n+    return length() + addedDeletesSize + existingDeletesSize;\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/FileFormat.java b/api/src/main/java/org/apache/iceberg/FileFormat.java\nindex d662437d5ddb..6b41aec42c3e 100644\n--- a/api/src/main/java/org/apache/iceberg/FileFormat.java\n+++ b/api/src/main/java/org/apache/iceberg/FileFormat.java\n@@ -24,6 +24,7 @@\n \n /** Enum of supported file formats. */\n public enum FileFormat {\n+  PUFFIN(\""puffin\"", false),\n   ORC(\""orc\"", true),\n   PARQUET(\""parquet\"", true),\n   AVRO(\""avro\"", true),\n\ndiff --git a/api/src/main/java/org/apache/iceberg/FileScanTask.java b/api/src/main/java/org/apache/iceberg/FileScanTask.java\nindex 5fb4b55459e3..94f153e56052 100644\n--- a/api/src/main/java/org/apache/iceberg/FileScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/FileScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /** A scan task over a range of bytes in a single data file. */\n public interface FileScanTask extends ContentScanTask<DataFile>, SplittableScanTask<FileScanTask> {\n@@ -36,7 +37,7 @@ default Schema schema() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(deletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java b/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\nindex bbe9824963fc..06ddd1869ace 100644\n--- a/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\n+++ b/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\n@@ -97,13 +97,14 @@ public boolean equals(Object o) {\n       }\n \n       DeleteFileWrapper that = (DeleteFileWrapper) o;\n-      // this needs to be updated once deletion vector support is added\n-      return Objects.equals(file.location(), that.file.location());\n+      return Objects.equals(file.location(), that.file.location())\n+          && Objects.equals(file.contentOffset(), that.file.contentOffset())\n+          && Objects.equals(file.contentSizeInBytes(), that.file.contentSizeInBytes());\n     }\n \n     @Override\n     public int hashCode() {\n-      return Objects.hashCode(file.location());\n+      return Objects.hash(file.location(), file.contentOffset(), file.contentSizeInBytes());\n     }\n \n     @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java b/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java\nnew file mode 100644\nindex 000000000000..276aae6e2caf\n--- /dev/null\n+++ b/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java\n@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import org.apache.iceberg.ContentFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+\n+public class ScanTaskUtil {\n+\n+  private ScanTaskUtil() {}\n+\n+  public static long contentSizeInBytes(ContentFile<?> file) {\n+    if (file.content() == FileContent.DATA) {\n+      return file.fileSizeInBytes();\n+    } else {\n+      DeleteFile deleteFile = (DeleteFile) file;\n+      return isDV(deleteFile) ? deleteFile.contentSizeInBytes() : deleteFile.fileSizeInBytes();\n+    }\n+  }\n+\n+  public static long contentSizeInBytes(Iterable<? extends ContentFile<?>> files) {\n+    long size = 0L;\n+    for (ContentFile<?> file : files) {\n+      size += contentSizeInBytes(file);\n+    }\n+    return size;\n+  }\n+\n+  private static boolean isDV(DeleteFile deleteFile) {\n+    return deleteFile.format() == FileFormat.PUFFIN;\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFile.java b/core/src/main/java/org/apache/iceberg/BaseFile.java\nindex f4fd94724e95..e9724637dfa3 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFile.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFile.java\n@@ -81,6 +81,8 @@ public PartitionData copy() {\n   private byte[] keyMetadata = null;\n   private Integer sortOrderId;\n   private String referencedDataFile = null;\n+  private Long contentOffset = null;\n+  private Long contentSizeInBytes = null;\n \n   // cached schema\n   private transient Schema avroSchema = null;\n@@ -110,6 +112,8 @@ public PartitionData copy() {\n           DataFile.EQUALITY_IDS,\n           DataFile.SORT_ORDER_ID,\n           DataFile.REFERENCED_DATA_FILE,\n+          DataFile.CONTENT_OFFSET,\n+          DataFile.CONTENT_SIZE,\n           MetadataColumns.ROW_POSITION);\n \n   /** Used by Avro reflection to instantiate this class when reading manifest files. */\n@@ -152,7 +156,9 @@ public PartitionData copy() {\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n       ByteBuffer keyMetadata,\n-      String referencedDataFile) {\n+      String referencedDataFile,\n+      Long contentOffset,\n+      Long contentSizeInBytes) {\n     super(BASE_TYPE.fields().size());\n     this.partitionSpecId = specId;\n     this.content = content;\n@@ -182,6 +188,8 @@ public PartitionData copy() {\n     this.sortOrderId = sortOrderId;\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n     this.referencedDataFile = referencedDataFile;\n+    this.contentOffset = contentOffset;\n+    this.contentSizeInBytes = contentSizeInBytes;\n   }\n \n   /**\n@@ -235,6 +243,8 @@ public PartitionData copy() {\n     this.dataSequenceNumber = toCopy.dataSequenceNumber;\n     this.fileSequenceNumber = toCopy.fileSequenceNumber;\n     this.referencedDataFile = toCopy.referencedDataFile;\n+    this.contentOffset = toCopy.contentOffset;\n+    this.contentSizeInBytes = toCopy.contentSizeInBytes;\n   }\n \n   /** Constructor for Java serialization. */\n@@ -347,6 +357,12 @@ protected <T> void internalSet(int pos, T value) {\n         this.referencedDataFile = value != null ? value.toString() : null;\n         return;\n       case 18:\n+        this.contentOffset = (Long) value;\n+        return;\n+      case 19:\n+        this.contentSizeInBytes = (Long) value;\n+        return;\n+      case 20:\n         this.fileOrdinal = (long) value;\n         return;\n       default:\n@@ -398,6 +414,10 @@ private Object getByPos(int basePos) {\n       case 17:\n         return referencedDataFile;\n       case 18:\n+        return contentOffset;\n+      case 19:\n+        return contentSizeInBytes;\n+      case 20:\n         return fileOrdinal;\n       default:\n         throw new UnsupportedOperationException(\""Unknown field ordinal: \"" + basePos);\n@@ -528,6 +548,14 @@ public String referencedDataFile() {\n     return referencedDataFile;\n   }\n \n+  public Long contentOffset() {\n+    return contentOffset;\n+  }\n+\n+  public Long contentSizeInBytes() {\n+    return contentSizeInBytes;\n+  }\n+\n   private static <K, V> Map<K, V> copyMap(Map<K, V> map, Set<K> keys) {\n     return keys == null ? SerializableMap.copyOf(map) : SerializableMap.filteredCopyOf(map, keys);\n   }\n@@ -580,6 +608,8 @@ public String toString() {\n         .add(\""data_sequence_number\"", dataSequenceNumber == null ? \""null\"" : dataSequenceNumber)\n         .add(\""file_sequence_number\"", fileSequenceNumber == null ? \""null\"" : fileSequenceNumber)\n         .add(\""referenced_data_file\"", referencedDataFile == null ? \""null\"" : referencedDataFile)\n+        .add(\""content_offset\"", contentOffset == null ? \""null\"" : contentOffset)\n+        .add(\""content_size_in_bytes\"", contentSizeInBytes == null ? \""null\"" : contentSizeInBytes)\n         .toString();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\nindex 2469395021d4..aa37f40be7c0 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n@@ -23,6 +23,7 @@\n import org.apache.iceberg.expressions.ResidualEvaluator;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class BaseFileScanTask extends BaseContentScanTask<FileScanTask, DataFile>\n     implements FileScanTask {\n@@ -79,7 +80,7 @@ private long deletesSizeBytes() {\n     if (deletesSizeBytes == 0L && deletes.length > 0) {\n       long size = 0L;\n       for (DeleteFile deleteFile : deletes) {\n-        size += deleteFile.fileSizeInBytes();\n+        size += ScanTaskUtil.contentSizeInBytes(deleteFile);\n       }\n       this.deletesSizeBytes = size;\n     }\n@@ -180,11 +181,7 @@ public SplitScanTask merge(ScanTask other) {\n \n     private long deletesSizeBytes() {\n       if (deletesSizeBytes == 0L && fileScanTask.filesCount() > 1) {\n-        long size = 0L;\n-        for (DeleteFile deleteFile : fileScanTask.deletes()) {\n-          size += deleteFile.fileSizeInBytes();\n-        }\n-        this.deletesSizeBytes = size;\n+        this.deletesSizeBytes = ScanTaskUtil.contentSizeInBytes(fileScanTask.deletes());\n       }\n \n       return deletesSizeBytes;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseScan.java b/core/src/main/java/org/apache/iceberg/BaseScan.java\nindex a011d03d59ad..618b2e95f29f 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseScan.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseScan.java\n@@ -78,6 +78,8 @@ abstract class BaseScan<ThisT, T extends ScanTask, G extends ScanTaskGroup<T>>\n           \""key_metadata\"",\n           \""split_offsets\"",\n           \""referenced_data_file\"",\n+          \""content_offset\"",\n+          \""content_size_in_bytes\"",\n           \""equality_ids\"");\n \n   protected static final List<String> DELETE_SCAN_WITH_STATS_COLUMNS =\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ContentFileParser.java b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\nindex 96dfa5586c31..e6d7c8043f3f 100644\n--- a/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n+++ b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n@@ -46,6 +46,8 @@ class ContentFileParser {\n   private static final String EQUALITY_IDS = \""equality-ids\"";\n   private static final String SORT_ORDER_ID = \""sort-order-id\"";\n   private static final String REFERENCED_DATA_FILE = \""referenced-data-file\"";\n+  private static final String CONTENT_OFFSET = \""content-offset\"";\n+  private static final String CONTENT_SIZE = \""content-size-in-bytes\"";\n \n   private ContentFileParser() {}\n \n@@ -116,6 +118,14 @@ static void toJson(ContentFile<?> contentFile, PartitionSpec spec, JsonGenerator\n       if (deleteFile.referencedDataFile() != null) {\n         generator.writeStringField(REFERENCED_DATA_FILE, deleteFile.referencedDataFile());\n       }\n+\n+      if (deleteFile.contentOffset() != null) {\n+        generator.writeNumberField(CONTENT_OFFSET, deleteFile.contentOffset());\n+      }\n+\n+      if (deleteFile.contentSizeInBytes() != null) {\n+        generator.writeNumberField(CONTENT_SIZE, deleteFile.contentSizeInBytes());\n+      }\n     }\n \n     generator.writeEndObject();\n@@ -155,6 +165,8 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n     int[] equalityFieldIds = JsonUtil.getIntArrayOrNull(EQUALITY_IDS, jsonNode);\n     Integer sortOrderId = JsonUtil.getIntOrNull(SORT_ORDER_ID, jsonNode);\n     String referencedDataFile = JsonUtil.getStringOrNull(REFERENCED_DATA_FILE, jsonNode);\n+    Long contentOffset = JsonUtil.getLongOrNull(CONTENT_OFFSET, jsonNode);\n+    Long contentSizeInBytes = JsonUtil.getLongOrNull(CONTENT_SIZE, jsonNode);\n \n     if (fileContent == FileContent.DATA) {\n       return new GenericDataFile(\n@@ -180,7 +192,9 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n           sortOrderId,\n           splitOffsets,\n           keyMetadata,\n-          referencedDataFile);\n+          referencedDataFile,\n+          contentOffset,\n+          contentSizeInBytes);\n     }\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/FileMetadata.java b/core/src/main/java/org/apache/iceberg/FileMetadata.java\nindex ef229593bcab..7bb8d886dd16 100644\n--- a/core/src/main/java/org/apache/iceberg/FileMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/FileMetadata.java\n@@ -60,6 +60,8 @@ public static class Builder {\n     private Integer sortOrderId = null;\n     private List<Long> splitOffsets = null;\n     private String referencedDataFile = null;\n+    private Long contentOffset = null;\n+    private Long contentSizeInBytes = null;\n \n     Builder(PartitionSpec spec) {\n       this.spec = spec;\n@@ -230,6 +232,16 @@ public Builder withReferencedDataFile(CharSequence newReferencedDataFile) {\n       return this;\n     }\n \n+    public Builder withContentOffset(long newContentOffset) {\n+      this.contentOffset = newContentOffset;\n+      return this;\n+    }\n+\n+    public Builder withContentSizeInBytes(long newContentSizeInBytes) {\n+      this.contentSizeInBytes = newContentSizeInBytes;\n+      return this;\n+    }\n+\n     public DeleteFile build() {\n       Preconditions.checkArgument(filePath != null, \""File path is required\"");\n       if (format == null) {\n@@ -240,6 +252,15 @@ public DeleteFile build() {\n       Preconditions.checkArgument(fileSizeInBytes >= 0, \""File size is required\"");\n       Preconditions.checkArgument(recordCount >= 0, \""Record count is required\"");\n \n+      if (format == FileFormat.PUFFIN) {\n+        Preconditions.checkArgument(contentOffset != null, \""Content offset is required for DV\"");\n+        Preconditions.checkArgument(contentSizeInBytes != null, \""Content size is required for DV\"");\n+      } else {\n+        Preconditions.checkArgument(contentOffset == null, \""Content offset can only be set for DV\"");\n+        Preconditions.checkArgument(\n+            contentSizeInBytes == null, \""Content size can only be set for DV\"");\n+      }\n+\n       switch (content) {\n         case POSITION_DELETES:\n           Preconditions.checkArgument(\n@@ -273,7 +294,9 @@ public DeleteFile build() {\n           sortOrderId,\n           splitOffsets,\n           keyMetadata,\n-          referencedDataFile);\n+          referencedDataFile,\n+          contentOffset,\n+          contentSizeInBytes);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDataFile.java b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\nindex aa34cd22cdaa..a61cc1e0fb72 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n@@ -65,7 +65,9 @@ class GenericDataFile extends BaseFile<DataFile> implements DataFile {\n         null /* no equality field IDs */,\n         sortOrderId,\n         keyMetadata,\n-        null /* no referenced data file */);\n+        null /* no referenced data file */,\n+        null /* no content offset */,\n+        null /* no content size */);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\nindex 05eb7c97dbab..9205551f24b3 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n@@ -49,7 +49,9 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n       Integer sortOrderId,\n       List<Long> splitOffsets,\n       ByteBuffer keyMetadata,\n-      String referencedDataFile) {\n+      String referencedDataFile,\n+      Long contentOffset,\n+      Long contentSizeInBytes) {\n     super(\n         specId,\n         content,\n@@ -68,7 +70,9 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n         equalityFieldIds,\n         sortOrderId,\n         keyMetadata,\n-        referencedDataFile);\n+        referencedDataFile,\n+        contentOffset,\n+        contentSizeInBytes);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ScanSummary.java b/core/src/main/java/org/apache/iceberg/ScanSummary.java\nindex 1ea171c5b2c3..5f8e66c0b450 100644\n--- a/core/src/main/java/org/apache/iceberg/ScanSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/ScanSummary.java\n@@ -47,6 +47,7 @@\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.DateTimeUtil;\n import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanSummary {\n   private ScanSummary() {}\n@@ -294,7 +295,7 @@ PartitionMetrics updateFromCounts(\n     private PartitionMetrics updateFromFile(ContentFile<?> file, Long timestampMillis) {\n       this.fileCount += 1;\n       this.recordCount += file.recordCount();\n-      this.totalSize += file.fileSizeInBytes();\n+      this.totalSize += ScanTaskUtil.contentSizeInBytes(file);\n       if (timestampMillis != null\n           && (dataTimestampMillis == null || dataTimestampMillis < timestampMillis)) {\n         this.dataTimestampMillis = timestampMillis;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\nindex daf1c3d72b89..45b71d654344 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n@@ -928,5 +928,15 @@ public Integer sortOrderId() {\n     public String referencedDataFile() {\n       return deleteFile.referencedDataFile();\n     }\n+\n+    @Override\n+    public Long contentOffset() {\n+      return deleteFile.contentOffset();\n+    }\n+\n+    @Override\n+    public Long contentSizeInBytes() {\n+      return deleteFile.contentSizeInBytes();\n+    }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\nindex 22c9df2a8eaf..ad832a5e78e2 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class SnapshotSummary {\n   public static final String ADDED_FILES_PROP = \""added-data-files\"";\n@@ -275,7 +276,7 @@ void addTo(ImmutableMap.Builder<String, String> builder) {\n     }\n \n     void addedFile(ContentFile<?> file) {\n-      this.addedSize += file.fileSizeInBytes();\n+      this.addedSize += ScanTaskUtil.contentSizeInBytes(file);\n       switch (file.content()) {\n         case DATA:\n           this.addedFiles += 1;\n@@ -298,7 +299,7 @@ void addedFile(ContentFile<?> file) {\n     }\n \n     void removedFile(ContentFile<?> file) {\n-      this.removedSize += file.fileSizeInBytes();\n+      this.removedSize += ScanTaskUtil.contentSizeInBytes(file);\n       switch (file.content()) {\n         case DATA:\n           this.removedFiles += 1;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V3Metadata.java b/core/src/main/java/org/apache/iceberg/V3Metadata.java\nindex a418a868564e..70461ac74a70 100644\n--- a/core/src/main/java/org/apache/iceberg/V3Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V3Metadata.java\n@@ -275,7 +275,9 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n         DataFile.SORT_ORDER_ID,\n-        DataFile.REFERENCED_DATA_FILE);\n+        DataFile.REFERENCED_DATA_FILE,\n+        DataFile.CONTENT_OFFSET,\n+        DataFile.CONTENT_SIZE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -455,6 +457,18 @@ public Object get(int pos) {\n           } else {\n             return null;\n           }\n+        case 17:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).contentOffset();\n+          } else {\n+            return null;\n+          }\n+        case 18:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).contentSizeInBytes();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\""Unknown field ordinal: \"" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\nindex c5aa6e1dd673..1ba891f58474 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n@@ -21,6 +21,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanMetricsUtil {\n \n@@ -43,7 +44,7 @@ public static void fileTask(ScanMetrics metrics, DataFile dataFile, DeleteFile[]\n \n     long deletesSizeInBytes = 0L;\n     for (DeleteFile deleteFile : deleteFiles) {\n-      deletesSizeInBytes += deleteFile.fileSizeInBytes();\n+      deletesSizeInBytes += ScanTaskUtil.contentSizeInBytes(deleteFile);\n     }\n \n     metrics.totalDeleteFileSizeInBytes().increment(deletesSizeInBytes);\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java b/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\nindex e2dbcb61e9b7..2d80e88ae328 100644\n--- a/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\n@@ -25,7 +25,6 @@\n import org.apache.iceberg.BaseCombinedScanTask;\n import org.apache.iceberg.BaseScanTaskGroup;\n import org.apache.iceberg.CombinedScanTask;\n-import org.apache.iceberg.ContentFile;\n import org.apache.iceberg.FileContent;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MergeableScanTask;\n@@ -92,8 +91,7 @@ public static CloseableIterable<CombinedScanTask> planTasks(\n     Function<FileScanTask, Long> weightFunc =\n         file ->\n             Math.max(\n-                file.length()\n-                    + file.deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum(),\n+                file.length() + ScanTaskUtil.contentSizeInBytes(file.deletes()),\n                 (1 + file.deletes().size()) * openFileCost);\n \n     return CloseableIterable.transform(\n\ndiff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n"", ""test_patch"": ""diff --git a/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java b/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java\nnew file mode 100644\nindex 000000000000..a449cf20a65b\n--- /dev/null\n+++ b/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java\n@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+public class TestScanTaskUtil {\n+\n+  @Test\n+  public void testContentSize() {\n+    DeleteFile dv1 = mockDV(\""dv1.puffin\"", 20L, 25L, \""data1.parquet\"");\n+    DeleteFile dv2 = mockDV(\""dv2.puffin\"", 4L, 15L, \""data2.parquet\"");\n+\n+    long size1 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of());\n+    assertThat(size1).isEqualTo(0);\n+\n+    long size2 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of(dv1));\n+    assertThat(size2).isEqualTo(25L);\n+\n+    long size3 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of(dv1, dv2));\n+    assertThat(size3).isEqualTo(40L);\n+  }\n+\n+  private static DeleteFile mockDV(\n+      String location, long contentOffset, long contentSize, String referencedDataFile) {\n+    DeleteFile mockFile = Mockito.mock(DeleteFile.class);\n+    Mockito.when(mockFile.format()).thenReturn(FileFormat.PUFFIN);\n+    Mockito.when(mockFile.location()).thenReturn(location);\n+    Mockito.when(mockFile.contentOffset()).thenReturn(contentOffset);\n+    Mockito.when(mockFile.contentSizeInBytes()).thenReturn(contentSize);\n+    Mockito.when(mockFile.referencedDataFile()).thenReturn(referencedDataFile);\n+    return mockFile;\n+  }\n+}\n\ndiff --git a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\nindex f66496ae6624..e1c8ce9ccfed 100644\n--- a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n@@ -101,6 +101,24 @@ public static DeleteFile generateEqualityDeleteFile(Table table, StructLike part\n         .build();\n   }\n \n+  public static DeleteFile generateDV(Table table, DataFile dataFile) {\n+    PartitionSpec spec = table.specs().get(dataFile.specId());\n+    long fileSize = generateFileSize();\n+    long cardinality = generateRowCount();\n+    long offset = generateContentOffset();\n+    long length = generateContentLength();\n+    return FileMetadata.deleteFileBuilder(spec)\n+        .ofPositionDeletes()\n+        .withPath(\""/path/to/delete-\"" + UUID.randomUUID() + \"".puffin\"")\n+        .withFileSizeInBytes(fileSize)\n+        .withPartition(dataFile.partition())\n+        .withRecordCount(cardinality)\n+        .withReferencedDataFile(dataFile.location())\n+        .withContentOffset(offset)\n+        .withContentSizeInBytes(length)\n+        .build();\n+  }\n+\n   public static DeleteFile generatePositionDeleteFile(Table table, DataFile dataFile) {\n     PartitionSpec spec = table.spec();\n     StructLike partition = dataFile.partition();\n@@ -229,6 +247,14 @@ private static long generateFileSize() {\n     return random().nextInt(50_000);\n   }\n \n+  private static long generateContentOffset() {\n+    return random().nextInt(1_000_000);\n+  }\n+\n+  private static long generateContentLength() {\n+    return random().nextInt(10_000);\n+  }\n+\n   private static Pair<ByteBuffer, ByteBuffer> generateBounds(PrimitiveType type, MetricsMode mode) {\n     Comparator<Object> cmp = Comparators.forType(type);\n     Object value1 = generateBound(type, mode);\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex 45441631900c..9813d02910a6 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -666,6 +666,10 @@ protected DeleteFile newDeleteFileWithRef(DataFile dataFile) {\n         .build();\n   }\n \n+  protected DeleteFile newDV(DataFile dataFile) {\n+    return FileGenerationUtil.generateDV(table, dataFile);\n+  }\n+\n   protected DeleteFile newEqualityDeleteFile(int specId, String partitionPath, int... fieldIds) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\nindex fbe473931659..0c98e8448745 100644\n--- a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n@@ -198,6 +198,7 @@ private static DataFile dataFileWithAllOptional(PartitionSpec spec) {\n \n   private static Stream<Arguments> provideSpecAndDeleteFile() {\n     return Stream.of(\n+        Arguments.of(TestBase.SPEC, dv(TestBase.SPEC), dvJson()),\n         Arguments.of(\n             PartitionSpec.unpartitioned(),\n             deleteFileWithRequiredOnly(PartitionSpec.unpartitioned()),\n@@ -233,7 +234,9 @@ private static DeleteFile deleteFileWithDataRef(PartitionSpec spec) {\n         null,\n         null,\n         null,\n-        \""/path/to/data/file.parquet\"");\n+        \""/path/to/data/file.parquet\"",\n+        null,\n+        null);\n   }\n \n   private static String deleteFileWithDataRefJson() {\n@@ -242,6 +245,32 @@ private static String deleteFileWithDataRefJson() {\n         + \""\\\""record-count\\\"":10,\\\""referenced-data-file\\\"":\\\""/path/to/data/file.parquet\\\""}\"";\n   }\n \n+  private static DeleteFile dv(PartitionSpec spec) {\n+    PartitionData partitionData = new PartitionData(spec.partitionType());\n+    partitionData.set(0, 4);\n+    return new GenericDeleteFile(\n+        spec.specId(),\n+        FileContent.POSITION_DELETES,\n+        \""/path/to/delete.puffin\"",\n+        FileFormat.PUFFIN,\n+        partitionData,\n+        1234,\n+        new Metrics(10L, null, null, null, null),\n+        null,\n+        null,\n+        null,\n+        null,\n+        \""/path/to/data/file.parquet\"",\n+        4L,\n+        40L);\n+  }\n+\n+  private static String dvJson() {\n+    return \""{\\\""spec-id\\\"":0,\\\""content\\\"":\\\""POSITION_DELETES\\\"",\\\""file-path\\\"":\\\""/path/to/delete.puffin\\\"",\""\n+        + \""\\\""file-format\\\"":\\\""PUFFIN\\\"",\\\""partition\\\"":{\\\""1000\\\"":4},\\\""file-size-in-bytes\\\"":1234,\\\""record-count\\\"":10,\""\n+        + \""\\\""referenced-data-file\\\"":\\\""/path/to/data/file.parquet\\\"",\\\""content-offset\\\"":4,\\\""content-size-in-bytes\\\"":40}\"";\n+  }\n+\n   private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n     PartitionData partitionData = null;\n     if (spec.isPartitioned()) {\n@@ -261,6 +290,8 @@ private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n         null,\n         null,\n         null,\n+        null,\n+        null,\n         null);\n   }\n \n@@ -301,6 +332,8 @@ private static DeleteFile deleteFileWithAllOptional(PartitionSpec spec) {\n         1,\n         Collections.singletonList(128L),\n         ByteBuffer.wrap(new byte[16]),\n+        null,\n+        null,\n         null);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\nindex 1f29c0e5b85c..01d38dc129c9 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n@@ -111,6 +111,8 @@ public class TestManifestEncryption {\n           SORT_ORDER_ID,\n           null,\n           CONTENT_KEY_METADATA,\n+          null,\n+          null,\n           null);\n \n   private static final EncryptionManager ENCRYPTION_MANAGER =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestReader.java b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\nindex 4652da943003..63c6779298e0 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n@@ -130,7 +130,7 @@ public void testDataFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DataFile file : reader) {\n         assertThat(file.pos()).as(\""Position should match\"").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(18))\n+        assertThat(((BaseFile) file).get(20))\n             .as(\""Position from field index should match\"")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -158,7 +158,7 @@ public void testDeleteFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DeleteFile file : reader) {\n         assertThat(file.pos()).as(\""Position should match\"").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(18))\n+        assertThat(((BaseFile) file).get(20))\n             .as(\""Position from field index should match\"")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -199,6 +199,30 @@ public void testDeleteFilesWithReferences() throws IOException {\n     }\n   }\n \n+  @TestTemplate\n+  public void testDVs() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+    DeleteFile dv1 = newDV(FILE_A);\n+    DeleteFile dv2 = newDV(FILE_B);\n+    ManifestFile manifest = writeDeleteManifest(formatVersion, 1000L, dv1, dv2);\n+    try (ManifestReader<DeleteFile> reader =\n+        ManifestFiles.readDeleteManifest(manifest, FILE_IO, table.specs())) {\n+      for (DeleteFile dv : reader) {\n+        if (dv.location().equals(dv1.location())) {\n+          assertThat(dv.location()).isEqualTo(dv1.location());\n+          assertThat(dv.referencedDataFile()).isEqualTo(FILE_A.location());\n+          assertThat(dv.contentOffset()).isEqualTo(dv1.contentOffset());\n+          assertThat(dv.contentSizeInBytes()).isEqualTo(dv1.contentSizeInBytes());\n+        } else {\n+          assertThat(dv.location()).isEqualTo(dv2.location());\n+          assertThat(dv.referencedDataFile()).isEqualTo(FILE_B.location());\n+          assertThat(dv.contentOffset()).isEqualTo(dv2.contentOffset());\n+          assertThat(dv.contentSizeInBytes()).isEqualTo(dv2.contentSizeInBytes());\n+        }\n+      }\n+    }\n+  }\n+\n   @TestTemplate\n   public void testDataFileSplitOffsetsNullWhenInvalid() throws IOException {\n     DataFile invalidOffset =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\nindex 88dcc6ff9ca4..9abe7c426f32 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n@@ -97,6 +97,8 @@ public class TestManifestWriterVersions {\n           SORT_ORDER_ID,\n           null,\n           null,\n+          null,\n+          null,\n           null);\n \n   @TempDir private Path temp;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java b/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\nindex eb713a4d2e0b..8f8343733525 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MergeableScanTask;\n import org.apache.iceberg.MockFileScanTask;\n@@ -74,6 +75,13 @@ private DataFile dataFileWithSize(long size) {\n     return mockFile;\n   }\n \n+  private DeleteFile dvWithSize(long size) {\n+    DeleteFile mockDeleteFile = Mockito.mock(DeleteFile.class);\n+    Mockito.when(mockDeleteFile.format()).thenReturn(FileFormat.PUFFIN);\n+    Mockito.when(mockDeleteFile.contentSizeInBytes()).thenReturn(size);\n+    return mockDeleteFile;\n+  }\n+\n   private DeleteFile[] deleteFilesWithSizes(long... sizes) {\n     return Arrays.stream(sizes)\n         .mapToObj(\n@@ -85,6 +93,14 @@ private DeleteFile[] deleteFilesWithSizes(long... sizes) {\n         .toArray(DeleteFile[]::new);\n   }\n \n+  @Test\n+  public void testFileScanTaskSizeEstimation() {\n+    DataFile dataFile = dataFileWithSize(100L);\n+    DeleteFile dv = dvWithSize(20L);\n+    MockFileScanTask task = new MockFileScanTask(dataFile, new DeleteFile[] {dv});\n+    assertThat(task.sizeBytes()).isEqualTo(120L);\n+  }\n+\n   @Test\n   public void testPlanTaskWithDeleteFiles() {\n     List<FileScanTask> testFiles =\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11443"", ""pr_id"": 11443, ""issue_id"": 11122, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other"", ""issue_word_count"": 118, ""test_files_count"": 5, ""non_test_files_count"": 12, ""pr_changed_files"": [""api/src/main/java/org/apache/iceberg/DataFile.java"", ""api/src/main/java/org/apache/iceberg/DeleteFile.java"", ""core/src/main/java/org/apache/iceberg/BaseFile.java"", ""core/src/main/java/org/apache/iceberg/BaseScan.java"", ""core/src/main/java/org/apache/iceberg/ContentFileParser.java"", ""core/src/main/java/org/apache/iceberg/FileMetadata.java"", ""core/src/main/java/org/apache/iceberg/GenericDataFile.java"", ""core/src/main/java/org/apache/iceberg/GenericDeleteFile.java"", ""core/src/main/java/org/apache/iceberg/SnapshotProducer.java"", ""core/src/main/java/org/apache/iceberg/V2Metadata.java"", ""core/src/main/java/org/apache/iceberg/V3Metadata.java"", ""core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java"", ""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestContentFileParser.java"", ""core/src/test/java/org/apache/iceberg/TestManifestEncryption.java"", ""core/src/test/java/org/apache/iceberg/TestManifestReader.java"", ""core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/TestBase.java"", ""core/src/test/java/org/apache/iceberg/TestContentFileParser.java"", ""core/src/test/java/org/apache/iceberg/TestManifestEncryption.java"", ""core/src/test/java/org/apache/iceberg/TestManifestReader.java"", ""core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java""], ""base_commit"": ""caf424a373fa125d427401acda7079b08abea9de"", ""head_commit"": ""03e60e527a43223142b208f3ca96db8145ab9058"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11443"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11443"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-02T16:23:29.000Z"", ""patch"": ""diff --git a/api/src/main/java/org/apache/iceberg/DataFile.java b/api/src/main/java/org/apache/iceberg/DataFile.java\nindex 02ad0aff3128..3c6d77f34d8f 100644\n--- a/api/src/main/java/org/apache/iceberg/DataFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DataFile.java\n@@ -98,12 +98,18 @@ public interface DataFile extends ContentFile<DataFile> {\n   Types.NestedField SORT_ORDER_ID =\n       optional(140, \""sort_order_id\"", IntegerType.get(), \""Sort order ID\"");\n   Types.NestedField SPEC_ID = optional(141, \""spec_id\"", IntegerType.get(), \""Partition spec ID\"");\n+  Types.NestedField REFERENCED_DATA_FILE =\n+      optional(\n+          143,\n+          \""referenced_data_file\"",\n+          StringType.get(),\n+          \""Fully qualified location (URI with FS scheme) of a data file that all deletes reference\"");\n \n   int PARTITION_ID = 102;\n   String PARTITION_NAME = \""partition\"";\n   String PARTITION_DOC = \""Partition data tuple, schema based on the partition spec\"";\n \n-  // NEXT ID TO ASSIGN: 142\n+  // NEXT ID TO ASSIGN: 144\n \n   static StructType getType(StructType partitionType) {\n     // IDs start at 100 to leave room for changes to ManifestEntry\n@@ -124,7 +130,8 @@ static StructType getType(StructType partitionType) {\n         KEY_METADATA,\n         SPLIT_OFFSETS,\n         EQUALITY_IDS,\n-        SORT_ORDER_ID);\n+        SORT_ORDER_ID,\n+        REFERENCED_DATA_FILE);\n   }\n \n   /**\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeleteFile.java b/api/src/main/java/org/apache/iceberg/DeleteFile.java\nindex 0f8087e6a055..8e17e60fcccf 100644\n--- a/api/src/main/java/org/apache/iceberg/DeleteFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DeleteFile.java\n@@ -31,4 +31,15 @@ public interface DeleteFile extends ContentFile<DeleteFile> {\n   default List<Long> splitOffsets() {\n     return null;\n   }\n+\n+  /**\n+   * Returns the location of a data file that all deletes reference.\n+   *\n+   * <p>The referenced data file is required for deletion vectors and may be optionally captured for\n+   * position delete files that apply to only one data file. This method always returns null for\n+   * equality delete files.\n+   */\n+  default String referencedDataFile() {\n+    return null;\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFile.java b/core/src/main/java/org/apache/iceberg/BaseFile.java\nindex 8f84eb5737b9..f4fd94724e95 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFile.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFile.java\n@@ -80,6 +80,7 @@ public PartitionData copy() {\n   private int[] equalityIds = null;\n   private byte[] keyMetadata = null;\n   private Integer sortOrderId;\n+  private String referencedDataFile = null;\n \n   // cached schema\n   private transient Schema avroSchema = null;\n@@ -108,6 +109,7 @@ public PartitionData copy() {\n           DataFile.SPLIT_OFFSETS,\n           DataFile.EQUALITY_IDS,\n           DataFile.SORT_ORDER_ID,\n+          DataFile.REFERENCED_DATA_FILE,\n           MetadataColumns.ROW_POSITION);\n \n   /** Used by Avro reflection to instantiate this class when reading manifest files. */\n@@ -149,7 +151,8 @@ public PartitionData copy() {\n       List<Long> splitOffsets,\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n-      ByteBuffer keyMetadata) {\n+      ByteBuffer keyMetadata,\n+      String referencedDataFile) {\n     super(BASE_TYPE.fields().size());\n     this.partitionSpecId = specId;\n     this.content = content;\n@@ -178,6 +181,7 @@ public PartitionData copy() {\n     this.equalityIds = equalityFieldIds;\n     this.sortOrderId = sortOrderId;\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+    this.referencedDataFile = referencedDataFile;\n   }\n \n   /**\n@@ -230,6 +234,7 @@ public PartitionData copy() {\n     this.sortOrderId = toCopy.sortOrderId;\n     this.dataSequenceNumber = toCopy.dataSequenceNumber;\n     this.fileSequenceNumber = toCopy.fileSequenceNumber;\n+    this.referencedDataFile = toCopy.referencedDataFile;\n   }\n \n   /** Constructor for Java serialization. */\n@@ -339,6 +344,9 @@ protected <T> void internalSet(int pos, T value) {\n         this.sortOrderId = (Integer) value;\n         return;\n       case 17:\n+        this.referencedDataFile = value != null ? value.toString() : null;\n+        return;\n+      case 18:\n         this.fileOrdinal = (long) value;\n         return;\n       default:\n@@ -388,6 +396,8 @@ private Object getByPos(int basePos) {\n       case 16:\n         return sortOrderId;\n       case 17:\n+        return referencedDataFile;\n+      case 18:\n         return fileOrdinal;\n       default:\n         throw new UnsupportedOperationException(\""Unknown field ordinal: \"" + basePos);\n@@ -514,6 +524,10 @@ public Integer sortOrderId() {\n     return sortOrderId;\n   }\n \n+  public String referencedDataFile() {\n+    return referencedDataFile;\n+  }\n+\n   private static <K, V> Map<K, V> copyMap(Map<K, V> map, Set<K> keys) {\n     return keys == null ? SerializableMap.copyOf(map) : SerializableMap.filteredCopyOf(map, keys);\n   }\n@@ -565,6 +579,7 @@ public String toString() {\n         .add(\""sort_order_id\"", sortOrderId)\n         .add(\""data_sequence_number\"", dataSequenceNumber == null ? \""null\"" : dataSequenceNumber)\n         .add(\""file_sequence_number\"", fileSequenceNumber == null ? \""null\"" : fileSequenceNumber)\n+        .add(\""referenced_data_file\"", referencedDataFile == null ? \""null\"" : referencedDataFile)\n         .toString();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseScan.java b/core/src/main/java/org/apache/iceberg/BaseScan.java\nindex 804df01d31ba..a011d03d59ad 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseScan.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseScan.java\n@@ -77,6 +77,7 @@ abstract class BaseScan<ThisT, T extends ScanTask, G extends ScanTaskGroup<T>>\n           \""partition\"",\n           \""key_metadata\"",\n           \""split_offsets\"",\n+          \""referenced_data_file\"",\n           \""equality_ids\"");\n \n   protected static final List<String> DELETE_SCAN_WITH_STATS_COLUMNS =\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ContentFileParser.java b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\nindex dd08c5c69e7d..96dfa5586c31 100644\n--- a/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n+++ b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n@@ -45,6 +45,7 @@ class ContentFileParser {\n   private static final String SPLIT_OFFSETS = \""split-offsets\"";\n   private static final String EQUALITY_IDS = \""equality-ids\"";\n   private static final String SORT_ORDER_ID = \""sort-order-id\"";\n+  private static final String REFERENCED_DATA_FILE = \""referenced-data-file\"";\n \n   private ContentFileParser() {}\n \n@@ -109,6 +110,14 @@ static void toJson(ContentFile<?> contentFile, PartitionSpec spec, JsonGenerator\n       generator.writeNumberField(SORT_ORDER_ID, contentFile.sortOrderId());\n     }\n \n+    if (contentFile instanceof DeleteFile) {\n+      DeleteFile deleteFile = (DeleteFile) contentFile;\n+\n+      if (deleteFile.referencedDataFile() != null) {\n+        generator.writeStringField(REFERENCED_DATA_FILE, deleteFile.referencedDataFile());\n+      }\n+    }\n+\n     generator.writeEndObject();\n   }\n \n@@ -145,6 +154,7 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n     List<Long> splitOffsets = JsonUtil.getLongListOrNull(SPLIT_OFFSETS, jsonNode);\n     int[] equalityFieldIds = JsonUtil.getIntArrayOrNull(EQUALITY_IDS, jsonNode);\n     Integer sortOrderId = JsonUtil.getIntOrNull(SORT_ORDER_ID, jsonNode);\n+    String referencedDataFile = JsonUtil.getStringOrNull(REFERENCED_DATA_FILE, jsonNode);\n \n     if (fileContent == FileContent.DATA) {\n       return new GenericDataFile(\n@@ -169,7 +179,8 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n           equalityFieldIds,\n           sortOrderId,\n           splitOffsets,\n-          keyMetadata);\n+          keyMetadata,\n+          referencedDataFile);\n     }\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/FileMetadata.java b/core/src/main/java/org/apache/iceberg/FileMetadata.java\nindex 9a201d1b3b6f..ef229593bcab 100644\n--- a/core/src/main/java/org/apache/iceberg/FileMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/FileMetadata.java\n@@ -59,6 +59,7 @@ public static class Builder {\n     private ByteBuffer keyMetadata = null;\n     private Integer sortOrderId = null;\n     private List<Long> splitOffsets = null;\n+    private String referencedDataFile = null;\n \n     Builder(PartitionSpec spec) {\n       this.spec = spec;\n@@ -220,6 +221,15 @@ public Builder withSortOrder(SortOrder newSortOrder) {\n       return this;\n     }\n \n+    public Builder withReferencedDataFile(CharSequence newReferencedDataFile) {\n+      if (newReferencedDataFile != null) {\n+        this.referencedDataFile = newReferencedDataFile.toString();\n+      } else {\n+        this.referencedDataFile = null;\n+      }\n+      return this;\n+    }\n+\n     public DeleteFile build() {\n       Preconditions.checkArgument(filePath != null, \""File path is required\"");\n       if (format == null) {\n@@ -262,7 +272,8 @@ public DeleteFile build() {\n           equalityFieldIds,\n           sortOrderId,\n           splitOffsets,\n-          keyMetadata);\n+          keyMetadata,\n+          referencedDataFile);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDataFile.java b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\nindex 7b99e7b60ab8..aa34cd22cdaa 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n@@ -64,7 +64,8 @@ class GenericDataFile extends BaseFile<DataFile> implements DataFile {\n         splitOffsets,\n         null /* no equality field IDs */,\n         sortOrderId,\n-        keyMetadata);\n+        keyMetadata,\n+        null /* no referenced data file */);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\nindex 77e0d8505af6..05eb7c97dbab 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n@@ -48,7 +48,8 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n       List<Long> splitOffsets,\n-      ByteBuffer keyMetadata) {\n+      ByteBuffer keyMetadata,\n+      String referencedDataFile) {\n     super(\n         specId,\n         content,\n@@ -66,7 +67,8 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n         splitOffsets,\n         equalityFieldIds,\n         sortOrderId,\n-        keyMetadata);\n+        keyMetadata,\n+        referencedDataFile);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\nindex 89f9eab7192a..daf1c3d72b89 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n@@ -923,5 +923,10 @@ public List<Integer> equalityFieldIds() {\n     public Integer sortOrderId() {\n       return deleteFile.sortOrderId();\n     }\n+\n+    @Override\n+    public String referencedDataFile() {\n+      return deleteFile.referencedDataFile();\n+    }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V2Metadata.java b/core/src/main/java/org/apache/iceberg/V2Metadata.java\nindex be4c3734e40b..20b2169b8dad 100644\n--- a/core/src/main/java/org/apache/iceberg/V2Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V2Metadata.java\n@@ -274,7 +274,8 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.KEY_METADATA,\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n-        DataFile.SORT_ORDER_ID);\n+        DataFile.SORT_ORDER_ID,\n+        DataFile.REFERENCED_DATA_FILE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -448,6 +449,12 @@ public Object get(int pos) {\n           return wrapped.equalityFieldIds();\n         case 15:\n           return wrapped.sortOrderId();\n+        case 16:\n+          if (wrapped instanceof DeleteFile) {\n+            return ((DeleteFile) wrapped).referencedDataFile();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\""Unknown field ordinal: \"" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V3Metadata.java b/core/src/main/java/org/apache/iceberg/V3Metadata.java\nindex f295af3e109d..a418a868564e 100644\n--- a/core/src/main/java/org/apache/iceberg/V3Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V3Metadata.java\n@@ -274,7 +274,8 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.KEY_METADATA,\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n-        DataFile.SORT_ORDER_ID);\n+        DataFile.SORT_ORDER_ID,\n+        DataFile.REFERENCED_DATA_FILE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -448,6 +449,12 @@ public Object get(int pos) {\n           return wrapped.equalityFieldIds();\n         case 15:\n           return wrapped.sortOrderId();\n+        case 16:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).referencedDataFile();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\""Unknown field ordinal: \"" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\nindex 04fc077d10ea..c82b3ff828cf 100644\n--- a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n@@ -54,6 +54,10 @@ public static CharSequence referencedDataFile(DeleteFile deleteFile) {\n       return null;\n     }\n \n+    if (deleteFile.referencedDataFile() != null) {\n+      return deleteFile.referencedDataFile();\n+    }\n+\n     int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n     Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n \n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex f3bbb7979547..45441631900c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -654,6 +654,18 @@ protected DeleteFile newDeleteFile(int specId, String partitionPath) {\n         .build();\n   }\n \n+  protected DeleteFile newDeleteFileWithRef(DataFile dataFile) {\n+    PartitionSpec spec = table.specs().get(dataFile.specId());\n+    return FileMetadata.deleteFileBuilder(spec)\n+        .ofPositionDeletes()\n+        .withPath(\""/path/to/delete-\"" + UUID.randomUUID() + \"".parquet\"")\n+        .withFileSizeInBytes(10)\n+        .withPartition(dataFile.partition())\n+        .withReferencedDataFile(dataFile.location())\n+        .withRecordCount(1)\n+        .build();\n+  }\n+\n   protected DeleteFile newEqualityDeleteFile(int specId, String partitionPath, int... fieldIds) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\nindex 83f7fc1f6220..fbe473931659 100644\n--- a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n@@ -213,7 +213,33 @@ private static Stream<Arguments> provideSpecAndDeleteFile() {\n         Arguments.of(\n             TestBase.SPEC,\n             deleteFileWithAllOptional(TestBase.SPEC),\n-            deleteFileJsonWithAllOptional(TestBase.SPEC)));\n+            deleteFileJsonWithAllOptional(TestBase.SPEC)),\n+        Arguments.of(\n+            TestBase.SPEC, deleteFileWithDataRef(TestBase.SPEC), deleteFileWithDataRefJson()));\n+  }\n+\n+  private static DeleteFile deleteFileWithDataRef(PartitionSpec spec) {\n+    PartitionData partitionData = new PartitionData(spec.partitionType());\n+    partitionData.set(0, 4);\n+    return new GenericDeleteFile(\n+        spec.specId(),\n+        FileContent.POSITION_DELETES,\n+        \""/path/to/delete.parquet\"",\n+        FileFormat.PARQUET,\n+        partitionData,\n+        1234,\n+        new Metrics(10L, null, null, null, null),\n+        null,\n+        null,\n+        null,\n+        null,\n+        \""/path/to/data/file.parquet\"");\n+  }\n+\n+  private static String deleteFileWithDataRefJson() {\n+    return \""{\\\""spec-id\\\"":0,\\\""content\\\"":\\\""POSITION_DELETES\\\"",\\\""file-path\\\"":\\\""/path/to/delete.parquet\\\"",\""\n+        + \""\\\""file-format\\\"":\\\""PARQUET\\\"",\\\""partition\\\"":{\\\""1000\\\"":4},\\\""file-size-in-bytes\\\"":1234,\""\n+        + \""\\\""record-count\\\"":10,\\\""referenced-data-file\\\"":\\\""/path/to/data/file.parquet\\\""}\"";\n   }\n \n   private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n@@ -234,6 +260,7 @@ private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n         null,\n         null,\n         null,\n+        null,\n         null);\n   }\n \n@@ -273,7 +300,8 @@ private static DeleteFile deleteFileWithAllOptional(PartitionSpec spec) {\n         new int[] {3},\n         1,\n         Collections.singletonList(128L),\n-        ByteBuffer.wrap(new byte[16]));\n+        ByteBuffer.wrap(new byte[16]),\n+        null);\n   }\n \n   private static String deleteFileJsonWithRequiredOnly(PartitionSpec spec) {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\nindex 13e8985cdb56..1f29c0e5b85c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n@@ -110,7 +110,8 @@ public class TestManifestEncryption {\n           EQUALITY_ID_ARR,\n           SORT_ORDER_ID,\n           null,\n-          CONTENT_KEY_METADATA);\n+          CONTENT_KEY_METADATA,\n+          null);\n \n   private static final EncryptionManager ENCRYPTION_MANAGER =\n       EncryptionTestHelpers.createEncryptionManager();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestReader.java b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\nindex e45415f1f2d2..4652da943003 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n@@ -130,7 +130,7 @@ public void testDataFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DataFile file : reader) {\n         assertThat(file.pos()).as(\""Position should match\"").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(17))\n+        assertThat(((BaseFile) file).get(18))\n             .as(\""Position from field index should match\"")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -158,7 +158,7 @@ public void testDeleteFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DeleteFile file : reader) {\n         assertThat(file.pos()).as(\""Position should match\"").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(17))\n+        assertThat(((BaseFile) file).get(18))\n             .as(\""Position from field index should match\"")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -181,6 +181,24 @@ public void testDeleteFileManifestPaths() throws IOException {\n     }\n   }\n \n+  @TestTemplate\n+  public void testDeleteFilesWithReferences() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+    DeleteFile deleteFile1 = newDeleteFileWithRef(FILE_A);\n+    DeleteFile deleteFile2 = newDeleteFileWithRef(FILE_B);\n+    ManifestFile manifest = writeDeleteManifest(formatVersion, 1000L, deleteFile1, deleteFile2);\n+    try (ManifestReader<DeleteFile> reader =\n+        ManifestFiles.readDeleteManifest(manifest, FILE_IO, table.specs())) {\n+      for (DeleteFile deleteFile : reader) {\n+        if (deleteFile.location().equals(deleteFile1.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(FILE_A.location());\n+        } else {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(FILE_B.location());\n+        }\n+      }\n+    }\n+  }\n+\n   @TestTemplate\n   public void testDataFileSplitOffsetsNullWhenInvalid() throws IOException {\n     DataFile invalidOffset =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\nindex 1d5c34fa4b16..88dcc6ff9ca4 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n@@ -96,6 +96,7 @@ public class TestManifestWriterVersions {\n           EQUALITY_ID_ARR,\n           SORT_ORDER_ID,\n           null,\n+          null,\n           null);\n \n   @TempDir private Path temp;\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__iceberg-11437"", ""pr_id"": 11437, ""issue_id"": 11435, ""repo"": ""apache/iceberg"", ""problem_statement"": ""Bad Table Properties cause commit failure\n### Apache Iceberg version\n\n1.6.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug \ud83d\udc1e\n\nToday iceberg does not validate the type of table properties values on and this can be problematic if those table properties value are used for commit. \r\n\r\nExample setup\r\n```\r\n                sql(\r\n                    \""CREATE TABLE foo.bar \""\r\n                        + \""(id BIGINT NOT NULL, data STRING) \""\r\n                        + \""USING iceberg \""\r\n                        + \""TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\"",\r\n```\r\n\r\nSee the value for `commit.retry.num-retries` is accidentally set to some non-integer value, we will unable to rectify this through either spark SQL or iceberg API \r\n\r\n```\r\ntable.updateProperties.remove(\""commit.retry.max-wait-ms\"").apply.commit\r\n\r\ntable.updateProperties.remove(\""commit.retry.max-wait-ms\"").commit\r\njava.lang.NumberFormatException: For input string: \""commit.retry.max-wait-ms\""\r\n  at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:668)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:786)\r\n  at org.apache.iceberg.util.PropertyUtil.propertyAsInt(PropertyUtil.java:64)\r\n  at org.apache.iceberg.TableMetadata.propertyAsInt(TableMetadata.java:472)\r\n  at org.apache.iceberg.PropertiesUpdate.commit(PropertiesUpdate.java:105)\r\n  ... 48 elided\r\n```\r\n\r\nAfter some look, I believe this block of code might be the problem: https://github.com/apache/iceberg/blob/main/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java#L100-L114, where  propertyAsInt will throw NumberFormatException before commit can proceed to rectify or remove incorrectly set table properties. \r\n\r\n\r\nCurrent workaround is move back to previous table metadata before the table properties change, but I plan to contribute the patch to fix the problem in 2 ways\r\n1. add validation for new table where commit related table properties need to have value type checked (as integer)\r\n2. relax the condition in `PropertiesUpdate` class to allow update when existing value is corrupted, this help with existing table to move forward. \r\n\r\n\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time"", ""issue_word_count"": 348, ""test_files_count"": 2, ""non_test_files_count"": 3, ""pr_changed_files"": [""core/src/main/java/org/apache/iceberg/PropertiesUpdate.java"", ""core/src/main/java/org/apache/iceberg/TableMetadata.java"", ""core/src/main/java/org/apache/iceberg/util/PropertyUtil.java"", ""core/src/test/java/org/apache/iceberg/TestTransaction.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java""], ""pr_changed_test_files"": [""core/src/test/java/org/apache/iceberg/TestTransaction.java"", ""spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java""], ""base_commit"": ""91e04c9c88b63dc01d6c8e69dfdc8cd27ee811cc"", ""head_commit"": ""38efa71e028a14b77079cdcf8bc9953367950f48"", ""repo_url"": ""https://github.com/apache/iceberg/pull/11437"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__iceberg/11437"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-02T00:22:44.000Z"", ""patch"": ""diff --git a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\nindex 35338a689205..9389aec50c0a 100644\n--- a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n@@ -98,12 +98,13 @@ public Map<String, String> apply() {\n \n   @Override\n   public void commit() {\n+    // If existing table commit properties in base are corrupted, allow rectification\n     Tasks.foreach(ops)\n-        .retry(base.propertyAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n+        .retry(base.propertyTryAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n         .exponentialBackoff(\n-            base.propertyAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n             2.0 /* exponential */)\n         .onlyRetryOn(CommitFailedException.class)\n         .run(\n\ndiff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java\nindex d20dd59d2b97..3cdc53995dce 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java\n@@ -134,6 +134,8 @@ static TableMetadata newTableMetadata(\n     // break existing tables.\n     MetricsConfig.fromProperties(properties).validateReferencedColumns(schema);\n \n+    PropertyUtil.validateCommitProperties(properties);\n+\n     return new Builder()\n         .setInitialFormatVersion(formatVersion)\n         .setCurrentSchema(freshSchema, lastColumnId.get())\n@@ -486,6 +488,10 @@ public int propertyAsInt(String property, int defaultValue) {\n     return PropertyUtil.propertyAsInt(properties, property, defaultValue);\n   }\n \n+  public int propertyTryAsInt(String property, int defaultValue) {\n+    return PropertyUtil.propertyTryAsInt(properties, property, defaultValue);\n+  }\n+\n   public long propertyAsLong(String property, long defaultValue) {\n     return PropertyUtil.propertyAsLong(properties, property, defaultValue);\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\nindex 68c8f3e9efda..633b0a6ae739 100644\n--- a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n@@ -24,10 +24,23 @@\n import java.util.Set;\n import java.util.function.Predicate;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class PropertyUtil {\n+  private static final Logger LOG = LoggerFactory.getLogger(PropertyUtil.class);\n+\n+  private static final Set<String> COMMIT_PROPERTIES =\n+      ImmutableSet.of(\n+          TableProperties.COMMIT_NUM_RETRIES,\n+          TableProperties.COMMIT_MIN_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_MAX_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_TOTAL_RETRY_TIME_MS);\n \n   private PropertyUtil() {}\n \n@@ -57,6 +70,20 @@ public static double propertyAsDouble(\n     return defaultValue;\n   }\n \n+  public static int propertyTryAsInt(\n+      Map<String, String> properties, String property, int defaultValue) {\n+    String value = properties.get(property);\n+    if (value == null) {\n+      return defaultValue;\n+    }\n+    try {\n+      return Integer.parseInt(value);\n+    } catch (NumberFormatException e) {\n+      LOG.warn(\""Failed to parse value of {} as integer, default to {}\"", property, defaultValue, e);\n+      return defaultValue;\n+    }\n+  }\n+\n   public static int propertyAsInt(\n       Map<String, String> properties, String property, int defaultValue) {\n     String value = properties.get(property);\n@@ -100,6 +127,29 @@ public static String propertyAsString(\n     return defaultValue;\n   }\n \n+  /**\n+   * Validate the table commit related properties to have non-negative integer on table creation to\n+   * prevent commit failure\n+   */\n+  public static void validateCommitProperties(Map<String, String> properties) {\n+    for (String commitProperty : COMMIT_PROPERTIES) {\n+      String value = properties.get(commitProperty);\n+      if (value != null) {\n+        int parsedValue;\n+        try {\n+          parsedValue = Integer.parseInt(value);\n+        } catch (NumberFormatException e) {\n+          throw new ValidationException(\n+              \""Table property %s must have integer value\"", commitProperty);\n+        }\n+        ValidationException.check(\n+            parsedValue >= 0,\n+            \""Table property %s must have non negative integer value\"",\n+            commitProperty);\n+      }\n+    }\n+  }\n+\n   /**\n    * Returns subset of provided map with keys matching the provided prefix. Matching is\n    * case-sensitive and the matching prefix is removed from the keys in returned map.\n"", ""test_patch"": ""diff --git a/core/src/test/java/org/apache/iceberg/TestTransaction.java b/core/src/test/java/org/apache/iceberg/TestTransaction.java\nindex 8fed7134fae1..8770e24f8e40 100644\n--- a/core/src/test/java/org/apache/iceberg/TestTransaction.java\n+++ b/core/src/test/java/org/apache/iceberg/TestTransaction.java\n@@ -714,4 +714,22 @@ public void testTransactionRecommit() {\n     assertThat(paths).isEqualTo(expectedPaths);\n     assertThat(table.currentSnapshot().allManifests(table.io())).hasSize(2);\n   }\n+\n+  @TestTemplate\n+  public void testCommitProperties() {\n+    table\n+        .updateProperties()\n+        .set(TableProperties.COMMIT_MAX_RETRY_WAIT_MS, \""foo\"")\n+        .set(TableProperties.COMMIT_NUM_RETRIES, \""bar\"")\n+        .set(TableProperties.COMMIT_TOTAL_RETRY_TIME_MS, Integer.toString(60 * 60 * 1000))\n+        .commit();\n+    table.updateProperties().remove(TableProperties.COMMIT_MAX_RETRY_WAIT_MS).commit();\n+    table.updateProperties().remove(TableProperties.COMMIT_NUM_RETRIES).commit();\n+\n+    assertThat(table.properties())\n+        .doesNotContainKey(TableProperties.COMMIT_NUM_RETRIES)\n+        .doesNotContainKey(TableProperties.COMMIT_MAX_RETRY_WAIT_MS)\n+        .containsEntry(\n+            TableProperties.COMMIT_TOTAL_RETRY_TIME_MS, Integer.toString(60 * 60 * 1000));\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\nindex ae0aa2cda49b..11d4cfebfea6 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.types.Types;\n@@ -348,6 +349,47 @@ public void testCreateTableProperties() {\n     assertThat(table.properties()).containsEntry(\""p1\"", \""2\"").containsEntry(\""p2\"", \""x\"");\n   }\n \n+  @TestTemplate\n+  public void testCreateTableCommitProperties() {\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\""Table should not already exist\"")\n+        .isFalse();\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \""CREATE TABLE %s \""\n+                        + \""(id BIGINT NOT NULL, data STRING) \""\n+                        + \""USING iceberg \""\n+                        + \""TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\"",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\""Table property commit.retry.num-retries must have integer value\"");\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \""CREATE TABLE %s \""\n+                        + \""(id BIGINT NOT NULL, data STRING) \""\n+                        + \""USING iceberg \""\n+                        + \""TBLPROPERTIES ('commit.retry.max-wait-ms'='-1')\"",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\""Table property commit.retry.max-wait-ms must have non negative integer value\"");\n+\n+    sql(\n+        \""CREATE TABLE %s \""\n+            + \""(id BIGINT NOT NULL, data STRING) \""\n+            + \""USING iceberg \""\n+            + \""TBLPROPERTIES ('commit.retry.num-retries'='1', 'commit.retry.max-wait-ms'='3000')\"",\n+        tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    assertThat(table.properties())\n+        .containsEntry(TableProperties.COMMIT_NUM_RETRIES, \""1\"")\n+        .containsEntry(TableProperties.COMMIT_MAX_RETRY_WAIT_MS, \""3000\"");\n+  }\n+\n   @TestTemplate\n   public void testCreateTableWithFormatV2ThroughTableProperty() {\n     assertThat(validationCatalog.tableExists(tableIdent))\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"

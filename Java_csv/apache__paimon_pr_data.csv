metadata
"{""instance_id"": ""apache__paimon-5710"", ""pr_id"": 5710, ""issue_id"": 5706, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Bug] Exception thrown on negative unix timestamp with FlinkCDC writer\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Paimon version\n\nUsing the FlinkCDC writer, if it encounters a negative unix timestamp - i.e a timestamp representing a date before 1970 - it will throw the following exception:\n\n```\nCaused by: java.time.format.DateTimeParseException: Text '-22383318' could not be parsed at index 9\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046) ~[?:?]\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874) ~[?:?]\n\tat org.apache.paimon.utils.DateTimeUtils.parseTimestampData(DateTimeUtils.java:537) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.utils.BinaryStringUtils.toTimestamp(BinaryStringUtils.java:313) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.utils.TypeUtils.castFromStringInternal(TypeUtils.java:163) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.utils.TypeUtils.castFromCdcValueString(TypeUtils.java:97) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.flink.sink.cdc.CdcRecordUtils.projectAsInsert(CdcRecordUtils.java:60) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.flink.sink.cdc.CdcRecordKeyAndBucketExtractor.bucket(CdcRecordKeyAndBucketExtractor.java:97) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.flink.sink.cdc.CdcFixedBucketChannelComputerBase.channel(CdcFixedBucketChannelComputerBase.java:52) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.flink.sink.FlinkStreamPartitioner.selectChannel(FlinkStreamPartitioner.java:48) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.paimon.flink.sink.FlinkStreamPartitioner.selectChannel(FlinkStreamPartitioner.java:32) ~[paimon-flink-1.20-1.1-SNAPSHOT.jar:1.1-SNAPSHOT]\n\tat org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:140) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:120) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:53) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.runtime.tasks.CopyingBroadcastingOutputCollector.collect(CopyingBroadcastingOutputCollector.java:47) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.runtime.tasks.CopyingBroadcastingOutputCollector.collect(CopyingBroadcastingOutputCollector.java:28) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:60) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:32) ~[flink-dist-1.20.1.jar:1.20.1]\n\tat org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:52) ~[flink-dist-1.20.1.jar:1.20.1]\n```\n\nThis is due to StringUtils.isNumeric returning false as soon as it sees the minus sign.\nFurther changes to BinaryStringUtils.fromMillisToTimestamp are required to handle negative nanoseconds.\n\nRelates to #4239 as this exception was also seen there.\n\n### Compute Engine\n\nFlink 1.20\n\n### Minimal reproduce step\n\nCall `BinaryStringUtils.toTimestamp(BinaryString.fromString(\""-123456789\"", 9))` in a unit test.\n\n### What doesn't meet your expectations?\n\nNegative unix timestamps should be parsed correctly rather than an exception thrown.\n\n### Anything else?\n\nI have a PR incoming :)\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 574, ""test_files_count"": 2, ""non_test_files_count"": 2, ""pr_changed_files"": [""paimon-api/src/main/java/org/apache/paimon/utils/StringUtils.java"", ""paimon-common/src/main/java/org/apache/paimon/utils/BinaryStringUtils.java"", ""paimon-common/src/test/java/org/apache/paimon/utils/BinaryStringUtilsTest.java"", ""paimon-common/src/test/java/org/apache/paimon/utils/StringUtilsTest.java""], ""pr_changed_test_files"": [""paimon-common/src/test/java/org/apache/paimon/utils/BinaryStringUtilsTest.java"", ""paimon-common/src/test/java/org/apache/paimon/utils/StringUtilsTest.java""], ""base_commit"": ""c825b37dfbf12a16b9fd5ecd04fa021769e28be5"", ""head_commit"": ""0fa2c7ab510c1e8ac621b459a61977eefe7e1b69"", ""repo_url"": ""https://github.com/apache/paimon/pull/5710"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5710"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-12T05:47:27.000Z"", ""patch"": ""diff --git a/paimon-api/src/main/java/org/apache/paimon/utils/StringUtils.java b/paimon-api/src/main/java/org/apache/paimon/utils/StringUtils.java\nindex 140d0750a5ec..ca32b50b6896 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/utils/StringUtils.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/utils/StringUtils.java\n@@ -515,7 +515,17 @@ public static boolean isNumeric(final CharSequence cs) {\n             return false;\n         }\n         final int sz = cs.length();\n-        for (int i = 0; i < sz; i++) {\n+        int startIndex = 0;\n+\n+        // Handle negative sign\n+        if (sz > 0 && cs.charAt(0) == '-') {\n+            if (sz == 1) {\n+                return false; // Just a minus sign is not numeric\n+            }\n+            startIndex = 1;\n+        }\n+\n+        for (int i = startIndex; i < sz; i++) {\n             if (!Character.isDigit(cs.charAt(i))) {\n                 return false;\n             }\n\ndiff --git a/paimon-common/src/main/java/org/apache/paimon/utils/BinaryStringUtils.java b/paimon-common/src/main/java/org/apache/paimon/utils/BinaryStringUtils.java\nindex 43ab156f1777..ae306c3764ec 100644\n--- a/paimon-common/src/main/java/org/apache/paimon/utils/BinaryStringUtils.java\n+++ b/paimon-common/src/main/java/org/apache/paimon/utils/BinaryStringUtils.java\n@@ -345,6 +345,15 @@ private static Timestamp fromMillisToTimestamp(long epoch, int precision) {\n             default:\n                 throw new RuntimeException(\""Unsupported precision: \"" + precision);\n         }\n+\n+        // If nanoseconds is negative, remove a millisecond\n+        // and calculate the nanosecond offset forwards instead\n+        // as nanoseconds should always be a positive offset on top of the milliseconds.\n+        if (nanosOfMillis < 0) {\n+            nanosOfMillis = 1000000 + nanosOfMillis;\n+            millis -= 1;\n+        }\n+\n         return Timestamp.fromEpochMillis(millis, nanosOfMillis);\n     }\n \n"", ""test_patch"": ""diff --git a/paimon-common/src/test/java/org/apache/paimon/utils/BinaryStringUtilsTest.java b/paimon-common/src/test/java/org/apache/paimon/utils/BinaryStringUtilsTest.java\nnew file mode 100644\nindex 000000000000..b3b360259c4d\n--- /dev/null\n+++ b/paimon-common/src/test/java/org/apache/paimon/utils/BinaryStringUtilsTest.java\n@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.utils;\n+\n+import org.apache.paimon.data.BinaryString;\n+import org.apache.paimon.data.Timestamp;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+import java.time.DateTimeException;\n+import java.util.TimeZone;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+/** Test for {@link BinaryStringUtils}. */\n+class BinaryStringUtilsTest {\n+    @ParameterizedTest\n+    @CsvSource({\n+        \""0, 0, 0, 0\"", // Unix epoch\n+        \""86400, 0, 86400000, 0\"", // One day in seconds\n+        \""3600000, 3, 3600000, 0\"", // One hour in milliseconds\n+        \""3600000000, 6, 3600000, 0\"", // One hour in microseconds\n+        \""3600000000000, 9, 3600000, 0\"", // One hour in nanoseconds\n+        \""1609459200123456789, 9, 1609459200123, 456789\"", // 2021-01-01 00:00:00.123456789 UTC\n+        \""1609459200123456, 6, 1609459200123, 456000\"", // 2021-01-01 00:00:00.123456 UTC\n+        \""1609459200123, 3, 1609459200123, 000000\"", // 2021-01-01 00:00:00 UTC\n+        \""1609459200, 0, 1609459200000, 000000\"", // 2021-01-01 00:00:00 UTC\n+        \""-1, 0, -1000, 0\"", // One second before epoch\n+        \""-1000, 3, -1000, 0\"", // One second before epoch in milliseconds\n+        \""-1000000, 6, -1000, 0\"", // One second before epoch in microseconds\n+        \""-1000000000, 9, -1000, 0\"", // One second before epoch in nanoseconds\n+        // One second and one nanosecond before epoch in nanoseconds\n+        // The negative nanosecond gets flipped and the milliseconds decremented\n+        \""-1000000001, 9, -1001, 999999\"",\n+        \""-86400123456, 6, -86400124, 544000\""\n+    })\n+    void testToTimestamp(String input, int precision, long expectedMillis, int expectedNanos) {\n+        BinaryString binaryInput = BinaryString.fromString(input);\n+        Timestamp result = BinaryStringUtils.toTimestamp(binaryInput, precision);\n+\n+        assertThat(result.getMillisecond()).isEqualTo(expectedMillis);\n+        assertThat(result.getNanoOfMillisecond()).isEqualTo(expectedNanos);\n+    }\n+\n+    @ParameterizedTest\n+    @ValueSource(ints = {1, 2, 4, 5, 7, 8, 10, -1})\n+    void testInvalidPrecisions(int precision) {\n+        BinaryString input = BinaryString.fromString(\""1609459200\"");\n+\n+        assertThatThrownBy(() -> BinaryStringUtils.toTimestamp(input, precision))\n+                .isInstanceOf(RuntimeException.class)\n+                .hasMessageContaining(\""Unsupported precision: \"" + precision);\n+    }\n+\n+    @Test\n+    void testDateStringInput() {\n+        // Test with date string input - should delegate to DateTimeUtils.parseTimestampData\n+        BinaryString input = BinaryString.fromString(\""2021-01-01 12:30:45\"");\n+        Timestamp result = BinaryStringUtils.toTimestamp(input, 3);\n+\n+        // Verify it's not null and has reasonable values\n+        assertThat(result).isNotNull();\n+        assertThat(result.getMillisecond()).isEqualTo(1609504245000L);\n+    }\n+\n+    @Test\n+    void testDateOnlyStringInput() {\n+        // Test with date-only string input\n+        BinaryString input = BinaryString.fromString(\""2021-01-01\"");\n+        Timestamp result = BinaryStringUtils.toTimestamp(input, 3);\n+\n+        assertThat(result).isNotNull();\n+        assertThat(result.getMillisecond()).isEqualTo(1609459200000L);\n+    }\n+\n+    @Test\n+    void testInvalidStringInput() {\n+        // Test with invalid string input\n+        BinaryString input = BinaryString.fromString(\""invalid-date\"");\n+\n+        assertThatThrownBy(() -> BinaryStringUtils.toTimestamp(input, 3))\n+                .isInstanceOf(DateTimeException.class);\n+    }\n+\n+    @Test\n+    void testToTimestampWithTimeZone() {\n+        // Test the timezone variant of toTimestamp method\n+        BinaryString input = BinaryString.fromString(\""2021-01-01 12:30:45\"");\n+        TimeZone timeZone = TimeZone.getTimeZone(\""UTC\"");\n+\n+        Timestamp result = BinaryStringUtils.toTimestamp(input, 3, timeZone);\n+\n+        assertThat(result).isNotNull();\n+        assertThat(result.getMillisecond()).isEqualTo(1609504245000L);\n+    }\n+\n+    @Test\n+    void testToTimestampWithDifferentTimeZones() {\n+        BinaryString input = BinaryString.fromString(\""2021-01-01 12:30:45\"");\n+\n+        Timestamp utcResult = BinaryStringUtils.toTimestamp(input, 3, TimeZone.getTimeZone(\""UTC\""));\n+        Timestamp estResult =\n+                BinaryStringUtils.toTimestamp(input, 3, TimeZone.getTimeZone(\""America/New_York\""));\n+\n+        assertThat(utcResult).isNotNull();\n+        assertThat(estResult).isNotNull();\n+        // The results should be different due to timezone offset\n+        assertThat(utcResult.getMillisecond()).isNotEqualTo(estResult.getMillisecond());\n+    }\n+}\n\ndiff --git a/paimon-common/src/test/java/org/apache/paimon/utils/StringUtilsTest.java b/paimon-common/src/test/java/org/apache/paimon/utils/StringUtilsTest.java\nnew file mode 100644\nindex 000000000000..10b56a97b7eb\n--- /dev/null\n+++ b/paimon-common/src/test/java/org/apache/paimon/utils/StringUtilsTest.java\n@@ -0,0 +1,445 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.utils;\n+\n+import org.junit.jupiter.api.Nested;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+import org.junit.jupiter.params.provider.NullAndEmptySource;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+/** Test for {@link StringUtils}. */\n+class StringUtilsTest {\n+\n+    @Nested\n+    class IsNullOrWhitespaceOnlyTests {\n+\n+        @ParameterizedTest\n+        @NullAndEmptySource\n+        @ValueSource(strings = {\"" \"", \""  \"", \""\\t\"", \""\\n\"", \""\\r\"", \"" \\t\\n\\r \""})\n+        void testNullOrWhitespaceOnlyStrings(String input) {\n+            assertThat(StringUtils.isNullOrWhitespaceOnly(input)).isTrue();\n+        }\n+\n+        @ParameterizedTest\n+        @ValueSource(strings = {\""a\"", \"" a \"", \""hello\"", \""hello world\""})\n+        void testNonWhitespaceStrings(String input) {\n+            assertThat(StringUtils.isNullOrWhitespaceOnly(input)).isFalse();\n+        }\n+    }\n+\n+    @Nested\n+    class ByteToHexStringTests {\n+\n+        @Test\n+        void testByteToHexStringWithRange() {\n+            byte[] bytes = {0x00, 0x0F, (byte) 0xFF, 0x12, 0x34};\n+            String result = StringUtils.byteToHexString(bytes, 1, 4);\n+            assertThat(result).isEqualTo(\""0fff12\"");\n+        }\n+\n+        @Test\n+        void testByteToHexStringFullArray() {\n+            byte[] bytes = {0x00, 0x0F, (byte) 0xFF};\n+            String result = StringUtils.byteToHexString(bytes);\n+            assertThat(result).isEqualTo(\""000fff\"");\n+        }\n+\n+        @Test\n+        void testByteToHexStringEmptyRange() {\n+            byte[] bytes = {0x00, 0x0F, (byte) 0xFF};\n+            String result = StringUtils.byteToHexString(bytes, 1, 1);\n+            assertThat(result).isEmpty();\n+        }\n+\n+        @Test\n+        void testByteToHexStringNullArray() {\n+            assertThatThrownBy(() -> StringUtils.byteToHexString(null, 0, 1))\n+                    .isInstanceOf(IllegalArgumentException.class)\n+                    .hasMessage(\""bytes == null\"");\n+        }\n+\n+        @Test\n+        void testByteToHexStringAllValues() {\n+            byte[] bytes = new byte[256];\n+            for (int i = 0; i < 256; i++) {\n+                bytes[i] = (byte) i;\n+            }\n+            String result = StringUtils.byteToHexString(bytes);\n+            assertThat(result).hasSize(512); // 256 bytes * 2 hex chars each\n+            assertThat(result).startsWith(\""000102\"");\n+            assertThat(result).endsWith(\""fdfeff\"");\n+        }\n+    }\n+\n+    @Nested\n+    class BytesToBinaryStringTests {\n+\n+        @Test\n+        void testBytesToBinaryString() {\n+            byte[] bytes = {0x00, 0x0F, (byte) 0xFF};\n+            String result = StringUtils.bytesToBinaryString(bytes);\n+            assertThat(result).isEqualTo(\""000000000000111111111111\"");\n+        }\n+\n+        @Test\n+        void testBytesToBinaryStringEmptyArray() {\n+            byte[] bytes = {};\n+            String result = StringUtils.bytesToBinaryString(bytes);\n+            assertThat(result).isEmpty();\n+        }\n+\n+        @Test\n+        void testBytesToBinaryStringSingleByte() {\n+            byte[] bytes = {(byte) 0xAA}; // 10101010\n+            String result = StringUtils.bytesToBinaryString(bytes);\n+            assertThat(result).isEqualTo(\""10101010\"");\n+        }\n+    }\n+\n+    @Nested\n+    class GetRandomStringTests {\n+\n+        @Test\n+        void testGetRandomStringWithinRange() {\n+            Random rnd = new Random(42);\n+            String result = StringUtils.getRandomString(rnd, 5, 10);\n+            assertThat(result.length()).isBetween(5, 10);\n+        }\n+\n+        @Test\n+        void testGetRandomStringExactLength() {\n+            Random rnd = new Random(42);\n+            String result = StringUtils.getRandomString(rnd, 7, 7);\n+            assertThat(result).hasSize(7);\n+        }\n+\n+        @Test\n+        void testGetRandomStringWithCharRange() {\n+            Random rnd = new Random(42);\n+            String result = StringUtils.getRandomString(rnd, 10, 10, 'a', 'z');\n+            assertThat(result).hasSize(10);\n+            for (char c : result.toCharArray()) {\n+                assertThat(c).isBetween('a', 'z');\n+            }\n+        }\n+\n+        @Test\n+        void testGetRandomStringMinLength() {\n+            Random rnd = new Random(42);\n+            String result = StringUtils.getRandomString(rnd, 0, 5);\n+            assertThat(result.length()).isBetween(0, 5);\n+        }\n+    }\n+\n+    @Nested\n+    class RepeatTests {\n+\n+        @ParameterizedTest\n+        @CsvSource({\""abc, 3, abcabcabc\"", \""abc, 0, ''\"", \""abc, 1, abc\"", \""'', 5, ''\""})\n+        void testRepeatValidCases(String input, int count, String expected) {\n+            String result = StringUtils.repeat(input, count);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+\n+        @Test\n+        void testRepeatNullString() {\n+            assertThatThrownBy(() -> StringUtils.repeat(null, 3))\n+                    .isInstanceOf(NullPointerException.class);\n+        }\n+\n+        @Test\n+        void testRepeatNegativeCount() {\n+            assertThatThrownBy(() -> StringUtils.repeat(\""abc\"", -1))\n+                    .isInstanceOf(IllegalArgumentException.class);\n+        }\n+\n+        @Test\n+        void testRepeatLargeString() {\n+            StringBuilder sb = new StringBuilder();\n+            for (int i = 0; i < 1000; i++) {\n+                sb.append(\""a\"");\n+            }\n+            String input = sb.toString();\n+            String result = StringUtils.repeat(input, 2);\n+            assertThat(result).hasSize(2000);\n+        }\n+    }\n+\n+    @Nested\n+    class ReplaceTests {\n+\n+        @ParameterizedTest\n+        @CsvSource({\n+            \""aba, a, z, zbz\"",\n+            \""aba, a, '', b\"",\n+            \""abc, x, z, abc\"",\n+            \""'', a, z, ''\"",\n+            \""abc, '', z, abc\""\n+        })\n+        void testReplaceBasicCases(\n+                String text, String search, String replacement, String expected) {\n+            String result = StringUtils.replace(text, search, replacement);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+\n+        @ParameterizedTest\n+        @CsvSource({\n+            \""abaa, a, z, 1, zbaa\"",\n+            \""abaa, a, z, 2, zbza\"",\n+            \""abaa, a, z, -1, zbzz\"",\n+            \""aba, a, z, 0, aba\""\n+        })\n+        void testReplaceWithMaxCases(\n+                String text, String search, String replacement, int max, String expected) {\n+            String result = StringUtils.replace(text, search, replacement, max);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+\n+        @ParameterizedTest\n+        @CsvSource(\n+                value = {\""null, a, z, null\"", \""abc, null, z, abc\"", \""aba, a, null, aba\""},\n+                nullValues = \""null\"")\n+        void testReplaceWithNullValues(\n+                String text, String search, String replacement, String expected) {\n+            String result = StringUtils.replace(text, search, replacement);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+    }\n+\n+    @Nested\n+    class IsEmptyTests {\n+\n+        @Test\n+        void testEmptyOrNull() {\n+            assertThat(StringUtils.isEmpty(null)).isTrue();\n+            assertThat(StringUtils.isEmpty(\""\"")).isTrue();\n+        }\n+\n+        @ParameterizedTest\n+        @ValueSource(strings = {\"" \"", \""a\"", \""hello\"", \""  hello  \""})\n+        void testNonEmpty(String input) {\n+            assertThat(StringUtils.isEmpty(input)).isFalse();\n+        }\n+\n+        @Test\n+        void testEmptyStringBuilder() {\n+            StringBuilder sb = new StringBuilder();\n+            assertThat(StringUtils.isEmpty(sb)).isTrue();\n+        }\n+\n+        @Test\n+        void testNonEmptyStringBuilder() {\n+            StringBuilder sb = new StringBuilder(\""test\"");\n+            assertThat(StringUtils.isEmpty(sb)).isFalse();\n+        }\n+    }\n+\n+    @Nested\n+    class RandomNumericStringTests {\n+\n+        @Test\n+        void testRandomNumericStringLength() {\n+            String result = StringUtils.randomNumericString(5);\n+            assertThat(result).hasSize(5);\n+        }\n+\n+        @Test\n+        void testRandomNumericStringContainsOnlyDigits() {\n+            String result = StringUtils.randomNumericString(10);\n+            assertThat(result).matches(\""\\\\d+\"");\n+        }\n+\n+        @Test\n+        void testRandomNumericStringZeroLength() {\n+            String result = StringUtils.randomNumericString(0);\n+            assertThat(result).isEmpty();\n+        }\n+\n+        @Test\n+        void testRandomNumericStringLargeLength() {\n+            String result = StringUtils.randomNumericString(100);\n+            assertThat(result).hasSize(100);\n+            assertThat(result).matches(\""\\\\d+\"");\n+        }\n+    }\n+\n+    @Nested\n+    class SplitTests {\n+\n+        @Test\n+        void testSplitBasicCases() {\n+            assertThat(StringUtils.split(\""ab:cd:ef\"", \"":\"")).containsExactly(\""ab\"", \""cd\"", \""ef\"");\n+            assertThat(StringUtils.split(\""abc def\"", \"" \"")).containsExactly(\""abc\"", \""def\"");\n+            assertThat(StringUtils.split(\""abc  def\"", \"" \"")).containsExactly(\""abc\"", \""def\"");\n+            assertThat(StringUtils.split(\""a,b,c\"", \"",\"")).containsExactly(\""a\"", \""b\"", \""c\"");\n+        }\n+\n+        @Test\n+        void testSplitEdgeCases() {\n+            assertThat(StringUtils.split(null, \"":\"")).isNull();\n+            assertThat(StringUtils.split(\""\"", \"":\"")).isEmpty();\n+            assertThat(StringUtils.split(\""abc def\"", null)).containsExactly(\""abc\"", \""def\"");\n+        }\n+\n+        @Test\n+        void testSplitWithMax() {\n+            String[] result = StringUtils.split(\""a:b:c:d\"", \"":\"", 2, false);\n+            assertThat(result).containsExactly(\""a\"", \""b:c:d\"");\n+        }\n+\n+        @Test\n+        void testSplitPreserveAllTokens() {\n+            String[] result = StringUtils.split(\""a::b\"", \"":\"", -1, true);\n+            assertThat(result).containsExactly(\""a\"", \""\"", \""b\"");\n+        }\n+    }\n+\n+    @Nested\n+    class JoinTests {\n+\n+        @Test\n+        void testJoinIterableBasicCases() {\n+            assertThat(StringUtils.join(Arrays.asList(\""a\"", \""b\"", \""c\""), \"",\"")).isEqualTo(\""a,b,c\"");\n+            assertThat(StringUtils.join(Arrays.asList(\""a\"", \""b\"", \""c\""), null)).isEqualTo(\""abc\"");\n+            assertThat(StringUtils.join(Arrays.asList(\""single\""), \"",\"")).isEqualTo(\""single\"");\n+            assertThat(StringUtils.join(Arrays.asList(\""a\"", null, \""c\""), \"",\"")).isEqualTo(\""a,,c\"");\n+        }\n+\n+        @Test\n+        void testJoinIterableEdgeCases() {\n+            assertThat(StringUtils.join((Iterable<?>) null, \"",\"")).isNull();\n+            assertThat(StringUtils.join(Arrays.asList(), \"",\"")).isEmpty();\n+        }\n+\n+        @Test\n+        void testJoinIterator() {\n+            List<String> items = Arrays.asList(\""x\"", \""y\"", \""z\"");\n+            String result = StringUtils.join(items.iterator(), \""-\"");\n+            assertThat(result).isEqualTo(\""x-y-z\"");\n+        }\n+\n+        @Test\n+        void testJoinNullIterator() {\n+            String result = StringUtils.join((java.util.Iterator<?>) null, \"",\"");\n+            assertThat(result).isNull();\n+        }\n+    }\n+\n+    @Nested\n+    class QuoteTests {\n+\n+        @ParameterizedTest\n+        @CsvSource({\""hello, `hello`\"", \""'', ``\"", \""hello world!, `hello world!`\""})\n+        void testQuote(String input, String expected) {\n+            String result = StringUtils.quote(input);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+    }\n+\n+    @Nested\n+    class ToLowerCaseIfNeedTests {\n+\n+        @ParameterizedTest\n+        @CsvSource({\n+            \""HELLO, true, HELLO\"",\n+            \""HELLO, false, hello\"",\n+            \""hello, false, hello\"",\n+            \""HeLLo, false, hello\""\n+        })\n+        void testToLowerCaseIfNeed(String input, boolean caseSensitive, String expected) {\n+            String result = StringUtils.toLowerCaseIfNeed(input, caseSensitive);\n+            assertThat(result).isEqualTo(expected);\n+        }\n+    }\n+\n+    @Nested\n+    class IsNumericTests {\n+\n+        @Test\n+        void testIsNumericNull() {\n+            assertThat(StringUtils.isNumeric(null)).isFalse();\n+        }\n+\n+        @ParameterizedTest\n+        @ValueSource(\n+                strings = {\n+                    \""0\"",\n+                    \""1\"",\n+                    \""123\"",\n+                    \""999\"",\n+                    \""-1\"",\n+                    \""-123\"",\n+                    \""-999\"",\n+                    \""-0\"",\n+                    \""123456789012345\"",\n+                    \""-123456789012345\""\n+                })\n+        void testIsNumericValidNumbers(String input) {\n+            assertThat(StringUtils.isNumeric(input)).isTrue();\n+        }\n+\n+        @ParameterizedTest\n+        @ValueSource(\n+                strings = {\""\"", \"" \"", \""abc\"", \""12.3\"", \""12a\"", \""a12\"", \"" 12\"", \""12 \"", \""+12\"", \""-\"", \""1 23\""})\n+        void testIsNumericInvalidNumbers(String input) {\n+            assertThat(StringUtils.isNumeric(input)).isFalse();\n+        }\n+    }\n+\n+    @Nested\n+    class EdgeCaseTests {\n+\n+        @Test\n+        void testLargeStringOperations() {\n+            StringBuilder sb = new StringBuilder();\n+            for (int i = 0; i < 10000; i++) {\n+                sb.append(\""a\"");\n+            }\n+            String largeString = sb.toString();\n+            assertThat(StringUtils.isEmpty(largeString)).isFalse();\n+            assertThat(StringUtils.isNullOrWhitespaceOnly(largeString)).isFalse();\n+        }\n+\n+        @Test\n+        void testUnicodeCharacters() {\n+            String unicode = \""Hello \u4e16\u754c \ud83c\udf0d\"";\n+            assertThat(StringUtils.isEmpty(unicode)).isFalse();\n+            assertThat(StringUtils.isNullOrWhitespaceOnly(unicode)).isFalse();\n+            assertThat(StringUtils.quote(unicode)).isEqualTo(\""`Hello \u4e16\u754c \ud83c\udf0d`\"");\n+        }\n+\n+        @Test\n+        void testSpecialWhitespaceCharacters() {\n+            // Test various Unicode whitespace characters that are recognized by\n+            // Character.isWhitespace()\n+            String specialWhitespace =\n+                    \""\\u0009\\u000B\\u000C\\u001C\\u001D\\u001E\\u001F\""; // Tab, VT, FF, FS, GS, RS, US\n+            assertThat(StringUtils.isNullOrWhitespaceOnly(specialWhitespace)).isTrue();\n+        }\n+    }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5682"", ""pr_id"": 5682, ""issue_id"": 3922, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support schema evolution for nested struct add new children fields.\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nSupport schema evolution about nested data type such as StructType(Spark).\r\nFor example:\r\nStruct<name: String, age: Int> to Stuct<name: String, age: Int, phone: Str>\n\n### Solution\n\n- \n\n### Anything else?\n\n- \n\n### Are you willing to submit a PR?\n\n- [ ] I'm willing to submit a PR!"", ""issue_word_count"": 74, ""test_files_count"": 2, ""non_test_files_count"": 5, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/schema/NestedSchemaUtils.java"", ""paimon-core/src/test/java/org/apache/paimon/schema/NestedSchemaUtilsTest.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/schema/NestedSchemaUtilsTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java""], ""base_commit"": ""a1df942b454871d18a3836e3eb0dc038bbcc6cfb"", ""head_commit"": ""b1fa8f52a2a114e5e86876d10fa0fd6f6ec44263"", ""repo_url"": ""https://github.com/apache/paimon/pull/5682"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5682"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-09T07:51:49.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/schema/NestedSchemaUtils.java b/paimon-core/src/main/java/org/apache/paimon/schema/NestedSchemaUtils.java\nnew file mode 100644\nindex 000000000000..56d5fd99b3c4\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/schema/NestedSchemaUtils.java\n@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.schema;\n+\n+import org.apache.paimon.types.ArrayType;\n+import org.apache.paimon.types.DataField;\n+import org.apache.paimon.types.DataType;\n+import org.apache.paimon.types.DataTypeRoot;\n+import org.apache.paimon.types.MapType;\n+import org.apache.paimon.types.MultisetType;\n+import org.apache.paimon.types.RowType;\n+import org.apache.paimon.utils.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+/**\n+ * Utility class for handling nested column schema changes. This provides shared logic for both\n+ * FlinkCatalog and CDC sources to handle schema evolution for nested types (arrays, maps,\n+ * multisets, rows) consistently.\n+ */\n+public class NestedSchemaUtils {\n+\n+    /**\n+     * Generates nested column updates for schema evolution. Handles all nested types: ROW, ARRAY,\n+     * MAP, MULTISET. Creates proper field paths with markers like \""element\"" for arrays and \""value\""\n+     * for maps.\n+     *\n+     * @param fieldNames The current field path as a list of field names\n+     * @param oldType The old data type\n+     * @param newType The new data type\n+     * @param schemaChanges List to collect the generated schema changes\n+     */\n+    public static void generateNestedColumnUpdates(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        if (oldType.getTypeRoot() == DataTypeRoot.ROW) {\n+            handleRowTypeUpdate(fieldNames, oldType, newType, schemaChanges);\n+        } else if (oldType.getTypeRoot() == DataTypeRoot.ARRAY) {\n+            handleArrayTypeUpdate(fieldNames, oldType, newType, schemaChanges);\n+        } else if (oldType.getTypeRoot() == DataTypeRoot.MAP) {\n+            handleMapTypeUpdate(fieldNames, oldType, newType, schemaChanges);\n+        } else if (oldType.getTypeRoot() == DataTypeRoot.MULTISET) {\n+            handleMultisetTypeUpdate(fieldNames, oldType, newType, schemaChanges);\n+        } else {\n+            // For primitive types, update the column type directly\n+            handlePrimitiveTypeUpdate(fieldNames, oldType, newType, schemaChanges);\n+        }\n+\n+        // Handle nullability changes for all types\n+        handleNullabilityChange(fieldNames, oldType, newType, schemaChanges);\n+    }\n+\n+    private static void handleRowTypeUpdate(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        String joinedNames = String.join(\"".\"", fieldNames);\n+\n+        Preconditions.checkArgument(\n+                newType.getTypeRoot() == DataTypeRoot.ROW,\n+                \""Column %s can only be updated to row type, and cannot be updated to %s type\"",\n+                joinedNames,\n+                newType.getTypeRoot());\n+\n+        RowType oldRowType = (RowType) oldType;\n+        RowType newRowType = (RowType) newType;\n+\n+        // check that existing fields maintain their order\n+        Map<String, Integer> oldFieldOrders = new HashMap<>();\n+        for (int i = 0; i < oldRowType.getFieldCount(); i++) {\n+            oldFieldOrders.put(oldRowType.getFields().get(i).name(), i);\n+        }\n+\n+        int lastIdx = -1;\n+        String lastFieldName = \""\"";\n+        for (DataField newField : newRowType.getFields()) {\n+            String name = newField.name();\n+            if (oldFieldOrders.containsKey(name)) {\n+                int idx = oldFieldOrders.get(name);\n+                Preconditions.checkState(\n+                        lastIdx < idx,\n+                        \""Order of existing fields in column %s must be kept the same. \""\n+                                + \""However, field %s and %s have changed their orders.\"",\n+                        joinedNames,\n+                        lastFieldName,\n+                        name);\n+                lastIdx = idx;\n+                lastFieldName = name;\n+            }\n+        }\n+\n+        // drop fields\n+        Set<String> newFieldNames = new HashSet<>(newRowType.getFieldNames());\n+        for (String name : oldRowType.getFieldNames()) {\n+            if (!newFieldNames.contains(name)) {\n+                List<String> dropColumnNames = new ArrayList<>(fieldNames);\n+                dropColumnNames.add(name);\n+                schemaChanges.add(SchemaChange.dropColumn(dropColumnNames.toArray(new String[0])));\n+            }\n+        }\n+\n+        for (int i = 0; i < newRowType.getFieldCount(); i++) {\n+            DataField field = newRowType.getFields().get(i);\n+            String name = field.name();\n+            List<String> fullFieldNames = new ArrayList<>(fieldNames);\n+            fullFieldNames.add(name);\n+\n+            if (!oldFieldOrders.containsKey(name)) {\n+                // add fields\n+                SchemaChange.Move move;\n+                if (i == 0) {\n+                    move = SchemaChange.Move.first(name);\n+                } else {\n+                    String lastName = newRowType.getFields().get(i - 1).name();\n+                    move = SchemaChange.Move.after(name, lastName);\n+                }\n+                schemaChanges.add(\n+                        SchemaChange.addColumn(\n+                                fullFieldNames.toArray(new String[0]),\n+                                field.type(),\n+                                field.description(),\n+                                move));\n+            } else {\n+                // update existing fields\n+                DataField oldField = oldRowType.getFields().get(oldFieldOrders.get(name));\n+                if (!Objects.equals(oldField.description(), field.description())) {\n+                    schemaChanges.add(\n+                            SchemaChange.updateColumnComment(\n+                                    fullFieldNames.toArray(new String[0]), field.description()));\n+                }\n+                generateNestedColumnUpdates(\n+                        fullFieldNames, oldField.type(), field.type(), schemaChanges);\n+            }\n+        }\n+    }\n+\n+    private static void handleArrayTypeUpdate(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        String joinedNames = String.join(\"".\"", fieldNames);\n+        Preconditions.checkArgument(\n+                newType.getTypeRoot() == DataTypeRoot.ARRAY,\n+                \""Column %s can only be updated to array type, and cannot be updated to %s type\"",\n+                joinedNames,\n+                newType);\n+\n+        List<String> fullFieldNames = new ArrayList<>(fieldNames);\n+        // add a dummy column name indicating the element of array\n+        fullFieldNames.add(\""element\"");\n+\n+        generateNestedColumnUpdates(\n+                fullFieldNames,\n+                ((ArrayType) oldType).getElementType(),\n+                ((ArrayType) newType).getElementType(),\n+                schemaChanges);\n+    }\n+\n+    private static void handleMapTypeUpdate(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        String joinedNames = String.join(\"".\"", fieldNames);\n+        Preconditions.checkArgument(\n+                newType.getTypeRoot() == DataTypeRoot.MAP,\n+                \""Column %s can only be updated to map type, and cannot be updated to %s type\"",\n+                joinedNames,\n+                newType);\n+\n+        MapType oldMapType = (MapType) oldType;\n+        MapType newMapType = (MapType) newType;\n+\n+        Preconditions.checkArgument(\n+                oldMapType.getKeyType().equals(newMapType.getKeyType()),\n+                \""Cannot update key type of column %s from %s type to %s type\"",\n+                joinedNames,\n+                oldMapType.getKeyType(),\n+                newMapType.getKeyType());\n+\n+        List<String> fullFieldNames = new ArrayList<>(fieldNames);\n+        // add a dummy column name indicating the value of map\n+        fullFieldNames.add(\""value\"");\n+\n+        generateNestedColumnUpdates(\n+                fullFieldNames,\n+                oldMapType.getValueType(),\n+                newMapType.getValueType(),\n+                schemaChanges);\n+    }\n+\n+    private static void handleMultisetTypeUpdate(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        String joinedNames = String.join(\"".\"", fieldNames);\n+\n+        Preconditions.checkArgument(\n+                newType.getTypeRoot() == DataTypeRoot.MULTISET,\n+                \""Column %s can only be updated to multiset type, and cannot be updated to %s type\"",\n+                joinedNames,\n+                newType);\n+\n+        List<String> fullFieldNames = new ArrayList<>(fieldNames);\n+        // Add the special \""element\"" marker for multiset element access\n+        fullFieldNames.add(\""element\"");\n+\n+        generateNestedColumnUpdates(\n+                fullFieldNames,\n+                ((MultisetType) oldType).getElementType(),\n+                ((MultisetType) newType).getElementType(),\n+                schemaChanges);\n+    }\n+\n+    private static void handlePrimitiveTypeUpdate(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        if (!oldType.equalsIgnoreNullable(newType)) {\n+            schemaChanges.add(\n+                    SchemaChange.updateColumnType(\n+                            fieldNames.toArray(new String[0]), newType, false));\n+        }\n+    }\n+\n+    private static void handleNullabilityChange(\n+            List<String> fieldNames,\n+            DataType oldType,\n+            DataType newType,\n+            List<SchemaChange> schemaChanges) {\n+\n+        if (oldType.isNullable() != newType.isNullable()) {\n+            schemaChanges.add(\n+                    SchemaChange.updateColumnNullability(\n+                            fieldNames.toArray(new String[0]), newType.isNullable()));\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\nindex 42effccf7f9c..503f3593c236 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\n@@ -101,7 +101,7 @@ public void processElement(\n                         updatedSchema.f1.comment());\n \n         for (SchemaChange schemaChange : extractSchemaChanges(schemaManager, actualUpdatedSchema)) {\n-            applySchemaChange(schemaManager, schemaChange, tableId);\n+            applySchemaChange(schemaManager, schemaChange, tableId, actualUpdatedSchema);\n         }\n         /*\n          * Here, actualUpdatedDataFields cannot be used to update latestFields because there is a\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\nindex e1b78926cc85..d4319e868e6f 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\n@@ -74,7 +74,7 @@ public void processElement(CdcSchema updatedSchema, Context context, Collector<V\n                         updatedSchema.primaryKeys(),\n                         updatedSchema.comment());\n         for (SchemaChange schemaChange : extractSchemaChanges(schemaManager, actualUpdatedSchema)) {\n-            applySchemaChange(schemaManager, schemaChange, identifier);\n+            applySchemaChange(schemaManager, schemaChange, identifier, actualUpdatedSchema);\n         }\n         /*\n          * Here, actualUpdatedDataFields cannot be used to update latestFields because there is a\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\nindex 410425a7da04..bb49de2ac198 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\n@@ -22,14 +22,18 @@\n import org.apache.paimon.catalog.CatalogLoader;\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.flink.action.cdc.TypeMapping;\n+import org.apache.paimon.schema.NestedSchemaUtils;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.schema.TableSchema;\n+import org.apache.paimon.types.ArrayType;\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataType;\n import org.apache.paimon.types.DataTypeChecks;\n import org.apache.paimon.types.DataTypeRoot;\n import org.apache.paimon.types.FieldIdentifier;\n+import org.apache.paimon.types.MapType;\n+import org.apache.paimon.types.MultisetType;\n import org.apache.paimon.types.RowType;\n import org.apache.paimon.utils.Preconditions;\n import org.apache.paimon.utils.StringUtils;\n@@ -98,7 +102,10 @@ public void open(Configuration parameters) {\n     }\n \n     protected void applySchemaChange(\n-            SchemaManager schemaManager, SchemaChange schemaChange, Identifier identifier)\n+            SchemaManager schemaManager,\n+            SchemaChange schemaChange,\n+            Identifier identifier,\n+            CdcSchema actualUpdatedSchema)\n             throws Exception {\n         if (schemaChange instanceof SchemaChange.AddColumn) {\n             try {\n@@ -118,9 +125,6 @@ protected void applySchemaChange(\n         } else if (schemaChange instanceof SchemaChange.UpdateColumnType) {\n             SchemaChange.UpdateColumnType updateColumnType =\n                     (SchemaChange.UpdateColumnType) schemaChange;\n-            Preconditions.checkState(\n-                    updateColumnType.fieldNames().length == 1,\n-                    \""Paimon CDC currently does not support nested type schema evolution.\"");\n             TableSchema schema =\n                     schemaManager\n                             .latest()\n@@ -136,6 +140,19 @@ protected void applySchemaChange(\n                             + \"" does not exist in table. This is unexpected.\"");\n             DataType oldType = schema.fields().get(idx).type();\n             DataType newType = updateColumnType.newDataType();\n+\n+            // For complex types, extract the full new type from actualUpdatedSchema\n+            // to preserve type context (e.g., ARRAY<BIGINT> instead of just BIGINT)\n+            if (actualUpdatedSchema != null) {\n+                String fieldName = updateColumnType.fieldNames()[0];\n+                for (DataField field : actualUpdatedSchema.fields()) {\n+                    if (fieldName.equals(field.name())) {\n+                        newType = field.type();\n+                        break;\n+                    }\n+                }\n+            }\n+\n             switch (canConvert(oldType, newType, typeMapping)) {\n                 case CONVERT:\n                     catalog.alterTable(identifier, schemaChange, false);\n@@ -165,9 +182,41 @@ protected void applySchemaChange(\n     public static ConvertAction canConvert(\n             DataType oldType, DataType newType, TypeMapping typeMapping) {\n         if (oldType.equalsIgnoreNullable(newType)) {\n+            if (oldType.isNullable() && !newType.isNullable()) {\n+                return ConvertAction.EXCEPTION; // Cannot make nullable field non-nullable\n+            }\n             return ConvertAction.CONVERT;\n         }\n \n+        if (oldType.getTypeRoot() == DataTypeRoot.ARRAY\n+                && newType.getTypeRoot() == DataTypeRoot.ARRAY) {\n+\n+            ArrayType oldArrayType = (ArrayType) oldType;\n+            ArrayType newArrayType = (ArrayType) newType;\n+            return canConvertArray(oldArrayType, newArrayType, typeMapping);\n+        }\n+\n+        if (oldType.getTypeRoot() == DataTypeRoot.MAP\n+                && newType.getTypeRoot() == DataTypeRoot.MAP) {\n+            MapType oldMapType = (MapType) oldType;\n+            MapType newMapType = (MapType) newType;\n+\n+            return canConvertMap(oldMapType, newMapType, typeMapping);\n+        }\n+\n+        if (oldType.getTypeRoot() == DataTypeRoot.MULTISET\n+                && newType.getTypeRoot() == DataTypeRoot.MULTISET) {\n+            MultisetType oldMultisetType = (MultisetType) oldType;\n+            MultisetType newMultisetType = (MultisetType) newType;\n+\n+            return canConvertMultisetType(oldMultisetType, newMultisetType, typeMapping);\n+        }\n+\n+        if (oldType.getTypeRoot() == DataTypeRoot.ROW\n+                && newType.getTypeRoot() == DataTypeRoot.ROW) {\n+            return canConvertRowType((RowType) oldType, (RowType) newType, typeMapping);\n+        }\n+\n         int oldIdx = STRING_TYPES.indexOf(oldType.getTypeRoot());\n         int newIdx = STRING_TYPES.indexOf(newType.getTypeRoot());\n         if (oldIdx >= 0 && newIdx >= 0) {\n@@ -223,6 +272,88 @@ public static ConvertAction canConvert(\n         return ConvertAction.EXCEPTION;\n     }\n \n+    private static ConvertAction canConvertArray(\n+            ArrayType oldArrayType, ArrayType newArrayType, TypeMapping typeMapping) {\n+        if (oldArrayType.isNullable() && !newArrayType.isNullable()) {\n+            return ConvertAction.EXCEPTION;\n+        }\n+\n+        return canConvert(\n+                oldArrayType.getElementType(), newArrayType.getElementType(), typeMapping);\n+    }\n+\n+    private static ConvertAction canConvertMap(\n+            MapType oldMapType, MapType newMapType, TypeMapping typeMapping) {\n+        if (oldMapType.isNullable() && !newMapType.isNullable()) {\n+            return ConvertAction.EXCEPTION;\n+        }\n+\n+        // For map keys, don't allow key type evolution\n+        // hashcode will be different even if the value of the key is the same\n+        if (!oldMapType.getKeyType().equals(newMapType.getKeyType())) {\n+            return ConvertAction.EXCEPTION;\n+        }\n+\n+        return canConvert(oldMapType.getValueType(), newMapType.getValueType(), typeMapping);\n+    }\n+\n+    private static ConvertAction canConvertRowType(\n+            RowType oldRowType, RowType newRowType, TypeMapping typeMapping) {\n+        Map<String, DataField> oldFieldMap = new HashMap<>();\n+        for (DataField field : oldRowType.getFields()) {\n+            oldFieldMap.put(field.name(), field);\n+        }\n+\n+        Map<String, DataField> newFieldMap = new HashMap<>();\n+        for (DataField field : newRowType.getFields()) {\n+            newFieldMap.put(field.name(), field);\n+        }\n+\n+        // Rule 1: Check all non-nullable fields in the old type must exist in the new type\n+        for (DataField oldField : oldRowType.getFields()) {\n+            if (!oldField.type().isNullable()) {\n+                if (!newFieldMap.containsKey(oldField.name())) {\n+                    return ConvertAction.EXCEPTION;\n+                }\n+            }\n+        }\n+\n+        // Rule 2: All fields common to both schemas must have compatible types\n+        boolean needsConversion = false;\n+        for (DataField newField : newRowType.getFields()) {\n+            DataField oldField = oldFieldMap.get(newField.name());\n+            if (oldField != null) {\n+                ConvertAction fieldAction =\n+                        canConvert(oldField.type(), newField.type(), typeMapping);\n+                if (fieldAction == ConvertAction.EXCEPTION) {\n+                    return ConvertAction.EXCEPTION;\n+                }\n+                if (fieldAction == ConvertAction.CONVERT) {\n+                    needsConversion = true;\n+                }\n+            } else {\n+                // Rule 3: New fields must be nullable\n+                if (!newField.type().isNullable()) {\n+                    return ConvertAction.EXCEPTION;\n+                }\n+                needsConversion = true;\n+            }\n+        }\n+\n+        return needsConversion ? ConvertAction.CONVERT : ConvertAction.IGNORE;\n+    }\n+\n+    private static ConvertAction canConvertMultisetType(\n+            MultisetType oldMultisetType, MultisetType newMultisetType, TypeMapping typeMapping) {\n+\n+        if (oldMultisetType.isNullable() && !newMultisetType.isNullable()) {\n+            return ConvertAction.EXCEPTION;\n+        }\n+\n+        return canConvert(\n+                oldMultisetType.getElementType(), newMultisetType.getElementType(), typeMapping);\n+    }\n+\n     protected List<SchemaChange> extractSchemaChanges(\n             SchemaManager schemaManager, CdcSchema updatedSchema) {\n         TableSchema oldTableSchema = schemaManager.latest().get();\n@@ -258,8 +389,9 @@ protected List<SchemaChange> extractSchemaChanges(\n                     if (oldField.type().is(DataTypeRoot.DECIMAL) && !allowDecimalTypeChange) {\n                         continue;\n                     }\n-                    // update column type\n-                    result.add(SchemaChange.updateColumnType(newFieldName, newField.type()));\n+                    // Generate nested column updates if needed\n+                    NestedSchemaUtils.generateNestedColumnUpdates(\n+                            Arrays.asList(newFieldName), oldField.type(), newField.type(), result);\n                     // update column comment\n                     if (newField.description() != null) {\n                         result.add(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\nindex 10f437e8bd37..d791d33d89b5 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n@@ -34,6 +34,7 @@\n import org.apache.paimon.function.FunctionImpl;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.schema.NestedSchemaUtils;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n@@ -43,8 +44,6 @@\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.sink.BatchTableCommit;\n import org.apache.paimon.table.source.ReadBuilder;\n-import org.apache.paimon.types.DataField;\n-import org.apache.paimon.types.DataTypeRoot;\n import org.apache.paimon.utils.FileStorePathFactory;\n import org.apache.paimon.utils.InternalRowPartitionComputer;\n import org.apache.paimon.utils.Preconditions;\n@@ -122,13 +121,11 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Optional;\n-import java.util.Set;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n \n@@ -672,132 +669,8 @@ private void generateNestedColumnUpdates(\n             org.apache.paimon.types.DataType oldType,\n             org.apache.paimon.types.DataType newType,\n             List<SchemaChange> schemaChanges) {\n-        String joinedNames = String.join(\"".\"", fieldNames);\n-        if (oldType.getTypeRoot() == DataTypeRoot.ROW) {\n-            Preconditions.checkArgument(\n-                    newType.getTypeRoot() == DataTypeRoot.ROW,\n-                    \""Column %s can only be updated to row type, and cannot be updated to %s type\"",\n-                    joinedNames,\n-                    newType.getTypeRoot());\n-            org.apache.paimon.types.RowType oldRowType = (org.apache.paimon.types.RowType) oldType;\n-            org.apache.paimon.types.RowType newRowType = (org.apache.paimon.types.RowType) newType;\n-\n-            // check that existing fields have same order\n-            Map<String, Integer> oldFieldOrders = new HashMap<>();\n-            for (int i = 0; i < oldRowType.getFieldCount(); i++) {\n-                oldFieldOrders.put(oldRowType.getFields().get(i).name(), i);\n-            }\n-            int lastIdx = -1;\n-            String lastFieldName = \""\"";\n-            for (DataField newField : newRowType.getFields()) {\n-                String name = newField.name();\n-                if (oldFieldOrders.containsKey(name)) {\n-                    int idx = oldFieldOrders.get(name);\n-                    Preconditions.checkState(\n-                            lastIdx < idx,\n-                            \""Order of existing fields in column %s must be kept the same. \""\n-                                    + \""However, field %s and %s have changed their orders.\"",\n-                            joinedNames,\n-                            lastFieldName,\n-                            name);\n-                    lastIdx = idx;\n-                    lastFieldName = name;\n-                }\n-            }\n \n-            // drop fields\n-            Set<String> newFieldNames = new HashSet<>(newRowType.getFieldNames());\n-            for (String name : oldRowType.getFieldNames()) {\n-                if (!newFieldNames.contains(name)) {\n-                    List<String> dropColumnNames = new ArrayList<>(fieldNames);\n-                    dropColumnNames.add(name);\n-                    schemaChanges.add(\n-                            SchemaChange.dropColumn(dropColumnNames.toArray(new String[0])));\n-                }\n-            }\n-\n-            for (int i = 0; i < newRowType.getFieldCount(); i++) {\n-                DataField field = newRowType.getFields().get(i);\n-                String name = field.name();\n-                List<String> fullFieldNames = new ArrayList<>(fieldNames);\n-                fullFieldNames.add(name);\n-                if (!oldFieldOrders.containsKey(name)) {\n-                    // add fields\n-                    SchemaChange.Move move;\n-                    if (i == 0) {\n-                        move = SchemaChange.Move.first(name);\n-                    } else {\n-                        String lastName = newRowType.getFields().get(i - 1).name();\n-                        move = SchemaChange.Move.after(name, lastName);\n-                    }\n-                    schemaChanges.add(\n-                            SchemaChange.addColumn(\n-                                    fullFieldNames.toArray(new String[0]),\n-                                    field.type(),\n-                                    field.description(),\n-                                    move));\n-                } else {\n-                    // update existing fields\n-                    DataField oldField = oldRowType.getFields().get(oldFieldOrders.get(name));\n-                    if (!Objects.equals(oldField.description(), field.description())) {\n-                        schemaChanges.add(\n-                                SchemaChange.updateColumnComment(\n-                                        fullFieldNames.toArray(new String[0]),\n-                                        field.description()));\n-                    }\n-                    generateNestedColumnUpdates(\n-                            fullFieldNames, oldField.type(), field.type(), schemaChanges);\n-                }\n-            }\n-        } else if (oldType.getTypeRoot() == DataTypeRoot.ARRAY) {\n-            Preconditions.checkArgument(\n-                    newType.getTypeRoot() == DataTypeRoot.ARRAY,\n-                    \""Column %s can only be updated to array type, and cannot be updated to %s type\"",\n-                    joinedNames,\n-                    newType);\n-            List<String> fullFieldNames = new ArrayList<>(fieldNames);\n-            // add a dummy column name indicating the element of array\n-            fullFieldNames.add(\""element\"");\n-            generateNestedColumnUpdates(\n-                    fullFieldNames,\n-                    ((org.apache.paimon.types.ArrayType) oldType).getElementType(),\n-                    ((org.apache.paimon.types.ArrayType) newType).getElementType(),\n-                    schemaChanges);\n-        } else if (oldType.getTypeRoot() == DataTypeRoot.MAP) {\n-            Preconditions.checkArgument(\n-                    newType.getTypeRoot() == DataTypeRoot.MAP,\n-                    \""Column %s can only be updated to map type, and cannot be updated to %s type\"",\n-                    joinedNames,\n-                    newType);\n-            org.apache.paimon.types.MapType oldMapType = (org.apache.paimon.types.MapType) oldType;\n-            org.apache.paimon.types.MapType newMapType = (org.apache.paimon.types.MapType) newType;\n-            Preconditions.checkArgument(\n-                    oldMapType.getKeyType().equals(newMapType.getKeyType()),\n-                    \""Cannot update key type of column %s from %s type to %s type\"",\n-                    joinedNames,\n-                    oldMapType.getKeyType(),\n-                    newMapType.getKeyType());\n-            List<String> fullFieldNames = new ArrayList<>(fieldNames);\n-            // add a dummy column name indicating the value of map\n-            fullFieldNames.add(\""value\"");\n-            generateNestedColumnUpdates(\n-                    fullFieldNames,\n-                    oldMapType.getValueType(),\n-                    newMapType.getValueType(),\n-                    schemaChanges);\n-        } else {\n-            if (!oldType.equalsIgnoreNullable(newType)) {\n-                schemaChanges.add(\n-                        SchemaChange.updateColumnType(\n-                                fieldNames.toArray(new String[0]), newType, false));\n-            }\n-        }\n-\n-        if (oldType.isNullable() != newType.isNullable()) {\n-            schemaChanges.add(\n-                    SchemaChange.updateColumnNullability(\n-                            fieldNames.toArray(new String[0]), newType.isNullable()));\n-        }\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n     }\n \n     /**\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/schema/NestedSchemaUtilsTest.java b/paimon-core/src/test/java/org/apache/paimon/schema/NestedSchemaUtilsTest.java\nnew file mode 100644\nindex 000000000000..ae5f7ada39a0\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/schema/NestedSchemaUtilsTest.java\n@@ -0,0 +1,669 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.schema;\n+\n+import org.apache.paimon.types.ArrayType;\n+import org.apache.paimon.types.DataField;\n+import org.apache.paimon.types.DataTypes;\n+import org.apache.paimon.types.MapType;\n+import org.apache.paimon.types.MultisetType;\n+import org.apache.paimon.types.RowType;\n+import org.apache.paimon.types.VarCharType;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+/** Tests for {@link NestedSchemaUtils}. */\n+public class NestedSchemaUtilsTest {\n+\n+    @Test\n+    public void testPrimitiveTypeUpdate() {\n+        List<String> fieldNames = Arrays.asList(\""column1\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Test type conversion (INT -> BIGINT)\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames, DataTypes.INT(), DataTypes.BIGINT(), schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""column1\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+    }\n+\n+    @Test\n+    public void testPrimitiveTypeUpdateWithSameType() {\n+        List<String> fieldNames = Arrays.asList(\""column1\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Same type should not generate any changes\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames, DataTypes.INT(), DataTypes.INT(), schemaChanges);\n+\n+        assertThat(schemaChanges).isEmpty();\n+    }\n+\n+    @Test\n+    public void testNullabilityChangeOnly() {\n+        List<String> fieldNames = Arrays.asList(\""column1\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Test nullable to non-nullable\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames, DataTypes.INT().nullable(), DataTypes.INT().notNull(), schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnNullability.class);\n+        SchemaChange.UpdateColumnNullability nullabilityChange =\n+                (SchemaChange.UpdateColumnNullability) schemaChanges.get(0);\n+        assertThat(nullabilityChange.fieldNames()).containsExactly(\""column1\"");\n+        assertThat(nullabilityChange.newNullability()).isFalse();\n+    }\n+\n+    @Test\n+    public void testTypeAndNullabilityChange() {\n+        List<String> fieldNames = Arrays.asList(\""column1\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Test type change and nullability change together\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames,\n+                DataTypes.INT().nullable(),\n+                DataTypes.BIGINT().notNull(),\n+                schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(2);\n+\n+        // Should have both type and nullability changes\n+        assertThat(schemaChanges)\n+                .anyMatch(change -> change instanceof SchemaChange.UpdateColumnType);\n+        assertThat(schemaChanges)\n+                .anyMatch(change -> change instanceof SchemaChange.UpdateColumnNullability);\n+    }\n+\n+    @Test\n+    public void testArrayTypeUpdateElement() {\n+        List<String> fieldNames = Arrays.asList(\""arr_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        ArrayType oldType = new ArrayType(true, DataTypes.INT());\n+        ArrayType newType = new ArrayType(true, DataTypes.BIGINT());\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""arr_column\"", \""element\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+    }\n+\n+    @Test\n+    public void testArrayTypeUpdateNullability() {\n+        List<String> fieldNames = Arrays.asList(\""arr_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        ArrayType oldType = new ArrayType(true, DataTypes.INT());\n+        ArrayType newType = new ArrayType(false, DataTypes.INT());\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnNullability.class);\n+        SchemaChange.UpdateColumnNullability nullabilityChange =\n+                (SchemaChange.UpdateColumnNullability) schemaChanges.get(0);\n+        assertThat(nullabilityChange.fieldNames()).containsExactly(\""arr_column\"");\n+        assertThat(nullabilityChange.newNullability()).isFalse();\n+    }\n+\n+    @Test\n+    public void testArrayWithIncompatibleTypeThrowsException() {\n+        List<String> fieldNames = Arrays.asList(\""arr_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        ArrayType oldType = new ArrayType(true, DataTypes.INT());\n+        RowType newElementType = RowType.of(DataTypes.STRING()); // Incompatible type\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newElementType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalArgumentException.class)\n+                .hasMessageContaining(\""can only be updated to array type\"");\n+    }\n+\n+    @Test\n+    public void testMapTypeUpdateValue() {\n+        List<String> fieldNames = Arrays.asList(\""map_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MapType oldType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+        MapType newType = new MapType(true, DataTypes.STRING(), DataTypes.BIGINT());\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""map_column\"", \""value\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+    }\n+\n+    @Test\n+    public void testMapTypeUpdateNullability() {\n+        List<String> fieldNames = Arrays.asList(\""map_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MapType oldType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+        MapType newType = new MapType(false, DataTypes.STRING(), DataTypes.INT());\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnNullability.class);\n+        SchemaChange.UpdateColumnNullability nullabilityChange =\n+                (SchemaChange.UpdateColumnNullability) schemaChanges.get(0);\n+        assertThat(nullabilityChange.fieldNames()).containsExactly(\""map_column\"");\n+        assertThat(nullabilityChange.newNullability()).isFalse();\n+    }\n+\n+    @Test\n+    public void testMapWithIncompatibleKeyTypeThrowsException() {\n+        List<String> fieldNames = Arrays.asList(\""map_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MapType oldType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+        MapType newType = new MapType(true, DataTypes.INT(), DataTypes.INT()); // Different key type\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalArgumentException.class)\n+                .hasMessageContaining(\""Cannot update key type\"");\n+    }\n+\n+    @Test\n+    public void testMapWithIncompatibleTypeThrowsException() {\n+        List<String> fieldNames = Arrays.asList(\""map_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MapType oldType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+        ArrayType newType = new ArrayType(true, DataTypes.INT()); // Incompatible type\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalArgumentException.class)\n+                .hasMessageContaining(\""can only be updated to map type\"");\n+    }\n+\n+    @Test\n+    public void testMultisetTypeUpdateElement() {\n+        List<String> fieldNames = Arrays.asList(\""multiset_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MultisetType oldType = new MultisetType(true, DataTypes.INT());\n+        MultisetType newType = new MultisetType(true, DataTypes.BIGINT());\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""multiset_column\"", \""element\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+    }\n+\n+    @Test\n+    public void testMultisetWithIncompatibleTypeThrowsException() {\n+        List<String> fieldNames = Arrays.asList(\""multiset_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        MultisetType oldType = new MultisetType(true, DataTypes.INT());\n+        MapType newType =\n+                new MapType(true, DataTypes.STRING(), DataTypes.INT()); // Incompatible type\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalArgumentException.class)\n+                .hasMessageContaining(\""can only be updated to multiset type\"");\n+    }\n+\n+    @Test\n+    public void testRowTypeAddField() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()));\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.AddColumn.class);\n+        SchemaChange.AddColumn addColumn = (SchemaChange.AddColumn) schemaChanges.get(0);\n+        assertThat(addColumn.fieldNames()).containsExactly(\""row_column\"", \""f3\"");\n+        assertThat(addColumn.dataType()).isEqualTo(DataTypes.DOUBLE());\n+    }\n+\n+    @Test\n+    public void testRowTypeDropField() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()));\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.DropColumn.class);\n+        SchemaChange.DropColumn dropColumn = (SchemaChange.DropColumn) schemaChanges.get(0);\n+        assertThat(dropColumn.fieldNames()).containsExactly(\""row_column\"", \""f3\"");\n+    }\n+\n+    @Test\n+    public void testRowTypeUpdateExistingField() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.BIGINT()), // Type changed\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""row_column\"", \""f1\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+    }\n+\n+    @Test\n+    public void testRowTypeUpdateFieldComment() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT(), \""old comment\""),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT(), \""new comment\""),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnComment.class);\n+        SchemaChange.UpdateColumnComment commentChange =\n+                (SchemaChange.UpdateColumnComment) schemaChanges.get(0);\n+        assertThat(commentChange.fieldNames()).containsExactly(\""row_column\"", \""f1\"");\n+        assertThat(commentChange.newDescription()).isEqualTo(\""new comment\"");\n+    }\n+\n+    @Test\n+    public void testRowTypeReorderFields() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()));\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()) // f2 and f3 swapped\n+                        );\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalStateException.class)\n+                .hasMessageContaining(\""Order of existing fields\"");\n+    }\n+\n+    @Test\n+    public void testRowWithIncompatibleTypeThrowsException() {\n+        List<String> fieldNames = Arrays.asList(\""row_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        RowType oldType = RowType.of(new DataField(0, \""f1\"", DataTypes.INT()));\n+        ArrayType newType = new ArrayType(true, DataTypes.INT()); // Incompatible type\n+\n+        assertThatThrownBy(\n+                        () -> {\n+                            NestedSchemaUtils.generateNestedColumnUpdates(\n+                                    fieldNames, oldType, newType, schemaChanges);\n+                        })\n+                .isInstanceOf(IllegalArgumentException.class)\n+                .hasMessageContaining(\""can only be updated to row type\"");\n+    }\n+\n+    @Test\n+    public void testComplexNestedStructure() {\n+        List<String> fieldNames = Arrays.asList(\""complex_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Old: ARRAY<MAP<STRING, ROW<f1 INT, f2 STRING>>>\n+        RowType oldInnerRowType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+        MapType oldMapType = new MapType(true, DataTypes.STRING(), oldInnerRowType);\n+        ArrayType oldType = new ArrayType(true, oldMapType);\n+\n+        // New: ARRAY<MAP<STRING, ROW<f1 BIGINT, f2 STRING, f3 DOUBLE>>>\n+        RowType newInnerRowType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.BIGINT()), // Type changed\n+                        new DataField(1, \""f2\"", DataTypes.STRING()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()) // New field added\n+                        );\n+        MapType newMapType = new MapType(true, DataTypes.STRING(), newInnerRowType);\n+        ArrayType newType = new ArrayType(true, newMapType);\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(2);\n+\n+        // Should have one type update and one add column\n+        SchemaChange.UpdateColumnType typeChange =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.UpdateColumnType)\n+                        .map(change -> (SchemaChange.UpdateColumnType) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(typeChange).isNotNull();\n+        assertThat(typeChange.fieldNames())\n+                .containsExactly(\""complex_column\"", \""element\"", \""value\"", \""f1\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(DataTypes.BIGINT());\n+\n+        SchemaChange.AddColumn addColumn =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.AddColumn)\n+                        .map(change -> (SchemaChange.AddColumn) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(addColumn).isNotNull();\n+        assertThat(addColumn.fieldNames())\n+                .containsExactly(\""complex_column\"", \""element\"", \""value\"", \""f3\"");\n+        assertThat(addColumn.dataType()).isEqualTo(DataTypes.DOUBLE());\n+    }\n+\n+    @Test\n+    public void testNestedFieldPaths() {\n+        List<String> fieldNames = Arrays.asList(\""outer\"", \""inner\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Test that nested field names are properly combined\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames, DataTypes.INT(), DataTypes.BIGINT(), schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""outer\"", \""inner\"");\n+    }\n+\n+    @Test\n+    public void testEmptyFieldNames() {\n+        List<String> fieldNames = Collections.emptyList();\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(\n+                fieldNames, DataTypes.INT(), DataTypes.BIGINT(), schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).isEmpty();\n+    }\n+\n+    @Test\n+    public void testVarCharTypeExtension() {\n+        List<String> fieldNames = Arrays.asList(\""varchar_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        VarCharType oldType = new VarCharType(true, 10);\n+        VarCharType newType = new VarCharType(true, 20);\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(1);\n+        assertThat(schemaChanges.get(0)).isInstanceOf(SchemaChange.UpdateColumnType.class);\n+        SchemaChange.UpdateColumnType typeChange =\n+                (SchemaChange.UpdateColumnType) schemaChanges.get(0);\n+        assertThat(typeChange.fieldNames()).containsExactly(\""varchar_column\"");\n+        assertThat(typeChange.newDataType()).isEqualTo(newType);\n+    }\n+\n+    @Test\n+    public void testMultipleNestedChanges() {\n+        List<String> fieldNames = Arrays.asList(\""root\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Old: ROW<f1 INT NULL, f2 STRING>\n+        RowType oldType =\n+                RowType.of(\n+                        new DataField(0, \""f1\"", DataTypes.INT().nullable()),\n+                        new DataField(1, \""f2\"", DataTypes.STRING()));\n+\n+        // New: ROW<f1 BIGINT NOT NULL, f2 STRING, f3 DOUBLE>\n+        RowType newType =\n+                RowType.of(\n+                        new DataField(\n+                                0,\n+                                \""f1\"",\n+                                DataTypes.BIGINT().notNull()), // Type and nullability changed\n+                        new DataField(1, \""f2\"", DataTypes.STRING()),\n+                        new DataField(2, \""f3\"", DataTypes.DOUBLE()) // New field\n+                        );\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(3);\n+\n+        // Should have: add column, update type, update nullability\n+        assertThat(schemaChanges).anyMatch(change -> change instanceof SchemaChange.AddColumn);\n+        assertThat(schemaChanges)\n+                .anyMatch(change -> change instanceof SchemaChange.UpdateColumnType);\n+        assertThat(schemaChanges)\n+                .anyMatch(change -> change instanceof SchemaChange.UpdateColumnNullability);\n+    }\n+\n+    @Test\n+    public void testArrayOfRowsWithFieldChanges() {\n+        List<String> fieldNames = Arrays.asList(\""array_of_rows\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Old: ARRAY<ROW<id INT, name STRING>>\n+        RowType oldRowType =\n+                RowType.of(\n+                        new DataField(0, \""id\"", DataTypes.INT()),\n+                        new DataField(1, \""name\"", DataTypes.STRING()));\n+        ArrayType oldType = new ArrayType(true, oldRowType);\n+\n+        // New: ARRAY<ROW<id BIGINT, name STRING, active BOOLEAN>>\n+        RowType newRowType =\n+                RowType.of(\n+                        new DataField(0, \""id\"", DataTypes.BIGINT()), // Type changed\n+                        new DataField(1, \""name\"", DataTypes.STRING()),\n+                        new DataField(2, \""active\"", DataTypes.BOOLEAN()) // New field\n+                        );\n+        ArrayType newType = new ArrayType(true, newRowType);\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(2);\n+\n+        // Verify field paths include \""element\"" for array access\n+        SchemaChange.UpdateColumnType typeChange =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.UpdateColumnType)\n+                        .map(change -> (SchemaChange.UpdateColumnType) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(typeChange).isNotNull();\n+        assertThat(typeChange.fieldNames()).containsExactly(\""array_of_rows\"", \""element\"", \""id\"");\n+\n+        SchemaChange.AddColumn addColumn =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.AddColumn)\n+                        .map(change -> (SchemaChange.AddColumn) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(addColumn).isNotNull();\n+        assertThat(addColumn.fieldNames()).containsExactly(\""array_of_rows\"", \""element\"", \""active\"");\n+    }\n+\n+    @Test\n+    public void testMapOfRowsWithFieldChanges() {\n+        List<String> fieldNames = Arrays.asList(\""map_of_rows\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Old: MAP<STRING, ROW<id INT, name STRING>>\n+        RowType oldRowType =\n+                RowType.of(\n+                        new DataField(0, \""id\"", DataTypes.INT()),\n+                        new DataField(1, \""name\"", DataTypes.STRING()));\n+        MapType oldType = new MapType(true, DataTypes.STRING(), oldRowType);\n+\n+        // New: MAP<STRING, ROW<id BIGINT, name STRING, active BOOLEAN>>\n+        RowType newRowType =\n+                RowType.of(\n+                        new DataField(0, \""id\"", DataTypes.BIGINT()), // Type changed\n+                        new DataField(1, \""name\"", DataTypes.STRING()),\n+                        new DataField(2, \""active\"", DataTypes.BOOLEAN()) // New field\n+                        );\n+        MapType newType = new MapType(true, DataTypes.STRING(), newRowType);\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(2);\n+\n+        // Verify field paths include \""value\"" for map value access\n+        SchemaChange.UpdateColumnType typeChange =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.UpdateColumnType)\n+                        .map(change -> (SchemaChange.UpdateColumnType) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(typeChange).isNotNull();\n+        assertThat(typeChange.fieldNames()).containsExactly(\""map_of_rows\"", \""value\"", \""id\"");\n+\n+        SchemaChange.AddColumn addColumn =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.AddColumn)\n+                        .map(change -> (SchemaChange.AddColumn) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(addColumn).isNotNull();\n+        assertThat(addColumn.fieldNames()).containsExactly(\""map_of_rows\"", \""value\"", \""active\"");\n+    }\n+\n+    @Test\n+    public void testMultisetOfComplexType() {\n+        List<String> fieldNames = Arrays.asList(\""multiset_column\"");\n+        List<SchemaChange> schemaChanges = new ArrayList<>();\n+\n+        // Old: MULTISET<ROW<id INT>>\n+        RowType oldRowType = RowType.of(new DataField(0, \""id\"", DataTypes.INT()));\n+        MultisetType oldType = new MultisetType(true, oldRowType);\n+\n+        // New: MULTISET<ROW<id BIGINT, name STRING>>\n+        RowType newRowType =\n+                RowType.of(\n+                        new DataField(0, \""id\"", DataTypes.BIGINT()),\n+                        new DataField(1, \""name\"", DataTypes.STRING()));\n+        MultisetType newType = new MultisetType(true, newRowType);\n+\n+        NestedSchemaUtils.generateNestedColumnUpdates(fieldNames, oldType, newType, schemaChanges);\n+\n+        assertThat(schemaChanges).hasSize(2);\n+\n+        // Verify field paths include \""element\"" for multiset element access\n+        SchemaChange.UpdateColumnType typeChange =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.UpdateColumnType)\n+                        .map(change -> (SchemaChange.UpdateColumnType) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(typeChange).isNotNull();\n+        assertThat(typeChange.fieldNames()).containsExactly(\""multiset_column\"", \""element\"", \""id\"");\n+\n+        SchemaChange.AddColumn addColumn =\n+                schemaChanges.stream()\n+                        .filter(change -> change instanceof SchemaChange.AddColumn)\n+                        .map(change -> (SchemaChange.AddColumn) change)\n+                        .findFirst()\n+                        .orElse(null);\n+        assertThat(addColumn).isNotNull();\n+        assertThat(addColumn.fieldNames()).containsExactly(\""multiset_column\"", \""element\"", \""name\"");\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java\nindex 24d0a51453b3..a201e8308c52 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBaseTest.java\n@@ -19,15 +19,23 @@\n package org.apache.paimon.flink.sink.cdc;\n \n import org.apache.paimon.flink.action.cdc.TypeMapping;\n+import org.apache.paimon.types.ArrayType;\n import org.apache.paimon.types.BigIntType;\n+import org.apache.paimon.types.DataField;\n+import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.DecimalType;\n import org.apache.paimon.types.IntType;\n+import org.apache.paimon.types.MapType;\n+import org.apache.paimon.types.MultisetType;\n+import org.apache.paimon.types.RowType;\n import org.apache.paimon.types.SmallIntType;\n import org.apache.paimon.types.TimestampType;\n import org.apache.paimon.types.VarCharType;\n \n import org.junit.Test;\n \n+import java.util.Arrays;\n+\n import static org.junit.jupiter.api.Assertions.assertEquals;\n \n /** IT cases for {@link UpdatedDataFieldsProcessFunctionBaseTest}. */\n@@ -104,4 +112,508 @@ public void testCanConvertTimestamp() {\n \n         assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.IGNORE, convertAction);\n     }\n+\n+    @Test\n+    public void testArrayNullabilityEvolution() {\n+        // Test 1: Nullable array to non-nullable array (should fail)\n+        ArrayType oldType = new ArrayType(true, DataTypes.INT());\n+        ArrayType newType = new ArrayType(false, DataTypes.INT());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+\n+        // Test 2: Non-nullable array to nullable array (should succeed)\n+        oldType = new ArrayType(false, DataTypes.INT());\n+        newType = new ArrayType(true, DataTypes.INT());\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+\n+        // Test 3: Array with nullable elements to non-nullable elements (should fail)\n+        oldType = new ArrayType(true, new IntType(true));\n+        newType = new ArrayType(true, new IntType(false));\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testNestedRowNullabilityEvolution() {\n+        // Old type: ROW(f1 ROW(inner1 INT NULL, inner2 STRING) NULL)\n+        RowType oldInnerType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""inner1\"", new IntType(true)),\n+                                new DataField(2, \""inner2\"", DataTypes.STRING())));\n+        RowType oldRowType = new RowType(true, Arrays.asList(new DataField(1, \""f1\"", oldInnerType)));\n+\n+        // Test 1: Making nested row non-nullable (should fail)\n+        RowType newInnerType =\n+                new RowType(\n+                        false, // Changed to non-nullable\n+                        Arrays.asList(\n+                                new DataField(1, \""inner1\"", new IntType(true)),\n+                                new DataField(2, \""inner2\"", DataTypes.STRING())));\n+        RowType newRowType = new RowType(true, Arrays.asList(new DataField(1, \""f1\"", newInnerType)));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+\n+        // Test 2: Making nested field non-nullable (should fail)\n+        newInnerType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(\n+                                        1, \""inner1\"", new IntType(false)), // Changed to non-nullable\n+                                new DataField(2, \""inner2\"", DataTypes.STRING())));\n+        newRowType = new RowType(true, Arrays.asList(new DataField(1, \""f1\"", newInnerType)));\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testMultisetNullabilityEvolution() {\n+        // Test 1: Nullable multiset to non-nullable multiset (should fail)\n+        MultisetType oldType = new MultisetType(true, DataTypes.INT());\n+        MultisetType newType = new MultisetType(false, DataTypes.INT());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+\n+        // Test 2: Non-nullable multiset to nullable multiset (should succeed)\n+        oldType = new MultisetType(false, DataTypes.INT());\n+        newType = new MultisetType(true, DataTypes.INT());\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+\n+        // Test 3: Multiset with nullable elements to non-nullable elements (should fail)\n+        oldType = new MultisetType(true, new IntType(true));\n+        newType = new MultisetType(true, new IntType(false));\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testComplexNestedNullabilityEvolution() {\n+        // Old type: ROW(\n+        //   f1 ARRAY<ROW<x INT NULL, y STRING> NULL> NULL,\n+        //   f2 MAP<STRING, ROW<a INT NULL, b STRING> NULL> NULL\n+        // )\n+        RowType oldArrayInnerType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""x\"", new IntType(true)),\n+                                new DataField(2, \""y\"", DataTypes.STRING())));\n+        ArrayType oldArrayType = new ArrayType(true, oldArrayInnerType);\n+\n+        RowType oldMapValueType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""a\"", new IntType(true)),\n+                                new DataField(2, \""b\"", DataTypes.STRING())));\n+        MapType oldMapType = new MapType(true, DataTypes.STRING(), oldMapValueType);\n+\n+        RowType oldRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", oldArrayType),\n+                                new DataField(2, \""f2\"", oldMapType)));\n+\n+        // Test 1: Making nested array's row type non-nullable (should fail)\n+        RowType newArrayInnerType =\n+                new RowType(\n+                        false, // Changed to non-nullable\n+                        Arrays.asList(\n+                                new DataField(1, \""x\"", new IntType(true)),\n+                                new DataField(2, \""y\"", DataTypes.STRING())));\n+        ArrayType newArrayType = new ArrayType(true, newArrayInnerType);\n+        MapType newMapType = oldMapType; // Keep the same\n+\n+        RowType newRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", newArrayType),\n+                                new DataField(2, \""f2\"", newMapType)));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+\n+        // Test 2: Making map's value type's field non-nullable (should fail)\n+        newArrayType = oldArrayType; // Restore to original\n+        RowType newMapValueType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(\n+                                        1, \""a\"", new IntType(false)), // Changed to non-nullable\n+                                new DataField(2, \""b\"", DataTypes.STRING())));\n+        newMapType = new MapType(true, DataTypes.STRING(), newMapValueType);\n+\n+        newRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", newArrayType),\n+                                new DataField(2, \""f2\"", newMapType)));\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testNestedRowTypeConversion() {\n+        // Old type: ROW(f1 INT, f2 STRING)\n+        RowType oldRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.INT()),\n+                                new DataField(2, \""f2\"", DataTypes.STRING())));\n+\n+        // New type: ROW(f1 BIGINT, f2 STRING, f3 DOUBLE)\n+        RowType newRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.BIGINT()),\n+                                new DataField(2, \""f2\"", DataTypes.STRING()),\n+                                new DataField(3, \""f3\"", DataTypes.DOUBLE())));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testNestedArrayTypeConversion() {\n+        // Old type: ARRAY<INT>\n+        ArrayType oldArrayType = new ArrayType(true, DataTypes.INT());\n+\n+        // New type: ARRAY<BIGINT>\n+        ArrayType newArrayType = new ArrayType(true, DataTypes.BIGINT());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldArrayType, newArrayType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testNestedMapTypeConversion() {\n+        // Old type: MAP<STRING, INT>\n+        MapType oldMapType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+\n+        // New type: MAP<STRING, BIGINT>\n+        MapType newMapType = new MapType(true, DataTypes.STRING(), DataTypes.BIGINT());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldMapType, newMapType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testMapWithNullableComplexTypes() {\n+        // Old type: MAP<STRING, ROW<f1 ARRAY<INT> NULL, f2 ROW<x INT, y STRING> NULL>>\n+        RowType oldInnerRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""x\"", DataTypes.INT()),\n+                                new DataField(2, \""y\"", DataTypes.STRING())));\n+\n+        RowType oldValueType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", new ArrayType(true, DataTypes.INT())),\n+                                new DataField(2, \""f2\"", oldInnerRowType)));\n+\n+        MapType oldType = new MapType(true, DataTypes.STRING(), oldValueType);\n+\n+        // New type: MAP<STRING, ROW<f1 ARRAY<BIGINT> NULL, f2 ROW<x BIGINT, y STRING, z DOUBLE>\n+        // NULL>>\n+        RowType newInnerRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""x\"", DataTypes.BIGINT()),\n+                                new DataField(2, \""y\"", DataTypes.STRING()),\n+                                new DataField(3, \""z\"", DataTypes.DOUBLE())));\n+\n+        RowType newValueType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", new ArrayType(true, DataTypes.BIGINT())),\n+                                new DataField(2, \""f2\"", newInnerRowType)));\n+\n+        MapType newType = new MapType(true, DataTypes.STRING(), newValueType);\n+\n+        // Test compatible evolution\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+\n+        // Test incompatible element type\n+        RowType incompatibleValueType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(\n+                                        1,\n+                                        \""f1\"",\n+                                        new ArrayType(\n+                                                true, DataTypes.STRING())), // INT to STRING is\n+                                // incompatible\n+                                new DataField(2, \""f2\"", newInnerRowType)));\n+\n+        MapType incompatibleType = new MapType(true, DataTypes.STRING(), incompatibleValueType);\n+\n+        convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, incompatibleType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testComplexNestedTypeConversion() {\n+        // Old type: ARRAY<MAP<INT, ROW(f1 INT, f2 STRING)>>\n+        RowType oldInnerRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.INT()),\n+                                new DataField(2, \""f2\"", DataTypes.STRING())));\n+        MapType oldMapType = new MapType(true, DataTypes.INT(), oldInnerRowType);\n+        ArrayType oldType = new ArrayType(true, oldMapType);\n+\n+        // New type: ARRAY<MAP<INT, ROW(f1 BIGINT, f2 STRING, f3 DOUBLE)>>\n+        RowType newInnerRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.BIGINT()),\n+                                new DataField(2, \""f2\"", DataTypes.STRING()),\n+                                new DataField(3, \""f3\"", DataTypes.DOUBLE())));\n+        MapType newMapType = new MapType(true, DataTypes.INT(), newInnerRowType);\n+        ArrayType newType = new ArrayType(true, newMapType);\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testRowTypeWithMultipleChanges() {\n+        // Old type: ROW(f1 INT, f2 STRING, f3 DOUBLE)\n+        RowType oldRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.INT()),\n+                                new DataField(2, \""f2\"", DataTypes.STRING()),\n+                                new DataField(3, \""f3\"", DataTypes.DOUBLE())));\n+\n+        // New type: ROW(f1 BIGINT, f2 VARCHAR(100), f4 INT)\n+        RowType newRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.BIGINT()),\n+                                new DataField(2, \""f2\"", new VarCharType(true, 100)),\n+                                new DataField(4, \""f4\"", DataTypes.INT())));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testNonNullableFieldRemoval() {\n+        // Old type: ROW(f1 INT NOT NULL, f2 STRING)\n+        RowType oldRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", new IntType(false)),\n+                                new DataField(2, \""f2\"", DataTypes.STRING())));\n+\n+        // New type: ROW(f2 STRING)\n+        RowType newRowType =\n+                new RowType(true, Arrays.asList(new DataField(2, \""f2\"", DataTypes.STRING())));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testAddingNonNullableField() {\n+        // Old type: ROW(f1 INT)\n+        RowType oldRowType =\n+                new RowType(true, Arrays.asList(new DataField(1, \""f1\"", DataTypes.INT())));\n+\n+        // New type: ROW(f1 INT, f2 STRING NOT NULL)\n+        RowType newRowType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""f1\"", DataTypes.INT()),\n+                                new DataField(2, \""f2\"", new VarCharType(false, 100))));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testNestedRowWithinRowEvolution() {\n+        // Old type: ROW(f1 ROW(inner1 INT, inner2 STRING))\n+        RowType oldInnerType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""inner1\"", DataTypes.INT()),\n+                                new DataField(2, \""inner2\"", DataTypes.STRING())));\n+        RowType oldRowType = new RowType(true, Arrays.asList(new DataField(1, \""f1\"", oldInnerType)));\n+\n+        // New type: ROW(f1 ROW(inner1 BIGINT, inner2 STRING, inner3 DOUBLE))\n+        RowType newInnerType =\n+                new RowType(\n+                        true,\n+                        Arrays.asList(\n+                                new DataField(1, \""inner1\"", DataTypes.BIGINT()),\n+                                new DataField(2, \""inner2\"", DataTypes.STRING()),\n+                                new DataField(3, \""inner3\"", DataTypes.DOUBLE())));\n+        RowType newRowType = new RowType(true, Arrays.asList(new DataField(1, \""f1\"", newInnerType)));\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldRowType, newRowType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testArrayOfArraysEvolution() {\n+        // Old type: ARRAY<ARRAY<INT>>\n+        ArrayType oldInnerArray = new ArrayType(true, DataTypes.INT());\n+        ArrayType oldType = new ArrayType(true, oldInnerArray);\n+\n+        // New type: ARRAY<ARRAY<BIGINT>>\n+        ArrayType newInnerArray = new ArrayType(true, DataTypes.BIGINT());\n+        ArrayType newType = new ArrayType(true, newInnerArray);\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testArrayWithIncompatibleElementType() {\n+        // Old type: ARRAY<INT>\n+        ArrayType oldType = new ArrayType(true, DataTypes.INT());\n+\n+        // New type: ARRAY<STRING>\n+        ArrayType newType = new ArrayType(true, DataTypes.STRING());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testMapOfMapsEvolution() {\n+        // Old type: MAP<STRING, MAP<INT, STRING>>\n+        MapType oldInnerMap = new MapType(true, DataTypes.INT(), DataTypes.STRING());\n+        MapType oldType = new MapType(true, DataTypes.STRING(), oldInnerMap);\n+\n+        // New type: MAP<STRING, MAP<INT, VARCHAR(100)>>\n+        MapType newInnerMap = new MapType(true, DataTypes.INT(), new VarCharType(true, 100));\n+        MapType newType = new MapType(true, DataTypes.STRING(), newInnerMap);\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.IGNORE, convertAction);\n+    }\n+\n+    @Test\n+    public void testMapWithIncompatibleValueType() {\n+        // Old type: MAP<STRING, INT>\n+        MapType oldType = new MapType(true, DataTypes.STRING(), DataTypes.INT());\n+\n+        // New type: MAP<STRING, BOOLEAN>\n+        MapType newType = new MapType(true, DataTypes.STRING(), DataTypes.BOOLEAN());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n+\n+    @Test\n+    public void testMultisetTypeEvolution() {\n+        // Old type: MULTISET<INT>\n+        MultisetType oldType = new MultisetType(true, DataTypes.INT());\n+\n+        // New type: MULTISET<BIGINT>\n+        MultisetType newType = new MultisetType(true, DataTypes.BIGINT());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.CONVERT, convertAction);\n+    }\n+\n+    @Test\n+    public void testIncompatibleMultisetTypeEvolution() {\n+        // Old type: MULTISET<INT>\n+        MultisetType oldType = new MultisetType(true, DataTypes.INT());\n+\n+        // New type: MULTISET<STRING>\n+        MultisetType newType = new MultisetType(true, DataTypes.STRING());\n+\n+        UpdatedDataFieldsProcessFunctionBase.ConvertAction convertAction =\n+                UpdatedDataFieldsProcessFunctionBase.canConvert(\n+                        oldType, newType, TypeMapping.defaultMapping());\n+        assertEquals(UpdatedDataFieldsProcessFunctionBase.ConvertAction.EXCEPTION, convertAction);\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5674"", ""pr_id"": 5674, ""issue_id"": 3627, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Multi-Location Management\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\n\r\nCurrently, Paimon metadata does not store the absolute paths of files but uses relative paths to construct the absolute file paths instead. This is very cool because it saves a long string of identical path prefixes.\r\nHowever, this also limits scenarios that require the use of different locations. For example, our warehouse is an internally hosted HDFS cluster, but for the purpose of saving resources, we would like to implement tiered storage. This means keeping only the hot data in the internal HDFS cluster and moving the cold data to public cloud object storage, which can save a lot of costs. But without support for table location management, we cannot achieve this in Paimon.\r\n\r\n### Solution\r\n\r\nTherefore, I suggest introducing **the ability to manage relative paths**   in Paimon (this does not include the management of metadata paths such as snapshots and schemas, as these metadata always relies on the warehouse path), allowing table data to be stored in different locations.\r\n\r\n### Anything else?\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 202, ""test_files_count"": 7, ""non_test_files_count"": 8, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/core_configuration.html"", ""paimon-api/src/main/java/org/apache/paimon/CoreOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/append/BucketedAppendCompactManager.java"", ""paimon-core/src/main/java/org/apache/paimon/mergetree/compact/CompactStrategy.java"", ""paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManager.java"", ""paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactTask.java"", ""paimon-core/src/main/java/org/apache/paimon/operation/BucketedAppendFileStoreWrite.java"", ""paimon-core/src/main/java/org/apache/paimon/operation/KeyValueFileStoreWrite.java"", ""paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java"", ""paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java"", ""paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java"", ""paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java"", ""paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java"", ""paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java"", ""paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java""], ""base_commit"": ""b0bb00fc91ef122c429aee8976f833eddbc6eab8"", ""head_commit"": ""8d6e8cec24cfe6263311d201b25f76a159ae8c6b"", ""repo_url"": ""https://github.com/apache/paimon/pull/5674"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5674"", ""dockerfile"": """", ""pr_merged_at"": ""2025-06-15T01:38:27.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/core_configuration.html b/docs/layouts/shortcodes/generated/core_configuration.html\nindex 020b03c0c812..dde7daf30960 100644\n--- a/docs/layouts/shortcodes/generated/core_configuration.html\n+++ b/docs/layouts/shortcodes/generated/core_configuration.html\n@@ -182,6 +182,12 @@\n             <td>Double</td>\n             <td>Ratio of the deleted rows in a data file to be forced compacted for append-only table.</td>\n         </tr>\n+        <tr>\n+            <td><h5>compaction.force-compact-all-files</h5></td>\n+            <td style=\""word-wrap: break-word;\"">false</td>\n+            <td>Boolean</td>\n+            <td>Whether to force pick all files for a full compaction. Usually seen in a compaction task to external paths.</td>\n+        </tr>\n         <tr>\n             <td><h5>compaction.force-up-level-0</h5></td>\n             <td style=\""word-wrap: break-word;\"">false</td>\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/CoreOptions.java b/paimon-api/src/main/java/org/apache/paimon/CoreOptions.java\nindex e79d1593d5db..4ded978ee691 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/CoreOptions.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/CoreOptions.java\n@@ -151,6 +151,13 @@ public class CoreOptions implements Serializable {\n                                     + ExternalPathStrategy.SPECIFIC_FS\n                                     + \"", should be the prefix scheme of the external path, now supported are s3 and oss.\"");\n \n+    public static final ConfigOption<Boolean> COMPACTION_FORCE_COMPACT_ALL_FILES =\n+            key(\""compaction.force-compact-all-files\"")\n+                    .booleanType()\n+                    .defaultValue(false)\n+                    .withDescription(\n+                            \""Whether to force pick all files for a full compaction. Usually seen in a compaction task to external paths.\"");\n+\n     @ExcludeFromDocumentation(\""Internal use only\"")\n     public static final ConfigOption<String> PATH =\n             key(\""path\"")\n@@ -2466,6 +2473,10 @@ public String externalSpecificFS() {\n         return options.get(DATA_FILE_EXTERNAL_PATHS_SPECIFIC_FS);\n     }\n \n+    public Boolean forceCompactAllFiles() {\n+        return options.get(COMPACTION_FORCE_COMPACT_ALL_FILES);\n+    }\n+\n     public String partitionTimestampFormatter() {\n         return options.get(PARTITION_TIMESTAMP_FORMATTER);\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/append/BucketedAppendCompactManager.java b/paimon-core/src/main/java/org/apache/paimon/append/BucketedAppendCompactManager.java\nindex d5c47c3b883e..520fd8edb3fc 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/append/BucketedAppendCompactManager.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/append/BucketedAppendCompactManager.java\n@@ -60,6 +60,7 @@ public class BucketedAppendCompactManager extends CompactFutureManager {\n     private final PriorityQueue<DataFileMeta> toCompact;\n     private final int minFileNum;\n     private final long targetFileSize;\n+    private final boolean forceCompactAllFiles;\n     private final CompactRewriter rewriter;\n \n     private List<DataFileMeta> compacting;\n@@ -72,6 +73,7 @@ public BucketedAppendCompactManager(\n             @Nullable DeletionVectorsMaintainer dvMaintainer,\n             int minFileNum,\n             long targetFileSize,\n+            boolean forceCompactAllFiles,\n             CompactRewriter rewriter,\n             @Nullable CompactionMetrics.Reporter metricsReporter) {\n         this.executor = executor;\n@@ -80,6 +82,7 @@ public BucketedAppendCompactManager(\n         this.toCompact.addAll(restored);\n         this.minFileNum = minFileNum;\n         this.targetFileSize = targetFileSize;\n+        this.forceCompactAllFiles = forceCompactAllFiles;\n         this.rewriter = rewriter;\n         this.metricsReporter = metricsReporter;\n     }\n@@ -98,9 +101,10 @@ private void triggerFullCompaction() {\n                 taskFuture == null,\n                 \""A compaction task is still running while the user \""\n                         + \""forces a new compaction. This is unexpected.\"");\n-        // if deletion vector enables, always trigger compaction.\n-        if (toCompact.isEmpty()\n-                || (dvMaintainer == null && toCompact.size() < FULL_COMPACT_MIN_FILE)) {\n+        // if all files are force picked or deletion vector enables, always trigger compaction.\n+        if (!forceCompactAllFiles\n+                && (toCompact.isEmpty()\n+                        || (dvMaintainer == null && toCompact.size() < FULL_COMPACT_MIN_FILE))) {\n             return;\n         }\n \n@@ -114,6 +118,7 @@ private void triggerFullCompaction() {\n                                 dvMaintainer,\n                                 toCompact,\n                                 targetFileSize,\n+                                forceCompactAllFiles,\n                                 rewriter,\n                                 metricsReporter));\n         recordCompactionsQueuedRequest();\n@@ -238,25 +243,28 @@ public static class FullCompactTask extends CompactTask {\n         private final DeletionVectorsMaintainer dvMaintainer;\n         private final LinkedList<DataFileMeta> toCompact;\n         private final long targetFileSize;\n+        private final boolean forceCompactAllFiles;\n         private final CompactRewriter rewriter;\n \n         public FullCompactTask(\n                 DeletionVectorsMaintainer dvMaintainer,\n                 Collection<DataFileMeta> inputs,\n                 long targetFileSize,\n+                boolean forceCompactAllFiles,\n                 CompactRewriter rewriter,\n                 @Nullable CompactionMetrics.Reporter metricsReporter) {\n             super(metricsReporter);\n             this.dvMaintainer = dvMaintainer;\n             this.toCompact = new LinkedList<>(inputs);\n             this.targetFileSize = targetFileSize;\n+            this.forceCompactAllFiles = forceCompactAllFiles;\n             this.rewriter = rewriter;\n         }\n \n         @Override\n         protected CompactResult doCompact() throws Exception {\n             // remove large files\n-            while (!toCompact.isEmpty()) {\n+            while (!forceCompactAllFiles && !toCompact.isEmpty()) {\n                 DataFileMeta file = toCompact.peekFirst();\n                 // the data file with deletion file always need to be compacted.\n                 if (file.fileSize() >= targetFileSize && !hasDeletionFile(file)) {\n@@ -281,7 +289,8 @@ protected CompactResult doCompact() throws Exception {\n                         small++;\n                     }\n                 }\n-                if (small > big && toCompact.size() >= FULL_COMPACT_MIN_FILE) {\n+                if (forceCompactAllFiles\n+                        || (small > big && toCompact.size() >= FULL_COMPACT_MIN_FILE)) {\n                     return compact(null, toCompact, rewriter);\n                 } else {\n                     return result(emptyList(), emptyList());\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/CompactStrategy.java b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/CompactStrategy.java\nindex ec82e9e530e0..ef56196e0a66 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/CompactStrategy.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/CompactStrategy.java\n@@ -55,7 +55,8 @@ static Optional<CompactUnit> pickFullCompaction(\n             int numLevels,\n             List<LevelSortedRun> runs,\n             @Nullable RecordLevelExpire recordLevelExpire,\n-            @Nullable DeletionVectorsMaintainer dvMaintainer) {\n+            @Nullable DeletionVectorsMaintainer dvMaintainer,\n+            boolean forceCompactAllFiles) {\n         int maxLevel = numLevels - 1;\n         if (runs.isEmpty()) {\n             // no sorted run, no need to compact\n@@ -64,7 +65,10 @@ static Optional<CompactUnit> pickFullCompaction(\n             List<DataFileMeta> filesToBeCompacted = new ArrayList<>();\n \n             for (DataFileMeta file : runs.get(0).run().files()) {\n-                if (recordLevelExpire != null && recordLevelExpire.isExpireFile(file)) {\n+                if (forceCompactAllFiles) {\n+                    // add all files when force compacted\n+                    filesToBeCompacted.add(file);\n+                } else if (recordLevelExpire != null && recordLevelExpire.isExpireFile(file)) {\n                     // check record level expire for large files\n                     filesToBeCompacted.add(file);\n                 } else if (dvMaintainer != null\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManager.java b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManager.java\nindex a4852e73464e..953a23539110 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManager.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManager.java\n@@ -65,6 +65,7 @@ public class MergeTreeCompactManager extends CompactFutureManager {\n     @Nullable private final DeletionVectorsMaintainer dvMaintainer;\n     private final boolean lazyGenDeletionFile;\n     private final boolean needLookup;\n+    private final boolean forceCompactAllFiles;\n \n     @Nullable private final RecordLevelExpire recordLevelExpire;\n \n@@ -80,7 +81,8 @@ public MergeTreeCompactManager(\n             @Nullable DeletionVectorsMaintainer dvMaintainer,\n             boolean lazyGenDeletionFile,\n             boolean needLookup,\n-            @Nullable RecordLevelExpire recordLevelExpire) {\n+            @Nullable RecordLevelExpire recordLevelExpire,\n+            boolean forceCompactAllFiles) {\n         this.executor = executor;\n         this.levels = levels;\n         this.strategy = strategy;\n@@ -93,6 +95,7 @@ public MergeTreeCompactManager(\n         this.lazyGenDeletionFile = lazyGenDeletionFile;\n         this.recordLevelExpire = recordLevelExpire;\n         this.needLookup = needLookup;\n+        this.forceCompactAllFiles = forceCompactAllFiles;\n \n         MetricUtils.safeCall(this::reportMetrics, LOG);\n     }\n@@ -135,7 +138,11 @@ public void triggerCompaction(boolean fullCompaction) {\n             }\n             optionalUnit =\n                     CompactStrategy.pickFullCompaction(\n-                            levels.numberOfLevels(), runs, recordLevelExpire, dvMaintainer);\n+                            levels.numberOfLevels(),\n+                            runs,\n+                            recordLevelExpire,\n+                            dvMaintainer,\n+                            forceCompactAllFiles);\n         } else {\n             if (taskFuture != null) {\n                 return;\n@@ -210,7 +217,8 @@ private void submitCompaction(CompactUnit unit, boolean dropDelete) {\n                         metricsReporter,\n                         compactDfSupplier,\n                         dvMaintainer,\n-                        recordLevelExpire);\n+                        recordLevelExpire,\n+                        forceCompactAllFiles);\n         if (LOG.isDebugEnabled()) {\n             LOG.debug(\n                     \""Pick these files (name, level, size) for compaction: {}\"",\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactTask.java b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactTask.java\nindex 0ce9fbb4c202..4aa10e9b7953 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactTask.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/mergetree/compact/MergeTreeCompactTask.java\n@@ -55,6 +55,7 @@ public class MergeTreeCompactTask extends CompactTask {\n     private int upgradeFilesNum;\n \n     @Nullable private final RecordLevelExpire recordLevelExpire;\n+    private final boolean forceCompactAllFiles;\n     @Nullable private final DeletionVectorsMaintainer dvMaintainer;\n \n     public MergeTreeCompactTask(\n@@ -67,7 +68,8 @@ public MergeTreeCompactTask(\n             @Nullable CompactionMetrics.Reporter metricsReporter,\n             Supplier<CompactDeletionFile> compactDfSupplier,\n             @Nullable DeletionVectorsMaintainer dvMaintainer,\n-            @Nullable RecordLevelExpire recordLevelExpire) {\n+            @Nullable RecordLevelExpire recordLevelExpire,\n+            boolean forceCompactAllFiles) {\n         super(metricsReporter);\n         this.minFileSize = minFileSize;\n         this.rewriter = rewriter;\n@@ -78,6 +80,7 @@ public MergeTreeCompactTask(\n         this.dropDelete = dropDelete;\n         this.maxLevel = maxLevel;\n         this.recordLevelExpire = recordLevelExpire;\n+        this.forceCompactAllFiles = forceCompactAllFiles;\n \n         this.upgradeFilesNum = 0;\n     }\n@@ -126,12 +129,14 @@ protected String logMetric(\n \n     private void upgrade(DataFileMeta file, CompactResult toUpdate) throws Exception {\n         if (file.level() == outputLevel) {\n-            if (isContainExpiredRecords(file)\n+            if (forceCompactAllFiles\n+                    || isContainExpiredRecords(file)\n                     || (dvMaintainer != null\n                             && dvMaintainer.deletionVectorOf(file.fileName()).isPresent())) {\n                 /*\n-                 * 1. if the large file in maxLevel has expired records, we need to rewrite it.\n-                 * 2. if the large file in maxLevel has corresponding deletion vector, we need to rewrite it.\n+                 * 1. if files are force picked, we need to rewrite all files.\n+                 * 2. if the large file in maxLevel has expired records, we need to rewrite it.\n+                 * 3. if the large file in maxLevel has corresponding deletion vector, we need to rewrite it.\n                  */\n                 rewriteFile(file, toUpdate);\n             }\n@@ -139,9 +144,9 @@ private void upgrade(DataFileMeta file, CompactResult toUpdate) throws Exception\n         }\n \n         if (outputLevel != maxLevel || file.deleteRowCount().map(d -> d == 0).orElse(false)) {\n-            if (isContainExpiredRecords(file)) {\n-                // if the file which could be directly upgraded has expired records, we need to\n-                // rewrite it\n+            if (forceCompactAllFiles || isContainExpiredRecords(file)) {\n+                // if all files are force picked, or the file which could be directly upgraded has\n+                // expired records, we need to rewrite it\n                 rewriteFile(file, toUpdate);\n             } else {\n                 CompactResult upgradeResult = rewriter.upgrade(outputLevel, file);\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/operation/BucketedAppendFileStoreWrite.java b/paimon-core/src/main/java/org/apache/paimon/operation/BucketedAppendFileStoreWrite.java\nindex 36201c7f7f0c..d8f2d9211b2b 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/operation/BucketedAppendFileStoreWrite.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/operation/BucketedAppendFileStoreWrite.java\n@@ -91,6 +91,7 @@ protected CompactManager getCompactManager(\n                     dvMaintainer,\n                     options.compactionMinFileNum(),\n                     options.targetFileSize(false),\n+                    options.forceCompactAllFiles(),\n                     files -> compactRewrite(partition, bucket, dvFactory, files),\n                     compactionMetrics == null\n                             ? null\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/operation/KeyValueFileStoreWrite.java b/paimon-core/src/main/java/org/apache/paimon/operation/KeyValueFileStoreWrite.java\nindex 9a65e83d0688..7c0b36701fe5 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/operation/KeyValueFileStoreWrite.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/operation/KeyValueFileStoreWrite.java\n@@ -290,7 +290,8 @@ private CompactManager createCompactManager(\n                     dvMaintainer,\n                     options.prepareCommitWaitCompaction(),\n                     options.needLookup(),\n-                    recordLevelExpire);\n+                    recordLevelExpire,\n+                    options.forceCompactAllFiles());\n         }\n     }\n \n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java b/paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java\nindex 2b280220ee70..8cb293cd8ee4 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/append/AppendOnlyWriterTest.java\n@@ -652,6 +652,7 @@ private Pair<AppendOnlyWriter, List<DataFileMeta>> createWriter(\n                         null,\n                         MIN_FILE_NUM,\n                         targetFileSize,\n+                        false,\n                         compactBefore -> {\n                             latch.await();\n                             return compactBefore.isEmpty()\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java b/paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java\nindex cfdf38558fdb..2f031548a846 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/append/BucketedAppendCompactManagerTest.java\n@@ -207,6 +207,7 @@ private void innerTest(\n                         null,\n                         minFileNum,\n                         targetFileSize,\n+                        false,\n                         null, // not used\n                         null);\n         Optional<List<DataFileMeta>> actual = manager.pickCompactBefore();\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java b/paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java\nindex e7c3cce01da3..262acd5d4f05 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/append/FullCompactTaskTest.java\n@@ -123,7 +123,7 @@ public MockFullCompactTask(\n                 Collection<DataFileMeta> inputs,\n                 long targetFileSize,\n                 BucketedAppendCompactManager.CompactRewriter rewriter) {\n-            super(null, inputs, targetFileSize, rewriter, null);\n+            super(null, inputs, targetFileSize, false, rewriter, null);\n         }\n     }\n \n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java b/paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java\nindex c43b3c20c6a8..c6761d9c929d 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/format/FileFormatSuffixTest.java\n@@ -87,7 +87,7 @@ public void testFileSuffix(@TempDir java.nio.file.Path tempDir) throws Exception\n                         SCHEMA,\n                         0,\n                         new BucketedAppendCompactManager(\n-                                null, toCompact, null, 4, 10, null, null), // not used\n+                                null, toCompact, null, 4, 10, false, null, null), // not used\n                         null,\n                         false,\n                         dataFilePathFactory,\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java b/paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java\nindex 4af6c3980008..95daf384e797 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/mergetree/MergeTreeTestBase.java\n@@ -454,7 +454,8 @@ private MergeTreeCompactManager createCompactManager(\n                 null,\n                 false,\n                 options.needLookup(),\n-                null);\n+                null,\n+                false);\n     }\n \n     static class MockFailResultCompactionManager extends MergeTreeCompactManager {\n@@ -478,7 +479,8 @@ public MockFailResultCompactionManager(\n                     null,\n                     false,\n                     false,\n-                    null);\n+                    null,\n+                    false);\n         }\n \n         protected CompactResult obtainCompactResult()\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java b/paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java\nindex 424055597735..4adff9477857 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/mergetree/compact/MergeTreeCompactManagerTest.java\n@@ -208,7 +208,8 @@ public void testIsCompacting() {\n                         null,\n                         false,\n                         true,\n-                        null);\n+                        null,\n+                        false);\n \n         MergeTreeCompactManager defaultManager =\n                 new MergeTreeCompactManager(\n@@ -223,7 +224,8 @@ public void testIsCompacting() {\n                         null,\n                         false,\n                         false,\n-                        null);\n+                        null,\n+                        false);\n \n         assertThat(lookupManager.compactNotCompleted()).isTrue();\n         assertThat(defaultManager.compactNotCompleted()).isFalse();\n@@ -259,7 +261,8 @@ private void innerTest(\n                         null,\n                         false,\n                         false,\n-                        null);\n+                        null,\n+                        false);\n         manager.triggerCompaction(false);\n         manager.getCompactionResult(true);\n         List<LevelMinMax> outputs =\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java\nindex d79d13f0260c..d58c09d3a852 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/procedure/CompactProcedureITCase.java\n@@ -20,6 +20,7 @@\n \n import org.apache.paimon.Snapshot;\n import org.apache.paimon.flink.CatalogITCaseBase;\n+import org.apache.paimon.io.DataFileMeta;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.source.DataSplit;\n import org.apache.paimon.table.source.StreamTableScan;\n@@ -35,6 +36,7 @@\n import org.junit.jupiter.api.Test;\n \n import java.util.List;\n+import java.util.UUID;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n@@ -196,6 +198,71 @@ public void testHistoryPartitionCompact() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testForceCompactToExternalPath() throws Exception {\n+        // test for pk table\n+        String tmpPath = getTempDirPath(\""external/\"" + UUID.randomUUID());\n+        sql(\n+                \""CREATE TABLE Tpk (\""\n+                        + \"" k INT,\""\n+                        + \"" v INT,\""\n+                        + \"" hh INT,\""\n+                        + \"" dt STRING,\""\n+                        + \"" PRIMARY KEY (k, dt, hh) NOT ENFORCED\""\n+                        + \"") PARTITIONED BY (dt, hh) WITH (\""\n+                        + \"" 'write-only' = 'true',\""\n+                        + \"" 'bucket' = '1'\""\n+                        + \"")\"");\n+        FileStoreTable pkTable = paimonTable(\""Tpk\"");\n+\n+        sql(\n+                \""INSERT INTO Tpk VALUES (1, 100, 15, '20221208'), (1, 100, 16, '20221208'), (1, 100, 15, '20221209')\"");\n+        sql(\n+                \""INSERT INTO Tpk VALUES (2, 100, 15, '20221208'), (2, 100, 16, '20221208'), (2, 100, 15, '20221209')\"");\n+        tEnv.getConfig().set(TableConfigOptions.TABLE_DML_SYNC, true);\n+        sql(\n+                \""CALL sys.compact(`table` => 'default.Tpk',\""\n+                        + \"" options => 'compaction.force-compact-all-files=true,data-file.external-paths=file://%s,data-file.external-paths.strategy=specific-fs,data-file.external-paths.specific-fs=file')\"",\n+                tmpPath);\n+        List<DataSplit> splits = pkTable.newSnapshotReader().read().dataSplits();\n+        for (DataSplit split : splits) {\n+            for (DataFileMeta meta : split.dataFiles()) {\n+                assertThat(meta.externalPath().get().startsWith(\""file:\"" + tmpPath)).isTrue();\n+            }\n+        }\n+\n+        // test for append table\n+        tmpPath = getTempDirPath(\""external/\"" + UUID.randomUUID());\n+        sql(\n+                \""CREATE TABLE Tap (\""\n+                        + \"" k INT,\""\n+                        + \"" v INT,\""\n+                        + \"" hh INT,\""\n+                        + \"" dt STRING\""\n+                        + \"") PARTITIONED BY (dt, hh) WITH (\""\n+                        + \"" 'write-only' = 'true',\""\n+                        + \"" 'bucket' = '1',\""\n+                        + \"" 'bucket-key' = 'k'\""\n+                        + \"")\"");\n+        FileStoreTable apTable = paimonTable(\""Tap\"");\n+\n+        sql(\n+                \""INSERT INTO Tap VALUES (1, 100, 15, '20221208'), (1, 100, 16, '20221208'), (1, 100, 15, '20221209')\"");\n+        sql(\n+                \""INSERT INTO Tap VALUES (2, 100, 15, '20221208'), (2, 100, 16, '20221208'), (2, 100, 15, '20221209')\"");\n+        tEnv.getConfig().set(TableConfigOptions.TABLE_DML_SYNC, true);\n+        sql(\n+                \""CALL sys.compact(`table` => 'default.Tap',\""\n+                        + \"" options => 'compaction.force-compact-all-files=true,data-file.external-paths=file://%s,data-file.external-paths.strategy=specific-fs,data-file.external-paths.specific-fs=file')\"",\n+                tmpPath);\n+        splits = apTable.newSnapshotReader().read().dataSplits();\n+        for (DataSplit split : splits) {\n+            for (DataFileMeta meta : split.dataFiles()) {\n+                assertThat(meta.externalPath().get().startsWith(\""file:\"" + tmpPath)).isTrue();\n+            }\n+        }\n+    }\n+\n     // ----------------------- Sort Compact -----------------------\n \n     @Test\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5653"", ""pr_id"": 5653, ""issue_id"": 5628, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support limit the max snapshot count per checkpoint\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nWhen running the streaming compaction job and catching up the delay, the compact source may consume much snapshot in one checkpoint, which lead to\n\n- The checkpoint time is too long, because the input compaction files too much\n- The delta committable is too big to hold in the CommitterOperator.\n\nWe can limit the snapshot count per checkpoint to make the compaction job more smooth.\n\n### Solution\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 114, ""test_files_count"": 2, ""non_test_files_count"": 6, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/flink_connector_configuration.html"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkConnectorOptions.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumerator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileStoreSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumerator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java""], ""base_commit"": ""25e06f2bce707afd03ff99f519af3891da4c80b1"", ""head_commit"": ""a3a57070542314bee638081c99734090d5997f0c"", ""repo_url"": ""https://github.com/apache/paimon/pull/5653"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5653"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-26T23:54:08.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/flink_connector_configuration.html b/docs/layouts/shortcodes/generated/flink_connector_configuration.html\nindex 53f688427960..b73ffc9dffa4 100644\n--- a/docs/layouts/shortcodes/generated/flink_connector_configuration.html\n+++ b/docs/layouts/shortcodes/generated/flink_connector_configuration.html\n@@ -140,6 +140,12 @@\n             <td>Integer</td>\n             <td>If scan.infer-parallelism is true, limit the parallelism of source through this option.</td>\n         </tr>\n+        <tr>\n+            <td><h5>scan.max-snapshot.count</h5></td>\n+            <td style=\""word-wrap: break-word;\"">-1</td>\n+            <td>Integer</td>\n+            <td>The max snapshot count to scan per checkpoint. Not limited when it's negative.</td>\n+        </tr>\n         <tr>\n             <td><h5>scan.parallelism</h5></td>\n             <td style=\""word-wrap: break-word;\"">(none)</td>\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkConnectorOptions.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkConnectorOptions.java\nindex a0ff46335624..b39e9e57e386 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkConnectorOptions.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkConnectorOptions.java\n@@ -191,6 +191,13 @@ public class FlinkConnectorOptions {\n                             \""How many splits should assign to subtask per batch in StaticFileStoreSplitEnumerator \""\n                                     + \""to avoid exceed `akka.framesize` limit.\"");\n \n+    public static final ConfigOption<Integer> SCAN_MAX_SNAPSHOT_COUNT =\n+            key(\""scan.max-snapshot.count\"")\n+                    .intType()\n+                    .defaultValue(-1)\n+                    .withDescription(\n+                            \""The max snapshot count to scan per checkpoint. Not limited when it's negative.\"");\n+\n     public static final ConfigOption<SplitAssignMode> SCAN_SPLIT_ENUMERATOR_ASSIGN_MODE =\n             key(\""scan.split-enumerator.mode\"")\n                     .enumType(SplitAssignMode.class)\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumerator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumerator.java\nindex 56e19dcf628b..79597d0cd21f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumerator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumerator.java\n@@ -84,6 +84,10 @@ public class ContinuousFileSplitEnumerator\n \n     private boolean stopTriggerScan = false;\n \n+    private long handledSnapshotCount = 0;\n+\n+    private final int maxSnapshotCount;\n+\n     public ContinuousFileSplitEnumerator(\n             SplitEnumeratorContext<FileStoreSourceSplit> context,\n             Collection<FileStoreSourceSplit> remainSplits,\n@@ -92,7 +96,8 @@ public ContinuousFileSplitEnumerator(\n             StreamTableScan scan,\n             BucketMode bucketMode,\n             int splitMaxPerTask,\n-            boolean shuffleBucketWithPartition) {\n+            boolean shuffleBucketWithPartition,\n+            int maxSnapshotCount) {\n         checkArgument(discoveryInterval > 0L);\n         this.context = checkNotNull(context);\n         this.nextSnapshotId = nextSnapshotId;\n@@ -107,6 +112,7 @@ public ContinuousFileSplitEnumerator(\n \n         this.consumerProgressCalculator =\n                 new ConsumerProgressCalculator(context.currentParallelism());\n+        this.maxSnapshotCount = maxSnapshotCount;\n     }\n \n     @VisibleForTesting\n@@ -189,6 +195,7 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n         consumerProgressCalculator\n                 .notifyCheckpointComplete(checkpointId)\n                 .ifPresent(scan::notifyCheckpointComplete);\n+        handledSnapshotCount = 0;\n     }\n \n     // ------------------------------------------------------------------------\n@@ -200,8 +207,22 @@ protected synchronized Optional<PlanWithNextSnapshotId> scanNextSnapshot() {\n         if (splitAssigner.numberOfRemainingSplits() >= splitMaxNum) {\n             return Optional.empty();\n         }\n+        if (maxSnapshotCount > 0 && handledSnapshotCount >= maxSnapshotCount) {\n+            LOG.debug(\n+                    \""There is {} in-flight snapshot, pending to scan next snapshot.\"",\n+                    handledSnapshotCount);\n+            return Optional.empty();\n+        }\n+\n         TableScan.Plan plan = scan.plan();\n         Long nextSnapshotId = scan.checkpoint();\n+        if (nextSnapshotId != null && !plan.splits().isEmpty()) {\n+            if (this.nextSnapshotId == null) {\n+                handledSnapshotCount++;\n+            } else if (!nextSnapshotId.equals(this.nextSnapshotId)) {\n+                handledSnapshotCount++;\n+            }\n+        }\n         return Optional.of(new PlanWithNextSnapshotId(plan, nextSnapshotId));\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileStoreSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileStoreSource.java\nindex db39f90455b4..879f262bbdc0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileStoreSource.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/ContinuousFileStoreSource.java\n@@ -112,6 +112,7 @@ protected SplitEnumerator<FileStoreSourceSplit, PendingSplitsCheckpoint> buildEn\n                 scan,\n                 bucketMode,\n                 options.get(CoreOptions.SCAN_MAX_SPLITS_PER_TASK),\n-                options.get(FlinkConnectorOptions.STREAMING_READ_SHUFFLE_BUCKET_WITH_PARTITION));\n+                options.get(FlinkConnectorOptions.STREAMING_READ_SHUFFLE_BUCKET_WITH_PARTITION),\n+                options.get(FlinkConnectorOptions.SCAN_MAX_SNAPSHOT_COUNT));\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumerator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumerator.java\nindex 4ed4461dd796..857c1b7ab7de 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumerator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumerator.java\n@@ -95,7 +95,8 @@ public AlignedContinuousFileSplitEnumerator(\n             BucketMode bucketMode,\n             long alignTimeout,\n             int splitPerTaskMax,\n-            boolean shuffleBucketWithPartition) {\n+            boolean shuffleBucketWithPartition,\n+            int maxSnapshotCount) {\n         super(\n                 context,\n                 remainSplits,\n@@ -104,7 +105,8 @@ public AlignedContinuousFileSplitEnumerator(\n                 scan,\n                 bucketMode,\n                 splitPerTaskMax,\n-                shuffleBucketWithPartition);\n+                shuffleBucketWithPartition,\n+                maxSnapshotCount);\n         this.pendingPlans = new ArrayBlockingQueue<>(MAX_PENDING_PLAN);\n         this.alignedAssigner = (AlignedSplitAssigner) super.splitAssigner;\n         this.nextSnapshotId = nextSnapshotId;\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\nindex 1b3e7b5b19af..8904720c908b 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\n@@ -85,6 +85,7 @@ protected SplitEnumerator<FileStoreSourceSplit, PendingSplitsCheckpoint> buildEn\n                 bucketMode,\n                 options.get(FlinkConnectorOptions.SOURCE_CHECKPOINT_ALIGN_TIMEOUT).toMillis(),\n                 options.get(CoreOptions.SCAN_MAX_SPLITS_PER_TASK),\n-                options.get(FlinkConnectorOptions.STREAMING_READ_SHUFFLE_BUCKET_WITH_PARTITION));\n+                options.get(FlinkConnectorOptions.STREAMING_READ_SHUFFLE_BUCKET_WITH_PARTITION),\n+                options.get(FlinkConnectorOptions.SCAN_MAX_SNAPSHOT_COUNT));\n     }\n }\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java\nindex 6f05e5dd7d5f..e3912cc70762 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/ContinuousFileSplitEnumeratorTest.java\n@@ -802,6 +802,56 @@ public void testEnumeratorSplitMax() throws Exception {\n         Assertions.assertThat(enumerator.splitAssigner.numberOfRemainingSplits()).isEqualTo(15 * 2);\n     }\n \n+    @Test\n+    public void testEnumeratorSnapshotMax() throws Exception {\n+        final TestingSplitEnumeratorContext<FileStoreSourceSplit> context =\n+                getSplitEnumeratorContext(2);\n+\n+        TreeMap<Long, TableScan.Plan> results = new TreeMap<>();\n+        StreamTableScan scan = new MockScan(results);\n+        ContinuousFileSplitEnumerator enumerator =\n+                new Builder()\n+                        .setSplitEnumeratorContext(context)\n+                        .setInitialSplits(Collections.emptyList())\n+                        .setDiscoveryInterval(1)\n+                        .setScan(scan)\n+                        .withBucketMode(BucketMode.BUCKET_UNAWARE)\n+                        .withMaxSnapshotCount(1)\n+                        .build();\n+        enumerator.start();\n+\n+        long snapshot = 0;\n+        List<DataSplit> splits = new ArrayList<>();\n+        // splits 1\n+        splits.add(createDataSplit(snapshot++, 0, Collections.emptyList()));\n+        results.put(1L, new DataFilePlan(splits));\n+        context.triggerAllActions();\n+\n+        Assertions.assertThat(enumerator.splitAssigner.remainingSplits().size()).isEqualTo(1);\n+\n+        // splits 2\n+        splits = new ArrayList<>();\n+        splits.add(createDataSplit(snapshot++, 0, Collections.emptyList()));\n+        results.put(2L, new DataFilePlan(splits));\n+        context.triggerAllActions();\n+\n+        // The snapshot 2 is pending to scan.\n+        Assertions.assertThat(enumerator.splitAssigner.remainingSplits().size()).isEqualTo(1);\n+\n+        // consumed splits 1\n+        enumerator.handleSplitRequest(0, \""test\"");\n+        Assertions.assertThat(enumerator.splitAssigner.remainingSplits().size()).isEqualTo(0);\n+        context.triggerAllActions();\n+\n+        // no new snapshot is scanned, because checkpoint is not completed.\n+        Assertions.assertThat(enumerator.splitAssigner.remainingSplits().size()).isEqualTo(0);\n+\n+        enumerator.notifyCheckpointComplete(1);\n+        context.triggerAllActions();\n+        Assertions.assertThat(enumerator.splitAssigner.remainingSplits().size()).isEqualTo(1);\n+        Assertions.assertThat(enumerator.nextSnapshotId).isEqualTo(3);\n+    }\n+\n     private void triggerCheckpointAndComplete(\n             ContinuousFileSplitEnumerator enumerator, long checkpointId) throws Exception {\n         enumerator.snapshotState(checkpointId);\n@@ -850,6 +900,7 @@ private static class Builder {\n \n         private StreamTableScan scan;\n         private BucketMode bucketMode = BucketMode.HASH_FIXED;\n+        private int maxSnapshotCount = -1;\n \n         public Builder setSplitEnumeratorContext(\n                 SplitEnumeratorContext<FileStoreSourceSplit> context) {\n@@ -877,9 +928,22 @@ public Builder withBucketMode(BucketMode bucketMode) {\n             return this;\n         }\n \n+        public Builder withMaxSnapshotCount(int maxSnapshotCount) {\n+            this.maxSnapshotCount = maxSnapshotCount;\n+            return this;\n+        }\n+\n         public ContinuousFileSplitEnumerator build() {\n             return new ContinuousFileSplitEnumerator(\n-                    context, initialSplits, null, discoveryInterval, scan, bucketMode, 10, false);\n+                    context,\n+                    initialSplits,\n+                    null,\n+                    discoveryInterval,\n+                    scan,\n+                    bucketMode,\n+                    10,\n+                    false,\n+                    maxSnapshotCount);\n         }\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java\nindex a5edc2804061..a3c26ce80d89 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/align/AlignedContinuousFileSplitEnumeratorTest.java\n@@ -253,7 +253,8 @@ public AlignedContinuousFileSplitEnumerator build() {\n                     bucketMode,\n                     timeout,\n                     10,\n-                    false);\n+                    false,\n+                    -1);\n         }\n     }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5613"", ""pr_id"": 5613, ""issue_id"": 5420, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support function in paimon\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nThe defining function includes the parameter, output, and function body. The output result is a single value or a table. It is modeled after unity catalog\uff0cand it could support the following scenarios:\n1. Support filtering or encrypting column in row: select decode_email(email) from user; \n2. Support parameterized view: select filter_fruit_by_color('red');\n3. Support use the function in AI.\n\n### Solution\n\nProvide the ability to define function.\n[PIP-31: Introduce Function](https://cwiki.apache.org/confluence/display/PAIMON/PIP-31%3A+Introduce+Function)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 127, ""test_files_count"": 3, ""non_test_files_count"": 11, ""pr_changed_files"": [""docs/static/rest-catalog-open-api.yaml"", ""paimon-api/src/main/java/org/apache/paimon/function/Function.java"", ""paimon-api/src/main/java/org/apache/paimon/function/FunctionImpl.java"", ""paimon-api/src/main/java/org/apache/paimon/rest/RESTApi.java"", ""paimon-api/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-api/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java"", ""paimon-api/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java""], ""base_commit"": ""5690dff018ab4d6178f0f4f895593467b1478a68"", ""head_commit"": ""62b90696c0346f7cee40144eedec7d21aaf3b471"", ""repo_url"": ""https://github.com/apache/paimon/pull/5613"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5613"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-16T09:48:34.000Z"", ""patch"": ""diff --git a/docs/static/rest-catalog-open-api.yaml b/docs/static/rest-catalog-open-api.yaml\nindex 0ee548e92676..3619c12b0a5f 100644\n--- a/docs/static/rest-catalog-open-api.yaml\n+++ b/docs/static/rest-catalog-open-api.yaml\n@@ -1184,7 +1184,7 @@ paths:\n           $ref: '#/components/responses/ViewAlreadyExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n-  /v1/{prefix}/functions:\n+  /v1/{prefix}/databases/{database}/functions:\n     get:\n       tags:\n         - function\n@@ -1196,6 +1196,11 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n         - name: maxResults\n           in: query\n           schema:\n@@ -1214,6 +1219,8 @@ paths:\n                 $ref: '#/components/schemas/ListFunctionsResponse'\n         \""401\"":\n           $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/DatabaseNotExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n     post:\n@@ -1227,6 +1234,11 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n       requestBody:\n         content:\n           application/json:\n@@ -1239,12 +1251,14 @@ paths:\n           $ref: '#/components/responses/BadRequestErrorResponse'\n         \""401\"":\n           $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/DatabaseNotExistErrorResponse'\n         \""409\"":\n           $ref: '#/components/responses/FunctionAlreadyExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n \n-  /v1/{prefix}/functions/{function}:\n+  /v1/{prefix}/databases/{database}/functions/{function}:\n     get:\n       tags:\n         - function\n@@ -1256,6 +1270,11 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n         - name: function\n           in: path\n           required: true\n@@ -1271,7 +1290,7 @@ paths:\n         \""401\"":\n           $ref: '#/components/responses/UnauthorizedErrorResponse'\n         \""404\"":\n-          $ref: '#/components/responses/DatabaseNotExistErrorResponse'\n+          $ref: '#/components/responses/FunctionNotExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n     post:\n@@ -1285,6 +1304,11 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n         - name: function\n           in: path\n           required: true\n@@ -1315,6 +1339,11 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n         - name: function\n           in: path\n           required: true\n@@ -2483,8 +2512,6 @@ components:\n     GetFunctionResponse:\n       type: object\n       properties:\n-        uuid:\n-          type: string\n         name:\n           type: string\n         inputParams:\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/function/Function.java b/paimon-api/src/main/java/org/apache/paimon/function/Function.java\nindex 1f892387d4c0..b5d01e63c15b 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/function/Function.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/function/Function.java\n@@ -22,17 +22,18 @@\n \n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n \n /** Interface for function. */\n public interface Function {\n \n-    String uuid();\n-\n     String name();\n \n-    List<DataField> inputParams();\n+    String fullName();\n+\n+    Optional<List<DataField>> inputParams();\n \n-    List<DataField> returnParams();\n+    Optional<List<DataField>> returnParams();\n \n     boolean isDeterministic();\n \n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/function/FunctionImpl.java b/paimon-api/src/main/java/org/apache/paimon/function/FunctionImpl.java\nindex 85ad987de9bb..b0290ea50e22 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/function/FunctionImpl.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/function/FunctionImpl.java\n@@ -18,17 +18,17 @@\n \n package org.apache.paimon.function;\n \n+import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.types.DataField;\n \n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n \n /** Function implementation. */\n public class FunctionImpl implements Function {\n \n-    private final String uuid;\n-\n-    private final String name;\n+    private final Identifier identifier;\n \n     private final List<DataField> inputParams;\n \n@@ -43,16 +43,14 @@ public class FunctionImpl implements Function {\n     private final Map<String, String> options;\n \n     public FunctionImpl(\n-            String uuid,\n-            String functionName,\n+            Identifier identifier,\n             List<DataField> inputParams,\n             List<DataField> returnParams,\n             boolean deterministic,\n             Map<String, FunctionDefinition> definitions,\n             String comment,\n             Map<String, String> options) {\n-        this.uuid = uuid;\n-        this.name = functionName;\n+        this.identifier = identifier;\n         this.inputParams = inputParams;\n         this.returnParams = returnParams;\n         this.deterministic = deterministic;\n@@ -62,23 +60,23 @@ public FunctionImpl(\n     }\n \n     @Override\n-    public String uuid() {\n-        return this.uuid;\n+    public String name() {\n+        return identifier.getObjectName();\n     }\n \n     @Override\n-    public String name() {\n-        return this.name;\n+    public String fullName() {\n+        return identifier.getFullName();\n     }\n \n     @Override\n-    public List<DataField> inputParams() {\n-        return inputParams;\n+    public Optional<List<DataField>> inputParams() {\n+        return Optional.ofNullable(inputParams);\n     }\n \n     @Override\n-    public List<DataField> returnParams() {\n-        return returnParams;\n+    public Optional<List<DataField>> returnParams() {\n+        return Optional.ofNullable(returnParams);\n     }\n \n     @Override\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/rest/RESTApi.java b/paimon-api/src/main/java/org/apache/paimon/rest/RESTApi.java\nindex f05052065327..60355c5d1302 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/rest/RESTApi.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/rest/RESTApi.java\n@@ -702,38 +702,79 @@ public List<String> listBranches(Identifier identifier) {\n         return response.branches();\n     }\n \n-    /** TODO. */\n-    public List<String> listFunctions() {\n+    /**\n+     * List functions for database.\n+     *\n+     * @param databaseName\n+     * @return a list of function name\n+     */\n+    public List<String> listFunctions(String databaseName) {\n         return listDataFromPageApi(\n                 queryParams ->\n                         client.get(\n-                                resourcePaths.functions(),\n+                                resourcePaths.functions(databaseName),\n                                 queryParams,\n                                 ListFunctionsResponse.class,\n                                 restAuthFunction));\n     }\n \n-    /** TODO. */\n-    public GetFunctionResponse getFunction(String functionName) {\n+    /**\n+     * Get a function by identifier.\n+     *\n+     * @param identifier the identifier of the function to retrieve\n+     * @return the function response object\n+     * @throws NoSuchResourceException if the function does not exist\n+     * @throws ForbiddenException if the user lacks permission to access the function\n+     */\n+    public GetFunctionResponse getFunction(Identifier identifier) {\n         return client.get(\n-                resourcePaths.function(functionName), GetFunctionResponse.class, restAuthFunction);\n+                resourcePaths.function(identifier.getDatabaseName(), identifier.getObjectName()),\n+                GetFunctionResponse.class,\n+                restAuthFunction);\n     }\n \n-    /** TODO. */\n-    public void createFunction(org.apache.paimon.function.Function function) {\n+    /**\n+     * Create a function.\n+     *\n+     * @param identifier database name and function name.\n+     * @param function the function to be created\n+     * @throws AlreadyExistsException Exception thrown on HTTP 409 means a function already exists\n+     * @throws ForbiddenException Exception thrown on HTTP 403 means don't have the permission for\n+     *     creating function\n+     */\n+    public void createFunction(\n+            Identifier identifier, org.apache.paimon.function.Function function) {\n         client.post(\n-                resourcePaths.functions(), new CreateFunctionRequest(function), restAuthFunction);\n+                resourcePaths.functions(identifier.getDatabaseName()),\n+                new CreateFunctionRequest(function),\n+                restAuthFunction);\n     }\n \n-    /** TODO. */\n-    public void dropFunction(String functionName) {\n-        client.delete(resourcePaths.function(functionName), restAuthFunction);\n+    /**\n+     * Drop a function.\n+     *\n+     * @param identifier database name and function name.\n+     * @throws NoSuchResourceException Exception thrown on HTTP 404 means the function not exists\n+     * @throws ForbiddenException Exception thrown on HTTP 403 means don't have the permission for\n+     *     this function\n+     */\n+    public void dropFunction(Identifier identifier) {\n+        client.delete(\n+                resourcePaths.function(identifier.getDatabaseName(), identifier.getObjectName()),\n+                restAuthFunction);\n     }\n \n-    /** TODO. */\n-    public void alterFunction(String functionName, List<FunctionChange> changes) {\n+    /**\n+     * Alter a function.\n+     *\n+     * @param identifier database name and function name.\n+     * @param changes list of function changes to apply\n+     * @throws NoSuchResourceException if the function does not exist\n+     * @throws ForbiddenException if the user lacks permission to modify the function\n+     */\n+    public void alterFunction(Identifier identifier, List<FunctionChange> changes) {\n         client.post(\n-                resourcePaths.function(functionName),\n+                resourcePaths.function(identifier.getDatabaseName(), identifier.getObjectName()),\n                 new AlterFunctionRequest(changes),\n                 restAuthFunction);\n     }\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-api/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex ac79e977cbed..6c4f82e3bfa8 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -215,11 +215,17 @@ public String renameView() {\n         return SLASH.join(V1, prefix, VIEWS, \""rename\"");\n     }\n \n-    public String functions() {\n-        return SLASH.join(V1, prefix, FUNCTIONS);\n+    public String functions(String databaseName) {\n+        return SLASH.join(V1, prefix, DATABASES, encodeString(databaseName), FUNCTIONS);\n     }\n \n-    public String function(String functionName) {\n-        return SLASH.join(V1, prefix, FUNCTIONS, encodeString(functionName));\n+    public String function(String databaseName, String functionName) {\n+        return SLASH.join(\n+                V1,\n+                prefix,\n+                DATABASES,\n+                encodeString(databaseName),\n+                FUNCTIONS,\n+                encodeString(functionName));\n     }\n }\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java b/paimon-api/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java\nindex 31c816f835af..670099569119 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java\n@@ -84,8 +84,8 @@ public CreateFunctionRequest(\n \n     public CreateFunctionRequest(Function function) {\n         this.functionName = function.name();\n-        this.inputParams = function.inputParams();\n-        this.returnParams = function.returnParams();\n+        this.inputParams = function.inputParams().orElse(null);\n+        this.returnParams = function.returnParams().orElse(null);\n         this.deterministic = function.isDeterministic();\n         this.definitions = function.definitions();\n         this.comment = function.comment();\n\ndiff --git a/paimon-api/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java b/paimon-api/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java\nindex 27fce00ef8a8..22c95cc64861 100644\n--- a/paimon-api/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java\n+++ b/paimon-api/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java\n@@ -18,6 +18,7 @@\n \n package org.apache.paimon.rest.responses;\n \n+import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.function.Function;\n import org.apache.paimon.function.FunctionDefinition;\n import org.apache.paimon.function.FunctionImpl;\n@@ -136,10 +137,9 @@ public Map<String, String> options() {\n         return options;\n     }\n \n-    public Function toFunction() {\n+    public Function toFunction(Identifier identifier) {\n         return new FunctionImpl(\n-                uuid,\n-                functionName,\n+                identifier,\n                 inputParams,\n                 returnParams,\n                 deterministic,\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex 8286155fe2e6..b86ccc03ed33 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -523,30 +523,30 @@ public void alterPartitions(Identifier identifier, List<PartitionStatistics> par\n             throws TableNotExistException {}\n \n     @Override\n-    public List<String> listFunctions() {\n+    public List<String> listFunctions(String databaseName) {\n         return Collections.emptyList();\n     }\n \n     @Override\n-    public Function getFunction(String functionName) throws FunctionNotExistException {\n-        throw new FunctionNotExistException(functionName);\n+    public Function getFunction(Identifier identifier) throws FunctionNotExistException {\n+        throw new FunctionNotExistException(identifier);\n     }\n \n     @Override\n-    public void createFunction(String functionName, Function function, boolean ignoreIfExists)\n-            throws FunctionAlreadyExistException {\n+    public void createFunction(Identifier identifier, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException, DatabaseNotExistException {\n         throw new UnsupportedOperationException();\n     }\n \n     @Override\n-    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+    public void dropFunction(Identifier identifier, boolean ignoreIfNotExists)\n             throws FunctionNotExistException {\n         throw new UnsupportedOperationException();\n     }\n \n     @Override\n     public void alterFunction(\n-            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            Identifier identifier, List<FunctionChange> changes, boolean ignoreIfNotExists)\n             throws FunctionNotExistException, DefinitionAlreadyExistException,\n                     DefinitionNotExistException {\n         throw new UnsupportedOperationException();\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\nindex 5183891cef3e..44a359a96212 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n@@ -668,47 +668,59 @@ void dropPartitions(Identifier identifier, List<Map<String, String>> partitions)\n     void alterPartitions(Identifier identifier, List<PartitionStatistics> partitions)\n             throws TableNotExistException;\n \n-    /** List all functions in catalog. */\n-    List<String> listFunctions();\n+    /**\n+     * Get the names of all functions in this catalog.\n+     *\n+     * @return a list of the names of all functions\n+     * @throws DatabaseNotExistException if the database does not exist\n+     */\n+    List<String> listFunctions(String databaseName) throws DatabaseNotExistException;\n \n     /**\n      * Get function by name.\n      *\n-     * @param functionName\n-     * @throws FunctionNotExistException\n+     * @param identifier Path of the function to get\n+     * @return The requested function\n+     * @throws FunctionNotExistException if the function does not exist\n      */\n-    Function getFunction(String functionName) throws FunctionNotExistException;\n+    Function getFunction(Identifier identifier) throws FunctionNotExistException;\n \n     /**\n-     * Create function.\n+     * Create a new function.\n+     *\n+     * <p>NOTE: System functions can not be created.\n      *\n-     * @param functionName\n-     * @param function\n-     * @param ignoreIfExists\n-     * @throws FunctionAlreadyExistException\n+     * @param identifier path of the function to be created\n+     * @param function the function definition\n+     * @param ignoreIfExists flag to specify behavior when a function already exists at the given\n+     *     path: if set to false, it throws a FunctionAlreadyExistException, if set to true, do\n+     *     nothing.\n+     * @throws FunctionAlreadyExistException if function already exists and ignoreIfExists is false\n+     * @throws DatabaseNotExistException if the database in identifier doesn't exist\n      */\n-    void createFunction(String functionName, Function function, boolean ignoreIfExists)\n-            throws FunctionAlreadyExistException;\n+    void createFunction(Identifier identifier, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException, DatabaseNotExistException;\n \n     /**\n      * Drop function.\n      *\n-     * @param functionName\n+     * @param identifier\n      * @param ignoreIfNotExists\n      * @throws FunctionNotExistException\n      */\n-    void dropFunction(String functionName, boolean ignoreIfNotExists)\n+    void dropFunction(Identifier identifier, boolean ignoreIfNotExists)\n             throws FunctionNotExistException;\n \n     /**\n      * Alter function.\n      *\n-     * @param functionName\n+     * @param identifier\n      * @param changes\n      * @param ignoreIfNotExists\n      * @throws FunctionNotExistException\n      */\n-    void alterFunction(String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+    void alterFunction(\n+            Identifier identifier, List<FunctionChange> changes, boolean ignoreIfNotExists)\n             throws FunctionNotExistException, DefinitionAlreadyExistException,\n                     DefinitionNotExistException;\n \n@@ -1148,19 +1160,19 @@ class FunctionAlreadyExistException extends Exception {\n \n         private static final String MSG = \""Function %s already exists.\"";\n \n-        private final String functionName;\n+        private final Identifier identifier;\n \n-        public FunctionAlreadyExistException(String functionName) {\n-            this(functionName, null);\n+        public FunctionAlreadyExistException(Identifier identifier) {\n+            this(identifier, null);\n         }\n \n-        public FunctionAlreadyExistException(String functionName, Throwable cause) {\n-            super(String.format(MSG, functionName), cause);\n-            this.functionName = functionName;\n+        public FunctionAlreadyExistException(Identifier identifier, Throwable cause) {\n+            super(String.format(MSG, identifier.getFullName()), cause);\n+            this.identifier = identifier;\n         }\n \n-        public String functionName() {\n-            return functionName;\n+        public Identifier identifier() {\n+            return identifier;\n         }\n     }\n \n@@ -1169,19 +1181,19 @@ class FunctionNotExistException extends Exception {\n \n         private static final String MSG = \""Function %s doesn't exist.\"";\n \n-        private final String functionName;\n+        private final Identifier identifier;\n \n-        public FunctionNotExistException(String functionName) {\n-            this(functionName, null);\n+        public FunctionNotExistException(Identifier identifier) {\n+            this(identifier, null);\n         }\n \n-        public FunctionNotExistException(String functionName, Throwable cause) {\n-            super(String.format(MSG, functionName), cause);\n-            this.functionName = functionName;\n+        public FunctionNotExistException(Identifier identifier, Throwable cause) {\n+            super(String.format(MSG, identifier), cause);\n+            this.identifier = identifier;\n         }\n \n-        public String functionName() {\n-            return functionName;\n+        public Identifier identifier() {\n+            return identifier;\n         }\n     }\n \n@@ -1190,21 +1202,22 @@ class DefinitionAlreadyExistException extends Exception {\n \n         private static final String MSG = \""Definition %s in function %s already exists.\"";\n \n-        private final String functionName;\n+        private final Identifier identifier;\n         private final String name;\n \n-        public DefinitionAlreadyExistException(String functionName, String name) {\n-            this(functionName, name, null);\n+        public DefinitionAlreadyExistException(Identifier identifier, String name) {\n+            this(identifier, name, null);\n         }\n \n-        public DefinitionAlreadyExistException(String functionName, String name, Throwable cause) {\n-            super(String.format(MSG, name, functionName), cause);\n-            this.functionName = functionName;\n+        public DefinitionAlreadyExistException(\n+                Identifier identifier, String name, Throwable cause) {\n+            super(String.format(MSG, name, identifier.getFullName()), cause);\n+            this.identifier = identifier;\n             this.name = name;\n         }\n \n-        public String functionName() {\n-            return functionName;\n+        public Identifier identifier() {\n+            return identifier;\n         }\n \n         public String name() {\n@@ -1217,21 +1230,21 @@ class DefinitionNotExistException extends Exception {\n \n         private static final String MSG = \""Definition %s in function %s doesn't exist.\"";\n \n-        private final String functionName;\n+        private final Identifier identifier;\n         private final String name;\n \n-        public DefinitionNotExistException(String functionName, String name) {\n-            this(functionName, name, null);\n+        public DefinitionNotExistException(Identifier identifier, String name) {\n+            this(identifier, name, null);\n         }\n \n-        public DefinitionNotExistException(String functionName, String name, Throwable cause) {\n-            super(String.format(MSG, name, functionName), cause);\n-            this.functionName = functionName;\n+        public DefinitionNotExistException(Identifier identifier, String name, Throwable cause) {\n+            super(String.format(MSG, name, identifier.getFullName()), cause);\n+            this.identifier = identifier;\n             this.name = name;\n         }\n \n-        public String functionName() {\n-            return functionName;\n+        public Identifier identifier() {\n+            return identifier;\n         }\n \n         public String name() {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\nindex 768e4dfc3da2..1e3a021eb48f 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n@@ -212,33 +212,33 @@ public void alterPartitions(Identifier identifier, List<PartitionStatistics> par\n     }\n \n     @Override\n-    public List<String> listFunctions() {\n-        return wrapped.listFunctions();\n+    public List<String> listFunctions(String databaseName) throws DatabaseNotExistException {\n+        return wrapped.listFunctions(databaseName);\n     }\n \n     @Override\n-    public Function getFunction(String functionName) throws FunctionNotExistException {\n-        return wrapped.getFunction(functionName);\n+    public Function getFunction(Identifier identifier) throws FunctionNotExistException {\n+        return wrapped.getFunction(identifier);\n     }\n \n     @Override\n-    public void createFunction(String functionName, Function function, boolean ignoreIfExists)\n-            throws FunctionAlreadyExistException {\n-        wrapped.createFunction(functionName, function, ignoreIfExists);\n+    public void createFunction(Identifier identifier, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException, DatabaseNotExistException {\n+        wrapped.createFunction(identifier, function, ignoreIfExists);\n     }\n \n     @Override\n-    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+    public void dropFunction(Identifier identifier, boolean ignoreIfNotExists)\n             throws FunctionNotExistException {\n-        wrapped.dropFunction(functionName, ignoreIfNotExists);\n+        wrapped.dropFunction(identifier, ignoreIfNotExists);\n     }\n \n     @Override\n     public void alterFunction(\n-            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            Identifier identifier, List<FunctionChange> changes, boolean ignoreIfNotExists)\n             throws FunctionNotExistException, DefinitionAlreadyExistException,\n                     DefinitionNotExistException {\n-        wrapped.alterFunction(functionName, changes, ignoreIfNotExists);\n+        wrapped.alterFunction(identifier, changes, ignoreIfNotExists);\n     }\n \n     @Override\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex 428fc3da26ff..492c38ecb740 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -612,65 +612,71 @@ public void alterPartitions(Identifier identifier, List<PartitionStatistics> par\n     }\n \n     @Override\n-    public List<String> listFunctions() {\n-        return api.listFunctions();\n+    public List<String> listFunctions(String databaseName) throws DatabaseNotExistException {\n+        try {\n+            return api.listFunctions(databaseName);\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(databaseName, e);\n+        }\n     }\n \n     @Override\n-    public org.apache.paimon.function.Function getFunction(String functionName)\n+    public org.apache.paimon.function.Function getFunction(Identifier identifier)\n             throws FunctionNotExistException {\n         try {\n-            GetFunctionResponse response = api.getFunction(functionName);\n-            return response.toFunction();\n+            GetFunctionResponse response = api.getFunction(identifier);\n+            return response.toFunction(identifier);\n         } catch (NoSuchResourceException e) {\n-            throw new FunctionNotExistException(functionName, e);\n+            throw new FunctionNotExistException(identifier, e);\n         }\n     }\n \n     @Override\n     public void createFunction(\n-            String functionName,\n+            Identifier identifier,\n             org.apache.paimon.function.Function function,\n             boolean ignoreIfExists)\n-            throws FunctionAlreadyExistException {\n+            throws FunctionAlreadyExistException, DatabaseNotExistException {\n         try {\n-            api.createFunction(function);\n+            api.createFunction(identifier, function);\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(identifier.getDatabaseName(), e);\n         } catch (AlreadyExistsException e) {\n             if (ignoreIfExists) {\n                 return;\n             }\n-            throw new FunctionAlreadyExistException(functionName, e);\n+            throw new FunctionAlreadyExistException(identifier, e);\n         }\n     }\n \n     @Override\n-    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+    public void dropFunction(Identifier identifier, boolean ignoreIfNotExists)\n             throws FunctionNotExistException {\n         try {\n-            api.dropFunction(functionName);\n+            api.dropFunction(identifier);\n         } catch (NoSuchResourceException e) {\n             if (ignoreIfNotExists) {\n                 return;\n             }\n-            throw new FunctionNotExistException(functionName, e);\n+            throw new FunctionNotExistException(identifier, e);\n         }\n     }\n \n     @Override\n     public void alterFunction(\n-            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            Identifier identifier, List<FunctionChange> changes, boolean ignoreIfNotExists)\n             throws FunctionNotExistException, DefinitionAlreadyExistException,\n                     DefinitionNotExistException {\n         try {\n-            api.alterFunction(functionName, changes);\n+            api.alterFunction(identifier, changes);\n         } catch (AlreadyExistsException e) {\n-            throw new DefinitionAlreadyExistException(functionName, e.resourceName());\n+            throw new DefinitionAlreadyExistException(identifier, e.resourceName());\n         } catch (NoSuchResourceException e) {\n             if (StringUtils.equals(e.resourceType(), ErrorResponse.RESOURCE_TYPE_DEFINITION)) {\n-                throw new DefinitionNotExistException(functionName, e.resourceName());\n+                throw new DefinitionNotExistException(identifier, e.resourceName());\n             }\n             if (!ignoreIfNotExists) {\n-                throw new FunctionNotExistException(functionName);\n+                throw new FunctionNotExistException(identifier, e);\n             }\n         } catch (BadRequestException e) {\n             throw new IllegalArgumentException(e.getMessage());\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 0bf9a1792074..acb7a8ff5689 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -286,12 +286,12 @@ public static AlterViewRequest alterViewRequest() {\n     }\n \n     public static GetFunctionResponse getFunctionResponse() {\n-        Function function = function(\""function\"");\n+        Function function = function(Identifier.create(databaseName(), \""function\""));\n         return new GetFunctionResponse(\n-                function.uuid(),\n+                UUID.randomUUID().toString(),\n                 function.name(),\n-                function.inputParams(),\n-                function.returnParams(),\n+                function.inputParams().orElse(null),\n+                function.returnParams().orElse(null),\n                 function.isDeterministic(),\n                 function.definitions(),\n                 function.comment(),\n@@ -304,18 +304,18 @@ public static GetFunctionResponse getFunctionResponse() {\n     }\n \n     public static CreateFunctionRequest createFunctionRequest() {\n-        Function function = function(\""function\"");\n+        Function function = function(Identifier.create(databaseName(), \""function\""));\n         return new CreateFunctionRequest(\n                 function.name(),\n-                function.inputParams(),\n-                function.returnParams(),\n+                function.inputParams().orElse(null),\n+                function.returnParams().orElse(null),\n                 function.isDeterministic(),\n                 function.definitions(),\n                 function.comment(),\n                 function.options());\n     }\n \n-    public static Function function(String functionName) {\n+    public static Function function(Identifier identifier) {\n         List<DataField> inputParams =\n                 Lists.newArrayList(\n                         new DataField(0, \""length\"", DataTypes.DOUBLE()),\n@@ -334,8 +334,7 @@ public static Function function(String functionName) {\n         definitions.put(\""spark\"", sparkFunction);\n         definitions.put(\""trino\"", trinoFunction);\n         return new FunctionImpl(\n-                UUID.randomUUID().toString(),\n-                functionName,\n+                identifier,\n                 inputParams,\n                 returnParams,\n                 false,\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 6ac647f4ff64..e4b07d5a1558 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -135,7 +135,6 @@ public class RESTCatalogServer {\n     public static final String AUTHORIZATION_HEADER_KEY = \""Authorization\"";\n \n     private final String databaseUri;\n-    private final String functionUri;\n \n     private final FileSystemCatalog catalog;\n     private final MockWebServer server;\n@@ -163,7 +162,6 @@ public RESTCatalogServer(\n                 this.configResponse.getDefaults().get(RESTCatalogInternalOptions.PREFIX.key());\n         this.resourcePaths = new ResourcePaths(prefix);\n         this.databaseUri = resourcePaths.databases();\n-        this.functionUri = resourcePaths.functions();\n         Options conf = new Options();\n         this.configResponse.getDefaults().forEach(conf::setString);\n         conf.setString(WAREHOUSE.key(), dataPath);\n@@ -274,11 +272,6 @@ public MockResponse dispatch(RecordedRequest request) {\n                     } else if (databaseUri.equals(request.getPath())\n                             || request.getPath().contains(databaseUri + \""?\"")) {\n                         return databasesApiHandler(restAuthParameter.method(), data, parameters);\n-                    } else if (functionUri.equals(request.getPath())) {\n-                        return functionsApiHandler(restAuthParameter.method(), data, parameters);\n-                    } else if (request.getPath().startsWith(functionUri)) {\n-                        return functionApiHandler(\n-                                request.getPath(), restAuthParameter.method(), data, parameters);\n                     } else if (resourcePaths.renameTable().equals(request.getPath())) {\n                         return renameTableHandle(restAuthParameter.data());\n                     } else if (resourcePaths.renameView().equals(request.getPath())) {\n@@ -295,6 +288,10 @@ public MockResponse dispatch(RecordedRequest request) {\n                         if (!databaseStore.containsKey(databaseName)) {\n                             throw new Catalog.DatabaseNotExistException(databaseName);\n                         }\n+                        boolean isFunctions =\n+                                resources.length == 2 && resources[1].startsWith(\""functions\"");\n+                        boolean isFunction =\n+                                resources.length == 3 && resources[1].startsWith(\""functions\"");\n                         boolean isViews = resources.length == 2 && resources[1].startsWith(\""views\"");\n                         boolean isViewsDetails =\n                                 resources.length == 2 && resources[1].startsWith(\""view-details\"");\n@@ -429,6 +426,12 @@ public MockResponse dispatch(RecordedRequest request) {\n                                     parameters);\n                         } else if (isTableDetails) {\n                             return tableDetailsHandle(parameters, databaseName);\n+                        } else if (isFunctions) {\n+                            return functionsApiHandler(\n+                                    databaseName, restAuthParameter.method(), data, parameters);\n+                        } else if (isFunction) {\n+                            return functionApiHandler(\n+                                    identifier, restAuthParameter.method(), data, parameters);\n                         } else if (isViews) {\n                             return viewsHandle(\n                                     restAuthParameter.method(),\n@@ -518,8 +521,8 @@ public MockResponse dispatch(RecordedRequest request) {\n                 } catch (Catalog.FunctionAlreadyExistException e) {\n                     response =\n                             new ErrorResponse(\n-                                    ErrorResponse.RESOURCE_TYPE_COLUMN,\n-                                    e.functionName(),\n+                                    ErrorResponse.RESOURCE_TYPE_DEFINITION,\n+                                    e.identifier().getObjectName(),\n                                     e.getMessage(),\n                                     409);\n                     return mockResponse(response, 409);\n@@ -527,7 +530,7 @@ public MockResponse dispatch(RecordedRequest request) {\n                     response =\n                             new ErrorResponse(\n                                     ErrorResponse.RESOURCE_TYPE_DEFINITION,\n-                                    e.functionName(),\n+                                    e.name(),\n                                     e.getMessage(),\n                                     409);\n                     return mockResponse(response, 409);\n@@ -551,7 +554,7 @@ public MockResponse dispatch(RecordedRequest request) {\n                     response =\n                             new ErrorResponse(\n                                     ErrorResponse.RESOURCE_TYPE_FUNCTION,\n-                                    e.functionName(),\n+                                    e.identifier().getObjectName(),\n                                     e.getMessage(),\n                                     404);\n                     return mockResponse(response, 404);\n@@ -559,7 +562,7 @@ public MockResponse dispatch(RecordedRequest request) {\n                     response =\n                             new ErrorResponse(\n                                     ErrorResponse.RESOURCE_TYPE_DEFINITION,\n-                                    e.functionName(),\n+                                    e.name(),\n                                     e.getMessage(),\n                                     404);\n                     return mockResponse(response, 404);\n@@ -757,32 +760,9 @@ private void cleanSnapshot(Identifier identifier, Long snapshotId, Long latestSn\n         }\n     }\n \n-    private MockResponse functionDetailsHandler(String functionName) throws Exception {\n-        if (functionStore.containsKey(functionName)) {\n-            Function function = functionStore.get(functionName);\n-            GetFunctionResponse response =\n-                    new GetFunctionResponse(\n-                            function.uuid(),\n-                            function.name(),\n-                            function.inputParams(),\n-                            function.returnParams(),\n-                            function.isDeterministic(),\n-                            function.definitions(),\n-                            function.comment(),\n-                            function.options(),\n-                            \""owner\"",\n-                            1L,\n-                            \""owner\"",\n-                            1L,\n-                            \""owner\"");\n-            return mockResponse(response, 200);\n-        } else {\n-            throw new Catalog.FunctionNotExistException(functionName);\n-        }\n-    }\n-\n     private MockResponse functionsApiHandler(\n-            String method, String data, Map<String, String> parameters) throws Exception {\n+            String databaseName, String method, String data, Map<String, String> parameters)\n+            throws Exception {\n         switch (method) {\n             case \""GET\"":\n                 List<String> functions = new ArrayList<>(functionStore.keySet());\n@@ -794,8 +774,7 @@ private MockResponse functionsApiHandler(\n                 if (!functionStore.containsKey(functionName)) {\n                     Function function =\n                             new FunctionImpl(\n-                                    UUID.randomUUID().toString(),\n-                                    functionName,\n+                                    Identifier.create(databaseName, functionName),\n                                     requestBody.inputParams(),\n                                     requestBody.returnParams(),\n                                     requestBody.isDeterministic(),\n@@ -805,7 +784,8 @@ private MockResponse functionsApiHandler(\n                     functionStore.put(functionName, function);\n                     return new MockResponse().setResponseCode(200);\n                 } else {\n-                    throw new Catalog.FunctionAlreadyExistException(functionName);\n+                    throw new Catalog.FunctionAlreadyExistException(\n+                            Identifier.create(databaseName, functionName));\n                 }\n             default:\n                 return new MockResponse().setResponseCode(404);\n@@ -813,12 +793,11 @@ private MockResponse functionsApiHandler(\n     }\n \n     private MockResponse functionApiHandler(\n-            String path, String method, String data, Map<String, String> parameters)\n+            Identifier identifier, String method, String data, Map<String, String> parameters)\n             throws Exception {\n-        String[] resources = path.substring((functionUri + \""/\"").length()).split(\""/\"");\n-        String functionName = RESTUtil.decodeString(resources[0]);\n+        String functionName = identifier.getObjectName();\n         if (!functionStore.containsKey(functionName)) {\n-            throw new Catalog.FunctionNotExistException(functionName);\n+            throw new Catalog.FunctionNotExistException(identifier);\n         }\n         Function function = functionStore.get(functionName);\n         switch (method) {\n@@ -828,10 +807,10 @@ private MockResponse functionApiHandler(\n             case \""GET\"":\n                 GetFunctionResponse response =\n                         new GetFunctionResponse(\n-                                function.uuid(),\n+                                UUID.randomUUID().toString(),\n                                 function.name(),\n-                                function.inputParams(),\n-                                function.returnParams(),\n+                                function.inputParams().orElse(null),\n+                                function.returnParams().orElse(null),\n                                 function.isDeterministic(),\n                                 function.definitions(),\n                                 function.comment(),\n@@ -867,7 +846,7 @@ private MockResponse functionApiHandler(\n                                 (FunctionChange.AddDefinition) functionChange;\n                         if (function.definition(addDefinition.name()) != null) {\n                             throw new Catalog.DefinitionAlreadyExistException(\n-                                    functionName, addDefinition.name());\n+                                    identifier, addDefinition.name());\n                         }\n                         newDefinitions.put(addDefinition.name(), addDefinition.definition());\n                     } else if (functionChange instanceof FunctionChange.UpdateDefinition) {\n@@ -878,7 +857,7 @@ private MockResponse functionApiHandler(\n                                     updateDefinition.name(), updateDefinition.definition());\n                         } else {\n                             throw new Catalog.DefinitionNotExistException(\n-                                    functionName, updateDefinition.name());\n+                                    identifier, updateDefinition.name());\n                         }\n                     } else if (functionChange instanceof FunctionChange.DropDefinition) {\n                         FunctionChange.DropDefinition dropDefinition =\n@@ -887,16 +866,15 @@ private MockResponse functionApiHandler(\n                             newDefinitions.remove(dropDefinition.name());\n                         } else {\n                             throw new Catalog.DefinitionNotExistException(\n-                                    functionName, dropDefinition.name());\n+                                    identifier, dropDefinition.name());\n                         }\n                     }\n                 }\n                 function =\n                         new FunctionImpl(\n-                                functionName,\n-                                function.uuid(),\n-                                function.inputParams(),\n-                                function.returnParams(),\n+                                Identifier.create(null, functionName),\n+                                function.inputParams().orElse(null),\n+                                function.returnParams().orElse(null),\n                                 function.isDeterministic(),\n                                 newDefinitions,\n                                 newComment,\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex f6e259d2da93..15bc815c0e04 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -1323,86 +1323,90 @@ void testAlterView() throws Exception {\n \n     @Test\n     void testFunction() throws Exception {\n-        Function function = MockRESTMessage.function(\""function\"");\n+        Identifier identifier = new Identifier(\""rest_catalog_db\"", \""function\"");\n+        catalog.createDatabase(identifier.getDatabaseName(), false);\n+        Function function = MockRESTMessage.function(identifier);\n \n-        catalog.createFunction(function.name(), function, true);\n+        catalog.createFunction(identifier, function, true);\n         assertThrows(\n                 Catalog.FunctionAlreadyExistException.class,\n-                () -> catalog.createFunction(function.name(), function, false));\n+                () -> catalog.createFunction(identifier, function, false));\n \n-        assertThat(catalog.listFunctions().contains(function.name())).isTrue();\n+        assertThat(catalog.listFunctions(identifier.getDatabaseName()).contains(function.name()))\n+                .isTrue();\n \n-        Function getFunction = catalog.getFunction(function.name());\n+        Function getFunction = catalog.getFunction(identifier);\n         assertThat(getFunction.name()).isEqualTo(function.name());\n         for (String dialect : function.definitions().keySet()) {\n             assertThat(getFunction.definition(dialect)).isEqualTo(function.definition(dialect));\n         }\n-        catalog.dropFunction(function.name(), true);\n+        catalog.dropFunction(identifier, true);\n \n-        assertThat(catalog.listFunctions().contains(function.name())).isFalse();\n+        assertThat(catalog.listFunctions(identifier.getDatabaseName()).contains(function.name()))\n+                .isFalse();\n         assertThrows(\n                 Catalog.FunctionNotExistException.class,\n-                () -> catalog.dropFunction(function.name(), false));\n+                () -> catalog.dropFunction(identifier, false));\n         assertThrows(\n-                Catalog.FunctionNotExistException.class,\n-                () -> catalog.getFunction(function.name()));\n+                Catalog.FunctionNotExistException.class, () -> catalog.getFunction(identifier));\n     }\n \n     @Test\n     void testAlterFunction() throws Exception {\n-        String functionName = \""alter_function_name\"";\n-        Function function = MockRESTMessage.function(functionName);\n+        Identifier identifier = new Identifier(\""rest_catalog_db\"", \""alter_function_name\"");\n+        catalog.createDatabase(identifier.getDatabaseName(), false);\n+        Function function = MockRESTMessage.function(identifier);\n         FunctionDefinition definition = FunctionDefinition.sql(\""x * y + 1\"");\n         FunctionChange.AddDefinition addDefinition =\n                 (FunctionChange.AddDefinition) FunctionChange.addDefinition(\""flink_1\"", definition);\n         assertDoesNotThrow(\n-                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), true));\n+                () -> catalog.alterFunction(identifier, ImmutableList.of(addDefinition), true));\n         assertThrows(\n                 Catalog.FunctionNotExistException.class,\n-                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false));\n-        catalog.createFunction(function.name(), function, true);\n+                () -> catalog.alterFunction(identifier, ImmutableList.of(addDefinition), false));\n+        catalog.createFunction(identifier, function, true);\n         // set options\n         String key = UUID.randomUUID().toString();\n         String value = UUID.randomUUID().toString();\n         FunctionChange setOption = FunctionChange.setOption(key, value);\n-        catalog.alterFunction(functionName, ImmutableList.of(setOption), false);\n-        Function catalogFunction = catalog.getFunction(functionName);\n+        catalog.alterFunction(identifier, ImmutableList.of(setOption), false);\n+        Function catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.options().get(key)).isEqualTo(value);\n \n         // remove options\n         catalog.alterFunction(\n-                functionName, ImmutableList.of(FunctionChange.removeOption(key)), false);\n-        catalogFunction = catalog.getFunction(functionName);\n+                identifier, ImmutableList.of(FunctionChange.removeOption(key)), false);\n+        catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.options().containsKey(key)).isEqualTo(false);\n \n         // update comment\n         String newComment = \""new comment\"";\n         catalog.alterFunction(\n-                functionName, ImmutableList.of(FunctionChange.updateComment(newComment)), false);\n-        catalogFunction = catalog.getFunction(functionName);\n+                identifier, ImmutableList.of(FunctionChange.updateComment(newComment)), false);\n+        catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.comment()).isEqualTo(newComment);\n         // add definition\n-        catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false);\n-        catalogFunction = catalog.getFunction(functionName);\n+        catalog.alterFunction(identifier, ImmutableList.of(addDefinition), false);\n+        catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.definition(addDefinition.name()))\n                 .isEqualTo(addDefinition.definition());\n         assertThrows(\n                 Catalog.DefinitionAlreadyExistException.class,\n-                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false));\n+                () -> catalog.alterFunction(identifier, ImmutableList.of(addDefinition), false));\n \n         // update definition\n         FunctionChange.UpdateDefinition updateDefinition =\n                 (FunctionChange.UpdateDefinition)\n                         FunctionChange.updateDefinition(\""flink_1\"", definition);\n-        catalog.alterFunction(functionName, ImmutableList.of(updateDefinition), false);\n-        catalogFunction = catalog.getFunction(functionName);\n+        catalog.alterFunction(identifier, ImmutableList.of(updateDefinition), false);\n+        catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.definition(updateDefinition.name()))\n                 .isEqualTo(updateDefinition.definition());\n         assertThrows(\n                 Catalog.DefinitionNotExistException.class,\n                 () ->\n                         catalog.alterFunction(\n-                                functionName,\n+                                identifier,\n                                 ImmutableList.of(\n                                         FunctionChange.updateDefinition(\""no_exist\"", definition)),\n                                 false));\n@@ -1411,13 +1415,13 @@ void testAlterFunction() throws Exception {\n         FunctionChange.DropDefinition dropDefinition =\n                 (FunctionChange.DropDefinition)\n                         FunctionChange.dropDefinition(updateDefinition.name());\n-        catalog.alterFunction(functionName, ImmutableList.of(dropDefinition), false);\n-        catalogFunction = catalog.getFunction(functionName);\n+        catalog.alterFunction(identifier, ImmutableList.of(dropDefinition), false);\n+        catalogFunction = catalog.getFunction(identifier);\n         assertThat(catalogFunction.definition(updateDefinition.name())).isNull();\n \n         assertThrows(\n                 Catalog.DefinitionNotExistException.class,\n-                () -> catalog.alterFunction(functionName, ImmutableList.of(dropDefinition), false));\n+                () -> catalog.alterFunction(identifier, ImmutableList.of(dropDefinition), false));\n     }\n \n     @Test\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5548"", ""pr_id"": 5548, ""issue_id"": 5420, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support function in paimon\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nThe defining function includes the parameter, output, and function body. The output result is a single value or a table. It is modeled after unity catalog\uff0cand it could support the following scenarios:\n1. Support filtering or encrypting column in row: select decode_email(email) from user; \n2. Support parameterized view: select filter_fruit_by_color('red');\n3. Support use the function in AI.\n\n### Solution\n\nProvide the ability to define function.\n[PIP-31: Introduce Function](https://cwiki.apache.org/confluence/display/PAIMON/PIP-31%3A+Introduce+Function)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 127, ""test_files_count"": 4, ""non_test_files_count"": 17, ""pr_changed_files"": [""docs/static/rest-catalog-open-api.yaml"", ""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/function/Function.java"", ""paimon-core/src/main/java/org/apache/paimon/function/FunctionChange.java"", ""paimon-core/src/main/java/org/apache/paimon/function/FunctionDefinition.java"", ""paimon-core/src/main/java/org/apache/paimon/function/FunctionImpl.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterFunctionRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ListFunctionsResponse.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java""], ""base_commit"": ""6bc04261a1df8720ba9163d334022fd9764f6f8c"", ""head_commit"": ""5ae4caa1eeb6955f5302b38260e687fed6dd7eec"", ""repo_url"": ""https://github.com/apache/paimon/pull/5548"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5548"", ""dockerfile"": """", ""pr_merged_at"": ""2025-05-14T05:26:26.000Z"", ""patch"": ""diff --git a/docs/static/rest-catalog-open-api.yaml b/docs/static/rest-catalog-open-api.yaml\nindex 47336b069d05..78ca769ec6e2 100644\n--- a/docs/static/rest-catalog-open-api.yaml\n+++ b/docs/static/rest-catalog-open-api.yaml\n@@ -95,8 +95,8 @@ paths:\n     post:\n       tags:\n         - database\n-      summary: Create Databases\n-      operationId: createDatabases\n+      summary: Create Database\n+      operationId: createDatabase\n       parameters:\n         - name: prefix\n           in: path\n@@ -622,6 +622,8 @@ paths:\n           description: Success, no content\n         \""401\"":\n           $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""403\"":\n+          $ref: '#/components/responses/ForbiddenErrorResponse'\n         404:\n           description:\n             Not Found\n@@ -1182,6 +1184,152 @@ paths:\n           $ref: '#/components/responses/ViewAlreadyExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n+  /v1/{prefix}/functions:\n+    get:\n+      tags:\n+        - function\n+      summary: List functions\n+      operationId: listFunctions\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: maxResults\n+          in: query\n+          schema:\n+            type: integer\n+            format: int32\n+        - name: pageToken\n+          in: query\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ListFunctionsResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    post:\n+      tags:\n+        - function\n+      summary: Create Function\n+      operationId: createFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/CreateFunctionRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""400\"":\n+          $ref: '#/components/responses/BadRequestErrorResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""409\"":\n+          $ref: '#/components/responses/FunctionAlreadyExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+\n+  /v1/{prefix}/functions/{function}:\n+    get:\n+      tags:\n+        - function\n+      summary: Get function\n+      operationId: getFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetFunctionResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/DatabaseNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    post:\n+      tags:\n+        - function\n+      summary: Alter function\n+      operationId: alterFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/AlterFunctionRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/FunctionNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    delete:\n+      tags:\n+        - function\n+      summary: Drop function\n+      operationId: dropFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/FunctionNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+\n components:\n   #############################\n   # Reusable Response Objects #\n@@ -1209,6 +1357,17 @@ components:\n             \""message\"": \""No auth for this resource\"",\n             \""code\"": 401\n           }\n+    ForbiddenErrorResponse:\n+      description:\n+        Used for 403 errors.\n+      content:\n+        application/json:\n+          schema:\n+            $ref: '#/components/schemas/ErrorResponse'\n+          example: {\n+            \""message\"": \""Table has no permission\"",\n+            \""code\"": 403\n+          }\n     ResourceNotExistErrorResponse:\n       description:\n         Used for 404 errors, which means the resource does not exist.\n@@ -1292,6 +1451,20 @@ components:\n               \""resourceName\"": \""view\"",\n               \""code\"": 404\n             }\n+    FunctionNotExistErrorResponse:\n+      description:\n+        Not Found - FunctionNotExistException, the function does not exist\n+      content:\n+        application/json:\n+          schema:\n+            $ref: '#/components/responses/ResourceNotExistErrorResponse'\n+          example:\n+            {\n+              \""message\"": \""The given function does not exist\"",\n+              \""resourceType\"": \""FUNCTION\"",\n+              \""resourceName\"": \""function\"",\n+              \""code\"": 404\n+            }\n     ResourceAlreadyExistErrorResponse:\n       description:\n         Used for 409 errors.\n@@ -1357,6 +1530,19 @@ components:\n               \""resourceName\"": \""view\"",\n               \""code\"": 409\n             }\n+    FunctionAlreadyExistErrorResponse:\n+      description: Conflict - The view already exists\n+      content:\n+        application/json:\n+          schema:\n+            $ref: '#/components/responses/ResourceAlreadyExistErrorResponse'\n+          example:\n+            {\n+              \""message\"": \""The given function already exists\"",\n+              \""resourceType\"": \""FUNCTION\"",\n+              \""resourceName\"": \""function\"",\n+              \""code\"": 409\n+            }\n     ServerErrorResponse:\n       description:\n         Used for server 5xx errors.\n@@ -1451,6 +1637,179 @@ components:\n           type: array\n           items:\n             $ref: '#/components/schemas/ViewChange'\n+    CreateFunctionRequest:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+        inputParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        returnParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        deterministic:\n+          type: boolean\n+        definitions:\n+          type: object\n+          additionalProperties:\n+            $ref: \""#/components/schemas/FunctionDefinition\""\n+        comment:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+    AlterFunctionRequest:\n+      type: object\n+      properties:\n+        changes:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/FunctionChange'\n+    FunctionChange:\n+      anyOf:\n+        - $ref: '#/components/schemas/SetFunctionOption'\n+        - $ref: '#/components/schemas/RemoveFunctionOption'\n+        - $ref: '#/components/schemas/UpdateFunctionComment'\n+        - $ref: '#/components/schemas/AddDefinition'\n+        - $ref: '#/components/schemas/UpdateDefinition'\n+        - $ref: '#/components/schemas/DropDefinition'\n+    BaseFunctionChange:\n+      discriminator:\n+        propertyName: action\n+        mapping:\n+          setOption: '#/components/schemas/SetFunctionOption'\n+          removeOption: '#/components/schemas/RemoveFunctionOption'\n+          updateComment: '#/components/schemas/UpdateFunctionComment'\n+          addDefinition: '#/components/schemas/AddDefinition'\n+          updateDefinition: '#/components/schemas/UpdateDefinition'\n+          dropDefinition: '#/components/schemas/DropDefinition'\n+      type: object\n+      required:\n+        - action\n+      properties:\n+        action:\n+          type: string\n+    SetFunctionOption:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""setOption\""\n+        key:\n+          type: string\n+        value:\n+          type: string\n+    RemoveFunctionOption:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""removeOption\""\n+        key:\n+          type: string\n+    UpdateFunctionComment:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""updateComment\""\n+        comment:\n+          type: string\n+    AddDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""addDefinition\""\n+        name:\n+          type: string\n+        definition:\n+          $ref: \""#/components/schemas/FunctionDefinition\""\n+    UpdateDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""updateDefinition\""\n+        name:\n+          type: string\n+        definition:\n+          $ref: \""#/components/schemas/FunctionDefinition\""\n+    DropDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""dropDefinition\""\n+        name:\n+          type: string\n+    FunctionDefinition:\n+      anyOf:\n+        - $ref: '#/components/schemas/FileFunctionDefinition'\n+        - $ref: '#/components/schemas/SQLFunctionDefinition'\n+        - $ref: '#/components/schemas/LambdaFunctionDefinition'\n+    BaseFunctionDefinition:\n+      discriminator:\n+        propertyName: type\n+        mapping:\n+          file: '#/components/schemas/FileFunctionDefinition'\n+          sql: '#/components/schemas/SQLFunctionDefinition'\n+          lambda: '#/components/schemas/LambdaFunctionDefinition'\n+      type: object\n+      required:\n+        - type\n+      properties:\n+        type:\n+          type: string\n+    SQLFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""sql\""\n+        definition:\n+          type: string\n+    FileFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""file\""\n+        fileType:\n+          type: string\n+        storagePaths:\n+          type: array\n+          items:\n+            type: string\n+        language:\n+          type: string\n+        className:\n+          type: string\n+        functionName:\n+          type: string\n+    LambdaFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""lambda\""\n+        definition:\n+          type: string\n+        language:\n+          type: string\n     ViewChange:\n       anyOf:\n         - $ref: '#/components/schemas/SetViewOption'\n@@ -1846,13 +2205,13 @@ components:\n       required:\n         - type\n       properties:\n-        'type':\n+        type:\n           type: string\n     SnapshotInstant:\n       allOf:\n         - $ref: '#/components/schemas/BaseInstant'\n       properties:\n-        'type':\n+        type:\n           type: string\n           const: \""snapshot\""\n         snapshotId:\n@@ -1862,7 +2221,7 @@ components:\n       allOf:\n         - $ref: '#/components/schemas/BaseInstant'\n       properties:\n-        'type':\n+        type:\n           type: string\n           const: \""tag\""\n         tagName:\n@@ -2126,6 +2485,52 @@ components:\n             $ref: '#/components/schemas/GetViewResponse'\n         nextPageToken:\n           type: string\n+    ListFunctionsResponse:\n+      type: object\n+      properties:\n+        functions:\n+          type: array\n+          items:\n+            type: string\n+        nextPageToken:\n+          type: string\n+    GetFunctionResponse:\n+      type: object\n+      properties:\n+        uuid:\n+          type: string\n+        name:\n+          type: string\n+        inputParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        returnParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        deterministic:\n+          type: boolean\n+        definitions:\n+          type: object\n+          additionalProperties:\n+            $ref: \""#/components/schemas/FunctionDefinition\""\n+        comment:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+        owner:\n+          type: string\n+        createdAt:\n+          format: int64\n+        createdBy:\n+          type: string\n+        updatedAt:\n+          format: int64\n+        updatedBy:\n+          type: string\n     ViewSchema:\n       type: object\n       properties:\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex 87d34f301a83..bd3624b66ea7 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -25,6 +25,8 @@\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.FileStatus;\n import org.apache.paimon.fs.Path;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.partition.PartitionStatistics;\n@@ -49,6 +51,7 @@\n \n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -519,6 +522,36 @@ public void dropPartitions(Identifier identifier, List<Map<String, String>> part\n     public void alterPartitions(Identifier identifier, List<PartitionStatistics> partitions)\n             throws TableNotExistException {}\n \n+    @Override\n+    public List<String> listFunctions() {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public Function getFunction(String functionName) throws FunctionNotExistException {\n+        throw new FunctionNotExistException(functionName);\n+    }\n+\n+    @Override\n+    public void createFunction(String functionName, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void alterFunction(\n+            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException, DefinitionAlreadyExistException,\n+                    DefinitionNotExistException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n     /**\n      * Create a {@link FormatTable} identified by the given {@link Identifier}.\n      *\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\nindex c940da3ac2b3..1494c2df1909 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n@@ -21,6 +21,8 @@\n import org.apache.paimon.PagedList;\n import org.apache.paimon.Snapshot;\n import org.apache.paimon.annotation.Public;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.partition.PartitionStatistics;\n import org.apache.paimon.schema.Schema;\n@@ -666,6 +668,50 @@ void dropPartitions(Identifier identifier, List<Map<String, String>> partitions)\n     void alterPartitions(Identifier identifier, List<PartitionStatistics> partitions)\n             throws TableNotExistException;\n \n+    /** List all functions in catalog. */\n+    List<String> listFunctions();\n+\n+    /**\n+     * Get function by name.\n+     *\n+     * @param functionName\n+     * @throws FunctionNotExistException\n+     */\n+    Function getFunction(String functionName) throws FunctionNotExistException;\n+\n+    /**\n+     * Create function.\n+     *\n+     * @param functionName\n+     * @param function\n+     * @param ignoreIfExists\n+     * @throws FunctionAlreadyExistException\n+     */\n+    void createFunction(String functionName, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException;\n+\n+    /**\n+     * Drop function.\n+     *\n+     * @param functionName\n+     * @param ignoreIfNotExists\n+     * @throws FunctionNotExistException\n+     */\n+    void dropFunction(String functionName, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException;\n+\n+    /**\n+     * Alter function.\n+     *\n+     * @param functionName\n+     * @param changes\n+     * @param ignoreIfNotExists\n+     * @throws FunctionNotExistException\n+     */\n+    void alterFunction(String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException, DefinitionAlreadyExistException,\n+                    DefinitionNotExistException;\n+\n     // ==================== Table Auth ==========================\n \n     /**\n@@ -1098,4 +1144,100 @@ public String dialect() {\n             return dialect;\n         }\n     }\n+\n+    /** Exception for trying to create a function that already exists. */\n+    class FunctionAlreadyExistException extends Exception {\n+\n+        private static final String MSG = \""Function %s already exists.\"";\n+\n+        private final String functionName;\n+\n+        public FunctionAlreadyExistException(String functionName) {\n+            this(functionName, null);\n+        }\n+\n+        public FunctionAlreadyExistException(String functionName, Throwable cause) {\n+            super(String.format(MSG, functionName), cause);\n+            this.functionName = functionName;\n+        }\n+\n+        public String functionName() {\n+            return functionName;\n+        }\n+    }\n+\n+    /** Exception for trying to get a function that doesn't exist. */\n+    class FunctionNotExistException extends Exception {\n+\n+        private static final String MSG = \""Function %s doesn't exist.\"";\n+\n+        private final String functionName;\n+\n+        public FunctionNotExistException(String functionName) {\n+            this(functionName, null);\n+        }\n+\n+        public FunctionNotExistException(String functionName, Throwable cause) {\n+            super(String.format(MSG, functionName), cause);\n+            this.functionName = functionName;\n+        }\n+\n+        public String functionName() {\n+            return functionName;\n+        }\n+    }\n+\n+    /** Exception for trying to add a definition that already exists. */\n+    class DefinitionAlreadyExistException extends Exception {\n+\n+        private static final String MSG = \""Definition %s in function %s already exists.\"";\n+\n+        private final String functionName;\n+        private final String name;\n+\n+        public DefinitionAlreadyExistException(String functionName, String name) {\n+            this(functionName, name, null);\n+        }\n+\n+        public DefinitionAlreadyExistException(String functionName, String name, Throwable cause) {\n+            super(String.format(MSG, name, functionName), cause);\n+            this.functionName = functionName;\n+            this.name = name;\n+        }\n+\n+        public String functionName() {\n+            return functionName;\n+        }\n+\n+        public String name() {\n+            return name;\n+        }\n+    }\n+\n+    /** Exception for trying to update definition that doesn't exist. */\n+    class DefinitionNotExistException extends Exception {\n+\n+        private static final String MSG = \""Definition %s in function %s doesn't exist.\"";\n+\n+        private final String functionName;\n+        private final String name;\n+\n+        public DefinitionNotExistException(String functionName, String name) {\n+            this(functionName, name, null);\n+        }\n+\n+        public DefinitionNotExistException(String functionName, String name, Throwable cause) {\n+            super(String.format(MSG, name, functionName), cause);\n+            this.functionName = functionName;\n+            this.name = name;\n+        }\n+\n+        public String functionName() {\n+            return functionName;\n+        }\n+\n+        public String name() {\n+            return name;\n+        }\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\nindex c781cb9f8342..768e4dfc3da2 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n@@ -20,6 +20,8 @@\n \n import org.apache.paimon.PagedList;\n import org.apache.paimon.Snapshot;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.partition.PartitionStatistics;\n import org.apache.paimon.schema.Schema;\n@@ -209,6 +211,36 @@ public void alterPartitions(Identifier identifier, List<PartitionStatistics> par\n         wrapped.alterPartitions(identifier, partitions);\n     }\n \n+    @Override\n+    public List<String> listFunctions() {\n+        return wrapped.listFunctions();\n+    }\n+\n+    @Override\n+    public Function getFunction(String functionName) throws FunctionNotExistException {\n+        return wrapped.getFunction(functionName);\n+    }\n+\n+    @Override\n+    public void createFunction(String functionName, Function function, boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException {\n+        wrapped.createFunction(functionName, function, ignoreIfExists);\n+    }\n+\n+    @Override\n+    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException {\n+        wrapped.dropFunction(functionName, ignoreIfNotExists);\n+    }\n+\n+    @Override\n+    public void alterFunction(\n+            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException, DefinitionAlreadyExistException,\n+                    DefinitionNotExistException {\n+        wrapped.alterFunction(functionName, changes, ignoreIfNotExists);\n+    }\n+\n     @Override\n     public void markDonePartitions(Identifier identifier, List<Map<String, String>> partitions)\n             throws TableNotExistException {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/function/Function.java b/paimon-core/src/main/java/org/apache/paimon/function/Function.java\nnew file mode 100644\nindex 000000000000..1f892387d4c0\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/function/Function.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.function;\n+\n+import org.apache.paimon.types.DataField;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Interface for function. */\n+public interface Function {\n+\n+    String uuid();\n+\n+    String name();\n+\n+    List<DataField> inputParams();\n+\n+    List<DataField> returnParams();\n+\n+    boolean isDeterministic();\n+\n+    Map<String, FunctionDefinition> definitions();\n+\n+    FunctionDefinition definition(String name);\n+\n+    String comment();\n+\n+    Map<String, String> options();\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/function/FunctionChange.java b/paimon-core/src/main/java/org/apache/paimon/function/FunctionChange.java\nnew file mode 100644\nindex 000000000000..01bcac532d9f\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/function/FunctionChange.java\n@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.function;\n+\n+import org.apache.paimon.annotation.Public;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonSubTypes;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonTypeInfo;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.Serializable;\n+import java.util.Objects;\n+\n+/** Function change. */\n+@Public\n+@JsonTypeInfo(\n+        use = JsonTypeInfo.Id.NAME,\n+        include = JsonTypeInfo.As.PROPERTY,\n+        property = FunctionChange.Actions.FIELD_TYPE)\n+@JsonSubTypes({\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.SetFunctionOption.class,\n+            name = FunctionChange.Actions.SET_OPTION_ACTION),\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.RemoveFunctionOption.class,\n+            name = FunctionChange.Actions.REMOVE_OPTION_ACTION),\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.UpdateFunctionComment.class,\n+            name = FunctionChange.Actions.UPDATE_COMMENT_ACTION),\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.AddDefinition.class,\n+            name = FunctionChange.Actions.ADD_DEFINITION_ACTION),\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.UpdateDefinition.class,\n+            name = FunctionChange.Actions.UPDATE_DEFINITION_ACTION),\n+    @JsonSubTypes.Type(\n+            value = FunctionChange.DropDefinition.class,\n+            name = FunctionChange.Actions.DROP_DEFINITION_ACTION)\n+})\n+public interface FunctionChange extends Serializable {\n+\n+    static FunctionChange setOption(String key, String value) {\n+        return new FunctionChange.SetFunctionOption(key, value);\n+    }\n+\n+    static FunctionChange removeOption(String key) {\n+        return new FunctionChange.RemoveFunctionOption(key);\n+    }\n+\n+    static FunctionChange updateComment(String comment) {\n+        return new FunctionChange.UpdateFunctionComment(comment);\n+    }\n+\n+    static FunctionChange addDefinition(String name, FunctionDefinition definition) {\n+        return new FunctionChange.AddDefinition(name, definition);\n+    }\n+\n+    static FunctionChange updateDefinition(String name, FunctionDefinition definition) {\n+        return new FunctionChange.UpdateDefinition(name, definition);\n+    }\n+\n+    static FunctionChange dropDefinition(String name) {\n+        return new FunctionChange.DropDefinition(name);\n+    }\n+\n+    /** set a function option for function change. */\n+    final class SetFunctionOption implements FunctionChange {\n+\n+        private static final long serialVersionUID = 1L;\n+\n+        private static final String FIELD_KEY = \""key\"";\n+        private static final String FIELD_VALUE = \""value\"";\n+\n+        @JsonProperty(FIELD_KEY)\n+        private final String key;\n+\n+        @JsonProperty(FIELD_VALUE)\n+        private final String value;\n+\n+        @JsonCreator\n+        private SetFunctionOption(\n+                @JsonProperty(FIELD_KEY) String key, @JsonProperty(FIELD_VALUE) String value) {\n+            this.key = key;\n+            this.value = value;\n+        }\n+\n+        @JsonGetter(FIELD_KEY)\n+        public String key() {\n+            return key;\n+        }\n+\n+        @JsonGetter(FIELD_VALUE)\n+        public String value() {\n+            return value;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            SetFunctionOption that = (SetFunctionOption) o;\n+            return key.equals(that.key) && value.equals(that.value);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(key, value);\n+        }\n+    }\n+\n+    /** remove a function option for function change. */\n+    final class RemoveFunctionOption implements FunctionChange {\n+\n+        private static final long serialVersionUID = 1L;\n+\n+        private static final String FIELD_KEY = \""key\"";\n+\n+        @JsonProperty(FIELD_KEY)\n+        private final String key;\n+\n+        private RemoveFunctionOption(@JsonProperty(FIELD_KEY) String key) {\n+            this.key = key;\n+        }\n+\n+        @JsonGetter(FIELD_KEY)\n+        public String key() {\n+            return key;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            RemoveFunctionOption that = (RemoveFunctionOption) o;\n+            return key.equals(that.key);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(key);\n+        }\n+    }\n+\n+    /** update a function comment for function change. */\n+    final class UpdateFunctionComment implements FunctionChange {\n+\n+        private static final long serialVersionUID = 1L;\n+\n+        private static final String FIELD_COMMENT = \""comment\"";\n+\n+        // If comment is null, means to remove comment\n+        @JsonProperty(FIELD_COMMENT)\n+        private final @Nullable String comment;\n+\n+        private UpdateFunctionComment(@JsonProperty(FIELD_COMMENT) @Nullable String comment) {\n+            this.comment = comment;\n+        }\n+\n+        @JsonGetter(FIELD_COMMENT)\n+        public @Nullable String comment() {\n+            return comment;\n+        }\n+\n+        @Override\n+        public boolean equals(Object object) {\n+            if (this == object) {\n+                return true;\n+            }\n+            if (object == null || getClass() != object.getClass()) {\n+                return false;\n+            }\n+            UpdateFunctionComment that = (UpdateFunctionComment) object;\n+            return Objects.equals(comment, that.comment);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(comment);\n+        }\n+    }\n+\n+    /** add definition for function change. */\n+    final class AddDefinition implements FunctionChange {\n+        private static final long serialVersionUID = 1L;\n+        private static final String FIELD_DEFINITION_NAME = \""name\"";\n+        private static final String FIELD_DEFINITION = \""definition\"";\n+\n+        @JsonProperty(FIELD_DEFINITION_NAME)\n+        private final String name;\n+\n+        @JsonProperty(FIELD_DEFINITION)\n+        private final FunctionDefinition definition;\n+\n+        @JsonCreator\n+        public AddDefinition(\n+                @JsonProperty(FIELD_DEFINITION_NAME) String name,\n+                @JsonProperty(FIELD_DEFINITION) FunctionDefinition definition) {\n+            this.name = name;\n+            this.definition = definition;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION_NAME)\n+        public String name() {\n+            return name;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION)\n+        public FunctionDefinition definition() {\n+            return definition;\n+        }\n+\n+        @Override\n+        public boolean equals(Object object) {\n+            if (this == object) {\n+                return true;\n+            }\n+            if (object == null || getClass() != object.getClass()) {\n+                return false;\n+            }\n+            AddDefinition that = (AddDefinition) object;\n+            return Objects.equals(name, that.name) && Objects.equals(definition, that.definition);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(name, definition);\n+        }\n+    }\n+\n+    /** update definition for function change. */\n+    final class UpdateDefinition implements FunctionChange {\n+        private static final long serialVersionUID = 1L;\n+        private static final String FIELD_DEFINITION_NAME = \""name\"";\n+        private static final String FIELD_DEFINITION = \""definition\"";\n+\n+        @JsonProperty(FIELD_DEFINITION_NAME)\n+        private final String name;\n+\n+        @JsonProperty(FIELD_DEFINITION)\n+        private final FunctionDefinition definition;\n+\n+        @JsonCreator\n+        public UpdateDefinition(\n+                @JsonProperty(FIELD_DEFINITION_NAME) String name,\n+                @JsonProperty(FIELD_DEFINITION) FunctionDefinition definition) {\n+            this.name = name;\n+            this.definition = definition;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION_NAME)\n+        public String name() {\n+            return name;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION)\n+        public FunctionDefinition definition() {\n+            return definition;\n+        }\n+\n+        @Override\n+        public boolean equals(Object object) {\n+            if (this == object) {\n+                return true;\n+            }\n+            if (object == null || getClass() != object.getClass()) {\n+                return false;\n+            }\n+            UpdateDefinition that = (UpdateDefinition) object;\n+            return Objects.equals(name, that.name) && Objects.equals(definition, that.definition);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(definition, definition);\n+        }\n+    }\n+\n+    /** drop definition for function change. */\n+    final class DropDefinition implements FunctionChange {\n+        private static final long serialVersionUID = 1L;\n+        private static final String FIELD_DEFINITION_NAME = \""name\"";\n+\n+        @JsonProperty(FIELD_DEFINITION_NAME)\n+        private final String name;\n+\n+        @JsonCreator\n+        public DropDefinition(@JsonProperty(FIELD_DEFINITION_NAME) String name) {\n+            this.name = name;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION_NAME)\n+        public String name() {\n+            return name;\n+        }\n+\n+        @Override\n+        public boolean equals(Object object) {\n+            if (this == object) {\n+                return true;\n+            }\n+            if (object == null || getClass() != object.getClass()) {\n+                return false;\n+            }\n+            DropDefinition that = (DropDefinition) object;\n+            return Objects.equals(name, that.name);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(name);\n+        }\n+    }\n+\n+    /** Actions for function alter. */\n+    class Actions {\n+        public static final String FIELD_TYPE = \""action\"";\n+        public static final String ADD_DEFINITION_ACTION = \""addDefinition\"";\n+        public static final String UPDATE_DEFINITION_ACTION = \""updateDefinition\"";\n+        public static final String DROP_DEFINITION_ACTION = \""dropDefinition\"";\n+        public static final String SET_OPTION_ACTION = \""setOption\"";\n+        public static final String REMOVE_OPTION_ACTION = \""removeOption\"";\n+        public static final String UPDATE_COMMENT_ACTION = \""updateComment\"";\n+\n+        private Actions() {}\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/function/FunctionDefinition.java b/paimon-core/src/main/java/org/apache/paimon/function/FunctionDefinition.java\nnew file mode 100644\nindex 000000000000..5683797ee2c8\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/function/FunctionDefinition.java\n@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.function;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonSubTypes;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonTypeInfo;\n+\n+import java.util.List;\n+import java.util.Objects;\n+\n+/** Function definition. */\n+@JsonTypeInfo(\n+        use = JsonTypeInfo.Id.NAME,\n+        include = JsonTypeInfo.As.PROPERTY,\n+        property = FunctionDefinition.Types.FIELD_TYPE)\n+@JsonSubTypes({\n+    @JsonSubTypes.Type(\n+            value = FunctionDefinition.FileFunctionDefinition.class,\n+            name = FunctionDefinition.Types.FILE_TYPE),\n+    @JsonSubTypes.Type(\n+            value = FunctionDefinition.SQLFunctionDefinition.class,\n+            name = FunctionDefinition.Types.SQL_TYPE),\n+    @JsonSubTypes.Type(\n+            value = FunctionDefinition.LambdaFunctionDefinition.class,\n+            name = FunctionDefinition.Types.LAMBDA_TYPE)\n+})\n+public interface FunctionDefinition {\n+\n+    static FunctionDefinition file(\n+            String fileType,\n+            List<String> storagePaths,\n+            String language,\n+            String className,\n+            String functionName) {\n+        return new FunctionDefinition.FileFunctionDefinition(\n+                fileType, storagePaths, language, className, functionName);\n+    }\n+\n+    static FunctionDefinition sql(String definition) {\n+        return new FunctionDefinition.SQLFunctionDefinition(definition);\n+    }\n+\n+    static FunctionDefinition lambda(String definition, String language) {\n+        return new FunctionDefinition.LambdaFunctionDefinition(definition, language);\n+    }\n+\n+    /** File function definition. */\n+    @JsonIgnoreProperties(ignoreUnknown = true)\n+    final class FileFunctionDefinition implements FunctionDefinition {\n+\n+        private static final String FIELD_FILE_TYPE = \""fileType\"";\n+        private static final String FIELD_STORAGE_PATHS = \""storagePaths\"";\n+        private static final String FIELD_LANGUAGE = \""language\"";\n+        private static final String FIELD_CLASS_NAME = \""className\"";\n+        private static final String FIELD_FUNCTION_NAME = \""functionName\"";\n+\n+        @JsonProperty(FIELD_FILE_TYPE)\n+        private final String fileType;\n+\n+        @JsonProperty(FIELD_STORAGE_PATHS)\n+        private final List<String> storagePaths;\n+\n+        @JsonProperty(FIELD_LANGUAGE)\n+        private String language;\n+\n+        @JsonProperty(FIELD_CLASS_NAME)\n+        private String className;\n+\n+        @JsonProperty(FIELD_FUNCTION_NAME)\n+        private String functionName;\n+\n+        public FileFunctionDefinition(\n+                @JsonProperty(FIELD_FILE_TYPE) String fileType,\n+                @JsonProperty(FIELD_STORAGE_PATHS) List<String> storagePaths,\n+                @JsonProperty(FIELD_LANGUAGE) String language,\n+                @JsonProperty(FIELD_CLASS_NAME) String className,\n+                @JsonProperty(FIELD_FUNCTION_NAME) String functionName) {\n+            this.fileType = fileType;\n+            this.storagePaths = storagePaths;\n+            this.language = language;\n+            this.className = className;\n+            this.functionName = functionName;\n+        }\n+\n+        @JsonGetter(FIELD_FILE_TYPE)\n+        public String fileType() {\n+            return fileType;\n+        }\n+\n+        @JsonGetter(FIELD_STORAGE_PATHS)\n+        public List<String> storagePaths() {\n+            return storagePaths;\n+        }\n+\n+        @JsonGetter(FIELD_LANGUAGE)\n+        public String language() {\n+            return language;\n+        }\n+\n+        @JsonGetter(FIELD_CLASS_NAME)\n+        public String className() {\n+            return className;\n+        }\n+\n+        @JsonGetter(FIELD_FUNCTION_NAME)\n+        public String functionName() {\n+            return functionName;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            FileFunctionDefinition that = (FileFunctionDefinition) o;\n+            return fileType.equals(that.fileType)\n+                    && Objects.equals(storagePaths, that.storagePaths)\n+                    && Objects.equals(language, that.language)\n+                    && Objects.equals(className, that.className)\n+                    && Objects.equals(functionName, that.functionName);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            int result = Objects.hash(fileType, language, className, functionName);\n+            result = 31 * result + Objects.hashCode(storagePaths);\n+            return result;\n+        }\n+    }\n+\n+    /** SQL function definition. */\n+    @JsonIgnoreProperties(ignoreUnknown = true)\n+    final class SQLFunctionDefinition implements FunctionDefinition {\n+\n+        private static final String FIELD_DEFINITION = \""definition\"";\n+\n+        private final String definition;\n+\n+        public SQLFunctionDefinition(@JsonProperty(FIELD_DEFINITION) String definition) {\n+            this.definition = definition;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION)\n+        public String definition() {\n+            return definition;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            SQLFunctionDefinition that = (SQLFunctionDefinition) o;\n+            return Objects.equals(definition, that.definition);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(definition);\n+        }\n+    }\n+\n+    /** Lambda function definition. */\n+    @JsonIgnoreProperties(ignoreUnknown = true)\n+    final class LambdaFunctionDefinition implements FunctionDefinition {\n+\n+        private static final String FIELD_DEFINITION = \""definition\"";\n+        private static final String FIELD_LANGUAGE = \""language\"";\n+\n+        private final String definition;\n+        private final String language;\n+\n+        public LambdaFunctionDefinition(\n+                @JsonProperty(FIELD_DEFINITION) String definition,\n+                @JsonProperty(FIELD_LANGUAGE) String language) {\n+            this.definition = definition;\n+            this.language = language;\n+        }\n+\n+        @JsonGetter(FIELD_DEFINITION)\n+        public String definition() {\n+            return definition;\n+        }\n+\n+        @JsonGetter(FIELD_LANGUAGE)\n+        public String language() {\n+            return language;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            LambdaFunctionDefinition that = (LambdaFunctionDefinition) o;\n+            return definition.equals(that.definition) && language.equals(that.language);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(definition, language);\n+        }\n+    }\n+\n+    /** Types for FunctionDefinition. */\n+    class Types {\n+        public static final String FIELD_TYPE = \""type\"";\n+        public static final String FILE_TYPE = \""file\"";\n+        public static final String SQL_TYPE = \""sql\"";\n+        public static final String LAMBDA_TYPE = \""lambda\"";\n+\n+        private Types() {}\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/function/FunctionImpl.java b/paimon-core/src/main/java/org/apache/paimon/function/FunctionImpl.java\nnew file mode 100644\nindex 000000000000..683d23d1739f\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/function/FunctionImpl.java\n@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.function;\n+\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n+import org.apache.paimon.types.DataField;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Function implementation. */\n+public class FunctionImpl implements Function {\n+\n+    private final String uuid;\n+\n+    private final String name;\n+\n+    private final List<DataField> inputParams;\n+\n+    private final List<DataField> returnParams;\n+\n+    private final boolean deterministic;\n+\n+    private final Map<String, FunctionDefinition> definitions;\n+\n+    private final String comment;\n+\n+    private final Map<String, String> options;\n+\n+    public FunctionImpl(\n+            String uuid,\n+            String functionName,\n+            List<DataField> inputParams,\n+            List<DataField> returnParams,\n+            boolean deterministic,\n+            Map<String, FunctionDefinition> definitions,\n+            String comment,\n+            Map<String, String> options) {\n+        this.uuid = uuid;\n+        this.name = functionName;\n+        this.inputParams = inputParams;\n+        this.returnParams = returnParams;\n+        this.deterministic = deterministic;\n+        this.definitions = definitions;\n+        this.comment = comment;\n+        this.options = options;\n+    }\n+\n+    public FunctionImpl(GetFunctionResponse response) {\n+        this.uuid = response.uuid();\n+        this.name = response.name();\n+        this.inputParams = response.inputParams();\n+        this.returnParams = response.returnParams();\n+        this.deterministic = response.isDeterministic();\n+        this.definitions = response.definitions();\n+        this.comment = response.comment();\n+        this.options = response.options();\n+    }\n+\n+    @Override\n+    public String uuid() {\n+        return this.uuid;\n+    }\n+\n+    @Override\n+    public String name() {\n+        return this.name;\n+    }\n+\n+    @Override\n+    public List<DataField> inputParams() {\n+        return inputParams;\n+    }\n+\n+    @Override\n+    public List<DataField> returnParams() {\n+        return returnParams;\n+    }\n+\n+    @Override\n+    public boolean isDeterministic() {\n+        return deterministic;\n+    }\n+\n+    @Override\n+    public Map<String, FunctionDefinition> definitions() {\n+        return definitions;\n+    }\n+\n+    @Override\n+    public FunctionDefinition definition(String name) {\n+        return definitions.get(name);\n+    }\n+\n+    @Override\n+    public String comment() {\n+        return comment;\n+    }\n+\n+    @Override\n+    public Map<String, String> options() {\n+        return options;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex d443658e386f..a8a1a695ff94 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -30,6 +30,8 @@\n import org.apache.paimon.catalog.TableMetadata;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.Path;\n+import org.apache.paimon.function.FunctionChange;\n+import org.apache.paimon.function.FunctionImpl;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.partition.PartitionStatistics;\n@@ -43,12 +45,14 @@\n import org.apache.paimon.rest.exceptions.NotImplementedException;\n import org.apache.paimon.rest.exceptions.ServiceFailureException;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterFunctionRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.AlterViewRequest;\n import org.apache.paimon.rest.requests.AuthTableQueryRequest;\n import org.apache.paimon.rest.requests.CommitTableRequest;\n import org.apache.paimon.rest.requests.CreateBranchRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateFunctionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.ForwardBranchRequest;\n@@ -60,12 +64,14 @@\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetTableSnapshotResponse;\n import org.apache.paimon.rest.responses.GetTableTokenResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListBranchesResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListFunctionsResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTableDetailsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n@@ -823,6 +829,88 @@ public void alterPartitions(Identifier identifier, List<PartitionStatistics> par\n         // not require special reporting.\n     }\n \n+    @Override\n+    public List<String> listFunctions() {\n+        return listDataFromPageApi(\n+                queryParams ->\n+                        client.get(\n+                                resourcePaths.functions(),\n+                                queryParams,\n+                                ListFunctionsResponse.class,\n+                                restAuthFunction));\n+    }\n+\n+    @Override\n+    public org.apache.paimon.function.Function getFunction(String functionName)\n+            throws FunctionNotExistException {\n+        try {\n+            GetFunctionResponse response =\n+                    client.get(\n+                            resourcePaths.function(functionName),\n+                            GetFunctionResponse.class,\n+                            restAuthFunction);\n+            return new FunctionImpl(response);\n+        } catch (NoSuchResourceException e) {\n+            throw new FunctionNotExistException(functionName, e);\n+        }\n+    }\n+\n+    @Override\n+    public void createFunction(\n+            String functionName,\n+            org.apache.paimon.function.Function function,\n+            boolean ignoreIfExists)\n+            throws FunctionAlreadyExistException {\n+        try {\n+            client.post(\n+                    resourcePaths.functions(),\n+                    new CreateFunctionRequest(function),\n+                    restAuthFunction);\n+        } catch (AlreadyExistsException e) {\n+            if (ignoreIfExists) {\n+                return;\n+            }\n+            throw new FunctionAlreadyExistException(functionName, e);\n+        }\n+    }\n+\n+    @Override\n+    public void dropFunction(String functionName, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException {\n+        try {\n+            client.delete(resourcePaths.function(functionName), restAuthFunction);\n+        } catch (NoSuchResourceException e) {\n+            if (ignoreIfNotExists) {\n+                return;\n+            }\n+            throw new FunctionNotExistException(functionName, e);\n+        }\n+    }\n+\n+    @Override\n+    public void alterFunction(\n+            String functionName, List<FunctionChange> changes, boolean ignoreIfNotExists)\n+            throws FunctionNotExistException, DefinitionAlreadyExistException,\n+                    DefinitionNotExistException {\n+        try {\n+            client.post(\n+                    resourcePaths.function(functionName),\n+                    new AlterFunctionRequest(changes),\n+                    restAuthFunction);\n+        } catch (AlreadyExistsException e) {\n+            throw new DefinitionAlreadyExistException(functionName, e.resourceName());\n+        } catch (NoSuchResourceException e) {\n+            if (StringUtils.equals(e.resourceType(), ErrorResponse.RESOURCE_TYPE_DEFINITION)) {\n+                throw new DefinitionNotExistException(functionName, e.resourceName());\n+            }\n+            if (!ignoreIfNotExists) {\n+                throw new FunctionNotExistException(functionName);\n+            }\n+        } catch (BadRequestException e) {\n+            throw new IllegalArgumentException(e.getMessage());\n+        }\n+    }\n+\n     @Override\n     public View getView(Identifier identifier) throws ViewNotExistException {\n         try {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex cf1bcd3bcc36..ac79e977cbed 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -36,6 +36,7 @@ public class ResourcePaths {\n     protected static final String TABLE_DETAILS = \""table-details\"";\n     protected static final String VIEW_DETAILS = \""view-details\"";\n     protected static final String ROLLBACK = \""rollback\"";\n+    protected static final String FUNCTIONS = \""functions\"";\n \n     private static final Joiner SLASH = Joiner.on(\""/\"").skipNulls();\n \n@@ -213,4 +214,12 @@ public String view(String databaseName, String viewName) {\n     public String renameView() {\n         return SLASH.join(V1, prefix, VIEWS, \""rename\"");\n     }\n+\n+    public String functions() {\n+        return SLASH.join(V1, prefix, FUNCTIONS);\n+    }\n+\n+    public String function(String functionName) {\n+        return SLASH.join(V1, prefix, FUNCTIONS, encodeString(functionName));\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterFunctionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterFunctionRequest.java\nnew file mode 100644\nindex 000000000000..79986aedc361\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterFunctionRequest.java\n@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.function.FunctionChange;\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Request for altering function. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class AlterFunctionRequest implements RESTRequest {\n+\n+    private static final String FIELD_CHANGES = \""changes\"";\n+\n+    @JsonProperty(FIELD_CHANGES)\n+    private final List<FunctionChange> changes;\n+\n+    @JsonCreator\n+    public AlterFunctionRequest(@JsonProperty(FIELD_CHANGES) List<FunctionChange> changes) {\n+        this.changes = changes;\n+    }\n+\n+    @JsonGetter(FIELD_CHANGES)\n+    public List<FunctionChange> changes() {\n+        return changes;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java\nnew file mode 100644\nindex 000000000000..31c816f835af\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateFunctionRequest.java\n@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionDefinition;\n+import org.apache.paimon.rest.RESTRequest;\n+import org.apache.paimon.types.DataField;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Request for creating function. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class CreateFunctionRequest implements RESTRequest {\n+\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_INPUT_PARAMETERS = \""inputParams\"";\n+    private static final String FIELD_RETURN_PARAMETERS = \""returnParams\"";\n+    private static final String FIELD_DEFINITIONS = \""definitions\"";\n+    private static final String FIELD_DETERMINISTIC = \""deterministic\"";\n+    private static final String FIELD_COMMENT = \""comment\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+\n+    @JsonProperty(FIELD_NAME)\n+    private final String functionName;\n+\n+    @JsonProperty(FIELD_INPUT_PARAMETERS)\n+    private final List<DataField> inputParams;\n+\n+    @JsonProperty(FIELD_RETURN_PARAMETERS)\n+    private final List<DataField> returnParams;\n+\n+    @JsonProperty(FIELD_DETERMINISTIC)\n+    private final boolean deterministic;\n+\n+    @JsonProperty(FIELD_DEFINITIONS)\n+    private final Map<String, FunctionDefinition> definitions;\n+\n+    @JsonProperty(FIELD_COMMENT)\n+    private final String comment;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private final Map<String, String> options;\n+\n+    @JsonCreator\n+    public CreateFunctionRequest(\n+            @JsonProperty(FIELD_NAME) String functionName,\n+            @JsonProperty(FIELD_INPUT_PARAMETERS) List<DataField> inputParams,\n+            @JsonProperty(FIELD_RETURN_PARAMETERS) List<DataField> returnParams,\n+            @JsonProperty(FIELD_DETERMINISTIC) boolean deterministic,\n+            @JsonProperty(FIELD_DEFINITIONS) Map<String, FunctionDefinition> definitions,\n+            @JsonProperty(FIELD_COMMENT) String comment,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options) {\n+        this.functionName = functionName;\n+        this.inputParams = inputParams;\n+        this.returnParams = returnParams;\n+        this.deterministic = deterministic;\n+        this.definitions = definitions;\n+        this.comment = comment;\n+        this.options = options;\n+    }\n+\n+    public CreateFunctionRequest(Function function) {\n+        this.functionName = function.name();\n+        this.inputParams = function.inputParams();\n+        this.returnParams = function.returnParams();\n+        this.deterministic = function.isDeterministic();\n+        this.definitions = function.definitions();\n+        this.comment = function.comment();\n+        this.options = function.options();\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String name() {\n+        return functionName;\n+    }\n+\n+    @JsonGetter(FIELD_INPUT_PARAMETERS)\n+    public List<DataField> inputParams() {\n+        return inputParams;\n+    }\n+\n+    @JsonGetter(FIELD_RETURN_PARAMETERS)\n+    public List<DataField> returnParams() {\n+        return returnParams;\n+    }\n+\n+    @JsonGetter(FIELD_DETERMINISTIC)\n+    public boolean isDeterministic() {\n+        return deterministic;\n+    }\n+\n+    @JsonGetter(FIELD_DEFINITIONS)\n+    public Map<String, FunctionDefinition> definitions() {\n+        return definitions;\n+    }\n+\n+    public FunctionDefinition definition(String dialect) {\n+        return definitions.get(dialect);\n+    }\n+\n+    @JsonGetter(FIELD_COMMENT)\n+    public String comment() {\n+        return comment;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> options() {\n+        return options;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\nindex d1de92e7d647..d4f9662dfa28 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n@@ -47,6 +47,10 @@ public class ErrorResponse implements RESTResponse {\n \n     public static final String RESOURCE_TYPE_DIALECT = \""DIALECT\"";\n \n+    public static final String RESOURCE_TYPE_FUNCTION = \""FUNCTION\"";\n+\n+    public static final String RESOURCE_TYPE_DEFINITION = \""DEFINITION\"";\n+\n     private static final String FIELD_MESSAGE = \""message\"";\n     private static final String FIELD_RESOURCE_TYPE = \""resourceType\"";\n     private static final String FIELD_RESOURCE_NAME = \""resourceName\"";\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java\nnew file mode 100644\nindex 000000000000..6780c389e4df\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetFunctionResponse.java\n@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.function.FunctionDefinition;\n+import org.apache.paimon.types.DataField;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Response for getting a function. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class GetFunctionResponse extends AuditRESTResponse {\n+\n+    private static final String FIELD_UUID = \""uuid\"";\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_INPUT_PARAMETERS = \""inputParams\"";\n+    private static final String FIELD_RETURN_PARAMETERS = \""returnParams\"";\n+    private static final String FIELD_DEFINITIONS = \""definitions\"";\n+    private static final String FIELD_DETERMINISTIC = \""deterministic\"";\n+    private static final String FIELD_COMMENT = \""comment\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+\n+    @JsonProperty(FIELD_UUID)\n+    private final String uuid;\n+\n+    @JsonProperty(FIELD_NAME)\n+    private final String functionName;\n+\n+    @JsonProperty(FIELD_INPUT_PARAMETERS)\n+    private final List<DataField> inputParams;\n+\n+    @JsonProperty(FIELD_RETURN_PARAMETERS)\n+    private final List<DataField> returnParams;\n+\n+    @JsonProperty(FIELD_DETERMINISTIC)\n+    private final boolean deterministic;\n+\n+    @JsonProperty(FIELD_DEFINITIONS)\n+    private final Map<String, FunctionDefinition> definitions;\n+\n+    @JsonProperty(FIELD_COMMENT)\n+    private final String comment;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private final Map<String, String> options;\n+\n+    @JsonCreator\n+    public GetFunctionResponse(\n+            @JsonProperty(FIELD_UUID) String uuid,\n+            @JsonProperty(FIELD_NAME) String functionName,\n+            @JsonProperty(FIELD_INPUT_PARAMETERS) List<DataField> inputParams,\n+            @JsonProperty(FIELD_RETURN_PARAMETERS) List<DataField> returnParams,\n+            @JsonProperty(FIELD_DETERMINISTIC) boolean deterministic,\n+            @JsonProperty(FIELD_DEFINITIONS) Map<String, FunctionDefinition> definitions,\n+            @JsonProperty(FIELD_COMMENT) String comment,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options,\n+            @JsonProperty(FIELD_OWNER) String owner,\n+            @JsonProperty(FIELD_CREATED_AT) long createdAt,\n+            @JsonProperty(FIELD_CREATED_BY) String createdBy,\n+            @JsonProperty(FIELD_UPDATED_AT) long updatedAt,\n+            @JsonProperty(FIELD_UPDATED_BY) String updatedBy) {\n+        super(owner, createdAt, createdBy, updatedAt, updatedBy);\n+        this.functionName = functionName;\n+        this.uuid = uuid;\n+        this.inputParams = inputParams;\n+        this.returnParams = returnParams;\n+        this.deterministic = deterministic;\n+        this.definitions = definitions;\n+        this.comment = comment;\n+        this.options = options;\n+    }\n+\n+    public String uuid() {\n+        return this.uuid;\n+    }\n+\n+    public String name() {\n+        return this.functionName;\n+    }\n+\n+    @JsonGetter(FIELD_INPUT_PARAMETERS)\n+    public List<DataField> inputParams() {\n+        return inputParams;\n+    }\n+\n+    @JsonGetter(FIELD_RETURN_PARAMETERS)\n+    public List<DataField> returnParams() {\n+        return returnParams;\n+    }\n+\n+    @JsonGetter(FIELD_DETERMINISTIC)\n+    public boolean isDeterministic() {\n+        return deterministic;\n+    }\n+\n+    @JsonGetter(FIELD_DEFINITIONS)\n+    public Map<String, FunctionDefinition> definitions() {\n+        return definitions;\n+    }\n+\n+    public FunctionDefinition definition(String dialect) {\n+        return definitions.get(dialect);\n+    }\n+\n+    @JsonGetter(FIELD_COMMENT)\n+    public String comment() {\n+        return comment;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> options() {\n+        return options;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListFunctionsResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListFunctionsResponse.java\nnew file mode 100644\nindex 000000000000..0c2165077a43\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListFunctionsResponse.java\n@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Response for listing functions. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ListFunctionsResponse implements PagedResponse<String> {\n+\n+    private static final String FIELD_FUNCTIONS = \""functions\"";\n+    private static final String FIELD_NEXT_PAGE_TOKEN = \""nextPageToken\"";\n+\n+    @JsonProperty(FIELD_FUNCTIONS)\n+    private final List<String> functions;\n+\n+    @JsonProperty(FIELD_NEXT_PAGE_TOKEN)\n+    private final String nextPageToken;\n+\n+    public ListFunctionsResponse(@JsonProperty(FIELD_FUNCTIONS) List<String> functions) {\n+        this(functions, null);\n+    }\n+\n+    @JsonCreator\n+    public ListFunctionsResponse(\n+            @JsonProperty(FIELD_FUNCTIONS) List<String> functions,\n+            @JsonProperty(FIELD_NEXT_PAGE_TOKEN) String nextPageToken) {\n+        this.functions = functions;\n+        this.nextPageToken = nextPageToken;\n+    }\n+\n+    @JsonGetter(FIELD_FUNCTIONS)\n+    public List<String> functions() {\n+        return this.functions;\n+    }\n+\n+    @JsonGetter(FIELD_NEXT_PAGE_TOKEN)\n+    public String getNextPageToken() {\n+        return this.nextPageToken;\n+    }\n+\n+    @Override\n+    public List<String> data() {\n+        return functions();\n+    }\n+}\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 182ebfa31be6..78ca769ec6e2 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -95,8 +95,8 @@ paths:\n     post:\n       tags:\n         - database\n-      summary: Create Databases\n-      operationId: createDatabases\n+      summary: Create Database\n+      operationId: createDatabase\n       parameters:\n         - name: prefix\n           in: path\n@@ -1184,6 +1184,152 @@ paths:\n           $ref: '#/components/responses/ViewAlreadyExistErrorResponse'\n         \""500\"":\n           $ref: '#/components/responses/ServerErrorResponse'\n+  /v1/{prefix}/functions:\n+    get:\n+      tags:\n+        - function\n+      summary: List functions\n+      operationId: listFunctions\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: maxResults\n+          in: query\n+          schema:\n+            type: integer\n+            format: int32\n+        - name: pageToken\n+          in: query\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ListFunctionsResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    post:\n+      tags:\n+        - function\n+      summary: Create Function\n+      operationId: createFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/CreateFunctionRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""400\"":\n+          $ref: '#/components/responses/BadRequestErrorResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""409\"":\n+          $ref: '#/components/responses/FunctionAlreadyExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+\n+  /v1/{prefix}/functions/{function}:\n+    get:\n+      tags:\n+        - function\n+      summary: Get function\n+      operationId: getFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetFunctionResponse'\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/DatabaseNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    post:\n+      tags:\n+        - function\n+      summary: Alter function\n+      operationId: alterFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/AlterFunctionRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/FunctionNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+    delete:\n+      tags:\n+        - function\n+      summary: Drop function\n+      operationId: dropFunction\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: function\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""401\"":\n+          $ref: '#/components/responses/UnauthorizedErrorResponse'\n+        \""404\"":\n+          $ref: '#/components/responses/FunctionNotExistErrorResponse'\n+        \""500\"":\n+          $ref: '#/components/responses/ServerErrorResponse'\n+\n components:\n   #############################\n   # Reusable Response Objects #\n@@ -1305,6 +1451,20 @@ components:\n               \""resourceName\"": \""view\"",\n               \""code\"": 404\n             }\n+    FunctionNotExistErrorResponse:\n+      description:\n+        Not Found - FunctionNotExistException, the function does not exist\n+      content:\n+        application/json:\n+          schema:\n+            $ref: '#/components/responses/ResourceNotExistErrorResponse'\n+          example:\n+            {\n+              \""message\"": \""The given function does not exist\"",\n+              \""resourceType\"": \""FUNCTION\"",\n+              \""resourceName\"": \""function\"",\n+              \""code\"": 404\n+            }\n     ResourceAlreadyExistErrorResponse:\n       description:\n         Used for 409 errors.\n@@ -1370,6 +1530,19 @@ components:\n               \""resourceName\"": \""view\"",\n               \""code\"": 409\n             }\n+    FunctionAlreadyExistErrorResponse:\n+      description: Conflict - The view already exists\n+      content:\n+        application/json:\n+          schema:\n+            $ref: '#/components/responses/ResourceAlreadyExistErrorResponse'\n+          example:\n+            {\n+              \""message\"": \""The given function already exists\"",\n+              \""resourceType\"": \""FUNCTION\"",\n+              \""resourceName\"": \""function\"",\n+              \""code\"": 409\n+            }\n     ServerErrorResponse:\n       description:\n         Used for server 5xx errors.\n@@ -1464,6 +1637,179 @@ components:\n           type: array\n           items:\n             $ref: '#/components/schemas/ViewChange'\n+    CreateFunctionRequest:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+        inputParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        returnParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        deterministic:\n+          type: boolean\n+        definitions:\n+          type: object\n+          additionalProperties:\n+            $ref: \""#/components/schemas/FunctionDefinition\""\n+        comment:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+    AlterFunctionRequest:\n+      type: object\n+      properties:\n+        changes:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/FunctionChange'\n+    FunctionChange:\n+      anyOf:\n+        - $ref: '#/components/schemas/SetFunctionOption'\n+        - $ref: '#/components/schemas/RemoveFunctionOption'\n+        - $ref: '#/components/schemas/UpdateFunctionComment'\n+        - $ref: '#/components/schemas/AddDefinition'\n+        - $ref: '#/components/schemas/UpdateDefinition'\n+        - $ref: '#/components/schemas/DropDefinition'\n+    BaseFunctionChange:\n+      discriminator:\n+        propertyName: action\n+        mapping:\n+          setOption: '#/components/schemas/SetFunctionOption'\n+          removeOption: '#/components/schemas/RemoveFunctionOption'\n+          updateComment: '#/components/schemas/UpdateFunctionComment'\n+          addDefinition: '#/components/schemas/AddDefinition'\n+          updateDefinition: '#/components/schemas/UpdateDefinition'\n+          dropDefinition: '#/components/schemas/DropDefinition'\n+      type: object\n+      required:\n+        - action\n+      properties:\n+        action:\n+          type: string\n+    SetFunctionOption:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""setOption\""\n+        key:\n+          type: string\n+        value:\n+          type: string\n+    RemoveFunctionOption:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""removeOption\""\n+        key:\n+          type: string\n+    UpdateFunctionComment:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""updateComment\""\n+        comment:\n+          type: string\n+    AddDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""addDefinition\""\n+        name:\n+          type: string\n+        definition:\n+          $ref: \""#/components/schemas/FunctionDefinition\""\n+    UpdateDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""updateDefinition\""\n+        name:\n+          type: string\n+        definition:\n+          $ref: \""#/components/schemas/FunctionDefinition\""\n+    DropDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionChange'\n+      properties:\n+        action:\n+          type: string\n+          const: \""dropDefinition\""\n+        name:\n+          type: string\n+    FunctionDefinition:\n+      anyOf:\n+        - $ref: '#/components/schemas/FileFunctionDefinition'\n+        - $ref: '#/components/schemas/SQLFunctionDefinition'\n+        - $ref: '#/components/schemas/LambdaFunctionDefinition'\n+    BaseFunctionDefinition:\n+      discriminator:\n+        propertyName: type\n+        mapping:\n+          file: '#/components/schemas/FileFunctionDefinition'\n+          sql: '#/components/schemas/SQLFunctionDefinition'\n+          lambda: '#/components/schemas/LambdaFunctionDefinition'\n+      type: object\n+      required:\n+        - type\n+      properties:\n+        type:\n+          type: string\n+    SQLFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""sql\""\n+        definition:\n+          type: string\n+    FileFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""file\""\n+        fileType:\n+          type: string\n+        storagePaths:\n+          type: array\n+          items:\n+            type: string\n+        language:\n+          type: string\n+        className:\n+          type: string\n+        functionName:\n+          type: string\n+    LambdaFunctionDefinition:\n+      allOf:\n+        - $ref: '#/components/schemas/BaseFunctionDefinition'\n+      properties:\n+        type:\n+          type: string\n+          const: \""lambda\""\n+        definition:\n+          type: string\n+        language:\n+          type: string\n     ViewChange:\n       anyOf:\n         - $ref: '#/components/schemas/SetViewOption'\n@@ -1859,13 +2205,13 @@ components:\n       required:\n         - type\n       properties:\n-        'type':\n+        type:\n           type: string\n     SnapshotInstant:\n       allOf:\n         - $ref: '#/components/schemas/BaseInstant'\n       properties:\n-        'type':\n+        type:\n           type: string\n           const: \""snapshot\""\n         snapshotId:\n@@ -1875,7 +2221,7 @@ components:\n       allOf:\n         - $ref: '#/components/schemas/BaseInstant'\n       properties:\n-        'type':\n+        type:\n           type: string\n           const: \""tag\""\n         tagName:\n@@ -2139,6 +2485,52 @@ components:\n             $ref: '#/components/schemas/GetViewResponse'\n         nextPageToken:\n           type: string\n+    ListFunctionsResponse:\n+      type: object\n+      properties:\n+        functions:\n+          type: array\n+          items:\n+            type: string\n+        nextPageToken:\n+          type: string\n+    GetFunctionResponse:\n+      type: object\n+      properties:\n+        uuid:\n+          type: string\n+        name:\n+          type: string\n+        inputParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        returnParams:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DataField'\n+        deterministic:\n+          type: boolean\n+        definitions:\n+          type: object\n+          additionalProperties:\n+            $ref: \""#/components/schemas/FunctionDefinition\""\n+        comment:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+        owner:\n+          type: string\n+        createdAt:\n+          format: int64\n+        createdBy:\n+          type: string\n+        updatedAt:\n+          format: int64\n+        updatedBy:\n+          type: string\n     ViewSchema:\n       type: object\n       properties:\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex bcb3f70bc6eb..5b60d10d7564 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -20,12 +20,14 @@\n \n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterFunctionRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.AlterViewRequest;\n import org.apache.paimon.rest.requests.AuthTableQueryRequest;\n import org.apache.paimon.rest.requests.CommitTableRequest;\n import org.apache.paimon.rest.requests.CreateBranchRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateFunctionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.ForwardBranchRequest;\n@@ -37,12 +39,14 @@\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetTableSnapshotResponse;\n import org.apache.paimon.rest.responses.GetTableTokenResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListBranchesResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListFunctionsResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTableDetailsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n@@ -149,7 +153,7 @@ public ListDatabasesResponse listDatabases(\n                 content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n     })\n     @PostMapping(\""/v1/{prefix}/databases\"")\n-    public void createDatabases(\n+    public void createDatabase(\n             @PathVariable String prefix, @RequestBody CreateDatabaseRequest request) {}\n \n     @Operation(\n@@ -953,4 +957,135 @@ public void alterView(\n             @PathVariable String database,\n             @PathVariable String view,\n             @RequestBody AlterViewRequest request) {}\n+\n+    @Operation(\n+            summary = \""List functions\"",\n+            tags = {\""function\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(schema = @Schema(implementation = ListFunctionsResponse.class))\n+                }),\n+        @ApiResponse(\n+                responseCode = \""401\"",\n+                description = \""Unauthorized\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n+    })\n+    @GetMapping(\""/v1/{prefix}/functions\"")\n+    public ListFunctionsResponse listFunctions(\n+            @PathVariable String prefix,\n+            @RequestParam(required = false) Integer maxResults,\n+            @RequestParam(required = false) String pageToken) {\n+        return new ListFunctionsResponse(ImmutableList.of(\""f1\""), null);\n+    }\n+\n+    @Operation(\n+            summary = \""Get function\"",\n+            tags = {\""function\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {@Content(schema = @Schema(implementation = GetFunctionResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""401\"",\n+                description = \""Unauthorized\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n+    })\n+    @GetMapping(\""/v1/{prefix}/functions/{function}\"")\n+    public GetFunctionResponse getFunction(\n+            @PathVariable String prefix, @PathVariable String function) {\n+        return new GetFunctionResponse(\n+                UUID.randomUUID().toString(),\n+                function,\n+                ImmutableList.of(),\n+                ImmutableList.of(),\n+                false,\n+                ImmutableMap.of(),\n+                null,\n+                null,\n+                \""owner\"",\n+                1L,\n+                \""owner\"",\n+                1L,\n+                \""owner\"");\n+    }\n+\n+    @Operation(\n+            summary = \""Create Function\"",\n+            tags = {\""function\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""401\"",\n+                description = \""Unauthorized\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""409\"",\n+                description = \""Resource has exist\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n+    })\n+    @PostMapping(\""/v1/{prefix}/functions\"")\n+    public void createFunction(\n+            @PathVariable String prefix, @RequestBody CreateFunctionRequest request) {}\n+\n+    @Operation(\n+            summary = \""Drop function\"",\n+            tags = {\""function\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""401\"",\n+                description = \""Unauthorized\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n+    })\n+    @GetMapping(\""/v1/{prefix}/functions/{function}\"")\n+    public void dropFunction(@PathVariable String prefix, @PathVariable String function) {}\n+\n+    @Operation(\n+            summary = \""Alter function\"",\n+            tags = {\""function\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""401\"",\n+                description = \""Unauthorized\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""409\"",\n+                description = \""Resource has exist\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))})\n+    })\n+    @PostMapping(\""/v1/{prefix}/functions/{function}\"")\n+    public void alterFunction(\n+            @PathVariable String prefix,\n+            @PathVariable String function,\n+            @RequestBody AlterFunctionRequest request) {}\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 6adfd66cafbf..0bf9a1792074 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -19,17 +19,24 @@\n package org.apache.paimon.rest;\n \n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n+import org.apache.paimon.function.FunctionDefinition;\n+import org.apache.paimon.function.FunctionImpl;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterFunctionRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.AlterViewRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateFunctionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.requests.RollbackTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetTableTokenResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n@@ -51,6 +58,7 @@\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n@@ -112,10 +120,6 @@ public static ListTablesResponse listTablesResponse() {\n         return new ListTablesResponse(Lists.newArrayList(\""table\""));\n     }\n \n-    public static ListTablesResponse listTablesEmptyResponse() {\n-        return new ListTablesResponse(Lists.newArrayList());\n-    }\n-\n     public static CreateTableRequest createTableRequest(String name) {\n         Identifier identifier = Identifier.create(databaseName(), name);\n         Map<String, String> options = new HashMap<>();\n@@ -140,7 +144,7 @@ public static RenameTableRequest renameRequest(String sourceTable, String toTabl\n     }\n \n     public static AlterTableRequest alterTableRequest() {\n-        return new AlterTableRequest(getChanges());\n+        return new AlterTableRequest(getSchemaChanges());\n     }\n \n     public static ListPartitionsResponse listPartitionsResponse() {\n@@ -150,7 +154,7 @@ public static ListPartitionsResponse listPartitionsResponse() {\n         return new ListPartitionsResponse(ImmutableList.of(partition));\n     }\n \n-    public static List<SchemaChange> getChanges() {\n+    public static List<SchemaChange> getSchemaChanges() {\n         // add option\n         SchemaChange addOption = SchemaChange.setOption(\""snapshot.time-retained\"", \""2h\"");\n         // update comment\n@@ -281,6 +285,78 @@ public static AlterViewRequest alterViewRequest() {\n         return new AlterViewRequest(viewChanges);\n     }\n \n+    public static GetFunctionResponse getFunctionResponse() {\n+        Function function = function(\""function\"");\n+        return new GetFunctionResponse(\n+                function.uuid(),\n+                function.name(),\n+                function.inputParams(),\n+                function.returnParams(),\n+                function.isDeterministic(),\n+                function.definitions(),\n+                function.comment(),\n+                function.options(),\n+                \""owner\"",\n+                1L,\n+                \""owner\"",\n+                1L,\n+                \""owner\"");\n+    }\n+\n+    public static CreateFunctionRequest createFunctionRequest() {\n+        Function function = function(\""function\"");\n+        return new CreateFunctionRequest(\n+                function.name(),\n+                function.inputParams(),\n+                function.returnParams(),\n+                function.isDeterministic(),\n+                function.definitions(),\n+                function.comment(),\n+                function.options());\n+    }\n+\n+    public static Function function(String functionName) {\n+        List<DataField> inputParams =\n+                Lists.newArrayList(\n+                        new DataField(0, \""length\"", DataTypes.DOUBLE()),\n+                        new DataField(1, \""width\"", DataTypes.DOUBLE()));\n+        List<DataField> returnParams =\n+                Lists.newArrayList(new DataField(0, \""area\"", DataTypes.DOUBLE()));\n+        FunctionDefinition flinkFunction =\n+                FunctionDefinition.file(\n+                        \""jar\"", Lists.newArrayList(\""/a/b/c.jar\""), \""java\"", \""className\"", \""eval\"");\n+        FunctionDefinition sparkFunction =\n+                FunctionDefinition.lambda(\n+                        \""(Double length, Double width) -> length * width\"", \""java\"");\n+        FunctionDefinition trinoFunction = FunctionDefinition.sql(\""length * width\"");\n+        Map<String, FunctionDefinition> definitions = Maps.newHashMap();\n+        definitions.put(\""flink\"", flinkFunction);\n+        definitions.put(\""spark\"", sparkFunction);\n+        definitions.put(\""trino\"", trinoFunction);\n+        return new FunctionImpl(\n+                UUID.randomUUID().toString(),\n+                functionName,\n+                inputParams,\n+                returnParams,\n+                false,\n+                definitions,\n+                \""comment\"",\n+                ImmutableMap.of());\n+    }\n+\n+    public static AlterFunctionRequest alterFunctionRequest() {\n+        List<FunctionChange> functionChanges = new ArrayList<>();\n+        functionChanges.add(FunctionChange.setOption(\""key\"", \""value\""));\n+        functionChanges.add(FunctionChange.removeOption(\""key\""));\n+        functionChanges.add(FunctionChange.updateComment(\""comment\""));\n+        functionChanges.add(\n+                FunctionChange.addDefinition(\""engine\"", FunctionDefinition.sql(\""x * y\"")));\n+        functionChanges.add(\n+                FunctionChange.updateDefinition(\""engine\"", FunctionDefinition.sql(\""x * y\"")));\n+        functionChanges.add(FunctionChange.dropDefinition(\""engine\""));\n+        return new AlterFunctionRequest(functionChanges);\n+    }\n+\n     private static ViewSchema viewSchema() {\n         List<DataField> fields =\n                 Arrays.asList(\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 0bce34dbbcdc..ff5d9e411698 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -33,6 +33,10 @@\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.fs.local.LocalFileIO;\n import org.apache.paimon.fs.local.LocalFileIOLoader;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n+import org.apache.paimon.function.FunctionDefinition;\n+import org.apache.paimon.function.FunctionImpl;\n import org.apache.paimon.operation.Lock;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n@@ -41,12 +45,14 @@\n import org.apache.paimon.rest.auth.AuthProvider;\n import org.apache.paimon.rest.auth.RESTAuthParameter;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterFunctionRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.AlterViewRequest;\n import org.apache.paimon.rest.requests.AuthTableQueryRequest;\n import org.apache.paimon.rest.requests.CommitTableRequest;\n import org.apache.paimon.rest.requests.CreateBranchRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateFunctionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n@@ -57,12 +63,14 @@\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetTableSnapshotResponse;\n import org.apache.paimon.rest.responses.GetTableTokenResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListBranchesResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListFunctionsResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTableDetailsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n@@ -128,6 +136,7 @@ public class RESTCatalogServer {\n     public static final String AUTHORIZATION_HEADER_KEY = \""Authorization\"";\n \n     private final String databaseUri;\n+    private final String functionUri;\n \n     private final FileSystemCatalog catalog;\n     private final MockWebServer server;\n@@ -140,6 +149,7 @@ public class RESTCatalogServer {\n     private final Map<String, TableSnapshot> tableWithSnapshotId2SnapshotStore = new HashMap<>();\n     private final List<String> noPermissionDatabases = new ArrayList<>();\n     private final List<String> noPermissionTables = new ArrayList<>();\n+    private final Map<String, Function> functionStore = new HashMap<>();\n     private final Map<String, List<String>> columnAuthHandler = new HashMap<>();\n     public final ConfigResponse configResponse;\n     public final String warehouse;\n@@ -154,6 +164,7 @@ public RESTCatalogServer(\n                 this.configResponse.getDefaults().get(RESTCatalogInternalOptions.PREFIX.key());\n         this.resourcePaths = new ResourcePaths(prefix);\n         this.databaseUri = resourcePaths.databases();\n+        this.functionUri = resourcePaths.functions();\n         Options conf = new Options();\n         this.configResponse.getDefaults().forEach(conf::setString);\n         conf.setString(CatalogOptions.WAREHOUSE.key(), dataPath);\n@@ -264,6 +275,11 @@ public MockResponse dispatch(RecordedRequest request) {\n                     } else if (databaseUri.equals(request.getPath())\n                             || request.getPath().contains(databaseUri + \""?\"")) {\n                         return databasesApiHandler(restAuthParameter.method(), data, parameters);\n+                    } else if (functionUri.equals(request.getPath())) {\n+                        return functionsApiHandler(restAuthParameter.method(), data, parameters);\n+                    } else if (request.getPath().startsWith(functionUri)) {\n+                        return functionApiHandler(\n+                                request.getPath(), restAuthParameter.method(), data, parameters);\n                     } else if (resourcePaths.renameTable().equals(request.getPath())) {\n                         return renameTableHandle(restAuthParameter.data());\n                     } else if (resourcePaths.renameView().equals(request.getPath())) {\n@@ -500,6 +516,22 @@ public MockResponse dispatch(RecordedRequest request) {\n                                     e.getMessage(),\n                                     409);\n                     return mockResponse(response, 409);\n+                } catch (Catalog.FunctionAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponse.RESOURCE_TYPE_COLUMN,\n+                                    e.functionName(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n+                } catch (Catalog.DefinitionAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponse.RESOURCE_TYPE_DEFINITION,\n+                                    e.functionName(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n                 } catch (Catalog.ViewNotExistException e) {\n                     response =\n                             new ErrorResponse(\n@@ -516,6 +548,22 @@ public MockResponse dispatch(RecordedRequest request) {\n                                     e.getMessage(),\n                                     404);\n                     return mockResponse(response, 404);\n+                } catch (Catalog.FunctionNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponse.RESOURCE_TYPE_FUNCTION,\n+                                    e.functionName(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n+                } catch (Catalog.DefinitionNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponse.RESOURCE_TYPE_DEFINITION,\n+                                    e.functionName(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n                 } catch (Catalog.ViewAlreadyExistException e) {\n                     response =\n                             new ErrorResponse(\n@@ -711,6 +759,158 @@ private void cleanSnapshot(Identifier identifier, Long snapshotId, Long latestSn\n         }\n     }\n \n+    private MockResponse functionDetailsHandler(String functionName) throws Exception {\n+        if (functionStore.containsKey(functionName)) {\n+            Function function = functionStore.get(functionName);\n+            GetFunctionResponse response =\n+                    new GetFunctionResponse(\n+                            function.uuid(),\n+                            function.name(),\n+                            function.inputParams(),\n+                            function.returnParams(),\n+                            function.isDeterministic(),\n+                            function.definitions(),\n+                            function.comment(),\n+                            function.options(),\n+                            \""owner\"",\n+                            1L,\n+                            \""owner\"",\n+                            1L,\n+                            \""owner\"");\n+            return mockResponse(response, 200);\n+        } else {\n+            throw new Catalog.FunctionNotExistException(functionName);\n+        }\n+    }\n+\n+    private MockResponse functionsApiHandler(\n+            String method, String data, Map<String, String> parameters) throws Exception {\n+        switch (method) {\n+            case \""GET\"":\n+                List<String> functions = new ArrayList<>(functionStore.keySet());\n+                return generateFinalListFunctionsResponse(parameters, functions);\n+            case \""POST\"":\n+                CreateFunctionRequest requestBody =\n+                        OBJECT_MAPPER.readValue(data, CreateFunctionRequest.class);\n+                String functionName = requestBody.name();\n+                if (!functionStore.containsKey(functionName)) {\n+                    Function function =\n+                            new FunctionImpl(\n+                                    UUID.randomUUID().toString(),\n+                                    functionName,\n+                                    requestBody.inputParams(),\n+                                    requestBody.returnParams(),\n+                                    requestBody.isDeterministic(),\n+                                    requestBody.definitions(),\n+                                    requestBody.comment(),\n+                                    requestBody.options());\n+                    functionStore.put(functionName, function);\n+                    return new MockResponse().setResponseCode(200);\n+                } else {\n+                    throw new Catalog.FunctionAlreadyExistException(functionName);\n+                }\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+    }\n+\n+    private MockResponse functionApiHandler(\n+            String path, String method, String data, Map<String, String> parameters)\n+            throws Exception {\n+        String[] resources = path.substring((functionUri + \""/\"").length()).split(\""/\"");\n+        String functionName = RESTUtil.decodeString(resources[0]);\n+        if (!functionStore.containsKey(functionName)) {\n+            throw new Catalog.FunctionNotExistException(functionName);\n+        }\n+        Function function = functionStore.get(functionName);\n+        switch (method) {\n+            case \""DELETE\"":\n+                functionStore.remove(functionName);\n+                break;\n+            case \""GET\"":\n+                GetFunctionResponse response =\n+                        new GetFunctionResponse(\n+                                function.uuid(),\n+                                function.name(),\n+                                function.inputParams(),\n+                                function.returnParams(),\n+                                function.isDeterministic(),\n+                                function.definitions(),\n+                                function.comment(),\n+                                function.options(),\n+                                \""owner\"",\n+                                1L,\n+                                \""owner\"",\n+                                1L,\n+                                \""owner\"");\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                AlterFunctionRequest requestBody =\n+                        OBJECT_MAPPER.readValue(data, AlterFunctionRequest.class);\n+                HashMap<String, FunctionDefinition> newDefinitions =\n+                        new HashMap<>(function.definitions());\n+                Map<String, String> newOptions = new HashMap<>(function.options());\n+                String newComment = function.comment();\n+                for (FunctionChange functionChange : requestBody.changes()) {\n+                    if (functionChange instanceof FunctionChange.SetFunctionOption) {\n+                        FunctionChange.SetFunctionOption setFunctionOption =\n+                                (FunctionChange.SetFunctionOption) functionChange;\n+                        newOptions.put(setFunctionOption.key(), setFunctionOption.value());\n+                    } else if (functionChange instanceof FunctionChange.RemoveFunctionOption) {\n+                        FunctionChange.RemoveFunctionOption removeFunctionOption =\n+                                (FunctionChange.RemoveFunctionOption) functionChange;\n+                        newOptions.remove(removeFunctionOption.key());\n+                    } else if (functionChange instanceof FunctionChange.UpdateFunctionComment) {\n+                        FunctionChange.UpdateFunctionComment updateFunctionComment =\n+                                (FunctionChange.UpdateFunctionComment) functionChange;\n+                        newComment = updateFunctionComment.comment();\n+                    } else if (functionChange instanceof FunctionChange.AddDefinition) {\n+                        FunctionChange.AddDefinition addDefinition =\n+                                (FunctionChange.AddDefinition) functionChange;\n+                        if (function.definition(addDefinition.name()) != null) {\n+                            throw new Catalog.DefinitionAlreadyExistException(\n+                                    functionName, addDefinition.name());\n+                        }\n+                        newDefinitions.put(addDefinition.name(), addDefinition.definition());\n+                    } else if (functionChange instanceof FunctionChange.UpdateDefinition) {\n+                        FunctionChange.UpdateDefinition updateDefinition =\n+                                (FunctionChange.UpdateDefinition) functionChange;\n+                        if (function.definition(updateDefinition.name()) != null) {\n+                            newDefinitions.put(\n+                                    updateDefinition.name(), updateDefinition.definition());\n+                        } else {\n+                            throw new Catalog.DefinitionNotExistException(\n+                                    functionName, updateDefinition.name());\n+                        }\n+                    } else if (functionChange instanceof FunctionChange.DropDefinition) {\n+                        FunctionChange.DropDefinition dropDefinition =\n+                                (FunctionChange.DropDefinition) functionChange;\n+                        if (function.definitions().containsKey(dropDefinition.name())) {\n+                            newDefinitions.remove(dropDefinition.name());\n+                        } else {\n+                            throw new Catalog.DefinitionNotExistException(\n+                                    functionName, dropDefinition.name());\n+                        }\n+                    }\n+                }\n+                function =\n+                        new FunctionImpl(\n+                                functionName,\n+                                function.uuid(),\n+                                function.inputParams(),\n+                                function.returnParams(),\n+                                function.isDeterministic(),\n+                                newDefinitions,\n+                                newComment,\n+                                newOptions);\n+                functionStore.put(functionName, function);\n+                break;\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+        return new MockResponse().setResponseCode(200);\n+    }\n+\n     private MockResponse databasesApiHandler(\n             String method, String data, Map<String, String> parameters) throws Exception {\n         switch (method) {\n@@ -733,6 +933,36 @@ private MockResponse databasesApiHandler(\n         }\n     }\n \n+    private MockResponse generateFinalListFunctionsResponse(\n+            Map<String, String> parameters, List<String> functions) {\n+        RESTResponse response;\n+        if (!functions.isEmpty()) {\n+            int maxResults;\n+            try {\n+                maxResults = getMaxResults(parameters);\n+            } catch (Exception e) {\n+                LOG.error(\n+                        \""parse maxResults {} to int failed\"",\n+                        parameters.getOrDefault(MAX_RESULTS, null));\n+                return mockResponse(\n+                        new ErrorResponse(\n+                                ErrorResponse.RESOURCE_TYPE_TABLE,\n+                                null,\n+                                \""invalid input queryParameter maxResults\""\n+                                        + parameters.get(MAX_RESULTS),\n+                                400),\n+                        400);\n+            }\n+            String pageToken = parameters.getOrDefault(PAGE_TOKEN, null);\n+            PagedList<String> pagedDbs = buildPagedEntities(functions, maxResults, pageToken);\n+            response =\n+                    new ListFunctionsResponse(pagedDbs.getElements(), pagedDbs.getNextPageToken());\n+        } else {\n+            response = new ListFunctionsResponse(new ArrayList<>(), null);\n+        }\n+        return mockResponse(response, 200);\n+    }\n+\n     private MockResponse generateFinalListDatabasesResponse(\n             Map<String, String> parameters, List<String> databases) {\n         RESTResponse response;\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex fe3e4cbcfb52..29632df4a434 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -27,6 +27,9 @@\n import org.apache.paimon.data.BinaryString;\n import org.apache.paimon.data.GenericRow;\n import org.apache.paimon.data.InternalRow;\n+import org.apache.paimon.function.Function;\n+import org.apache.paimon.function.FunctionChange;\n+import org.apache.paimon.function.FunctionDefinition;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.partition.PartitionStatistics;\n@@ -1311,6 +1314,105 @@ void testAlterView() throws Exception {\n                                 false));\n     }\n \n+    @Test\n+    void testFunction() throws Exception {\n+        Function function = MockRESTMessage.function(\""function\"");\n+\n+        catalog.createFunction(function.name(), function, true);\n+        assertThrows(\n+                Catalog.FunctionAlreadyExistException.class,\n+                () -> catalog.createFunction(function.name(), function, false));\n+\n+        assertThat(catalog.listFunctions().contains(function.name())).isTrue();\n+\n+        Function getFunction = catalog.getFunction(function.name());\n+        assertThat(getFunction.name()).isEqualTo(function.name());\n+        for (String dialect : function.definitions().keySet()) {\n+            assertThat(getFunction.definition(dialect)).isEqualTo(function.definition(dialect));\n+        }\n+        catalog.dropFunction(function.name(), true);\n+\n+        assertThat(catalog.listFunctions().contains(function.name())).isFalse();\n+        assertThrows(\n+                Catalog.FunctionNotExistException.class,\n+                () -> catalog.dropFunction(function.name(), false));\n+        assertThrows(\n+                Catalog.FunctionNotExistException.class,\n+                () -> catalog.getFunction(function.name()));\n+    }\n+\n+    @Test\n+    void testAlterFunction() throws Exception {\n+        String functionName = \""alter_function_name\"";\n+        Function function = MockRESTMessage.function(functionName);\n+        FunctionDefinition definition = FunctionDefinition.sql(\""x * y + 1\"");\n+        FunctionChange.AddDefinition addDefinition =\n+                (FunctionChange.AddDefinition) FunctionChange.addDefinition(\""flink_1\"", definition);\n+        assertDoesNotThrow(\n+                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), true));\n+        assertThrows(\n+                Catalog.FunctionNotExistException.class,\n+                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false));\n+        catalog.createFunction(function.name(), function, true);\n+        // set options\n+        String key = UUID.randomUUID().toString();\n+        String value = UUID.randomUUID().toString();\n+        FunctionChange setOption = FunctionChange.setOption(key, value);\n+        catalog.alterFunction(functionName, ImmutableList.of(setOption), false);\n+        Function catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.options().get(key)).isEqualTo(value);\n+\n+        // remove options\n+        catalog.alterFunction(\n+                functionName, ImmutableList.of(FunctionChange.removeOption(key)), false);\n+        catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.options().containsKey(key)).isEqualTo(false);\n+\n+        // update comment\n+        String newComment = \""new comment\"";\n+        catalog.alterFunction(\n+                functionName, ImmutableList.of(FunctionChange.updateComment(newComment)), false);\n+        catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.comment()).isEqualTo(newComment);\n+        // add definition\n+        catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false);\n+        catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.definition(addDefinition.name()))\n+                .isEqualTo(addDefinition.definition());\n+        assertThrows(\n+                Catalog.DefinitionAlreadyExistException.class,\n+                () -> catalog.alterFunction(functionName, ImmutableList.of(addDefinition), false));\n+\n+        // update definition\n+        FunctionChange.UpdateDefinition updateDefinition =\n+                (FunctionChange.UpdateDefinition)\n+                        FunctionChange.updateDefinition(\""flink_1\"", definition);\n+        catalog.alterFunction(functionName, ImmutableList.of(updateDefinition), false);\n+        catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.definition(updateDefinition.name()))\n+                .isEqualTo(updateDefinition.definition());\n+        assertThrows(\n+                Catalog.DefinitionNotExistException.class,\n+                () ->\n+                        catalog.alterFunction(\n+                                functionName,\n+                                ImmutableList.of(\n+                                        FunctionChange.updateDefinition(\""no_exist\"", definition)),\n+                                false));\n+\n+        // drop dialect\n+        FunctionChange.DropDefinition dropDefinition =\n+                (FunctionChange.DropDefinition)\n+                        FunctionChange.dropDefinition(updateDefinition.name());\n+        catalog.alterFunction(functionName, ImmutableList.of(dropDefinition), false);\n+        catalogFunction = catalog.getFunction(functionName);\n+        assertThat(catalogFunction.definition(updateDefinition.name())).isNull();\n+\n+        assertThrows(\n+                Catalog.DefinitionNotExistException.class,\n+                () -> catalog.alterFunction(functionName, ImmutableList.of(dropDefinition), false));\n+    }\n+\n     @Test\n     void testTableAuth() throws Exception {\n         Identifier identifier = Identifier.create(\""test_table_db\"", \""auth_table\"");\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex c395d2e303d1..9a36191d0996 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -19,9 +19,11 @@\n package org.apache.paimon.rest;\n \n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterFunctionRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.AlterViewRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateFunctionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n@@ -30,6 +32,7 @@\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetFunctionResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetTableTokenResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n@@ -42,6 +45,8 @@\n import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.IntType;\n \n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n+\n import org.junit.Test;\n \n import java.util.HashMap;\n@@ -280,4 +285,31 @@ public void alterViewRequestParseTest() throws Exception {\n             assertEquals(parseData.viewChanges().get(i), request.viewChanges().get(i));\n         }\n     }\n+\n+    @Test\n+    public void getFunctionResponseParseTest() throws Exception {\n+        GetFunctionResponse response = MockRESTMessage.getFunctionResponse();\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        GetFunctionResponse parseData =\n+                OBJECT_MAPPER.readValue(responseStr, GetFunctionResponse.class);\n+        assertEquals(response.uuid(), parseData.uuid());\n+    }\n+\n+    @Test\n+    public void createFunctionRequestParseTest() throws JsonProcessingException {\n+        CreateFunctionRequest request = MockRESTMessage.createFunctionRequest();\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        CreateFunctionRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, CreateFunctionRequest.class);\n+        assertEquals(parseData.name(), request.name());\n+    }\n+\n+    @Test\n+    public void alterFunctionRequestParseTest() throws JsonProcessingException {\n+        AlterFunctionRequest request = MockRESTMessage.alterFunctionRequest();\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        AlterFunctionRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, AlterFunctionRequest.class);\n+        assertEquals(parseData.changes().size(), request.changes().size());\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5248"", ""pr_id"": 5248, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 2, ""non_test_files_count"": 4, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java""], ""base_commit"": ""f778c6534cb3d536acb75c1ab1c248965e1dfd64"", ""head_commit"": ""9651f6634a6867085bafe54a976f7a9ca66ea036"", ""repo_url"": ""https://github.com/apache/paimon/pull/5248"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5248"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-10T12:40:56.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex d28a6cd670bc..8a2fd3a4bd07 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -151,7 +151,8 @@ public RESTCatalog(CatalogContext context, boolean configRequired) {\n             String warehouse = options.get(WAREHOUSE);\n             Map<String, String> queryParams =\n                     StringUtils.isNotEmpty(warehouse)\n-                            ? ImmutableMap.of(QUERY_PARAMETER_WAREHOUSE_KEY, warehouse)\n+                            ? ImmutableMap.of(\n+                                    QUERY_PARAMETER_WAREHOUSE_KEY, RESTUtil.encodeString(warehouse))\n                             : ImmutableMap.of();\n             baseHeaders = extractPrefixMap(context.options(), HEADER_PREFIX);\n             options =\n@@ -380,7 +381,7 @@ public boolean commitSnapshot(\n         try {\n             response =\n                     client.post(\n-                            resourcePaths.commitTable(identifier.getDatabaseName()),\n+                            resourcePaths.commitTable(),\n                             request,\n                             CommitTableResponse.class,\n                             restAuthFunction);\n@@ -477,10 +478,7 @@ public void renameTable(Identifier fromTable, Identifier toTable, boolean ignore\n         checkNotSystemTable(toTable, \""renameTable\"");\n         try {\n             RenameTableRequest request = new RenameTableRequest(fromTable, toTable);\n-            client.post(\n-                    resourcePaths.renameTable(fromTable.getDatabaseName()),\n-                    request,\n-                    restAuthFunction);\n+            client.post(resourcePaths.renameTable(), request, restAuthFunction);\n         } catch (NoSuchResourceException e) {\n             if (!ignoreIfNotExists) {\n                 throw new TableNotExistException(fromTable);\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex 301e862beb75..aceca4b40bc4 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -48,7 +48,7 @@ public static ResourcePaths forCatalogProperties(Options options) {\n     private final String prefix;\n \n     public ResourcePaths(String prefix) {\n-        this.prefix = prefix;\n+        this.prefix = encodeString(prefix);\n     }\n \n     public String databases() {\n@@ -81,12 +81,12 @@ public String table(String databaseName, String objectName) {\n                 encodeString(objectName));\n     }\n \n-    public String renameTable(String databaseName) {\n-        return SLASH.join(V1, prefix, DATABASES, encodeString(databaseName), TABLES, \""rename\"");\n+    public String renameTable() {\n+        return SLASH.join(V1, prefix, TABLES, \""rename\"");\n     }\n \n-    public String commitTable(String databaseName) {\n-        return SLASH.join(V1, prefix, DATABASES, encodeString(databaseName), TABLES, \""commit\"");\n+    public String commitTable() {\n+        return SLASH.join(V1, prefix, TABLES, \""commit\"");\n     }\n \n     public String tableToken(String databaseName, String objectName) {\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 982f91bf8578..db0cf39ea41f 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -410,7 +410,7 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}/tables/rename:\n+  /v1/{prefix}/tables/rename:\n     post:\n       tags:\n         - table\n@@ -449,7 +449,7 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}/tables/commit:\n+  /v1/{prefix}/tables/commit:\n     post:\n       tags:\n         - table\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex 1e8c2efeed55..536d8b7177cf 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -375,11 +375,8 @@ public void dropTable(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n-    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/rename\"")\n-    public void renameTable(\n-            @PathVariable String prefix,\n-            @PathVariable String database,\n-            @RequestBody RenameTableRequest request) {}\n+    @PostMapping(\""/v1/{prefix}/tables/rename\"")\n+    public void renameTable(@PathVariable String prefix, @RequestBody RenameTableRequest request) {}\n \n     @Operation(\n             summary = \""Commit table\"",\n@@ -396,11 +393,9 @@ public void renameTable(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n-    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/commit\"")\n+    @PostMapping(\""/v1/{prefix}/tables/commit\"")\n     public CommitTableResponse commitTable(\n-            @PathVariable String prefix,\n-            @PathVariable String database,\n-            @RequestBody CommitTableRequest request) {\n+            @PathVariable String prefix, @RequestBody CommitTableRequest request) {\n         return new CommitTableResponse(true);\n     }\n \n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 7ed259aad8d5..8f876f92e7db 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -147,7 +147,7 @@ public RESTCatalogServer(\n         this.configResponse = config;\n         this.prefix =\n                 this.configResponse.getDefaults().get(RESTCatalogInternalOptions.PREFIX.key());\n-        ResourcePaths resourcePaths = new ResourcePaths(prefix);\n+        this.resourcePaths = new ResourcePaths(prefix);\n         this.databaseUri = resourcePaths.databases();\n         Options conf = new Options();\n         this.configResponse.getDefaults().forEach((k, v) -> conf.setString(k, v));\n@@ -247,6 +247,10 @@ public MockResponse dispatch(RecordedRequest request) {\n                         return mockResponse(configResponse, 200);\n                     } else if (databaseUri.equals(request.getPath())) {\n                         return databasesApiHandler(restAuthParameter.method(), data);\n+                    } else if (resourcePaths.renameTable().equals(request.getPath())) {\n+                        return renameTableHandle(restAuthParameter.data());\n+                    } else if (resourcePaths.commitTable().equals(request.getPath())) {\n+                        return commitTableHandle(restAuthParameter.data());\n                     } else if (request.getPath().startsWith(databaseUri)) {\n                         String[] resources =\n                                 request.getPath()\n@@ -266,10 +270,6 @@ public MockResponse dispatch(RecordedRequest request) {\n                                 resources.length == 2 && resources[1].startsWith(\""tables\"");\n                         boolean isTableDetails =\n                                 resources.length == 2 && resources[1].startsWith(\""table-details\"");\n-                        boolean isTableRename =\n-                                resources.length == 3\n-                                        && \""tables\"".equals(resources[1])\n-                                        && \""rename\"".equals(resources[2]);\n                         boolean isViewRename =\n                                 resources.length == 3\n                                         && \""views\"".equals(resources[1])\n@@ -283,10 +283,6 @@ public MockResponse dispatch(RecordedRequest request) {\n                                         && \""tables\"".equals(resources[1])\n                                         && !\""rename\"".equals(resources[2])\n                                         && !\""commit\"".equals(resources[2]);\n-                        boolean isTableCommit =\n-                                resources.length == 3\n-                                        && \""tables\"".equals(resources[1])\n-                                        && \""commit\"".equals(resources[2]);\n                         boolean isTableToken =\n                                 resources.length == 4\n                                         && \""tables\"".equals(resources[1])\n@@ -375,10 +371,6 @@ public MockResponse dispatch(RecordedRequest request) {\n                             return getDataTokenHandle(identifier);\n                         } else if (isTableSnapshot) {\n                             return snapshotHandle(identifier);\n-                        } else if (isTableRename) {\n-                            return renameTableHandle(restAuthParameter.data());\n-                        } else if (isTableCommit) {\n-                            return commitTableHandle(restAuthParameter.data());\n                         } else if (isTable) {\n                             return tableHandle(\n                                     restAuthParameter.method(),\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java\nindex 18a8a3cf42b6..e3979c437a35 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/ResourcePathsTest.java\n@@ -33,5 +33,10 @@ public void testUrlEncode() {\n         assertEquals(\n                 \""/v1/paimon/databases/test_db/tables/test_table%24snapshot\"",\n                 resourcePaths.table(database, objectName));\n+\n+        resourcePaths = new ResourcePaths(\""paimon/aaaa\"");\n+        assertEquals(\n+                \""/v1/paimon%2Faaaa/databases/test_db/tables/test_table%24snapshot\"",\n+                resourcePaths.table(database, objectName));\n     }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5182"", ""pr_id"": 5182, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 2, ""non_test_files_count"": 2, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/auth/DLFAuthProviderFactory.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/DLFAuthProviderFactoryTest.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/DLFAuthProviderFactoryTest.java""], ""base_commit"": ""b1ccfb82eb23a830e1fdf0452d1f4a42d85c557f"", ""head_commit"": ""6772bf932635640d1a889e8a9143350fbfebbbb0"", ""repo_url"": ""https://github.com/apache/paimon/pull/5182"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5182"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-28T07:38:14.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\nindex c9dd0fcfac54..3fb44314d36c 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n@@ -50,6 +50,12 @@ public class RESTCatalogOptions {\n                     .noDefaultValue()\n                     .withDescription(\""REST Catalog auth token provider.\"");\n \n+    public static final ConfigOption<String> DLF_REGION =\n+            ConfigOptions.key(\""dlf.region\"")\n+                    .stringType()\n+                    .noDefaultValue()\n+                    .withDescription(\""REST Catalog auth DLF region.\"");\n+\n     public static final ConfigOption<String> DLF_TOKEN_PATH =\n             ConfigOptions.key(\""dlf.token-path\"")\n                     .stringType()\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/auth/DLFAuthProviderFactory.java b/paimon-core/src/main/java/org/apache/paimon/rest/auth/DLFAuthProviderFactory.java\nindex a78c44dea2df..c8aff5132043 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/auth/DLFAuthProviderFactory.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/auth/DLFAuthProviderFactory.java\n@@ -21,6 +21,10 @@\n import org.apache.paimon.options.Options;\n import org.apache.paimon.rest.RESTCatalogOptions;\n \n+import java.util.Optional;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n import static org.apache.paimon.rest.RESTCatalogOptions.DLF_TOKEN_PATH;\n import static org.apache.paimon.rest.RESTCatalogOptions.TOKEN_REFRESH_TIME;\n import static org.apache.paimon.rest.RESTCatalogOptions.URI;\n@@ -35,7 +39,8 @@ public String identifier() {\n \n     @Override\n     public AuthProvider create(Options options) {\n-        String region = getRegion(options);\n+        String region =\n+                getRegion(options.getOptional(RESTCatalogOptions.DLF_REGION), options.get(URI));\n         if (options.getOptional(RESTCatalogOptions.DLF_TOKEN_PATH).isPresent()) {\n             String tokenFilePath = options.get(DLF_TOKEN_PATH);\n             long tokenRefreshInMills = options.get(TOKEN_REFRESH_TIME).toMillis();\n@@ -51,15 +56,22 @@ public AuthProvider create(Options options) {\n         throw new IllegalArgumentException(\""DLF token path or AK must be set for DLF Auth.\"");\n     }\n \n-    private static String getRegion(Options options) {\n-        String region = \""undefined\"";\n+    protected static String getRegion(Optional<String> region, String uri) {\n+        if (region.isPresent()) {\n+            return region.get();\n+        }\n         try {\n-            String[] paths = options.get(URI).split(\""\\\\.\"");\n-            if (paths.length > 1) {\n-                region = paths[1];\n+            String regex = \""dlf-(?:pre-)?([a-z]+-[a-z]+(?:-\\\\d+)?)(?:-internal)?\"";\n+\n+            Pattern pattern = Pattern.compile(regex);\n+            Matcher matcher = pattern.matcher(uri);\n+\n+            if (matcher.find()) {\n+                return matcher.group(1);\n             }\n         } catch (Exception ignore) {\n         }\n-        return region;\n+        throw new IllegalArgumentException(\n+                \""Could not get region from conf or uri, please check your config.\"");\n     }\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\nindex e20ff5cb1d60..bbcf4f70127d 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n@@ -43,6 +43,7 @@\n \n import static org.apache.paimon.rest.RESTCatalogOptions.DLF_ACCESS_KEY_ID;\n import static org.apache.paimon.rest.RESTCatalogOptions.DLF_ACCESS_KEY_SECRET;\n+import static org.apache.paimon.rest.RESTCatalogOptions.DLF_REGION;\n import static org.apache.paimon.rest.RESTCatalogOptions.DLF_SECURITY_TOKEN;\n import static org.apache.paimon.rest.RESTCatalogOptions.DLF_TOKEN_PATH;\n import static org.apache.paimon.rest.RESTCatalogOptions.TOKEN;\n@@ -171,6 +172,7 @@ public void testCreateDLFAuthProviderByStsToken() throws IOException {\n         options.set(DLF_ACCESS_KEY_ID.key(), token.getAccessKeyId());\n         options.set(DLF_ACCESS_KEY_SECRET.key(), token.getAccessKeySecret());\n         options.set(DLF_SECURITY_TOKEN.key(), token.getSecurityToken());\n+        options.set(DLF_REGION.key(), \""cn-hangzhou\"");\n         AuthProvider authProvider =\n                 AuthProviderFactory.createAuthProvider(AuthProviderEnum.DLF.identifier(), options);\n         AuthSession session = AuthSession.fromRefreshAuthProvider(null, authProvider);\n@@ -187,6 +189,7 @@ public void testCreateDLFAuthProviderByAk() throws IOException {\n         DLFToken token = new DLFToken(akId, akSecret, null, null);\n         options.set(DLF_ACCESS_KEY_ID.key(), token.getAccessKeyId());\n         options.set(DLF_ACCESS_KEY_SECRET.key(), token.getAccessKeySecret());\n+        options.set(DLF_REGION.key(), \""cn-hangzhou\"");\n         AuthProvider authProvider =\n                 AuthProviderFactory.createAuthProvider(AuthProviderEnum.DLF.identifier(), options);\n         AuthSession session = AuthSession.fromRefreshAuthProvider(null, authProvider);\n@@ -224,7 +227,7 @@ public void testDLFAuthProviderAuthHeaderWhenDataIsNotEmpty() throws Exception {\n         String fileName = UUID.randomUUID().toString();\n         Pair<File, String> tokenFile2Token = generateTokenAndWriteToFile(fileName);\n         String tokenStr = tokenFile2Token.getRight();\n-        String serverUrl = \""https://dlf.cn-hangzhou.aliyuncs.com\"";\n+        String serverUrl = \""https://dlf-cn-hangzhou.aliyuncs.com\"";\n         AuthProvider authProvider = generateDLFAuthProvider(Optional.empty(), fileName, serverUrl);\n         DLFToken token = OBJECT_MAPPER_INSTANCE.readValue(tokenStr, DLFToken.class);\n         Map<String, String> parameters = new HashMap<>();\n@@ -280,6 +283,7 @@ private AuthProvider generateDLFAuthProvider(\n         Options options = new Options();\n         options.set(DLF_TOKEN_PATH.key(), folder.getRoot().getPath() + \""/\"" + fileName);\n         options.set(RESTCatalogOptions.URI.key(), serverUrl);\n+        options.set(DLF_REGION.key(), \""cn-hangzhou\"");\n         tokenRefreshInMillsOpt.ifPresent(\n                 tokenRefreshInMills ->\n                         options.set(TOKEN_REFRESH_TIME.key(), tokenRefreshInMills + \""ms\""));\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/auth/DLFAuthProviderFactoryTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/auth/DLFAuthProviderFactoryTest.java\nnew file mode 100644\nindex 000000000000..c0b6eccd4d10\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/auth/DLFAuthProviderFactoryTest.java\n@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.auth;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Optional;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/** Test for {@link DLFAuthProviderFactory}. */\n+class DLFAuthProviderFactoryTest {\n+\n+    @Test\n+    void getRegion() {\n+        String region = \""cn-hangzhou\"";\n+        String ipPortUri = \""http://127.0.0.1:8080\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.of(region), ipPortUri));\n+        String url = \""https://dlf-\"" + region + \""-internal.aliyuncs.com\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.empty(), url));\n+        url = \""https://dlf-\"" + region + \"".aliyuncs.com\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.empty(), url));\n+        url = \""https://dlf-pre-\"" + region + \"".aliyuncs.com\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.empty(), url));\n+        region = \""us-east-1\"";\n+        url = \""https://dlf-\"" + region + \"".aliyuncs.com\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.empty(), url));\n+        url = \""https://dlf-\"" + region + \""-internal.aliyuncs.com\"";\n+        assertEquals(region, DLFAuthProviderFactory.getRegion(Optional.empty(), url));\n+        assertThrows(\n+                IllegalArgumentException.class,\n+                () -> DLFAuthProviderFactory.getRegion(Optional.empty(), ipPortUri));\n+    }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5170"", ""pr_id"": 5170, ""issue_id"": 5169, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Force creating snapshot for those specified tables even no data ingested\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nWhen ingesting data with kafka_sync_database action, if one table has no data coming since job restarted, no snapshot would be created for it even set as **_commit.force-create-snapshot=true_**, or **_tag.automatic-creation_=process-time**.  This is a **_lazy_** initialization. Only those tables passed before would have the chance to froce creating snapshot.\nNo snapshot means no tag. But the tag is crucial for batchly processing. We need the tag to detect changes incrementally by calling paimon_incremental_to_auto_tag.\n\n### Solution\n\nInitializing eagerly which means _**register**_ the tales at each startup, instead of waiting the first record passing through.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 151, ""test_files_count"": 3, ""non_test_files_count"": 6, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/kafka_sync_database.html"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableFilter.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/TableFilterTest.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/TableFilterTest.java""], ""base_commit"": ""95ab66043ff3b36977254166e0b1a649a11ed731"", ""head_commit"": ""187fbc396aaf161f621f2a4f85cef04c12465988"", ""repo_url"": ""https://github.com/apache/paimon/pull/5170"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5170"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-27T09:57:35.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/kafka_sync_database.html b/docs/layouts/shortcodes/generated/kafka_sync_database.html\nindex f5a37300c6f9..efba21b8d9dc 100644\n--- a/docs/layouts/shortcodes/generated/kafka_sync_database.html\n+++ b/docs/layouts/shortcodes/generated/kafka_sync_database.html\n@@ -90,6 +90,11 @@\n             </ul>\n         </td>\n     </tr>\n+    <tr>\n+        <td><h5>--eager_init</h5></td>\n+        <td>It is default false. If true, all relevant tables commiter will be initialized eagerly, which means those tables could be forced to create snapshot.</td>\n+    </tr>\n+    <tr>\n     <tr>\n         <td><h5>--partition_keys</h5></td>\n         <td>The partition keys for Paimon table. If there are multiple partition keys, connect them with comma, for example \""dt,hh,mm\"".\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\nindex d876fe484b50..267d682ca698 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.flink.action.Action;\n import org.apache.paimon.flink.action.MultiTablesSinkMode;\n+import org.apache.paimon.flink.sink.TableFilter;\n import org.apache.paimon.flink.sink.cdc.EventParser;\n import org.apache.paimon.flink.sink.cdc.FlinkCdcSyncDatabaseSinkBuilder;\n import org.apache.paimon.flink.sink.cdc.NewTableSchemaBuilder;\n@@ -48,6 +49,7 @@\n /** Base {@link Action} for synchronizing into one Paimon database. */\n public abstract class SyncDatabaseActionBase extends SynchronizationActionBase {\n \n+    protected boolean eagerInit = false;\n     protected boolean mergeShards = true;\n     protected MultiTablesSinkMode mode = COMBINED;\n     protected String tablePrefix = \""\"";\n@@ -81,6 +83,11 @@ public SyncDatabaseActionBase mergeShards(boolean mergeShards) {\n         return this;\n     }\n \n+    public SyncDatabaseActionBase eagerInit(boolean eagerInit) {\n+        this.eagerInit = eagerInit;\n+        return this;\n+    }\n+\n     public SyncDatabaseActionBase withMode(MultiTablesSinkMode mode) {\n         this.mode = mode;\n         return this;\n@@ -227,6 +234,13 @@ protected EventParser.Factory<RichCdcMultiplexRecord> buildEventParserFactory()\n     protected void buildSink(\n             DataStream<RichCdcMultiplexRecord> input,\n             EventParser.Factory<RichCdcMultiplexRecord> parserFactory) {\n+\n+        List<String> whiteList = new ArrayList<>(tableMapping.values());\n+        List<String> prefixList = new ArrayList<>(dbPrefix.values());\n+        prefixList.add(tablePrefix);\n+        List<String> suffixList = new ArrayList<>(dbSuffix.values());\n+        suffixList.add(tableSuffix);\n+\n         new FlinkCdcSyncDatabaseSinkBuilder<RichCdcMultiplexRecord>()\n                 .withInput(input)\n                 .withParserFactory(parserFactory)\n@@ -236,6 +250,15 @@ protected void buildSink(\n                 .withTables(tables)\n                 .withMode(mode)\n                 .withTableOptions(tableConfig)\n+                .withEagerInit(eagerInit)\n+                .withTableFilter(\n+                        new TableFilter(\n+                                database,\n+                                whiteList,\n+                                prefixList,\n+                                suffixList,\n+                                includingTables,\n+                                excludingTables))\n                 .build();\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\nindex a6c1eb2373f2..a825634a0925 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n@@ -31,6 +31,7 @@\n import org.apache.paimon.flink.sink.StoreMultiCommitter;\n import org.apache.paimon.flink.sink.StoreSinkWrite;\n import org.apache.paimon.flink.sink.StoreSinkWriteImpl;\n+import org.apache.paimon.flink.sink.TableFilter;\n import org.apache.paimon.flink.sink.WrappedManifestCommittableSerializer;\n import org.apache.paimon.manifest.WrappedManifestCommittable;\n import org.apache.paimon.options.MemorySize;\n@@ -46,6 +47,7 @@\n import javax.annotation.Nullable;\n \n import java.io.Serializable;\n+import java.util.Collections;\n \n import static org.apache.paimon.flink.sink.FlinkSink.assertStreamingConfiguration;\n import static org.apache.paimon.flink.sink.FlinkSink.configureGlobalCommitter;\n@@ -65,16 +67,22 @@ public class FlinkCdcMultiTableSink implements Serializable {\n     private final double commitCpuCores;\n     @Nullable private final MemorySize commitHeapMemory;\n     private final String commitUser;\n+    private boolean eagerInit = false;\n+    private TableFilter tableFilter;\n \n     public FlinkCdcMultiTableSink(\n             CatalogLoader catalogLoader,\n             double commitCpuCores,\n             @Nullable MemorySize commitHeapMemory,\n-            String commitUser) {\n+            String commitUser,\n+            boolean eagerInit,\n+            TableFilter tableFilter) {\n         this.catalogLoader = catalogLoader;\n         this.commitCpuCores = commitCpuCores;\n         this.commitHeapMemory = commitHeapMemory;\n         this.commitUser = commitUser;\n+        this.eagerInit = eagerInit;\n+        this.tableFilter = tableFilter;\n     }\n \n     private StoreSinkWrite.WithWriteBufferProvider createWriteProvider() {\n@@ -128,7 +136,7 @@ public DataStreamSink<?> sinkFrom(\n                                 true,\n                                 false,\n                                 commitUser,\n-                                createCommitterFactory(),\n+                                createCommitterFactory(tableFilter),\n                                 createCommittableStateManager()));\n         forwardParallelism(committed, input);\n         configureGlobalCommitter(committed, commitCpuCores, commitHeapMemory);\n@@ -144,12 +152,20 @@ public DataStreamSink<?> sinkFrom(\n \n     // Table committers are dynamically created at runtime\n     protected Committer.Factory<MultiTableCommittable, WrappedManifestCommittable>\n-            createCommitterFactory() {\n+            createCommitterFactory(TableFilter tableFilter) {\n+\n         // If checkpoint is enabled for streaming job, we have to\n         // commit new files list even if they're empty.\n         // Otherwise we can't tell if the commit is successful after\n         // a restart.\n-        return context -> new StoreMultiCommitter(catalogLoader, context);\n+        return context ->\n+                new StoreMultiCommitter(\n+                        catalogLoader,\n+                        context,\n+                        false,\n+                        Collections.emptyMap(),\n+                        eagerInit,\n+                        tableFilter);\n     }\n \n     protected CommittableStateManager<WrappedManifestCommittable> createCommittableStateManager() {\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\nindex 1d25929716e4..b15a17a679b2 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n@@ -24,6 +24,7 @@\n import org.apache.paimon.flink.action.MultiTablesSinkMode;\n import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.flink.sink.FlinkWriteSink;\n+import org.apache.paimon.flink.sink.TableFilter;\n import org.apache.paimon.flink.utils.SingleOutputStreamOperatorUtils;\n import org.apache.paimon.options.MemorySize;\n import org.apache.paimon.options.Options;\n@@ -79,8 +80,10 @@ public class FlinkCdcSyncDatabaseSinkBuilder<T> {\n \n     // database to sync, currently only support single database\n     private String database;\n+    private boolean eagerInit;\n     private MultiTablesSinkMode mode;\n     private String commitUser;\n+    private TableFilter tableFilter;\n \n     public FlinkCdcSyncDatabaseSinkBuilder<T> withInput(DataStream<T> input) {\n         this.input = input;\n@@ -125,6 +128,16 @@ public FlinkCdcSyncDatabaseSinkBuilder<T> withMode(MultiTablesSinkMode mode) {\n         return this;\n     }\n \n+    public FlinkCdcSyncDatabaseSinkBuilder<T> withEagerInit(boolean eagerInit) {\n+        this.eagerInit = eagerInit;\n+        return this;\n+    }\n+\n+    public FlinkCdcSyncDatabaseSinkBuilder<T> withTableFilter(TableFilter tableFilter) {\n+        this.tableFilter = tableFilter;\n+        return this;\n+    }\n+\n     public FlinkCdcSyncDatabaseSinkBuilder<T> withTypeMapping(TypeMapping typeMapping) {\n         this.typeMapping = typeMapping;\n         return this;\n@@ -176,7 +189,12 @@ private void buildCombinedCdcSink() {\n \n         FlinkCdcMultiTableSink sink =\n                 new FlinkCdcMultiTableSink(\n-                        catalogLoader, committerCpu, committerMemory, commitUser);\n+                        catalogLoader,\n+                        committerCpu,\n+                        committerMemory,\n+                        commitUser,\n+                        eagerInit,\n+                        tableFilter);\n         sink.sinkFrom(partitioned);\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\nindex 01acddb9ad99..67b2b6bd4627 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\n@@ -57,6 +57,8 @@ public class StoreMultiCommitter\n     private final boolean ignoreEmptyCommit;\n     private final Map<String, String> dynamicOptions;\n \n+    private final TableFilter tableFilter;\n+\n     public StoreMultiCommitter(CatalogLoader catalogLoader, Context context) {\n         this(catalogLoader, context, false, Collections.emptyMap());\n     }\n@@ -66,11 +68,28 @@ public StoreMultiCommitter(\n             Context context,\n             boolean ignoreEmptyCommit,\n             Map<String, String> dynamicOptions) {\n+        this(catalogLoader, context, ignoreEmptyCommit, dynamicOptions, false, null);\n+    }\n+\n+    public StoreMultiCommitter(\n+            CatalogLoader catalogLoader,\n+            Context context,\n+            boolean ignoreEmptyCommit,\n+            Map<String, String> dynamicOptions,\n+            boolean eagerInit,\n+            TableFilter tableFilter) {\n         this.catalog = catalogLoader.load();\n         this.context = context;\n         this.ignoreEmptyCommit = ignoreEmptyCommit;\n         this.dynamicOptions = dynamicOptions;\n         this.tableCommitters = new HashMap<>();\n+\n+        this.tableFilter = tableFilter;\n+\n+        if (eagerInit) {\n+            List<Identifier> tableIds = filterTables();\n+            tableIds.stream().forEach(this::getStoreCommitter);\n+        }\n     }\n \n     @Override\n@@ -218,4 +237,19 @@ public void close() throws Exception {\n             catalog.close();\n         }\n     }\n+\n+    private List<Identifier> filterTables() {\n+        // Get all tables in the catalog\n+        List<String> allTables = null;\n+        try {\n+            allTables = catalog.listTables(this.tableFilter.getDbName());\n+        } catch (Catalog.DatabaseNotExistException e) {\n+            allTables = Collections.emptyList();\n+        }\n+\n+        List<String> tblList = tableFilter.filterTables(allTables);\n+        return tblList.stream()\n+                .map(t -> Identifier.create(tableFilter.getDbName(), t))\n+                .collect(Collectors.toList());\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableFilter.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableFilter.java\nnew file mode 100644\nindex 000000000000..cfe04957924e\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableFilter.java\n@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/** TableFilter is used to filter tables according to whitelist and prefix/suffix patterns. */\n+public class TableFilter implements java.io.Serializable {\n+    private String dbName;\n+    private List<String> tableWhitelist;\n+    private List<String> tablePrefixes;\n+    private List<String> tableSuffixes;\n+    private String tblIncludingPattern = \"".*\"";\n+    private String tblExcludingPattern = \""\"";\n+\n+    public TableFilter(\n+            String dbName,\n+            List<String> tableWhitelist,\n+            List<String> tablePrefixes,\n+            List<String> tableSuffixes,\n+            String tblIncludingPattern,\n+            String tblExcludingPattern) {\n+        this.dbName = dbName;\n+        this.tableWhitelist = tableWhitelist;\n+        this.tablePrefixes = tablePrefixes;\n+        this.tableSuffixes = tableSuffixes;\n+        this.tblIncludingPattern = tblIncludingPattern;\n+        this.tblExcludingPattern = tblExcludingPattern;\n+    }\n+\n+    public String getDbName() {\n+        return dbName;\n+    }\n+\n+    public void setDbName(String dbName) {\n+        this.dbName = dbName;\n+    }\n+\n+    public List<String> getTableWhitelist() {\n+        return tableWhitelist;\n+    }\n+\n+    public void setTableWhitelist(List<String> tableWhitelist) {\n+        this.tableWhitelist = tableWhitelist;\n+    }\n+\n+    public List<String> getTablePrefixes() {\n+        return tablePrefixes;\n+    }\n+\n+    public void setTablePrefixes(List<String> tablePrefixes) {\n+        this.tablePrefixes = tablePrefixes;\n+    }\n+\n+    public List<String> getTableSuffixes() {\n+        return tableSuffixes;\n+    }\n+\n+    public void setTableSuffixes(List<String> tableSuffixes) {\n+        this.tableSuffixes = tableSuffixes;\n+    }\n+\n+    public String getTblIncludingPattern() {\n+        return tblIncludingPattern;\n+    }\n+\n+    public void setTblIncludingPattern(String tblIncludingPattern) {\n+        this.tblIncludingPattern = tblIncludingPattern;\n+    }\n+\n+    public String getTblExcludingPattern() {\n+        return tblExcludingPattern;\n+    }\n+\n+    public void setTblExcludingPattern(String tblExcludingPattern) {\n+        this.tblExcludingPattern = tblExcludingPattern;\n+    }\n+\n+    public List<String> filterTables(List<String> allTables) {\n+        List<String> inPatternList = Arrays.asList(tblIncludingPattern.split(\""\\\\|\""));\n+        List<String> exPatternList =\n+                (tblExcludingPattern == null || tblExcludingPattern.isEmpty())\n+                        ? Collections.emptyList()\n+                        : Arrays.asList(tblExcludingPattern.split(\""\\\\|\""));\n+        String inPattern =\n+                inPatternList.stream()\n+                        .flatMap(\n+                                p ->\n+                                        tablePrefixes.isEmpty()\n+                                                ? Stream.of(p)\n+                                                : tablePrefixes.stream().map(prefix -> prefix + p))\n+                        .flatMap(\n+                                p ->\n+                                        tableSuffixes.isEmpty()\n+                                                ? Stream.of(p)\n+                                                : tableSuffixes.stream().map(suffix -> p + suffix))\n+                        .collect(Collectors.joining(\""|\""));\n+\n+        String exPattern =\n+                exPatternList.isEmpty()\n+                        ? \""\""\n+                        : exPatternList.stream()\n+                                .flatMap(\n+                                        p ->\n+                                                tablePrefixes.isEmpty()\n+                                                        ? Stream.of(p)\n+                                                        : tablePrefixes.stream()\n+                                                                .map(prefix -> prefix + p))\n+                                .flatMap(\n+                                        p ->\n+                                                tableSuffixes.isEmpty()\n+                                                        ? Stream.of(p)\n+                                                        : tableSuffixes.stream()\n+                                                                .map(suffix -> p + suffix))\n+                                .collect(Collectors.joining(\""|\""));\n+\n+        return allTables.stream()\n+                .filter(t -> exPattern.isEmpty() || !t.matches(exPattern))\n+                .filter(t -> tableWhitelist.contains(t) || t.matches(inPattern))\n+                .collect(Collectors.toList());\n+    }\n+}\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\nindex ab81e37c7d04..2371fbfb9dfe 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n@@ -52,7 +52,9 @@ public void testTransformationParallelism() {\n                         () -> FlinkCatalogFactory.createPaimonCatalog(new Options()),\n                         FlinkConnectorOptions.SINK_COMMITTER_CPU.defaultValue(),\n                         null,\n-                        UUID.randomUUID().toString());\n+                        UUID.randomUUID().toString(),\n+                        false,\n+                        null);\n         DataStreamSink<?> dataStreamSink = sink.sinkFrom(input);\n \n         // check the transformation graph\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\nindex 1958d15a3fa5..f6cea7f5fb25 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\n@@ -82,6 +82,7 @@ class StoreMultiCommitterTest {\n     private Path warehouse;\n     private CatalogLoader catalogLoader;\n     private Catalog catalog;\n+    private String databaseName;\n     private Identifier firstTable;\n     private Identifier secondTable;\n     private Path firstTablePath;\n@@ -100,7 +101,7 @@ private final void createTestTables(Catalog catalog, Tuple2<Identifier, Schema>.\n     public void beforeEach() throws Exception {\n         initialCommitUser = UUID.randomUUID().toString();\n         warehouse = new Path(TraceableFileIO.SCHEME + \""://\"" + tempDir.toString());\n-        String databaseName = \""test_db\"";\n+        databaseName = \""test_db\"";\n         firstTable = Identifier.create(databaseName, \""test_table1\"");\n         secondTable = Identifier.create(databaseName, \""test_table2\"");\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/TableFilterTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/TableFilterTest.java\nnew file mode 100644\nindex 000000000000..10d6834156cc\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/TableFilterTest.java\n@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/** Test for TableFilter. */\n+public class TableFilterTest {\n+\n+    @Test\n+    public void testFilterTables() {\n+        List<String> tableWhitelist = Collections.singletonList(\""white_listed_table\"");\n+        List<String> tablePrefixes = Collections.singletonList(\""test_\"");\n+        List<String> tableSuffixes = Collections.EMPTY_LIST;\n+        String tblIncludingPattern = \"".*\"";\n+        String tblExcludingPattern = \""\"";\n+\n+        // prefix, no suffix, no whitelisting,\n+        List<String> allTables =\n+                Arrays.asList(\n+                        \""test_table1\"",\n+                        \""test_table2\"",\n+                        \""test_table1_suffix_in\"",\n+                        \""test_table2_suffix_in\"",\n+                        \""test_table1_suffix_ex\"",\n+                        \""test_table2_suffix_ex\"",\n+                        \""test_excluded1\"",\n+                        \""test_excluded2\"",\n+                        \""white_listed_table\"",\n+                        \""other_table1\"",\n+                        \""other_table2\"");\n+\n+        TableFilter tableFilter =\n+                new TableFilter(\n+                        \""\"",\n+                        tableWhitelist,\n+                        tablePrefixes,\n+                        tableSuffixes,\n+                        tblIncludingPattern,\n+                        tblExcludingPattern);\n+\n+        List<String> filteredTables = tableFilter.filterTables(allTables);\n+        assertThat(filteredTables.size()).isEqualTo(9);\n+        assertThat(filteredTables)\n+                .contains(\n+                        \""test_table1\"",\n+                        \""test_table2\"",\n+                        \""test_table1_suffix_in\"",\n+                        \""test_table2_suffix_in\"",\n+                        \""test_table1_suffix_ex\"",\n+                        \""test_table2_suffix_ex\"",\n+                        \""test_excluded1\"",\n+                        \""test_excluded2\"",\n+                        \""white_listed_table\"");\n+\n+        // exclude pattern\n+        tblExcludingPattern = \""excluded.*\"";\n+        tableFilter.setTblExcludingPattern(tblExcludingPattern);\n+        filteredTables = tableFilter.filterTables(allTables);\n+        assertThat(filteredTables.size()).isEqualTo(7);\n+        assertThat(filteredTables)\n+                .contains(\n+                        \""test_table1\"",\n+                        \""test_table2\"",\n+                        \""test_table1_suffix_in\"",\n+                        \""test_table2_suffix_in\"",\n+                        \""test_table1_suffix_ex\"",\n+                        \""test_table2_suffix_ex\"",\n+                        \""white_listed_table\"");\n+\n+        // suffix\n+        tableSuffixes = Collections.singletonList(\""_suffix_in\"");\n+        tableFilter.setTableSuffixes(tableSuffixes);\n+        filteredTables = tableFilter.filterTables(allTables);\n+        assertThat(filteredTables.size()).isEqualTo(3);\n+        assertThat(filteredTables)\n+                .contains(\""test_table1_suffix_in\"", \""test_table2_suffix_in\"", \""white_listed_table\"");\n+\n+        // No tables\n+        filteredTables = tableFilter.filterTables(Collections.emptyList());\n+        assertThat(filteredTables.size()).isEqualTo(0);\n+    }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5138"", ""pr_id"": 5138, ""issue_id"": 5068, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support special audition computed columns, especially for cdc sync database\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nWhen doing cdc sync database with flink action, that would be helpful if the ingest times are recorded. \nIngest times means when the record are created(+I), and when the record are updated (mainly+U, -D, but +I also does). \n\n\n\n### Solution\n\nTo do so, two special computed columns could be empolyeed. One could be `etl_create_time`, only set when rowkind is +I. Another could be `etl_update_time`, set whenever new records come in. \n\nAnd to achieve that, a new special expression should be introduced too.  The expression should compute the time according to rowkind.\n\n* If it's used for `etl_create_time`, only returns the current timestamp when rowkind is '+I', otherwise null.\n* If it's used for `etl_update_time` returns the current timestamp whenever called. \n\n### Anything else?\n\nAnother solution is recording the ingest times in audit_log. (refer to https://github.com/apache/paimon/issues/5022)\nBut that would affect paimon-core, and break some assumption.\n\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 197, ""test_files_count"": 7, ""non_test_files_count"": 3, ""pr_changed_files"": [""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/Expression.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionFactoryBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-1.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-2.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-3.txt"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-1.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-2.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-3.txt"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java""], ""base_commit"": ""b5ee79a70301666f54485f97951d8db94d258eb3"", ""head_commit"": ""0a44a0de3b1c6cb8f52eafaa864cba7803384705"", ""repo_url"": ""https://github.com/apache/paimon/pull/5138"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5138"", ""dockerfile"": """", ""pr_merged_at"": ""2025-03-15T11:22:14.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/Expression.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/Expression.java\nindex 3290ec18291f..50bd57da3687 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/Expression.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/Expression.java\n@@ -141,7 +141,8 @@ enum ExpressionFunction {\n                             referencedField.fieldType(),\n                             referencedField.literals());\n                 }),\n-        CAST((typeMapping, caseSensitive, args) -> cast(args));\n+        CAST((typeMapping, caseSensitive, args) -> cast(args)),\n+        NOW((typeMapping, caseSensitive, args) -> new NowExpression());\n \n         public final ExpressionCreator creator;\n \n@@ -608,4 +609,22 @@ public String eval(String input) {\n             return value;\n         }\n     }\n+\n+    /** Get current timestamp. */\n+    final class NowExpression implements Expression {\n+        @Override\n+        public String fieldReference() {\n+            return null;\n+        }\n+\n+        @Override\n+        public DataType outputType() {\n+            return DataTypes.TIMESTAMP(3);\n+        }\n+\n+        @Override\n+        public String eval(String input) {\n+            return DateTimeUtils.formatLocalDateTime(LocalDateTime.now(), 3);\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\nindex 267d682ca698..4ce4e7c250f7 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n@@ -45,6 +45,7 @@\n import java.util.regex.Pattern;\n \n import static org.apache.paimon.flink.action.MultiTablesSinkMode.COMBINED;\n+import static org.apache.paimon.flink.action.cdc.ComputedColumnUtils.buildComputedColumns;\n \n /** Base {@link Action} for synchronizing into one Paimon database. */\n public abstract class SyncDatabaseActionBase extends SynchronizationActionBase {\n@@ -60,6 +61,7 @@ public abstract class SyncDatabaseActionBase extends SynchronizationActionBase {\n     protected String includingTables = \"".*\"";\n     protected List<String> partitionKeys = new ArrayList<>();\n     protected List<String> primaryKeys = new ArrayList<>();\n+    protected List<ComputedColumn> computedColumns = new ArrayList<>();\n     @Nullable protected String excludingTables;\n     protected String includingDbs = \"".*\"";\n     @Nullable protected String excludingDbs;\n@@ -172,10 +174,15 @@ public SyncDatabaseActionBase withPrimaryKeys(String... primaryKeys) {\n         return this;\n     }\n \n+    public SyncDatabaseActionBase withComputedColumnArgs(List<String> computedColumnArgs) {\n+        this.computedColumns = buildComputedColumns(computedColumnArgs, Collections.emptyList());\n+        return this;\n+    }\n+\n     @Override\n     protected FlatMapFunction<CdcSourceRecord, RichCdcMultiplexRecord> recordParse() {\n         return syncJobHandler.provideRecordParser(\n-                Collections.emptyList(), typeMapping, metadataConverters);\n+                this.computedColumns, typeMapping, metadataConverters);\n     }\n \n     public SyncDatabaseActionBase withPartitionKeyMultiple(\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionFactoryBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionFactoryBase.java\nindex 52bfb7271c8a..8d0b8b9cef31 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionFactoryBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionFactoryBase.java\n@@ -22,8 +22,10 @@\n import org.apache.paimon.flink.action.ActionFactory;\n import org.apache.paimon.flink.action.MultipleParameterToolAdapter;\n \n+import java.util.ArrayList;\n import java.util.Optional;\n \n+import static org.apache.paimon.flink.action.cdc.CdcActionCommonUtils.COMPUTED_COLUMN;\n import static org.apache.paimon.flink.action.cdc.CdcActionCommonUtils.EAGER_INIT;\n import static org.apache.paimon.flink.action.cdc.CdcActionCommonUtils.EXCLUDING_DBS;\n import static org.apache.paimon.flink.action.cdc.CdcActionCommonUtils.EXCLUDING_TABLES;\n@@ -79,5 +81,10 @@ protected void withParams(MultipleParameterToolAdapter params, T action) {\n             String[] options = params.get(TYPE_MAPPING).split(\"",\"");\n             action.withTypeMapping(TypeMapping.parse(options));\n         }\n+\n+        if (params.has(COMPUTED_COLUMN)) {\n+            action.withComputedColumnArgs(\n+                    new ArrayList<>(params.getMultiParameter(COMPUTED_COLUMN)));\n+        }\n     }\n }\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java\nindex 00a8b236173b..855623b1afab 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/CdcActionITCaseBase.java\n@@ -130,6 +130,16 @@ protected void assertTablePartitionKeys(Map<String, String> partitionKeyMultiple\n     protected void waitForResult(\n             List<String> expected, FileStoreTable table, RowType rowType, List<String> primaryKeys)\n             throws Exception {\n+        waitForResult(false, expected, table, rowType, primaryKeys);\n+    }\n+\n+    protected void waitForResult(\n+            boolean withRegx,\n+            List<String> expected,\n+            FileStoreTable table,\n+            RowType rowType,\n+            List<String> primaryKeys)\n+            throws Exception {\n         assertThat(table.schema().primaryKeys()).isEqualTo(primaryKeys);\n \n         // wait for table schema to become our expected schema\n@@ -165,7 +175,8 @@ protected void waitForResult(\n                             rowType);\n             List<String> sortedActual = new ArrayList<>(result);\n             Collections.sort(sortedActual);\n-            if (sortedExpected.equals(sortedActual)) {\n+            if (withRegx && isRegxMatchList(sortedActual, sortedExpected)\n+                    || sortedExpected.equals(sortedActual)) {\n                 break;\n             }\n             LOG.info(\""actual: \"" + sortedActual);\n@@ -174,6 +185,20 @@ protected void waitForResult(\n         }\n     }\n \n+    private boolean isRegxMatchList(List<String> actual, List<String> expected) {\n+        if (actual.size() != expected.size()) {\n+            return false;\n+        }\n+\n+        for (int i = 0; i < actual.size(); i++) {\n+            if (!actual.get(i).matches(expected.get(i))) {\n+                return false;\n+            }\n+        }\n+\n+        return true;\n+    }\n+\n     protected Map<String, String> getBasicTableConfig() {\n         Map<String, String> config = new HashMap<>();\n         ThreadLocalRandom random = ThreadLocalRandom.current();\n@@ -392,6 +417,7 @@ protected abstract class SyncDatabaseActionBuilder<T extends SynchronizationActi\n         private final List<String> partitionKeys = new ArrayList<>();\n         private final List<String> primaryKeys = new ArrayList<>();\n         private final List<String> metadataColumn = new ArrayList<>();\n+        private final List<String> computedColumnArgs = new ArrayList<>();\n         protected Map<String, String> partitionKeyMultiple = new HashMap<>();\n \n         public SyncDatabaseActionBuilder(Class<T> clazz, Map<String, String> sourceConfig) {\n@@ -464,6 +490,12 @@ public SyncDatabaseActionBuilder<T> withMetadataColumn(List<String> metadataColu\n             return this;\n         }\n \n+        public SyncDatabaseActionBuilder<T> withComputedColumnArgs(\n+                List<String> computedColumnArgs) {\n+            this.computedColumnArgs.addAll(computedColumnArgs);\n+            return this;\n+        }\n+\n         public SyncDatabaseActionBuilder<T> withPartitionKeyMultiple(\n                 Map<String, String> partitionKeyMultiple) {\n             if (partitionKeyMultiple != null) {\n@@ -500,6 +532,8 @@ public T build() {\n             args.addAll(listToArgs(\""--primary-keys\"", primaryKeys));\n             args.addAll(listToArgs(\""--metadata-column\"", metadataColumn));\n \n+            args.addAll(listToMultiArgs(\""--computed-column\"", computedColumnArgs));\n+\n             return createAction(clazz, args);\n         }\n     }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java\nindex f06268d700e5..6dea3ee54705 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/format/aliyun/AliyunJsonRecordParserTest.java\n@@ -18,7 +18,10 @@\n \n package org.apache.paimon.flink.action.cdc.format.aliyun;\n \n+import org.apache.paimon.data.BinaryString;\n import org.apache.paimon.flink.action.cdc.CdcSourceRecord;\n+import org.apache.paimon.flink.action.cdc.ComputedColumn;\n+import org.apache.paimon.flink.action.cdc.ComputedColumnUtils;\n import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.flink.action.cdc.kafka.KafkaActionITCaseBase;\n import org.apache.paimon.flink.action.cdc.watermark.MessageQueueCdcTimestampExtractor;\n@@ -26,6 +29,7 @@\n import org.apache.paimon.flink.sink.cdc.RichCdcMultiplexRecord;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.types.RowKind;\n+import org.apache.paimon.utils.BinaryStringUtils;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.JsonNode;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n@@ -43,6 +47,7 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n \n /** Test for AliyunJsonRecordParser. */\n public class AliyunJsonRecordParserTest extends KafkaActionITCaseBase {\n@@ -51,14 +56,20 @@ public class AliyunJsonRecordParserTest extends KafkaActionITCaseBase {\n     private static List<String> insertList = new ArrayList<>();\n     private static List<String> updateList = new ArrayList<>();\n     private static List<String> deleteList = new ArrayList<>();\n+    private static List<ComputedColumn> computedColumns = new ArrayList<>();\n \n     private static ObjectMapper objMapper = new ObjectMapper();\n \n+    String dateTimeRegex = \""\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\"";\n+\n     @Before\n     public void setup() {\n         String insertRes = \""kafka/aliyun/table/event/event-insert.txt\"";\n         String updateRes = \""kafka/aliyun/table/event/event-update-in-one.txt\"";\n         String deleteRes = \""kafka/aliyun/table/event/event-delete.txt\"";\n+\n+        String[] computedColumnArgs = {\""etl_create_time=now()\"", \""etl_update_time=now()\""};\n+\n         URL url;\n         try {\n             url = AliyunJsonRecordParserTest.class.getClassLoader().getResource(insertRes);\n@@ -76,6 +87,10 @@ public void setup() {\n                     .filter(this::isRecordLine)\n                     .forEach(e -> deleteList.add(e));\n \n+            computedColumns =\n+                    ComputedColumnUtils.buildComputedColumns(\n+                            Arrays.asList(computedColumnArgs), Collections.emptyList());\n+\n         } catch (Exception e) {\n             log.error(\""Fail to init aliyun-json cases\"", e);\n         }\n@@ -83,10 +98,11 @@ public void setup() {\n \n     @Test\n     public void extractInsertRecord() throws Exception {\n+\n         AliyunRecordParser parser =\n-                new AliyunRecordParser(TypeMapping.defaultMapping(), Collections.emptyList());\n+                new AliyunRecordParser(TypeMapping.defaultMapping(), computedColumns);\n         for (String json : insertList) {\n-            // \u5c06json\u89e3\u6790\u4e3aJsonNode\u5bf9\u8c61\n+\n             JsonNode rootNode = objMapper.readValue(json, JsonNode.class);\n             CdcSourceRecord cdcRecord = new CdcSourceRecord(rootNode);\n             Schema schema = parser.buildSchema(cdcRecord);\n@@ -106,15 +122,31 @@ public void extractInsertRecord() throws Exception {\n \n             MessageQueueCdcTimestampExtractor extractor = new MessageQueueCdcTimestampExtractor();\n             Assert.assertTrue(extractor.extractTimestamp(cdcRecord) > 0);\n+\n+            Map<String, String> data = records.get(0).toRichCdcRecord().toCdcRecord().data();\n+            String createTime = data.get(\""etl_create_time\"");\n+            String updateTime = data.get(\""etl_update_time\"");\n+\n+            // Mock the real timestamp string which retrieved from store and convert through paimon\n+            // Timestamp\n+            createTime =\n+                    BinaryStringUtils.toTimestamp(BinaryString.fromString(createTime), 6)\n+                            .toString();\n+            updateTime =\n+                    BinaryStringUtils.toTimestamp(BinaryString.fromString(updateTime), 6)\n+                            .toString();\n+\n+            Assert.assertTrue(createTime.matches(dateTimeRegex));\n+            Assert.assertTrue(updateTime.matches(dateTimeRegex));\n         }\n     }\n \n     @Test\n     public void extractUpdateRecord() throws Exception {\n         AliyunRecordParser parser =\n-                new AliyunRecordParser(TypeMapping.defaultMapping(), Collections.emptyList());\n+                new AliyunRecordParser(TypeMapping.defaultMapping(), computedColumns);\n         for (String json : updateList) {\n-            // \u5c06json\u89e3\u6790\u4e3aJsonNode\u5bf9\u8c61\n+\n             JsonNode jsonNode = objMapper.readValue(json, JsonNode.class);\n             CdcSourceRecord cdcRecord = new CdcSourceRecord(jsonNode);\n             Schema schema = parser.buildSchema(cdcRecord);\n@@ -134,15 +166,26 @@ public void extractUpdateRecord() throws Exception {\n \n             MessageQueueCdcTimestampExtractor extractor = new MessageQueueCdcTimestampExtractor();\n             Assert.assertTrue(extractor.extractTimestamp(cdcRecord) > 0);\n+\n+            Map<String, String> data = records.get(0).toRichCdcRecord().toCdcRecord().data();\n+            String createTime = data.get(\""etl_create_time\"");\n+            String updateTime = data.get(\""etl_update_time\"");\n+            Assert.assertNotNull(createTime);\n+\n+            updateTime =\n+                    BinaryStringUtils.toTimestamp(BinaryString.fromString(updateTime), 6)\n+                            .toString();\n+\n+            Assert.assertTrue(updateTime.matches(dateTimeRegex));\n         }\n     }\n \n     @Test\n     public void extractDeleteRecord() throws Exception {\n         AliyunRecordParser parser =\n-                new AliyunRecordParser(TypeMapping.defaultMapping(), Collections.emptyList());\n+                new AliyunRecordParser(TypeMapping.defaultMapping(), computedColumns);\n         for (String json : deleteList) {\n-            // \u5c06json\u89e3\u6790\u4e3aJsonNode\u5bf9\u8c61\n+\n             JsonNode jsonNode = objMapper.readValue(json, JsonNode.class);\n             CdcSourceRecord cdcRecord = new CdcSourceRecord(jsonNode);\n             Schema schema = parser.buildSchema(cdcRecord);\n@@ -162,6 +205,17 @@ public void extractDeleteRecord() throws Exception {\n \n             MessageQueueCdcTimestampExtractor extractor = new MessageQueueCdcTimestampExtractor();\n             Assert.assertTrue(extractor.extractTimestamp(cdcRecord) > 0);\n+\n+            Map<String, String> data = records.get(0).toRichCdcRecord().toCdcRecord().data();\n+            String createTime = data.get(\""etl_create_time\"");\n+            String updateTime = data.get(\""etl_update_time\"");\n+            Assert.assertNotNull(createTime);\n+\n+            updateTime =\n+                    BinaryStringUtils.toTimestamp(BinaryString.fromString(updateTime), 6)\n+                            .toString();\n+\n+            Assert.assertTrue(updateTime.matches(dateTimeRegex));\n         }\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java\nindex 6e37c589ac92..60aa70c34bf3 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncDatabaseActionITCase.java\n@@ -18,6 +18,8 @@\n \n package org.apache.paimon.flink.action.cdc.kafka;\n \n+import org.apache.paimon.data.InternalRow;\n+import org.apache.paimon.data.Timestamp;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.types.DataType;\n@@ -643,6 +645,98 @@ public void testSpecifyKeys() throws Exception {\n                 Collections.singletonList(\""k\""));\n     }\n \n+    @Test\n+    @Timeout(120)\n+    public void testExpressionNow() throws Exception {\n+        final String topic = \""expression-now\"";\n+        createTestTopic(topic, 1, 1);\n+        writeRecordsToKafka(topic, \""kafka/canal/database/audit-time/canal-data-1.txt\"");\n+\n+        Map<String, String> kafkaConfig = getBasicKafkaConfig();\n+        kafkaConfig.put(VALUE_FORMAT.key(), \""canal-json\"");\n+        kafkaConfig.put(TOPIC.key(), topic);\n+\n+        KafkaSyncDatabaseAction action =\n+                syncDatabaseActionBuilder(kafkaConfig)\n+                        .withTableConfig(getBasicTableConfig())\n+                        .withPrimaryKeys(\""k\"")\n+                        .withComputedColumnArgs(\n+                                Arrays.asList(\""etl_create_time=now()\"", \""etl_update_time=now()\""))\n+                        .build();\n+        runActionWithDefaultEnv(action);\n+\n+        waitingTables(\""t1\"");\n+\n+        FileStoreTable table1 = getFileStoreTable(\""t1\"");\n+        assertThat(table1.primaryKeys()).containsExactly(\""k\"");\n+\n+        RowType rowType1 =\n+                RowType.of(\n+                        new DataType[] {\n+                            DataTypes.INT().notNull(),\n+                            DataTypes.VARCHAR(10),\n+                            DataTypes.TIMESTAMP(3),\n+                            DataTypes.TIMESTAMP(3)\n+                        },\n+                        new String[] {\""k\"", \""v1\"", \""etl_create_time\"", \""etl_update_time\""});\n+\n+        // INSERT\n+        waitForResult(\n+                true,\n+                Collections.singletonList(\n+                        \""\\\\+I\\\\[1, A, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\\\\]\""),\n+                table1,\n+                rowType1,\n+                Collections.singletonList(\""k\""));\n+\n+        List<InternalRow> data = getData(\""t1\"");\n+        Timestamp createTime1 = data.get(0).getTimestamp(2, 3);\n+        Timestamp updateTime1 = data.get(0).getTimestamp(3, 3);\n+\n+        assertThat(createTime1.toLocalDateTime()).isBefore(Timestamp.now().toLocalDateTime());\n+        assertThat(updateTime1.toLocalDateTime()).isBefore(Timestamp.now().toLocalDateTime());\n+\n+        Thread.sleep(1000);\n+\n+        // UPDATE1\n+        writeRecordsToKafka(topic, \""kafka/canal/database/audit-time/canal-data-2.txt\"");\n+        waitForResult(\n+                true,\n+                Collections.singletonList(\n+                        \""\\\\+I\\\\[1, B, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\\\\]\""),\n+                table1,\n+                rowType1,\n+                Collections.singletonList(\""k\""));\n+\n+        data = getData(\""t1\"");\n+        Timestamp createTime2 = data.get(0).getTimestamp(2, 3);\n+        Timestamp updateTime2 = data.get(0).getTimestamp(3, 3);\n+\n+        assertThat(createTime2.toLocalDateTime()).isAfter(createTime1.toLocalDateTime());\n+        assertThat(updateTime2.toLocalDateTime()).isAfter(updateTime1.toLocalDateTime());\n+        assertThat(updateTime2.toLocalDateTime()).isBefore(Timestamp.now().toLocalDateTime());\n+\n+        Thread.sleep(1000);\n+\n+        // UPDATE2\n+        writeRecordsToKafka(topic, \""kafka/canal/database/audit-time/canal-data-3.txt\"");\n+        waitForResult(\n+                true,\n+                Collections.singletonList(\n+                        \""\\\\+I\\\\[1, C, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}, \\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\\\\]\""),\n+                table1,\n+                rowType1,\n+                Collections.singletonList(\""k\""));\n+\n+        data = getData(\""t1\"");\n+        Timestamp createTime3 = data.get(0).getTimestamp(2, 3);\n+        Timestamp updateTime3 = data.get(0).getTimestamp(3, 3);\n+\n+        assertThat(createTime3.toLocalDateTime()).isAfter(createTime1.toLocalDateTime());\n+        assertThat(updateTime3.toLocalDateTime()).isAfter(updateTime2.toLocalDateTime());\n+        assertThat(updateTime3.toLocalDateTime()).isBefore(Timestamp.now().toLocalDateTime());\n+    }\n+\n     @Test\n     @Timeout(60)\n     public void testMultipleTablePartitionKeys() throws Exception {\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-1.txt b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-1.txt\nnew file mode 100644\nindex 000000000000..92935ccb200a\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-1.txt\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+{\""data\"":[{\""k\"":\""1\"",\""v1\"":\""A\""}],\""database\"":\""test_audit_time\"",\""es\"":1684770072000,\""id\"":81,\""isDdl\"":false,\""mysqlType\"":{\""k\"":\""INT\"",\""v1\"":\""VARCHAR(10)\""},\""old\"":[],\""pkNames\"":[\""k\""],\""sql\"":\""\"",\""sqlType\"":{\""k\"":4,\""v1\"":12},\""table\"":\""t1\"",\""ts\"":1684770072286,\""type\"":\""INSERT\""}\n\\ No newline at end of file\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-2.txt b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-2.txt\nnew file mode 100644\nindex 000000000000..b7b9d9b6359f\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-2.txt\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+{\""data\"":[{\""k\"":\""1\"",\""v1\"":\""B\""}],\""database\"":\""test_audit_time\"",\""es\"":1684770072000,\""id\"":81,\""isDdl\"":false,\""mysqlType\"":{\""k\"":\""INT\"",\""v1\"":\""VARCHAR(10)\""},\""old\"":[{\""k\"":\""1\"",\""v1\"":\""A\""}],\""pkNames\"":[\""k\""],\""sql\"":\""\"",\""sqlType\"":{\""k\"":4,\""v1\"":12},\""table\"":\""t1\"",\""ts\"":1684770072286,\""type\"":\""UPDATE\""}\n\\ No newline at end of file\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-3.txt b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-3.txt\nnew file mode 100644\nindex 000000000000..acd51960e472\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/database/audit-time/canal-data-3.txt\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+{\""data\"":[{\""k\"":\""1\"",\""v1\"":\""C\""}],\""database\"":\""test_audit_time\"",\""es\"":1684770072000,\""id\"":81,\""isDdl\"":false,\""mysqlType\"":{\""k\"":\""INT\"",\""v1\"":\""VARCHAR(10)\""},\""old\"":[{\""k\"":\""1\"",\""v1\"":\""B\""}],\""pkNames\"":[\""k\""],\""sql\"":\""\"",\""sqlType\"":{\""k\"":4,\""v1\"":12},\""table\"":\""t1\"",\""ts\"":1684770072286,\""type\"":\""UPDATE\""}\n\\ No newline at end of file\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java\nindex f8ea18950035..6be78e041a18 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/action/ActionITCaseBase.java\n@@ -32,8 +32,10 @@\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.sink.StreamTableCommit;\n import org.apache.paimon.table.sink.StreamTableWrite;\n+import org.apache.paimon.table.source.ReadBuilder;\n import org.apache.paimon.table.source.Split;\n import org.apache.paimon.table.source.TableRead;\n+import org.apache.paimon.table.source.TableScan;\n import org.apache.paimon.types.RowType;\n \n import org.apache.flink.table.api.TableEnvironment;\n@@ -45,6 +47,7 @@\n \n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -142,6 +145,22 @@ protected void writeData(GenericRow... data) throws Exception {\n         incrementalIdentifier++;\n     }\n \n+    protected List<InternalRow> getData(String tableName) throws Exception {\n+        List<InternalRow> result = new ArrayList<>();\n+\n+        FileStoreTable table = this.getFileStoreTable(tableName);\n+\n+        ReadBuilder readBuilder = table.newReadBuilder();\n+        TableScan.Plan plan = readBuilder.newScan().plan();\n+        List<Split> splits = plan == null ? Collections.emptyList() : plan.splits();\n+        TableRead read = readBuilder.newRead();\n+        try (RecordReader<InternalRow> recordReader = read.createReader(splits)) {\n+            recordReader.forEachRemaining(result::add);\n+        }\n+\n+        return result;\n+    }\n+\n     protected List<String> getResult(TableRead read, List<Split> splits, RowType rowType)\n             throws Exception {\n         try (RecordReader<InternalRow> recordReader = read.createReader(splits)) {\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5124"", ""pr_id"": 5124, ""issue_id"": 4783, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Ignore decimal type evolution in special case\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nWe use ticdc to collect data from tidb and send to kafka in canal-json format, then sink the data with paimon-flink-action.\r\n\r\nBut actually only type name are collected and sent to kafka for the metadata part. For example, the decimal fields, only the name 'decimal' provided, without the precision and scale. \r\n\r\nIn paimon-flink-action, the missing precision and scale will be set to maximum, which is decimal(38,16). That is too large and not expected.\r\nIn this case, the original format is prefer, which means not changing the deciaml type during cdc. \r\n\r\n### Solution\r\n\r\nAdd a type-mappint option for kafka cdc, like \r\n```shell\r\n# to ignore only decimal change\r\n--type_mapping decimal-no-change\r\n# to ingore any type change\r\n--type_mapping no_change\r\n``` \r\nAnd ignore the type change in $$RichEventParser::parseSchemaChange$$\r\n\r\n### Anything else?\r\n\r\nEnlarging type also occurs to char/varchar, which is acceptable.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 187, ""test_files_count"": 5, ""non_test_files_count"": 10, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/kafka_sync_database.html"", ""docs/layouts/shortcodes/generated/kafka_sync_table.html"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncTableActionBase.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/TypeMapping.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcSinkBuilder.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-3.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-4.txt""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-3.txt"", ""paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-4.txt""], ""base_commit"": ""680608bb48dd2cb617166a1069f6706f250b3c25"", ""head_commit"": ""111e94d830a033cf0089a9179781d14f8931984c"", ""repo_url"": ""https://github.com/apache/paimon/pull/5124"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5124"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-25T02:57:19.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/kafka_sync_database.html b/docs/layouts/shortcodes/generated/kafka_sync_database.html\nindex 923c1fcc4361..f5a37300c6f9 100644\n--- a/docs/layouts/shortcodes/generated/kafka_sync_database.html\n+++ b/docs/layouts/shortcodes/generated/kafka_sync_database.html\n@@ -86,6 +86,7 @@\n                 <li>\""char-to-string\"": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.</li>\n                 <li>\""longtext-to-bytes\"": maps MySQL LONGTEXT types to BYTES.</li>\n                 <li>\""bigint-unsigned-to-bigint\"": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.</li>\n+                <li>\""decimal-no-change\"": Ignore decimal type change.</li>\n             </ul>\n         </td>\n     </tr>\n\ndiff --git a/docs/layouts/shortcodes/generated/kafka_sync_table.html b/docs/layouts/shortcodes/generated/kafka_sync_table.html\nindex 122e5eb12177..2345a4d84ab2 100644\n--- a/docs/layouts/shortcodes/generated/kafka_sync_table.html\n+++ b/docs/layouts/shortcodes/generated/kafka_sync_table.html\n@@ -58,6 +58,7 @@\n                 <li>\""char-to-string\"": maps MySQL CHAR(length)/VARCHAR(length) types to STRING.</li>\n                 <li>\""longtext-to-bytes\"": maps MySQL LONGTEXT types to BYTES.</li>\n                 <li>\""bigint-unsigned-to-bigint\"": maps MySQL BIGINT UNSIGNED, BIGINT UNSIGNED ZEROFILL, SERIAL to BIGINT. You should ensure overflow won't occur when using this option.</li>\n+                <li>\""decimal-no-change\"": Ignore decimal type change.</li>\n             </ul>\n         </td>\n     </tr>\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\nindex d6d85e59bba8..d876fe484b50 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncDatabaseActionBase.java\n@@ -231,6 +231,7 @@ protected void buildSink(\n                 .withInput(input)\n                 .withParserFactory(parserFactory)\n                 .withCatalogLoader(catalogLoader())\n+                .withTypeMapping(typeMapping)\n                 .withDatabase(database)\n                 .withTables(tables)\n                 .withMode(mode)\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncTableActionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncTableActionBase.java\nindex ae4f5346b24c..e0de071d2995 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncTableActionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SyncTableActionBase.java\n@@ -168,6 +168,7 @@ protected void buildSink(\n                         .withParserFactory(parserFactory)\n                         .withTable(fileStoreTable)\n                         .withIdentifier(new Identifier(database, table))\n+                        .withTypeMapping(typeMapping)\n                         .withCatalogLoader(catalogLoader());\n         String sinkParallelism = tableConfig.get(FlinkConnectorOptions.SINK_PARALLELISM.key());\n         if (sinkParallelism != null) {\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/TypeMapping.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/TypeMapping.java\nindex 741ed5a35602..4630d9763b2b 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/TypeMapping.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/TypeMapping.java\n@@ -76,6 +76,7 @@ public enum TypeMappingMode {\n         TO_STRING,\n         CHAR_TO_STRING,\n         LONGTEXT_TO_BYTES,\n+        DECIMAL_NO_CHANGE,\n         BIGINT_UNSIGNED_TO_BIGINT;\n \n         private static final Map<String, TypeMappingMode> TYPE_MAPPING_OPTIONS =\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcSinkBuilder.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcSinkBuilder.java\nindex 5dc69b4349f0..7edde41f5b38 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcSinkBuilder.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcSinkBuilder.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.annotation.Experimental;\n import org.apache.paimon.catalog.CatalogLoader;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.flink.utils.SingleOutputStreamOperatorUtils;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.table.BucketMode;\n@@ -50,6 +51,7 @@ public class CdcSinkBuilder<T> {\n     private Table table = null;\n     private Identifier identifier = null;\n     private CatalogLoader catalogLoader = null;\n+    private TypeMapping typeMapping = null;\n \n     @Nullable private Integer parallelism;\n \n@@ -83,6 +85,11 @@ public CdcSinkBuilder<T> withCatalogLoader(CatalogLoader catalogLoader) {\n         return this;\n     }\n \n+    public CdcSinkBuilder<T> withTypeMapping(TypeMapping typeMapping) {\n+        this.typeMapping = typeMapping;\n+        return this;\n+    }\n+\n     public DataStreamSink<?> build() {\n         Preconditions.checkNotNull(input, \""Input DataStream can not be null.\"");\n         Preconditions.checkNotNull(parserFactory, \""Event ParserFactory can not be null.\"");\n@@ -110,7 +117,8 @@ public DataStreamSink<?> build() {\n                                 new UpdatedDataFieldsProcessFunction(\n                                         new SchemaManager(dataTable.fileIO(), dataTable.location()),\n                                         identifier,\n-                                        catalogLoader))\n+                                        catalogLoader,\n+                                        typeMapping))\n                         .name(\""Schema Evolution\"");\n         schemaChangeProcessFunction.getTransformation().setParallelism(1);\n         schemaChangeProcessFunction.getTransformation().setMaxParallelism(1);\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\nindex 036b040993ae..1d25929716e4 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.flink.FlinkConnectorOptions;\n import org.apache.paimon.flink.action.MultiTablesSinkMode;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.flink.sink.FlinkWriteSink;\n import org.apache.paimon.flink.utils.SingleOutputStreamOperatorUtils;\n import org.apache.paimon.options.MemorySize;\n@@ -74,6 +75,8 @@ public class FlinkCdcSyncDatabaseSinkBuilder<T> {\n     //     Paimon tables. 2) in multiplex sink where it is used to\n     //     initialize different writers to multiple tables.\n     private CatalogLoader catalogLoader;\n+    private TypeMapping typeMapping;\n+\n     // database to sync, currently only support single database\n     private String database;\n     private MultiTablesSinkMode mode;\n@@ -122,6 +125,11 @@ public FlinkCdcSyncDatabaseSinkBuilder<T> withMode(MultiTablesSinkMode mode) {\n         return this;\n     }\n \n+    public FlinkCdcSyncDatabaseSinkBuilder<T> withTypeMapping(TypeMapping typeMapping) {\n+        this.typeMapping = typeMapping;\n+        return this;\n+    }\n+\n     public void build() {\n         Preconditions.checkNotNull(input);\n         Preconditions.checkNotNull(parserFactory);\n@@ -154,7 +162,7 @@ private void buildCombinedCdcSink() {\n                         parsed,\n                         CdcDynamicTableParsingProcessFunction.DYNAMIC_SCHEMA_CHANGE_OUTPUT_TAG)\n                 .keyBy(t -> t.f0)\n-                .process(new MultiTableUpdatedDataFieldsProcessFunction(catalogLoader))\n+                .process(new MultiTableUpdatedDataFieldsProcessFunction(catalogLoader, typeMapping))\n                 .name(\""Schema Evolution\"");\n \n         DataStream<CdcMultiplexRecord> converted =\n@@ -202,7 +210,8 @@ private void buildDividedCdcSink() {\n                                     new UpdatedDataFieldsProcessFunction(\n                                             new SchemaManager(table.fileIO(), table.location()),\n                                             Identifier.create(database, table.name()),\n-                                            catalogLoader))\n+                                            catalogLoader,\n+                                            typeMapping))\n                             .name(\""Schema Evolution\"");\n             schemaChangeProcessFunction.getTransformation().setParallelism(1);\n             schemaChangeProcessFunction.getTransformation().setMaxParallelism(1);\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\nindex dd612a52c2eb..71f71b241224 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/MultiTableUpdatedDataFieldsProcessFunction.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogLoader;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.table.FileStoreTable;\n@@ -52,8 +53,9 @@ public class MultiTableUpdatedDataFieldsProcessFunction\n \n     private final Map<Identifier, SchemaManager> schemaManagers = new HashMap<>();\n \n-    public MultiTableUpdatedDataFieldsProcessFunction(CatalogLoader catalogLoader) {\n-        super(catalogLoader);\n+    public MultiTableUpdatedDataFieldsProcessFunction(\n+            CatalogLoader catalogLoader, TypeMapping typeMapping) {\n+        super(catalogLoader, typeMapping);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\nindex e143aabf6c13..93e22f1e62b6 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunction.java\n@@ -20,6 +20,7 @@\n \n import org.apache.paimon.catalog.CatalogLoader;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.types.DataField;\n@@ -53,8 +54,11 @@ public class UpdatedDataFieldsProcessFunction\n     private Set<FieldIdentifier> latestFields;\n \n     public UpdatedDataFieldsProcessFunction(\n-            SchemaManager schemaManager, Identifier identifier, CatalogLoader catalogLoader) {\n-        super(catalogLoader);\n+            SchemaManager schemaManager,\n+            Identifier identifier,\n+            CatalogLoader catalogLoader,\n+            TypeMapping typeMapping) {\n+        super(catalogLoader, typeMapping);\n         this.schemaManager = schemaManager;\n         this.identifier = identifier;\n         this.latestFields = new HashSet<>();\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\nindex 90edbc034a54..bd8097311382 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/UpdatedDataFieldsProcessFunctionBase.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogLoader;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.schema.TableSchema;\n@@ -52,6 +53,7 @@ public abstract class UpdatedDataFieldsProcessFunctionBase<I, O> extends Process\n     protected final CatalogLoader catalogLoader;\n     protected Catalog catalog;\n     private boolean caseSensitive;\n+    private TypeMapping typeMapping;\n \n     private static final List<DataTypeRoot> STRING_TYPES =\n             Arrays.asList(DataTypeRoot.CHAR, DataTypeRoot.VARCHAR);\n@@ -71,8 +73,10 @@ public abstract class UpdatedDataFieldsProcessFunctionBase<I, O> extends Process\n     private static final List<DataTypeRoot> TIMESTAMP_TYPES =\n             Arrays.asList(DataTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE);\n \n-    protected UpdatedDataFieldsProcessFunctionBase(CatalogLoader catalogLoader) {\n+    protected UpdatedDataFieldsProcessFunctionBase(\n+            CatalogLoader catalogLoader, TypeMapping typeMapping) {\n         this.catalogLoader = catalogLoader;\n+        this.typeMapping = typeMapping;\n     }\n \n     /**\n@@ -214,6 +218,11 @@ protected List<SchemaChange> extractSchemaChanges(\n             oldFields.put(oldField.name(), oldField);\n         }\n \n+        boolean allowDecimalTypeChange =\n+                this.typeMapping == null\n+                        || !this.typeMapping.containsMode(\n+                                TypeMapping.TypeMappingMode.DECIMAL_NO_CHANGE);\n+\n         List<SchemaChange> result = new ArrayList<>();\n         for (DataField newField : updatedDataFields) {\n             String newFieldName = StringUtils.toLowerCaseIfNeed(newField.name(), caseSensitive);\n@@ -232,6 +241,9 @@ protected List<SchemaChange> extractSchemaChanges(\n                                         new String[] {newFieldName}, newField.description()));\n                     }\n                 } else {\n+                    if (oldField.type().is(DataTypeRoot.DECIMAL) && !allowDecimalTypeChange) {\n+                        continue;\n+                    }\n                     // update column type\n                     result.add(SchemaChange.updateColumnType(newFieldName, newField.type()));\n                     // update column comment\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java\nindex 46c8e98fb639..8d071150e07d 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/SchemaEvolutionTest.java\n@@ -210,7 +210,8 @@ public void testSchemaEvolution() throws Exception {\n                                 new UpdatedDataFieldsProcessFunction(\n                                         new SchemaManager(table.fileIO(), table.location()),\n                                         identifier,\n-                                        catalogLoader))\n+                                        catalogLoader,\n+                                        TypeMapping.defaultMapping()))\n                         .name(\""Schema Evolution\"");\n         schemaChangeProcessFunction.getTransformation().setParallelism(1);\n         schemaChangeProcessFunction.getTransformation().setMaxParallelism(1);\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java\nindex ed1885f5d774..eea2b6fa3471 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/kafka/KafkaCanalSyncTableActionITCase.java\n@@ -49,6 +49,7 @@\n import static org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.TOPIC;\n import static org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.TOPIC_PATTERN;\n import static org.apache.flink.streaming.connectors.kafka.table.KafkaConnectorOptions.VALUE_FORMAT;\n+import static org.apache.paimon.flink.action.cdc.TypeMapping.TypeMappingMode.DECIMAL_NO_CHANGE;\n import static org.apache.paimon.flink.action.cdc.TypeMapping.TypeMappingMode.TO_STRING;\n import static org.apache.paimon.testutils.assertj.PaimonAssertions.anyCauseMatches;\n import static org.assertj.core.api.Assertions.assertThat;\n@@ -1148,4 +1149,39 @@ public void testComputedColumnWithCaseInsensitive(boolean triggerSchemaRetrieval\n     public void testWaterMarkSyncTable() throws Exception {\n         testWaterMarkSyncTable(CANAL);\n     }\n+\n+    @Test\n+    @Timeout(60)\n+    public void testDecimalNoChange() throws Exception {\n+        String topic = \""decimal-no-change\"";\n+        createTestTopic(topic, 1, 1);\n+        writeRecordsToKafka(topic, \""kafka/canal/table/typenochange/canal-data-3.txt\"");\n+        Map<String, String> kafkaConfig = getBasicKafkaConfig();\n+        kafkaConfig.put(VALUE_FORMAT.key(), \""canal-json\"");\n+        kafkaConfig.put(TOPIC.key(), topic);\n+        KafkaSyncTableAction action =\n+                syncTableActionBuilder(kafkaConfig)\n+                        .withTableConfig(getBasicTableConfig())\n+                        .withCatalogConfig(\n+                                Collections.singletonMap(\n+                                        CatalogOptions.CASE_SENSITIVE.key(), \""false\""))\n+                        .withTypeMappingModes(DECIMAL_NO_CHANGE.configString())\n+                        .build();\n+        runActionWithDefaultEnv(action);\n+        RowType rowType =\n+                RowType.of(\n+                        new DataType[] {DataTypes.INT().notNull(), DataTypes.DECIMAL(10, 2)},\n+                        new String[] {\""k\"", \""v\""});\n+        waitForResult(\n+                Collections.singletonList(\""+I[1, 1.20]\""),\n+                getFileStoreTable(tableName),\n+                rowType,\n+                Collections.singletonList(\""k\""));\n+        writeRecordsToKafka(topic, \""kafka/canal/table/typenochange/canal-data-4.txt\"");\n+        waitForResult(\n+                Arrays.asList(\""+I[1, 1.20]\"", \""+I[2, 2.30]\""),\n+                getFileStoreTable(tableName),\n+                rowType, // should not change\n+                Collections.singletonList(\""k\""));\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\nindex 35286e3a88d4..484670e23ca3 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\n@@ -23,6 +23,7 @@\n import org.apache.paimon.catalog.CatalogUtils;\n import org.apache.paimon.data.InternalRow;\n import org.apache.paimon.flink.FlinkCatalogFactory;\n+import org.apache.paimon.flink.action.cdc.TypeMapping;\n import org.apache.paimon.flink.util.AbstractTestBase;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.Path;\n@@ -172,6 +173,7 @@ private void innerTestRandomCdcEvents(Supplier<Integer> bucket, boolean unawareB\n                 // each table can only get 2 slots\n                 .withTableOptions(Collections.singletonMap(SINK_PARALLELISM.key(), \""2\""))\n                 .withDatabase(DATABASE_NAME)\n+                .withTypeMapping(TypeMapping.defaultMapping())\n                 .withCatalogLoader(catalogLoader)\n                 .build();\n \n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-3.txt b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-3.txt\nnew file mode 100644\nindex 000000000000..f9380168b051\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-3.txt\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+{\""data\"":[{\""k\"":\""1\"",\""v\"":\""1.2\""}],\""database\"":\""paimon_sync_table\"",\""es\"":1683880554000,\""id\"":2150,\""isDdl\"":false,\""mysqlType\"":{\""k\"":\""int\"",\""v\"":\""decimal(10,2)\""},\""old\"":null,\""pkNames\"":[\""k\""],\""sql\"":\""\"",\""sqlType\"":{\""k\"":4,\""v\"":3},\""table\"":\""decimal_no_change\"",\""ts\"":1683880554351,\""type\"":\""INSERT\""}\n\\ No newline at end of file\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-4.txt b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-4.txt\nnew file mode 100644\nindex 000000000000..280c5cd70570\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/resources/kafka/canal/table/typenochange/canal-data-4.txt\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+{\""data\"":[{\""k\"":\""2\"",\""v\"":\""2.3\""}],\""database\"":\""paimon_sync_table\"",\""es\"":1683880554000,\""id\"":2150,\""isDdl\"":false,\""mysqlType\"":{\""k\"":\""int\"",\""v\"":\""decimal(15,4)\""},\""old\"":null,\""pkNames\"":[\""k\""],\""sql\"":\""\"",\""sqlType\"":{\""k\"":4,\""v\"":3},\""table\"":\""decimal_no_change\"",\""ts\"":1683880554351,\""type\"":\""INSERT\""}\n\\ No newline at end of file\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5082"", ""pr_id"": 5082, ""issue_id"": 5066, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Bug] Parquet format with timestamp with local time processing wrong\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Paimon version\n\nFor timestamp with local time type, the local time should be normalized to UTC time then written to parquet/orc file.\n\nOrc format process well as expected, but parquet not.\n\n\n\n### Compute Engine\n\nJust Paimon SDK itself.\n\n### Minimal reproduce step\n\nthe code(0.8.2):\n\n```java\npackage org.byconity.paimon.demo;\n\nimport org.apache.commons.io.FileUtils;\nimport org.apache.paimon.catalog.Catalog;\nimport org.apache.paimon.catalog.CatalogContext;\nimport org.apache.paimon.catalog.CatalogFactory;\nimport org.apache.paimon.catalog.Identifier;\nimport org.apache.paimon.data.GenericRow;\nimport org.apache.paimon.data.InternalRow;\nimport org.apache.paimon.data.Timestamp;\nimport org.apache.paimon.fs.Path;\nimport org.apache.paimon.options.CatalogOptions;\nimport org.apache.paimon.options.Options;\nimport org.apache.paimon.reader.RecordReader;\nimport org.apache.paimon.schema.Schema;\nimport org.apache.paimon.table.Table;\nimport org.apache.paimon.table.sink.BatchTableCommit;\nimport org.apache.paimon.table.sink.BatchTableWrite;\nimport org.apache.paimon.table.sink.BatchWriteBuilder;\nimport org.apache.paimon.table.sink.CommitMessage;\nimport org.apache.paimon.table.source.ReadBuilder;\nimport org.apache.paimon.table.source.Split;\nimport org.apache.paimon.table.source.TableRead;\nimport org.apache.paimon.types.DataTypes;\n\nimport java.io.File;\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\nimport java.util.List;\n\npublic class ParquetWriteTest {\n\n    private static void testTimestampWithFormat(String localPath, String fileFormat)\n            throws Exception {\n        Options options = new Options();\n        options.set(CatalogOptions.METASTORE, \""filesystem\"");\n        options.set(CatalogOptions.WAREHOUSE, new Path(localPath).toUri().toString());\n        CatalogContext context = CatalogContext.create(options);\n        Catalog catalog = CatalogFactory.createCatalog(context);\n        String dbName = \""testDb\"" + \""_\"" + fileFormat;\n        String tblName = \""testTbl\"";\n        catalog.createDatabase(dbName, false);\n        Schema.Builder schemaBuilder = Schema.newBuilder();\n        schemaBuilder.column(\""col_local_zoned_timestamp\"",\n                DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE());\n        schemaBuilder.option(\""file.format\"", fileFormat);\n        Schema schema = schemaBuilder.build();\n        Identifier tableId = Identifier.create(dbName, tblName);\n        catalog.createTable(tableId, schema, false);\n        Table table = catalog.getTable(tableId);\n        BatchWriteBuilder writeBuilder = table.newBatchWriteBuilder().withOverwrite();\n        try (BatchTableWrite write = writeBuilder.newWrite()) {\n            LocalDateTime localDateTime = LocalDateTime.parse(\""2024-12-12 10:10:10.000000\"",\n                    DateTimeFormatter.ofPattern(\""yyyy-MM-dd HH:mm:ss.SSSSSS\""));\n            GenericRow record = GenericRow.of(Timestamp.fromLocalDateTime(localDateTime));\n            write.write(record);\n            List<CommitMessage> messages = write.prepareCommit();\n            try (BatchTableCommit commit = writeBuilder.newCommit()) {\n                commit.commit(messages);\n            }\n        }\n\n        ReadBuilder readBuilder = table.newReadBuilder();\n        List<Split> splits = readBuilder.newScan().plan().splits();\n        TableRead read = readBuilder.newRead();\n\n        try (RecordReader<InternalRow> reader = read.createReader(splits)) {\n            RecordReader.RecordIterator<InternalRow> batch;\n            while ((batch = reader.readBatch()) != null) {\n                InternalRow row;\n                while ((row = batch.next()) != null) {\n                    Timestamp timestamp = row.getTimestamp(0, 6);\n                    System.out.println(timestamp);\n                }\n                batch.releaseBatch();\n            }\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        String localPath = \""/tmp/paimon_warehouse\"";\n        FileUtils.deleteDirectory(new File(localPath));\n\n        testTimestampWithFormat(localPath, \""orc\"");\n        testTimestampWithFormat(localPath, \""parquet\"");\n    }\n}\n```\n\nand, you can check the orc data file and parquet data file, it stores different UTC timestamp.\n\nHere are two py scrips for display orc and parquet file\n\n```py\nimport pyarrow.orc as orc\nimport sys\n\ndef read_orc_file(file_path):\n    orc_file = orc.ORCFile(file_path)\n    table = orc_file.read()\n\n    print(\""Schema:\"")\n    print(table.schema)\n    print(\""\\nData:\"")\n    print(table.to_pandas())\n\nif __name__ == \""__main__\"":\n    if len(sys.argv) != 2:\n        print(\""Usage: python read_orc.py <path_to_orc_file>\"")\n        sys.exit(1)\n\n    orc_file_path = sys.argv[1]\n    read_orc_file(orc_file_path)\n```\n\n\n```py\nimport pyarrow.parquet as pq\nimport sys\n\ndef read_parquet_file(file_path):\n    parquet_file = pq.ParquetFile(file_path)\n    table = parquet_file.read()\n\n    print(\""Schema:\"")\n    print(table.schema)\n    print(\""\\nData:\"")\n    print(table.to_pandas())\n\nif __name__ == \""__main__\"":\n    if len(sys.argv) != 2:\n        print(\""Usage: python read_parquet.py <path_to_parquet_file>\"")\n        sys.exit(1)\n\n    parquet_file_path = sys.argv[1]\n    read_parquet_file(parquet_file_path)\n```\n\n\nResults:\n\n```\npython3 ~/Code/py/read_orc.py /tmp/paimon_warehouse/testDb_orc.db/testTbl/bucket-0/data-945a146d-9ee2-4f41-987f-0231c63b97e8-0.orc\nSchema:\ncol_local_zoned_timestamp: timestamp[ns, tz=UTC]\n\nData:\n  col_local_zoned_timestamp\n0 2024-12-12 02:10:10+00:00\n```\n\n```\npython3 ~/Code/py/read_parquet.py /tmp/paimon_warehouse/testDb_parquet.db/testTbl/bucket-0/data-82e3abb6-98e9-4eb5-9a6f-126a96fc056c-0.parquet\nSchema:\ncol_local_zoned_timestamp: timestamp[us, tz=UTC]\n\nData:\n  col_local_zoned_timestamp\n0 2024-12-12 10:10:10+00:00\n```\n\nAs you can see, parquet didn't normalize the local time to UTC timestamp, but simply treated it as UTC timestamp and write into file directly.\n\n### What doesn't meet your expectations?\n\nthe data stored in parquet is not met expectations.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] I'm willing to submit a PR!"", ""issue_word_count"": 715, ""test_files_count"": 2, ""non_test_files_count"": 9, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/orc_configuration.html"", ""paimon-format/src/main/java/org/apache/paimon/format/OrcOptions.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/OrcFileFormat.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/OrcReaderFactory.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/filter/OrcSimpleStatsExtractor.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/reader/AbstractOrcColumnVector.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/reader/OrcTimestampColumnVector.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/writer/FieldWriterFactory.java"", ""paimon-format/src/main/java/org/apache/paimon/format/orc/writer/RowDataVectorizer.java"", ""paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java"", ""paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java""], ""pr_changed_test_files"": [""paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java"", ""paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java""], ""base_commit"": ""a2ed1915069cb2df83bb514ad66ee42d9cc9a723"", ""head_commit"": ""6e6a48bba0a524a05db2d25d29b4847e7652ebd5"", ""repo_url"": ""https://github.com/apache/paimon/pull/5082"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5082"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-17T04:15:35.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/orc_configuration.html b/docs/layouts/shortcodes/generated/orc_configuration.html\nindex 390b26359e3d..fab33e596333 100644\n--- a/docs/layouts/shortcodes/generated/orc_configuration.html\n+++ b/docs/layouts/shortcodes/generated/orc_configuration.html\n@@ -38,5 +38,11 @@\n             <td>Double</td>\n             <td>If the number of distinct keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding in orc. Use 0 to always disable dictionary encoding. Use 1 to always use dictionary encoding.</td>\n         </tr>\n+        <tr>\n+            <td><h5>orc.timestamp-ltz.legacy.type</h5></td>\n+            <td style=\""word-wrap: break-word;\"">false</td>\n+            <td>Boolean</td>\n+            <td>This option is used to be compatible with the paimon-orc\u2018s old behavior for the `timestamp_ltz` data type.</td>\n+        </tr>\n     </tbody>\n </table>\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/OrcOptions.java b/paimon-format/src/main/java/org/apache/paimon/format/OrcOptions.java\nindex dad2628679fa..e102543d6b2c 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/OrcOptions.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/OrcOptions.java\n@@ -41,4 +41,11 @@ public class OrcOptions {\n                                     + \""fraction of the total number of non-null rows, turn off \""\n                                     + \""dictionary encoding in orc. Use 0 to always disable dictionary encoding. \""\n                                     + \""Use 1 to always use dictionary encoding.\"");\n+\n+    public static final ConfigOption<Boolean> ORC_TIMESTAMP_LTZ_LEGACY_TYPE =\n+            key(\""orc.timestamp-ltz.legacy.type\"")\n+                    .booleanType()\n+                    .defaultValue(false)\n+                    .withDescription(\n+                            \""This option is used to be compatible with the paimon-orc\u2018s old behavior for the `timestamp_ltz` data type.\"");\n }\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcFileFormat.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcFileFormat.java\nindex 9acea56ab393..be257aef4b8e 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcFileFormat.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcFileFormat.java\n@@ -56,6 +56,7 @@\n import java.util.stream.Collectors;\n \n import static org.apache.paimon.CoreOptions.DELETION_VECTORS_ENABLED;\n+import static org.apache.paimon.format.OrcOptions.ORC_TIMESTAMP_LTZ_LEGACY_TYPE;\n import static org.apache.paimon.types.DataTypeChecks.getFieldTypes;\n \n /** Orc {@link FileFormat}. */\n@@ -70,6 +71,7 @@ public class OrcFileFormat extends FileFormat {\n     private final int readBatchSize;\n     private final int writeBatchSize;\n     private final boolean deletionVectorsEnabled;\n+    private final boolean legacyTimestampLtzType;\n \n     public OrcFileFormat(FormatContext formatContext) {\n         super(IDENTIFIER);\n@@ -81,6 +83,7 @@ public OrcFileFormat(FormatContext formatContext) {\n         this.readBatchSize = formatContext.readBatchSize();\n         this.writeBatchSize = formatContext.writeBatchSize();\n         this.deletionVectorsEnabled = formatContext.options().get(DELETION_VECTORS_ENABLED);\n+        this.legacyTimestampLtzType = formatContext.options().get(ORC_TIMESTAMP_LTZ_LEGACY_TYPE);\n     }\n \n     @VisibleForTesting\n@@ -96,7 +99,8 @@ public int readBatchSize() {\n     @Override\n     public Optional<SimpleStatsExtractor> createStatsExtractor(\n             RowType type, SimpleColStatsCollector.Factory[] statsCollectors) {\n-        return Optional.of(new OrcSimpleStatsExtractor(type, statsCollectors));\n+        return Optional.of(\n+                new OrcSimpleStatsExtractor(type, statsCollectors, legacyTimestampLtzType));\n     }\n \n     @Override\n@@ -116,7 +120,8 @@ public FormatReaderFactory createReaderFactory(\n                 (RowType) refineDataType(projectedRowType),\n                 orcPredicates,\n                 readBatchSize,\n-                deletionVectorsEnabled);\n+                deletionVectorsEnabled,\n+                legacyTimestampLtzType);\n     }\n \n     @Override\n@@ -141,7 +146,8 @@ public FormatWriterFactory createWriterFactory(RowType type) {\n         DataType[] orcTypes = getFieldTypes(refinedType).toArray(new DataType[0]);\n \n         TypeDescription typeDescription = OrcTypeUtil.convertToOrcSchema((RowType) refinedType);\n-        Vectorizer<InternalRow> vectorizer = new RowDataVectorizer(typeDescription, orcTypes);\n+        Vectorizer<InternalRow> vectorizer =\n+                new RowDataVectorizer(typeDescription, orcTypes, legacyTimestampLtzType);\n \n         return new OrcWriterFactory(vectorizer, orcProperties, writerConf, writeBatchSize);\n     }\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcReaderFactory.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcReaderFactory.java\nindex fad4c7e36a66..a94eaf2356d8 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcReaderFactory.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/OrcReaderFactory.java\n@@ -69,6 +69,7 @@ public class OrcReaderFactory implements FormatReaderFactory {\n     protected final List<OrcFilters.Predicate> conjunctPredicates;\n     protected final int batchSize;\n     protected final boolean deletionVectorsEnabled;\n+    protected final boolean legacyTimestampLtzType;\n \n     /**\n      * @param hadoopConfig the hadoop config for orc reader.\n@@ -80,13 +81,15 @@ public OrcReaderFactory(\n             final RowType readType,\n             final List<OrcFilters.Predicate> conjunctPredicates,\n             final int batchSize,\n-            final boolean deletionVectorsEnabled) {\n+            final boolean deletionVectorsEnabled,\n+            final boolean legacyTimestampLtzType) {\n         this.hadoopConfig = checkNotNull(hadoopConfig);\n         this.schema = convertToOrcSchema(readType);\n         this.tableType = readType;\n         this.conjunctPredicates = checkNotNull(conjunctPredicates);\n         this.batchSize = batchSize;\n         this.deletionVectorsEnabled = deletionVectorsEnabled;\n+        this.legacyTimestampLtzType = legacyTimestampLtzType;\n     }\n \n     // ------------------------------------------------------------------------\n@@ -131,7 +134,10 @@ public OrcReaderBatch createReaderBatch(\n             DataType type = tableFieldTypes.get(i);\n             vectors[i] =\n                     createPaimonVector(\n-                            orcBatch.cols[tableFieldNames.indexOf(name)], orcBatch, type);\n+                            orcBatch.cols[tableFieldNames.indexOf(name)],\n+                            orcBatch,\n+                            type,\n+                            legacyTimestampLtzType);\n         }\n         return new OrcReaderBatch(filePath, orcBatch, new VectorizedColumnBatch(vectors), recycler);\n     }\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/filter/OrcSimpleStatsExtractor.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/filter/OrcSimpleStatsExtractor.java\nindex c6772c484942..c0b9b6f59ba5 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/filter/OrcSimpleStatsExtractor.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/filter/OrcSimpleStatsExtractor.java\n@@ -56,11 +56,15 @@ public class OrcSimpleStatsExtractor implements SimpleStatsExtractor {\n \n     private final RowType rowType;\n     private final SimpleColStatsCollector.Factory[] statsCollectors;\n+    private final boolean legacyTimestampLtzType;\n \n     public OrcSimpleStatsExtractor(\n-            RowType rowType, SimpleColStatsCollector.Factory[] statsCollectors) {\n+            RowType rowType,\n+            SimpleColStatsCollector.Factory[] statsCollectors,\n+            boolean legacyTimestampLtzType) {\n         this.rowType = rowType;\n         this.statsCollectors = statsCollectors;\n+        this.legacyTimestampLtzType = legacyTimestampLtzType;\n         Preconditions.checkArgument(\n                 rowType.getFieldCount() == statsCollectors.length,\n                 \""The stats collector is not aligned to write schema.\"");\n@@ -228,7 +232,6 @@ private SimpleColStats toFieldStats(\n                                 nullCount);\n                 break;\n             case TIMESTAMP_WITHOUT_TIME_ZONE:\n-            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n                 assertStatsClass(field, stats, TimestampColumnStatistics.class);\n                 TimestampColumnStatistics timestampStats = (TimestampColumnStatistics) stats;\n                 fieldStats =\n@@ -237,6 +240,22 @@ private SimpleColStats toFieldStats(\n                                 Timestamp.fromSQLTimestamp(timestampStats.getMaximum()),\n                                 nullCount);\n                 break;\n+            case TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+                assertStatsClass(field, stats, TimestampColumnStatistics.class);\n+                TimestampColumnStatistics timestampLtzStats = (TimestampColumnStatistics) stats;\n+                fieldStats =\n+                        legacyTimestampLtzType\n+                                ? new SimpleColStats(\n+                                        Timestamp.fromSQLTimestamp(timestampLtzStats.getMinimum()),\n+                                        Timestamp.fromSQLTimestamp(timestampLtzStats.getMaximum()),\n+                                        nullCount)\n+                                : new SimpleColStats(\n+                                        Timestamp.fromInstant(\n+                                                timestampLtzStats.getMinimum().toInstant()),\n+                                        Timestamp.fromInstant(\n+                                                timestampLtzStats.getMaximum().toInstant()),\n+                                        nullCount);\n+                break;\n             default:\n                 fieldStats = new SimpleColStats(null, null, nullCount);\n         }\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/AbstractOrcColumnVector.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/AbstractOrcColumnVector.java\nindex 0557a72230cc..93ae8a2aea39 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/AbstractOrcColumnVector.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/AbstractOrcColumnVector.java\n@@ -62,6 +62,14 @@ public boolean isNullAt(int i) {\n \n     public static org.apache.paimon.data.columnar.ColumnVector createPaimonVector(\n             ColumnVector vector, VectorizedRowBatch orcBatch, DataType dataType) {\n+        return createPaimonVector(vector, orcBatch, dataType, false);\n+    }\n+\n+    public static org.apache.paimon.data.columnar.ColumnVector createPaimonVector(\n+            ColumnVector vector,\n+            VectorizedRowBatch orcBatch,\n+            DataType dataType,\n+            boolean legacyTimestampLtzType) {\n         if (vector instanceof LongColumnVector) {\n             if (dataType.getTypeRoot() == DataTypeRoot.TIMESTAMP_WITHOUT_TIME_ZONE) {\n                 return new OrcLegacyTimestampColumnVector((LongColumnVector) vector, orcBatch);\n@@ -75,7 +83,7 @@ public static org.apache.paimon.data.columnar.ColumnVector createPaimonVector(\n         } else if (vector instanceof DecimalColumnVector) {\n             return new OrcDecimalColumnVector((DecimalColumnVector) vector, orcBatch);\n         } else if (vector instanceof TimestampColumnVector) {\n-            return new OrcTimestampColumnVector(vector, orcBatch);\n+            return new OrcTimestampColumnVector(vector, orcBatch, dataType, legacyTimestampLtzType);\n         } else if (vector instanceof ListColumnVector) {\n             return new OrcArrayColumnVector(\n                     (ListColumnVector) vector, orcBatch, (ArrayType) dataType);\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/OrcTimestampColumnVector.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/OrcTimestampColumnVector.java\nindex a6e71d6016f2..4a3661959462 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/OrcTimestampColumnVector.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/reader/OrcTimestampColumnVector.java\n@@ -19,6 +19,8 @@\n package org.apache.paimon.format.orc.reader;\n \n import org.apache.paimon.data.Timestamp;\n+import org.apache.paimon.types.DataType;\n+import org.apache.paimon.types.TimestampType;\n import org.apache.paimon.utils.DateTimeUtils;\n \n import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n@@ -31,17 +33,28 @@\n  */\n public class OrcTimestampColumnVector extends AbstractOrcColumnVector\n         implements org.apache.paimon.data.columnar.TimestampColumnVector {\n-\n+    private final Boolean legacyTimestampLtzType;\n+    private final DataType dataType;\n     private final TimestampColumnVector vector;\n \n-    public OrcTimestampColumnVector(ColumnVector vector, VectorizedRowBatch orcBatch) {\n+    public OrcTimestampColumnVector(\n+            ColumnVector vector,\n+            VectorizedRowBatch orcBatch,\n+            DataType dataType,\n+            boolean legacyTimestampLtzType) {\n         super(vector, orcBatch);\n         this.vector = (TimestampColumnVector) vector;\n+        this.dataType = dataType;\n+        this.legacyTimestampLtzType = legacyTimestampLtzType;\n     }\n \n     @Override\n     public Timestamp getTimestamp(int i, int precision) {\n         i = rowMapper(i);\n-        return DateTimeUtils.toInternal(vector.time[i], vector.nanos[i] % 1_000_000);\n+        if (dataType instanceof TimestampType || legacyTimestampLtzType) {\n+            return DateTimeUtils.toInternal(vector.time[i], vector.nanos[i] % 1_000_000);\n+        } else {\n+            return Timestamp.fromEpochMillis(vector.time[i], vector.nanos[i] % 1_000_000);\n+        }\n     }\n }\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/FieldWriterFactory.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/FieldWriterFactory.java\nindex 26b008ab1c60..61f54c786494 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/FieldWriterFactory.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/FieldWriterFactory.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.data.InternalArray;\n import org.apache.paimon.data.InternalMap;\n import org.apache.paimon.data.InternalRow;\n+import org.apache.paimon.data.LocalZoneTimestamp;\n import org.apache.paimon.types.ArrayType;\n import org.apache.paimon.types.BigIntType;\n import org.apache.paimon.types.BinaryType;\n@@ -63,8 +64,6 @@\n /** Factory to create {@link FieldWriter}. */\n public class FieldWriterFactory implements DataTypeVisitor<FieldWriter> {\n \n-    public static final FieldWriterFactory WRITER_FACTORY = new FieldWriterFactory();\n-\n     private static final FieldWriter STRING_WRITER =\n             (rowId, column, getters, columnId) -> {\n                 BytesColumnVector vector = (BytesColumnVector) column;\n@@ -108,6 +107,12 @@ public class FieldWriterFactory implements DataTypeVisitor<FieldWriter> {\n             (rowId, column, getters, columnId) ->\n                     ((DoubleColumnVector) column).vector[rowId] = getters.getDouble(columnId);\n \n+    private final boolean legacyTimestampLtzType;\n+\n+    public FieldWriterFactory(boolean legacyTimestampLtzType) {\n+        this.legacyTimestampLtzType = legacyTimestampLtzType;\n+    }\n+\n     @Override\n     public FieldWriter visit(CharType charType) {\n         return STRING_WRITER;\n@@ -186,9 +191,20 @@ public FieldWriter visit(TimestampType timestampType) {\n     @Override\n     public FieldWriter visit(LocalZonedTimestampType localZonedTimestampType) {\n         return (rowId, column, getters, columnId) -> {\n-            Timestamp timestamp =\n-                    getters.getTimestamp(columnId, localZonedTimestampType.getPrecision())\n-                            .toSQLTimestamp();\n+            org.apache.paimon.data.Timestamp localTimestamp =\n+                    getters.getTimestamp(columnId, localZonedTimestampType.getPrecision());\n+            Timestamp timestamp;\n+\n+            if (legacyTimestampLtzType) {\n+                timestamp = localTimestamp.toSQLTimestamp();\n+            } else {\n+                LocalZoneTimestamp localZoneTimestamp =\n+                        LocalZoneTimestamp.fromEpochMillis(\n+                                localTimestamp.getMillisecond(),\n+                                localTimestamp.getNanoOfMillisecond());\n+                timestamp = java.sql.Timestamp.from(localZoneTimestamp.toInstant());\n+            }\n+\n             TimestampColumnVector vector = (TimestampColumnVector) column;\n             vector.set(rowId, timestamp);\n         };\n\ndiff --git a/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/RowDataVectorizer.java b/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/RowDataVectorizer.java\nindex 46c936a0263e..47c448c17f8d 100644\n--- a/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/RowDataVectorizer.java\n+++ b/paimon-format/src/main/java/org/apache/paimon/format/orc/writer/RowDataVectorizer.java\n@@ -29,18 +29,22 @@\n import java.util.List;\n import java.util.stream.Collectors;\n \n-import static org.apache.paimon.format.orc.writer.FieldWriterFactory.WRITER_FACTORY;\n-\n /** A {@link Vectorizer} of {@link InternalRow} type element. */\n public class RowDataVectorizer extends Vectorizer<InternalRow> {\n \n     private final List<FieldWriter> fieldWriters;\n \n     public RowDataVectorizer(TypeDescription schema, DataType[] fieldTypes) {\n+        this(schema, fieldTypes, false);\n+    }\n+\n+    public RowDataVectorizer(\n+            TypeDescription schema, DataType[] fieldTypes, boolean legacyTimestampLtzType) {\n         super(schema);\n+        FieldWriterFactory fieldWriterFactory = new FieldWriterFactory(legacyTimestampLtzType);\n         this.fieldWriters =\n                 Arrays.stream(fieldTypes)\n-                        .map(t -> t.accept(WRITER_FACTORY))\n+                        .map(t -> t.accept(fieldWriterFactory))\n                         .collect(Collectors.toList());\n     }\n \n"", ""test_patch"": ""diff --git a/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java b/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java\nindex 782d0fc116ac..c4c7500dbed1 100644\n--- a/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java\n+++ b/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcFormatReadWriteTest.java\n@@ -18,20 +18,200 @@\n \n package org.apache.paimon.format.orc;\n \n+import org.apache.paimon.data.GenericRow;\n+import org.apache.paimon.data.InternalRow;\n+import org.apache.paimon.data.Timestamp;\n+import org.apache.paimon.data.serializer.InternalRowSerializer;\n import org.apache.paimon.format.FileFormat;\n import org.apache.paimon.format.FileFormatFactory;\n import org.apache.paimon.format.FormatReadWriteTest;\n+import org.apache.paimon.format.FormatReaderContext;\n+import org.apache.paimon.format.FormatWriter;\n+import org.apache.paimon.format.OrcOptions;\n+import org.apache.paimon.fs.PositionOutputStream;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.reader.RecordReader;\n+import org.apache.paimon.types.DataTypes;\n+import org.apache.paimon.types.RowType;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.time.LocalDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.TimeZone;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n \n /** An orc {@link FormatReadWriteTest}. */\n public class OrcFormatReadWriteTest extends FormatReadWriteTest {\n \n+    private final FileFormat legacyFormat =\n+            new OrcFileFormat(\n+                    new FileFormatFactory.FormatContext(\n+                            new Options(\n+                                    new HashMap<String, String>() {\n+                                        {\n+                                            put(\n+                                                    OrcOptions.ORC_TIMESTAMP_LTZ_LEGACY_TYPE.key(),\n+                                                    \""true\"");\n+                                        }\n+                                    }),\n+                            1024,\n+                            1024));\n+\n+    private final FileFormat newFormat =\n+            new OrcFileFormat(\n+                    new FileFormatFactory.FormatContext(\n+                            new Options(\n+                                    new HashMap<String, String>() {\n+                                        {\n+                                            put(\n+                                                    OrcOptions.ORC_TIMESTAMP_LTZ_LEGACY_TYPE.key(),\n+                                                    \""false\"");\n+                                        }\n+                                    }),\n+                            1024,\n+                            1024));\n+\n     protected OrcFormatReadWriteTest() {\n         super(\""orc\"");\n     }\n \n+    @Test\n+    public void testTimestampLTZWithLegacyWriteAndRead() throws IOException {\n+        RowType rowType = DataTypes.ROW(DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE());\n+        InternalRowSerializer serializer = new InternalRowSerializer(rowType);\n+        PositionOutputStream out = fileIO.newOutputStream(file, false);\n+        FormatWriter writer = legacyFormat.createWriterFactory(rowType).create(out, \""zstd\"");\n+        Timestamp localTimestamp =\n+                Timestamp.fromLocalDateTime(\n+                        LocalDateTime.parse(\n+                                \""2024-12-12 10:10:10\"",\n+                                DateTimeFormatter.ofPattern(\""yyyy-MM-dd HH:mm:ss\"")));\n+        GenericRow record = GenericRow.of(localTimestamp);\n+        writer.addElement(record);\n+        writer.close();\n+        out.close();\n+\n+        RecordReader<InternalRow> reader =\n+                legacyFormat\n+                        .createReaderFactory(rowType)\n+                        .createReader(\n+                                new FormatReaderContext(fileIO, file, fileIO.getFileSize(file)));\n+        List<InternalRow> result = new ArrayList<>();\n+        reader.forEachRemaining(row -> result.add(serializer.copy(row)));\n+\n+        assertThat(result).containsExactly(GenericRow.of(localTimestamp));\n+    }\n+\n+    @Test\n+    public void testTimestampLTZWithNewWriteAndRead() throws IOException {\n+        RowType rowType = DataTypes.ROW(DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE());\n+        InternalRowSerializer serializer = new InternalRowSerializer(rowType);\n+        PositionOutputStream out = fileIO.newOutputStream(file, false);\n+        FormatWriter writer = newFormat.createWriterFactory(rowType).create(out, \""zstd\"");\n+        Timestamp localTimestamp =\n+                Timestamp.fromLocalDateTime(\n+                        LocalDateTime.parse(\n+                                \""2024-12-12 10:10:10\"",\n+                                DateTimeFormatter.ofPattern(\""yyyy-MM-dd HH:mm:ss\"")));\n+        GenericRow record = GenericRow.of(localTimestamp);\n+        writer.addElement(record);\n+        writer.close();\n+        out.close();\n+\n+        RecordReader<InternalRow> reader =\n+                newFormat\n+                        .createReaderFactory(rowType)\n+                        .createReader(\n+                                new FormatReaderContext(fileIO, file, fileIO.getFileSize(file)));\n+        List<InternalRow> result = new ArrayList<>();\n+        reader.forEachRemaining(row -> result.add(serializer.copy(row)));\n+\n+        assertThat(result).containsExactly(GenericRow.of(localTimestamp));\n+    }\n+\n+    @Test\n+    public void testTimestampLTZWithNewWriteAndLegacyRead() throws IOException {\n+        RowType rowType = DataTypes.ROW(DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE());\n+        InternalRowSerializer serializer = new InternalRowSerializer(rowType);\n+        PositionOutputStream out = fileIO.newOutputStream(file, false);\n+        FormatWriter writer = newFormat.createWriterFactory(rowType).create(out, \""zstd\"");\n+        Timestamp localTimestamp =\n+                Timestamp.fromLocalDateTime(\n+                        LocalDateTime.parse(\n+                                \""2024-12-12 10:10:10\"",\n+                                DateTimeFormatter.ofPattern(\""yyyy-MM-dd HH:mm:ss\"")));\n+        GenericRow record = GenericRow.of(localTimestamp);\n+        writer.addElement(record);\n+        writer.close();\n+        out.close();\n+\n+        RecordReader<InternalRow> reader =\n+                legacyFormat\n+                        .createReaderFactory(rowType)\n+                        .createReader(\n+                                new FormatReaderContext(fileIO, file, fileIO.getFileSize(file)));\n+        List<InternalRow> result = new ArrayList<>();\n+        reader.forEachRemaining(row -> result.add(serializer.copy(row)));\n+        Timestamp shiftedTimestamp =\n+                Timestamp.fromEpochMillis(\n+                        localTimestamp.getMillisecond()\n+                                + TimeZone.getDefault().getOffset(localTimestamp.getMillisecond()),\n+                        localTimestamp.getNanoOfMillisecond());\n+\n+        assertThat(result).containsExactly(GenericRow.of(shiftedTimestamp));\n+    }\n+\n+    @Test\n+    public void testTimestampLTZWithLegacyWriteAndNewRead() throws IOException {\n+        RowType rowType = DataTypes.ROW(DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE());\n+        InternalRowSerializer serializer = new InternalRowSerializer(rowType);\n+        PositionOutputStream out = fileIO.newOutputStream(file, false);\n+        FormatWriter writer = legacyFormat.createWriterFactory(rowType).create(out, \""zstd\"");\n+        Timestamp localTimestamp =\n+                Timestamp.fromLocalDateTime(\n+                        LocalDateTime.parse(\n+                                \""2024-12-12 10:10:10\"",\n+                                DateTimeFormatter.ofPattern(\""yyyy-MM-dd HH:mm:ss\"")));\n+        GenericRow record = GenericRow.of(localTimestamp);\n+        writer.addElement(record);\n+        writer.close();\n+        out.close();\n+\n+        RecordReader<InternalRow> reader =\n+                newFormat\n+                        .createReaderFactory(rowType)\n+                        .createReader(\n+                                new FormatReaderContext(fileIO, file, fileIO.getFileSize(file)));\n+        List<InternalRow> result = new ArrayList<>();\n+        reader.forEachRemaining(row -> result.add(serializer.copy(row)));\n+        Timestamp shiftedTimestamp =\n+                Timestamp.fromEpochMillis(\n+                        localTimestamp.getMillisecond()\n+                                - TimeZone.getDefault().getOffset(localTimestamp.getMillisecond()),\n+                        localTimestamp.getNanoOfMillisecond());\n+\n+        assertThat(result).containsExactly(GenericRow.of(shiftedTimestamp));\n+    }\n+\n     @Override\n     protected FileFormat fileFormat() {\n-        return new OrcFileFormat(new FileFormatFactory.FormatContext(new Options(), 1024, 1024));\n+        return new OrcFileFormat(\n+                new FileFormatFactory.FormatContext(\n+                        new Options(\n+                                new HashMap<String, String>() {\n+                                    {\n+                                        put(\n+                                                OrcOptions.ORC_TIMESTAMP_LTZ_LEGACY_TYPE.key(),\n+                                                \""false\"");\n+                                    }\n+                                }),\n+                        1024,\n+                        1024));\n     }\n }\n\ndiff --git a/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java b/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java\nindex 63b391b44c44..87f7c8839a5f 100644\n--- a/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java\n+++ b/paimon-format/src/test/java/org/apache/paimon/format/orc/OrcReaderFactoryTest.java\n@@ -278,6 +278,7 @@ protected OrcReaderFactory createFormat(\n                 Projection.of(selectedFields).project(formatType),\n                 conjunctPredicates,\n                 BATCH_SIZE,\n+                false,\n                 false);\n     }\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-5062"", ""pr_id"": 5062, ""issue_id"": 5061, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support ValueEncodingFormat to use INSERT_ONLY log format.\n### Search before asking\n\n- [x] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\n- For append-only tables, we do not need to use `ValueEncodingFormat` with `ChangelogMode` set to `all`.\n- For the primary-key table, we hope to add a `log.ignore-delete` to ignore the retraction record. This way, we can more easily migrate existing jobs to paimon without modifying downstream jobs (downstream jobs may not only be flink sql jobs, but also some other jobs that use kafka client for consumption).\n\n### Solution\n\nSupport `ValueEncodingFormat` to use `INSERT_ONLY` log format.\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] I'm willing to submit a PR!"", ""issue_word_count"": 124, ""test_files_count"": 2, ""non_test_files_count"": 5, ""pr_changed_files"": [""paimon-common/src/main/java/org/apache/paimon/CoreOptions.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/kafka/KafkaLogStoreFactory.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/log/LogStoreTableFactory.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BaseDataTableSource.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java""], ""base_commit"": ""6facb714588c627a4f7f82ee08b982a950233737"", ""head_commit"": ""ef87a50a9ee6940440691cf23a9f457a470342f1"", ""repo_url"": ""https://github.com/apache/paimon/pull/5062"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/5062"", ""dockerfile"": """", ""pr_merged_at"": ""2025-02-12T12:59:07.000Z"", ""patch"": ""diff --git a/paimon-common/src/main/java/org/apache/paimon/CoreOptions.java b/paimon-common/src/main/java/org/apache/paimon/CoreOptions.java\nindex abc161608730..ab84aed36f08 100644\n--- a/paimon-common/src/main/java/org/apache/paimon/CoreOptions.java\n+++ b/paimon-common/src/main/java/org/apache/paimon/CoreOptions.java\n@@ -805,6 +805,13 @@ public class CoreOptions implements Serializable {\n                     .defaultValue(\""debezium-json\"")\n                     .withDescription(\""Specify the message format of log system.\"");\n \n+    @ExcludeFromDocumentation(\""Confused without log system\"")\n+    public static final ConfigOption<Boolean> LOG_IGNORE_DELETE =\n+            key(\""log.ignore-delete\"")\n+                    .booleanType()\n+                    .defaultValue(false)\n+                    .withDescription(\""Specify whether the log system ignores delete records.\"");\n+\n     public static final ConfigOption<Boolean> AUTO_CREATE =\n             key(\""auto-create\"")\n                     .booleanType()\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/kafka/KafkaLogStoreFactory.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/kafka/KafkaLogStoreFactory.java\nindex 8a9ec42a2542..f382a8cf18f2 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/kafka/KafkaLogStoreFactory.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/kafka/KafkaLogStoreFactory.java\n@@ -87,7 +87,7 @@ public KafkaLogSourceProvider createSourceProvider(\n                             .createRuntimeDecoder(sourceContext, keyType);\n         }\n         DeserializationSchema<RowData> valueDeserializer =\n-                LogStoreTableFactory.getValueDecodingFormat(helper)\n+                LogStoreTableFactory.getValueDecodingFormat(helper, primaryKey.length != 0)\n                         .createRuntimeDecoder(sourceContext, physicalType);\n         Options options = toOptions(helper.getOptions());\n         Long timestampMills = options.get(SCAN_TIMESTAMP_MILLIS);\n@@ -127,7 +127,7 @@ public KafkaLogSinkProvider createSinkProvider(\n                             .createRuntimeEncoder(sinkContext, keyType);\n         }\n         SerializationSchema<RowData> valueSerializer =\n-                LogStoreTableFactory.getValueEncodingFormat(helper)\n+                LogStoreTableFactory.getValueEncodingFormat(helper, primaryKey.length != 0)\n                         .createRuntimeEncoder(sinkContext, physicalType);\n         Options options = toOptions(helper.getOptions());\n         return new KafkaLogSinkProvider(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/log/LogStoreTableFactory.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/log/LogStoreTableFactory.java\nindex 324b6de2ffa9..4b15dc8369fd 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/log/LogStoreTableFactory.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/log/LogStoreTableFactory.java\n@@ -45,6 +45,7 @@\n import javax.annotation.Nullable;\n \n import static org.apache.paimon.CoreOptions.LOG_FORMAT;\n+import static org.apache.paimon.CoreOptions.LOG_IGNORE_DELETE;\n import static org.apache.paimon.CoreOptions.LOG_KEY_FORMAT;\n \n /**\n@@ -100,6 +101,12 @@ static ConfigOption<String> logFormat() {\n                 .defaultValue(LOG_FORMAT.defaultValue());\n     }\n \n+    static ConfigOption<Boolean> logIgnoreDelete() {\n+        return ConfigOptions.key(LOG_IGNORE_DELETE.key())\n+                .booleanType()\n+                .defaultValue(LOG_IGNORE_DELETE.defaultValue());\n+    }\n+\n     static LogStoreTableFactory discoverLogStoreFactory(ClassLoader cl, String identifier) {\n         return FactoryUtil.discoverFactory(cl, LogStoreTableFactory.class, identifier);\n     }\n@@ -121,18 +128,20 @@ static EncodingFormat<SerializationSchema<RowData>> getKeyEncodingFormat(\n     }\n \n     static DecodingFormat<DeserializationSchema<RowData>> getValueDecodingFormat(\n-            FlinkTableFactoryHelper helper) {\n+            FlinkTableFactoryHelper helper, boolean hasPrimaryKey) {\n         DecodingFormat<DeserializationSchema<RowData>> format =\n                 helper.discoverDecodingFormat(DeserializationFormatFactory.class, logFormat());\n-        validateValueFormat(format, helper.getOptions().get(logFormat()));\n+        boolean insertOnly = !hasPrimaryKey || helper.getOptions().get(logIgnoreDelete());\n+        validateValueFormat(format, helper.getOptions().get(logFormat()), insertOnly);\n         return format;\n     }\n \n     static EncodingFormat<SerializationSchema<RowData>> getValueEncodingFormat(\n-            FlinkTableFactoryHelper helper) {\n+            FlinkTableFactoryHelper helper, boolean hasPrimaryKey) {\n         EncodingFormat<SerializationSchema<RowData>> format =\n                 helper.discoverEncodingFormat(SerializationFormatFactory.class, logFormat());\n-        validateValueFormat(format, helper.getOptions().get(logFormat()));\n+        boolean insertOnly = !hasPrimaryKey || helper.getOptions().get(logIgnoreDelete());\n+        validateValueFormat(format, helper.getOptions().get(logFormat()), insertOnly);\n         return format;\n     }\n \n@@ -146,8 +155,8 @@ static void validateKeyFormat(Format format, String name) {\n         }\n     }\n \n-    static void validateValueFormat(Format format, String name) {\n-        if (!format.getChangelogMode().equals(ChangelogMode.all())) {\n+    static void validateValueFormat(Format format, String name, boolean insertOnly) {\n+        if (!insertOnly && !format.getChangelogMode().equals(ChangelogMode.all())) {\n             throw new ValidationException(\n                     String.format(\n                             \""A value format should deal with all records. \""\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\nindex 8009bec9677f..bd849f9d3eed 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\n@@ -20,6 +20,7 @@\n \n import org.apache.paimon.data.InternalRow;\n import org.apache.paimon.flink.log.LogWriteCallback;\n+import org.apache.paimon.options.Options;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.sink.SinkRecord;\n \n@@ -49,6 +50,8 @@\n import java.util.List;\n import java.util.Objects;\n \n+import static org.apache.paimon.CoreOptions.LOG_IGNORE_DELETE;\n+\n /** A {@link PrepareCommitOperator} to write {@link InternalRow}. Record schema is fixed. */\n public class RowDataStoreWriteOperator extends TableWriteOperator<InternalRow> {\n \n@@ -57,6 +60,7 @@ public class RowDataStoreWriteOperator extends TableWriteOperator<InternalRow> {\n     @Nullable private final LogSinkFunction logSinkFunction;\n     private transient SimpleContext sinkContext;\n     @Nullable private transient LogWriteCallback logCallback;\n+    private transient boolean logIgnoreDelete;\n \n     /** We listen to this ourselves because we don't have an {@link InternalTimerService}. */\n     private long currentWatermark = Long.MIN_VALUE;\n@@ -97,6 +101,7 @@ public void open() throws Exception {\n             openFunction(logSinkFunction);\n             logCallback = new LogWriteCallback();\n             logSinkFunction.setWriteCallback(logCallback);\n+            logIgnoreDelete = Options.fromMap(table.options()).get(LOG_IGNORE_DELETE);\n         }\n     }\n \n@@ -139,7 +144,9 @@ record = write.write(element.getValue());\n             throw new IOException(e);\n         }\n \n-        if (record != null && logSinkFunction != null) {\n+        if (record != null\n+                && logSinkFunction != null\n+                && (!logIgnoreDelete || record.row().getRowKind().isAdd())) {\n             // write to log store, need to preserve original pk (which includes partition fields)\n             SinkRecord logRecord = write.toLogRecord(record);\n             logSinkFunction.invoke(logRecord, sinkContext);\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BaseDataTableSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BaseDataTableSource.java\nindex 836c1372f70c..aaad71977d92 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BaseDataTableSource.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BaseDataTableSource.java\n@@ -65,6 +65,7 @@\n import static org.apache.paimon.CoreOptions.CHANGELOG_PRODUCER;\n import static org.apache.paimon.CoreOptions.LOG_CHANGELOG_MODE;\n import static org.apache.paimon.CoreOptions.LOG_CONSISTENCY;\n+import static org.apache.paimon.CoreOptions.LOG_IGNORE_DELETE;\n import static org.apache.paimon.CoreOptions.MergeEngine.FIRST_ROW;\n import static org.apache.paimon.flink.FlinkConnectorOptions.LOOKUP_ASYNC;\n import static org.apache.paimon.flink.FlinkConnectorOptions.LOOKUP_ASYNC_THREAD_NUMBER;\n@@ -149,6 +150,10 @@ public ChangelogMode getChangelogMode() {\n                 return ChangelogMode.all();\n             }\n \n+            if (logStoreTableFactory != null && options.get(LOG_IGNORE_DELETE)) {\n+                return ChangelogMode.insertOnly();\n+            }\n+\n             // optimization: transaction consistency and all changelog mode avoid the generation of\n             // normalized nodes. See FlinkTableSink.getChangelogMode validation.\n             return options.get(LOG_CONSISTENCY) == LogConsistency.TRANSACTIONAL\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java\nindex 9d8e2295a8de..28a746c307f9 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogSerializationTest.java\n@@ -23,6 +23,7 @@\n import org.apache.paimon.types.RowKind;\n \n import org.apache.flink.connector.kafka.source.reader.deserializer.KafkaRecordDeserializationSchema;\n+import org.apache.flink.table.api.ValidationException;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.factories.DynamicTableFactory;\n import org.apache.flink.util.Collector;\n@@ -30,8 +31,13 @@\n import org.apache.kafka.clients.producer.ProducerRecord;\n import org.junit.jupiter.api.Test;\n \n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.apache.paimon.CoreOptions.LOG_FORMAT;\n+import static org.apache.paimon.CoreOptions.LOG_IGNORE_DELETE;\n import static org.apache.paimon.flink.FlinkRowData.toFlinkRowKind;\n import static org.apache.paimon.flink.kafka.KafkaLogTestUtils.discoverKafkaLogFactory;\n import static org.apache.paimon.flink.kafka.KafkaLogTestUtils.testContext;\n@@ -69,6 +75,50 @@ public void testUnawareBucket() throws Exception {\n         checkNonKeyed(LogChangelogMode.ALL, -1, 5, 3);\n     }\n \n+    @Test\n+    public void testNonKeyedWithInsertOnlyFormat() throws Exception {\n+        check(\n+                LogChangelogMode.AUTO,\n+                false,\n+                -1,\n+                3,\n+                5,\n+                RowKind.INSERT,\n+                Collections.singletonMap(LOG_FORMAT.key(), \""json\""));\n+        check(\n+                LogChangelogMode.AUTO,\n+                false,\n+                -1,\n+                3,\n+                5,\n+                RowKind.UPDATE_AFTER,\n+                Collections.singletonMap(LOG_FORMAT.key(), \""json\""));\n+    }\n+\n+    @Test\n+    public void testKeyedWithInsertOnlyFormat() throws Exception {\n+        Map<String, String> dynamicOptions = new HashMap<>();\n+        dynamicOptions.put(LOG_FORMAT.key(), \""json\"");\n+\n+        assertThatThrownBy(\n+                        () ->\n+                                check(\n+                                        LogChangelogMode.AUTO,\n+                                        true,\n+                                        -1,\n+                                        3,\n+                                        5,\n+                                        RowKind.INSERT,\n+                                        dynamicOptions))\n+                .isInstanceOf(ValidationException.class)\n+                .hasMessageContaining(\n+                        \""A value format should deal with all records. But json has a changelog mode of [INSERT]\"");\n+\n+        dynamicOptions.put(LOG_IGNORE_DELETE.key(), \""true\"");\n+        check(LogChangelogMode.AUTO, true, -1, 3, 5, RowKind.INSERT, dynamicOptions);\n+        check(LogChangelogMode.AUTO, true, -1, 3, 5, RowKind.UPDATE_AFTER, dynamicOptions);\n+    }\n+\n     private void checkKeyed(LogChangelogMode mode, int bucket, int key, int value)\n             throws Exception {\n         check(mode, true, bucket, key, value, RowKind.INSERT);\n@@ -88,11 +138,23 @@ private void checkNonKeyed(LogChangelogMode mode, int bucket, int key, int value\n     private void check(\n             LogChangelogMode mode, boolean keyed, int bucket, int key, int value, RowKind rowKind)\n             throws Exception {\n+        check(mode, keyed, bucket, key, value, rowKind, Collections.emptyMap());\n+    }\n+\n+    private void check(\n+            LogChangelogMode mode,\n+            boolean keyed,\n+            int bucket,\n+            int key,\n+            int value,\n+            RowKind rowKind,\n+            Map<String, String> dynamicOptions)\n+            throws Exception {\n         KafkaLogSerializationSchema serializer =\n-                createTestSerializationSchema(testContext(\""\"", mode, keyed));\n+                createTestSerializationSchema(testContext(\""\"", mode, keyed, dynamicOptions));\n         serializer.open(null);\n         KafkaRecordDeserializationSchema<RowData> deserializer =\n-                createTestDeserializationSchema(testContext(\""\"", mode, keyed));\n+                createTestDeserializationSchema(testContext(\""\"", mode, keyed, dynamicOptions));\n         deserializer.open(null);\n \n         SinkRecord input = testRecord(keyed, bucket, key, value, rowKind);\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java\nindex 375cbb5c43a4..ef57941c52d8 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/kafka/KafkaLogTestUtils.java\n@@ -175,7 +175,21 @@ static ResolvedCatalogTable createResolvedTable(\n \n     public static DynamicTableFactory.Context testContext(\n             String servers, LogChangelogMode changelogMode, boolean keyed) {\n-        return testContext(\""table\"", servers, changelogMode, LogConsistency.TRANSACTIONAL, keyed);\n+        return testContext(servers, changelogMode, keyed, Collections.emptyMap());\n+    }\n+\n+    public static DynamicTableFactory.Context testContext(\n+            String servers,\n+            LogChangelogMode changelogMode,\n+            boolean keyed,\n+            Map<String, String> dynamicOptions) {\n+        return testContext(\n+                \""table\"",\n+                servers,\n+                changelogMode,\n+                LogConsistency.TRANSACTIONAL,\n+                keyed,\n+                dynamicOptions);\n     }\n \n     static DynamicTableFactory.Context testContext(\n@@ -183,7 +197,8 @@ static DynamicTableFactory.Context testContext(\n             String servers,\n             LogChangelogMode changelogMode,\n             LogConsistency consistency,\n-            boolean keyed) {\n+            boolean keyed,\n+            Map<String, String> dynamicOptions) {\n         return testContext(\n                 name,\n                 servers,\n@@ -191,7 +206,7 @@ static DynamicTableFactory.Context testContext(\n                 consistency,\n                 RowType.of(new IntType(), new IntType()),\n                 keyed ? new int[] {0} : new int[0],\n-                new HashMap<>());\n+                dynamicOptions);\n     }\n \n     public static DynamicTableFactory.Context testContext(\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4944"", ""pr_id"": 4944, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 4, ""non_test_files_count"": 9, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RefreshCredentialFileIO.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/GetTableCredentialsResponse.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java""], ""base_commit"": ""43abe2da7621523bd4508bf44fa705b53ed54e35"", ""head_commit"": ""a5132834402f0f224b392a0708141e2c02cef425"", ""repo_url"": ""https://github.com/apache/paimon/pull/4944"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4944"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-20T13:43:17.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex aa93b1ba3256..72d09b785f1a 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -369,7 +369,8 @@ public Table getTable(Identifier identifier) throws TableNotExistException {\n         SnapshotCommit.Factory commitFactory =\n                 new RenamingSnapshotCommit.Factory(\n                         lockFactory().orElse(null), lockContext().orElse(null));\n-        return CatalogUtils.loadTable(this, identifier, this::loadTableMetadata, commitFactory);\n+        return CatalogUtils.loadTable(\n+                this, identifier, fileIO(), this::loadTableMetadata, commitFactory);\n     }\n \n     /**\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\nindex fbd510692c30..7282b308e744 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n@@ -171,6 +171,7 @@ public static List<Partition> listPartitionsFromFileSystem(Table table) {\n     public static Table loadTable(\n             Catalog catalog,\n             Identifier identifier,\n+            FileIO fileIO,\n             TableMetadata.Loader metadataLoader,\n             SnapshotCommit.Factory commitFactory)\n             throws Catalog.TableNotExistException {\n@@ -189,8 +190,7 @@ public static Table loadTable(\n                 new CatalogEnvironment(\n                         identifier, metadata.uuid(), catalog.catalogLoader(), commitFactory);\n         Path path = new Path(schema.options().get(PATH.key()));\n-        FileStoreTable table =\n-                FileStoreTableFactory.create(catalog.fileIO(), path, schema, catalogEnv);\n+        FileStoreTable table = FileStoreTableFactory.create(fileIO, path, schema, catalogEnv);\n \n         if (options.type() == TableType.OBJECT_TABLE) {\n             table = toObjectTable(catalog, table);\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex 87b8e8cb9612..e06ef012b87a 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -109,6 +109,7 @@ public class RESTCatalog implements Catalog {\n     private final ResourcePaths resourcePaths;\n     private final AuthSession catalogAuth;\n     private final Options options;\n+    private final boolean fileIORefreshCredentialEnable;\n     private final FileIO fileIO;\n \n     private volatile ScheduledExecutorService refreshExecutor = null;\n@@ -130,13 +131,19 @@ public RESTCatalog(CatalogContext context) {\n                                 .merge(context.options().toMap()));\n         this.resourcePaths = ResourcePaths.forCatalogProperties(options);\n \n+        this.fileIORefreshCredentialEnable =\n+                options.get(RESTCatalogOptions.FILE_IO_REFRESH_CREDENTIAL_ENABLE);\n         try {\n-            String warehouseStr = options.get(CatalogOptions.WAREHOUSE);\n-            this.fileIO =\n-                    FileIO.get(\n-                            new Path(warehouseStr),\n-                            CatalogContext.create(\n-                                    options, context.preferIO(), context.fallbackIO()));\n+            if (fileIORefreshCredentialEnable) {\n+                this.fileIO = null;\n+            } else {\n+                String warehouseStr = options.get(CatalogOptions.WAREHOUSE);\n+                this.fileIO =\n+                        FileIO.get(\n+                                new Path(warehouseStr),\n+                                CatalogContext.create(\n+                                        options, context.preferIO(), context.fallbackIO()));\n+            }\n         } catch (IOException e) {\n             LOG.warn(\""Can not get FileIO from options.\"");\n             throw new RuntimeException(e);\n@@ -149,6 +156,8 @@ protected RESTCatalog(Options options, FileIO fileIO) {\n         this.options = options;\n         this.resourcePaths = ResourcePaths.forCatalogProperties(options);\n         this.fileIO = fileIO;\n+        this.fileIORefreshCredentialEnable =\n+                options.get(RESTCatalogOptions.FILE_IO_REFRESH_CREDENTIAL_ENABLE);\n     }\n \n     @Override\n@@ -168,12 +177,20 @@ public RESTCatalogLoader catalogLoader() {\n \n     @Override\n     public FileIO fileIO() {\n+        if (fileIORefreshCredentialEnable) {\n+            throw new UnsupportedOperationException();\n+        }\n         return fileIO;\n     }\n \n     @Override\n     public FileIO fileIO(Path path) {\n-        return fileIO;\n+        try {\n+            return FileIO.get(path, CatalogContext.create(options));\n+        } catch (IOException e) {\n+            LOG.warn(\""Can not get FileIO from options.\"");\n+            throw new RuntimeException(e);\n+        }\n     }\n \n     @Override\n@@ -289,6 +306,7 @@ public Table getTable(Identifier identifier) throws TableNotExistException {\n         return CatalogUtils.loadTable(\n                 this,\n                 identifier,\n+                this.fileIO(identifier),\n                 this::loadTableMetadata,\n                 new RESTSnapshotCommitFactory(catalogLoader()));\n     }\n@@ -645,4 +663,12 @@ private ScheduledExecutorService tokenRefreshExecutor() {\n \n         return refreshExecutor;\n     }\n+\n+    private FileIO fileIO(Identifier identifier) {\n+        if (fileIORefreshCredentialEnable) {\n+            return new RefreshCredentialFileIO(\n+                    resourcePaths, catalogAuth, options, client, identifier);\n+        }\n+        return fileIO;\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\nindex 843228fa0707..61aed5f7038f 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n@@ -77,4 +77,10 @@ public class RESTCatalogOptions {\n                     .stringType()\n                     .noDefaultValue()\n                     .withDescription(\""REST Catalog auth token provider path.\"");\n+\n+    public static final ConfigOption<Boolean> FILE_IO_REFRESH_CREDENTIAL_ENABLE =\n+            ConfigOptions.key(\""file-io-refresh-credential.enabled\"")\n+                    .booleanType()\n+                    .defaultValue(false)\n+                    .withDescription(\""Whether to support file io refresh credential.\"");\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RefreshCredentialFileIO.java b/paimon-core/src/main/java/org/apache/paimon/rest/RefreshCredentialFileIO.java\nnew file mode 100644\nindex 000000000000..b55486887d55\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RefreshCredentialFileIO.java\n@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import org.apache.paimon.catalog.CatalogContext;\n+import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.fs.FileIO;\n+import org.apache.paimon.fs.FileStatus;\n+import org.apache.paimon.fs.Path;\n+import org.apache.paimon.fs.PositionOutputStream;\n+import org.apache.paimon.fs.SeekableInputStream;\n+import org.apache.paimon.options.CatalogOptions;\n+import org.apache.paimon.options.Options;\n+import org.apache.paimon.rest.auth.AuthSession;\n+import org.apache.paimon.rest.responses.GetTableCredentialsResponse;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/** A {@link FileIO} to support refresh credential. */\n+public class RefreshCredentialFileIO implements FileIO {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    private final ResourcePaths resourcePaths;\n+    private final AuthSession catalogAuth;\n+    protected Options options;\n+    private final Identifier identifier;\n+    private Long expireAtMillis;\n+    private Map<String, String> credential;\n+    private final transient RESTClient client;\n+    private transient volatile FileIO lazyFileIO;\n+\n+    public RefreshCredentialFileIO(\n+            ResourcePaths resourcePaths,\n+            AuthSession catalogAuth,\n+            Options options,\n+            RESTClient client,\n+            Identifier identifier) {\n+        this.resourcePaths = resourcePaths;\n+        this.catalogAuth = catalogAuth;\n+        this.options = options;\n+        this.identifier = identifier;\n+        this.client = client;\n+    }\n+\n+    @Override\n+    public void configure(CatalogContext context) {\n+        this.options = context.options();\n+    }\n+\n+    @Override\n+    public SeekableInputStream newInputStream(Path path) throws IOException {\n+        return fileIO().newInputStream(path);\n+    }\n+\n+    @Override\n+    public PositionOutputStream newOutputStream(Path path, boolean overwrite) throws IOException {\n+        return fileIO().newOutputStream(path, overwrite);\n+    }\n+\n+    @Override\n+    public FileStatus getFileStatus(Path path) throws IOException {\n+        return fileIO().getFileStatus(path);\n+    }\n+\n+    @Override\n+    public FileStatus[] listStatus(Path path) throws IOException {\n+        return fileIO().listStatus(path);\n+    }\n+\n+    @Override\n+    public boolean exists(Path path) throws IOException {\n+        return fileIO().exists(path);\n+    }\n+\n+    @Override\n+    public boolean delete(Path path, boolean recursive) throws IOException {\n+        return fileIO().delete(path, recursive);\n+    }\n+\n+    @Override\n+    public boolean mkdirs(Path path) throws IOException {\n+        return fileIO().mkdirs(path);\n+    }\n+\n+    @Override\n+    public boolean rename(Path src, Path dst) throws IOException {\n+        return fileIO().rename(src, dst);\n+    }\n+\n+    @Override\n+    public boolean isObjectStore() {\n+        try {\n+            return fileIO().isObjectStore();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private FileIO fileIO() throws IOException {\n+        if (lazyFileIO == null || shouldRefresh()) {\n+            synchronized (this) {\n+                if (lazyFileIO == null || shouldRefresh()) {\n+                    GetTableCredentialsResponse response = getCredential();\n+                    expireAtMillis = response.getExpiresAtMillis();\n+                    credential = response.getCredential();\n+                    Map<String, String> conf = RESTUtil.merge(options.toMap(), credential);\n+                    Options updateCredentialOption = new Options(conf);\n+                    lazyFileIO =\n+                            FileIO.get(\n+                                    new Path(updateCredentialOption.get(CatalogOptions.WAREHOUSE)),\n+                                    CatalogContext.create(updateCredentialOption));\n+                }\n+            }\n+        }\n+        return lazyFileIO;\n+    }\n+\n+    // todo: handle exception\n+    private GetTableCredentialsResponse getCredential() {\n+        return client.get(\n+                resourcePaths.tableCredentials(\n+                        identifier.getDatabaseName(), identifier.getObjectName()),\n+                GetTableCredentialsResponse.class,\n+                catalogAuth.getHeaders());\n+    }\n+\n+    private boolean shouldRefresh() {\n+        return expireAtMillis != null && expireAtMillis > System.currentTimeMillis();\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex d77475fe40dc..1e843f99cb0e 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -70,6 +70,10 @@ public String commitTable(String databaseName) {\n         return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, \""commit\"");\n     }\n \n+    public String tableCredentials(String databaseName, String tableName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""credentials\"");\n+    }\n+\n     public String partitions(String databaseName, String tableName) {\n         return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"");\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetTableCredentialsResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetTableCredentialsResponse.java\nnew file mode 100644\nindex 000000000000..2792940ff6b1\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetTableCredentialsResponse.java\n@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Response for table credentials. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class GetTableCredentialsResponse implements RESTResponse {\n+\n+    private static final String FIELD_CREDENTIAL = \""credential\"";\n+    private static final String FIELD_EXPIREAT_MILLIS = \""expiresAtMillis\"";\n+\n+    @JsonProperty(FIELD_CREDENTIAL)\n+    private final Map<String, String> credential;\n+\n+    @JsonProperty(FIELD_EXPIREAT_MILLIS)\n+    private long expiresAtMillis;\n+\n+    @JsonCreator\n+    public GetTableCredentialsResponse(\n+            @JsonProperty(FIELD_EXPIREAT_MILLIS) long expiresAtMillis,\n+            @JsonProperty(FIELD_CREDENTIAL) Map<String, String> credential) {\n+        this.expiresAtMillis = expiresAtMillis;\n+        this.credential = credential;\n+    }\n+\n+    @JsonGetter(FIELD_CREDENTIAL)\n+    public Map<String, String> getCredential() {\n+        return credential;\n+    }\n+\n+    @JsonGetter(FIELD_EXPIREAT_MILLIS)\n+    public long getExpiresAtMillis() {\n+        return expiresAtMillis;\n+    }\n+}\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 02ea7de8d0df..128514d7a59e 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -432,6 +432,43 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/tables/{table}/credentials:\n+    get:\n+      tags:\n+        - table\n+      summary: List credentials\n+      operationId: listCredentials\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetTableCredentialsResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n   /v1/{prefix}/databases/{database}/tables/{table}/partitions:\n     get:\n       tags:\n@@ -1166,6 +1203,16 @@ components:\n       properties:\n         success:\n           type: boolean\n+    GetTableCredentialsResponse:\n+      type: object\n+      properties:\n+        expiresAt:\n+          type: integer\n+          format: int64\n+        credentials:\n+          type: object\n+          additionalProperties:\n+            type: string\n     AlterDatabaseRequest:\n       type: object\n       properties:\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex e657407a47c3..c8eae97dec64 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -37,6 +37,7 @@\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetTableCredentialsResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -49,6 +50,7 @@\n import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n import io.swagger.v3.oas.annotations.Operation;\n@@ -355,6 +357,32 @@ public CommitTableResponse commitTable(\n         return new CommitTableResponse(true);\n     }\n \n+    @Operation(\n+            summary = \""List credentials\"",\n+            tags = {\""table\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(schema = @Schema(implementation = GetTableCredentialsResponse.class))\n+                }),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/credentials\"")\n+    public GetTableCredentialsResponse listCredentials(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table) {\n+        return new GetTableCredentialsResponse(\n+                System.currentTimeMillis(), ImmutableMap.of(\""key\"", \""value\""));\n+    }\n+\n     @Operation(\n             summary = \""List partitions\"",\n             tags = {\""partition\""})\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 766cb09b0bdd..a8a2d6189632 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -32,6 +32,7 @@\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetTableCredentialsResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -48,6 +49,7 @@\n import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n import java.util.ArrayList;\n@@ -248,6 +250,11 @@ public static ListViewsResponse listViewsResponse() {\n         return new ListViewsResponse(ImmutableList.of(\""view\""));\n     }\n \n+    public static GetTableCredentialsResponse getTableCredentialsResponse() {\n+        return new GetTableCredentialsResponse(\n+                System.currentTimeMillis(), ImmutableMap.of(\""key\"", \""value\""));\n+    }\n+\n     private static ViewSchema viewSchema() {\n         List<DataField> fields =\n                 Arrays.asList(\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 91024867b7ea..16de35969dc6 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -42,6 +42,7 @@\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetTableCredentialsResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -63,6 +64,7 @@\n import okhttp3.mockwebserver.MockResponse;\n import okhttp3.mockwebserver.MockWebServer;\n import okhttp3.mockwebserver.RecordedRequest;\n+import org.testcontainers.shaded.com.google.common.collect.ImmutableMap;\n \n import java.io.IOException;\n import java.util.List;\n@@ -149,6 +151,10 @@ public MockResponse dispatch(RecordedRequest request) {\n                                 resources.length == 3\n                                         && \""tables\"".equals(resources[1])\n                                         && \""commit\"".equals(resources[2]);\n+                        boolean isTableCredentials =\n+                                resources.length == 4\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""credentials\"".equals(resources[3]);\n                         boolean isPartitions =\n                                 resources.length == 4\n                                         && \""tables\"".equals(resources[1])\n@@ -202,6 +208,16 @@ public MockResponse dispatch(RecordedRequest request) {\n                         } else if (isPartitions) {\n                             String tableName = resources[2];\n                             return partitionsApiHandler(catalog, request, databaseName, tableName);\n+                        } else if (isTableCredentials) {\n+                            GetTableCredentialsResponse getTableCredentialsResponse =\n+                                    new GetTableCredentialsResponse(\n+                                            System.currentTimeMillis(),\n+                                            ImmutableMap.of(\""key\"", \""value\""));\n+                            return new MockResponse()\n+                                    .setResponseCode(200)\n+                                    .setBody(\n+                                            OBJECT_MAPPER.writeValueAsString(\n+                                                    getTableCredentialsResponse));\n                         } else if (isTableRename) {\n                             return renameTableApiHandler(catalog, request);\n                         } else if (isTableCommit) {\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex d1ce64b6c543..752f492078b7 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -26,6 +26,7 @@\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.exceptions.NotAuthorizedException;\n import org.apache.paimon.schema.Schema;\n+import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataTypes;\n \n@@ -49,12 +50,12 @@\n class RESTCatalogTest extends CatalogTestBase {\n \n     private RESTCatalogServer restCatalogServer;\n+    private String initToken = \""init_token\"";\n \n     @BeforeEach\n     @Override\n     public void setUp() throws Exception {\n         super.setUp();\n-        String initToken = \""init_token\"";\n         restCatalogServer = new RESTCatalogServer(warehouse, initToken);\n         restCatalogServer.start();\n         Options options = new Options();\n@@ -107,6 +108,26 @@ void testListPartitionsFromFile() throws Exception {\n         assertEquals(0, result.size());\n     }\n \n+    @Test\n+    void testRefreshFileIO() throws Exception {\n+        Options options = new Options();\n+        options.set(RESTCatalogOptions.URI, restCatalogServer.getUrl());\n+        options.set(RESTCatalogOptions.TOKEN, initToken);\n+        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n+        options.set(RESTCatalogOptions.FILE_IO_REFRESH_CREDENTIAL_ENABLE, true);\n+        this.catalog = new RESTCatalog(CatalogContext.create(options));\n+        List<Identifier> identifiers =\n+                Lists.newArrayList(\n+                        Identifier.create(\""test_db_a\"", \""test_table_a\""),\n+                        Identifier.create(\""test_db_b\"", \""test_table_b\""),\n+                        Identifier.create(\""test_db_c\"", \""test_table_c\""));\n+        for (Identifier identifier : identifiers) {\n+            createTable(identifier, Maps.newHashMap(), Lists.newArrayList(\""col1\""));\n+            FileStoreTable fileStoreTable = (FileStoreTable) catalog.getTable(identifier);\n+            assertEquals(true, fileStoreTable.fileIO().exists(fileStoreTable.location()));\n+        }\n+    }\n+\n     @Override\n     protected boolean supportsFormatTable() {\n         return true;\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex fa56f8111828..4c3b622a8c7f 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -32,6 +32,7 @@\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetTableCredentialsResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -273,4 +274,14 @@ public void listViewsResponseParseTest() throws Exception {\n         ListViewsResponse parseData = OBJECT_MAPPER.readValue(responseStr, ListViewsResponse.class);\n         assertEquals(response.getViews(), parseData.getViews());\n     }\n+\n+    @Test\n+    public void getTableCredentialsResponseParseTest() throws Exception {\n+        GetTableCredentialsResponse response = MockRESTMessage.getTableCredentialsResponse();\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        GetTableCredentialsResponse parseData =\n+                OBJECT_MAPPER.readValue(responseStr, GetTableCredentialsResponse.class);\n+        assertEquals(response.getCredential(), parseData.getCredential());\n+        assertEquals(response.getExpiresAtMillis(), parseData.getExpiresAtMillis());\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4929"", ""pr_id"": 4929, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 2, ""non_test_files_count"": 4, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptor.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClientOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptorTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptorTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java""], ""base_commit"": ""98c9f9acec0d0e28c3115e7009997bd3d07b7c19"", ""head_commit"": ""17c434e44f5e7f443def06a440e27efcb21ec86f"", ""repo_url"": ""https://github.com/apache/paimon/pull/4929"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4929"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-17T05:09:33.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptor.java b/paimon-core/src/main/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptor.java\nnew file mode 100644\nindex 000000000000..dd16e47fc580\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptor.java\n@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableSet;\n+import org.apache.paimon.shade.guava30.com.google.common.net.HttpHeaders;\n+\n+import okhttp3.Interceptor;\n+import okhttp3.Request;\n+import okhttp3.Response;\n+\n+import javax.net.ssl.SSLException;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.net.ConnectException;\n+import java.net.NoRouteToHostException;\n+import java.net.UnknownHostException;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+/**\n+ * Defines exponential HTTP request retry interceptor.\n+ *\n+ * <p>The following retrievable IOException\n+ *\n+ * <ul>\n+ *   <li>InterruptedIOException\n+ *   <li>UnknownHostException\n+ *   <li>ConnectException\n+ *   <li>NoRouteToHostException\n+ *   <li>SSLException\n+ * </ul>\n+ *\n+ * <p>The following retrievable HTTP status codes are defined:\n+ *\n+ * <ul>\n+ *   <li>TOO_MANY_REQUESTS (429)\n+ *   <li>BAD_GATEWAY (502)\n+ *   <li>SERVICE_UNAVAILABLE (503)\n+ *   <li>GATEWAY_TIMEOUT (504)\n+ * </ul>\n+ *\n+ * <p>The following retrievable HTTP method which is idempotent are defined:\n+ *\n+ * <ul>\n+ *   <li>GET\n+ *   <li>HEAD\n+ *   <li>PUT\n+ *   <li>DELETE\n+ *   <li>TRACE\n+ *   <li>OPTIONS\n+ * </ul>\n+ */\n+public class ExponentialHttpRetryInterceptor implements Interceptor {\n+\n+    private final int maxRetries;\n+    private final Set<Class<? extends IOException>> nonRetriableExceptions;\n+    private final Set<Integer> retrievableCodes;\n+    private final Set<String> retrievableMethods;\n+\n+    public ExponentialHttpRetryInterceptor(int maxRetries) {\n+        this.maxRetries = maxRetries;\n+        this.retrievableMethods =\n+                ImmutableSet.of(\""GET\"", \""HEAD\"", \""PUT\"", \""DELETE\"", \""TRACE\"", \""OPTIONS\"");\n+        this.retrievableCodes = ImmutableSet.of(429, 502, 503, 504);\n+        this.nonRetriableExceptions =\n+                ImmutableSet.of(\n+                        InterruptedIOException.class,\n+                        UnknownHostException.class,\n+                        ConnectException.class,\n+                        NoRouteToHostException.class,\n+                        SSLException.class);\n+    }\n+\n+    @Override\n+    public Response intercept(Chain chain) throws IOException {\n+        Request request = chain.request();\n+        Response response = null;\n+\n+        for (int retryCount = 1; ; retryCount++) {\n+            try {\n+                response = chain.proceed(request);\n+            } catch (IOException e) {\n+                if (needRetry(request.method(), e, retryCount)) {\n+                    wait(response, retryCount);\n+                    continue;\n+                }\n+            }\n+            if (needRetry(response, retryCount)) {\n+                if (response != null) {\n+                    response.close();\n+                }\n+                wait(response, retryCount);\n+            } else {\n+                return response;\n+            }\n+        }\n+    }\n+\n+    public boolean needRetry(Response response, int execCount) {\n+        if (execCount > maxRetries) {\n+            return false;\n+        }\n+        return response == null\n+                || (!response.isSuccessful() && retrievableCodes.contains(response.code()));\n+    }\n+\n+    public boolean needRetry(String method, IOException e, int execCount) {\n+        if (execCount > maxRetries) {\n+            return false;\n+        }\n+        if (!retrievableMethods.contains(method)) {\n+            return false;\n+        }\n+        if (nonRetriableExceptions.contains(e.getClass())) {\n+            return false;\n+        } else {\n+            for (Class<? extends IOException> rejectException : nonRetriableExceptions) {\n+                if (rejectException.isInstance(e)) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    public long getRetryIntervalInMilliseconds(Response response, int execCount) {\n+        // a server may send a 429 / 503 with a Retry-After header\n+        // https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n+        String retryAfterStrInSecond =\n+                response == null ? null : response.header(HttpHeaders.RETRY_AFTER);\n+        Long retryAfter = null;\n+        if (retryAfterStrInSecond != null) {\n+            try {\n+                retryAfter = Long.parseLong(retryAfterStrInSecond) * 1000;\n+            } catch (Throwable ignore) {\n+            }\n+\n+            if (retryAfter != null && retryAfter > 0) {\n+                return retryAfter;\n+            }\n+        }\n+\n+        int delayMillis = 1000 * (int) Math.min(Math.pow(2.0, (long) execCount - 1.0), 64.0);\n+        int jitter = ThreadLocalRandom.current().nextInt(Math.max(1, (int) (delayMillis * 0.1)));\n+\n+        return delayMillis + jitter;\n+    }\n+\n+    private void wait(Response response, int retryCount) throws InterruptedIOException {\n+        try {\n+            Thread.sleep(getRetryIntervalInMilliseconds(response, retryCount));\n+        } catch (final InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new InterruptedIOException();\n+        }\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\nindex 08d6c8a050a0..5a13a51ef7fa 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n@@ -26,6 +26,7 @@\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n \n+import okhttp3.ConnectionPool;\n import okhttp3.Dispatcher;\n import okhttp3.Headers;\n import okhttp3.MediaType;\n@@ -40,6 +41,7 @@\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.TimeUnit;\n \n import static okhttp3.ConnectionSpec.CLEARTEXT;\n import static okhttp3.ConnectionSpec.COMPATIBLE_TLS;\n@@ -52,6 +54,7 @@ public class HttpClient implements RESTClient {\n \n     private static final String THREAD_NAME = \""REST-CATALOG-HTTP-CLIENT-THREAD-POOL\"";\n     private static final MediaType MEDIA_TYPE = MediaType.parse(\""application/json\"");\n+    private static final int CONNECTION_KEEP_ALIVE_DURATION_MS = 300_000;\n \n     private final OkHttpClient okHttpClient;\n     private final String uri;\n@@ -191,14 +194,30 @@ private static OkHttpClient createHttpClient(HttpClientOptions httpClientOptions\n         BlockingQueue<Runnable> workQueue = new SynchronousQueue<>();\n         ExecutorService executorService =\n                 createCachedThreadPool(httpClientOptions.threadPoolSize(), THREAD_NAME, workQueue);\n-\n+        ConnectionPool connectionPool =\n+                new ConnectionPool(\n+                        httpClientOptions.maxConnections(),\n+                        CONNECTION_KEEP_ALIVE_DURATION_MS,\n+                        TimeUnit.MILLISECONDS);\n+        Dispatcher dispatcher = new Dispatcher(executorService);\n+        // set max requests per host use max connections\n+        dispatcher.setMaxRequestsPerHost(httpClientOptions.maxConnections());\n         OkHttpClient.Builder builder =\n                 new OkHttpClient.Builder()\n-                        .dispatcher(new Dispatcher(executorService))\n+                        .dispatcher(dispatcher)\n                         .retryOnConnectionFailure(true)\n-                        .connectionSpecs(Arrays.asList(MODERN_TLS, COMPATIBLE_TLS, CLEARTEXT));\n-        httpClientOptions.connectTimeout().ifPresent(builder::connectTimeout);\n-        httpClientOptions.readTimeout().ifPresent(builder::readTimeout);\n+                        .connectionPool(connectionPool)\n+                        .connectionSpecs(Arrays.asList(MODERN_TLS, COMPATIBLE_TLS, CLEARTEXT))\n+                        .addInterceptor(\n+                                new ExponentialHttpRetryInterceptor(\n+                                        httpClientOptions.maxRetries()));\n+        httpClientOptions\n+                .connectTimeout()\n+                .ifPresent(\n+                        timeoutDuration -> {\n+                            builder.connectTimeout(timeoutDuration);\n+                            builder.readTimeout(timeoutDuration);\n+                        });\n \n         return builder.build();\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClientOptions.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClientOptions.java\nindex 00ae1a529e89..548a98956821 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClientOptions.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClientOptions.java\n@@ -30,26 +30,30 @@ public class HttpClientOptions {\n \n     private final String uri;\n     @Nullable private final Duration connectTimeout;\n-    @Nullable private final Duration readTimeout;\n     private final int threadPoolSize;\n+    private final int maxConnections;\n+    private final int maxRetries;\n \n     public HttpClientOptions(\n             String uri,\n             @Nullable Duration connectTimeout,\n-            @Nullable Duration readTimeout,\n-            int threadPoolSize) {\n+            int threadPoolSize,\n+            int maxConnections,\n+            int maxRetries) {\n         this.uri = uri;\n         this.connectTimeout = connectTimeout;\n-        this.readTimeout = readTimeout;\n         this.threadPoolSize = threadPoolSize;\n+        this.maxConnections = maxConnections;\n+        this.maxRetries = maxRetries;\n     }\n \n     public static HttpClientOptions create(Options options) {\n         return new HttpClientOptions(\n                 options.get(RESTCatalogOptions.URI),\n                 options.get(RESTCatalogOptions.CONNECTION_TIMEOUT),\n-                options.get(RESTCatalogOptions.READ_TIMEOUT),\n-                options.get(RESTCatalogOptions.THREAD_POOL_SIZE));\n+                options.get(RESTCatalogOptions.THREAD_POOL_SIZE),\n+                options.get(RESTCatalogOptions.MAX_CONNECTIONS),\n+                options.get(RESTCatalogOptions.MAX_RETIES));\n     }\n \n     public String uri() {\n@@ -60,11 +64,15 @@ public Optional<Duration> connectTimeout() {\n         return Optional.ofNullable(connectTimeout);\n     }\n \n-    public Optional<Duration> readTimeout() {\n-        return Optional.ofNullable(readTimeout);\n-    }\n-\n     public int threadPoolSize() {\n         return threadPoolSize;\n     }\n+\n+    public int maxConnections() {\n+        return maxConnections;\n+    }\n+\n+    public int maxRetries() {\n+        return Math.max(maxRetries, 0);\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\nindex 1af64def4f71..843228fa0707 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogOptions.java\n@@ -35,14 +35,20 @@ public class RESTCatalogOptions {\n     public static final ConfigOption<Duration> CONNECTION_TIMEOUT =\n             ConfigOptions.key(\""rest.client.connection-timeout\"")\n                     .durationType()\n-                    .noDefaultValue()\n+                    .defaultValue(Duration.ofSeconds(180))\n                     .withDescription(\""REST Catalog http client connect timeout.\"");\n \n-    public static final ConfigOption<Duration> READ_TIMEOUT =\n-            ConfigOptions.key(\""rest.client.read-timeout\"")\n-                    .durationType()\n-                    .noDefaultValue()\n-                    .withDescription(\""REST Catalog http client read timeout.\"");\n+    public static final ConfigOption<Integer> MAX_CONNECTIONS =\n+            ConfigOptions.key(\""rest.client.max-connections\"")\n+                    .intType()\n+                    .defaultValue(100)\n+                    .withDescription(\""REST Catalog http client's max connections.\"");\n+\n+    public static final ConfigOption<Integer> MAX_RETIES =\n+            ConfigOptions.key(\""rest.client.max-retries\"")\n+                    .intType()\n+                    .defaultValue(5)\n+                    .withDescription(\""REST Catalog http client's max retry times.\"");\n \n     public static final ConfigOption<Integer> THREAD_POOL_SIZE =\n             ConfigOptions.key(\""rest.client.num-threads\"")\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptorTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptorTest.java\nnew file mode 100644\nindex 000000000000..6510371f2d27\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/ExponentialHttpRetryInterceptorTest.java\n@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import okhttp3.Protocol;\n+import okhttp3.Request;\n+import okhttp3.Response;\n+import org.apache.hc.core5.http.HttpHeaders;\n+import org.junit.jupiter.api.Test;\n+\n+import javax.net.ssl.SSLException;\n+\n+import java.io.IOException;\n+import java.io.InterruptedIOException;\n+import java.net.ConnectException;\n+import java.net.NoRouteToHostException;\n+import java.net.UnknownHostException;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/** Test for {@link ExponentialHttpRetryInterceptor}. */\n+class ExponentialHttpRetryInterceptorTest {\n+\n+    private final int maxRetries = 5;\n+    private final ExponentialHttpRetryInterceptor interceptor =\n+            new ExponentialHttpRetryInterceptor(maxRetries);\n+\n+    @Test\n+    void testNeedRetryByMethod() {\n+\n+        assertThat(interceptor.needRetry(\""GET\"", new IOException(), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""HEAD\"", new IOException(), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""PUT\"", new IOException(), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""DELETE\"", new IOException(), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""TRACE\"", new IOException(), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""OPTIONS\"", new IOException(), 1)).isTrue();\n+\n+        assertThat(interceptor.needRetry(\""POST\"", new IOException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""PATCH\"", new IOException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""CONNECT\"", new IOException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""GET\"", new IOException(), maxRetries + 1)).isFalse();\n+    }\n+\n+    @Test\n+    void testNeedRetryByException() {\n+\n+        assertThat(interceptor.needRetry(\""GET\"", new InterruptedIOException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""GET\"", new UnknownHostException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""GET\"", new ConnectException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""GET\"", new NoRouteToHostException(), 1)).isFalse();\n+        assertThat(interceptor.needRetry(\""GET\"", new SSLException(\""error\""), 1)).isFalse();\n+\n+        assertThat(interceptor.needRetry(\""GET\"", new IOException(\""error\""), 1)).isTrue();\n+        assertThat(interceptor.needRetry(\""GET\"", new IOException(\""error\""), maxRetries + 1))\n+                .isFalse();\n+    }\n+\n+    @Test\n+    void testRetryByResponse() {\n+\n+        assertThat(interceptor.needRetry(createResponse(429), 1)).isTrue();\n+        assertThat(interceptor.needRetry(createResponse(503), 1)).isTrue();\n+        assertThat(interceptor.needRetry(createResponse(502), 1)).isTrue();\n+        assertThat(interceptor.needRetry(createResponse(504), 1)).isTrue();\n+\n+        assertThat(interceptor.needRetry(createResponse(500), 1)).isFalse();\n+        assertThat(interceptor.needRetry(createResponse(404), 1)).isFalse();\n+        assertThat(interceptor.needRetry(createResponse(200), 1)).isFalse();\n+    }\n+\n+    @Test\n+    void invalidRetryAfterHeader() {\n+        Response response = createResponse(429, \""Stuff\"");\n+\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 3)).isBetween(4000L, 5000L);\n+    }\n+\n+    @Test\n+    void validRetryAfterHeader() {\n+        long retryAfter = 3;\n+        Response response = createResponse(429, retryAfter + \""\"");\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 3))\n+                .isEqualTo(retryAfter * 1000);\n+    }\n+\n+    @Test\n+    void exponentialRetry() {\n+        ExponentialHttpRetryInterceptor interceptor = new ExponentialHttpRetryInterceptor(10);\n+        Response response = createResponse(429, \""Stuff\"");\n+\n+        // note that the upper limit includes ~10% variability\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 0)).isEqualTo(0);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 1)).isBetween(1000L, 2000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 2)).isBetween(2000L, 3000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 3)).isBetween(4000L, 5000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 4)).isBetween(8000L, 9000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 5))\n+                .isBetween(16000L, 18000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 6))\n+                .isBetween(32000L, 36000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 7))\n+                .isBetween(64000L, 72000L);\n+        assertThat(interceptor.getRetryIntervalInMilliseconds(response, 10))\n+                .isBetween(64000L, 72000L);\n+    }\n+\n+    private static Response createResponse(int httpCode) {\n+        return createResponse(httpCode, \""\"");\n+    }\n+\n+    private static Response createResponse(int httpCode, String retryAfter) {\n+        return new Response.Builder()\n+                .code(httpCode)\n+                .message(\""message\"")\n+                .protocol(Protocol.HTTP_1_1)\n+                .request(new Request.Builder().url(\""http://localhost\"").build())\n+                .addHeader(HttpHeaders.RETRY_AFTER, retryAfter)\n+                .build();\n+    }\n+}\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\nindex 161dbaf3bb50..05078cf805f7 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n@@ -56,8 +56,7 @@ public void setUp() throws Exception {\n         server.start();\n         errorHandler = DefaultErrorHandler.getInstance();\n         HttpClientOptions httpClientOptions =\n-                new HttpClientOptions(\n-                        server.getBaseUrl(), Duration.ofSeconds(3), Duration.ofSeconds(3), 1);\n+                new HttpClientOptions(server.getBaseUrl(), Duration.ofSeconds(3), 1, 10, 2);\n         mockResponseData = new MockRESTData(MOCK_PATH);\n         mockResponseDataStr = server.createResponseBody(mockResponseData);\n         errorResponseStr =\n@@ -116,4 +115,15 @@ public void testDeleteFail() {\n         server.enqueueResponse(errorResponseStr, 400);\n         assertThrows(BadRequestException.class, () -> httpClient.delete(MOCK_PATH, headers));\n     }\n+\n+    @Test\n+    public void testRetry() {\n+        HttpClient httpClient =\n+                new HttpClient(\n+                        new HttpClientOptions(\n+                                server.getBaseUrl(), Duration.ofSeconds(30), 1, 10, 2));\n+        server.enqueueResponse(mockResponseDataStr, 429);\n+        server.enqueueResponse(mockResponseDataStr, 200);\n+        assertDoesNotThrow(() -> httpClient.get(MOCK_PATH, MockRESTData.class, headers));\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4908"", ""pr_id"": 4908, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 6, ""non_test_files_count"": 12, ""pr_changed_files"": [""paimon-common/src/main/java/org/apache/paimon/types/RowType.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateViewRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/RenameTableRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/GetViewResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ListViewsResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/view/ViewImpl.java"", ""paimon-core/src/main/java/org/apache/paimon/view/ViewSchema.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java""], ""base_commit"": ""1a5915d7d813c4fc5e04fd08e14deb12adf413fe"", ""head_commit"": ""90e0d48d1cf33112ce1629e4095f9b6a3313cc50"", ""repo_url"": ""https://github.com/apache/paimon/pull/4908"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4908"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-15T05:23:25.000Z"", ""patch"": ""diff --git a/paimon-common/src/main/java/org/apache/paimon/types/RowType.java b/paimon-common/src/main/java/org/apache/paimon/types/RowType.java\nindex 681a07af584d..d0c0881b6aa3 100644\n--- a/paimon-common/src/main/java/org/apache/paimon/types/RowType.java\n+++ b/paimon-common/src/main/java/org/apache/paimon/types/RowType.java\n@@ -24,6 +24,8 @@\n import org.apache.paimon.utils.Preconditions;\n import org.apache.paimon.utils.StringUtils;\n \n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonGenerator;\n \n import java.io.IOException;\n@@ -52,6 +54,8 @@ public final class RowType extends DataType {\n \n     private static final long serialVersionUID = 1L;\n \n+    private static final String FIELD_FIELDS = \""fields\"";\n+\n     public static final String FORMAT = \""ROW<%s>\"";\n \n     private final List<DataField> fields;\n@@ -67,7 +71,8 @@ public RowType(boolean isNullable, List<DataField> fields) {\n         validateFields(fields);\n     }\n \n-    public RowType(List<DataField> fields) {\n+    @JsonCreator\n+    public RowType(@JsonProperty(FIELD_FIELDS) List<DataField> fields) {\n         this(true, fields);\n     }\n \n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex b2bbcbb7673a..87b8e8cb9612 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -45,6 +45,7 @@\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n@@ -55,16 +56,22 @@\n import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.ListViewsResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.TableSchema;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.sink.BatchWriteBuilder;\n+import org.apache.paimon.types.RowType;\n import org.apache.paimon.utils.Pair;\n+import org.apache.paimon.view.View;\n+import org.apache.paimon.view.ViewImpl;\n+import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n \n@@ -290,8 +297,7 @@ public boolean commitSnapshot(Identifier identifier, Snapshot snapshot) {\n         CommitTableRequest request = new CommitTableRequest(identifier, snapshot);\n         CommitTableResponse response =\n                 client.post(\n-                        resourcePaths.commitTable(\n-                                identifier.getDatabaseName(), identifier.getTableName()),\n+                        resourcePaths.commitTable(identifier.getDatabaseName()),\n                         request,\n                         CommitTableResponse.class,\n                         headers());\n@@ -325,11 +331,7 @@ public void createTable(Identifier identifier, Schema schema, boolean ignoreIfEx\n             checkNotSystemTable(identifier, \""createTable\"");\n             validateAutoCreateClose(schema.options());\n             CreateTableRequest request = new CreateTableRequest(identifier, schema);\n-            client.post(\n-                    resourcePaths.tables(identifier.getDatabaseName()),\n-                    request,\n-                    GetTableResponse.class,\n-                    headers());\n+            client.post(resourcePaths.tables(identifier.getDatabaseName()), request, headers());\n         } catch (AlreadyExistsException e) {\n             if (!ignoreIfExists) {\n                 throw new TableAlreadyExistException(identifier);\n@@ -353,13 +355,8 @@ public void renameTable(Identifier fromTable, Identifier toTable, boolean ignore\n         checkNotSystemTable(fromTable, \""renameTable\"");\n         checkNotSystemTable(toTable, \""renameTable\"");\n         try {\n-            RenameTableRequest request = new RenameTableRequest(toTable);\n-            client.post(\n-                    resourcePaths.renameTable(\n-                            fromTable.getDatabaseName(), fromTable.getTableName()),\n-                    request,\n-                    GetTableResponse.class,\n-                    headers());\n+            RenameTableRequest request = new RenameTableRequest(fromTable, toTable);\n+            client.post(resourcePaths.renameTable(fromTable.getDatabaseName()), request, headers());\n         } catch (NoSuchResourceException e) {\n             if (!ignoreIfNotExists) {\n                 throw new TableNotExistException(fromTable);\n@@ -381,7 +378,6 @@ public void alterTable(\n             client.post(\n                     resourcePaths.table(identifier.getDatabaseName(), identifier.getTableName()),\n                     request,\n-                    GetTableResponse.class,\n                     headers());\n         } catch (NoSuchResourceException e) {\n             if (!ignoreIfNotExists) {\n@@ -532,6 +528,88 @@ public List<Partition> listPartitions(Identifier identifier) throws TableNotExis\n         return response.getPartitions();\n     }\n \n+    @Override\n+    public View getView(Identifier identifier) throws ViewNotExistException {\n+        try {\n+            GetViewResponse response =\n+                    client.get(\n+                            resourcePaths.view(\n+                                    identifier.getDatabaseName(), identifier.getTableName()),\n+                            GetViewResponse.class,\n+                            headers());\n+            return new ViewImpl(\n+                    identifier,\n+                    response.getSchema().rowType(),\n+                    response.getSchema().query(),\n+                    response.getSchema().comment(),\n+                    response.getSchema().options());\n+        } catch (NoSuchResourceException e) {\n+            throw new ViewNotExistException(identifier);\n+        }\n+    }\n+\n+    @Override\n+    public void dropView(Identifier identifier, boolean ignoreIfNotExists)\n+            throws ViewNotExistException {\n+        try {\n+            client.delete(\n+                    resourcePaths.view(identifier.getDatabaseName(), identifier.getTableName()),\n+                    headers());\n+        } catch (NoSuchResourceException e) {\n+            if (!ignoreIfNotExists) {\n+                throw new ViewNotExistException(identifier);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public void createView(Identifier identifier, View view, boolean ignoreIfExists)\n+            throws ViewAlreadyExistException, DatabaseNotExistException {\n+        try {\n+            ViewSchema schema =\n+                    new ViewSchema(\n+                            new RowType(view.rowType().getFields()),\n+                            view.options(),\n+                            view.comment().orElse(null),\n+                            view.query());\n+            CreateViewRequest request = new CreateViewRequest(identifier, schema);\n+            client.post(resourcePaths.views(identifier.getDatabaseName()), request, headers());\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(identifier.getDatabaseName());\n+        } catch (AlreadyExistsException e) {\n+            if (!ignoreIfExists) {\n+                throw new ViewAlreadyExistException(identifier);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public List<String> listViews(String databaseName) throws DatabaseNotExistException {\n+        try {\n+            ListViewsResponse response =\n+                    client.get(\n+                            resourcePaths.views(databaseName), ListViewsResponse.class, headers());\n+            return response.getViews();\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(databaseName);\n+        }\n+    }\n+\n+    @Override\n+    public void renameView(Identifier fromView, Identifier toView, boolean ignoreIfNotExists)\n+            throws ViewNotExistException, ViewAlreadyExistException {\n+        try {\n+            RenameTableRequest request = new RenameTableRequest(fromView, toView);\n+            client.post(resourcePaths.renameView(fromView.getDatabaseName()), request, headers());\n+        } catch (NoSuchResourceException e) {\n+            if (!ignoreIfNotExists) {\n+                throw new ViewNotExistException(fromView);\n+            }\n+        } catch (AlreadyExistsException e) {\n+            throw new ViewAlreadyExistException(toView);\n+        }\n+    }\n+\n     @Override\n     public boolean caseSensitive() {\n         return options.getOptional(CASE_SENSITIVE).orElse(true);\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex a41dad25df56..d77475fe40dc 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -62,12 +62,12 @@ public String table(String databaseName, String tableName) {\n         return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName);\n     }\n \n-    public String renameTable(String databaseName, String tableName) {\n-        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""rename\"");\n+    public String renameTable(String databaseName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, \""rename\"");\n     }\n \n-    public String commitTable(String databaseName, String tableName) {\n-        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""commit\"");\n+    public String commitTable(String databaseName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, \""commit\"");\n     }\n \n     public String partitions(String databaseName, String tableName) {\n@@ -88,4 +88,16 @@ public String markDonePartitions(String databaseName, String tableName) {\n         return SLASH.join(\n                 V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"", \""mark\"");\n     }\n+\n+    public String views(String databaseName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, \""views\"");\n+    }\n+\n+    public String view(String databaseName, String viewName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, \""views\"", viewName);\n+    }\n+\n+    public String renameView(String databaseName) {\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, \""views\"", \""rename\"");\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateViewRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateViewRequest.java\nnew file mode 100644\nindex 000000000000..0951f822a96d\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateViewRequest.java\n@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.rest.RESTRequest;\n+import org.apache.paimon.view.ViewSchema;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+/** Request for creating view. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class CreateViewRequest implements RESTRequest {\n+\n+    private static final String FIELD_IDENTIFIER = \""identifier\"";\n+    private static final String FIELD_SCHEMA = \""schema\"";\n+\n+    @JsonProperty(FIELD_IDENTIFIER)\n+    private final Identifier identifier;\n+\n+    @JsonProperty(FIELD_SCHEMA)\n+    private final ViewSchema schema;\n+\n+    @JsonCreator\n+    public CreateViewRequest(\n+            @JsonProperty(FIELD_IDENTIFIER) Identifier identifier,\n+            @JsonProperty(FIELD_SCHEMA) ViewSchema schema) {\n+        this.schema = schema;\n+        this.identifier = identifier;\n+    }\n+\n+    @JsonGetter(FIELD_IDENTIFIER)\n+    public Identifier getIdentifier() {\n+        return identifier;\n+    }\n+\n+    @JsonGetter(FIELD_SCHEMA)\n+    public ViewSchema getSchema() {\n+        return schema;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/RenameTableRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/RenameTableRequest.java\nindex fd2eb4f9518b..483e26538e5e 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/requests/RenameTableRequest.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/RenameTableRequest.java\n@@ -26,22 +26,34 @@\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n-/** Request for renaming table. */\n+/** Request for renaming. */\n @JsonIgnoreProperties(ignoreUnknown = true)\n public class RenameTableRequest implements RESTRequest {\n \n-    private static final String FIELD_NEW_IDENTIFIER_NAME = \""newIdentifier\"";\n+    private static final String FIELD_SOURCE = \""source\"";\n+    private static final String FIELD_DESTINATION = \""destination\"";\n \n-    @JsonProperty(FIELD_NEW_IDENTIFIER_NAME)\n-    private final Identifier newIdentifier;\n+    @JsonProperty(FIELD_SOURCE)\n+    private final Identifier source;\n+\n+    @JsonProperty(FIELD_DESTINATION)\n+    private final Identifier destination;\n \n     @JsonCreator\n-    public RenameTableRequest(@JsonProperty(FIELD_NEW_IDENTIFIER_NAME) Identifier newIdentifier) {\n-        this.newIdentifier = newIdentifier;\n+    public RenameTableRequest(\n+            @JsonProperty(FIELD_SOURCE) Identifier source,\n+            @JsonProperty(FIELD_DESTINATION) Identifier destination) {\n+        this.source = source;\n+        this.destination = destination;\n+    }\n+\n+    @JsonGetter(FIELD_DESTINATION)\n+    public Identifier getDestination() {\n+        return destination;\n     }\n \n-    @JsonGetter(FIELD_NEW_IDENTIFIER_NAME)\n-    public Identifier getNewIdentifier() {\n-        return newIdentifier;\n+    @JsonGetter(FIELD_SOURCE)\n+    public Identifier getSource() {\n+        return source;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java\nindex 590f38e720d4..5dc6cffade1a 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java\n@@ -23,4 +23,5 @@ public enum ErrorResponseResourceType {\n     DATABASE,\n     TABLE,\n     COLUMN,\n+    VIEW\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetViewResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetViewResponse.java\nnew file mode 100644\nindex 000000000000..7fe1237691b1\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetViewResponse.java\n@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+import org.apache.paimon.view.ViewSchema;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+/** Response for getting view. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class GetViewResponse implements RESTResponse {\n+\n+    private static final String FIELD_ID = \""id\"";\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_SCHEMA = \""schema\"";\n+\n+    @JsonProperty(FIELD_ID)\n+    private final String id;\n+\n+    @JsonProperty(FIELD_NAME)\n+    private final String name;\n+\n+    @JsonProperty(FIELD_SCHEMA)\n+    private final ViewSchema schema;\n+\n+    @JsonCreator\n+    public GetViewResponse(\n+            @JsonProperty(FIELD_ID) String id,\n+            @JsonProperty(FIELD_NAME) String name,\n+            @JsonProperty(FIELD_SCHEMA) ViewSchema schema) {\n+        this.id = id;\n+        this.name = name;\n+        this.schema = schema;\n+    }\n+\n+    @JsonGetter(FIELD_ID)\n+    public String getId() {\n+        return this.id;\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String getName() {\n+        return this.name;\n+    }\n+\n+    @JsonGetter(FIELD_SCHEMA)\n+    public ViewSchema getSchema() {\n+        return this.schema;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListViewsResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListViewsResponse.java\nnew file mode 100644\nindex 000000000000..d785fd68c2b8\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListViewsResponse.java\n@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Response for listing tables. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ListViewsResponse implements RESTResponse {\n+\n+    private static final String FIELD_VIEWS = \""views\"";\n+\n+    @JsonProperty(FIELD_VIEWS)\n+    private final List<String> views;\n+\n+    @JsonCreator\n+    public ListViewsResponse(@JsonProperty(FIELD_VIEWS) List<String> views) {\n+        this.views = views;\n+    }\n+\n+    @JsonGetter(FIELD_VIEWS)\n+    public List<String> getViews() {\n+        return this.views;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/view/ViewImpl.java b/paimon-core/src/main/java/org/apache/paimon/view/ViewImpl.java\nindex 1cd48d4ce445..8edb517d93ec 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/view/ViewImpl.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/view/ViewImpl.java\n@@ -21,6 +21,8 @@\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.types.RowType;\n \n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+\n import javax.annotation.Nullable;\n \n import java.util.HashMap;\n@@ -29,13 +31,11 @@\n import java.util.Optional;\n \n /** Implementation of {@link View}. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n public class ViewImpl implements View {\n \n     private final Identifier identifier;\n-    private final RowType rowType;\n-    private final String query;\n-    @Nullable private final String comment;\n-    private final Map<String, String> options;\n+    private final ViewSchema viewSchema;\n \n     public ViewImpl(\n             Identifier identifier,\n@@ -44,10 +44,7 @@ public ViewImpl(\n             @Nullable String comment,\n             Map<String, String> options) {\n         this.identifier = identifier;\n-        this.rowType = rowType;\n-        this.query = query;\n-        this.comment = comment;\n-        this.options = options;\n+        this.viewSchema = new ViewSchema(query, comment, options, rowType);\n     }\n \n     @Override\n@@ -62,29 +59,29 @@ public String fullName() {\n \n     @Override\n     public RowType rowType() {\n-        return rowType;\n+        return this.viewSchema.rowType();\n     }\n \n     @Override\n     public String query() {\n-        return query;\n+        return this.viewSchema.query();\n     }\n \n     @Override\n     public Optional<String> comment() {\n-        return Optional.ofNullable(comment);\n+        return Optional.ofNullable(this.viewSchema.comment());\n     }\n \n     @Override\n     public Map<String, String> options() {\n-        return options;\n+        return this.viewSchema.options();\n     }\n \n     @Override\n     public View copy(Map<String, String> dynamicOptions) {\n-        Map<String, String> newOptions = new HashMap<>(options);\n+        Map<String, String> newOptions = new HashMap<>(options());\n         newOptions.putAll(dynamicOptions);\n-        return new ViewImpl(identifier, rowType, query, comment, newOptions);\n+        return new ViewImpl(identifier, rowType(), query(), this.viewSchema.comment(), newOptions);\n     }\n \n     @Override\n@@ -97,14 +94,11 @@ public boolean equals(Object o) {\n         }\n         ViewImpl view = (ViewImpl) o;\n         return Objects.equals(identifier, view.identifier)\n-                && Objects.equals(rowType, view.rowType)\n-                && Objects.equals(query, view.query)\n-                && Objects.equals(comment, view.comment)\n-                && Objects.equals(options, view.options);\n+                && Objects.equals(viewSchema, view.viewSchema);\n     }\n \n     @Override\n     public int hashCode() {\n-        return Objects.hash(identifier, rowType, query, comment, options);\n+        return Objects.hash(identifier, viewSchema);\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/view/ViewSchema.java b/paimon-core/src/main/java/org/apache/paimon/view/ViewSchema.java\nnew file mode 100644\nindex 000000000000..ff64f9d5c068\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/view/ViewSchema.java\n@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.view;\n+\n+import org.apache.paimon.types.RowType;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonInclude;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/** Schema for view. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ViewSchema {\n+    private static final String FIELD_FIELDS = \""rowType\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+    private static final String FIELD_COMMENT = \""comment\"";\n+    private static final String FIELD_QUERY = \""query\"";\n+\n+    @JsonProperty(FIELD_QUERY)\n+    private final String query;\n+\n+    @Nullable\n+    @JsonProperty(FIELD_COMMENT)\n+    @JsonInclude(JsonInclude.Include.NON_NULL)\n+    private final String comment;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private final Map<String, String> options;\n+\n+    @JsonProperty(FIELD_FIELDS)\n+    private final RowType rowType;\n+\n+    @JsonCreator\n+    public ViewSchema(\n+            @JsonProperty(FIELD_FIELDS) RowType rowType,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options,\n+            @Nullable @JsonProperty(FIELD_COMMENT) String comment,\n+            @JsonProperty(FIELD_QUERY) String query) {\n+        this.options = options;\n+        this.comment = comment;\n+        this.query = query;\n+        this.rowType = rowType;\n+    }\n+\n+    public ViewSchema(\n+            String query, @Nullable String comment, Map<String, String> options, RowType rowType) {\n+        this.query = query;\n+        this.comment = comment;\n+        this.options = options;\n+        this.rowType = rowType;\n+    }\n+\n+    @JsonGetter(FIELD_FIELDS)\n+    public RowType rowType() {\n+        return rowType;\n+    }\n+\n+    @JsonGetter(FIELD_QUERY)\n+    public String query() {\n+        return query;\n+    }\n+\n+    @Nullable\n+    @JsonGetter(FIELD_COMMENT)\n+    public String comment() {\n+        return comment;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> options() {\n+        return options;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        ViewSchema that = (ViewSchema) o;\n+        return Objects.equals(query, that.query)\n+                && Objects.equals(comment, that.comment)\n+                && Objects.equals(options, that.options)\n+                && Objects.equals(rowType, that.rowType);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(query, comment, options, rowType);\n+    }\n+}\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex e60bb6617767..02ea7de8d0df 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -240,11 +240,7 @@ paths:\n               $ref: '#/components/schemas/CreateTableRequest'\n       responses:\n         \""200\"":\n-          description: OK\n-          content:\n-            application/json:\n-              schema:\n-                $ref: '#/components/schemas/GetTableResponse'\n+          description: Success, no content\n         \""500\"":\n           description: Internal Server Error\n   /v1/{prefix}/databases/{database}/tables/{table}:\n@@ -312,11 +308,7 @@ paths:\n               $ref: '#/components/schemas/AlterTableRequest'\n       responses:\n         \""200\"":\n-          description: OK\n-          content:\n-            application/json:\n-              schema:\n-                $ref: '#/components/schemas/GetTableResponse'\n+          description: Success, no content\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -358,7 +350,7 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}/tables/{table}/rename:\n+  /v1/{prefix}/databases/{database}/tables/rename:\n     post:\n       tags:\n         - table\n@@ -375,11 +367,6 @@ paths:\n           required: true\n           schema:\n             type: string\n-        - name: table\n-          in: path\n-          required: true\n-          schema:\n-            type: string\n       requestBody:\n         content:\n           application/json:\n@@ -387,11 +374,7 @@ paths:\n               $ref: '#/components/schemas/RenameTableRequest'\n       responses:\n         \""200\"":\n-          description: OK\n-          content:\n-            application/json:\n-              schema:\n-                $ref: '#/components/schemas/GetTableResponse'\n+          description: Success, no content\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -406,7 +389,7 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}/tables/{table}/commit:\n+  /v1/{prefix}/databases/{database}/tables/commit:\n     post:\n       tags:\n         - table\n@@ -423,11 +406,6 @@ paths:\n           required: true\n           schema:\n             type: string\n-        - name: table\n-          in: path\n-          required: true\n-          schema:\n-            type: string\n       requestBody:\n         content:\n           application/json:\n@@ -642,6 +620,172 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/views:\n+    get:\n+      tags:\n+        - view\n+      summary: List views\n+      operationId: listViews\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ListViewsResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+    post:\n+      tags:\n+        - view\n+      summary: Create view\n+      operationId: createView\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/CreateViewRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/views/{view}:\n+    get:\n+      tags:\n+        - view\n+      summary: Get view\n+      operationId: getView\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: view\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetViewResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+    delete:\n+      tags:\n+        - view\n+      summary: Drop view\n+      operationId: dropView\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: view\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/views/rename:\n+    post:\n+      tags:\n+        - view\n+      summary: Rename view\n+      operationId: renameView\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/RenameTableRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""409\"":\n+          description: Resource has exist\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n components:\n   schemas:\n     CreateDatabaseRequest:\n@@ -709,6 +853,13 @@ components:\n           $ref: '#/components/schemas/Identifier'\n         schema:\n           $ref: '#/components/schemas/Schema'\n+    CreateViewRequest:\n+      type: object\n+      properties:\n+        identifier:\n+          $ref: '#/components/schemas/Identifier'\n+        schema:\n+          $ref: '#/components/schemas/ViewSchema'\n     DataField:\n       type: object\n       properties:\n@@ -767,7 +918,9 @@ components:\n           pattern: ^ROW.*\n           example: ROW\n         fields:\n-          $ref: '#/components/schemas/DataField'\n+          type: array\n+          items:\n+           $ref: '#/components/schemas/DataField'\n     Identifier:\n       type: object\n       properties:\n@@ -946,7 +1099,9 @@ components:\n     RenameTableRequest:\n       type: object\n       properties:\n-        newIdentifier:\n+        source:\n+          $ref: '#/components/schemas/Identifier'\n+        destination:\n           $ref: '#/components/schemas/Identifier'\n     CommitTableRequest:\n       type: object\n@@ -960,8 +1115,52 @@ components:\n       properties:\n         version:\n           type: integer\n+          format: int32\n+          nullable: true\n         id:\n           type: integer\n+          format: int64\n+        schemaId:\n+          type: integer\n+          format: int64\n+        baseManifestList:\n+          type: string\n+        deltaManifestList:\n+          type: string\n+        changelogManifestList:\n+          type: string\n+          nullable: true\n+        indexManifest:\n+          type: string\n+        commitUser:\n+          type: string\n+        commitIdentifier:\n+          type: string\n+        commitKind:\n+          type: string\n+          enum: [\""APPEND\"", \""COMPACT\"", \""OVERWRITE\"", \""ANALYZE\""]\n+        timeMillis:\n+          type: integer\n+          format: int64\n+        logOffsets:\n+          type: object\n+          additionalProperties:\n+            type: integer\n+            format: int64\n+        totalRecordCount:\n+          type: integer\n+          format: int64\n+        deltaRecordCount:\n+          type: integer\n+          format: int64\n+        changelogRecordCount:\n+          type: integer\n+          format: int64\n+        watermark:\n+          type: integer\n+          format: int64\n+        statistics:\n+          type: string\n     CommitTableResponse:\n       type: object\n       properties:\n@@ -1043,6 +1242,35 @@ components:\n           type: array\n           items:\n             $ref: '#/components/schemas/Partition'\n+    GetViewResponse:\n+      type: object\n+      properties:\n+        id:\n+          type: string\n+        name:\n+          type: string\n+        schema:\n+          $ref: '#/components/schemas/ViewSchema'\n+    ListViewsResponse:\n+      type: object\n+      properties:\n+        views:\n+          type: array\n+          items:\n+            type: string\n+    ViewSchema:\n+      type: object\n+      properties:\n+        rowType:\n+          $ref: '#/components/schemas/RowType'\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+        comment:\n+          type: string\n+        query:\n+          type: string\n     Partition:\n       type: object\n       properties:\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex 745a1e9ffd34..e657407a47c3 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -27,6 +27,7 @@\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n@@ -37,9 +38,15 @@\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.ListViewsResponse;\n+import org.apache.paimon.types.DataField;\n+import org.apache.paimon.types.IntType;\n+import org.apache.paimon.types.RowType;\n+import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n@@ -57,7 +64,10 @@\n import org.springframework.web.bind.annotation.PostMapping;\n import org.springframework.web.bind.annotation.RestController;\n \n+import java.util.Arrays;\n+import java.util.Collections;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.UUID;\n \n@@ -253,37 +263,22 @@ public GetTableResponse getTable(\n             summary = \""Create table\"",\n             tags = {\""table\""})\n     @ApiResponses({\n-        @ApiResponse(\n-                responseCode = \""200\"",\n-                content = {@Content(schema = @Schema(implementation = GetTableResponse.class))}),\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n     @PostMapping(\""/v1/{prefix}/databases/{database}/tables\"")\n-    public GetTableResponse createTable(\n+    public void createTable(\n             @PathVariable String prefix,\n             @PathVariable String database,\n-            @RequestBody CreateTableRequest request) {\n-        return new GetTableResponse(\n-                UUID.randomUUID().toString(),\n-                \""\"",\n-                1,\n-                new org.apache.paimon.schema.Schema(\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        new HashMap<>(),\n-                        \""comment\""));\n-    }\n+            @RequestBody CreateTableRequest request) {}\n \n     @Operation(\n             summary = \""Alter table\"",\n             tags = {\""table\""})\n     @ApiResponses({\n-        @ApiResponse(\n-                responseCode = \""200\"",\n-                content = {@Content(schema = @Schema(implementation = GetTableResponse.class))}),\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""404\"",\n                 description = \""Resource not found\"",\n@@ -293,22 +288,11 @@ public GetTableResponse createTable(\n                 content = {@Content(schema = @Schema())})\n     })\n     @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}\"")\n-    public GetTableResponse alterTable(\n+    public void alterTable(\n             @PathVariable String prefix,\n             @PathVariable String database,\n             @PathVariable String table,\n-            @RequestBody AlterTableRequest request) {\n-        return new GetTableResponse(\n-                UUID.randomUUID().toString(),\n-                \""\"",\n-                1,\n-                new org.apache.paimon.schema.Schema(\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        new HashMap<>(),\n-                        \""comment\""));\n-    }\n+            @RequestBody AlterTableRequest request) {}\n \n     @Operation(\n             summary = \""Drop table\"",\n@@ -333,9 +317,7 @@ public void dropTable(\n             summary = \""Rename table\"",\n             tags = {\""table\""})\n     @ApiResponses({\n-        @ApiResponse(\n-                responseCode = \""200\"",\n-                content = {@Content(schema = @Schema(implementation = GetTableResponse.class))}),\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""404\"",\n                 description = \""Resource not found\"",\n@@ -344,23 +326,11 @@ public void dropTable(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n-    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/rename\"")\n-    public GetTableResponse renameTable(\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/rename\"")\n+    public void renameTable(\n             @PathVariable String prefix,\n             @PathVariable String database,\n-            @PathVariable String table,\n-            @RequestBody RenameTableRequest request) {\n-        return new GetTableResponse(\n-                UUID.randomUUID().toString(),\n-                \""\"",\n-                1,\n-                new org.apache.paimon.schema.Schema(\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        ImmutableList.of(),\n-                        new HashMap<>(),\n-                        \""comment\""));\n-    }\n+            @RequestBody RenameTableRequest request) {}\n \n     @Operation(\n             summary = \""Commit table\"",\n@@ -377,11 +347,10 @@ public GetTableResponse renameTable(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n-    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/commit\"")\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/commit\"")\n     public CommitTableResponse commitTable(\n             @PathVariable String prefix,\n             @PathVariable String database,\n-            @PathVariable String table,\n             @RequestBody CommitTableRequest request) {\n         return new CommitTableResponse(true);\n     }\n@@ -493,4 +462,112 @@ public void markDonePartitions(\n             @PathVariable String database,\n             @PathVariable String table,\n             @RequestBody MarkDonePartitionsRequest request) {}\n+\n+    @Operation(\n+            summary = \""List views\"",\n+            tags = {\""view\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {@Content(schema = @Schema(implementation = ListViewsResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/v1/{prefix}/databases/{database}/views\"")\n+    public ListViewsResponse listViews(@PathVariable String prefix, @PathVariable String database) {\n+        return new ListViewsResponse(ImmutableList.of(\""view1\""));\n+    }\n+\n+    @Operation(\n+            summary = \""create view\"",\n+            tags = {\""view\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/views\"")\n+    public void createView(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @RequestBody CreateViewRequest request) {}\n+\n+    @Operation(\n+            summary = \""Get view\"",\n+            tags = {\""view\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {@Content(schema = @Schema(implementation = GetViewResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/v1/{prefix}/databases/{database}/views/{view}\"")\n+    public GetViewResponse getView(\n+            @PathVariable String prefix, @PathVariable String database, @PathVariable String view) {\n+        List<DataField> fields =\n+                Arrays.asList(\n+                        new DataField(0, \""f0\"", new IntType()),\n+                        new DataField(1, \""f1\"", new IntType()));\n+        ViewSchema schema =\n+                new ViewSchema(\n+                        new RowType(fields),\n+                        Collections.singletonMap(\""pt\"", \""1\""),\n+                        \""comment\"",\n+                        \""select * from t1\"");\n+        return new GetViewResponse(\""id\"", \""name\"", schema);\n+    }\n+\n+    @Operation(\n+            summary = \""Rename view\"",\n+            tags = {\""view\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/views/rename\"")\n+    public void renameView(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @RequestBody RenameTableRequest request) {}\n+\n+    @Operation(\n+            summary = \""Drop view\"",\n+            tags = {\""view\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @DeleteMapping(\""/v1/{prefix}/databases/{database}/views/{view}\"")\n+    public void dropView(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String view) {}\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\nindex 1c453a1b3bcf..b8da733e7740 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n@@ -970,7 +970,6 @@ public void testView() throws Exception {\n                 .isInstanceOf(Catalog.ViewNotExistException.class);\n         catalog.renameView(identifier, newIdentifier, false);\n \n-        catalog.dropView(newIdentifier, false);\n         catalog.dropView(newIdentifier, true);\n         assertThatThrownBy(() -> catalog.dropView(newIdentifier, false))\n                 .isInstanceOf(Catalog.ViewNotExistException.class);\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 822e06d7fbbc..766cb09b0bdd 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -26,15 +26,18 @@\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.ListViewsResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.types.DataField;\n@@ -42,6 +45,7 @@\n import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.IntType;\n import org.apache.paimon.types.RowType;\n+import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n@@ -124,9 +128,10 @@ public static CreateTableRequest createTableRequest(String name) {\n         return new CreateTableRequest(identifier, schema);\n     }\n \n-    public static RenameTableRequest renameRequest(String toTableName) {\n-        Identifier newIdentifier = Identifier.create(databaseName(), toTableName);\n-        return new RenameTableRequest(newIdentifier);\n+    public static RenameTableRequest renameRequest(String sourceTable, String toTableName) {\n+        Identifier source = Identifier.create(databaseName(), sourceTable);\n+        Identifier destination = Identifier.create(databaseName(), toTableName);\n+        return new RenameTableRequest(source, destination);\n     }\n \n     public static AlterTableRequest alterTableRequest() {\n@@ -230,6 +235,31 @@ public static AlterPartitionsRequest alterPartitionsRequest() {\n         return new AlterPartitionsRequest(ImmutableList.of(partition()));\n     }\n \n+    public static CreateViewRequest createViewRequest(String name) {\n+        Identifier identifier = Identifier.create(databaseName(), name);\n+        return new CreateViewRequest(identifier, viewSchema());\n+    }\n+\n+    public static GetViewResponse getViewResponse() {\n+        return new GetViewResponse(UUID.randomUUID().toString(), \""\"", viewSchema());\n+    }\n+\n+    public static ListViewsResponse listViewsResponse() {\n+        return new ListViewsResponse(ImmutableList.of(\""view\""));\n+    }\n+\n+    private static ViewSchema viewSchema() {\n+        List<DataField> fields =\n+                Arrays.asList(\n+                        new DataField(0, \""f0\"", new IntType()),\n+                        new DataField(1, \""f1\"", new IntType()));\n+        return new ViewSchema(\n+                new RowType(fields),\n+                Collections.singletonMap(\""pt\"", \""1\""),\n+                \""comment\"",\n+                \""select * from t1\"");\n+    }\n+\n     private static Partition partition() {\n         return new Partition(Collections.singletonMap(\""pt\"", \""1\""), 1, 1, 1, 1);\n     }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 21284629ccc9..91024867b7ea 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -33,6 +33,7 @@\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n@@ -42,14 +43,19 @@\n import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.ListViewsResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.FormatTable;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.types.DataField;\n+import org.apache.paimon.view.View;\n+import org.apache.paimon.view.ViewImpl;\n+import org.apache.paimon.view.ViewSchema;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n \n@@ -120,12 +126,29 @@ public MockResponse dispatch(RecordedRequest request) {\n                                         .substring((DATABASE_URI + \""/\"").length())\n                                         .split(\""/\"");\n                         String databaseName = resources[0];\n+                        boolean isViews = resources.length == 2 && \""views\"".equals(resources[1]);\n                         boolean isTables = resources.length == 2 && \""tables\"".equals(resources[1]);\n-                        boolean isTable = resources.length == 3 && \""tables\"".equals(resources[1]);\n                         boolean isTableRename =\n-                                resources.length == 4 && \""rename\"".equals(resources[3]);\n+                                resources.length == 3\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""rename\"".equals(resources[2]);\n+                        boolean isViewRename =\n+                                resources.length == 3\n+                                        && \""views\"".equals(resources[1])\n+                                        && \""rename\"".equals(resources[2]);\n+                        boolean isView =\n+                                resources.length == 3\n+                                        && \""views\"".equals(resources[1])\n+                                        && !\""rename\"".equals(resources[2]);\n+                        boolean isTable =\n+                                resources.length == 3\n+                                        && \""tables\"".equals(resources[1])\n+                                        && !\""rename\"".equals(resources[2])\n+                                        && !\""commit\"".equals(resources[2]);\n                         boolean isTableCommit =\n-                                resources.length == 4 && \""commit\"".equals(resources[3]);\n+                                resources.length == 3\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""commit\"".equals(resources[2]);\n                         boolean isPartitions =\n                                 resources.length == 4\n                                         && \""tables\"".equals(resources[1])\n@@ -180,8 +203,7 @@ public MockResponse dispatch(RecordedRequest request) {\n                             String tableName = resources[2];\n                             return partitionsApiHandler(catalog, request, databaseName, tableName);\n                         } else if (isTableRename) {\n-                            return renameTableApiHandler(\n-                                    catalog, request, databaseName, resources[2]);\n+                            return renameTableApiHandler(catalog, request);\n                         } else if (isTableCommit) {\n                             return commitTableApiHandler(\n                                     catalog, request, databaseName, resources[2]);\n@@ -190,6 +212,13 @@ public MockResponse dispatch(RecordedRequest request) {\n                             return tableApiHandler(catalog, request, databaseName, tableName);\n                         } else if (isTables) {\n                             return tablesApiHandler(catalog, request, databaseName);\n+                        } else if (isViews) {\n+                            return viewsApiHandler(catalog, request, databaseName);\n+                        } else if (isViewRename) {\n+                            return renameViewApiHandler(catalog, request);\n+                        } else if (isView) {\n+                            String viewName = resources[2];\n+                            return viewApiHandler(catalog, request, databaseName, viewName);\n                         } else {\n                             return databaseApiHandler(catalog, request, databaseName);\n                         }\n@@ -243,6 +272,22 @@ public MockResponse dispatch(RecordedRequest request) {\n                                     e.getMessage(),\n                                     409);\n                     return mockResponse(response, 409);\n+                } catch (Catalog.ViewNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.VIEW,\n+                                    e.identifier().getTableName(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n+                } catch (Catalog.ViewAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.VIEW,\n+                                    e.identifier().getTableName(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n                 } catch (IllegalArgumentException e) {\n                     response = new ErrorResponse(null, null, e.getMessage(), 400);\n                     return mockResponse(response, 400);\n@@ -265,21 +310,6 @@ public MockResponse dispatch(RecordedRequest request) {\n         };\n     }\n \n-    private static MockResponse renameTableApiHandler(\n-            Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n-            throws Exception {\n-        RenameTableRequest requestBody =\n-                OBJECT_MAPPER.readValue(request.getBody().readUtf8(), RenameTableRequest.class);\n-        catalog.renameTable(\n-                Identifier.create(databaseName, tableName), requestBody.getNewIdentifier(), false);\n-        GetTableResponse response =\n-                getTable(\n-                        catalog,\n-                        requestBody.getNewIdentifier().getDatabaseName(),\n-                        requestBody.getNewIdentifier().getTableName());\n-        return mockResponse(response, 200);\n-    }\n-\n     private static MockResponse commitTableApiHandler(\n             Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n             throws Exception {\n@@ -342,7 +372,6 @@ private static MockResponse tablesApiHandler(\n         RESTResponse response;\n         switch (request.getMethod()) {\n             case \""GET\"":\n-                catalog.listTables(databaseName);\n                 response = new ListTablesResponse(catalog.listTables(databaseName));\n                 return mockResponse(response, 200);\n             case \""POST\"":\n@@ -350,10 +379,7 @@ private static MockResponse tablesApiHandler(\n                         OBJECT_MAPPER.readValue(\n                                 request.getBody().readUtf8(), CreateTableRequest.class);\n                 catalog.createTable(requestBody.getIdentifier(), requestBody.getSchema(), false);\n-                response =\n-                        new GetTableResponse(\n-                                UUID.randomUUID().toString(), \""\"", 1L, requestBody.getSchema());\n-                return mockResponse(response, 200);\n+                return new MockResponse().setResponseCode(200);\n             default:\n                 return new MockResponse().setResponseCode(404);\n         }\n@@ -373,8 +399,7 @@ private static MockResponse tableApiHandler(\n                         OBJECT_MAPPER.readValue(\n                                 request.getBody().readUtf8(), AlterTableRequest.class);\n                 catalog.alterTable(identifier, requestBody.getChanges(), false);\n-                response = getTable(catalog, databaseName, tableName);\n-                return mockResponse(response, 200);\n+                return new MockResponse().setResponseCode(200);\n             case \""DELETE\"":\n                 catalog.dropTable(identifier, false);\n                 return new MockResponse().setResponseCode(200);\n@@ -383,6 +408,14 @@ private static MockResponse tableApiHandler(\n         }\n     }\n \n+    private static MockResponse renameTableApiHandler(Catalog catalog, RecordedRequest request)\n+            throws Exception {\n+        RenameTableRequest requestBody =\n+                OBJECT_MAPPER.readValue(request.getBody().readUtf8(), RenameTableRequest.class);\n+        catalog.renameTable(requestBody.getSource(), requestBody.getDestination(), false);\n+        return new MockResponse().setResponseCode(200);\n+    }\n+\n     private static MockResponse partitionsApiHandler(\n             Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n             throws Exception {\n@@ -404,6 +437,63 @@ private static MockResponse partitionsApiHandler(\n         }\n     }\n \n+    private static MockResponse viewsApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName) throws Exception {\n+        RESTResponse response;\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                response = new ListViewsResponse(catalog.listViews(databaseName));\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                CreateViewRequest requestBody =\n+                        OBJECT_MAPPER.readValue(\n+                                request.getBody().readUtf8(), CreateViewRequest.class);\n+                ViewImpl view =\n+                        new ViewImpl(\n+                                requestBody.getIdentifier(),\n+                                requestBody.getSchema().rowType(),\n+                                requestBody.getSchema().query(),\n+                                requestBody.getSchema().comment(),\n+                                requestBody.getSchema().options());\n+                catalog.createView(requestBody.getIdentifier(), view, false);\n+                return new MockResponse().setResponseCode(200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+    }\n+\n+    private static MockResponse viewApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName, String viewName)\n+            throws Exception {\n+        RESTResponse response;\n+        Identifier identifier = Identifier.create(databaseName, viewName);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                View view = catalog.getView(identifier);\n+                ViewSchema schema =\n+                        new ViewSchema(\n+                                view.rowType(),\n+                                view.options(),\n+                                view.comment().orElse(null),\n+                                view.query());\n+                response = new GetViewResponse(\""id\"", identifier.getTableName(), schema);\n+                return mockResponse(response, 200);\n+            case \""DELETE\"":\n+                catalog.dropView(identifier, false);\n+                return new MockResponse().setResponseCode(200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+    }\n+\n+    private static MockResponse renameViewApiHandler(Catalog catalog, RecordedRequest request)\n+            throws Exception {\n+        RenameTableRequest requestBody =\n+                OBJECT_MAPPER.readValue(request.getBody().readUtf8(), RenameTableRequest.class);\n+        catalog.renameView(requestBody.getSource(), requestBody.getDestination(), false);\n+        return new MockResponse().setResponseCode(200);\n+    }\n+\n     private static GetTableResponse getTable(Catalog catalog, String databaseName, String tableName)\n             throws Exception {\n         Identifier identifier = Identifier.create(databaseName, tableName);\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex f63d70332650..d1ce64b6c543 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -117,6 +117,11 @@ protected boolean supportPartitions() {\n         return true;\n     }\n \n+    @Override\n+    protected boolean supportsView() {\n+        return true;\n+    }\n+\n     private void createTable(\n             Identifier identifier, Map<String, String> options, List<String> partitionKeys)\n             throws Exception {\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 6e260a5e7341..fa56f8111828 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -24,6 +24,7 @@\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.CreateViewRequest;\n import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n@@ -32,9 +33,11 @@\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.GetViewResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.ListViewsResponse;\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.IntType;\n@@ -172,11 +175,12 @@ public void dataFieldParseTest() throws Exception {\n \n     @Test\n     public void renameTableRequestParseTest() throws Exception {\n-        RenameTableRequest request = MockRESTMessage.renameRequest(\""t2\"");\n+        RenameTableRequest request = MockRESTMessage.renameRequest(\""t1\"", \""t2\"");\n         String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n         RenameTableRequest parseData =\n                 OBJECT_MAPPER.readValue(requestStr, RenameTableRequest.class);\n-        assertEquals(request.getNewIdentifier(), parseData.getNewIdentifier());\n+        assertEquals(request.getSource(), parseData.getSource());\n+        assertEquals(request.getDestination(), parseData.getDestination());\n     }\n \n     @Test\n@@ -242,4 +246,31 @@ public void alterPartitionsRequestParseTest() throws Exception {\n                 OBJECT_MAPPER.readValue(requestStr, AlterPartitionsRequest.class);\n         assertEquals(request.getPartitions(), parseData.getPartitions());\n     }\n+\n+    @Test\n+    public void createViewRequestParseTest() throws Exception {\n+        CreateViewRequest request = MockRESTMessage.createViewRequest(\""t1\"");\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        CreateViewRequest parseData = OBJECT_MAPPER.readValue(requestStr, CreateViewRequest.class);\n+        assertEquals(request.getIdentifier(), parseData.getIdentifier());\n+        assertEquals(request.getSchema(), parseData.getSchema());\n+    }\n+\n+    @Test\n+    public void getViewResponseParseTest() throws Exception {\n+        GetViewResponse response = MockRESTMessage.getViewResponse();\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        GetViewResponse parseData = OBJECT_MAPPER.readValue(responseStr, GetViewResponse.class);\n+        assertEquals(response.getId(), parseData.getId());\n+        assertEquals(response.getName(), parseData.getName());\n+        assertEquals(response.getSchema(), parseData.getSchema());\n+    }\n+\n+    @Test\n+    public void listViewsResponseParseTest() throws Exception {\n+        ListViewsResponse response = MockRESTMessage.listViewsResponse();\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        ListViewsResponse parseData = OBJECT_MAPPER.readValue(responseStr, ListViewsResponse.class);\n+        assertEquals(response.getViews(), parseData.getViews());\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java b/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java\nindex 4f150493110b..f56030252bfd 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java\n@@ -32,6 +32,7 @@\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.TableSchema;\n+import org.apache.paimon.view.View;\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n@@ -46,6 +47,7 @@ public class TestRESTCatalog extends FileSystemCatalog {\n     public Map<String, TableSchema> tableFullName2Schema = new HashMap<String, TableSchema>();\n     public Map<String, List<Partition>> tableFullName2Partitions =\n             new HashMap<String, List<Partition>>();\n+    public final Map<String, View> viewFullName2View = new HashMap<String, View>();\n \n     public TestRESTCatalog(FileIO fileIO, Path warehouse, Options options) {\n         super(fileIO, warehouse, options);\n@@ -130,6 +132,61 @@ public List<Partition> listPartitions(Identifier identifier) throws TableNotExis\n         return tableFullName2Partitions.get(identifier.getFullName());\n     }\n \n+    @Override\n+    public View getView(Identifier identifier) throws ViewNotExistException {\n+        if (viewFullName2View.containsKey(identifier.getFullName())) {\n+            return viewFullName2View.get(identifier.getFullName());\n+        }\n+        throw new ViewNotExistException(identifier);\n+    }\n+\n+    @Override\n+    public void dropView(Identifier identifier, boolean ignoreIfNotExists)\n+            throws ViewNotExistException {\n+        if (viewFullName2View.containsKey(identifier.getFullName())) {\n+            viewFullName2View.remove(identifier.getFullName());\n+        }\n+        if (!ignoreIfNotExists) {\n+            throw new ViewNotExistException(identifier);\n+        }\n+    }\n+\n+    @Override\n+    public void createView(Identifier identifier, View view, boolean ignoreIfExists)\n+            throws ViewAlreadyExistException, DatabaseNotExistException {\n+        getDatabase(identifier.getDatabaseName());\n+        if (viewFullName2View.containsKey(identifier.getFullName()) && !ignoreIfExists) {\n+            throw new ViewAlreadyExistException(identifier);\n+        }\n+        viewFullName2View.put(identifier.getFullName(), view);\n+    }\n+\n+    @Override\n+    public List<String> listViews(String databaseName) throws DatabaseNotExistException {\n+        getDatabase(databaseName);\n+        return viewFullName2View.keySet().stream()\n+                .map(v -> Identifier.fromString(v))\n+                .filter(identifier -> identifier.getDatabaseName().equals(databaseName))\n+                .map(identifier -> identifier.getTableName())\n+                .collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public void renameView(Identifier fromView, Identifier toView, boolean ignoreIfNotExists)\n+            throws ViewNotExistException, ViewAlreadyExistException {\n+        if (!viewFullName2View.containsKey(fromView.getFullName()) && !ignoreIfNotExists) {\n+            throw new ViewNotExistException(fromView);\n+        }\n+        if (viewFullName2View.containsKey(toView.getFullName())) {\n+            throw new ViewAlreadyExistException(toView);\n+        }\n+        if (viewFullName2View.containsKey(fromView.getFullName())) {\n+            View view = viewFullName2View.get(fromView.getFullName());\n+            viewFullName2View.remove(fromView.getFullName());\n+            viewFullName2View.put(toView.getFullName(), view);\n+        }\n+    }\n+\n     @Override\n     protected List<String> listTablesImpl(String databaseName) {\n         List<String> tables = super.listTablesImpl(databaseName);\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4879"", ""pr_id"": 4879, ""issue_id"": 4550, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Bug] Use hive cannot insert data from not partition table to a partition table.\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Paimon version\n\n0.9\n\n### Compute Engine\n\nHive: CDH6.3.2\n\n### Minimal reproduce step\n\n\r\n\r\n```sql\r\nSET hive.metastore.warehouse.dir=/user/hive/warehouse;\r\n\r\n\r\ncreate table hive_test_np(\r\na int,\r\nb string,\r\nc string)\r\nSTORED BY 'org.apache.paimon.hive.PaimonStorageHandler';\r\n\r\ninsert into hive_test_np values(1, 'aaa', 'nice');\r\n\r\n\r\ncreate table hive_test(\r\na int,\r\nb string)\r\nPARTITIONED BY ( c string) \r\nSTORED BY 'org.apache.paimon.hive.PaimonStorageHandler';\r\n\r\ninsert into hive_test partition(c) select * from hive_test_np;\r\n```\n\n### What doesn't meet your expectations?\n\n\r\n```\r\n2024-11-19 21:45:16,486 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\""a\"":1,\""b\"":\""aaa\"",\""c\"":\""nice\""}\r\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:157)\r\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\r\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\r\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\r\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\r\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\""a\"":1,\""b\"":\""aaa\"",\""c\"":\""nice\""}\r\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:494)\r\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:148)\r\n\t... 8 more\r\nCaused by: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 2\r\n\tat org.apache.paimon.hive.mapred.PaimonRecordWriter.write(PaimonRecordWriter.java:69)\r\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:769)\r\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)\r\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)\r\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:882)\r\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\r\n\tat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:146)\r\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:484)\r\n\t... 9 more\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 2\r\n\tat org.apache.paimon.data.GenericRow.isNullAt(GenericRow.java:131)\r\n\tat Projection$0.apply(Unknown Source)\r\n\tat org.apache.paimon.table.sink.RowPartitionKeyExtractor.partition(RowPartitionKeyExtractor.java:44)\r\n\tat org.apache.paimon.table.sink.RowKeyExtractor.partition(RowKeyExtractor.java:57)\r\n\tat org.apache.paimon.table.sink.TableWriteImpl.toSinkRecord(TableWriteImpl.java:204)\r\n\tat org.apache.paimon.table.sink.TableWriteImpl.writeAndReturn(TableWriteImpl.java:174)\r\n\tat org.apache.paimon.table.sink.TableWriteImpl.write(TableWriteImpl.java:147)\r\n\tat org.apache.paimon.hive.mapred.PaimonRecordWriter.write(PaimonRecordWriter.java:67)\r\n\t... 16 more\r\n\r\n```\r\n\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] I'm willing to submit a PR!"", ""issue_word_count"": 502, ""test_files_count"": 7, ""non_test_files_count"": 14, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterPartitionsRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/BasePartitionsRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionsRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionsRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/MarkDonePartitionsRequest.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java"", ""paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java""], ""base_commit"": ""97441c32707f74704fd342073d0c06cd4641943d"", ""head_commit"": ""9505948c715dd0ac80687a04d55de04a21736781"", ""repo_url"": ""https://github.com/apache/paimon/pull/4879"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4879"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-14T04:55:07.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex 702d5229cf20..dedfd5f11276 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -54,11 +54,14 @@\n import java.util.Optional;\n import java.util.stream.Collectors;\n \n+import static org.apache.paimon.CoreOptions.OBJECT_LOCATION;\n import static org.apache.paimon.CoreOptions.TYPE;\n import static org.apache.paimon.CoreOptions.createCommitUser;\n+import static org.apache.paimon.catalog.CatalogUtils.buildFormatTableByTableSchema;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotBranch;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemTable;\n+import static org.apache.paimon.catalog.CatalogUtils.getTableType;\n import static org.apache.paimon.catalog.CatalogUtils.isSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.listPartitionsFromFileSystem;\n import static org.apache.paimon.catalog.CatalogUtils.validateAutoCreateClose;\n@@ -309,7 +312,7 @@ private void createObjectTable(Identifier identifier, Schema schema) {\n                 ObjectTable.SCHEMA,\n                 rowType);\n         checkArgument(\n-                schema.options().containsKey(CoreOptions.OBJECT_LOCATION.key()),\n+                schema.options().containsKey(OBJECT_LOCATION.key()),\n                 \""Object table should have object-location option.\"");\n         createTableImpl(identifier, schema.copy(ObjectTable.SCHEMA));\n     }\n@@ -383,9 +386,20 @@ public Table getTable(Identifier identifier) throws TableNotExistException {\n         }\n     }\n \n+    // hive override this method.\n     protected Table getDataOrFormatTable(Identifier identifier) throws TableNotExistException {\n         Preconditions.checkArgument(identifier.getSystemTableName() == null);\n         TableMeta tableMeta = getDataTableMeta(identifier);\n+        TableType tableType = getTableType(tableMeta.schema().options());\n+        if (tableType == TableType.FORMAT_TABLE) {\n+            TableSchema schema = tableMeta.schema();\n+            return buildFormatTableByTableSchema(\n+                    identifier,\n+                    schema.options(),\n+                    schema.logicalRowType(),\n+                    schema.partitionKeys(),\n+                    schema.comment());\n+        }\n         FileStoreTable table =\n                 FileStoreTableFactory.create(\n                         fileIO,\n@@ -399,9 +413,8 @@ protected Table getDataOrFormatTable(Identifier identifier) throws TableNotExist\n                                         lockContext().orElse(null),\n                                         identifier),\n                                 catalogLoader()));\n-        CoreOptions options = table.coreOptions();\n-        if (options.type() == TableType.OBJECT_TABLE) {\n-            String objectLocation = options.objectLocation();\n+        if (tableType == TableType.OBJECT_TABLE) {\n+            String objectLocation = table.coreOptions().objectLocation();\n             checkNotNull(objectLocation, \""Object location should not be null for object table.\"");\n             table =\n                     ObjectTable.builder()\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\nindex 9b69248d6d0e..3c245040ccd6 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n@@ -19,16 +19,19 @@\n package org.apache.paimon.catalog;\n \n import org.apache.paimon.CoreOptions;\n+import org.apache.paimon.TableType;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.table.FileStoreTable;\n+import org.apache.paimon.table.FormatTable;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.system.AllTableOptionsTable;\n import org.apache.paimon.table.system.CatalogOptionsTable;\n import org.apache.paimon.table.system.SystemTableLoader;\n+import org.apache.paimon.types.RowType;\n import org.apache.paimon.utils.InternalRowPartitionComputer;\n import org.apache.paimon.utils.Preconditions;\n \n@@ -191,4 +194,33 @@ public static List<Partition> listPartitionsFromFileSystem(Table table) {\n         }\n         return partitions;\n     }\n+\n+    public static TableType getTableType(Map<String, String> options) {\n+        return options.containsKey(CoreOptions.TYPE.key())\n+                ? TableType.fromString(options.get(CoreOptions.TYPE.key()))\n+                : CoreOptions.TYPE.defaultValue();\n+    }\n+\n+    public static FormatTable buildFormatTableByTableSchema(\n+            Identifier identifier,\n+            Map<String, String> options,\n+            RowType rowType,\n+            List<String> partitionKeys,\n+            String comment) {\n+        FormatTable.Format format =\n+                FormatTable.parseFormat(\n+                        options.getOrDefault(\n+                                CoreOptions.FILE_FORMAT.key(),\n+                                CoreOptions.FILE_FORMAT.defaultValue()));\n+        String location = options.get(CoreOptions.PATH.key());\n+        return FormatTable.builder()\n+                .identifier(identifier)\n+                .rowType(rowType)\n+                .partitionKeys(partitionKeys)\n+                .location(location)\n+                .format(format)\n+                .options(options)\n+                .comment(comment)\n+                .build();\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\nindex eee6abffb1ec..08d6c8a050a0 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n@@ -89,6 +89,12 @@ public <T extends RESTResponse> T get(\n         return exec(request, responseType);\n     }\n \n+    @Override\n+    public <T extends RESTResponse> T post(\n+            String path, RESTRequest body, Map<String, String> headers) {\n+        return post(path, body, null, headers);\n+    }\n+\n     @Override\n     public <T extends RESTResponse> T post(\n             String path, RESTRequest body, Class<T> responseType, Map<String, String> headers) {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex 06eba96d1f13..4b4e90fb7c20 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -18,7 +18,6 @@\n \n package org.apache.paimon.rest;\n \n-import org.apache.paimon.CoreOptions;\n import org.apache.paimon.TableType;\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogContext;\n@@ -29,6 +28,7 @@\n import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.Path;\n+import org.apache.paimon.operation.FileStoreCommit;\n import org.apache.paimon.operation.Lock;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n@@ -40,9 +40,13 @@\n import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n import org.apache.paimon.rest.exceptions.ServiceFailureException;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterPartitionsRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionsRequest;\n+import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -61,6 +65,7 @@\n import org.apache.paimon.table.FileStoreTableFactory;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.object.ObjectTable;\n+import org.apache.paimon.table.sink.BatchWriteBuilder;\n import org.apache.paimon.utils.Pair;\n import org.apache.paimon.utils.Preconditions;\n \n@@ -79,9 +84,12 @@\n \n import static org.apache.paimon.CoreOptions.METASTORE_PARTITIONED_TABLE;\n import static org.apache.paimon.CoreOptions.PATH;\n+import static org.apache.paimon.CoreOptions.createCommitUser;\n+import static org.apache.paimon.catalog.CatalogUtils.buildFormatTableByTableSchema;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotBranch;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemTable;\n+import static org.apache.paimon.catalog.CatalogUtils.getTableType;\n import static org.apache.paimon.catalog.CatalogUtils.isSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.listPartitionsFromFileSystem;\n import static org.apache.paimon.catalog.CatalogUtils.validateAutoCreateClose;\n@@ -390,32 +398,89 @@ public void dropTable(Identifier identifier, boolean ignoreIfNotExists)\n     @Override\n     public void createPartitions(Identifier identifier, List<Map<String, String>> partitions)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        Table table = getTable(identifier);\n+        if (isMetaStorePartitionedTable(table)) {\n+            try {\n+                CreatePartitionsRequest request = new CreatePartitionsRequest(partitions);\n+                client.post(\n+                        resourcePaths.partitions(\n+                                identifier.getDatabaseName(), identifier.getTableName()),\n+                        request,\n+                        headers());\n+            } catch (NoSuchResourceException e) {\n+                throw new TableNotExistException(identifier);\n+            }\n+        }\n     }\n \n     @Override\n     public void dropPartitions(Identifier identifier, List<Map<String, String>> partitions)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        Table table = getTable(identifier);\n+        if (isMetaStorePartitionedTable(table)) {\n+            try {\n+                DropPartitionsRequest request = new DropPartitionsRequest(partitions);\n+                client.post(\n+                        resourcePaths.dropPartitions(\n+                                identifier.getDatabaseName(), identifier.getTableName()),\n+                        request,\n+                        headers());\n+            } catch (NoSuchResourceException e) {\n+                throw new TableNotExistException(identifier);\n+            }\n+        } else {\n+            FileStoreTable fileStoreTable = (FileStoreTable) table;\n+            try (FileStoreCommit commit =\n+                    fileStoreTable\n+                            .store()\n+                            .newCommit(\n+                                    createCommitUser(\n+                                            fileStoreTable.coreOptions().toConfiguration()))) {\n+                commit.dropPartitions(partitions, BatchWriteBuilder.COMMIT_IDENTIFIER);\n+            }\n+        }\n     }\n \n     @Override\n     public void alterPartitions(Identifier identifier, List<Partition> partitions)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        Table table = getTable(identifier);\n+        if (isMetaStorePartitionedTable(table)) {\n+            try {\n+                AlterPartitionsRequest request = new AlterPartitionsRequest(partitions);\n+                client.post(\n+                        resourcePaths.alterPartitions(\n+                                identifier.getDatabaseName(), identifier.getTableName()),\n+                        request,\n+                        headers());\n+            } catch (NoSuchResourceException e) {\n+                throw new TableNotExistException(identifier);\n+            }\n+        }\n     }\n \n     @Override\n     public void markDonePartitions(Identifier identifier, List<Map<String, String>> partitions)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        Table table = getTable(identifier);\n+        if (isMetaStorePartitionedTable(table)) {\n+            try {\n+                MarkDonePartitionsRequest request = new MarkDonePartitionsRequest(partitions);\n+                client.post(\n+                        resourcePaths.markDonePartitions(\n+                                identifier.getDatabaseName(), identifier.getTableName()),\n+                        request,\n+                        headers());\n+            } catch (NoSuchResourceException e) {\n+                throw new TableNotExistException(identifier);\n+            }\n+        }\n     }\n \n     @Override\n     public List<Partition> listPartitions(Identifier identifier) throws TableNotExistException {\n         Table table = getTable(identifier);\n-        Options options = Options.fromMap(table.options());\n-        if (!options.get(METASTORE_PARTITIONED_TABLE)) {\n+        if (!isMetaStorePartitionedTable(table)) {\n             return listPartitionsFromFileSystem(table);\n         }\n \n@@ -471,7 +536,16 @@ private Table getDataOrFormatTable(Identifier identifier) throws TableNotExistEx\n         } catch (ForbiddenException e) {\n             throw new TableNoPermissionException(identifier, e);\n         }\n-\n+        TableType tableType = getTableType(response.getSchema().options());\n+        if (tableType == TableType.FORMAT_TABLE) {\n+            Schema schema = response.getSchema();\n+            return buildFormatTableByTableSchema(\n+                    identifier,\n+                    schema.options(),\n+                    schema.rowType(),\n+                    schema.partitionKeys(),\n+                    schema.comment());\n+        }\n         TableSchema schema = TableSchema.create(response.getSchemaId(), response.getSchema());\n         FileStoreTable table =\n                 FileStoreTableFactory.create(\n@@ -483,9 +557,8 @@ private Table getDataOrFormatTable(Identifier identifier) throws TableNotExistEx\n                                 response.getId(),\n                                 Lock.emptyFactory(),\n                                 catalogLoader()));\n-        CoreOptions options = table.coreOptions();\n-        if (options.type() == TableType.OBJECT_TABLE) {\n-            String objectLocation = options.objectLocation();\n+        if (tableType == TableType.OBJECT_TABLE) {\n+            String objectLocation = table.coreOptions().objectLocation();\n             checkNotNull(objectLocation, \""Object location should not be null for object table.\"");\n             table =\n                     ObjectTable.builder()\n@@ -497,6 +570,11 @@ private Table getDataOrFormatTable(Identifier identifier) throws TableNotExistEx\n         return table;\n     }\n \n+    private boolean isMetaStorePartitionedTable(Table table) {\n+        Options options = Options.fromMap(table.options());\n+        return Boolean.TRUE.equals(options.get(METASTORE_PARTITIONED_TABLE));\n+    }\n+\n     private Map<String, String> headers() {\n         return catalogAuth.getHeaders();\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\nindex d90cb5fa4ad9..d816f09ed0d9 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n@@ -26,6 +26,8 @@ public interface RESTClient extends Closeable {\n \n     <T extends RESTResponse> T get(String path, Class<T> responseType, Map<String, String> headers);\n \n+    <T extends RESTResponse> T post(String path, RESTRequest body, Map<String, String> headers);\n+\n     <T extends RESTResponse> T post(\n             String path, RESTRequest body, Class<T> responseType, Map<String, String> headers);\n \n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex f7d2f7116930..7b092196626f 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -69,4 +69,19 @@ public String renameTable(String databaseName, String tableName) {\n     public String partitions(String databaseName, String tableName) {\n         return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"");\n     }\n+\n+    public String dropPartitions(String databaseName, String tableName) {\n+        return SLASH.join(\n+                V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"", \""drop\"");\n+    }\n+\n+    public String alterPartitions(String databaseName, String tableName) {\n+        return SLASH.join(\n+                V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"", \""alter\"");\n+    }\n+\n+    public String markDonePartitions(String databaseName, String tableName) {\n+        return SLASH.join(\n+                V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"", \""mark\"");\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterPartitionsRequest.java\nsimilarity index 70%\nrename from paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java\nrename to paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterPartitionsRequest.java\nindex 4fabf1163651..e8f542eff578 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterPartitionsRequest.java\n@@ -18,6 +18,7 @@\n \n package org.apache.paimon.rest.requests;\n \n+import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.RESTRequest;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n@@ -25,25 +26,24 @@\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n-import java.util.Map;\n+import java.util.List;\n \n-/** Request for deleting partition. */\n+/** Request for altering partitions. */\n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class DropPartitionRequest implements RESTRequest {\n+public class AlterPartitionsRequest implements RESTRequest {\n \n-    private static final String FIELD_PARTITION_SPEC = \""spec\"";\n+    public static final String FIELD_PARTITIONS = \""partitions\"";\n \n-    @JsonProperty(FIELD_PARTITION_SPEC)\n-    private final Map<String, String> partitionSpec;\n+    @JsonProperty(FIELD_PARTITIONS)\n+    private final List<Partition> partitions;\n \n     @JsonCreator\n-    public DropPartitionRequest(\n-            @JsonProperty(FIELD_PARTITION_SPEC) Map<String, String> partitionSpec) {\n-        this.partitionSpec = partitionSpec;\n+    public AlterPartitionsRequest(@JsonProperty(FIELD_PARTITIONS) List<Partition> partitions) {\n+        this.partitions = partitions;\n     }\n \n-    @JsonGetter(FIELD_PARTITION_SPEC)\n-    public Map<String, String> getPartitionSpec() {\n-        return partitionSpec;\n+    @JsonGetter(FIELD_PARTITIONS)\n+    public List<Partition> getPartitions() {\n+        return partitions;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/BasePartitionsRequest.java\nsimilarity index 63%\nrename from paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java\nrename to paimon-core/src/main/java/org/apache/paimon/rest/requests/BasePartitionsRequest.java\nindex f4486b9260d0..25dc20061ebb 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/BasePartitionsRequest.java\n@@ -16,32 +16,35 @@\n  * limitations under the License.\n  */\n \n-package org.apache.paimon.rest.responses;\n+package org.apache.paimon.rest.requests;\n \n-import org.apache.paimon.partition.Partition;\n-import org.apache.paimon.rest.RESTResponse;\n+import org.apache.paimon.rest.RESTRequest;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n-/** Partition for rest api. */\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Request for partitions action. */\n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class PartitionResponse implements RESTResponse {\n+public abstract class BasePartitionsRequest implements RESTRequest {\n \n-    public static final String FIELD_PARTITION = \""partition\"";\n+    protected static final String FIELD_PARTITION_SPECS = \""specs\"";\n \n-    @JsonProperty(FIELD_PARTITION)\n-    private final Partition partition;\n+    @JsonProperty(FIELD_PARTITION_SPECS)\n+    private final List<Map<String, String>> partitionSpecs;\n \n     @JsonCreator\n-    public PartitionResponse(@JsonProperty(FIELD_PARTITION) Partition partition) {\n-        this.partition = partition;\n+    public BasePartitionsRequest(\n+            @JsonProperty(FIELD_PARTITION_SPECS) List<Map<String, String>> partitionSpecs) {\n+        this.partitionSpecs = partitionSpecs;\n     }\n \n-    @JsonGetter(FIELD_PARTITION)\n-    public Partition getPartition() {\n-        return partition;\n+    @JsonGetter(FIELD_PARTITION_SPECS)\n+    public List<Map<String, String>> getPartitionSpecs() {\n+        return partitionSpecs;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionsRequest.java\nsimilarity index 54%\nrename from paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java\nrename to paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionsRequest.java\nindex e8094ab821bf..420998c1fc5c 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionsRequest.java\n@@ -18,44 +18,20 @@\n \n package org.apache.paimon.rest.requests;\n \n-import org.apache.paimon.catalog.Identifier;\n-import org.apache.paimon.rest.RESTRequest;\n-\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n-import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n+import java.util.List;\n import java.util.Map;\n \n /** Request for creating partition. */\n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class CreatePartitionRequest implements RESTRequest {\n-\n-    private static final String FIELD_IDENTIFIER = \""identifier\"";\n-    private static final String FIELD_PARTITION_SPEC = \""spec\"";\n-\n-    @JsonProperty(FIELD_IDENTIFIER)\n-    private final Identifier identifier;\n-\n-    @JsonProperty(FIELD_PARTITION_SPEC)\n-    private final Map<String, String> partitionSpec;\n+public class CreatePartitionsRequest extends BasePartitionsRequest {\n \n     @JsonCreator\n-    public CreatePartitionRequest(\n-            @JsonProperty(FIELD_IDENTIFIER) Identifier identifier,\n-            @JsonProperty(FIELD_PARTITION_SPEC) Map<String, String> partitionSpec) {\n-        this.identifier = identifier;\n-        this.partitionSpec = partitionSpec;\n-    }\n-\n-    @JsonGetter(FIELD_IDENTIFIER)\n-    public Identifier getIdentifier() {\n-        return identifier;\n-    }\n-\n-    @JsonGetter(FIELD_PARTITION_SPEC)\n-    public Map<String, String> getPartitionSpec() {\n-        return partitionSpec;\n+    public CreatePartitionsRequest(\n+            @JsonProperty(FIELD_PARTITION_SPECS) List<Map<String, String>> partitionSpecs) {\n+        super(partitionSpecs);\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionsRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionsRequest.java\nnew file mode 100644\nindex 000000000000..d84c0d39008b\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionsRequest.java\n@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Request for dropping partition. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class DropPartitionsRequest extends BasePartitionsRequest {\n+\n+    @JsonCreator\n+    public DropPartitionsRequest(\n+            @JsonProperty(FIELD_PARTITION_SPECS) List<Map<String, String>> partitionSpecs) {\n+        super(partitionSpecs);\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/MarkDonePartitionsRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/MarkDonePartitionsRequest.java\nnew file mode 100644\nindex 000000000000..88345e962027\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/MarkDonePartitionsRequest.java\n@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Request for marking done partition. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class MarkDonePartitionsRequest extends BasePartitionsRequest {\n+\n+    @JsonCreator\n+    public MarkDonePartitionsRequest(\n+            @JsonProperty(FIELD_PARTITION_SPECS) List<Map<String, String>> partitionSpecs) {\n+        super(partitionSpecs);\n+    }\n+}\n\ndiff --git a/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java b/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\nindex ec9420dca4ff..4e2d7b6acb95 100644\n--- a/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\n+++ b/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\n@@ -71,6 +71,7 @@\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.metastore.api.UnknownTableException;\n import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -492,6 +493,8 @@ public void markDonePartitions(Identifier identifier, List<Map<String, String>>\n                     });\n         } catch (NoSuchObjectException e) {\n             // do nothing if the partition not exists\n+        } catch (UnknownTableException e) {\n+            throw new TableNotExistException(identifier);\n         } catch (TException | InterruptedException e) {\n             throw new RuntimeException(e);\n         }\n@@ -513,15 +516,30 @@ public List<org.apache.paimon.partition.Partition> listPartitions(Identifier ide\n                                                 Short.MAX_VALUE));\n                 return partitions.stream()\n                         .map(\n-                                part ->\n-                                        new org.apache.paimon.partition.Partition(\n-                                                Collections.singletonMap(\n-                                                        tagToPartitionField,\n-                                                        part.getValues().get(0)),\n-                                                1L,\n-                                                1L,\n-                                                1L,\n-                                                System.currentTimeMillis()))\n+                                part -> {\n+                                    Map<String, String> parameters = part.getParameters();\n+                                    long recordCount =\n+                                            Long.parseLong(\n+                                                    parameters.getOrDefault(NUM_ROWS_PROP, \""1\""));\n+                                    long fileSizeInBytes =\n+                                            Long.parseLong(\n+                                                    parameters.getOrDefault(TOTAL_SIZE_PROP, \""1\""));\n+                                    long fileCount =\n+                                            Long.parseLong(\n+                                                    parameters.getOrDefault(NUM_FILES_PROP, \""1\""));\n+                                    long lastFileCreationTime =\n+                                            Long.parseLong(\n+                                                    parameters.getOrDefault(\n+                                                            LAST_UPDATE_TIME_PROP,\n+                                                            System.currentTimeMillis() + \""\""));\n+                                    return new org.apache.paimon.partition.Partition(\n+                                            Collections.singletonMap(\n+                                                    tagToPartitionField, part.getValues().get(0)),\n+                                            recordCount,\n+                                            fileSizeInBytes,\n+                                            fileCount,\n+                                            lastFileCreationTime);\n+                                })\n                         .collect(Collectors.toList());\n             } catch (Exception e) {\n                 throw new RuntimeException(e);\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 4bf2b879d8b5..41d2632454db 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -446,8 +446,8 @@ paths:\n     post:\n       tags:\n         - partition\n-      summary: Create partition\n-      operationId: createPartition\n+      summary: Create partitions\n+      operationId: createPartitions\n       parameters:\n         - name: prefix\n           in: path\n@@ -468,14 +468,48 @@ paths:\n         content:\n           application/json:\n             schema:\n-              $ref: '#/components/schemas/CreatePartitionRequest'\n+              $ref: '#/components/schemas/CreatePartitionsRequest'\n       responses:\n         \""200\"":\n-          description: OK\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n           content:\n             application/json:\n               schema:\n-                $ref: '#/components/schemas/PartitionResponse'\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/tables/{table}/partitions/drop:\n+    post:\n+      tags:\n+        - partition\n+      summary: Drop partitions\n+      operationId: dropPartitions\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/DropPartitionsRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -484,11 +518,50 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-    delete:\n+  /v1/{prefix}/databases/{database}/tables/{table}/partitions/alter:\n+    post:\n+      tags:\n+        - partition\n+      summary: Alter partitions\n+      operationId: alterPartitions\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/AlterPartitionsRequest'\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/tables/{table}/partitions/mark:\n+    post:\n       tags:\n         - partition\n-      summary: Drop partition\n-      operationId: dropPartition\n+      summary: MarkDone partitions\n+      operationId: markDonePartitions\n       parameters:\n         - name: prefix\n           in: path\n@@ -509,7 +582,7 @@ paths:\n         content:\n           application/json:\n             schema:\n-              $ref: '#/components/schemas/DropPartitionRequest'\n+              $ref: '#/components/schemas/MarkDonePartitionsRequest'\n       responses:\n         \""200\"":\n           description: Success, no content\n@@ -532,18 +605,34 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n-    CreatePartitionRequest:\n+    CreatePartitionsRequest:\n       type: object\n       properties:\n-        identifier:\n-          $ref: '#/components/schemas/Identifier'\n-        spec:\n-          type: object\n-    DropPartitionRequest:\n+        specs:\n+          type: array\n+          items:\n+            type: object\n+    DropPartitionsRequest:\n       type: object\n       properties:\n-        spec:\n-          type: object\n+        specs:\n+          type: array\n+          items:\n+            type: object\n+    AlterTableRequest:\n+      type: object\n+      properties:\n+        changes:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/SchemaChange'\n+    MarkDonePartitionsRequest:\n+      type: object\n+      properties:\n+        specs:\n+          type: array\n+          items:\n+            type: object\n     CreateDatabaseResponse:\n       type: object\n       properties:\n@@ -673,13 +762,8 @@ components:\n           format: int64\n         schema:\n           $ref: '#/components/schemas/Schema'\n-    AlterTableRequest:\n-      type: object\n-      properties:\n-        changes:\n-          type: array\n-          items:\n-            $ref: '#/components/schemas/SchemaChange'\n+        uuid:\n+          type: string\n     SchemaChange:\n       anyOf:\n         - $ref: '#/components/schemas/SetOption'\n@@ -827,6 +911,13 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n+    AlterPartitionsRequest:\n+      type: object\n+      properties:\n+        partitions:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/Partition'\n     AlterDatabaseResponse:\n       type: object\n       properties:\n@@ -885,11 +976,6 @@ components:\n           type: array\n           items:\n             $ref: '#/components/schemas/Partition'\n-    PartitionResponse:\n-      type: object\n-      properties:\n-        partition:\n-          $ref: '#/components/schemas/Partition'\n     Partition:\n       type: object\n       properties:\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex e2bd33769280..a9f3d02f5442 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -21,11 +21,13 @@\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.ResourcePaths;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterPartitionsRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n-import org.apache.paimon.rest.requests.CreatePartitionRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n-import org.apache.paimon.rest.requests.DropPartitionRequest;\n+import org.apache.paimon.rest.requests.DropPartitionsRequest;\n+import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -36,7 +38,6 @@\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n-import org.apache.paimon.rest.responses.PartitionResponse;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n@@ -391,9 +392,7 @@ public ListPartitionsResponse listPartitions(\n             summary = \""Create partition\"",\n             tags = {\""partition\""})\n     @ApiResponses({\n-        @ApiResponse(\n-                responseCode = \""200\"",\n-                content = {@Content(schema = @Schema(implementation = PartitionResponse.class))}),\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""404\"",\n                 description = \""Resource not found\"",\n@@ -403,18 +402,54 @@ public ListPartitionsResponse listPartitions(\n                 content = {@Content(schema = @Schema())})\n     })\n     @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions\"")\n-    public PartitionResponse createPartition(\n+    public void createPartitions(\n             @PathVariable String prefix,\n             @PathVariable String database,\n             @PathVariable String table,\n-            @RequestBody CreatePartitionRequest request) {\n-        Map<String, String> spec = new HashMap<>();\n-        spec.put(\""f1\"", \""1\"");\n-        return new PartitionResponse(new Partition(spec, 0, 0, 0, 4));\n-    }\n+            @RequestBody CreatePartitionsRequest request) {}\n+\n+    @Operation(\n+            summary = \""Drop partitions\"",\n+            tags = {\""partition\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions/drop\"")\n+    public void dropPartitions(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table,\n+            @RequestBody DropPartitionsRequest request) {}\n+\n+    @Operation(\n+            summary = \""Alter partitions\"",\n+            tags = {\""partition\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions/alter\"")\n+    public void alterPartitions(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table,\n+            @RequestBody AlterPartitionsRequest request) {}\n \n     @Operation(\n-            summary = \""Drop partition\"",\n+            summary = \""MarkDone partitions\"",\n             tags = {\""partition\""})\n     @ApiResponses({\n         @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n@@ -426,10 +461,10 @@ public PartitionResponse createPartition(\n                 responseCode = \""500\"",\n                 content = {@Content(schema = @Schema())})\n     })\n-    @DeleteMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions\"")\n-    public void dropPartition(\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions/mark\"")\n+    public void markDonePartitions(\n             @PathVariable String prefix,\n             @PathVariable String database,\n             @PathVariable String table,\n-            @RequestBody DropPartitionRequest request) {}\n+            @RequestBody MarkDonePartitionsRequest request) {}\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\nindex 6448972cde04..c8b9192c1c38 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n@@ -23,6 +23,7 @@\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.partition.Partition;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.table.FileStoreTable;\n@@ -43,12 +44,15 @@\n import org.junit.jupiter.api.io.TempDir;\n \n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n+import static org.apache.paimon.CoreOptions.METASTORE_PARTITIONED_TABLE;\n+import static org.apache.paimon.CoreOptions.METASTORE_TAG_TO_PARTITION;\n import static org.apache.paimon.catalog.Catalog.SYSTEM_DATABASE_NAME;\n import static org.apache.paimon.table.system.AllTableOptionsTable.ALL_TABLE_OPTIONS;\n import static org.apache.paimon.table.system.CatalogOptionsTable.CATALOG_OPTIONS;\n@@ -57,6 +61,7 @@\n import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatExceptionOfType;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertFalse;\n \n@@ -1025,6 +1030,102 @@ public void testTableUUID() throws Exception {\n                 .isGreaterThan(0);\n     }\n \n+    @Test\n+    public void testPartitions() throws Exception {\n+        if (!supportPartitions()) {\n+            return;\n+        }\n+        String databaseName = \""testPartitionTable\"";\n+        List<Map<String, String>> partitionSpecs =\n+                Arrays.asList(\n+                        Collections.singletonMap(\""dt\"", \""20250101\""),\n+                        Collections.singletonMap(\""dt\"", \""20250102\""));\n+        catalog.dropDatabase(databaseName, true, true);\n+        catalog.createDatabase(databaseName, true);\n+        Identifier identifier = Identifier.create(databaseName, \""table\"");\n+        catalog.createTable(\n+                identifier,\n+                Schema.newBuilder()\n+                        .option(METASTORE_PARTITIONED_TABLE.key(), \""true\"")\n+                        .option(METASTORE_TAG_TO_PARTITION.key(), \""dt\"")\n+                        .column(\""col\"", DataTypes.INT())\n+                        .column(\""dt\"", DataTypes.STRING())\n+                        .partitionKeys(\""dt\"")\n+                        .build(),\n+                true);\n+\n+        catalog.createPartitions(identifier, partitionSpecs);\n+        assertThat(catalog.listPartitions(identifier).stream().map(Partition::spec))\n+                .containsExactlyInAnyOrder(partitionSpecs.get(0), partitionSpecs.get(1));\n+\n+        assertDoesNotThrow(() -> catalog.markDonePartitions(identifier, partitionSpecs));\n+\n+        catalog.dropPartitions(identifier, partitionSpecs);\n+        assertThat(catalog.listPartitions(identifier)).isEmpty();\n+\n+        // Test when table does not exist\n+        assertThatExceptionOfType(Catalog.TableNotExistException.class)\n+                .isThrownBy(\n+                        () ->\n+                                catalog.createPartitions(\n+                                        Identifier.create(databaseName, \""non_existing_table\""),\n+                                        partitionSpecs));\n+        assertThatExceptionOfType(Catalog.TableNotExistException.class)\n+                .isThrownBy(\n+                        () ->\n+                                catalog.listPartitions(\n+                                        Identifier.create(databaseName, \""non_existing_table\"")));\n+        assertThatExceptionOfType(Catalog.TableNotExistException.class)\n+                .isThrownBy(\n+                        () ->\n+                                catalog.markDonePartitions(\n+                                        Identifier.create(databaseName, \""non_existing_table\""),\n+                                        partitionSpecs));\n+    }\n+\n+    @Test\n+    public void testAlterPartitions() throws Exception {\n+        if (!supportPartitions()) {\n+            return;\n+        }\n+        String databaseName = \""testAlterPartitionTable\"";\n+        catalog.dropDatabase(databaseName, true, true);\n+        catalog.createDatabase(databaseName, true);\n+        Identifier alterIdentifier = Identifier.create(databaseName, \""alert_partitions\"");\n+        catalog.createTable(\n+                alterIdentifier,\n+                Schema.newBuilder()\n+                        .option(METASTORE_PARTITIONED_TABLE.key(), \""true\"")\n+                        .option(METASTORE_TAG_TO_PARTITION.key(), \""dt\"")\n+                        .column(\""col\"", DataTypes.INT())\n+                        .column(\""dt\"", DataTypes.STRING())\n+                        .partitionKeys(\""dt\"")\n+                        .build(),\n+                true);\n+        catalog.createPartitions(\n+                alterIdentifier, Arrays.asList(Collections.singletonMap(\""dt\"", \""20250101\"")));\n+        assertThat(catalog.listPartitions(alterIdentifier).stream().map(Partition::spec))\n+                .containsExactlyInAnyOrder(Collections.singletonMap(\""dt\"", \""20250101\""));\n+        Partition partition =\n+                new Partition(\n+                        Collections.singletonMap(\""dt\"", \""20250101\""),\n+                        1,\n+                        2,\n+                        3,\n+                        System.currentTimeMillis());\n+        catalog.alterPartitions(alterIdentifier, Arrays.asList(partition));\n+        Partition partitionFromServer = catalog.listPartitions(alterIdentifier).get(0);\n+        checkPartition(partition, partitionFromServer);\n+\n+        // Test when table does not exist\n+        assertThatExceptionOfType(Catalog.TableNotExistException.class)\n+                .isThrownBy(\n+                        () ->\n+                                catalog.alterPartitions(\n+                                        Identifier.create(databaseName, \""non_existing_table\""),\n+                                        Arrays.asList(partition)));\n+    }\n+\n     protected boolean supportsAlterDatabase() {\n         return false;\n     }\n@@ -1036,4 +1137,12 @@ protected boolean supportsFormatTable() {\n     protected boolean supportsView() {\n         return false;\n     }\n+\n+    protected void checkPartition(Partition expected, Partition actual) {\n+        assertThat(actual).isEqualTo(expected);\n+    }\n+\n+    protected boolean supportPartitions() {\n+        return false;\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex ca6be9d00d82..822e06d7fbbc 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -21,11 +21,12 @@\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterPartitionsRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n-import org.apache.paimon.rest.requests.CreatePartitionRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n-import org.apache.paimon.rest.requests.DropPartitionRequest;\n+import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n@@ -34,7 +35,6 @@\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n-import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.types.DataField;\n@@ -46,8 +46,6 @@\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n-import okhttp3.mockwebserver.MockResponse;\n-\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n@@ -135,23 +133,18 @@ public static AlterTableRequest alterTableRequest() {\n         return new AlterTableRequest(getChanges());\n     }\n \n-    public static CreatePartitionRequest createPartitionRequest(String tableName) {\n-        Identifier identifier = Identifier.create(databaseName(), tableName);\n-        return new CreatePartitionRequest(identifier, Collections.singletonMap(\""pt\"", \""1\""));\n+    public static CreatePartitionsRequest createPartitionRequest() {\n+        return new CreatePartitionsRequest(ImmutableList.of(Collections.singletonMap(\""pt\"", \""1\"")));\n     }\n \n-    public static DropPartitionRequest dropPartitionRequest() {\n-        return new DropPartitionRequest(Collections.singletonMap(\""pt\"", \""1\""));\n+    public static DropPartitionsRequest dropPartitionsRequest() {\n+        return new DropPartitionsRequest(ImmutableList.of(Collections.singletonMap(\""pt\"", \""1\"")));\n     }\n \n-    public static PartitionResponse partitionResponse() {\n+    public static ListPartitionsResponse listPartitionsResponse() {\n         Map<String, String> spec = new HashMap<>();\n         spec.put(\""f0\"", \""1\"");\n-        return new PartitionResponse(new Partition(spec, 1, 1, 1, 1));\n-    }\n-\n-    public static ListPartitionsResponse listPartitionsResponse() {\n-        Partition partition = partitionResponse().getPartition();\n+        Partition partition = new Partition(spec, 1, 1, 1, 1);\n         return new ListPartitionsResponse(ImmutableList.of(partition));\n     }\n \n@@ -233,11 +226,12 @@ public static GetTableResponse getTableResponse() {\n         return new GetTableResponse(UUID.randomUUID().toString(), \""\"", 1, schema(options));\n     }\n \n-    public static MockResponse mockResponse(String body, int httpCode) {\n-        return new MockResponse()\n-                .setResponseCode(httpCode)\n-                .setBody(body)\n-                .addHeader(\""Content-Type\"", \""application/json\"");\n+    public static AlterPartitionsRequest alterPartitionsRequest() {\n+        return new AlterPartitionsRequest(ImmutableList.of(partition()));\n+    }\n+\n+    private static Partition partition() {\n+        return new Partition(Collections.singletonMap(\""pt\"", \""1\""), 1, 1, 1, 1);\n     }\n \n     private static Schema schema(Map<String, String> options) {\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nindex 4e0c911c382d..1c802cf4c0ef 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -20,15 +20,18 @@\n \n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogContext;\n-import org.apache.paimon.catalog.CatalogFactory;\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n+import org.apache.paimon.rest.requests.AlterPartitionsRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionsRequest;\n+import org.apache.paimon.rest.requests.MarkDonePartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n@@ -38,7 +41,11 @@\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.schema.Schema;\n import org.apache.paimon.table.FileStoreTable;\n+import org.apache.paimon.table.FormatTable;\n+import org.apache.paimon.table.Table;\n+import org.apache.paimon.types.DataField;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n \n@@ -68,9 +75,7 @@ public RESTCatalogServer(String warehouse, String initToken) {\n         authToken = initToken;\n         Options conf = new Options();\n         conf.setString(\""warehouse\"", warehouse);\n-        this.catalog =\n-                CatalogFactory.createCatalog(\n-                        CatalogContext.create(conf), this.getClass().getClassLoader());\n+        this.catalog = TestRESTCatalog.create(CatalogContext.create(conf));\n         this.dispatcher = initDispatcher(catalog, authToken);\n         MockWebServer mockWebServer = new MockWebServer();\n         mockWebServer.setDispatcher(dispatcher);\n@@ -119,13 +124,55 @@ public MockResponse dispatch(RecordedRequest request) {\n                                 resources.length == 4\n                                         && \""tables\"".equals(resources[1])\n                                         && \""partitions\"".equals(resources[3]);\n-                        if (isPartitions) {\n+\n+                        boolean isDropPartitions =\n+                                resources.length == 5\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""partitions\"".equals(resources[3])\n+                                        && \""drop\"".equals(resources[4]);\n+                        boolean isAlterPartitions =\n+                                resources.length == 5\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""partitions\"".equals(resources[3])\n+                                        && \""alter\"".equals(resources[4]);\n+                        boolean isMarkDonePartitions =\n+                                resources.length == 5\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""partitions\"".equals(resources[3])\n+                                        && \""mark\"".equals(resources[4]);\n+                        if (isDropPartitions) {\n+                            String tableName = resources[2];\n+                            Identifier identifier = Identifier.create(databaseName, tableName);\n+                            DropPartitionsRequest dropPartitionsRequest =\n+                                    OBJECT_MAPPER.readValue(\n+                                            request.getBody().readUtf8(),\n+                                            DropPartitionsRequest.class);\n+                            catalog.dropPartitions(\n+                                    identifier, dropPartitionsRequest.getPartitionSpecs());\n+                            return new MockResponse().setResponseCode(200);\n+                        } else if (isAlterPartitions) {\n                             String tableName = resources[2];\n-                            List<Partition> partitions =\n-                                    catalog.listPartitions(\n-                                            Identifier.create(databaseName, tableName));\n-                            response = new ListPartitionsResponse(partitions);\n-                            return mockResponse(response, 200);\n+                            Identifier identifier = Identifier.create(databaseName, tableName);\n+                            AlterPartitionsRequest alterPartitionsRequest =\n+                                    OBJECT_MAPPER.readValue(\n+                                            request.getBody().readUtf8(),\n+                                            AlterPartitionsRequest.class);\n+                            catalog.alterPartitions(\n+                                    identifier, alterPartitionsRequest.getPartitions());\n+                            return new MockResponse().setResponseCode(200);\n+                        } else if (isMarkDonePartitions) {\n+                            String tableName = resources[2];\n+                            Identifier identifier = Identifier.create(databaseName, tableName);\n+                            MarkDonePartitionsRequest markDonePartitionsRequest =\n+                                    OBJECT_MAPPER.readValue(\n+                                            request.getBody().readUtf8(),\n+                                            MarkDonePartitionsRequest.class);\n+                            catalog.markDonePartitions(\n+                                    identifier, markDonePartitionsRequest.getPartitionSpecs());\n+                            return new MockResponse().setResponseCode(200);\n+                        } else if (isPartitions) {\n+                            String tableName = resources[2];\n+                            return partitionsApiHandler(catalog, request, databaseName, tableName);\n                         } else if (isTableRename) {\n                             return renameTableApiHandler(\n                                     catalog, request, databaseName, resources[2]);\n@@ -216,106 +263,142 @@ private static MockResponse renameTableApiHandler(\n                 OBJECT_MAPPER.readValue(request.getBody().readUtf8(), RenameTableRequest.class);\n         catalog.renameTable(\n                 Identifier.create(databaseName, tableName), requestBody.getNewIdentifier(), false);\n-        FileStoreTable table = (FileStoreTable) catalog.getTable(requestBody.getNewIdentifier());\n-        RESTResponse response =\n-                new GetTableResponse(\n-                        UUID.randomUUID().toString(),\n-                        tableName,\n-                        table.schema().id(),\n-                        table.schema().toSchema());\n+        GetTableResponse response =\n+                getTable(\n+                        catalog,\n+                        requestBody.getNewIdentifier().getDatabaseName(),\n+                        requestBody.getNewIdentifier().getTableName());\n         return mockResponse(response, 200);\n     }\n \n     private static MockResponse databasesApiHandler(Catalog catalog, RecordedRequest request)\n             throws Exception {\n         RESTResponse response;\n-        if (request.getMethod().equals(\""GET\"")) {\n-            List<String> databaseNameList = catalog.listDatabases();\n-            response = new ListDatabasesResponse(databaseNameList);\n-            return mockResponse(response, 200);\n-        } else if (request.getMethod().equals(\""POST\"")) {\n-            CreateDatabaseRequest requestBody =\n-                    OBJECT_MAPPER.readValue(\n-                            request.getBody().readUtf8(), CreateDatabaseRequest.class);\n-            String databaseName = requestBody.getName();\n-            catalog.createDatabase(databaseName, false);\n-            response = new CreateDatabaseResponse(databaseName, requestBody.getOptions());\n-            return mockResponse(response, 200);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                List<String> databaseNameList = catalog.listDatabases();\n+                response = new ListDatabasesResponse(databaseNameList);\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                CreateDatabaseRequest requestBody =\n+                        OBJECT_MAPPER.readValue(\n+                                request.getBody().readUtf8(), CreateDatabaseRequest.class);\n+                String databaseName = requestBody.getName();\n+                catalog.createDatabase(databaseName, false);\n+                response = new CreateDatabaseResponse(databaseName, requestBody.getOptions());\n+                return mockResponse(response, 200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n         }\n-        return new MockResponse().setResponseCode(404);\n     }\n \n     private static MockResponse databaseApiHandler(\n             Catalog catalog, RecordedRequest request, String databaseName) throws Exception {\n         RESTResponse response;\n-        if (request.getMethod().equals(\""GET\"")) {\n-            Database database = catalog.getDatabase(databaseName);\n-            response =\n-                    new GetDatabaseResponse(\n-                            UUID.randomUUID().toString(), database.name(), database.options());\n-            return mockResponse(response, 200);\n-        } else if (request.getMethod().equals(\""DELETE\"")) {\n-            catalog.dropDatabase(databaseName, false, true);\n-            return new MockResponse().setResponseCode(200);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                Database database = catalog.getDatabase(databaseName);\n+                response =\n+                        new GetDatabaseResponse(\n+                                UUID.randomUUID().toString(), database.name(), database.options());\n+                return mockResponse(response, 200);\n+            case \""DELETE\"":\n+                catalog.dropDatabase(databaseName, false, true);\n+                return new MockResponse().setResponseCode(200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n         }\n-        return new MockResponse().setResponseCode(404);\n     }\n \n     private static MockResponse tablesApiHandler(\n             Catalog catalog, RecordedRequest request, String databaseName) throws Exception {\n         RESTResponse response;\n-        if (request.getMethod().equals(\""POST\"")) {\n-            CreateTableRequest requestBody =\n-                    OBJECT_MAPPER.readValue(request.getBody().readUtf8(), CreateTableRequest.class);\n-            catalog.createTable(requestBody.getIdentifier(), requestBody.getSchema(), false);\n-            response =\n-                    new GetTableResponse(\n-                            UUID.randomUUID().toString(),\n-                            requestBody.getIdentifier().getTableName(),\n-                            1L,\n-                            requestBody.getSchema());\n-            return mockResponse(response, 200);\n-        } else if (request.getMethod().equals(\""GET\"")) {\n-            catalog.listTables(databaseName);\n-            response = new ListTablesResponse(catalog.listTables(databaseName));\n-            return mockResponse(response, 200);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                catalog.listTables(databaseName);\n+                response = new ListTablesResponse(catalog.listTables(databaseName));\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                CreateTableRequest requestBody =\n+                        OBJECT_MAPPER.readValue(\n+                                request.getBody().readUtf8(), CreateTableRequest.class);\n+                catalog.createTable(requestBody.getIdentifier(), requestBody.getSchema(), false);\n+                response =\n+                        new GetTableResponse(\n+                                UUID.randomUUID().toString(), \""\"", 1L, requestBody.getSchema());\n+                return mockResponse(response, 200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n         }\n-        return new MockResponse().setResponseCode(404);\n     }\n \n     private static MockResponse tableApiHandler(\n             Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n             throws Exception {\n         RESTResponse response;\n-        if (request.getMethod().equals(\""GET\"")) {\n-            Identifier identifier = Identifier.create(databaseName, tableName);\n-            FileStoreTable table = (FileStoreTable) catalog.getTable(identifier);\n-            response =\n-                    new GetTableResponse(\n-                            UUID.randomUUID().toString(),\n-                            tableName,\n-                            table.schema().id(),\n-                            table.schema().toSchema());\n-            return mockResponse(response, 200);\n-        } else if (request.getMethod().equals(\""POST\"")) {\n-            Identifier identifier = Identifier.create(databaseName, tableName);\n-            AlterTableRequest requestBody =\n-                    OBJECT_MAPPER.readValue(request.getBody().readUtf8(), AlterTableRequest.class);\n-            catalog.alterTable(identifier, requestBody.getChanges(), false);\n-            FileStoreTable table = (FileStoreTable) catalog.getTable(identifier);\n-            response =\n-                    new GetTableResponse(\n-                            UUID.randomUUID().toString(),\n-                            tableName,\n-                            table.schema().id(),\n-                            table.schema().toSchema());\n-            return mockResponse(response, 200);\n-        } else if (request.getMethod().equals(\""DELETE\"")) {\n-            Identifier identifier = Identifier.create(databaseName, tableName);\n-            catalog.dropTable(identifier, false);\n-            return new MockResponse().setResponseCode(200);\n+        Identifier identifier = Identifier.create(databaseName, tableName);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                response = getTable(catalog, databaseName, tableName);\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                AlterTableRequest requestBody =\n+                        OBJECT_MAPPER.readValue(\n+                                request.getBody().readUtf8(), AlterTableRequest.class);\n+                catalog.alterTable(identifier, requestBody.getChanges(), false);\n+                response = getTable(catalog, databaseName, tableName);\n+                return mockResponse(response, 200);\n+            case \""DELETE\"":\n+                catalog.dropTable(identifier, false);\n+                return new MockResponse().setResponseCode(200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+    }\n+\n+    private static MockResponse partitionsApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n+            throws Exception {\n+        RESTResponse response;\n+        Identifier identifier = Identifier.create(databaseName, tableName);\n+        switch (request.getMethod()) {\n+            case \""GET\"":\n+                List<Partition> partitions = catalog.listPartitions(identifier);\n+                response = new ListPartitionsResponse(partitions);\n+                return mockResponse(response, 200);\n+            case \""POST\"":\n+                CreatePartitionsRequest requestBody =\n+                        OBJECT_MAPPER.readValue(\n+                                request.getBody().readUtf8(), CreatePartitionsRequest.class);\n+                catalog.createPartitions(identifier, requestBody.getPartitionSpecs());\n+                return new MockResponse().setResponseCode(200);\n+            default:\n+                return new MockResponse().setResponseCode(404);\n+        }\n+    }\n+\n+    private static GetTableResponse getTable(Catalog catalog, String databaseName, String tableName)\n+            throws Exception {\n+        Identifier identifier = Identifier.create(databaseName, tableName);\n+        Table table = catalog.getTable(identifier);\n+        Schema schema;\n+        Long schemaId = 1L;\n+        if (table instanceof FileStoreTable) {\n+            FileStoreTable fileStoreTable = (FileStoreTable) table;\n+            schema = fileStoreTable.schema().toSchema();\n+            schemaId = fileStoreTable.schema().id();\n+        } else {\n+            FormatTable formatTable = (FormatTable) table;\n+            List<DataField> fields = formatTable.rowType().getFields();\n+            schema =\n+                    new Schema(\n+                            fields,\n+                            table.partitionKeys(),\n+                            table.primaryKeys(),\n+                            table.options(),\n+                            table.comment().orElse(null));\n         }\n-        return new MockResponse().setResponseCode(404);\n+        return new GetTableResponse(table.uuid(), table.name(), schemaId, schema);\n     }\n \n     private static MockResponse mockResponse(RESTResponse response, int httpCode) {\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex d9f3bd2a61c4..f63d70332650 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -29,6 +29,7 @@\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataTypes;\n \n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n \n@@ -40,6 +41,7 @@\n import java.util.List;\n import java.util.Map;\n \n+import static org.apache.paimon.CoreOptions.METASTORE_PARTITIONED_TABLE;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n \n@@ -89,7 +91,10 @@ void testAuthFail() {\n     @Test\n     void testListPartitionsWhenMetastorePartitionedIsTrue() throws Exception {\n         Identifier identifier = Identifier.create(\""test_db\"", \""test_table\"");\n-        createTable(identifier, Maps.newHashMap(), Lists.newArrayList(\""col1\""));\n+        createTable(\n+                identifier,\n+                ImmutableMap.of(METASTORE_PARTITIONED_TABLE.key(), \""\"" + true),\n+                Lists.newArrayList(\""col1\""));\n         List<Partition> result = catalog.listPartitions(identifier);\n         assertEquals(0, result.size());\n     }\n@@ -102,6 +107,16 @@ void testListPartitionsFromFile() throws Exception {\n         assertEquals(0, result.size());\n     }\n \n+    @Override\n+    protected boolean supportsFormatTable() {\n+        return true;\n+    }\n+\n+    @Override\n+    protected boolean supportPartitions() {\n+        return true;\n+    }\n+\n     private void createTable(\n             Identifier identifier, Map<String, String> options, List<String> partitionKeys)\n             throws Exception {\n@@ -109,7 +124,7 @@ private void createTable(\n         catalog.createTable(\n                 identifier,\n                 new Schema(\n-                        Lists.newArrayList(new DataField(0, \""col1\"", DataTypes.STRING())),\n+                        Lists.newArrayList(new DataField(0, \""col1\"", DataTypes.INT())),\n                         partitionKeys,\n                         Collections.emptyList(),\n                         options,\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 57db06f0d160..6e260a5e7341 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -19,11 +19,12 @@\n package org.apache.paimon.rest;\n \n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n+import org.apache.paimon.rest.requests.AlterPartitionsRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n-import org.apache.paimon.rest.requests.CreatePartitionRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionsRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n-import org.apache.paimon.rest.requests.DropPartitionRequest;\n+import org.apache.paimon.rest.requests.DropPartitionsRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -34,7 +35,6 @@\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n-import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.IntType;\n@@ -49,7 +49,6 @@\n \n import static org.apache.paimon.rest.RESTObjectMapper.OBJECT_MAPPER;\n import static org.junit.Assert.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n \n /** Test for {@link RESTObjectMapper}. */\n public class RESTObjectMapperTest {\n@@ -208,21 +207,20 @@ public void alterTableRequestParseTest() throws Exception {\n \n     @Test\n     public void createPartitionRequestParseTest() throws JsonProcessingException {\n-        CreatePartitionRequest request = MockRESTMessage.createPartitionRequest(\""t1\"");\n+        CreatePartitionsRequest request = MockRESTMessage.createPartitionRequest();\n         String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n-        CreatePartitionRequest parseData =\n-                OBJECT_MAPPER.readValue(requestStr, CreatePartitionRequest.class);\n-        assertEquals(parseData.getIdentifier(), parseData.getIdentifier());\n-        assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n+        CreatePartitionsRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, CreatePartitionsRequest.class);\n+        assertEquals(parseData.getPartitionSpecs().size(), parseData.getPartitionSpecs().size());\n     }\n \n     @Test\n     public void dropPartitionRequestParseTest() throws JsonProcessingException {\n-        DropPartitionRequest request = MockRESTMessage.dropPartitionRequest();\n+        DropPartitionsRequest request = MockRESTMessage.dropPartitionsRequest();\n         String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n-        DropPartitionRequest parseData =\n-                OBJECT_MAPPER.readValue(requestStr, DropPartitionRequest.class);\n-        assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n+        DropPartitionsRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, DropPartitionsRequest.class);\n+        assertEquals(parseData.getPartitionSpecs().size(), parseData.getPartitionSpecs().size());\n     }\n \n     @Test\n@@ -237,13 +235,11 @@ public void listPartitionsResponseParseTest() throws Exception {\n     }\n \n     @Test\n-    public void partitionResponseParseTest() throws Exception {\n-        PartitionResponse response = MockRESTMessage.partitionResponse();\n-        assertDoesNotThrow(() -> OBJECT_MAPPER.writeValueAsString(response));\n-        assertDoesNotThrow(\n-                () ->\n-                        OBJECT_MAPPER.readValue(\n-                                OBJECT_MAPPER.writeValueAsString(response),\n-                                PartitionResponse.class));\n+    public void alterPartitionsRequestParseTest() throws Exception {\n+        AlterPartitionsRequest request = MockRESTMessage.alterPartitionsRequest();\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        AlterPartitionsRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, AlterPartitionsRequest.class);\n+        assertEquals(request.getPartitions(), parseData.getPartitions());\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java b/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java\nnew file mode 100644\nindex 000000000000..a0f820e7ad0b\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/TestRESTCatalog.java\n@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import org.apache.paimon.TableType;\n+import org.apache.paimon.catalog.CatalogContext;\n+import org.apache.paimon.catalog.CatalogFactory;\n+import org.apache.paimon.catalog.CatalogUtils;\n+import org.apache.paimon.catalog.FileSystemCatalog;\n+import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.fs.FileIO;\n+import org.apache.paimon.fs.Path;\n+import org.apache.paimon.options.Options;\n+import org.apache.paimon.partition.Partition;\n+import org.apache.paimon.schema.Schema;\n+import org.apache.paimon.schema.SchemaChange;\n+import org.apache.paimon.schema.TableSchema;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/** A catalog for testing RESTCatalog. */\n+public class TestRESTCatalog extends FileSystemCatalog {\n+\n+    public Map<String, TableSchema> tableFullName2Schema = new HashMap<String, TableSchema>();\n+    public Map<String, List<Partition>> tableFullName2Partitions =\n+            new HashMap<String, List<Partition>>();\n+\n+    public TestRESTCatalog(FileIO fileIO, Path warehouse, Options options) {\n+        super(fileIO, warehouse, options);\n+    }\n+\n+    public static TestRESTCatalog create(CatalogContext context) {\n+        String warehouse = CatalogFactory.warehouse(context).toUri().toString();\n+\n+        Path warehousePath = new Path(warehouse);\n+        FileIO fileIO;\n+\n+        try {\n+            fileIO = FileIO.get(warehousePath, context);\n+            fileIO.checkOrMkdirs(warehousePath);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+\n+        return new TestRESTCatalog(fileIO, warehousePath, context.options());\n+    }\n+\n+    @Override\n+    public void createPartitions(Identifier identifier, List<Map<String, String>> partitions)\n+            throws TableNotExistException {\n+        getTable(identifier);\n+        tableFullName2Partitions.put(\n+                identifier.getFullName(),\n+                partitions.stream()\n+                        .map(partition -> spec2Partition(partition))\n+                        .collect(Collectors.toList()));\n+    }\n+\n+    @Override\n+    public void dropPartitions(Identifier identifier, List<Map<String, String>> partitions)\n+            throws TableNotExistException {\n+        getTable(identifier);\n+        List<Partition> existPartitions = tableFullName2Partitions.get(identifier.getFullName());\n+        partitions.forEach(\n+                partition -> {\n+                    for (Map.Entry<String, String> entry : partition.entrySet()) {\n+                        existPartitions.stream()\n+                                .filter(\n+                                        p ->\n+                                                p.spec().containsKey(entry.getKey())\n+                                                        && p.spec()\n+                                                                .get(entry.getKey())\n+                                                                .equals(entry.getValue()))\n+                                .findFirst()\n+                                .ifPresent(\n+                                        existPartition -> existPartitions.remove(existPartition));\n+                    }\n+                });\n+    }\n+\n+    @Override\n+    public void alterPartitions(Identifier identifier, List<Partition> partitions)\n+            throws TableNotExistException {\n+        getTable(identifier);\n+        List<Partition> existPartitions = tableFullName2Partitions.get(identifier.getFullName());\n+        partitions.forEach(\n+                partition -> {\n+                    for (Map.Entry<String, String> entry : partition.spec().entrySet()) {\n+                        existPartitions.stream()\n+                                .filter(\n+                                        p ->\n+                                                p.spec().containsKey(entry.getKey())\n+                                                        && p.spec()\n+                                                                .get(entry.getKey())\n+                                                                .equals(entry.getValue()))\n+                                .findFirst()\n+                                .ifPresent(\n+                                        existPartition -> existPartitions.remove(existPartition));\n+                    }\n+                });\n+        existPartitions.addAll(partitions);\n+        tableFullName2Partitions.put(identifier.getFullName(), existPartitions);\n+    }\n+\n+    @Override\n+    public List<Partition> listPartitions(Identifier identifier) throws TableNotExistException {\n+        getTable(identifier);\n+        return tableFullName2Partitions.get(identifier.getFullName());\n+    }\n+\n+    @Override\n+    protected List<String> listTablesImpl(String databaseName) {\n+        List<String> tables = super.listTablesImpl(databaseName);\n+        for (Map.Entry<String, TableSchema> entry : tableFullName2Schema.entrySet()) {\n+            Identifier identifier = Identifier.fromString(entry.getKey());\n+            if (databaseName.equals(identifier.getDatabaseName())) {\n+                tables.add(identifier.getTableName());\n+            }\n+        }\n+        return tables;\n+    }\n+\n+    @Override\n+    protected void dropTableImpl(Identifier identifier) {\n+        if (tableFullName2Schema.containsKey(identifier.getFullName())) {\n+            tableFullName2Schema.remove(identifier.getFullName());\n+        } else {\n+            super.dropTableImpl(identifier);\n+        }\n+    }\n+\n+    @Override\n+    public void renameTableImpl(Identifier fromTable, Identifier toTable) {\n+        if (tableFullName2Schema.containsKey(fromTable.getFullName())) {\n+            TableSchema tableSchema = tableFullName2Schema.get(fromTable.getFullName());\n+            tableFullName2Schema.remove(fromTable.getFullName());\n+            tableFullName2Schema.put(toTable.getFullName(), tableSchema);\n+        } else {\n+            super.renameTableImpl(fromTable, toTable);\n+        }\n+    }\n+\n+    @Override\n+    protected void alterTableImpl(Identifier identifier, List<SchemaChange> changes)\n+            throws TableNotExistException, ColumnAlreadyExistException, ColumnNotExistException {\n+        if (tableFullName2Schema.containsKey(identifier.getFullName())) {\n+            TableSchema schema = tableFullName2Schema.get(identifier.getFullName());\n+            if (CatalogUtils.getTableType(schema.options()) == TableType.FORMAT_TABLE) {\n+                throw new UnsupportedOperationException(\""Only data table support alter table.\"");\n+            }\n+        } else {\n+            super.alterTableImpl(identifier, changes);\n+        }\n+    }\n+\n+    @Override\n+    public void createFormatTable(Identifier identifier, Schema schema) {\n+        TableSchema tableSchema =\n+                new TableSchema(\n+                        1L,\n+                        schema.fields(),\n+                        1,\n+                        schema.partitionKeys(),\n+                        schema.primaryKeys(),\n+                        schema.options(),\n+                        schema.comment());\n+        tableFullName2Schema.put(identifier.getFullName(), tableSchema);\n+    }\n+\n+    @Override\n+    protected TableMeta getDataTableMeta(Identifier identifier) throws TableNotExistException {\n+        if (tableFullName2Schema.containsKey(identifier.getFullName())) {\n+            TableSchema tableSchema = tableFullName2Schema.get(identifier.getFullName());\n+            return new TableMeta(tableSchema, \""uuid\"");\n+        }\n+        return super.getDataTableMeta(identifier);\n+    }\n+\n+    private Partition spec2Partition(Map<String, String> spec) {\n+        return new Partition(spec, 123, 456, 789, 123);\n+    }\n+}\n\ndiff --git a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\nindex e185e5acbf50..ea669d254fbd 100644\n--- a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n+++ b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n@@ -56,7 +56,6 @@\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTORECONNECTURLKEY;\n-import static org.apache.paimon.CoreOptions.METASTORE_PARTITIONED_TABLE;\n import static org.apache.paimon.CoreOptions.METASTORE_TAG_TO_PARTITION;\n import static org.apache.paimon.hive.HiveCatalog.PAIMON_TABLE_IDENTIFIER;\n import static org.apache.paimon.hive.HiveCatalog.TABLE_TYPE_PROP;\n@@ -410,11 +409,22 @@ protected boolean supportsView() {\n         return true;\n     }\n \n+    @Override\n+    protected boolean supportPartitions() {\n+        return true;\n+    }\n+\n     @Override\n     protected boolean supportsFormatTable() {\n         return true;\n     }\n \n+    @Override\n+    protected void checkPartition(Partition expected, Partition actual) {\n+        assertThat(actual.recordCount()).isEqualTo(expected.recordCount());\n+        assertThat(actual.lastFileCreationTime()).isEqualTo(expected.lastFileCreationTime() / 1000);\n+    }\n+\n     @Test\n     public void testCreateExternalTableWithLocation(@TempDir java.nio.file.Path tempDir)\n             throws Exception {\n@@ -473,32 +483,6 @@ public void testTagToPartitionTable() throws Exception {\n                         Collections.singletonMap(\""dt\"", \""20250101\""));\n     }\n \n-    @Test\n-    public void testPartitionTable() throws Exception {\n-        String databaseName = \""testPartitionTable\"";\n-        catalog.dropDatabase(databaseName, true, true);\n-        catalog.createDatabase(databaseName, true);\n-        Identifier identifier = Identifier.create(databaseName, \""table\"");\n-        catalog.createTable(\n-                identifier,\n-                Schema.newBuilder()\n-                        .option(METASTORE_PARTITIONED_TABLE.key(), \""true\"")\n-                        .column(\""col\"", DataTypes.INT())\n-                        .column(\""dt\"", DataTypes.STRING())\n-                        .partitionKeys(\""dt\"")\n-                        .build(),\n-                true);\n-\n-        catalog.createPartitions(\n-                identifier,\n-                Arrays.asList(\n-                        Collections.singletonMap(\""dt\"", \""20250101\""),\n-                        Collections.singletonMap(\""dt\"", \""20250102\"")));\n-\n-        // hive catalog list partitions from filesystem, so here return empty.\n-        assertThat(catalog.listPartitions(identifier)).isEmpty();\n-    }\n-\n     @Override\n     protected boolean supportsAlterDatabase() {\n         return true;\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4805"", ""pr_id"": 4805, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 12, ""non_test_files_count"": 20, ""pr_changed_files"": [""paimon-core/pom.xml"", ""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ForbiddenException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NotAuthorizedException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/RESTException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceFailureException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceUnavailableException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/UnsupportedOperationException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java"", ""paimon-flink/paimon-flink-common/pom.xml"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RESTCatalogITCase.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java"", ""pom.xml""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RESTCatalogITCase.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java""], ""base_commit"": ""24c703aac03ff2db51a6522c82a39bff0f9c1043"", ""head_commit"": ""84169597ead214dbd9387e7fcb4740ab425b4831"", ""repo_url"": ""https://github.com/apache/paimon/pull/4805"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4805"", ""dockerfile"": """", ""pr_merged_at"": ""2025-01-09T12:06:31.000Z"", ""patch"": ""diff --git a/paimon-core/pom.xml b/paimon-core/pom.xml\nindex 6e9dfa716a05..586483d4ac6f 100644\n--- a/paimon-core/pom.xml\n+++ b/paimon-core/pom.xml\n@@ -33,7 +33,6 @@ under the License.\n \n     <properties>\n         <frocksdbjni.version>6.20.3-ververica-2.0</frocksdbjni.version>\n-        <okhttp.version>4.12.0</okhttp.version>\n     </properties>\n \n     <dependencies>\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex 2ecbcf61b334..a4c47f54a6ab 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -63,6 +63,7 @@\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemTable;\n import static org.apache.paimon.catalog.CatalogUtils.isSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.listPartitionsFromFileSystem;\n+import static org.apache.paimon.catalog.CatalogUtils.validateAutoCreateClose;\n import static org.apache.paimon.options.CatalogOptions.LOCK_ENABLED;\n import static org.apache.paimon.options.CatalogOptions.LOCK_TYPE;\n import static org.apache.paimon.table.system.AllTableOptionsTable.ALL_TABLE_OPTIONS;\n@@ -504,17 +505,6 @@ private void copyTableDefaultOptions(Map<String, String> options) {\n         tableDefaultOptions.forEach(options::putIfAbsent);\n     }\n \n-    private void validateAutoCreateClose(Map<String, String> options) {\n-        checkArgument(\n-                !Boolean.parseBoolean(\n-                        options.getOrDefault(\n-                                CoreOptions.AUTO_CREATE.key(),\n-                                CoreOptions.AUTO_CREATE.defaultValue().toString())),\n-                String.format(\n-                        \""The value of %s property should be %s.\"",\n-                        CoreOptions.AUTO_CREATE.key(), Boolean.FALSE));\n-    }\n-\n     private void validateCustomTablePath(Map<String, String> options) {\n         if (!allowCustomTablePath() && options.containsKey(CoreOptions.PATH.key())) {\n             throw new UnsupportedOperationException(\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\nindex fabfa50fc4d7..9267532f9d22 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/CatalogUtils.java\n@@ -18,6 +18,7 @@\n \n package org.apache.paimon.catalog;\n \n+import org.apache.paimon.CoreOptions;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.Options;\n@@ -38,6 +39,7 @@\n import static org.apache.paimon.catalog.Catalog.SYSTEM_DATABASE_NAME;\n import static org.apache.paimon.catalog.Catalog.TABLE_DEFAULT_OPTION_PREFIX;\n import static org.apache.paimon.options.OptionsUtils.convertToPropertiesPrefixKey;\n+import static org.apache.paimon.utils.Preconditions.checkArgument;\n \n /** Utils for {@link Catalog}. */\n public class CatalogUtils {\n@@ -108,6 +110,17 @@ public static void checkNotBranch(Identifier identifier, String method) {\n         }\n     }\n \n+    public static void validateAutoCreateClose(Map<String, String> options) {\n+        checkArgument(\n+                !Boolean.parseBoolean(\n+                        options.getOrDefault(\n+                                CoreOptions.AUTO_CREATE.key(),\n+                                CoreOptions.AUTO_CREATE.defaultValue().toString())),\n+                String.format(\n+                        \""The value of %s property should be %s.\"",\n+                        CoreOptions.AUTO_CREATE.key(), Boolean.FALSE));\n+    }\n+\n     public static Table createSystemTable(Identifier identifier, Table originTable)\n             throws Catalog.TableNotExistException {\n         if (!(originTable instanceof FileStoreTable)) {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java b/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\nindex ce2cbb56ae24..944b986b3f1d 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\n@@ -26,6 +26,7 @@\n import org.apache.paimon.rest.exceptions.RESTException;\n import org.apache.paimon.rest.exceptions.ServiceFailureException;\n import org.apache.paimon.rest.exceptions.ServiceUnavailableException;\n+import org.apache.paimon.rest.exceptions.UnsupportedOperationException;\n import org.apache.paimon.rest.responses.ErrorResponse;\n \n /** Default error handler. */\n@@ -43,18 +44,20 @@ public void accept(ErrorResponse error) {\n         String message = error.getMessage();\n         switch (code) {\n             case 400:\n-                throw new BadRequestException(String.format(\""Malformed request: %s\"", message));\n+                throw new BadRequestException(String.format(\""%s\"", message));\n             case 401:\n                 throw new NotAuthorizedException(\""Not authorized: %s\"", message);\n             case 403:\n                 throw new ForbiddenException(\""Forbidden: %s\"", message);\n             case 404:\n-                throw new NoSuchResourceException(\""%s\"", message);\n+                throw new NoSuchResourceException(\n+                        error.getResourceType(), error.getResourceName(), \""%s\"", message);\n             case 405:\n             case 406:\n                 break;\n             case 409:\n-                throw new AlreadyExistsException(\""%s\"", message);\n+                throw new AlreadyExistsException(\n+                        error.getResourceType(), error.getResourceName(), \""%s\"", message);\n             case 500:\n                 throw new ServiceFailureException(\""Server error: %s\"", message);\n             case 501:\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\nindex 08284fc454b0..2862e5ef02ed 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n@@ -63,7 +63,11 @@ public HttpClient(Options options) {\n     }\n \n     public HttpClient(HttpClientOptions httpClientOptions) {\n-        this.uri = httpClientOptions.uri();\n+        if (httpClientOptions.uri() != null && httpClientOptions.uri().endsWith(\""/\"")) {\n+            this.uri = httpClientOptions.uri().substring(0, httpClientOptions.uri().length() - 1);\n+        } else {\n+            this.uri = httpClientOptions.uri();\n+        }\n         this.okHttpClient = createHttpClient(httpClientOptions);\n         this.errorHandler = DefaultErrorHandler.getInstance();\n     }\n@@ -132,10 +136,19 @@ private <T extends RESTResponse> T exec(Request request, Class<T> responseType)\n         try (Response response = okHttpClient.newCall(request).execute()) {\n             String responseBodyStr = response.body() != null ? response.body().string() : null;\n             if (!response.isSuccessful()) {\n-                ErrorResponse error =\n-                        new ErrorResponse(\n-                                responseBodyStr != null ? responseBodyStr : \""response body is null\"",\n-                                response.code());\n+                ErrorResponse error;\n+                try {\n+                    error = OBJECT_MAPPER.readValue(responseBodyStr, ErrorResponse.class);\n+                } catch (JsonProcessingException e) {\n+                    error =\n+                            new ErrorResponse(\n+                                    null,\n+                                    null,\n+                                    responseBodyStr != null\n+                                            ? responseBodyStr\n+                                            : \""response body is null\"",\n+                                    response.code());\n+                }\n                 errorHandler.accept(error);\n             }\n             if (responseType != null && responseBodyStr != null) {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex 2c36f75a3713..3f7647ca84af 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -35,8 +35,10 @@\n import org.apache.paimon.partition.Partition;\n import org.apache.paimon.rest.auth.AuthSession;\n import org.apache.paimon.rest.exceptions.AlreadyExistsException;\n+import org.apache.paimon.rest.exceptions.BadRequestException;\n import org.apache.paimon.rest.exceptions.ForbiddenException;\n import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n+import org.apache.paimon.rest.exceptions.ServiceFailureException;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n@@ -45,6 +47,7 @@\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -75,9 +78,12 @@\n import java.util.concurrent.ScheduledExecutorService;\n \n import static org.apache.paimon.CoreOptions.METASTORE_PARTITIONED_TABLE;\n+import static org.apache.paimon.catalog.CatalogUtils.checkNotBranch;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemDatabase;\n+import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemTable;\n import static org.apache.paimon.catalog.CatalogUtils.isSystemDatabase;\n import static org.apache.paimon.catalog.CatalogUtils.listPartitionsFromFileSystem;\n+import static org.apache.paimon.catalog.CatalogUtils.validateAutoCreateClose;\n import static org.apache.paimon.options.CatalogOptions.CASE_SENSITIVE;\n import static org.apache.paimon.rest.RESTUtil.extractPrefixMap;\n import static org.apache.paimon.rest.auth.AuthSession.createAuthSession;\n@@ -209,7 +215,7 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n                 throw new DatabaseNotEmptyException(name);\n             }\n             client.delete(resourcePaths.database(name), headers());\n-        } catch (NoSuchResourceException e) {\n+        } catch (NoSuchResourceException | DatabaseNotExistException e) {\n             if (!ignoreIfNotExists) {\n                 throw new DatabaseNotExistException(name);\n             }\n@@ -249,12 +255,19 @@ public void alterDatabase(String name, List<PropertyChange> changes, boolean ign\n \n     @Override\n     public List<String> listTables(String databaseName) throws DatabaseNotExistException {\n-        ListTablesResponse response =\n-                client.get(resourcePaths.tables(databaseName), ListTablesResponse.class, headers());\n-        if (response.getTables() != null) {\n-            return response.getTables();\n+        try {\n+            ListTablesResponse response =\n+                    client.get(\n+                            resourcePaths.tables(databaseName),\n+                            ListTablesResponse.class,\n+                            headers());\n+            if (response.getTables() != null) {\n+                return response.getTables();\n+            }\n+            return ImmutableList.of();\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(databaseName);\n         }\n-        return ImmutableList.of();\n     }\n \n     @Override\n@@ -272,6 +285,9 @@ public Table getTable(Identifier identifier) throws TableNotExistException {\n     public void createTable(Identifier identifier, Schema schema, boolean ignoreIfExists)\n             throws TableAlreadyExistException, DatabaseNotExistException {\n         try {\n+            checkNotBranch(identifier, \""createTable\"");\n+            checkNotSystemTable(identifier, \""createTable\"");\n+            validateAutoCreateClose(schema.options());\n             CreateTableRequest request = new CreateTableRequest(identifier, schema);\n             client.post(\n                     resourcePaths.tables(identifier.getDatabaseName()),\n@@ -282,12 +298,24 @@ public void createTable(Identifier identifier, Schema schema, boolean ignoreIfEx\n             if (!ignoreIfExists) {\n                 throw new TableAlreadyExistException(identifier);\n             }\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(identifier.getDatabaseName());\n+        } catch (BadRequestException e) {\n+            throw new RuntimeException(new IllegalArgumentException(e.getMessage()));\n+        } catch (IllegalArgumentException e) {\n+            throw e;\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n         }\n     }\n \n     @Override\n     public void renameTable(Identifier fromTable, Identifier toTable, boolean ignoreIfNotExists)\n             throws TableNotExistException, TableAlreadyExistException {\n+        checkNotBranch(fromTable, \""renameTable\"");\n+        checkNotBranch(toTable, \""renameTable\"");\n+        checkNotSystemTable(fromTable, \""renameTable\"");\n+        checkNotSystemTable(toTable, \""renameTable\"");\n         try {\n             RenameTableRequest request = new RenameTableRequest(toTable);\n             client.post(\n@@ -311,6 +339,7 @@ public void renameTable(Identifier fromTable, Identifier toTable, boolean ignore\n     public void alterTable(\n             Identifier identifier, List<SchemaChange> changes, boolean ignoreIfNotExists)\n             throws TableNotExistException, ColumnAlreadyExistException, ColumnNotExistException {\n+        checkNotSystemTable(identifier, \""alterTable\"");\n         try {\n             AlterTableRequest request = new AlterTableRequest(changes);\n             client.post(\n@@ -320,16 +349,30 @@ public void alterTable(\n                     headers());\n         } catch (NoSuchResourceException e) {\n             if (!ignoreIfNotExists) {\n-                throw new TableNotExistException(identifier);\n+                if (e.resourceType() == ErrorResponseResourceType.TABLE) {\n+                    throw new TableNotExistException(identifier);\n+                } else if (e.resourceType() == ErrorResponseResourceType.COLUMN) {\n+                    throw new ColumnNotExistException(identifier, e.resourceName());\n+                }\n             }\n+        } catch (AlreadyExistsException e) {\n+            throw new ColumnAlreadyExistException(identifier, e.resourceName());\n         } catch (ForbiddenException e) {\n             throw new TableNoPermissionException(identifier, e);\n+        } catch (org.apache.paimon.rest.exceptions.UnsupportedOperationException e) {\n+            throw new UnsupportedOperationException(e.getMessage());\n+        } catch (ServiceFailureException e) {\n+            throw new IllegalStateException(e.getMessage());\n+        } catch (BadRequestException e) {\n+            throw new RuntimeException(new IllegalArgumentException(e.getMessage()));\n         }\n     }\n \n     @Override\n     public void dropTable(Identifier identifier, boolean ignoreIfNotExists)\n             throws TableNotExistException {\n+        checkNotBranch(identifier, \""dropTable\"");\n+        checkNotSystemTable(identifier, \""dropTable\"");\n         try {\n             client.delete(\n                     resourcePaths.table(identifier.getDatabaseName(), identifier.getTableName()),\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex 780582c33cb7..f7d2f7116930 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -20,13 +20,17 @@\n \n import org.apache.paimon.options.Options;\n \n-import java.util.StringJoiner;\n+import org.apache.paimon.shade.guava30.com.google.common.base.Joiner;\n \n /** Resource paths for REST catalog. */\n public class ResourcePaths {\n \n-    public static final String V1_CONFIG = \""/v1/config\"";\n-    private static final StringJoiner SLASH = new StringJoiner(\""/\"");\n+    private static final Joiner SLASH = Joiner.on(\""/\"").skipNulls();\n+    private static final String V1 = \""/v1\"";\n+    private static final String DATABASES = \""databases\"";\n+    private static final String TABLES = \""tables\"";\n+\n+    public static final String V1_CONFIG = V1 + \""/config\"";\n \n     public static ResourcePaths forCatalogProperties(Options options) {\n         return new ResourcePaths(options.get(RESTCatalogInternalOptions.PREFIX));\n@@ -39,60 +43,30 @@ public ResourcePaths(String prefix) {\n     }\n \n     public String databases() {\n-        return SLASH.add(\""v1\"").add(prefix).add(\""databases\"").toString();\n+        return SLASH.join(V1, prefix, DATABASES);\n     }\n \n     public String database(String databaseName) {\n-        return SLASH.add(\""v1\"").add(prefix).add(\""databases\"").add(databaseName).toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName);\n     }\n \n     public String databaseProperties(String databaseName) {\n-        return SLASH.add(\""v1\"")\n-                .add(prefix)\n-                .add(\""databases\"")\n-                .add(databaseName)\n-                .add(\""properties\"")\n-                .toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, \""properties\"");\n     }\n \n     public String tables(String databaseName) {\n-        return SLASH.add(\""v1\"")\n-                .add(prefix)\n-                .add(\""databases\"")\n-                .add(databaseName)\n-                .add(\""tables\"")\n-                .toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES);\n     }\n \n     public String table(String databaseName, String tableName) {\n-        return SLASH.add(\""v1\"")\n-                .add(prefix)\n-                .add(\""databases\"")\n-                .add(databaseName)\n-                .add(\""tables\"")\n-                .add(tableName)\n-                .toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName);\n     }\n \n     public String renameTable(String databaseName, String tableName) {\n-        return SLASH.add(\""v1\"")\n-                .add(prefix)\n-                .add(\""databases\"")\n-                .add(databaseName)\n-                .add(\""tables\"")\n-                .add(tableName)\n-                .add(\""rename\"")\n-                .toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""rename\"");\n     }\n \n     public String partitions(String databaseName, String tableName) {\n-        return SLASH.add(\""v1\"")\n-                .add(prefix)\n-                .add(\""databases\"")\n-                .add(databaseName)\n-                .add(\""tables\"")\n-                .add(tableName)\n-                .add(\""partitions\"")\n-                .toString();\n+        return SLASH.join(V1, prefix, DATABASES, databaseName, TABLES, tableName, \""partitions\"");\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java b/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\nindex 198b098687d4..470af7f6f699 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\n@@ -46,8 +46,8 @@ public class AuthSession {\n     private volatile Map<String, String> headers;\n \n     public AuthSession(Map<String, String> headers, CredentialsProvider credentialsProvider) {\n-        this.headers = headers;\n         this.credentialsProvider = credentialsProvider;\n+        this.headers = RESTUtil.merge(headers, this.credentialsProvider.authHeader());\n     }\n \n     public static AuthSession fromRefreshCredentialsProvider(\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java\nindex 8e30c8375bf9..6da7a492b6ed 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java\n@@ -18,10 +18,29 @@\n \n package org.apache.paimon.rest.exceptions;\n \n+import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n+\n /** Exception thrown on HTTP 409 means a resource already exists. */\n public class AlreadyExistsException extends RESTException {\n \n-    public AlreadyExistsException(String message, Object... args) {\n+    private final ErrorResponseResourceType resourceType;\n+    private final String resourceName;\n+\n+    public AlreadyExistsException(\n+            ErrorResponseResourceType resourceType,\n+            String resourceName,\n+            String message,\n+            Object... args) {\n         super(message, args);\n+        this.resourceType = resourceType;\n+        this.resourceName = resourceName;\n+    }\n+\n+    public ErrorResponseResourceType resourceType() {\n+        return resourceType;\n+    }\n+\n+    public String resourceName() {\n+        return resourceName;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ForbiddenException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ForbiddenException.java\nindex 3982e5b70417..76cb53bfc313 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ForbiddenException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ForbiddenException.java\n@@ -20,6 +20,7 @@\n \n /** Exception thrown on HTTP 403 Forbidden. */\n public class ForbiddenException extends RESTException {\n+\n     public ForbiddenException(String message, Object... args) {\n         super(message, args);\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java\nindex cc4c7881f465..6dfb12567151 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java\n@@ -18,10 +18,29 @@\n \n package org.apache.paimon.rest.exceptions;\n \n+import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n+\n /** Exception thrown on HTTP 404 means a resource not exists. */\n public class NoSuchResourceException extends RESTException {\n \n-    public NoSuchResourceException(String message, Object... args) {\n+    private final ErrorResponseResourceType resourceType;\n+    private final String resourceName;\n+\n+    public NoSuchResourceException(\n+            ErrorResponseResourceType resourceType,\n+            String resourceName,\n+            String message,\n+            Object... args) {\n         super(message, args);\n+        this.resourceType = resourceType;\n+        this.resourceName = resourceName;\n+    }\n+\n+    public ErrorResponseResourceType resourceType() {\n+        return resourceType;\n+    }\n+\n+    public String resourceName() {\n+        return resourceName;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NotAuthorizedException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NotAuthorizedException.java\nindex 43c13b1a1c97..79c9aa4e6773 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NotAuthorizedException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NotAuthorizedException.java\n@@ -20,6 +20,7 @@\n \n /** Exception thrown on HTTP 401 Unauthorized. */\n public class NotAuthorizedException extends RESTException {\n+\n     public NotAuthorizedException(String message, Object... args) {\n         super(String.format(message, args));\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/RESTException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/RESTException.java\nindex 532936f43032..f7648c5d1e36 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/RESTException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/RESTException.java\n@@ -20,6 +20,7 @@\n \n /** Base class for REST client exceptions. */\n public class RESTException extends RuntimeException {\n+\n     public RESTException(String message, Object... args) {\n         super(String.format(message, args));\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceFailureException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceFailureException.java\nindex 45c48ec0de09..1df196d90fd4 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceFailureException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceFailureException.java\n@@ -20,6 +20,7 @@\n \n /** Exception thrown on HTTP 500 - Bad Request. */\n public class ServiceFailureException extends RESTException {\n+\n     public ServiceFailureException(String message, Object... args) {\n         super(String.format(message, args));\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceUnavailableException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceUnavailableException.java\nindex fb6a05e89f9f..c466b4c901d1 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceUnavailableException.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/ServiceUnavailableException.java\n@@ -20,6 +20,7 @@\n \n /** Exception thrown on HTTP 503 - service is unavailable. */\n public class ServiceUnavailableException extends RESTException {\n+\n     public ServiceUnavailableException(String message, Object... args) {\n         super(String.format(message, args));\n     }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/UnsupportedOperationException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/UnsupportedOperationException.java\nnew file mode 100644\nindex 000000000000..2feae109d30e\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/UnsupportedOperationException.java\n@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.exceptions;\n+\n+/** Exception thrown on HTTP 501 - UnsupportedOperationException. */\n+public class UnsupportedOperationException extends RESTException {\n+\n+    public UnsupportedOperationException(String message, Object... args) {\n+        super(String.format(message, args));\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\nindex eb95ff448a2e..8e88a37b118d 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n@@ -36,9 +36,17 @@\n public class ErrorResponse implements RESTResponse {\n \n     private static final String FIELD_MESSAGE = \""message\"";\n+    private static final String FIELD_RESOURCE_TYPE = \""resourceType\"";\n+    private static final String FIELD_RESOURCE_NAME = \""resourceName\"";\n     private static final String FIELD_CODE = \""code\"";\n     private static final String FIELD_STACK = \""stack\"";\n \n+    @JsonProperty(FIELD_RESOURCE_TYPE)\n+    private final ErrorResponseResourceType resourceType;\n+\n+    @JsonProperty(FIELD_RESOURCE_NAME)\n+    private final String resourceName;\n+\n     @JsonProperty(FIELD_MESSAGE)\n     private final String message;\n \n@@ -48,7 +56,13 @@ public class ErrorResponse implements RESTResponse {\n     @JsonProperty(FIELD_STACK)\n     private final List<String> stack;\n \n-    public ErrorResponse(String message, Integer code) {\n+    public ErrorResponse(\n+            ErrorResponseResourceType resourceType,\n+            String resourceName,\n+            String message,\n+            Integer code) {\n+        this.resourceType = resourceType;\n+        this.resourceName = resourceName;\n         this.code = code;\n         this.message = message;\n         this.stack = new ArrayList<String>();\n@@ -56,25 +70,33 @@ public ErrorResponse(String message, Integer code) {\n \n     @JsonCreator\n     public ErrorResponse(\n+            @JsonProperty(FIELD_RESOURCE_TYPE) ErrorResponseResourceType resourceType,\n+            @JsonProperty(FIELD_RESOURCE_NAME) String resourceName,\n             @JsonProperty(FIELD_MESSAGE) String message,\n             @JsonProperty(FIELD_CODE) int code,\n             @JsonProperty(FIELD_STACK) List<String> stack) {\n+        this.resourceType = resourceType;\n+        this.resourceName = resourceName;\n         this.message = message;\n         this.code = code;\n         this.stack = stack;\n     }\n \n-    public ErrorResponse(String message, int code, Throwable throwable) {\n-        this.message = message;\n-        this.code = code;\n-        this.stack = getStackFromThrowable(throwable);\n-    }\n-\n     @JsonGetter(FIELD_MESSAGE)\n     public String getMessage() {\n         return message;\n     }\n \n+    @JsonGetter(FIELD_RESOURCE_TYPE)\n+    public ErrorResponseResourceType getResourceType() {\n+        return resourceType;\n+    }\n+\n+    @JsonGetter(FIELD_RESOURCE_NAME)\n+    public String getResourceName() {\n+        return resourceName;\n+    }\n+\n     @JsonGetter(FIELD_CODE)\n     public Integer getCode() {\n         return code;\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java\nnew file mode 100644\nindex 000000000000..590f38e720d4\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponseResourceType.java\n@@ -0,0 +1,26 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+/** The type of resource that caused the error. */\n+public enum ErrorResponseResourceType {\n+    DATABASE,\n+    TABLE,\n+    COLUMN,\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/pom.xml b/paimon-flink/paimon-flink-common/pom.xml\nindex e0f7ce245fa7..84d4622b02b8 100644\n--- a/paimon-flink/paimon-flink-common/pom.xml\n+++ b/paimon-flink/paimon-flink-common/pom.xml\n@@ -177,6 +177,13 @@ under the License.\n             <type>jar</type>\n             <scope>test</scope>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>com.squareup.okhttp3</groupId>\n+            <artifactId>mockwebserver</artifactId>\n+            <version>${okhttp.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n \n \n\ndiff --git a/pom.xml b/pom.xml\nindex 6600f040902c..d55694cec76d 100644\n--- a/pom.xml\n+++ b/pom.xml\n@@ -125,6 +125,7 @@ under the License.\n         <zstd-jni.version>1.5.5-11</zstd-jni.version>\n         <janino.version>3.0.11</janino.version>\n         <mockito.version>3.4.6</mockito.version>\n+        <okhttp.version>4.12.0</okhttp.version>\n         <jaxb.api.version>2.3.1</jaxb.api.version>\n         <findbugs.version>1.3.9</findbugs.version>\n         <json-smart.version>2.4.9</json-smart.version>\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\nindex 31c4c8e682b8..f7aa4ab5a601 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n@@ -25,6 +25,7 @@\n import org.apache.paimon.options.Options;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n+import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.FormatTable;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.types.DataField;\n@@ -37,7 +38,6 @@\n import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n \n import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.Assertions;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.io.TempDir;\n@@ -49,12 +49,16 @@\n import java.util.Map;\n import java.util.stream.Collectors;\n \n+import static org.apache.paimon.catalog.Catalog.SYSTEM_DATABASE_NAME;\n+import static org.apache.paimon.table.system.AllTableOptionsTable.ALL_TABLE_OPTIONS;\n+import static org.apache.paimon.table.system.CatalogOptionsTable.CATALOG_OPTIONS;\n import static org.apache.paimon.testutils.assertj.PaimonAssertions.anyCauseMatches;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatExceptionOfType;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.junit.Assert.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n \n /** Base test class of paimon catalog in {@link Catalog}. */\n public abstract class CatalogTestBase {\n@@ -123,7 +127,7 @@ public void testDuplicatedDatabaseAfterCreatingTable() throws Exception {\n \n         List<String> databases = catalog.listDatabases();\n         List<String> distinctDatabases = databases.stream().distinct().collect(Collectors.toList());\n-        Assertions.assertEquals(distinctDatabases.size(), databases.size());\n+        assertEquals(distinctDatabases.size(), databases.size());\n     }\n \n     @Test\n@@ -146,6 +150,56 @@ public void testCreateDatabase() throws Exception {\n                 .doesNotThrowAnyException();\n     }\n \n+    @Test\n+    public void testAlterDatabase() throws Exception {\n+        if (!supportsAlterDatabase()) {\n+            return;\n+        }\n+        // Alter database\n+        String databaseName = \""db_to_alter\"";\n+        catalog.createDatabase(databaseName, false);\n+        String key = \""key1\"";\n+        String key2 = \""key2\"";\n+        // Add property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.setProperty(key, \""value\""),\n+                        PropertyChange.setProperty(key2, \""value\"")),\n+                false);\n+        Database db = catalog.getDatabase(databaseName);\n+        assertEquals(\""value\"", db.options().get(key));\n+        assertEquals(\""value\"", db.options().get(key2));\n+        // Update property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.setProperty(key, \""value1\""),\n+                        PropertyChange.setProperty(key2, \""value1\"")),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertEquals(\""value1\"", db.options().get(key));\n+        assertEquals(\""value1\"", db.options().get(key2));\n+        // remove property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertFalse(db.options().containsKey(key));\n+        assertFalse(db.options().containsKey(key2));\n+        // Remove non-existent property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertFalse(db.options().containsKey(key));\n+        assertFalse(db.options().containsKey(key2));\n+    }\n+\n     @Test\n     public void testDropDatabase() throws Exception {\n         // Drop database deletes the database when it exists and there are no tables\n@@ -193,6 +247,10 @@ public void testListTables() throws Exception {\n \n         tables = catalog.listTables(\""test_db\"");\n         assertThat(tables).containsExactlyInAnyOrder(\""table1\"", \""table2\"", \""table3\"");\n+\n+        // List tables throws DatabaseNotExistException when the database does not exist\n+        assertThatExceptionOfType(Catalog.DatabaseNotExistException.class)\n+                .isThrownBy(() -> catalog.listTables(\""non_existing_db\""));\n     }\n \n     @Test\n@@ -225,8 +283,17 @@ public void testCreateTable() throws Exception {\n                 .withMessage(\""The value of auto-create property should be false.\"");\n         schema.options().remove(CoreOptions.AUTO_CREATE.key());\n \n+        // Create table and check the schema\n+        schema.options().put(\""k1\"", \""v1\"");\n         catalog.createTable(identifier, schema, false);\n-        catalog.getTable(identifier);\n+        FileStoreTable dataTable = (FileStoreTable) catalog.getTable(identifier);\n+        assertThat(dataTable.schema().toSchema().fields()).isEqualTo(schema.fields());\n+        assertThat(dataTable.schema().toSchema().partitionKeys()).isEqualTo(schema.partitionKeys());\n+        assertThat(dataTable.schema().toSchema().comment()).isEqualTo(schema.comment());\n+        assertThat(dataTable.schema().toSchema().primaryKeys()).isEqualTo(schema.primaryKeys());\n+        for (Map.Entry<String, String> option : schema.options().entrySet()) {\n+            assertThat(dataTable.options().get(option.getKey())).isEqualTo(option.getValue());\n+        }\n \n         // Create table throws Exception when table is system table\n         assertThatExceptionOfType(IllegalArgumentException.class)\n@@ -358,6 +425,20 @@ public void testGetTable() throws Exception {\n                 .isThrownBy(\n                         () -> catalog.getTable(Identifier.create(\""non_existing_db\"", \""test_table\"")))\n                 .withMessage(\""Table non_existing_db.test_table does not exist.\"");\n+\n+        // Get all table options from system database\n+        if (!supportGetFromSystemDatabase()) {\n+            return;\n+        }\n+        Table allTableOptionsTable =\n+                catalog.getTable(Identifier.create(SYSTEM_DATABASE_NAME, ALL_TABLE_OPTIONS));\n+        assertThat(allTableOptionsTable).isNotNull();\n+        Table catalogOptionsTable =\n+                catalog.getTable(Identifier.create(SYSTEM_DATABASE_NAME, CATALOG_OPTIONS));\n+        assertThat(catalogOptionsTable).isNotNull();\n+        assertThatExceptionOfType(Catalog.TableNotExistException.class)\n+                .isThrownBy(\n+                        () -> catalog.getTable(Identifier.create(SYSTEM_DATABASE_NAME, \""1111\"")));\n     }\n \n     @Test\n@@ -541,10 +622,7 @@ public void testAlterTableRenameColumn() throws Exception {\n                                         Lists.newArrayList(\n                                                 SchemaChange.renameColumn(\""col2\"", \""new_col1\"")),\n                                         false))\n-                .satisfies(\n-                        anyCauseMatches(\n-                                Catalog.ColumnAlreadyExistException.class,\n-                                \""Column new_col1 already exists in the test_db.test_table table.\""));\n+                .isInstanceOf(Catalog.ColumnAlreadyExistException.class);\n \n         // Alter table renames a column throws ColumnNotExistException when column does not exist\n         assertThatThrownBy(\n@@ -555,10 +633,7 @@ public void testAlterTableRenameColumn() throws Exception {\n                                                 SchemaChange.renameColumn(\n                                                         \""non_existing_col\"", \""new_col2\"")),\n                                         false))\n-                .satisfies(\n-                        anyCauseMatches(\n-                                Catalog.ColumnNotExistException.class,\n-                                \""Column non_existing_col does not exist in the test_db.test_table table.\""));\n+                .isInstanceOf(Catalog.ColumnNotExistException.class);\n     }\n \n     @Test\n@@ -839,10 +914,6 @@ public void testAlterTableUpdateComment() throws Exception {\n         assertThat(table.comment().isPresent()).isFalse();\n     }\n \n-    protected boolean supportsView() {\n-        return false;\n-    }\n-\n     @Test\n     public void testView() throws Exception {\n         if (!supportsView()) {\n@@ -904,10 +975,6 @@ public void testView() throws Exception {\n                 .isInstanceOf(Catalog.ViewNotExistException.class);\n     }\n \n-    protected boolean supportsFormatTable() {\n-        return false;\n-    }\n-\n     @Test\n     public void testFormatTable() throws Exception {\n         if (!supportsFormatTable()) {\n@@ -962,49 +1029,19 @@ public void testTableUUID() throws Exception {\n                 .isGreaterThan(0);\n     }\n \n-    protected void alterDatabaseWhenSupportAlter() throws Exception {\n-        // Alter database\n-        String databaseName = \""db_to_alter\"";\n-        catalog.createDatabase(databaseName, false);\n-        String key = \""key1\"";\n-        String key2 = \""key2\"";\n-        // Add property\n-        catalog.alterDatabase(\n-                databaseName,\n-                Lists.newArrayList(\n-                        PropertyChange.setProperty(key, \""value\""),\n-                        PropertyChange.setProperty(key2, \""value\"")),\n-                false);\n-        Database db = catalog.getDatabase(databaseName);\n-        assertEquals(\""value\"", db.options().get(key));\n-        assertEquals(\""value\"", db.options().get(key2));\n-        // Update property\n-        catalog.alterDatabase(\n-                databaseName,\n-                Lists.newArrayList(\n-                        PropertyChange.setProperty(key, \""value1\""),\n-                        PropertyChange.setProperty(key2, \""value1\"")),\n-                false);\n-        db = catalog.getDatabase(databaseName);\n-        assertEquals(\""value1\"", db.options().get(key));\n-        assertEquals(\""value1\"", db.options().get(key2));\n-        // remove property\n-        catalog.alterDatabase(\n-                databaseName,\n-                Lists.newArrayList(\n-                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n-                false);\n-        db = catalog.getDatabase(databaseName);\n-        assertEquals(false, db.options().containsKey(key));\n-        assertEquals(false, db.options().containsKey(key2));\n-        // Remove non-existent property\n-        catalog.alterDatabase(\n-                databaseName,\n-                Lists.newArrayList(\n-                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n-                false);\n-        db = catalog.getDatabase(databaseName);\n-        assertEquals(false, db.options().containsKey(key));\n-        assertEquals(false, db.options().containsKey(key2));\n+    protected boolean supportGetFromSystemDatabase() {\n+        return true;\n+    }\n+\n+    protected boolean supportsAlterDatabase() {\n+        return false;\n+    }\n+\n+    protected boolean supportsFormatTable() {\n+        return false;\n+    }\n+\n+    protected boolean supportsView() {\n+        return false;\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\nindex 51e2bf5c779d..0dea9209036d 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\n@@ -117,8 +117,8 @@ public void testSerializeTable() throws Exception {\n                 });\n     }\n \n-    @Test\n-    public void testAlterDatabase() throws Exception {\n-        this.alterDatabaseWhenSupportAlter();\n+    @Override\n+    protected boolean supportsAlterDatabase() {\n+        return true;\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\nindex 340e38f6a7f8..0de7bf05d813 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\n@@ -70,7 +70,7 @@ public void testHandleErrorResponse() {\n                 ServiceFailureException.class,\n                 () -> defaultErrorHandler.accept(generateErrorResponse(500)));\n         assertThrows(\n-                UnsupportedOperationException.class,\n+                org.apache.paimon.rest.exceptions.UnsupportedOperationException.class,\n                 () -> defaultErrorHandler.accept(generateErrorResponse(501)));\n         assertThrows(\n                 RESTException.class, () -> defaultErrorHandler.accept(generateErrorResponse(502)));\n@@ -80,6 +80,6 @@ public void testHandleErrorResponse() {\n     }\n \n     private ErrorResponse generateErrorResponse(int code) {\n-        return new ErrorResponse(\""message\"", code, new ArrayList<String>());\n+        return new ErrorResponse(null, null, \""message\"", code, new ArrayList<String>());\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\nindex 3baff1ccaa43..54e7d3a68eee 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n@@ -20,6 +20,9 @@\n \n import org.apache.paimon.rest.auth.BearTokenCredentialsProvider;\n import org.apache.paimon.rest.auth.CredentialsProvider;\n+import org.apache.paimon.rest.exceptions.BadRequestException;\n+import org.apache.paimon.rest.responses.ErrorResponse;\n+import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n@@ -31,28 +34,26 @@\n \n import java.io.IOException;\n import java.time.Duration;\n-import java.util.HashMap;\n import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n-import static org.mockito.ArgumentMatchers.any;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.times;\n-import static org.mockito.Mockito.verify;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n \n /** Test for {@link HttpClient}. */\n public class HttpClientTest {\n \n     private static final String MOCK_PATH = \""/v1/api/mock\"";\n     private static final String TOKEN = \""token\"";\n-\n-    private final ObjectMapper objectMapper = RESTObjectMapper.create();\n+    private static final ObjectMapper OBJECT_MAPPER = RESTObjectMapper.create();\n \n     private MockWebServer mockWebServer;\n     private HttpClient httpClient;\n     private ErrorHandler errorHandler;\n     private MockRESTData mockResponseData;\n     private String mockResponseDataStr;\n+    private ErrorResponse errorResponse;\n+    private String errorResponseStr;\n     private Map<String, String> headers;\n \n     @Before\n@@ -60,11 +61,13 @@ public void setUp() throws IOException {\n         mockWebServer = new MockWebServer();\n         mockWebServer.start();\n         String baseUrl = mockWebServer.url(\""\"").toString();\n-        errorHandler = mock(ErrorHandler.class);\n+        errorHandler = DefaultErrorHandler.getInstance();\n         HttpClientOptions httpClientOptions =\n                 new HttpClientOptions(baseUrl, Duration.ofSeconds(3), Duration.ofSeconds(3), 1);\n         mockResponseData = new MockRESTData(MOCK_PATH);\n-        mockResponseDataStr = objectMapper.writeValueAsString(mockResponseData);\n+        mockResponseDataStr = OBJECT_MAPPER.writeValueAsString(mockResponseData);\n+        errorResponse = new ErrorResponse(ErrorResponseResourceType.DATABASE, \""test\"", \""test\"", 400);\n+        errorResponseStr = OBJECT_MAPPER.writeValueAsString(errorResponse);\n         httpClient = new HttpClient(httpClientOptions);\n         httpClient.setErrorHandler(errorHandler);\n         CredentialsProvider credentialsProvider = new BearTokenCredentialsProvider(TOKEN);\n@@ -80,15 +83,15 @@ public void tearDown() throws IOException {\n     public void testGetSuccess() {\n         mockHttpCallWithCode(mockResponseDataStr, 200);\n         MockRESTData response = httpClient.get(MOCK_PATH, MockRESTData.class, headers);\n-        verify(errorHandler, times(0)).accept(any());\n         assertEquals(mockResponseData.data(), response.data());\n     }\n \n     @Test\n     public void testGetFail() {\n-        mockHttpCallWithCode(mockResponseDataStr, 400);\n-        httpClient.get(MOCK_PATH, MockRESTData.class, headers);\n-        verify(errorHandler, times(1)).accept(any());\n+        mockHttpCallWithCode(errorResponseStr, 400);\n+        assertThrows(\n+                BadRequestException.class,\n+                () -> httpClient.get(MOCK_PATH, MockRESTData.class, headers));\n     }\n \n     @Test\n@@ -96,35 +99,27 @@ public void testPostSuccess() {\n         mockHttpCallWithCode(mockResponseDataStr, 200);\n         MockRESTData response =\n                 httpClient.post(MOCK_PATH, mockResponseData, MockRESTData.class, headers);\n-        verify(errorHandler, times(0)).accept(any());\n         assertEquals(mockResponseData.data(), response.data());\n     }\n \n     @Test\n     public void testPostFail() {\n-        mockHttpCallWithCode(mockResponseDataStr, 400);\n-        httpClient.post(MOCK_PATH, mockResponseData, MockRESTData.class, headers);\n-        verify(errorHandler, times(1)).accept(any());\n+        mockHttpCallWithCode(errorResponseStr, 400);\n+        assertThrows(\n+                BadRequestException.class,\n+                () -> httpClient.post(MOCK_PATH, mockResponseData, ErrorResponse.class, headers));\n     }\n \n     @Test\n     public void testDeleteSuccess() {\n         mockHttpCallWithCode(mockResponseDataStr, 200);\n-        MockRESTData response = httpClient.delete(MOCK_PATH, headers);\n-        verify(errorHandler, times(0)).accept(any());\n+        assertDoesNotThrow(() -> httpClient.delete(MOCK_PATH, headers));\n     }\n \n     @Test\n     public void testDeleteFail() {\n-        mockHttpCallWithCode(mockResponseDataStr, 400);\n-        httpClient.delete(MOCK_PATH, headers);\n-        verify(errorHandler, times(1)).accept(any());\n-    }\n-\n-    private Map<String, String> headers(String token) {\n-        Map<String, String> header = new HashMap<>();\n-        header.put(\""Authorization\"", \""Bearer \"" + token);\n-        return header;\n+        mockHttpCallWithCode(errorResponseStr, 400);\n+        assertThrows(BadRequestException.class, () -> httpClient.delete(MOCK_PATH, headers));\n     }\n \n     private void mockHttpCallWithCode(String body, Integer code) {\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 4b228d93c6f7..58a73bbfcbaa 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -30,7 +30,6 @@\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n-import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n@@ -48,6 +47,8 @@\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n+import okhttp3.mockwebserver.MockResponse;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n@@ -89,10 +90,6 @@ public static ListDatabasesResponse listDatabasesResponse(String name) {\n         return new ListDatabasesResponse(databaseNameList);\n     }\n \n-    public static ErrorResponse noSuchResourceExceptionErrorResponse() {\n-        return new ErrorResponse(\""message\"", 404, new ArrayList<>());\n-    }\n-\n     public static AlterDatabaseRequest alterDatabaseRequest() {\n         Map<String, String> add = new HashMap<>();\n         add.put(\""add\"", \""value\"");\n@@ -243,6 +240,13 @@ public static GetTableResponse getTableResponse() {\n         return new GetTableResponse(\""/tmp/1\"", 1, schema(options));\n     }\n \n+    public static MockResponse mockResponse(String body, int httpCode) {\n+        return new MockResponse()\n+                .setResponseCode(httpCode)\n+                .setBody(body)\n+                .addHeader(\""Content-Type\"", \""application/json\"");\n+    }\n+\n     private static Schema schema(Map<String, String> options) {\n         List<DataField> fields =\n                 Arrays.asList(\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\nnew file mode 100644\nindex 000000000000..4fe20291135c\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogServer.java\n@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import org.apache.paimon.catalog.AbstractCatalog;\n+import org.apache.paimon.catalog.Catalog;\n+import org.apache.paimon.catalog.CatalogContext;\n+import org.apache.paimon.catalog.CatalogFactory;\n+import org.apache.paimon.catalog.Database;\n+import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.options.CatalogOptions;\n+import org.apache.paimon.options.Options;\n+import org.apache.paimon.partition.Partition;\n+import org.apache.paimon.rest.requests.AlterTableRequest;\n+import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.RenameTableRequest;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.ErrorResponse;\n+import org.apache.paimon.rest.responses.ErrorResponseResourceType;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetTableResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n+import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.table.FileStoreTable;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n+\n+import okhttp3.mockwebserver.Dispatcher;\n+import okhttp3.mockwebserver.MockResponse;\n+import okhttp3.mockwebserver.MockWebServer;\n+import okhttp3.mockwebserver.RecordedRequest;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/** Mock REST server for testing. */\n+public class RESTCatalogServer {\n+\n+    private static final ObjectMapper OBJECT_MAPPER = RESTObjectMapper.create();\n+    private static final String PREFIX = \""paimon\"";\n+    private static final String DATABASE_URI = String.format(\""/v1/%s/databases\"", PREFIX);\n+\n+    private final Catalog catalog;\n+    private final Dispatcher dispatcher;\n+    private final MockWebServer server;\n+    private final String authToken;\n+\n+    public RESTCatalogServer(String warehouse, String initToken) {\n+        authToken = initToken;\n+        Options conf = new Options();\n+        conf.setString(\""warehouse\"", warehouse);\n+        this.catalog =\n+                CatalogFactory.createCatalog(\n+                        CatalogContext.create(conf), this.getClass().getClassLoader());\n+        this.dispatcher = initDispatcher(catalog, authToken);\n+        MockWebServer mockWebServer = new MockWebServer();\n+        mockWebServer.setDispatcher(dispatcher);\n+        server = mockWebServer;\n+    }\n+\n+    public void start() throws IOException {\n+        server.start();\n+    }\n+\n+    public String getUrl() {\n+        return server.url(\""\"").toString();\n+    }\n+\n+    public void shutdown() throws IOException {\n+        server.shutdown();\n+    }\n+\n+    public static Dispatcher initDispatcher(Catalog catalog, String authToken) {\n+        return new Dispatcher() {\n+            @Override\n+            public MockResponse dispatch(RecordedRequest request) {\n+                String token = request.getHeaders().get(\""Authorization\"");\n+                RESTResponse response;\n+                try {\n+                    if (!(\""Bearer \"" + authToken).equals(token)) {\n+                        return new MockResponse().setResponseCode(401);\n+                    }\n+                    if (\""/v1/config\"".equals(request.getPath())) {\n+                        return new MockResponse()\n+                                .setResponseCode(200)\n+                                .setBody(getConfigBody(catalog.warehouse()));\n+                    } else if (DATABASE_URI.equals(request.getPath())) {\n+                        return databasesApiHandler(catalog, request);\n+                    } else if (request.getPath().startsWith(DATABASE_URI)) {\n+                        String[] resources =\n+                                request.getPath()\n+                                        .substring((DATABASE_URI + \""/\"").length())\n+                                        .split(\""/\"");\n+                        String databaseName = resources[0];\n+                        boolean isTables = resources.length == 2 && \""tables\"".equals(resources[1]);\n+                        boolean isTable = resources.length == 3 && \""tables\"".equals(resources[1]);\n+                        boolean isTableRename =\n+                                resources.length == 4 && \""rename\"".equals(resources[3]);\n+                        boolean isPartitions =\n+                                resources.length == 4\n+                                        && \""tables\"".equals(resources[1])\n+                                        && \""partitions\"".equals(resources[3]);\n+                        if (isPartitions) {\n+                            String tableName = resources[2];\n+                            List<Partition> partitions =\n+                                    catalog.listPartitions(\n+                                            Identifier.create(databaseName, tableName));\n+                            response = new ListPartitionsResponse(partitions);\n+                            return mockResponse(response, 200);\n+                        } else if (isTableRename) {\n+                            return renameTableApiHandler(\n+                                    catalog, request, databaseName, resources[2]);\n+                        } else if (isTable) {\n+                            String tableName = resources[2];\n+                            return tableApiHandler(catalog, request, databaseName, tableName);\n+                        } else if (isTables) {\n+                            return tablesApiHandler(catalog, request, databaseName);\n+                        } else {\n+                            return databaseApiHandler(catalog, request, databaseName);\n+                        }\n+                    }\n+                    return new MockResponse().setResponseCode(404);\n+                } catch (Catalog.DatabaseNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.DATABASE,\n+                                    e.database(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n+                } catch (Catalog.TableNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.TABLE,\n+                                    e.identifier().getTableName(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n+                } catch (Catalog.ColumnNotExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.COLUMN,\n+                                    e.column(),\n+                                    e.getMessage(),\n+                                    404);\n+                    return mockResponse(response, 404);\n+                } catch (Catalog.DatabaseAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.DATABASE,\n+                                    e.database(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n+                } catch (Catalog.TableAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.TABLE,\n+                                    e.identifier().getTableName(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n+                } catch (Catalog.ColumnAlreadyExistException e) {\n+                    response =\n+                            new ErrorResponse(\n+                                    ErrorResponseResourceType.COLUMN,\n+                                    e.column(),\n+                                    e.getMessage(),\n+                                    409);\n+                    return mockResponse(response, 409);\n+                } catch (IllegalArgumentException e) {\n+                    response = new ErrorResponse(null, null, e.getMessage(), 400);\n+                    return mockResponse(response, 400);\n+                } catch (Exception e) {\n+                    if (e.getCause() instanceof IllegalArgumentException) {\n+                        response =\n+                                new ErrorResponse(\n+                                        null, null, e.getCause().getCause().getMessage(), 400);\n+                        return mockResponse(response, 400);\n+                    } else if (e instanceof UnsupportedOperationException) {\n+                        response = new ErrorResponse(null, null, e.getMessage(), 501);\n+                        return mockResponse(response, 501);\n+                    } else if (e instanceof IllegalStateException) {\n+                        response = new ErrorResponse(null, null, e.getMessage(), 500);\n+                        return mockResponse(response, 500);\n+                    }\n+                    return new MockResponse().setResponseCode(500);\n+                }\n+            }\n+        };\n+    }\n+\n+    private static MockResponse renameTableApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n+            throws Exception {\n+        RenameTableRequest requestBody =\n+                OBJECT_MAPPER.readValue(request.getBody().readUtf8(), RenameTableRequest.class);\n+        catalog.renameTable(\n+                Identifier.create(databaseName, tableName), requestBody.getNewIdentifier(), false);\n+        FileStoreTable table = (FileStoreTable) catalog.getTable(requestBody.getNewIdentifier());\n+        RESTResponse response =\n+                new GetTableResponse(\n+                        AbstractCatalog.newTableLocation(\n+                                        catalog.warehouse(), requestBody.getNewIdentifier())\n+                                .toString(),\n+                        table.schema().id(),\n+                        table.schema().toSchema());\n+        return mockResponse(response, 200);\n+    }\n+\n+    private static MockResponse databasesApiHandler(Catalog catalog, RecordedRequest request)\n+            throws Exception {\n+        RESTResponse response;\n+        if (request.getMethod().equals(\""GET\"")) {\n+            List<String> databaseNameList = catalog.listDatabases();\n+            response = new ListDatabasesResponse(databaseNameList);\n+            return mockResponse(response, 200);\n+        } else if (request.getMethod().equals(\""POST\"")) {\n+            CreateDatabaseRequest requestBody =\n+                    OBJECT_MAPPER.readValue(\n+                            request.getBody().readUtf8(), CreateDatabaseRequest.class);\n+            String databaseName = requestBody.getName();\n+            catalog.createDatabase(databaseName, false);\n+            response = new CreateDatabaseResponse(databaseName, requestBody.getOptions());\n+            return mockResponse(response, 200);\n+        }\n+        return new MockResponse().setResponseCode(404);\n+    }\n+\n+    private static MockResponse databaseApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName) throws Exception {\n+        RESTResponse response;\n+        if (request.getMethod().equals(\""GET\"")) {\n+            Database database = catalog.getDatabase(databaseName);\n+            response = new GetDatabaseResponse(database.name(), database.options());\n+            return mockResponse(response, 200);\n+        } else if (request.getMethod().equals(\""DELETE\"")) {\n+            catalog.dropDatabase(databaseName, false, true);\n+            return new MockResponse().setResponseCode(200);\n+        }\n+        return new MockResponse().setResponseCode(404);\n+    }\n+\n+    private static MockResponse tablesApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName) throws Exception {\n+        RESTResponse response;\n+        if (request.getMethod().equals(\""POST\"")) {\n+            CreateTableRequest requestBody =\n+                    OBJECT_MAPPER.readValue(request.getBody().readUtf8(), CreateTableRequest.class);\n+            catalog.createTable(requestBody.getIdentifier(), requestBody.getSchema(), false);\n+            response = new GetTableResponse(\""\"", 1L, requestBody.getSchema());\n+            return mockResponse(response, 200);\n+        } else if (request.getMethod().equals(\""GET\"")) {\n+            catalog.listTables(databaseName);\n+            response = new ListTablesResponse(catalog.listTables(databaseName));\n+            return mockResponse(response, 200);\n+        }\n+        return new MockResponse().setResponseCode(404);\n+    }\n+\n+    private static MockResponse tableApiHandler(\n+            Catalog catalog, RecordedRequest request, String databaseName, String tableName)\n+            throws Exception {\n+        RESTResponse response;\n+        if (request.getMethod().equals(\""GET\"")) {\n+            Identifier identifier = Identifier.create(databaseName, tableName);\n+            FileStoreTable table = (FileStoreTable) catalog.getTable(identifier);\n+            response =\n+                    new GetTableResponse(\n+                            AbstractCatalog.newTableLocation(catalog.warehouse(), identifier)\n+                                    .toString(),\n+                            table.schema().id(),\n+                            table.schema().toSchema());\n+            return mockResponse(response, 200);\n+        } else if (request.getMethod().equals(\""POST\"")) {\n+            Identifier identifier = Identifier.create(databaseName, tableName);\n+            AlterTableRequest requestBody =\n+                    OBJECT_MAPPER.readValue(request.getBody().readUtf8(), AlterTableRequest.class);\n+            catalog.alterTable(identifier, requestBody.getChanges(), false);\n+            FileStoreTable table = (FileStoreTable) catalog.getTable(identifier);\n+            response = new GetTableResponse(\""\"", table.schema().id(), table.schema().toSchema());\n+            return mockResponse(response, 200);\n+        } else if (request.getMethod().equals(\""DELETE\"")) {\n+            Identifier identifier = Identifier.create(databaseName, tableName);\n+            catalog.dropTable(identifier, false);\n+            return new MockResponse().setResponseCode(200);\n+        }\n+        return new MockResponse().setResponseCode(404);\n+    }\n+\n+    private static MockResponse mockResponse(RESTResponse response, int httpCode) {\n+        try {\n+            return new MockResponse()\n+                    .setResponseCode(httpCode)\n+                    .setBody(OBJECT_MAPPER.writeValueAsString(response))\n+                    .addHeader(\""Content-Type\"", \""application/json\"");\n+        } catch (JsonProcessingException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static String getConfigBody(String warehouseStr) {\n+        return String.format(\n+                \""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\"", \\\""%s\\\"": \\\""%s\\\""}}\"",\n+                RESTCatalogInternalOptions.PREFIX.key(),\n+                PREFIX,\n+                CatalogOptions.WAREHOUSE.key(),\n+                warehouseStr);\n+    }\n+}\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex 344807b4c96e..b34ca1e5acd1 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -18,352 +18,107 @@\n \n package org.apache.paimon.rest;\n \n-import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogContext;\n-import org.apache.paimon.catalog.Database;\n+import org.apache.paimon.catalog.CatalogTestBase;\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.partition.Partition;\n-import org.apache.paimon.rest.requests.CreateTableRequest;\n-import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n-import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n-import org.apache.paimon.rest.responses.ErrorResponse;\n-import org.apache.paimon.rest.responses.GetDatabaseResponse;\n-import org.apache.paimon.rest.responses.GetTableResponse;\n-import org.apache.paimon.rest.responses.ListDatabasesResponse;\n-import org.apache.paimon.rest.responses.ListPartitionsResponse;\n-import org.apache.paimon.rest.responses.ListTablesResponse;\n-import org.apache.paimon.schema.SchemaChange;\n-import org.apache.paimon.table.Table;\n+import org.apache.paimon.rest.exceptions.NotAuthorizedException;\n+import org.apache.paimon.schema.Schema;\n+import org.apache.paimon.types.DataField;\n+import org.apache.paimon.types.DataTypes;\n \n-import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n-import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n \n-import okhttp3.mockwebserver.MockResponse;\n-import okhttp3.mockwebserver.MockWebServer;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n \n-import java.io.IOException;\n-import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertThrows;\n-import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n \n /** Test for REST Catalog. */\n-public class RESTCatalogTest {\n+class RESTCatalogTest extends CatalogTestBase {\n \n-    private final ObjectMapper mapper = RESTObjectMapper.create();\n-    private MockWebServer mockWebServer;\n-    private RESTCatalog restCatalog;\n-    private String warehouseStr;\n-    private String serverUrl;\n-    @Rule public TemporaryFolder folder = new TemporaryFolder();\n+    private RESTCatalogServer restCatalogServer;\n \n-    @Before\n-    public void setUp() throws IOException {\n-        mockWebServer = new MockWebServer();\n-        mockWebServer.start();\n-        serverUrl = mockWebServer.url(\""\"").toString();\n-        Options options = mockInitOptions();\n-        warehouseStr = folder.getRoot().getPath();\n-        mockConfig(warehouseStr);\n-        restCatalog = new RESTCatalog(CatalogContext.create(options));\n-    }\n-\n-    @After\n-    public void tearDown() throws IOException {\n-        mockWebServer.shutdown();\n-    }\n-\n-    @Test\n-    public void testInitFailWhenDefineWarehouse() {\n+    @BeforeEach\n+    @Override\n+    public void setUp() throws Exception {\n+        super.setUp();\n+        String initToken = \""init_token\"";\n+        restCatalogServer = new RESTCatalogServer(warehouse, initToken);\n+        restCatalogServer.start();\n         Options options = new Options();\n-        options.set(CatalogOptions.WAREHOUSE, warehouseStr);\n-        assertThrows(\n-                IllegalArgumentException.class,\n-                () -> new RESTCatalog(CatalogContext.create(options)));\n-    }\n-\n-    @Test\n-    public void testListDatabases() throws JsonProcessingException {\n-        String name = MockRESTMessage.databaseName();\n-        ListDatabasesResponse response = MockRESTMessage.listDatabasesResponse(name);\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        List<String> result = restCatalog.listDatabases();\n-        assertEquals(response.getDatabases().size(), result.size());\n-        assertEquals(name, result.get(0));\n-    }\n-\n-    @Test\n-    public void testCreateDatabase() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        CreateDatabaseResponse response = MockRESTMessage.createDatabaseResponse(name);\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(() -> restCatalog.createDatabase(name, false, response.getOptions()));\n-    }\n-\n-    @Test\n-    public void testGetDatabase() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        GetDatabaseResponse response = MockRESTMessage.getDatabaseResponse(name);\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        Database result = restCatalog.getDatabase(name);\n-        assertEquals(name, result.name());\n-        assertEquals(response.getOptions().size(), result.options().size());\n-        assertEquals(response.comment().get(), result.comment().get());\n+        options.set(RESTCatalogOptions.URI, restCatalogServer.getUrl());\n+        options.set(RESTCatalogOptions.TOKEN, initToken);\n+        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n+        this.catalog = new RESTCatalog(CatalogContext.create(options));\n     }\n \n-    @Test\n-    public void testDropDatabase() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        mockResponse(\""\"", 200);\n-        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, false, true));\n+    @AfterEach\n+    public void tearDown() throws Exception {\n+        restCatalogServer.shutdown();\n     }\n \n-    @Test\n-    public void testDropDatabaseWhenNoExistAndIgnoreIfNotExistsIsFalse() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n-        mockResponse(mapper.writeValueAsString(response), 404);\n-        assertThrows(\n-                Catalog.DatabaseNotExistException.class,\n-                () -> restCatalog.dropDatabase(name, false, true));\n+    @Override\n+    protected boolean supportGetFromSystemDatabase() {\n+        return false;\n     }\n \n     @Test\n-    public void testDropDatabaseWhenNoExistAndIgnoreIfNotExistsIsTrue() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n-        mockResponse(mapper.writeValueAsString(response), 404);\n-        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, true, true));\n+    void testInitFailWhenDefineWarehouse() {\n+        Options options = new Options();\n+        options.set(CatalogOptions.WAREHOUSE, warehouse);\n+        assertThatThrownBy(() -> new RESTCatalog(CatalogContext.create(options)))\n+                .isInstanceOf(IllegalArgumentException.class);\n     }\n \n     @Test\n-    public void testDropDatabaseWhenCascadeIsFalseAndNoTables() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        boolean cascade = false;\n-        ListTablesResponse response = MockRESTMessage.listTablesEmptyResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        mockResponse(\""\"", 200);\n-        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, false, cascade));\n+    void testAuthFail() {\n+        Options options = new Options();\n+        options.set(RESTCatalogOptions.URI, restCatalogServer.getUrl());\n+        options.set(RESTCatalogOptions.TOKEN, \""aaaaa\"");\n+        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n+        options.set(CatalogOptions.METASTORE, RESTCatalogFactory.IDENTIFIER);\n+        assertThatThrownBy(() -> new RESTCatalog(CatalogContext.create(options)))\n+                .isInstanceOf(NotAuthorizedException.class);\n     }\n \n     @Test\n-    public void testDropDatabaseWhenCascadeIsFalseAndTablesExist() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        boolean cascade = false;\n-        ListTablesResponse response = MockRESTMessage.listTablesResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertThrows(\n-                Catalog.DatabaseNotEmptyException.class,\n-                () -> restCatalog.dropDatabase(name, false, cascade));\n+    void testListPartitionsWhenMetastorePartitionedIsTrue() throws Exception {\n+        Identifier identifier = Identifier.create(\""test_db\"", \""test_table\"");\n+        createTable(identifier, Maps.newHashMap(), Lists.newArrayList(\""col1\""));\n+        List<Partition> result = catalog.listPartitions(identifier);\n+        assertEquals(0, result.size());\n     }\n \n     @Test\n-    public void testAlterDatabase() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        AlterDatabaseResponse response = MockRESTMessage.alterDatabaseResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(() -> restCatalog.alterDatabase(name, new ArrayList<>(), true));\n+    void testListPartitionsFromFile() throws Exception {\n+        Identifier identifier = Identifier.create(\""test_db\"", \""test_table\"");\n+        createTable(identifier, Maps.newHashMap(), Lists.newArrayList(\""col1\""));\n+        List<Partition> result = catalog.listPartitions(identifier);\n+        assertEquals(0, result.size());\n     }\n \n-    @Test\n-    public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsFalse()\n+    private void createTable(\n+            Identifier identifier, Map<String, String> options, List<String> partitionKeys)\n             throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n-        mockResponse(mapper.writeValueAsString(response), 404);\n-        assertThrows(\n-                Catalog.DatabaseNotExistException.class,\n-                () -> restCatalog.alterDatabase(name, new ArrayList<>(), false));\n-    }\n-\n-    @Test\n-    public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsTrue() throws Exception {\n-        String name = MockRESTMessage.databaseName();\n-        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n-        mockResponse(mapper.writeValueAsString(response), 404);\n-        assertDoesNotThrow(() -> restCatalog.alterDatabase(name, new ArrayList<>(), true));\n-    }\n-\n-    @Test\n-    public void testListTables() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        ListTablesResponse response = MockRESTMessage.listTablesResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        List<String> result = restCatalog.listTables(databaseName);\n-        assertEquals(response.getTables().size(), result.size());\n-    }\n-\n-    @Test\n-    public void testGetTable() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        GetTableResponse response = MockRESTMessage.getTableResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        Table result = restCatalog.getTable(Identifier.create(databaseName, \""table\""));\n-        assertEquals(response.getSchema().options().size() + 1, result.options().size());\n-    }\n-\n-    @Test\n-    public void testCreateTable() throws Exception {\n-        CreateTableRequest request = MockRESTMessage.createTableRequest(\""table\"");\n-        GetTableResponse response = MockRESTMessage.getTableResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(\n-                () -> restCatalog.createTable(request.getIdentifier(), request.getSchema(), false));\n-    }\n-\n-    @Test\n-    public void testCreateTableWhenTableAlreadyExistAndIgnoreIfExistsIsFalse() throws Exception {\n-        CreateTableRequest request = MockRESTMessage.createTableRequest(\""table\"");\n-        mockResponse(\""\"", 409);\n-        assertThrows(\n-                Catalog.TableAlreadyExistException.class,\n-                () -> restCatalog.createTable(request.getIdentifier(), request.getSchema(), false));\n-    }\n-\n-    @Test\n-    public void testRenameTable() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        String fromTableName = \""fromTable\"";\n-        String toTableName = \""toTable\"";\n-        GetTableResponse response = MockRESTMessage.getTableResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(\n-                () ->\n-                        restCatalog.renameTable(\n-                                Identifier.create(databaseName, fromTableName),\n-                                Identifier.create(databaseName, toTableName),\n-                                true));\n-    }\n-\n-    @Test\n-    public void testRenameTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        String fromTableName = \""fromTable\"";\n-        String toTableName = \""toTable\"";\n-        mockResponse(\""\"", 404);\n-        assertThrows(\n-                Catalog.TableNotExistException.class,\n-                () ->\n-                        restCatalog.renameTable(\n-                                Identifier.create(databaseName, fromTableName),\n-                                Identifier.create(databaseName, toTableName),\n-                                false));\n-    }\n-\n-    @Test\n-    public void testRenameTableWhenToTableAlreadyExist() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        String fromTableName = \""fromTable\"";\n-        String toTableName = \""toTable\"";\n-        mockResponse(\""\"", 409);\n-        assertThrows(\n-                Catalog.TableAlreadyExistException.class,\n-                () ->\n-                        restCatalog.renameTable(\n-                                Identifier.create(databaseName, fromTableName),\n-                                Identifier.create(databaseName, toTableName),\n-                                false));\n-    }\n-\n-    @Test\n-    public void testAlterTable() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        List<SchemaChange> changes = MockRESTMessage.getChanges();\n-        GetTableResponse response = MockRESTMessage.getTableResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(\n-                () -> restCatalog.alterTable(Identifier.create(databaseName, \""t1\""), changes, true));\n-    }\n-\n-    @Test\n-    public void testAlterTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        List<SchemaChange> changes = MockRESTMessage.getChanges();\n-        mockResponse(\""\"", 404);\n-        assertThrows(\n-                Catalog.TableNotExistException.class,\n-                () ->\n-                        restCatalog.alterTable(\n-                                Identifier.create(databaseName, \""t1\""), changes, false));\n-    }\n-\n-    @Test\n-    public void testDropTable() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        String tableName = \""table\"";\n-        mockResponse(\""\"", 200);\n-        assertDoesNotThrow(\n-                () -> restCatalog.dropTable(Identifier.create(databaseName, tableName), true));\n-    }\n-\n-    @Test\n-    public void testDropTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        String tableName = \""table\"";\n-        mockResponse(\""\"", 404);\n-        assertThrows(\n-                Catalog.TableNotExistException.class,\n-                () -> restCatalog.dropTable(Identifier.create(databaseName, tableName), false));\n-    }\n-\n-    @Test\n-    public void testListPartitionsWhenMetastorePartitionedIsTrue() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        GetTableResponse getTableResponse = MockRESTMessage.getTableResponseEnablePartition();\n-        mockResponse(mapper.writeValueAsString(getTableResponse), 200);\n-        ListPartitionsResponse response = MockRESTMessage.listPartitionsResponse();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        List<Partition> result =\n-                restCatalog.listPartitions(Identifier.create(databaseName, \""table\""));\n-        assertEquals(response.getPartitions().size(), result.size());\n-    }\n-\n-    @Test\n-    public void testListPartitionsFromFile() throws Exception {\n-        String databaseName = MockRESTMessage.databaseName();\n-        GetTableResponse response = MockRESTMessage.getTableResponseEnablePartition();\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        mockResponse(mapper.writeValueAsString(response), 200);\n-        List<Partition> partitionEntries =\n-                restCatalog.listPartitions(Identifier.create(databaseName, \""table\""));\n-        assertEquals(partitionEntries.size(), 0);\n-    }\n-\n-    private void mockResponse(String mockResponse, int httpCode) {\n-        MockResponse mockResponseObj =\n-                new MockResponse()\n-                        .setResponseCode(httpCode)\n-                        .setBody(mockResponse)\n-                        .addHeader(\""Content-Type\"", \""application/json\"");\n-        mockWebServer.enqueue(mockResponseObj);\n-    }\n-\n-    private void mockConfig(String warehouseStr) {\n-        String mockResponse =\n-                String.format(\n-                        \""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\"", \\\""%s\\\"": \\\""%s\\\""}}\"",\n-                        RESTCatalogInternalOptions.PREFIX.key(),\n-                        \""prefix\"",\n-                        CatalogOptions.WAREHOUSE.key(),\n-                        warehouseStr);\n-        mockResponse(mockResponse, 200);\n-    }\n-\n-    public Options mockInitOptions() {\n-        Options options = new Options();\n-        options.set(RESTCatalogOptions.URI, serverUrl);\n-        String initToken = \""init_token\"";\n-        options.set(RESTCatalogOptions.TOKEN, initToken);\n-        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n-        return options;\n+        catalog.createDatabase(identifier.getDatabaseName(), false);\n+        catalog.createTable(\n+                identifier,\n+                new Schema(\n+                        Lists.newArrayList(new DataField(0, \""col1\"", DataTypes.STRING())),\n+                        partitionKeys,\n+                        Collections.emptyList(),\n+                        options,\n+                        \""\""),\n+                true);\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 38a6e08751f9..354efe69d5d2 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -53,7 +53,8 @@\n \n /** Test for {@link RESTObjectMapper}. */\n public class RESTObjectMapperTest {\n-    private ObjectMapper mapper = RESTObjectMapper.create();\n+\n+    private static final ObjectMapper OBJECT_MAPPER = RESTObjectMapper.create();\n \n     @Test\n     public void configResponseParseTest() throws Exception {\n@@ -61,8 +62,8 @@ public void configResponseParseTest() throws Exception {\n         Map<String, String> conf = new HashMap<>();\n         conf.put(confKey, \""b\"");\n         ConfigResponse response = new ConfigResponse(conf, conf);\n-        String responseStr = mapper.writeValueAsString(response);\n-        ConfigResponse parseData = mapper.readValue(responseStr, ConfigResponse.class);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        ConfigResponse parseData = OBJECT_MAPPER.readValue(responseStr, ConfigResponse.class);\n         assertEquals(conf.get(confKey), parseData.getDefaults().get(confKey));\n     }\n \n@@ -70,9 +71,10 @@ public void configResponseParseTest() throws Exception {\n     public void errorResponseParseTest() throws Exception {\n         String message = \""message\"";\n         Integer code = 400;\n-        ErrorResponse response = new ErrorResponse(message, code, new ArrayList<String>());\n-        String responseStr = mapper.writeValueAsString(response);\n-        ErrorResponse parseData = mapper.readValue(responseStr, ErrorResponse.class);\n+        ErrorResponse response =\n+                new ErrorResponse(null, null, message, code, new ArrayList<String>());\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        ErrorResponse parseData = OBJECT_MAPPER.readValue(responseStr, ErrorResponse.class);\n         assertEquals(message, parseData.getMessage());\n         assertEquals(code, parseData.getCode());\n     }\n@@ -81,8 +83,9 @@ public void errorResponseParseTest() throws Exception {\n     public void createDatabaseRequestParseTest() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         CreateDatabaseRequest request = MockRESTMessage.createDatabaseRequest(name);\n-        String requestStr = mapper.writeValueAsString(request);\n-        CreateDatabaseRequest parseData = mapper.readValue(requestStr, CreateDatabaseRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        CreateDatabaseRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, CreateDatabaseRequest.class);\n         assertEquals(request.getName(), parseData.getName());\n         assertEquals(request.getOptions().size(), parseData.getOptions().size());\n     }\n@@ -91,9 +94,9 @@ public void createDatabaseRequestParseTest() throws Exception {\n     public void createDatabaseResponseParseTest() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         CreateDatabaseResponse response = MockRESTMessage.createDatabaseResponse(name);\n-        String responseStr = mapper.writeValueAsString(response);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n         CreateDatabaseResponse parseData =\n-                mapper.readValue(responseStr, CreateDatabaseResponse.class);\n+                OBJECT_MAPPER.readValue(responseStr, CreateDatabaseResponse.class);\n         assertEquals(name, parseData.getName());\n         assertEquals(response.getOptions().size(), parseData.getOptions().size());\n     }\n@@ -102,8 +105,9 @@ public void createDatabaseResponseParseTest() throws Exception {\n     public void getDatabaseResponseParseTest() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         GetDatabaseResponse response = MockRESTMessage.getDatabaseResponse(name);\n-        String responseStr = mapper.writeValueAsString(response);\n-        GetDatabaseResponse parseData = mapper.readValue(responseStr, GetDatabaseResponse.class);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        GetDatabaseResponse parseData =\n+                OBJECT_MAPPER.readValue(responseStr, GetDatabaseResponse.class);\n         assertEquals(name, parseData.getName());\n         assertEquals(response.getOptions().size(), parseData.getOptions().size());\n         assertEquals(response.comment().get(), parseData.comment().get());\n@@ -113,9 +117,9 @@ public void getDatabaseResponseParseTest() throws Exception {\n     public void listDatabaseResponseParseTest() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         ListDatabasesResponse response = MockRESTMessage.listDatabasesResponse(name);\n-        String responseStr = mapper.writeValueAsString(response);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n         ListDatabasesResponse parseData =\n-                mapper.readValue(responseStr, ListDatabasesResponse.class);\n+                OBJECT_MAPPER.readValue(responseStr, ListDatabasesResponse.class);\n         assertEquals(response.getDatabases().size(), parseData.getDatabases().size());\n         assertEquals(name, parseData.getDatabases().get(0));\n     }\n@@ -123,8 +127,9 @@ public void listDatabaseResponseParseTest() throws Exception {\n     @Test\n     public void alterDatabaseRequestParseTest() throws Exception {\n         AlterDatabaseRequest request = MockRESTMessage.alterDatabaseRequest();\n-        String requestStr = mapper.writeValueAsString(request);\n-        AlterDatabaseRequest parseData = mapper.readValue(requestStr, AlterDatabaseRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        AlterDatabaseRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, AlterDatabaseRequest.class);\n         assertEquals(request.getRemovals().size(), parseData.getRemovals().size());\n         assertEquals(request.getUpdates().size(), parseData.getUpdates().size());\n     }\n@@ -132,9 +137,9 @@ public void alterDatabaseRequestParseTest() throws Exception {\n     @Test\n     public void alterDatabaseResponseParseTest() throws Exception {\n         AlterDatabaseResponse response = MockRESTMessage.alterDatabaseResponse();\n-        String responseStr = mapper.writeValueAsString(response);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n         AlterDatabaseResponse parseData =\n-                mapper.readValue(responseStr, AlterDatabaseResponse.class);\n+                OBJECT_MAPPER.readValue(responseStr, AlterDatabaseResponse.class);\n         assertEquals(response.getRemoved().size(), parseData.getRemoved().size());\n         assertEquals(response.getUpdated().size(), parseData.getUpdated().size());\n         assertEquals(response.getMissing().size(), parseData.getMissing().size());\n@@ -143,8 +148,9 @@ public void alterDatabaseResponseParseTest() throws Exception {\n     @Test\n     public void createTableRequestParseTest() throws Exception {\n         CreateTableRequest request = MockRESTMessage.createTableRequest(\""t1\"");\n-        String requestStr = mapper.writeValueAsString(request);\n-        CreateTableRequest parseData = mapper.readValue(requestStr, CreateTableRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        CreateTableRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, CreateTableRequest.class);\n         assertEquals(request.getIdentifier(), parseData.getIdentifier());\n         assertEquals(request.getSchema(), parseData.getSchema());\n     }\n@@ -160,7 +166,7 @@ public void dataFieldParseTest() throws Exception {\n                 String.format(\n                         \""{\\\""id\\\"": %d,\\\""name\\\"":\\\""%s\\\"",\\\""type\\\"":\\\""%s\\\"", \\\""description\\\"":\\\""%s\\\""}\"",\n                         id, name, type, descStr);\n-        DataField parseData = mapper.readValue(dataFieldStr, DataField.class);\n+        DataField parseData = OBJECT_MAPPER.readValue(dataFieldStr, DataField.class);\n         assertEquals(id, parseData.id());\n         assertEquals(name, parseData.name());\n         assertEquals(type, parseData.type());\n@@ -170,16 +176,17 @@ public void dataFieldParseTest() throws Exception {\n     @Test\n     public void renameTableRequestParseTest() throws Exception {\n         RenameTableRequest request = MockRESTMessage.renameRequest(\""t2\"");\n-        String requestStr = mapper.writeValueAsString(request);\n-        RenameTableRequest parseData = mapper.readValue(requestStr, RenameTableRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        RenameTableRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, RenameTableRequest.class);\n         assertEquals(request.getNewIdentifier(), parseData.getNewIdentifier());\n     }\n \n     @Test\n     public void getTableResponseParseTest() throws Exception {\n         GetTableResponse response = MockRESTMessage.getTableResponse();\n-        String responseStr = mapper.writeValueAsString(response);\n-        GetTableResponse parseData = mapper.readValue(responseStr, GetTableResponse.class);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        GetTableResponse parseData = OBJECT_MAPPER.readValue(responseStr, GetTableResponse.class);\n         assertEquals(response.getSchemaId(), parseData.getSchemaId());\n         assertEquals(response.getSchema(), parseData.getSchema());\n     }\n@@ -187,25 +194,26 @@ public void getTableResponseParseTest() throws Exception {\n     @Test\n     public void listTablesResponseParseTest() throws Exception {\n         ListTablesResponse response = MockRESTMessage.listTablesResponse();\n-        String responseStr = mapper.writeValueAsString(response);\n-        ListTablesResponse parseData = mapper.readValue(responseStr, ListTablesResponse.class);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n+        ListTablesResponse parseData =\n+                OBJECT_MAPPER.readValue(responseStr, ListTablesResponse.class);\n         assertEquals(response.getTables(), parseData.getTables());\n     }\n \n     @Test\n     public void alterTableRequestParseTest() throws Exception {\n         AlterTableRequest request = MockRESTMessage.alterTableRequest();\n-        String requestStr = mapper.writeValueAsString(request);\n-        AlterTableRequest parseData = mapper.readValue(requestStr, AlterTableRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        AlterTableRequest parseData = OBJECT_MAPPER.readValue(requestStr, AlterTableRequest.class);\n         assertEquals(parseData.getChanges().size(), parseData.getChanges().size());\n     }\n \n     @Test\n     public void createPartitionRequestParseTest() throws JsonProcessingException {\n         CreatePartitionRequest request = MockRESTMessage.createPartitionRequest(\""t1\"");\n-        String requestStr = mapper.writeValueAsString(request);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n         CreatePartitionRequest parseData =\n-                mapper.readValue(requestStr, CreatePartitionRequest.class);\n+                OBJECT_MAPPER.readValue(requestStr, CreatePartitionRequest.class);\n         assertEquals(parseData.getIdentifier(), parseData.getIdentifier());\n         assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n     }\n@@ -213,17 +221,18 @@ public void createPartitionRequestParseTest() throws JsonProcessingException {\n     @Test\n     public void dropPartitionRequestParseTest() throws JsonProcessingException {\n         DropPartitionRequest request = MockRESTMessage.dropPartitionRequest();\n-        String requestStr = mapper.writeValueAsString(request);\n-        DropPartitionRequest parseData = mapper.readValue(requestStr, DropPartitionRequest.class);\n+        String requestStr = OBJECT_MAPPER.writeValueAsString(request);\n+        DropPartitionRequest parseData =\n+                OBJECT_MAPPER.readValue(requestStr, DropPartitionRequest.class);\n         assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n     }\n \n     @Test\n     public void listPartitionsResponseParseTest() throws Exception {\n         ListPartitionsResponse response = MockRESTMessage.listPartitionsResponse();\n-        String responseStr = mapper.writeValueAsString(response);\n+        String responseStr = OBJECT_MAPPER.writeValueAsString(response);\n         ListPartitionsResponse parseData =\n-                mapper.readValue(responseStr, ListPartitionsResponse.class);\n+                OBJECT_MAPPER.readValue(responseStr, ListPartitionsResponse.class);\n         assertEquals(\n                 response.getPartitions().get(0).fileCount(),\n                 parseData.getPartitions().get(0).fileCount());\n@@ -232,10 +241,11 @@ public void listPartitionsResponseParseTest() throws Exception {\n     @Test\n     public void partitionResponseParseTest() throws Exception {\n         PartitionResponse response = MockRESTMessage.partitionResponse();\n-        assertDoesNotThrow(() -> mapper.writeValueAsString(response));\n+        assertDoesNotThrow(() -> OBJECT_MAPPER.writeValueAsString(response));\n         assertDoesNotThrow(\n                 () ->\n-                        mapper.readValue(\n-                                mapper.writeValueAsString(response), PartitionResponse.class));\n+                        OBJECT_MAPPER.readValue(\n+                                OBJECT_MAPPER.writeValueAsString(response),\n+                                PartitionResponse.class));\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\nindex 1f4a48fd5e8c..fec749208273 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n@@ -47,6 +47,23 @@ public class AuthSessionTest {\n \n     @Rule public TemporaryFolder folder = new TemporaryFolder();\n \n+    @Test\n+    public void testBearToken() {\n+        String token = UUID.randomUUID().toString();\n+        Map<String, String> initialHeaders = new HashMap<>();\n+        initialHeaders.put(\""k1\"", \""v1\"");\n+        initialHeaders.put(\""k2\"", \""v2\"");\n+        CredentialsProvider credentialsProvider = new BearTokenCredentialsProvider(token);\n+        AuthSession session = new AuthSession(initialHeaders, credentialsProvider);\n+        Map<String, String> header = session.getHeaders();\n+        assertEquals(header.get(\""Authorization\""), \""Bearer \"" + token);\n+        assertEquals(header.get(\""k1\""), \""v1\"");\n+        for (Map.Entry<String, String> entry : initialHeaders.entrySet()) {\n+            assertEquals(entry.getValue(), header.get(entry.getKey()));\n+        }\n+        assertEquals(header.size(), initialHeaders.size() + 1);\n+    }\n+\n     @Test\n     public void testRefreshBearTokenFileCredentialsProvider()\n             throws IOException, InterruptedException {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java\nindex 19aa6d5d7439..01d615c3e18d 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogITCaseBase.java\n@@ -75,7 +75,9 @@ public void before() throws IOException {\n \n         Map<String, String> options = new HashMap<>(catalogOptions());\n         options.put(\""type\"", \""paimon\"");\n-        options.put(\""warehouse\"", toWarehouse(path));\n+        if (supportDefineWarehouse()) {\n+            options.put(\""warehouse\"", toWarehouse(path));\n+        }\n         tEnv.executeSql(\n                 String.format(\n                         \""CREATE CATALOG %s WITH (\"" + \""%s\"" + inferScan + \"")\"",\n@@ -97,6 +99,10 @@ protected Map<String, String> catalogOptions() {\n         return Collections.emptyMap();\n     }\n \n+    protected boolean supportDefineWarehouse() {\n+        return true;\n+    }\n+\n     protected boolean inferScanParallelism() {\n         return false;\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RESTCatalogITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RESTCatalogITCase.java\nnew file mode 100644\nindex 000000000000..c310de32fdb9\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RESTCatalogITCase.java\n@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink;\n+\n+import org.apache.paimon.rest.RESTCatalogOptions;\n+import org.apache.paimon.rest.RESTCatalogServer;\n+\n+import org.apache.flink.types.Row;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/** ITCase for REST catalog. */\n+class RESTCatalogITCase extends CatalogITCaseBase {\n+\n+    private static final String DATABASE_NAME = \""mydb\"";\n+    private static final String TABLE_NAME = \""t1\"";\n+\n+    private RESTCatalogServer restCatalogServer;\n+    private String serverUrl;\n+    private String warehouse;\n+    @TempDir java.nio.file.Path tempFile;\n+\n+    @BeforeEach\n+    @Override\n+    public void before() throws IOException {\n+        String initToken = \""init_token\"";\n+        warehouse = tempFile.toUri().toString();\n+        restCatalogServer = new RESTCatalogServer(warehouse, initToken);\n+        restCatalogServer.start();\n+        serverUrl = restCatalogServer.getUrl();\n+        super.before();\n+        sql(String.format(\""CREATE DATABASE %s\"", DATABASE_NAME));\n+        sql(String.format(\""CREATE TABLE %s.%s (a STRING, b DOUBLE)\"", DATABASE_NAME, TABLE_NAME));\n+    }\n+\n+    @AfterEach()\n+    public void after() throws IOException {\n+        sql(String.format(\""DROP TABLE  %s.%s\"", DATABASE_NAME, TABLE_NAME));\n+        sql(String.format(\""DROP DATABASE %s\"", DATABASE_NAME));\n+        restCatalogServer.shutdown();\n+    }\n+\n+    @Test\n+    void testCreateTable() {\n+        List<Row> result = sql(String.format(\""SHOW CREATE TABLE %s.%s\"", DATABASE_NAME, TABLE_NAME));\n+        assertThat(result.toString())\n+                .contains(\n+                        String.format(\n+                                \""CREATE TABLE `PAIMON`.`%s`.`%s` (\\n\""\n+                                        + \""  `a` VARCHAR(2147483647),\\n\""\n+                                        + \""  `b` DOUBLE\"",\n+                                DATABASE_NAME, TABLE_NAME));\n+    }\n+\n+    @Test\n+    void testAlterTable() {\n+        sql(String.format(\""ALTER TABLE %s.%s ADD e INT AFTER b\"", DATABASE_NAME, TABLE_NAME));\n+        sql(String.format(\""ALTER TABLE %s.%s DROP b\"", DATABASE_NAME, TABLE_NAME));\n+        sql(String.format(\""ALTER TABLE %s.%s RENAME a TO a1\"", DATABASE_NAME, TABLE_NAME));\n+        sql(String.format(\""ALTER TABLE %s.%s MODIFY e DOUBLE\"", DATABASE_NAME, TABLE_NAME));\n+        List<Row> result = sql(String.format(\""SHOW CREATE TABLE %s.%s\"", DATABASE_NAME, TABLE_NAME));\n+        assertThat(result.toString())\n+                .contains(\n+                        String.format(\n+                                \""CREATE TABLE `PAIMON`.`%s`.`%s` (\\n\""\n+                                        + \""  `a1` VARCHAR(2147483647),\\n\""\n+                                        + \""  `e` DOUBLE\"",\n+                                DATABASE_NAME, TABLE_NAME));\n+    }\n+\n+    @Override\n+    protected Map<String, String> catalogOptions() {\n+        String initToken = \""init_token\"";\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""metastore\"", \""rest\"");\n+        options.put(RESTCatalogOptions.URI.key(), serverUrl);\n+        options.put(RESTCatalogOptions.TOKEN.key(), initToken);\n+        options.put(RESTCatalogOptions.THREAD_POOL_SIZE.key(), \""\"" + 1);\n+        return options;\n+    }\n+\n+    @Override\n+    protected String getTempDirPath() {\n+        return this.warehouse;\n+    }\n+\n+    @Override\n+    protected boolean supportDefineWarehouse() {\n+        return false;\n+    }\n+}\n\ndiff --git a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\nindex d96fac808cab..e185e5acbf50 100644\n--- a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n+++ b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n@@ -172,11 +172,6 @@ private void testHiveConfDirFromEnvImpl() {\n         assertThat(hiveConf.get(\""hive.metastore.uris\"")).isEqualTo(\""dummy-hms\"");\n     }\n \n-    @Test\n-    public void testAlterDatabase() throws Exception {\n-        this.alterDatabaseWhenSupportAlter();\n-    }\n-\n     @Test\n     public void testAddHiveTableParameters() {\n         try {\n@@ -503,4 +498,9 @@ public void testPartitionTable() throws Exception {\n         // hive catalog list partitions from filesystem, so here return empty.\n         assertThat(catalog.listPartitions(identifier)).isEmpty();\n     }\n+\n+    @Override\n+    protected boolean supportsAlterDatabase() {\n+        return true;\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4786"", ""pr_id"": 4786, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 3, ""non_test_files_count"": 11, ""pr_changed_files"": [""paimon-common/src/main/java/org/apache/paimon/utils/InternalRowPartitionComputer.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ListPartitionsResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java""], ""base_commit"": ""71921c5146f1e2f9d6f260647853c1954d3baaa7"", ""head_commit"": ""67da13454a2a9fb23c9574ecbb09524d89816021"", ""repo_url"": ""https://github.com/apache/paimon/pull/4786"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4786"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-30T11:16:10.000Z"", ""patch"": ""diff --git a/paimon-common/src/main/java/org/apache/paimon/utils/InternalRowPartitionComputer.java b/paimon-common/src/main/java/org/apache/paimon/utils/InternalRowPartitionComputer.java\nindex 6bb26d76138e..f4aad8f03f4b 100644\n--- a/paimon-common/src/main/java/org/apache/paimon/utils/InternalRowPartitionComputer.java\n+++ b/paimon-common/src/main/java/org/apache/paimon/utils/InternalRowPartitionComputer.java\n@@ -111,7 +111,7 @@ public static GenericRow convertSpecToInternalRow(\n         List<String> fieldNames = partType.getFieldNames();\n         for (Map.Entry<String, String> entry : spec.entrySet()) {\n             Object value =\n-                    defaultPartValue.equals(entry.getValue())\n+                    defaultPartValue != null && defaultPartValue.equals(entry.getValue())\n                             ? null\n                             : castFromString(\n                                     entry.getValue(), partType.getField(entry.getKey()).type());\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\nindex 87f3fad9b2fd..d92cab510201 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n@@ -94,6 +94,23 @@ public <T extends RESTResponse> T delete(String path, Map<String, String> header\n         return exec(request, null);\n     }\n \n+    @Override\n+    public <T extends RESTResponse> T delete(\n+            String path, RESTRequest body, Map<String, String> headers) {\n+        try {\n+            RequestBody requestBody = buildRequestBody(body);\n+            Request request =\n+                    new Request.Builder()\n+                            .url(uri + path)\n+                            .delete(requestBody)\n+                            .headers(Headers.of(headers))\n+                            .build();\n+            return exec(request, null);\n+        } catch (JsonProcessingException e) {\n+            throw new RESTException(e, \""build request failed.\"");\n+        }\n+    }\n+\n     @Override\n     public void close() throws IOException {\n         okHttpClient.dispatcher().cancelAll();\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex e0b3dccdd56b..c430e303b276 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -26,9 +26,12 @@\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.catalog.PropertyChange;\n+import org.apache.paimon.data.GenericRow;\n+import org.apache.paimon.data.serializer.InternalRowSerializer;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.manifest.PartitionEntry;\n+import org.apache.paimon.operation.FileStoreCommit;\n import org.apache.paimon.operation.Lock;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n@@ -41,7 +44,9 @@\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -49,7 +54,9 @@\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.TableSchema;\n@@ -58,10 +65,11 @@\n import org.apache.paimon.table.FileStoreTableFactory;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.object.ObjectTable;\n+import org.apache.paimon.table.sink.BatchWriteBuilder;\n+import org.apache.paimon.types.RowType;\n import org.apache.paimon.utils.Pair;\n import org.apache.paimon.utils.Preconditions;\n \n-import org.apache.paimon.shade.guava30.com.google.common.annotations.VisibleForTesting;\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n@@ -71,15 +79,20 @@\n import java.io.IOException;\n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.stream.Collectors;\n \n+import static org.apache.paimon.CoreOptions.createCommitUser;\n import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemDatabase;\n+import static org.apache.paimon.catalog.CatalogUtils.checkNotSystemTable;\n import static org.apache.paimon.catalog.CatalogUtils.isSystemDatabase;\n import static org.apache.paimon.options.CatalogOptions.CASE_SENSITIVE;\n+import static org.apache.paimon.utils.InternalRowPartitionComputer.convertSpecToInternalRow;\n import static org.apache.paimon.utils.Preconditions.checkNotNull;\n import static org.apache.paimon.utils.ThreadPoolUtils.createScheduledThreadPool;\n \n@@ -132,7 +145,8 @@ public RESTCatalog(CatalogContext catalogContext) {\n         Map<String, String> initHeaders =\n                 RESTUtil.merge(\n                         configHeaders(catalogOptions.toMap()), this.catalogAuth.getHeaders());\n-        Options options = new Options(fetchOptionsFromServer(initHeaders, initHeaders));\n+        Options options =\n+                new Options(fetchOptionsFromServer(initHeaders, catalogContext.options().toMap()));\n         this.context =\n                 CatalogContext.create(\n                         options, catalogContext.preferIO(), catalogContext.fallbackIO());\n@@ -141,20 +155,6 @@ public RESTCatalog(CatalogContext catalogContext) {\n         this.fileIO = getFileIOFromOptions(context);\n     }\n \n-    private static FileIO getFileIOFromOptions(CatalogContext context) {\n-        try {\n-            Options options = context.options();\n-            String warehouseStr = options.get(CatalogOptions.WAREHOUSE);\n-            Path warehousePath = new Path(warehouseStr);\n-            CatalogContext contextWithNewOptions =\n-                    CatalogContext.create(options, context.preferIO(), context.fallbackIO());\n-            return FileIO.get(warehousePath, contextWithNewOptions);\n-        } catch (IOException e) {\n-            LOG.warn(\""Can not get FileIO from options.\"");\n-            throw new RuntimeException(e);\n-        }\n-    }\n-\n     @Override\n     public String warehouse() {\n         return context.options().get(CatalogOptions.WAREHOUSE);\n@@ -360,17 +360,43 @@ public void dropTable(Identifier identifier, boolean ignoreIfNotExists)\n     @Override\n     public void createPartition(Identifier identifier, Map<String, String> partitionSpec)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        try {\n+            CreatePartitionRequest request = new CreatePartitionRequest(identifier, partitionSpec);\n+            client.post(\n+                    resourcePaths.partitions(\n+                            identifier.getDatabaseName(), identifier.getTableName()),\n+                    request,\n+                    PartitionResponse.class,\n+                    headers());\n+        } catch (NoSuchResourceException e) {\n+            throw new TableNotExistException(identifier);\n+        } catch (ForbiddenException e) {\n+            throw new TableNoPermissionException(identifier, e);\n+        }\n     }\n \n     @Override\n     public void dropPartition(Identifier identifier, Map<String, String> partitions)\n-            throws TableNotExistException, PartitionNotExistException {}\n+            throws TableNotExistException, PartitionNotExistException {\n+        checkNotSystemTable(identifier, \""dropPartition\"");\n+        dropPartitionMetadata(identifier, partitions);\n+        Table table = getTable(identifier);\n+        cleanPartitionsInFileSystem(table, partitions);\n+    }\n \n     @Override\n     public List<PartitionEntry> listPartitions(Identifier identifier)\n             throws TableNotExistException {\n-        throw new UnsupportedOperationException();\n+        FileStoreTable table = (FileStoreTable) getTable(identifier);\n+        boolean whetherSupportListPartitions =\n+                Boolean.parseBoolean(\n+                        table.options().get(CoreOptions.METASTORE_PARTITIONED_TABLE.key()));\n+        if (whetherSupportListPartitions) {\n+            RowType rowType = table.schema().logicalPartitionType();\n+            return listPartitionsFromServer(identifier, rowType);\n+        } else {\n+            return getTable(identifier).newReadBuilder().newScan().listPartitionEntries();\n+        }\n     }\n \n     @Override\n@@ -388,16 +414,14 @@ public void close() throws Exception {\n         }\n     }\n \n-    @VisibleForTesting\n-    Map<String, String> fetchOptionsFromServer(\n+    protected Map<String, String> fetchOptionsFromServer(\n             Map<String, String> headers, Map<String, String> clientProperties) {\n         ConfigResponse response =\n                 client.get(ResourcePaths.V1_CONFIG, ConfigResponse.class, headers);\n         return response.merge(clientProperties);\n     }\n \n-    @VisibleForTesting\n-    Table getDataOrFormatTable(Identifier identifier) throws TableNotExistException {\n+    private Table getDataOrFormatTable(Identifier identifier) throws TableNotExistException {\n         Preconditions.checkArgument(identifier.getSystemTableName() == null);\n         GetTableResponse response = getTableResponse(identifier);\n         FileStoreTable table =\n@@ -420,8 +444,42 @@ Table getDataOrFormatTable(Identifier identifier) throws TableNotExistException\n         return table;\n     }\n \n-    protected GetTableResponse getTableResponse(Identifier identifier)\n+    private List<PartitionEntry> listPartitionsFromServer(Identifier identifier, RowType rowType)\n             throws TableNotExistException {\n+        try {\n+            ListPartitionsResponse response =\n+                    client.get(\n+                            resourcePaths.partitions(\n+                                    identifier.getDatabaseName(), identifier.getTableName()),\n+                            ListPartitionsResponse.class,\n+                            headers());\n+            if (response != null && response.getPartitions() != null) {\n+                return response.getPartitions().stream()\n+                        .map(p -> convertToPartitionEntry(p, rowType))\n+                        .collect(Collectors.toList());\n+            } else {\n+                return Collections.emptyList();\n+            }\n+        } catch (NoSuchResourceException e) {\n+            throw new TableNotExistException(identifier);\n+        } catch (ForbiddenException e) {\n+            throw new TableNoPermissionException(identifier, e);\n+        }\n+    }\n+\n+    private void cleanPartitionsInFileSystem(Table table, Map<String, String> partitions) {\n+        FileStoreTable fileStoreTable = (FileStoreTable) table;\n+        try (FileStoreCommit commit =\n+                fileStoreTable\n+                        .store()\n+                        .newCommit(\n+                                createCommitUser(fileStoreTable.coreOptions().toConfiguration()))) {\n+            commit.dropPartitions(\n+                    Collections.singletonList(partitions), BatchWriteBuilder.COMMIT_IDENTIFIER);\n+        }\n+    }\n+\n+    private GetTableResponse getTableResponse(Identifier identifier) throws TableNotExistException {\n         try {\n             return client.get(\n                     resourcePaths.table(identifier.getDatabaseName(), identifier.getTableName()),\n@@ -434,6 +492,23 @@ protected GetTableResponse getTableResponse(Identifier identifier)\n         }\n     }\n \n+    private boolean dropPartitionMetadata(Identifier identifier, Map<String, String> partitions)\n+            throws TableNoPermissionException, PartitionNotExistException {\n+        try {\n+            DropPartitionRequest request = new DropPartitionRequest(partitions);\n+            client.delete(\n+                    resourcePaths.partitions(\n+                            identifier.getDatabaseName(), identifier.getTableName()),\n+                    request,\n+                    headers());\n+            return true;\n+        } catch (NoSuchResourceException ignore) {\n+            throw new PartitionNotExistException(identifier, partitions);\n+        } catch (ForbiddenException e) {\n+            throw new TableNoPermissionException(identifier, e);\n+        }\n+    }\n+\n     private static Map<String, String> configHeaders(Map<String, String> properties) {\n         return RESTUtil.extractPrefixMap(properties, \""header.\"");\n     }\n@@ -464,4 +539,29 @@ private ScheduledExecutorService tokenRefreshExecutor() {\n \n         return refreshExecutor;\n     }\n+\n+    private PartitionEntry convertToPartitionEntry(PartitionResponse partition, RowType rowType) {\n+        InternalRowSerializer serializer = new InternalRowSerializer(rowType);\n+        GenericRow row = convertSpecToInternalRow(partition.getSpec(), rowType, null);\n+        return new PartitionEntry(\n+                serializer.toBinaryRow(row).copy(),\n+                partition.getRecordCount(),\n+                partition.getFileSizeInBytes(),\n+                partition.getFileCount(),\n+                partition.getLastFileCreationTime());\n+    }\n+\n+    private static FileIO getFileIOFromOptions(CatalogContext context) {\n+        try {\n+            Options options = context.options();\n+            String warehouseStr = options.get(CatalogOptions.WAREHOUSE);\n+            Path warehousePath = new Path(warehouseStr);\n+            CatalogContext contextWithNewOptions =\n+                    CatalogContext.create(options, context.preferIO(), context.fallbackIO());\n+            return FileIO.get(warehousePath, contextWithNewOptions);\n+        } catch (IOException e) {\n+            LOG.warn(\""Can not get FileIO from options.\"");\n+            throw new RuntimeException(e);\n+        }\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\nindex a255d688bc52..d90cb5fa4ad9 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n@@ -30,4 +30,6 @@ <T extends RESTResponse> T post(\n             String path, RESTRequest body, Class<T> responseType, Map<String, String> headers);\n \n     <T extends RESTResponse> T delete(String path, Map<String, String> headers);\n+\n+    <T extends RESTResponse> T delete(String path, RESTRequest body, Map<String, String> headers);\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex f006713fe2b6..ebfdd2db1eec 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -82,4 +82,15 @@ public String renameTable(String databaseName, String tableName) {\n                 .add(\""rename\"")\n                 .toString();\n     }\n+\n+    public String partitions(String databaseName, String tableName) {\n+        return SLASH.add(\""v1\"")\n+                .add(prefix)\n+                .add(\""databases\"")\n+                .add(databaseName)\n+                .add(\""tables\"")\n+                .add(tableName)\n+                .add(\""partitions\"")\n+                .toString();\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java\nnew file mode 100644\nindex 000000000000..e8094ab821bf\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreatePartitionRequest.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Request for creating partition. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class CreatePartitionRequest implements RESTRequest {\n+\n+    private static final String FIELD_IDENTIFIER = \""identifier\"";\n+    private static final String FIELD_PARTITION_SPEC = \""spec\"";\n+\n+    @JsonProperty(FIELD_IDENTIFIER)\n+    private final Identifier identifier;\n+\n+    @JsonProperty(FIELD_PARTITION_SPEC)\n+    private final Map<String, String> partitionSpec;\n+\n+    @JsonCreator\n+    public CreatePartitionRequest(\n+            @JsonProperty(FIELD_IDENTIFIER) Identifier identifier,\n+            @JsonProperty(FIELD_PARTITION_SPEC) Map<String, String> partitionSpec) {\n+        this.identifier = identifier;\n+        this.partitionSpec = partitionSpec;\n+    }\n+\n+    @JsonGetter(FIELD_IDENTIFIER)\n+    public Identifier getIdentifier() {\n+        return identifier;\n+    }\n+\n+    @JsonGetter(FIELD_PARTITION_SPEC)\n+    public Map<String, String> getPartitionSpec() {\n+        return partitionSpec;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java\nnew file mode 100644\nindex 000000000000..4fabf1163651\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropPartitionRequest.java\n@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Request for deleting partition. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class DropPartitionRequest implements RESTRequest {\n+\n+    private static final String FIELD_PARTITION_SPEC = \""spec\"";\n+\n+    @JsonProperty(FIELD_PARTITION_SPEC)\n+    private final Map<String, String> partitionSpec;\n+\n+    @JsonCreator\n+    public DropPartitionRequest(\n+            @JsonProperty(FIELD_PARTITION_SPEC) Map<String, String> partitionSpec) {\n+        this.partitionSpec = partitionSpec;\n+    }\n+\n+    @JsonGetter(FIELD_PARTITION_SPEC)\n+    public Map<String, String> getPartitionSpec() {\n+        return partitionSpec;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListPartitionsResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListPartitionsResponse.java\nnew file mode 100644\nindex 000000000000..1f194d208e99\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListPartitionsResponse.java\n@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Response for listing partitions. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ListPartitionsResponse implements RESTResponse {\n+\n+    public static final String FIELD_PARTITIONS = \""partitions\"";\n+\n+    @JsonProperty(FIELD_PARTITIONS)\n+    private final List<PartitionResponse> partitions;\n+\n+    @JsonCreator\n+    public ListPartitionsResponse(\n+            @JsonProperty(FIELD_PARTITIONS) List<PartitionResponse> partitions) {\n+        this.partitions = partitions;\n+    }\n+\n+    @JsonGetter(FIELD_PARTITIONS)\n+    public List<PartitionResponse> getPartitions() {\n+        return partitions;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java\nnew file mode 100644\nindex 000000000000..2706b5d7daf9\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/PartitionResponse.java\n@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Partition for rest api. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class PartitionResponse implements RESTResponse {\n+\n+    public static final String FIELD_SPEC = \""spec\"";\n+    public static final String FIELD_RECORD_COUNT = \""recordCount\"";\n+    public static final String FIELD_FILE_SIZE_IN_BYTES = \""fileSizeInBytes\"";\n+    public static final String FIELD_FILE_COUNT = \""fileCount\"";\n+    public static final String FIELD_LAST_FILE_CREATION_TIME = \""lastFileCreationTime\"";\n+\n+    @JsonProperty(FIELD_SPEC)\n+    private final Map<String, String> spec;\n+\n+    @JsonProperty(FIELD_RECORD_COUNT)\n+    private final long recordCount;\n+\n+    @JsonProperty(FIELD_FILE_SIZE_IN_BYTES)\n+    private final long fileSizeInBytes;\n+\n+    @JsonProperty(FIELD_FILE_COUNT)\n+    private final long fileCount;\n+\n+    @JsonProperty(FIELD_LAST_FILE_CREATION_TIME)\n+    private final long lastFileCreationTime;\n+\n+    @JsonCreator\n+    public PartitionResponse(\n+            @JsonProperty(FIELD_SPEC) Map<String, String> spec,\n+            @JsonProperty(FIELD_RECORD_COUNT) long recordCount,\n+            @JsonProperty(FIELD_FILE_SIZE_IN_BYTES) long fileSizeInBytes,\n+            @JsonProperty(FIELD_FILE_COUNT) long fileCount,\n+            @JsonProperty(FIELD_LAST_FILE_CREATION_TIME) long lastFileCreationTime) {\n+        this.spec = spec;\n+        this.recordCount = recordCount;\n+        this.fileSizeInBytes = fileSizeInBytes;\n+        this.fileCount = fileCount;\n+        this.lastFileCreationTime = lastFileCreationTime;\n+    }\n+\n+    @JsonGetter(FIELD_SPEC)\n+    public Map<String, String> getSpec() {\n+        return spec;\n+    }\n+\n+    @JsonGetter(FIELD_RECORD_COUNT)\n+    public long getRecordCount() {\n+        return recordCount;\n+    }\n+\n+    @JsonGetter(FIELD_FILE_SIZE_IN_BYTES)\n+    public long getFileSizeInBytes() {\n+        return fileSizeInBytes;\n+    }\n+\n+    @JsonGetter(FIELD_FILE_COUNT)\n+    public long getFileCount() {\n+        return fileCount;\n+    }\n+\n+    @JsonGetter(FIELD_LAST_FILE_CREATION_TIME)\n+    public long getLastFileCreationTime() {\n+        return lastFileCreationTime;\n+    }\n+}\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 7fefd0254b1b..7a0c9663b4f2 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -28,6 +28,21 @@ servers:\n   - url: http://localhost:8080\n     description: Server URL in Development environment\n paths:\n+  /v1/config:\n+    get:\n+      tags:\n+        - config\n+      summary: Get Config\n+      operationId: getConfig\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ConfigResponse'\n+        \""500\"":\n+          description: Internal Server Error\n   /v1/{prefix}/databases:\n     get:\n       tags:\n@@ -80,6 +95,102 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n+  /v1/{prefix}/databases/{database}:\n+    get:\n+      tags:\n+        - database\n+      summary: Get Database\n+      operationId: getDatabases\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetDatabaseResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+    delete:\n+      tags:\n+        - database\n+      summary: Drop Database\n+      operationId: dropDatabase\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: Success, no content\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/properties:\n+    post:\n+      tags:\n+        - database\n+      summary: Alter Database\n+      operationId: alterDatabase\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/AlterDatabaseRequest'\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/AlterDatabaseResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n   /v1/{prefix}/databases/{database}/tables:\n     get:\n       tags:\n@@ -237,6 +348,8 @@ paths:\n           schema:\n             type: string\n       responses:\n+        \""200\"":\n+          description: Success, no content\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -293,12 +406,12 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}/properties:\n-    post:\n+  /v1/{prefix}/databases/{database}/tables/{table}/partitions:\n+    get:\n       tags:\n-        - database\n-      summary: Alter Database\n-      operationId: alterDatabase\n+        - partition\n+      summary: List partitions\n+      operationId: listPartitions\n       parameters:\n         - name: prefix\n           in: path\n@@ -310,18 +423,18 @@ paths:\n           required: true\n           schema:\n             type: string\n-      requestBody:\n-        content:\n-          application/json:\n-            schema:\n-              $ref: '#/components/schemas/AlterDatabaseRequest'\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n       responses:\n         \""200\"":\n           description: OK\n           content:\n             application/json:\n               schema:\n-                $ref: '#/components/schemas/AlterDatabaseResponse'\n+                $ref: '#/components/schemas/ListPartitionsResponse'\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -330,12 +443,11 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/{prefix}/databases/{database}:\n-    get:\n+    post:\n       tags:\n-        - database\n-      summary: Get Database\n-      operationId: getDatabases\n+        - partition\n+      summary: Create partition\n+      operationId: createPartition\n       parameters:\n         - name: prefix\n           in: path\n@@ -347,13 +459,23 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/CreatePartitionRequest'\n       responses:\n         \""200\"":\n           description: OK\n           content:\n             application/json:\n               schema:\n-                $ref: '#/components/schemas/GetDatabaseResponse'\n+                $ref: '#/components/schemas/PartitionResponse'\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -364,9 +486,9 @@ paths:\n           description: Internal Server Error\n     delete:\n       tags:\n-        - database\n-      summary: Drop Database\n-      operationId: dropDatabase\n+        - partition\n+      summary: Drop partition\n+      operationId: dropPartition\n       parameters:\n         - name: prefix\n           in: path\n@@ -378,7 +500,19 @@ paths:\n           required: true\n           schema:\n             type: string\n+        - name: table\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/DropPartitionRequest'\n       responses:\n+        \""200\"":\n+          description: Success, no content\n         \""404\"":\n           description: Resource not found\n           content:\n@@ -387,21 +521,6 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n-  /v1/config:\n-    get:\n-      tags:\n-        - config\n-      summary: Get Config\n-      operationId: getConfig\n-      responses:\n-        \""200\"":\n-          description: OK\n-          content:\n-            application/json:\n-              schema:\n-                $ref: '#/components/schemas/ConfigResponse'\n-        \""500\"":\n-          description: Internal Server Error\n components:\n   schemas:\n     CreateDatabaseRequest:\n@@ -413,6 +532,18 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n+    CreatePartitionRequest:\n+      type: object\n+      properties:\n+        identifier:\n+          $ref: '#/components/schemas/Identifier'\n+        spec:\n+          type: object\n+    DropPartitionRequest:\n+      type: object\n+      properties:\n+        spec:\n+          type: object\n     CreateDatabaseResponse:\n       type: object\n       properties:\n@@ -468,39 +599,38 @@ components:\n         type:\n           type: string\n           pattern: ^ARRAY.*\n+          example: ARRAY\n         element:\n-          type:\n-            $ref: '#/components/schemas/DataType'\n+          $ref: '#/components/schemas/DataType'\n     MultisetType:\n       type: object\n       properties:\n         type:\n           type: string\n           pattern: ^MULTISET.*\n+          example: MULTISET\n         element:\n-          type:\n-            $ref: '#/components/schemas/DataType'\n+          $ref: '#/components/schemas/DataType'\n     MapType:\n       type: object\n       properties:\n         type:\n           type: string\n           pattern: ^MAP.*\n+          example: MAP\n         key:\n-          type:\n-            $ref: '#/components/schemas/DataType'\n+          $ref: '#/components/schemas/DataType'\n         value:\n-          type:\n-            $ref: '#/components/schemas/DataType'\n+          $ref: '#/components/schemas/DataType'\n     RowType:\n       type: object\n       properties:\n         type:\n           type: string\n           pattern: ^ROW.*\n+          example: ROW\n         fields:\n-          type:\n-            $ref: '#/components/schemas/DataField'\n+          $ref: '#/components/schemas/DataField'\n     Identifier:\n       type: object\n       properties:\n@@ -744,7 +874,30 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n-\n+    ListPartitionsResponse:\n+      type: object\n+      properties:\n+        partitions:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/PartitionResponse'\n+    PartitionResponse:\n+      type: object\n+      properties:\n+        spec:\n+          type: object\n+        recordCount:\n+          type: integer\n+          format: int64\n+        fileSizeInBytes:\n+          type: integer\n+          format: int64\n+        fileCount:\n+          type: integer\n+          format: int64\n+        lastFileCreationTime:\n+          type: integer\n+          format: int64\n   securitySchemes:\n     BearerAuth:\n       type: http\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex 179c23ce46f8..62f99876a397 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -22,7 +22,9 @@\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -31,7 +33,9 @@\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.PartitionResponse;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n@@ -143,6 +147,7 @@ public GetDatabaseResponse getDatabases(\n             summary = \""Drop Database\"",\n             tags = {\""database\""})\n     @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""404\"",\n                 description = \""Resource not found\"",\n@@ -301,6 +306,7 @@ public GetTableResponse alterTable(\n             summary = \""Drop table\"",\n             tags = {\""table\""})\n     @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n         @ApiResponse(\n                 responseCode = \""404\"",\n                 description = \""Resource not found\"",\n@@ -346,4 +352,78 @@ public GetTableResponse renameTable(\n                         new HashMap<>(),\n                         \""comment\""));\n     }\n+\n+    @Operation(\n+            summary = \""List partitions\"",\n+            tags = {\""partition\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(schema = @Schema(implementation = ListPartitionsResponse.class))\n+                }),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions\"")\n+    public ListPartitionsResponse listPartitions(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table) {\n+        Map<String, String> spec = new HashMap<>();\n+        spec.put(\""f1\"", \""1\"");\n+        PartitionResponse partition = new PartitionResponse(spec, 1, 2, 3, 4);\n+        return new ListPartitionsResponse(ImmutableList.of(partition));\n+    }\n+\n+    @Operation(\n+            summary = \""Create partition\"",\n+            tags = {\""partition\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {@Content(schema = @Schema(implementation = PartitionResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions\"")\n+    public PartitionResponse createPartition(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table,\n+            @RequestBody CreatePartitionRequest request) {\n+        Map<String, String> spec = new HashMap<>();\n+        spec.put(\""f1\"", \""1\"");\n+        return new PartitionResponse(spec, 0, 0, 0, 4);\n+    }\n+\n+    @Operation(\n+            summary = \""Drop partition\"",\n+            tags = {\""partition\""})\n+    @ApiResponses({\n+        @ApiResponse(responseCode = \""200\"", description = \""Success, no content\""),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @DeleteMapping(\""/v1/{prefix}/databases/{database}/tables/{table}/partitions\"")\n+    public void dropPartition(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @PathVariable String table,\n+            @RequestBody DropPartitionRequest request) {}\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex 96d0c9d7c724..9b686b683773 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -18,11 +18,14 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.CoreOptions;\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n@@ -30,7 +33,9 @@\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.types.DataField;\n@@ -39,6 +44,7 @@\n import org.apache.paimon.types.IntType;\n import org.apache.paimon.types.RowType;\n \n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n import java.util.ArrayList;\n@@ -131,6 +137,26 @@ public static AlterTableRequest alterTableRequest() {\n         return new AlterTableRequest(getChanges());\n     }\n \n+    public static CreatePartitionRequest createPartitionRequest(String tableName) {\n+        Identifier identifier = Identifier.create(databaseName(), tableName);\n+        return new CreatePartitionRequest(identifier, Collections.singletonMap(\""pt\"", \""1\""));\n+    }\n+\n+    public static DropPartitionRequest dropPartitionRequest() {\n+        return new DropPartitionRequest(Collections.singletonMap(\""pt\"", \""1\""));\n+    }\n+\n+    public static PartitionResponse partitionResponse() {\n+        Map<String, String> spec = new HashMap<>();\n+        spec.put(\""f0\"", \""1\"");\n+        return new PartitionResponse(spec, 1, 1, 1, 1);\n+    }\n+\n+    public static ListPartitionsResponse listPartitionsResponse() {\n+        PartitionResponse partition = partitionResponse();\n+        return new ListPartitionsResponse(ImmutableList.of(partition));\n+    }\n+\n     public static List<SchemaChange> getChanges() {\n         // add option\n         SchemaChange addOption = SchemaChange.setOption(\""snapshot.time-retained\"", \""2h\"");\n@@ -202,20 +228,27 @@ public static List<SchemaChange> getChanges() {\n         return schemaChanges;\n     }\n \n+    public static GetTableResponse getTableResponseEnablePartition() {\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""option-1\"", \""value-1\"");\n+        options.put(CoreOptions.METASTORE_PARTITIONED_TABLE.key(), \""true\"");\n+        return new GetTableResponse(\""/tmp/2\"", 1, schema(options));\n+    }\n+\n     public static GetTableResponse getTableResponse() {\n-        return new GetTableResponse(\""/tmp/1\"", 1, schema());\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""option-1\"", \""value-1\"");\n+        options.put(\""option-2\"", \""value-2\"");\n+        return new GetTableResponse(\""/tmp/1\"", 1, schema(options));\n     }\n \n-    private static Schema schema() {\n+    private static Schema schema(Map<String, String> options) {\n         List<DataField> fields =\n                 Arrays.asList(\n                         new DataField(0, \""f0\"", new IntType()),\n                         new DataField(1, \""f1\"", new IntType()));\n         List<String> partitionKeys = Collections.singletonList(\""f0\"");\n         List<String> primaryKeys = Arrays.asList(\""f0\"", \""f1\"");\n-        Map<String, String> options = new HashMap<>();\n-        options.put(\""option-1\"", \""value-1\"");\n-        options.put(\""option-2\"", \""value-2\"");\n         return new Schema(fields, partitionKeys, primaryKeys, options, \""comment\"");\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex 01555adc3df4..67103aaa5204 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.catalog.CatalogContext;\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n@@ -31,7 +32,9 @@\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.table.Table;\n \n@@ -55,13 +58,6 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n-import static org.mockito.ArgumentMatchers.any;\n-import static org.mockito.ArgumentMatchers.anyBoolean;\n-import static org.mockito.ArgumentMatchers.anyList;\n-import static org.mockito.ArgumentMatchers.eq;\n-import static org.mockito.Mockito.spy;\n-import static org.mockito.Mockito.times;\n-import static org.mockito.Mockito.verify;\n \n /** Test for REST Catalog. */\n public class RESTCatalogTest {\n@@ -69,31 +65,19 @@ public class RESTCatalogTest {\n     private final ObjectMapper mapper = RESTObjectMapper.create();\n     private MockWebServer mockWebServer;\n     private RESTCatalog restCatalog;\n-    private RESTCatalog mockRestCatalog;\n     private String warehouseStr;\n+    private String serverUrl;\n     @Rule public TemporaryFolder folder = new TemporaryFolder();\n \n     @Before\n     public void setUp() throws IOException {\n         mockWebServer = new MockWebServer();\n         mockWebServer.start();\n-        String baseUrl = mockWebServer.url(\""\"").toString();\n-        Options options = new Options();\n-        options.set(RESTCatalogOptions.URI, baseUrl);\n-        String initToken = \""init_token\"";\n-        options.set(RESTCatalogOptions.TOKEN, initToken);\n-        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n+        serverUrl = mockWebServer.url(\""\"").toString();\n+        Options options = mockInitOptions();\n         warehouseStr = folder.getRoot().getPath();\n-        String mockResponse =\n-                String.format(\n-                        \""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\"", \\\""%s\\\"": \\\""%s\\\""}}\"",\n-                        RESTCatalogInternalOptions.PREFIX.key(),\n-                        \""prefix\"",\n-                        CatalogOptions.WAREHOUSE.key(),\n-                        warehouseStr);\n-        mockResponse(mockResponse, 200);\n+        mockConfig(warehouseStr);\n         restCatalog = new RESTCatalog(CatalogContext.create(options));\n-        mockRestCatalog = spy(restCatalog);\n     }\n \n     @After\n@@ -154,9 +138,7 @@ public void testGetDatabase() throws Exception {\n     public void testDropDatabase() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         mockResponse(\""\"", 200);\n-        assertDoesNotThrow(() -> mockRestCatalog.dropDatabase(name, false, true));\n-        verify(mockRestCatalog, times(1)).dropDatabase(eq(name), eq(false), eq(true));\n-        verify(mockRestCatalog, times(0)).listTables(eq(name));\n+        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, false, true));\n     }\n \n     @Test\n@@ -166,7 +148,7 @@ public void testDropDatabaseWhenNoExistAndIgnoreIfNotExistsIsFalse() throws Exce\n         mockResponse(mapper.writeValueAsString(response), 404);\n         assertThrows(\n                 Catalog.DatabaseNotExistException.class,\n-                () -> mockRestCatalog.dropDatabase(name, false, true));\n+                () -> restCatalog.dropDatabase(name, false, true));\n     }\n \n     @Test\n@@ -174,9 +156,7 @@ public void testDropDatabaseWhenNoExistAndIgnoreIfNotExistsIsTrue() throws Excep\n         String name = MockRESTMessage.databaseName();\n         ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n         mockResponse(mapper.writeValueAsString(response), 404);\n-        assertDoesNotThrow(() -> mockRestCatalog.dropDatabase(name, true, true));\n-        verify(mockRestCatalog, times(1)).dropDatabase(eq(name), eq(true), eq(true));\n-        verify(mockRestCatalog, times(0)).listTables(eq(name));\n+        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, true, true));\n     }\n \n     @Test\n@@ -186,9 +166,7 @@ public void testDropDatabaseWhenCascadeIsFalseAndNoTables() throws Exception {\n         ListTablesResponse response = MockRESTMessage.listTablesEmptyResponse();\n         mockResponse(mapper.writeValueAsString(response), 200);\n         mockResponse(\""\"", 200);\n-        assertDoesNotThrow(() -> mockRestCatalog.dropDatabase(name, false, cascade));\n-        verify(mockRestCatalog, times(1)).dropDatabase(eq(name), eq(false), eq(cascade));\n-        verify(mockRestCatalog, times(1)).listTables(eq(name));\n+        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, false, cascade));\n     }\n \n     @Test\n@@ -199,9 +177,7 @@ public void testDropDatabaseWhenCascadeIsFalseAndTablesExist() throws Exception\n         mockResponse(mapper.writeValueAsString(response), 200);\n         assertThrows(\n                 Catalog.DatabaseNotEmptyException.class,\n-                () -> mockRestCatalog.dropDatabase(name, false, cascade));\n-        verify(mockRestCatalog, times(1)).dropDatabase(eq(name), eq(false), eq(cascade));\n-        verify(mockRestCatalog, times(1)).listTables(eq(name));\n+                () -> restCatalog.dropDatabase(name, false, cascade));\n     }\n \n     @Test\n@@ -209,7 +185,7 @@ public void testAlterDatabase() throws Exception {\n         String name = MockRESTMessage.databaseName();\n         AlterDatabaseResponse response = MockRESTMessage.alterDatabaseResponse();\n         mockResponse(mapper.writeValueAsString(response), 200);\n-        assertDoesNotThrow(() -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), true));\n+        assertDoesNotThrow(() -> restCatalog.alterDatabase(name, new ArrayList<>(), true));\n     }\n \n     @Test\n@@ -220,7 +196,7 @@ public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsFalse()\n         mockResponse(mapper.writeValueAsString(response), 404);\n         assertThrows(\n                 Catalog.DatabaseNotExistException.class,\n-                () -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), false));\n+                () -> restCatalog.alterDatabase(name, new ArrayList<>(), false));\n     }\n \n     @Test\n@@ -228,7 +204,7 @@ public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsTrue() th\n         String name = MockRESTMessage.databaseName();\n         ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n         mockResponse(mapper.writeValueAsString(response), 404);\n-        assertDoesNotThrow(() -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), true));\n+        assertDoesNotThrow(() -> restCatalog.alterDatabase(name, new ArrayList<>(), true));\n     }\n \n     @Test\n@@ -245,10 +221,8 @@ public void testGetTable() throws Exception {\n         String databaseName = MockRESTMessage.databaseName();\n         GetTableResponse response = MockRESTMessage.getTableResponse();\n         mockResponse(mapper.writeValueAsString(response), 200);\n-        Table result = mockRestCatalog.getTable(Identifier.create(databaseName, \""table\""));\n-        // catalog will add path option\n+        Table result = restCatalog.getTable(Identifier.create(databaseName, \""table\""));\n         assertEquals(response.getSchema().options().size() + 1, result.options().size());\n-        verify(mockRestCatalog, times(1)).getDataOrFormatTable(any());\n     }\n \n     @Test\n@@ -278,11 +252,10 @@ public void testRenameTable() throws Exception {\n         mockResponse(mapper.writeValueAsString(response), 200);\n         assertDoesNotThrow(\n                 () ->\n-                        mockRestCatalog.renameTable(\n+                        restCatalog.renameTable(\n                                 Identifier.create(databaseName, fromTableName),\n                                 Identifier.create(databaseName, toTableName),\n                                 true));\n-        verify(mockRestCatalog, times(1)).renameTable(any(), any(), anyBoolean());\n     }\n \n     @Test\n@@ -294,7 +267,7 @@ public void testRenameTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws\n         assertThrows(\n                 Catalog.TableNotExistException.class,\n                 () ->\n-                        mockRestCatalog.renameTable(\n+                        restCatalog.renameTable(\n                                 Identifier.create(databaseName, fromTableName),\n                                 Identifier.create(databaseName, toTableName),\n                                 false));\n@@ -309,7 +282,7 @@ public void testRenameTableWhenToTableAlreadyExist() throws Exception {\n         assertThrows(\n                 Catalog.TableAlreadyExistException.class,\n                 () ->\n-                        mockRestCatalog.renameTable(\n+                        restCatalog.renameTable(\n                                 Identifier.create(databaseName, fromTableName),\n                                 Identifier.create(databaseName, toTableName),\n                                 false));\n@@ -322,10 +295,7 @@ public void testAlterTable() throws Exception {\n         GetTableResponse response = MockRESTMessage.getTableResponse();\n         mockResponse(mapper.writeValueAsString(response), 200);\n         assertDoesNotThrow(\n-                () ->\n-                        mockRestCatalog.alterTable(\n-                                Identifier.create(databaseName, \""t1\""), changes, true));\n-        verify(mockRestCatalog, times(1)).alterTable(any(), anyList(), anyBoolean());\n+                () -> restCatalog.alterTable(Identifier.create(databaseName, \""t1\""), changes, true));\n     }\n \n     @Test\n@@ -336,7 +306,7 @@ public void testAlterTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws\n         assertThrows(\n                 Catalog.TableNotExistException.class,\n                 () ->\n-                        mockRestCatalog.alterTable(\n+                        restCatalog.alterTable(\n                                 Identifier.create(databaseName, \""t1\""), changes, false));\n     }\n \n@@ -359,6 +329,127 @@ public void testDropTableWhenTableNotExistAndIgnoreIfNotExistsIsFalse() throws E\n                 () -> restCatalog.dropTable(Identifier.create(databaseName, tableName), false));\n     }\n \n+    @Test\n+    public void testCreatePartition() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        partitionSpec.put(\""p1\"", \""v1\"");\n+        PartitionResponse response = MockRESTMessage.partitionResponse();\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        assertDoesNotThrow(\n+                () ->\n+                        restCatalog.createPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testCreatePartitionWhenTableNotExist() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        partitionSpec.put(\""p1\"", \""v1\"");\n+        mockResponse(\""\"", 404);\n+        assertThrows(\n+                Catalog.TableNotExistException.class,\n+                () ->\n+                        restCatalog.createPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testCreatePartitionWhenTableNoPermissionException() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        partitionSpec.put(\""p1\"", \""v1\"");\n+        mockResponse(\""\"", 403);\n+        assertThrows(\n+                Catalog.TableNoPermissionException.class,\n+                () ->\n+                        restCatalog.createPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testDropPartition() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        GetTableResponse response = MockRESTMessage.getTableResponse();\n+        partitionSpec.put(response.getSchema().primaryKeys().get(0), \""1\"");\n+        mockResponse(mapper.writeValueAsString(\""\""), 200);\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        assertThrows(\n+                RuntimeException.class,\n+                () ->\n+                        restCatalog.dropPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testDropPartitionWhenPartitionNoExist() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        GetTableResponse response = MockRESTMessage.getTableResponse();\n+        partitionSpec.put(response.getSchema().primaryKeys().get(0), \""1\"");\n+        mockResponse(mapper.writeValueAsString(\""\""), 404);\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        assertThrows(\n+                Catalog.PartitionNotExistException.class,\n+                () ->\n+                        restCatalog.dropPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testDropPartitionWhenTableNoPermission() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        GetTableResponse response = MockRESTMessage.getTableResponse();\n+        partitionSpec.put(response.getSchema().primaryKeys().get(0), \""1\"");\n+        mockResponse(mapper.writeValueAsString(\""\""), 403);\n+        assertThrows(\n+                Catalog.TableNoPermissionException.class,\n+                () ->\n+                        restCatalog.dropPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testDropPartitionWhenTableNoExist() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        Map<String, String> partitionSpec = new HashMap<>();\n+        GetTableResponse response = MockRESTMessage.getTableResponse();\n+        partitionSpec.put(response.getSchema().primaryKeys().get(0), \""1\"");\n+        mockResponse(mapper.writeValueAsString(\""\""), 200);\n+        mockResponse(\""\"", 404);\n+        assertThrows(\n+                Catalog.TableNotExistException.class,\n+                () ->\n+                        restCatalog.dropPartition(\n+                                Identifier.create(databaseName, \""table\""), partitionSpec));\n+    }\n+\n+    @Test\n+    public void testListPartitionsWhenMetastorePartitionedIsTrue() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        GetTableResponse getTableResponse = MockRESTMessage.getTableResponseEnablePartition();\n+        mockResponse(mapper.writeValueAsString(getTableResponse), 200);\n+        ListPartitionsResponse response = MockRESTMessage.listPartitionsResponse();\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        List<PartitionEntry> result =\n+                restCatalog.listPartitions(Identifier.create(databaseName, \""table\""));\n+        assertEquals(response.getPartitions().size(), result.size());\n+    }\n+\n+    @Test\n+    public void testListPartitionsFromFile() throws Exception {\n+        String databaseName = MockRESTMessage.databaseName();\n+        GetTableResponse response = MockRESTMessage.getTableResponse();\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        List<PartitionEntry> partitionEntries =\n+                restCatalog.listPartitions(Identifier.create(databaseName, \""table\""));\n+        assertEquals(partitionEntries.size(), 0);\n+    }\n+\n     private void mockResponse(String mockResponse, int httpCode) {\n         MockResponse mockResponseObj =\n                 new MockResponse()\n@@ -367,4 +458,24 @@ private void mockResponse(String mockResponse, int httpCode) {\n                         .addHeader(\""Content-Type\"", \""application/json\"");\n         mockWebServer.enqueue(mockResponseObj);\n     }\n+\n+    private void mockConfig(String warehouseStr) {\n+        String mockResponse =\n+                String.format(\n+                        \""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\"", \\\""%s\\\"": \\\""%s\\\""}}\"",\n+                        RESTCatalogInternalOptions.PREFIX.key(),\n+                        \""prefix\"",\n+                        CatalogOptions.WAREHOUSE.key(),\n+                        warehouseStr);\n+        mockResponse(mockResponse, 200);\n+    }\n+\n+    public Options mockInitOptions() {\n+        Options options = new Options();\n+        options.set(RESTCatalogOptions.URI, serverUrl);\n+        String initToken = \""init_token\"";\n+        options.set(RESTCatalogOptions.TOKEN, initToken);\n+        options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n+        return options;\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 26b3db615d11..6712b7b991f3 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -21,7 +21,9 @@\n import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.AlterTableRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.CreatePartitionRequest;\n import org.apache.paimon.rest.requests.CreateTableRequest;\n+import org.apache.paimon.rest.requests.DropPartitionRequest;\n import org.apache.paimon.rest.requests.RenameTableRequest;\n import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n@@ -30,11 +32,14 @@\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.GetTableResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+import org.apache.paimon.rest.responses.ListPartitionsResponse;\n import org.apache.paimon.rest.responses.ListTablesResponse;\n+import org.apache.paimon.rest.responses.PartitionResponse;\n import org.apache.paimon.types.DataField;\n import org.apache.paimon.types.DataTypes;\n import org.apache.paimon.types.IntType;\n \n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n import org.junit.Test;\n@@ -44,6 +49,7 @@\n import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n \n /** Test for {@link RESTObjectMapper}. */\n public class RESTObjectMapperTest {\n@@ -193,4 +199,43 @@ public void alterTableRequestParseTest() throws Exception {\n         AlterTableRequest parseData = mapper.readValue(requestStr, AlterTableRequest.class);\n         assertEquals(parseData.getChanges().size(), parseData.getChanges().size());\n     }\n+\n+    @Test\n+    public void createPartitionRequestParseTest() throws JsonProcessingException {\n+        CreatePartitionRequest request = MockRESTMessage.createPartitionRequest(\""t1\"");\n+        String requestStr = mapper.writeValueAsString(request);\n+        CreatePartitionRequest parseData =\n+                mapper.readValue(requestStr, CreatePartitionRequest.class);\n+        assertEquals(parseData.getIdentifier(), parseData.getIdentifier());\n+        assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n+    }\n+\n+    @Test\n+    public void dropPartitionRequestParseTest() throws JsonProcessingException {\n+        DropPartitionRequest request = MockRESTMessage.dropPartitionRequest();\n+        String requestStr = mapper.writeValueAsString(request);\n+        DropPartitionRequest parseData = mapper.readValue(requestStr, DropPartitionRequest.class);\n+        assertEquals(parseData.getPartitionSpec().size(), parseData.getPartitionSpec().size());\n+    }\n+\n+    @Test\n+    public void listPartitionsResponseParseTest() throws Exception {\n+        ListPartitionsResponse response = MockRESTMessage.listPartitionsResponse();\n+        String responseStr = mapper.writeValueAsString(response);\n+        ListPartitionsResponse parseData =\n+                mapper.readValue(responseStr, ListPartitionsResponse.class);\n+        assertEquals(\n+                response.getPartitions().get(0).getFileCount(),\n+                parseData.getPartitions().get(0).getFileCount());\n+    }\n+\n+    @Test\n+    public void partitionResponseParseTest() throws Exception {\n+        PartitionResponse response = MockRESTMessage.partitionResponse();\n+        assertDoesNotThrow(() -> mapper.writeValueAsString(response));\n+        assertDoesNotThrow(\n+                () ->\n+                        mapper.readValue(\n+                                mapper.writeValueAsString(response), PartitionResponse.class));\n+    }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4700"", ""pr_id"": 4700, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 10, ""non_test_files_count"": 27, ""pr_changed_files"": [""docs/content/flink/sql-alter.md"", ""docs/content/program-api/catalog-api.md"", ""docs/content/spark/sql-alter.md"", ""paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/FileSystemCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/PropertyChange.java"", ""paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcUtils.java"", ""paimon-core/src/main/java/org/apache/paimon/privilege/AllGrantedPrivilegeChecker.java"", ""paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeChecker.java"", ""paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeCheckerImpl.java"", ""paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeType.java"", ""paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegedCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterDatabaseRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/AlterDatabaseResponse.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-flink/paimon-flink-common/pom.xml"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java"", ""paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java"", ""paimon-spark/paimon-spark-common/src/main/java/org/apache/paimon/spark/SparkCatalog.java"", ""paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java"", ""paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java"", ""paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala""], ""base_commit"": ""cdd5bb72f706901f6978a71832e4ee1c78934e08"", ""head_commit"": ""91f3408d862e4f4dce4be4682f41f0f90e027f31"", ""repo_url"": ""https://github.com/apache/paimon/pull/4700"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4700"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-17T07:21:23.000Z"", ""patch"": ""diff --git a/docs/content/flink/sql-alter.md b/docs/content/flink/sql-alter.md\nindex 877995cc631b..6c3186b4af75 100644\n--- a/docs/content/flink/sql-alter.md\n+++ b/docs/content/flink/sql-alter.md\n@@ -227,3 +227,19 @@ The following SQL modifies the watermark strategy to `ts - INTERVAL '2' HOUR`.\n ```sql\n ALTER TABLE my_table MODIFY WATERMARK FOR ts AS ts - INTERVAL '2' HOUR\n ```\n+\n+# ALTER DATABASE\n+\n+The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\n+\n+```sql\n+ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...)\n+```\n+\n+## Altering Database Location\n+\n+The following SQL changes location of database `my_database` to `file:/temp/my_database`.\n+\n+```sql\n+ALTER DATABASE my_database SET ('location' =  'file:/temp/my_database')\n+```\n\ndiff --git a/docs/content/program-api/catalog-api.md b/docs/content/program-api/catalog-api.md\nindex 570577437d86..7e716aad15bb 100644\n--- a/docs/content/program-api/catalog-api.md\n+++ b/docs/content/program-api/catalog-api.md\n@@ -82,7 +82,7 @@ public class ListDatabases {\n \n ## Drop Database\n \n-You can use the catalog to drop databases.\n+You can use the catalog to drop database.\n \n ```java\n import org.apache.paimon.catalog.Catalog;\n@@ -102,6 +102,30 @@ public class DropDatabase {\n }\n ```\n \n+## Alter Database\n+\n+You can use the catalog to alter database's properties.(ps: only support hive and jdbc catalog)\n+\n+```java\n+import java.util.ArrayList;\n+import org.apache.paimon.catalog.Catalog;\n+\n+public class AlterDatabase {\n+\n+    public static void main(String[] args) {\n+        try {\n+            Catalog catalog = CreateCatalog.createHiveCatalog();\n+            List<DatabaseChange> changes = new ArrayList<>();\n+            changes.add(DatabaseChange.setProperty(\""k1\"", \""v1\""));\n+            changes.add(DatabaseChange.removeProperty(\""k2\""));\n+            catalog.alterDatabase(\""my_db\"", changes, true);\n+        } catch (Catalog.DatabaseNotExistException e) {\n+            // do something\n+        }\n+    }\n+}\n+```\n+\n ## Determine Whether Table Exists\n \n You can use the catalog to determine whether the table exists\n\ndiff --git a/docs/content/spark/sql-alter.md b/docs/content/spark/sql-alter.md\nindex 3ad72048029b..359b1187292d 100644\n--- a/docs/content/spark/sql-alter.md\n+++ b/docs/content/spark/sql-alter.md\n@@ -240,3 +240,21 @@ The following SQL changes the type of a nested column `f2` to `BIGINT` in a stru\n -- column v previously has type MAP<INT, STRUCT<f1: STRING, f2: INT>>\n ALTER TABLE my_table ALTER COLUMN v.value.f2 TYPE BIGINT;\n ```\n+\n+\n+# ALTER DATABASE\n+\n+The following SQL sets one or more properties in the specified database. If a particular property is already set in the database, override the old value with the new one.\n+\n+```sql\n+ALTER { DATABASE | SCHEMA | NAMESPACE } my_database\n+    SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] )\n+```\n+\n+## Altering Database Location\n+\n+The following SQL sets the location of the specified database to `file:/temp/my_database.db`.\n+\n+```sql\n+ALTER DATABASE my_database SET LOCATION 'file:/temp/my_database.db'\n+```\n\\ No newline at end of file\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\nindex db6909295556..892e77735b4b 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/AbstractCatalog.java\n@@ -223,6 +223,26 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n \n     protected abstract void dropDatabaseImpl(String name);\n \n+    @Override\n+    public void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException {\n+        checkNotSystemDatabase(name);\n+        try {\n+            if (changes == null || changes.isEmpty()) {\n+                return;\n+            }\n+            alterDatabaseImpl(name, changes);\n+        } catch (DatabaseNotExistException e) {\n+            if (ignoreIfNotExists) {\n+                return;\n+            }\n+            throw new DatabaseNotExistException(name);\n+        }\n+    }\n+\n+    protected abstract void alterDatabaseImpl(String name, List<PropertyChange> changes)\n+            throws DatabaseNotExistException;\n+\n     @Override\n     public List<String> listTables(String databaseName) throws DatabaseNotExistException {\n         if (isSystemDatabase(databaseName)) {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\nindex 82d503b7a272..99540cf0cea5 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\n@@ -187,6 +187,13 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n         }\n     }\n \n+    @Override\n+    public void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException {\n+        super.alterDatabase(name, changes, ignoreIfNotExists);\n+        databaseCache.invalidate(name);\n+    }\n+\n     @Override\n     public void dropTable(Identifier identifier, boolean ignoreIfNotExists)\n             throws TableNotExistException {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\nindex 7b1fe0ea072e..37ea6fa5e203 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/Catalog.java\n@@ -126,6 +126,19 @@ void createDatabase(String name, boolean ignoreIfExists, Map<String, String> pro\n     void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n             throws DatabaseNotExistException, DatabaseNotEmptyException;\n \n+    /**\n+     * Alter a database.\n+     *\n+     * @param name Name of the database to alter.\n+     * @param changes the property changes\n+     * @param ignoreIfNotExists Flag to specify behavior when the database does not exist: if set to\n+     *     false, throw an exception, if set to true, do nothing.\n+     * @throws DatabaseNotExistException if the given database is not exist and ignoreIfNotExists is\n+     *     false\n+     */\n+    void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException;\n+\n     /**\n      * Return a {@link Table} identified by the given {@link Identifier}.\n      *\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\nindex 93e8ce2581ad..968f00cfcae5 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/DelegateCatalog.java\n@@ -83,6 +83,12 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n         wrapped.dropDatabase(name, ignoreIfNotExists, cascade);\n     }\n \n+    @Override\n+    public void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException {\n+        wrapped.alterDatabase(name, changes, ignoreIfNotExists);\n+    }\n+\n     @Override\n     public List<String> listTables(String databaseName) throws DatabaseNotExistException {\n         return wrapped.listTables(databaseName);\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/FileSystemCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/FileSystemCatalog.java\nindex 279ddb26ee53..cb0c358259f8 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/FileSystemCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/FileSystemCatalog.java\n@@ -92,6 +92,11 @@ protected void dropDatabaseImpl(String name) {\n         uncheck(() -> fileIO.delete(newDatabasePath(name), true));\n     }\n \n+    @Override\n+    protected void alterDatabaseImpl(String name, List<PropertyChange> changes) {\n+        throw new UnsupportedOperationException(\""Alter database is not supported.\"");\n+    }\n+\n     @Override\n     protected List<String> listTablesImpl(String databaseName) {\n         return uncheck(() -> listTablesInFileSystem(newDatabasePath(databaseName)));\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/PropertyChange.java b/paimon-core/src/main/java/org/apache/paimon/catalog/PropertyChange.java\nnew file mode 100644\nindex 000000000000..c3423efd081e\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/PropertyChange.java\n@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.catalog;\n+\n+import org.apache.paimon.utils.Pair;\n+\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Sets;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/** define change to the database property. */\n+public interface PropertyChange {\n+\n+    static PropertyChange setProperty(String property, String value) {\n+        return new SetProperty(property, value);\n+    }\n+\n+    static PropertyChange removeProperty(String property) {\n+        return new RemoveProperty(property);\n+    }\n+\n+    static Pair<Map<String, String>, Set<String>> getSetPropertiesToRemoveKeys(\n+            List<PropertyChange> changes) {\n+        Map<String, String> setProperties = Maps.newHashMap();\n+        Set<String> removeKeys = Sets.newHashSet();\n+        changes.forEach(\n+                change -> {\n+                    if (change instanceof PropertyChange.SetProperty) {\n+                        PropertyChange.SetProperty setProperty =\n+                                (PropertyChange.SetProperty) change;\n+                        setProperties.put(setProperty.property(), setProperty.value());\n+                    } else {\n+                        removeKeys.add(((PropertyChange.RemoveProperty) change).property());\n+                    }\n+                });\n+        return Pair.of(setProperties, removeKeys);\n+    }\n+\n+    /** Set property for database change. */\n+    final class SetProperty implements PropertyChange {\n+\n+        private final String property;\n+        private final String value;\n+\n+        private SetProperty(String property, String value) {\n+            this.property = property;\n+            this.value = value;\n+        }\n+\n+        public String property() {\n+            return this.property;\n+        }\n+\n+        public String value() {\n+            return this.value;\n+        }\n+    }\n+\n+    /** Remove property for database change. */\n+    final class RemoveProperty implements PropertyChange {\n+\n+        private final String property;\n+\n+        private RemoveProperty(String property) {\n+            this.property = property;\n+        }\n+\n+        public String property() {\n+            return this.property;\n+        }\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcCatalog.java b/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcCatalog.java\nindex 551b2d8fc910..63cb54c180f5 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcCatalog.java\n@@ -24,6 +24,7 @@\n import org.apache.paimon.catalog.CatalogLockFactory;\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.operation.Lock;\n@@ -33,11 +34,13 @@\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.schema.SchemaManager;\n import org.apache.paimon.schema.TableSchema;\n+import org.apache.paimon.utils.Pair;\n import org.apache.paimon.utils.Preconditions;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Sets;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -52,12 +55,15 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n+import java.util.Set;\n import java.util.stream.Collectors;\n \n import static org.apache.paimon.jdbc.JdbcCatalogLock.acquireTimeout;\n import static org.apache.paimon.jdbc.JdbcCatalogLock.checkMaxSleep;\n+import static org.apache.paimon.jdbc.JdbcUtils.deleteProperties;\n import static org.apache.paimon.jdbc.JdbcUtils.execute;\n import static org.apache.paimon.jdbc.JdbcUtils.insertProperties;\n+import static org.apache.paimon.jdbc.JdbcUtils.updateProperties;\n import static org.apache.paimon.jdbc.JdbcUtils.updateTable;\n \n /* This file is based on source code from the Iceberg Project (http://iceberg.apache.org/), licensed by the Apache\n@@ -197,6 +203,45 @@ protected void dropDatabaseImpl(String name) {\n         execute(connections, JdbcUtils.DELETE_ALL_DATABASE_PROPERTIES_SQL, catalogKey, name);\n     }\n \n+    @Override\n+    protected void alterDatabaseImpl(String name, List<PropertyChange> changes) {\n+        Pair<Map<String, String>, Set<String>> setPropertiesToRemoveKeys =\n+                PropertyChange.getSetPropertiesToRemoveKeys(changes);\n+        Map<String, String> setProperties = setPropertiesToRemoveKeys.getLeft();\n+        Set<String> removeKeys = setPropertiesToRemoveKeys.getRight();\n+        Map<String, String> startingProperties = fetchProperties(name);\n+        Map<String, String> inserts = Maps.newHashMap();\n+        Map<String, String> updates = Maps.newHashMap();\n+        Set<String> removes = Sets.newHashSet();\n+        if (!setProperties.isEmpty()) {\n+            setProperties.forEach(\n+                    (k, v) -> {\n+                        if (!startingProperties.containsKey(k)) {\n+                            inserts.put(k, v);\n+                        } else {\n+                            updates.put(k, v);\n+                        }\n+                    });\n+        }\n+        if (!removeKeys.isEmpty()) {\n+            removeKeys.forEach(\n+                    k -> {\n+                        if (startingProperties.containsKey(k)) {\n+                            removes.add(k);\n+                        }\n+                    });\n+        }\n+        if (!inserts.isEmpty()) {\n+            insertProperties(connections, catalogKey, name, inserts);\n+        }\n+        if (!updates.isEmpty()) {\n+            updateProperties(connections, catalogKey, name, updates);\n+        }\n+        if (!removes.isEmpty()) {\n+            deleteProperties(connections, catalogKey, name, removes);\n+        }\n+    }\n+\n     @Override\n     protected List<String> listTablesImpl(String databaseName) {\n         return fetch(\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcUtils.java b/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcUtils.java\nindex 4acb0f25aa91..1b9e599d72bc 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcUtils.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/jdbc/JdbcUtils.java\n@@ -30,8 +30,10 @@\n import java.sql.ResultSet;\n import java.sql.SQLException;\n import java.sql.SQLIntegrityConstraintViolationException;\n+import java.util.Collections;\n import java.util.Map;\n import java.util.Properties;\n+import java.util.Set;\n import java.util.function.Consumer;\n import java.util.stream.Stream;\n \n@@ -202,6 +204,16 @@ public class JdbcUtils {\n                     + \"" = ? AND \""\n                     + DATABASE_NAME\n                     + \"" = ? \"";\n+    static final String DELETE_DATABASE_PROPERTIES_SQL =\n+            \""DELETE FROM \""\n+                    + DATABASE_PROPERTIES_TABLE_NAME\n+                    + \"" WHERE \""\n+                    + CATALOG_KEY\n+                    + \"" = ? AND \""\n+                    + DATABASE_NAME\n+                    + \"" = ? AND \""\n+                    + DATABASE_PROPERTY_KEY\n+                    + \"" IN \"";\n     static final String DELETE_ALL_DATABASE_PROPERTIES_SQL =\n             \""DELETE FROM \""\n                     + DATABASE_PROPERTIES_TABLE_NAME\n@@ -403,6 +415,75 @@ private static String insertPropertiesStatement(int size) {\n         return sqlStatement.toString();\n     }\n \n+    public static boolean updateProperties(\n+            JdbcClientPool connections,\n+            String storeKey,\n+            String databaseName,\n+            Map<String, String> properties) {\n+        Stream<String> caseArgs =\n+                properties.entrySet().stream()\n+                        .flatMap(entry -> Stream.of(entry.getKey(), entry.getValue()));\n+        Stream<String> whereArgs =\n+                Stream.concat(Stream.of(storeKey, databaseName), properties.keySet().stream());\n+\n+        String[] args = Stream.concat(caseArgs, whereArgs).toArray(String[]::new);\n+\n+        int updatedRecords =\n+                execute(connections, JdbcUtils.updatePropertiesStatement(properties.size()), args);\n+        if (updatedRecords == properties.size()) {\n+            return true;\n+        }\n+        throw new IllegalStateException(\n+                String.format(\n+                        \""Failed to update: %d of %d succeeded\"", updatedRecords, properties.size()));\n+    }\n+\n+    private static String updatePropertiesStatement(int size) {\n+        StringBuilder sqlStatement =\n+                new StringBuilder(\n+                        \""UPDATE \""\n+                                + DATABASE_PROPERTIES_TABLE_NAME\n+                                + \"" SET \""\n+                                + DATABASE_PROPERTY_VALUE\n+                                + \"" = CASE\"");\n+        for (int i = 0; i < size; i += 1) {\n+            sqlStatement.append(\"" WHEN \"" + DATABASE_PROPERTY_KEY + \"" = ? THEN ?\"");\n+        }\n+\n+        sqlStatement.append(\n+                \"" END WHERE \""\n+                        + CATALOG_KEY\n+                        + \"" = ? AND \""\n+                        + DATABASE_NAME\n+                        + \"" = ? AND \""\n+                        + DATABASE_PROPERTY_KEY\n+                        + \"" IN \"");\n+\n+        String values = String.join(\"",\"", Collections.nCopies(size, String.valueOf('?')));\n+        sqlStatement.append(\""(\"").append(values).append(\"")\"");\n+\n+        return sqlStatement.toString();\n+    }\n+\n+    public static boolean deleteProperties(\n+            JdbcClientPool connections,\n+            String storeKey,\n+            String databaseName,\n+            Set<String> removeKeys) {\n+        String[] args =\n+                Stream.concat(Stream.of(storeKey, databaseName), removeKeys.stream())\n+                        .toArray(String[]::new);\n+\n+        int deleteRecords =\n+                execute(connections, JdbcUtils.deletePropertiesStatement(removeKeys), args);\n+        if (deleteRecords > 0) {\n+            return true;\n+        }\n+        throw new IllegalStateException(\n+                String.format(\n+                        \""Failed to delete: %d of %d succeeded\"", deleteRecords, removeKeys.size()));\n+    }\n+\n     public static void createDistributedLockTable(JdbcClientPool connections, Options options)\n             throws SQLException, InterruptedException {\n         DistributedLockDialectFactory.create(connections.getProtocol())\n@@ -427,4 +508,13 @@ public static void release(JdbcClientPool connections, String lockId)\n         DistributedLockDialectFactory.create(connections.getProtocol())\n                 .releaseLock(connections, lockId);\n     }\n+\n+    private static String deletePropertiesStatement(Set<String> properties) {\n+        StringBuilder sqlStatement = new StringBuilder(JdbcUtils.DELETE_DATABASE_PROPERTIES_SQL);\n+        String values =\n+                String.join(\"",\"", Collections.nCopies(properties.size(), String.valueOf('?')));\n+        sqlStatement.append(\""(\"").append(values).append(\"")\"");\n+\n+        return sqlStatement.toString();\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/privilege/AllGrantedPrivilegeChecker.java b/paimon-core/src/main/java/org/apache/paimon/privilege/AllGrantedPrivilegeChecker.java\nindex 09944681a2e7..8e8e4cd53d04 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/privilege/AllGrantedPrivilegeChecker.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/privilege/AllGrantedPrivilegeChecker.java\n@@ -41,6 +41,9 @@ public void assertCanCreateTable(String databaseName) {}\n     @Override\n     public void assertCanDropDatabase(String databaseName) {}\n \n+    @Override\n+    public void assertCanAlterDatabase(String databaseName) {}\n+\n     @Override\n     public void assertCanCreateDatabase() {}\n \n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeChecker.java b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeChecker.java\nindex 1771d40f4028..14cbbc6f36a3 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeChecker.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeChecker.java\n@@ -53,6 +53,8 @@ default void assertCanSelectOrInsert(Identifier identifier) {\n \n     void assertCanDropDatabase(String databaseName);\n \n+    void assertCanAlterDatabase(String databaseName);\n+\n     void assertCanCreateDatabase();\n \n     void assertCanCreateUser();\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeCheckerImpl.java b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeCheckerImpl.java\nindex 19c1813ee852..7e7876fa4e44 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeCheckerImpl.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeCheckerImpl.java\n@@ -85,6 +85,14 @@ public void assertCanDropDatabase(String databaseName) {\n         }\n     }\n \n+    @Override\n+    public void assertCanAlterDatabase(String databaseName) {\n+        if (!check(databaseName, PrivilegeType.ALTER_DATABASE)) {\n+            throw new NoPrivilegeException(\n+                    user, \""database\"", databaseName, PrivilegeType.ALTER_DATABASE);\n+        }\n+    }\n+\n     @Override\n     public void assertCanCreateDatabase() {\n         if (!check(\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeType.java b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeType.java\nindex 375f5030d557..00b3a50596cb 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeType.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegeType.java\n@@ -34,6 +34,7 @@ public enum PrivilegeType {\n \n     CREATE_TABLE(PrivilegeTarget.DATABASE),\n     DROP_DATABASE(PrivilegeTarget.DATABASE),\n+    ALTER_DATABASE(PrivilegeTarget.DATABASE),\n \n     CREATE_DATABASE(PrivilegeTarget.CATALOG),\n     // you can create and drop users, grant and revoke any privileges to or from others\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegedCatalog.java b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegedCatalog.java\nindex 2e88213a24b9..35822471a2d6 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegedCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/privilege/PrivilegedCatalog.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.DelegateCatalog;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.options.ConfigOption;\n import org.apache.paimon.options.ConfigOptions;\n import org.apache.paimon.options.Options;\n@@ -82,6 +83,13 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n         privilegeManager.objectDropped(name);\n     }\n \n+    @Override\n+    public void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException {\n+        privilegeManager.getPrivilegeChecker().assertCanAlterDatabase(name);\n+        super.alterDatabase(name, changes, ignoreIfNotExists);\n+    }\n+\n     @Override\n     public void dropTable(Identifier identifier, boolean ignoreIfNotExists)\n             throws TableNotExistException {\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex c30e1109e2ec..8b53bef8486b 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.fs.FileIO;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.CatalogOptions;\n@@ -30,7 +31,9 @@\n import org.apache.paimon.rest.auth.CredentialsProviderFactory;\n import org.apache.paimon.rest.exceptions.AlreadyExistsException;\n import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n+import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.DatabaseName;\n@@ -39,6 +42,7 @@\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.table.Table;\n+import org.apache.paimon.utils.Pair;\n \n import org.apache.paimon.shade.guava30.com.google.common.annotations.VisibleForTesting;\n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n@@ -49,6 +53,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n+import java.util.Set;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.stream.Collectors;\n \n@@ -136,12 +141,14 @@ public List<String> listDatabases() {\n     @Override\n     public void createDatabase(String name, boolean ignoreIfExists, Map<String, String> properties)\n             throws DatabaseAlreadyExistException {\n-        CreateDatabaseRequest request = new CreateDatabaseRequest(name, ignoreIfExists, properties);\n+        CreateDatabaseRequest request = new CreateDatabaseRequest(name, properties);\n         try {\n             client.post(\n                     resourcePaths.databases(), request, CreateDatabaseResponse.class, headers());\n         } catch (AlreadyExistsException e) {\n-            throw new DatabaseAlreadyExistException(name);\n+            if (!ignoreIfExists) {\n+                throw new DatabaseAlreadyExistException(name);\n+            }\n         }\n     }\n \n@@ -172,6 +179,32 @@ public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade\n         }\n     }\n \n+    @Override\n+    public void alterDatabase(String name, List<PropertyChange> changes, boolean ignoreIfNotExists)\n+            throws DatabaseNotExistException {\n+        try {\n+            Pair<Map<String, String>, Set<String>> setPropertiesToRemoveKeys =\n+                    PropertyChange.getSetPropertiesToRemoveKeys(changes);\n+            Map<String, String> updateProperties = setPropertiesToRemoveKeys.getLeft();\n+            Set<String> removeKeys = setPropertiesToRemoveKeys.getRight();\n+            AlterDatabaseRequest request =\n+                    new AlterDatabaseRequest(new ArrayList<>(removeKeys), updateProperties);\n+            AlterDatabaseResponse response =\n+                    client.post(\n+                            resourcePaths.databaseProperties(name),\n+                            request,\n+                            AlterDatabaseResponse.class,\n+                            headers());\n+            if (response.getUpdated().isEmpty()) {\n+                throw new IllegalStateException(\""Failed to update properties\"");\n+            }\n+        } catch (NoSuchResourceException e) {\n+            if (!ignoreIfNotExists) {\n+                throw new DatabaseNotExistException(name);\n+            }\n+        }\n+    }\n+\n     @Override\n     public Table getTable(Identifier identifier) throws TableNotExistException {\n         throw new UnsupportedOperationException();\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex b58053374daa..51277454ffb0 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -43,4 +43,13 @@ public String databases() {\n     public String database(String databaseName) {\n         return SLASH.add(\""v1\"").add(prefix).add(\""databases\"").add(databaseName).toString();\n     }\n+\n+    public String databaseProperties(String databaseName) {\n+        return SLASH.add(\""v1\"")\n+                .add(prefix)\n+                .add(\""databases\"")\n+                .add(databaseName)\n+                .add(\""properties\"")\n+                .toString();\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterDatabaseRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterDatabaseRequest.java\nnew file mode 100644\nindex 000000000000..c1330142bb7e\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/AlterDatabaseRequest.java\n@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/** Request for altering database. */\n+public class AlterDatabaseRequest implements RESTRequest {\n+\n+    private static final String FIELD_REMOVALS = \""removals\"";\n+    private static final String FIELD_UPDATES = \""updates\"";\n+\n+    @JsonProperty(FIELD_REMOVALS)\n+    private List<String> removals;\n+\n+    @JsonProperty(FIELD_UPDATES)\n+    private Map<String, String> updates;\n+\n+    @JsonCreator\n+    public AlterDatabaseRequest(\n+            @JsonProperty(FIELD_REMOVALS) List<String> removals,\n+            @JsonProperty(FIELD_UPDATES) Map<String, String> updates) {\n+        this.removals = removals;\n+        this.updates = updates;\n+    }\n+\n+    @JsonGetter(FIELD_REMOVALS)\n+    public List<String> getRemovals() {\n+        return removals;\n+    }\n+\n+    @JsonGetter(FIELD_UPDATES)\n+    public Map<String, String> getUpdates() {\n+        return updates;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java\nindex 6067bf544b87..07e5cf2462f2 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java\n@@ -30,25 +30,19 @@\n public class CreateDatabaseRequest implements RESTRequest {\n \n     private static final String FIELD_NAME = \""name\"";\n-    private static final String FIELD_IGNORE_IF_EXISTS = \""ignoreIfExists\"";\n     private static final String FIELD_OPTIONS = \""options\"";\n \n     @JsonProperty(FIELD_NAME)\n     private String name;\n \n-    @JsonProperty(FIELD_IGNORE_IF_EXISTS)\n-    private boolean ignoreIfExists;\n-\n     @JsonProperty(FIELD_OPTIONS)\n     private Map<String, String> options;\n \n     @JsonCreator\n     public CreateDatabaseRequest(\n             @JsonProperty(FIELD_NAME) String name,\n-            @JsonProperty(FIELD_IGNORE_IF_EXISTS) boolean ignoreIfExists,\n             @JsonProperty(FIELD_OPTIONS) Map<String, String> options) {\n         this.name = name;\n-        this.ignoreIfExists = ignoreIfExists;\n         this.options = options;\n     }\n \n@@ -57,11 +51,6 @@ public String getName() {\n         return name;\n     }\n \n-    @JsonGetter(FIELD_IGNORE_IF_EXISTS)\n-    public boolean getIgnoreIfExists() {\n-        return ignoreIfExists;\n-    }\n-\n     @JsonGetter(FIELD_OPTIONS)\n     public Map<String, String> getOptions() {\n         return options;\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/AlterDatabaseResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/AlterDatabaseResponse.java\nnew file mode 100644\nindex 000000000000..08d751dc595c\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/AlterDatabaseResponse.java\n@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Response for altering database. */\n+public class AlterDatabaseResponse implements RESTResponse {\n+\n+    private static final String FIELD_REMOVED = \""removed\"";\n+    private static final String FIELD_UPDATED = \""updated\"";\n+    private static final String FIELD_MISSING = \""missing\"";\n+\n+    @JsonProperty(FIELD_REMOVED)\n+    private List<String> removed;\n+\n+    @JsonProperty(FIELD_UPDATED)\n+    private List<String> updated;\n+\n+    @JsonProperty(FIELD_MISSING)\n+    private List<String> missing;\n+\n+    @JsonCreator\n+    public AlterDatabaseResponse(\n+            @JsonProperty(FIELD_REMOVED) List<String> removed,\n+            @JsonProperty(FIELD_UPDATED) List<String> updated,\n+            @JsonProperty(FIELD_MISSING) List<String> missing) {\n+        this.removed = removed;\n+        this.updated = updated;\n+        this.missing = missing;\n+    }\n+\n+    @JsonGetter(FIELD_REMOVED)\n+    public List<String> getRemoved() {\n+        return removed;\n+    }\n+\n+    @JsonGetter(FIELD_UPDATED)\n+    public List<String> getUpdated() {\n+        return updated;\n+    }\n+\n+    @JsonGetter(FIELD_MISSING)\n+    public List<String> getMissing() {\n+        return missing;\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/pom.xml b/paimon-flink/paimon-flink-common/pom.xml\nindex 91222983bf6b..7388f8944109 100644\n--- a/paimon-flink/paimon-flink-common/pom.xml\n+++ b/paimon-flink/paimon-flink-common/pom.xml\n@@ -169,8 +169,17 @@ under the License.\n                 </exclusion>\n             </exclusions>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>org.mockito</groupId>\n+            <artifactId>mockito-inline</artifactId>\n+            <version>${mockito.version}</version>\n+            <type>jar</type>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n \n+\n     <build>\n         <plugins>\n             <plugin>\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\nindex 3407735b4b79..ec3c4a47a69d 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n@@ -20,9 +20,12 @@\n \n import org.apache.paimon.CoreOptions;\n import org.apache.paimon.TableType;\n+import org.apache.paimon.annotation.VisibleForTesting;\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogUtils;\n+import org.apache.paimon.catalog.Database;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.flink.procedure.ProcedureUtil;\n import org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil;\n import org.apache.paimon.flink.utils.FlinkDescriptorProperties;\n@@ -133,6 +136,7 @@\n import static org.apache.paimon.CoreOptions.MATERIALIZED_TABLE_REFRESH_MODE;\n import static org.apache.paimon.CoreOptions.MATERIALIZED_TABLE_REFRESH_STATUS;\n import static org.apache.paimon.CoreOptions.PATH;\n+import static org.apache.paimon.catalog.Catalog.COMMENT_PROP;\n import static org.apache.paimon.catalog.Catalog.LAST_UPDATE_TIME_PROP;\n import static org.apache.paimon.catalog.Catalog.NUM_FILES_PROP;\n import static org.apache.paimon.catalog.Catalog.NUM_ROWS_PROP;\n@@ -236,19 +240,19 @@ public CatalogDatabase getDatabase(String databaseName)\n     @Override\n     public void createDatabase(String name, CatalogDatabase database, boolean ignoreIfExists)\n             throws DatabaseAlreadyExistException, CatalogException {\n+        Map<String, String> properties;\n         if (database != null) {\n+            properties = new HashMap<>(database.getProperties());\n             if (database.getDescription().isPresent()\n                     && !database.getDescription().get().equals(\""\"")) {\n-                throw new UnsupportedOperationException(\n-                        \""Create database with description is unsupported.\"");\n+                properties.put(COMMENT_PROP, database.getDescription().get());\n             }\n+        } else {\n+            properties = Collections.emptyMap();\n         }\n \n         try {\n-            catalog.createDatabase(\n-                    name,\n-                    ignoreIfExists,\n-                    database == null ? Collections.emptyMap() : database.getProperties());\n+            catalog.createDatabase(name, ignoreIfExists, properties);\n         } catch (Catalog.DatabaseAlreadyExistException e) {\n             throw new DatabaseAlreadyExistException(getName(), e.database());\n         }\n@@ -620,7 +624,7 @@ private List<SchemaChange> toSchemaChange(\n \n             SchemaManager.checkAlterTablePath(key);\n \n-            if (Catalog.COMMENT_PROP.equals(key)) {\n+            if (COMMENT_PROP.equals(key)) {\n                 schemaChanges.add(SchemaChange.updateComment(value));\n             } else {\n                 schemaChanges.add(SchemaChange.setOption(key, value));\n@@ -629,7 +633,7 @@ private List<SchemaChange> toSchemaChange(\n         } else if (change instanceof ResetOption) {\n             ResetOption resetOption = (ResetOption) change;\n             String key = resetOption.getKey();\n-            if (Catalog.COMMENT_PROP.equals(key)) {\n+            if (COMMENT_PROP.equals(key)) {\n                 schemaChanges.add(SchemaChange.updateComment(null));\n             } else {\n                 schemaChanges.add(SchemaChange.removeOption(resetOption.getKey()));\n@@ -1209,13 +1213,22 @@ public static Identifier toIdentifier(ObjectPath path) {\n         return new Identifier(path.getDatabaseName(), path.getObjectName());\n     }\n \n-    // --------------------- unsupported methods ----------------------------\n-\n     @Override\n     public final void alterDatabase(\n             String name, CatalogDatabase newDatabase, boolean ignoreIfNotExists)\n-            throws CatalogException {\n-        throw new UnsupportedOperationException();\n+            throws CatalogException, DatabaseNotExistException {\n+        try {\n+            Database oldDatabase = catalog.getDatabase(name);\n+            List<PropertyChange> changes =\n+                    getPropertyChanges(oldDatabase.options(), newDatabase.getProperties());\n+            getPropertyChangeFromComment(oldDatabase.comment(), newDatabase.getDescription())\n+                    .ifPresent(changes::add);\n+            catalog.alterDatabase(name, changes, ignoreIfNotExists);\n+        } catch (Catalog.DatabaseNotExistException e) {\n+            if (!ignoreIfNotExists) {\n+                throw new DatabaseNotExistException(getName(), e.database());\n+            }\n+        }\n     }\n \n     @Override\n@@ -1264,6 +1277,36 @@ public final List<CatalogPartitionSpec> listPartitions(ObjectPath tablePath)\n         return getPartitionSpecs(tablePath, null);\n     }\n \n+    @VisibleForTesting\n+    static List<PropertyChange> getPropertyChanges(\n+            Map<String, String> oldOptions, Map<String, String> newOptions) {\n+        List<PropertyChange> changes = new ArrayList<>();\n+        newOptions.forEach(\n+                (k, v) -> {\n+                    if (!oldOptions.containsKey(k) || !oldOptions.get(k).equals(v)) {\n+                        changes.add(PropertyChange.setProperty(k, v));\n+                    }\n+                });\n+        oldOptions\n+                .keySet()\n+                .forEach(\n+                        (k) -> {\n+                            if (!newOptions.containsKey(k)) {\n+                                changes.add(PropertyChange.removeProperty(k));\n+                            }\n+                        });\n+        return changes;\n+    }\n+\n+    @VisibleForTesting\n+    static Optional<PropertyChange> getPropertyChangeFromComment(\n+            Optional<String> oldComment, Optional<String> newComment) {\n+        if (newComment.isPresent() && !oldComment.equals(newComment)) {\n+            return Optional.of(PropertyChange.setProperty(COMMENT_PROP, newComment.get()));\n+        }\n+        return Optional.empty();\n+    }\n+\n     private Table getPaimonTable(ObjectPath tablePath) throws TableNotExistException {\n         try {\n             Identifier identifier = toIdentifier(tablePath);\n\ndiff --git a/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java b/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\nindex 5744ac894d12..237b59e43c4d 100644\n--- a/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\n+++ b/paimon-hive/paimon-hive-catalog/src/main/java/org/apache/paimon/hive/HiveCatalog.java\n@@ -26,6 +26,7 @@\n import org.apache.paimon.catalog.CatalogLockContext;\n import org.apache.paimon.catalog.CatalogLockFactory;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.client.ClientPool;\n import org.apache.paimon.data.BinaryRow;\n import org.apache.paimon.fs.FileIO;\n@@ -53,6 +54,8 @@\n import org.apache.paimon.view.View;\n import org.apache.paimon.view.ViewImpl;\n \n+import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n+\n import org.apache.flink.table.hive.LegacyHiveClasses;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -405,6 +408,33 @@ protected void dropDatabaseImpl(String name) {\n         }\n     }\n \n+    @Override\n+    protected void alterDatabaseImpl(String name, List<PropertyChange> changes) {\n+        try {\n+            Database database = clients.run(client -> client.getDatabase(name));\n+            Map<String, String> parameter = Maps.newHashMap();\n+            parameter.putAll(database.getParameters());\n+            Pair<Map<String, String>, Set<String>> setPropertiesToRemoveKeys =\n+                    PropertyChange.getSetPropertiesToRemoveKeys(changes);\n+            Map<String, String> setProperties = setPropertiesToRemoveKeys.getLeft();\n+            Set<String> removeKeys = setPropertiesToRemoveKeys.getRight();\n+            if (setProperties.size() > 0) {\n+                parameter.putAll(setProperties);\n+            }\n+            if (removeKeys.size() > 0) {\n+                parameter.keySet().removeAll(removeKeys);\n+            }\n+            Map<String, String> newProperties = Collections.unmodifiableMap(parameter);\n+            Database alterDatabase = convertToHiveDatabase(name, newProperties);\n+            clients.execute(client -> client.alterDatabase(name, alterDatabase));\n+        } catch (TException e) {\n+            throw new RuntimeException(\""Failed to alter database \"" + name, e);\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new RuntimeException(\""Interrupted in call to alterDatabase \"" + name, e);\n+        }\n+    }\n+\n     @Override\n     protected List<String> listTablesImpl(String databaseName) {\n         try {\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 9b69b3de2776..f7f9529f53dd 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -80,6 +80,43 @@ paths:\n                 $ref: '#/components/schemas/ErrorResponse'\n         \""500\"":\n           description: Internal Server Error\n+  /v1/{prefix}/databases/{database}/properties:\n+    post:\n+      tags:\n+        - database\n+      summary: Alter Database\n+      operationId: alterDatabase\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/AlterDatabaseRequest'\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/AlterDatabaseResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n   /v1/{prefix}/databases/{database}:\n     get:\n       tags:\n@@ -116,7 +153,7 @@ paths:\n       tags:\n         - database\n       summary: Drop Database\n-      operationId: dropDatabases\n+      operationId: dropDatabase\n       parameters:\n         - name: prefix\n           in: path\n@@ -159,12 +196,22 @@ components:\n       properties:\n         name:\n           type: string\n-        ignoreIfExists:\n-          type: boolean\n         options:\n           type: object\n           additionalProperties:\n             type: string\n+    ErrorResponse:\n+      type: object\n+      properties:\n+        message:\n+          type: string\n+        code:\n+          type: integer\n+          format: int32\n+        stack:\n+          type: array\n+          items:\n+            type: string\n     CreateDatabaseResponse:\n       type: object\n       properties:\n@@ -174,15 +221,29 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n-    ErrorResponse:\n+    AlterDatabaseRequest:\n       type: object\n       properties:\n-        message:\n-          type: string\n-        code:\n-          type: integer\n-          format: int32\n-        stack:\n+        removals:\n+          type: array\n+          items:\n+            type: string\n+        updates:\n+          type: object\n+          additionalProperties:\n+            type: string\n+    AlterDatabaseResponse:\n+      type: object\n+      properties:\n+        removed:\n+          type: array\n+          items:\n+            type: string\n+        updated:\n+          type: array\n+          items:\n+            type: string\n+        missing:\n           type: array\n           items:\n             type: string\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex 19f6f8cdf673..5331b65d71b6 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -19,7 +19,9 @@\n package org.apache.paimon.open.api;\n \n import org.apache.paimon.rest.ResourcePaths;\n+import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.DatabaseName;\n@@ -28,6 +30,7 @@\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n \n import io.swagger.v3.oas.annotations.Operation;\n import io.swagger.v3.oas.annotations.media.Content;\n@@ -149,5 +152,33 @@ public GetDatabaseResponse getDatabases(\n                 content = {@Content(schema = @Schema())})\n     })\n     @DeleteMapping(\""/v1/{prefix}/databases/{database}\"")\n-    public void dropDatabases(@PathVariable String prefix, @PathVariable String database) {}\n+    public void dropDatabase(@PathVariable String prefix, @PathVariable String database) {}\n+\n+    @Operation(\n+            summary = \""Alter Database\"",\n+            tags = {\""database\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(schema = @Schema(implementation = AlterDatabaseResponse.class))\n+                }),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/v1/{prefix}/databases/{database}/properties\"")\n+    public AlterDatabaseResponse alterDatabase(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @RequestBody AlterDatabaseRequest request) {\n+        return new AlterDatabaseResponse(\n+                Lists.newArrayList(\""remove\""),\n+                Lists.newArrayList(\""add\""),\n+                Lists.newArrayList(\""missing\""));\n+    }\n }\n\ndiff --git a/paimon-spark/paimon-spark-common/src/main/java/org/apache/paimon/spark/SparkCatalog.java b/paimon-spark/paimon-spark-common/src/main/java/org/apache/paimon/spark/SparkCatalog.java\nindex de6e2414fc8f..12023cb84779 100644\n--- a/paimon-spark/paimon-spark-common/src/main/java/org/apache/paimon/spark/SparkCatalog.java\n+++ b/paimon-spark/paimon-spark-common/src/main/java/org/apache/paimon/spark/SparkCatalog.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.catalog.CatalogContext;\n import org.apache.paimon.catalog.CatalogFactory;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n@@ -130,7 +131,8 @@ public void createNamespace(String[] namespace, Map<String, String> metadata)\n             throws NamespaceAlreadyExistsException {\n         checkNamespace(namespace);\n         try {\n-            catalog.createDatabase(namespace[0], false, metadata);\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            catalog.createDatabase(databaseName, false, metadata);\n         } catch (Catalog.DatabaseAlreadyExistException e) {\n             throw new NamespaceAlreadyExistsException(namespace);\n         }\n@@ -153,7 +155,8 @@ public String[][] listNamespaces(String[] namespace) throws NoSuchNamespaceExcep\n         }\n         checkNamespace(namespace);\n         try {\n-            catalog.getDatabase(namespace[0]);\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            catalog.getDatabase(databaseName);\n             return new String[0][];\n         } catch (Catalog.DatabaseNotExistException e) {\n             throw new NoSuchNamespaceException(namespace);\n@@ -164,9 +167,9 @@ public String[][] listNamespaces(String[] namespace) throws NoSuchNamespaceExcep\n     public Map<String, String> loadNamespaceMetadata(String[] namespace)\n             throws NoSuchNamespaceException {\n         checkNamespace(namespace);\n-        String dataBaseName = namespace[0];\n         try {\n-            return catalog.getDatabase(dataBaseName).options();\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            return catalog.getDatabase(databaseName).options();\n         } catch (Catalog.DatabaseNotExistException e) {\n             throw new NoSuchNamespaceException(namespace);\n         }\n@@ -203,7 +206,8 @@ public boolean dropNamespace(String[] namespace, boolean cascade)\n             throws NoSuchNamespaceException {\n         checkNamespace(namespace);\n         try {\n-            catalog.dropDatabase(namespace[0], false, cascade);\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            catalog.dropDatabase(databaseName, false, cascade);\n             return true;\n         } catch (Catalog.DatabaseNotExistException e) {\n             throw new NoSuchNamespaceException(namespace);\n@@ -217,7 +221,8 @@ public boolean dropNamespace(String[] namespace, boolean cascade)\n     public Identifier[] listTables(String[] namespace) throws NoSuchNamespaceException {\n         checkNamespace(namespace);\n         try {\n-            return catalog.listTables(namespace[0]).stream()\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            return catalog.listTables(databaseName).stream()\n                     .map(table -> Identifier.of(namespace, table))\n                     .toArray(Identifier[]::new);\n         } catch (Catalog.DatabaseNotExistException e) {\n@@ -516,10 +521,35 @@ protected List<String> convertPartitionTransforms(Transform[] transforms) {\n         return partitionColNames;\n     }\n \n-    // --------------------- unsupported methods ----------------------------\n-\n     @Override\n-    public void alterNamespace(String[] namespace, NamespaceChange... changes) {\n-        throw new UnsupportedOperationException(\""Alter namespace in Spark is not supported yet.\"");\n+    public void alterNamespace(String[] namespace, NamespaceChange... changes)\n+            throws NoSuchNamespaceException {\n+        checkNamespace(namespace);\n+        try {\n+            String databaseName = getDatabaseNameFromNamespace(namespace);\n+            List<PropertyChange> propertyChanges =\n+                    Arrays.stream(changes).map(this::toPropertyChange).collect(Collectors.toList());\n+            catalog.alterDatabase(databaseName, propertyChanges, false);\n+        } catch (Catalog.DatabaseNotExistException e) {\n+            throw new NoSuchNamespaceException(namespace);\n+        }\n+    }\n+\n+    private PropertyChange toPropertyChange(NamespaceChange change) {\n+        if (change instanceof NamespaceChange.SetProperty) {\n+            NamespaceChange.SetProperty set = (NamespaceChange.SetProperty) change;\n+            return PropertyChange.setProperty(set.property(), set.value());\n+        } else if (change instanceof NamespaceChange.RemoveProperty) {\n+            NamespaceChange.RemoveProperty remove = (NamespaceChange.RemoveProperty) change;\n+            return PropertyChange.removeProperty(remove.property());\n+\n+        } else {\n+            throw new UnsupportedOperationException(\n+                    \""Change is not supported: \"" + change.getClass());\n+        }\n+    }\n+\n+    private String getDatabaseNameFromNamespace(String[] namespace) {\n+        return namespace[0];\n     }\n }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\nindex 4792e33c932b..fee6d1433143 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\n@@ -44,6 +44,7 @@\n import org.assertj.core.api.Assertions;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n \n import java.io.FileNotFoundException;\n import java.time.Duration;\n@@ -63,6 +64,8 @@\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_SMALL_FILE_THRESHOLD;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.when;\n \n class CachingCatalogTest extends CatalogTestBase {\n \n@@ -86,6 +89,26 @@ public void testListDatabasesWhenNoDatabases() {\n         assertThat(databases).contains(\""db\"");\n     }\n \n+    @Test\n+    public void testInvalidateWhenDatabaseIsAltered() throws Exception {\n+        Catalog mockcatalog = Mockito.mock(Catalog.class);\n+        Catalog catalog = new CachingCatalog(mockcatalog);\n+        String databaseName = \""db\"";\n+        boolean ignoreIfExists = false;\n+        Database database = Database.of(databaseName);\n+        Database secondDatabase = Database.of(databaseName);\n+        when(mockcatalog.getDatabase(databaseName)).thenReturn(database, secondDatabase);\n+        doNothing().when(mockcatalog).alterDatabase(databaseName, emptyList(), ignoreIfExists);\n+        Database cachingDatabase = catalog.getDatabase(databaseName);\n+        assertThat(cachingDatabase.name()).isEqualTo(databaseName);\n+        catalog.alterDatabase(databaseName, emptyList(), ignoreIfExists);\n+        Database newCachingDatabase = catalog.getDatabase(databaseName);\n+        // same as secondDatabase means cache is invalidated, so call getDatabase again then return\n+        // secondDatabase\n+        assertThat(newCachingDatabase).isNotSameAs(database);\n+        assertThat(newCachingDatabase).isSameAs(secondDatabase);\n+    }\n+\n     @Test\n     public void testInvalidateSystemTablesIfBaseTableIsModified() throws Exception {\n         Catalog catalog = new CachingCatalog(this.catalog);\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\nindex 98a9b92c5c38..31c4c8e682b8 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CatalogTestBase.java\n@@ -54,6 +54,7 @@\n import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatExceptionOfType;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.junit.Assert.assertEquals;\n \n /** Base test class of paimon catalog in {@link Catalog}. */\n public abstract class CatalogTestBase {\n@@ -960,4 +961,50 @@ public void testTableUUID() throws Exception {\n         assertThat(Long.parseLong(uuid.substring((identifier.getFullName() + \"".\"").length())))\n                 .isGreaterThan(0);\n     }\n+\n+    protected void alterDatabaseWhenSupportAlter() throws Exception {\n+        // Alter database\n+        String databaseName = \""db_to_alter\"";\n+        catalog.createDatabase(databaseName, false);\n+        String key = \""key1\"";\n+        String key2 = \""key2\"";\n+        // Add property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.setProperty(key, \""value\""),\n+                        PropertyChange.setProperty(key2, \""value\"")),\n+                false);\n+        Database db = catalog.getDatabase(databaseName);\n+        assertEquals(\""value\"", db.options().get(key));\n+        assertEquals(\""value\"", db.options().get(key2));\n+        // Update property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.setProperty(key, \""value1\""),\n+                        PropertyChange.setProperty(key2, \""value1\"")),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertEquals(\""value1\"", db.options().get(key));\n+        assertEquals(\""value1\"", db.options().get(key2));\n+        // remove property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertEquals(false, db.options().containsKey(key));\n+        assertEquals(false, db.options().containsKey(key2));\n+        // Remove non-existent property\n+        catalog.alterDatabase(\n+                databaseName,\n+                Lists.newArrayList(\n+                        PropertyChange.removeProperty(key), PropertyChange.removeProperty(key2)),\n+                false);\n+        db = catalog.getDatabase(databaseName);\n+        assertEquals(false, db.options().containsKey(key));\n+        assertEquals(false, db.options().containsKey(key2));\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java\nindex 65ea6721c220..7045daca8e86 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/FileSystemCatalogTest.java\n@@ -24,10 +24,13 @@\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.types.DataTypes;\n \n+import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n+\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n import static org.assertj.core.api.Assertions.assertThatExceptionOfType;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n /** Tests for {@link FileSystemCatalog}. */\n public class FileSystemCatalogTest extends CatalogTestBase {\n@@ -67,4 +70,17 @@ public void testCreateTableCaseSensitive() throws Exception {\n                 .isThrownBy(() -> catalog.createTable(identifier, schema, false))\n                 .withMessage(\""Field name [Pk1, Col1] cannot contain upper case in the catalog.\"");\n     }\n+\n+    @Test\n+    public void testAlterDatabase() throws Exception {\n+        String databaseName = \""test_alter_db\"";\n+        catalog.createDatabase(databaseName, false);\n+        assertThatThrownBy(\n+                        () ->\n+                                catalog.alterDatabase(\n+                                        databaseName,\n+                                        Lists.newArrayList(PropertyChange.removeProperty(\""a\"")),\n+                                        false))\n+                .isInstanceOf(UnsupportedOperationException.class);\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\nindex f5befc724f8b..f01a46fd6bb4 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/jdbc/JdbcCatalogTest.java\n@@ -122,4 +122,9 @@ public void testSerializeTable() throws Exception {\n                     }\n                 });\n     }\n+\n+    @Test\n+    public void testAlterDatabase() throws Exception {\n+        this.alterDatabaseWhenSupportAlter();\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nindex a605e5e77c2a..821257a0e10e 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -18,13 +18,17 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.DatabaseName;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n import org.apache.paimon.rest.responses.ListDatabasesResponse;\n \n+import org.apache.paimon.shade.guava30.com.google.common.collect.Lists;\n+\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n@@ -40,10 +44,9 @@ public static String databaseName() {\n     }\n \n     public static CreateDatabaseRequest createDatabaseRequest(String name) {\n-        boolean ignoreIfExists = true;\n         Map<String, String> options = new HashMap<>();\n         options.put(\""a\"", \""b\"");\n-        return new CreateDatabaseRequest(name, ignoreIfExists, options);\n+        return new CreateDatabaseRequest(name, options);\n     }\n \n     public static CreateDatabaseResponse createDatabaseResponse(String name) {\n@@ -69,4 +72,15 @@ public static ListDatabasesResponse listDatabasesResponse(String name) {\n     public static ErrorResponse noSuchResourceExceptionErrorResponse() {\n         return new ErrorResponse(\""message\"", 404, new ArrayList<>());\n     }\n+\n+    public static AlterDatabaseRequest alterDatabaseRequest() {\n+        Map<String, String> add = new HashMap<>();\n+        add.put(\""add\"", \""value\"");\n+        return new AlterDatabaseRequest(Lists.newArrayList(\""remove\""), add);\n+    }\n+\n+    public static AlterDatabaseResponse alterDatabaseResponse() {\n+        return new AlterDatabaseResponse(\n+                Lists.newArrayList(\""remove\""), Lists.newArrayList(\""add\""), new ArrayList<>());\n+    }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex 0fff81afdcde..9b1582929560 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.catalog.Database;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n import org.apache.paimon.rest.responses.GetDatabaseResponse;\n@@ -185,6 +186,33 @@ public void testDropDatabaseWhenCascadeIsFalseAndTablesExist() throws Exception\n         verify(mockRestCatalog, times(1)).listTables(eq(name));\n     }\n \n+    @Test\n+    public void testAlterDatabase() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        AlterDatabaseResponse response = MockRESTMessage.alterDatabaseResponse();\n+        mockResponse(mapper.writeValueAsString(response), 200);\n+        assertDoesNotThrow(() -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), true));\n+    }\n+\n+    @Test\n+    public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsFalse()\n+            throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n+        mockResponse(mapper.writeValueAsString(response), 404);\n+        assertThrows(\n+                Catalog.DatabaseNotExistException.class,\n+                () -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), false));\n+    }\n+\n+    @Test\n+    public void testAlterDatabaseWhenDatabaseNotExistAndIgnoreIfNotExistsIsTrue() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        ErrorResponse response = MockRESTMessage.noSuchResourceExceptionErrorResponse();\n+        mockResponse(mapper.writeValueAsString(response), 404);\n+        assertDoesNotThrow(() -> mockRestCatalog.alterDatabase(name, new ArrayList<>(), true));\n+    }\n+\n     private void mockResponse(String mockResponse, int httpCode) {\n         MockResponse mockResponseObj =\n                 new MockResponse()\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 7fee81ef1024..0e5a71be39c0 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -18,7 +18,9 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.rest.requests.AlterDatabaseRequest;\n import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.responses.AlterDatabaseResponse;\n import org.apache.paimon.rest.responses.ConfigResponse;\n import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n@@ -68,7 +70,6 @@ public void createDatabaseRequestParseTest() throws Exception {\n         String requestStr = mapper.writeValueAsString(request);\n         CreateDatabaseRequest parseData = mapper.readValue(requestStr, CreateDatabaseRequest.class);\n         assertEquals(request.getName(), parseData.getName());\n-        assertEquals(request.getIgnoreIfExists(), parseData.getIgnoreIfExists());\n         assertEquals(request.getOptions().size(), parseData.getOptions().size());\n     }\n \n@@ -104,4 +105,24 @@ public void listDatabaseResponseParseTest() throws Exception {\n         assertEquals(response.getDatabases().size(), parseData.getDatabases().size());\n         assertEquals(name, parseData.getDatabases().get(0).getName());\n     }\n+\n+    @Test\n+    public void alterDatabaseRequestParseTest() throws Exception {\n+        AlterDatabaseRequest request = MockRESTMessage.alterDatabaseRequest();\n+        String requestStr = mapper.writeValueAsString(request);\n+        AlterDatabaseRequest parseData = mapper.readValue(requestStr, AlterDatabaseRequest.class);\n+        assertEquals(request.getRemovals().size(), parseData.getRemovals().size());\n+        assertEquals(request.getUpdates().size(), parseData.getUpdates().size());\n+    }\n+\n+    @Test\n+    public void alterDatabaseResponseParseTest() throws Exception {\n+        AlterDatabaseResponse response = MockRESTMessage.alterDatabaseResponse();\n+        String responseStr = mapper.writeValueAsString(response);\n+        AlterDatabaseResponse parseData =\n+                mapper.readValue(responseStr, AlterDatabaseResponse.class);\n+        assertEquals(response.getRemoved().size(), parseData.getRemoved().size());\n+        assertEquals(response.getUpdated().size(), parseData.getUpdated().size());\n+        assertEquals(response.getMissing().size(), parseData.getMissing().size());\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\nindex 734a47dead06..4b8cf7912192 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.TableType;\n import org.apache.paimon.catalog.CatalogContext;\n import org.apache.paimon.catalog.Identifier;\n+import org.apache.paimon.catalog.PropertyChange;\n import org.apache.paimon.flink.log.LogSinkProvider;\n import org.apache.paimon.flink.log.LogSourceProvider;\n import org.apache.paimon.flink.log.LogStoreRegister;\n@@ -83,6 +84,7 @@\n import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.Set;\n import java.util.UUID;\n import java.util.stream.Stream;\n@@ -98,7 +100,13 @@\n import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatCollection;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n /** Test for {@link FlinkCatalog}. */\n public class FlinkCatalogTest {\n@@ -571,11 +579,9 @@ public void testCreateDb_DatabaseWithProperties() throws Exception {\n     }\n \n     @Test\n-    public void testCreateDb_DatabaseWithCommentException() {\n+    public void testCreateDb_DatabaseWithCommentSuccessful() throws DatabaseAlreadyExistException {\n         CatalogDatabaseImpl database = new CatalogDatabaseImpl(Collections.emptyMap(), \""haha\"");\n-        assertThatThrownBy(() -> catalog.createDatabase(path1.getDatabaseName(), database, false))\n-                .isInstanceOf(UnsupportedOperationException.class)\n-                .hasMessage(\""Create database with description is unsupported.\"");\n+        assertDoesNotThrow(() -> catalog.createDatabase(path1.getDatabaseName(), database, false));\n     }\n \n     @ParameterizedTest\n@@ -596,6 +602,75 @@ public void testDropDb_DatabaseNotExistException() {\n                 .hasMessage(\""Database db1 does not exist in Catalog test-catalog.\"");\n     }\n \n+    @Test\n+    public void testAlterDb() throws DatabaseAlreadyExistException, DatabaseNotExistException {\n+        CatalogDatabaseImpl database = new CatalogDatabaseImpl(Collections.emptyMap(), null);\n+        catalog.createDatabase(path1.getDatabaseName(), database, false);\n+        Map<String, String> properties = Collections.singletonMap(\""haa\"", \""ccc\"");\n+        CatalogDatabaseImpl newDatabase = new CatalogDatabaseImpl(properties, \""haha\"");\n+        // as file system catalog don't support alter database, so we have to use mock to overview\n+        // this method to test\n+        Catalog mockCatalog = spy(catalog);\n+        doNothing().when(mockCatalog).alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        when(mockCatalog.getDatabase(path1.getDatabaseName())).thenReturn(database);\n+        mockCatalog.alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        verify(mockCatalog, times(1)).alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        verify(mockCatalog, times(1)).getDatabase(path1.getDatabaseName());\n+    }\n+\n+    @Test\n+    public void testAlterDbComment()\n+            throws DatabaseAlreadyExistException, DatabaseNotExistException {\n+        CatalogDatabaseImpl database = new CatalogDatabaseImpl(Collections.emptyMap(), null);\n+        catalog.createDatabase(path1.getDatabaseName(), database, false);\n+        Catalog mockCatalog = spy(catalog);\n+        when(mockCatalog.getDatabase(path1.getDatabaseName())).thenReturn(database);\n+        CatalogDatabaseImpl newDatabase = new CatalogDatabaseImpl(Collections.emptyMap(), \""aa\"");\n+        doNothing().when(mockCatalog).alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        mockCatalog.alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        verify(mockCatalog, times(1)).alterDatabase(path1.getDatabaseName(), newDatabase, false);\n+        verify(mockCatalog, times(1)).getDatabase(path1.getDatabaseName());\n+    }\n+\n+    @Test\n+    public void testAlterDb_DatabaseNotExistException() {\n+        CatalogDatabaseImpl database = new CatalogDatabaseImpl(Collections.emptyMap(), null);\n+        assertThatThrownBy(() -> catalog.alterDatabase(path1.getDatabaseName(), database, false))\n+                .isInstanceOf(DatabaseNotExistException.class)\n+                .hasMessage(\""Database db1 does not exist in Catalog test-catalog.\"");\n+    }\n+\n+    @Test\n+    public void testGetProperties() throws Exception {\n+        Map<String, String> oldProperties = Collections.emptyMap();\n+        Map<String, String> newProperties = Collections.singletonMap(\""haa\"", \""ccc\"");\n+        List<PropertyChange> propertyChanges =\n+                FlinkCatalog.getPropertyChanges(oldProperties, newProperties);\n+        assertThat(propertyChanges.size()).isEqualTo(1);\n+        oldProperties = newProperties;\n+        propertyChanges = FlinkCatalog.getPropertyChanges(oldProperties, newProperties);\n+        assertThat(propertyChanges.size()).isEqualTo(0);\n+        oldProperties = Collections.singletonMap(\""aa\"", \""ccc\"");\n+        propertyChanges = FlinkCatalog.getPropertyChanges(oldProperties, newProperties);\n+        assertThat(propertyChanges.size()).isEqualTo(2);\n+    }\n+\n+    @Test\n+    public void testGetPropertyChangeFromComment() {\n+        Optional<PropertyChange> commentChange =\n+                FlinkCatalog.getPropertyChangeFromComment(Optional.empty(), Optional.empty());\n+        assertThat(commentChange.isPresent()).isFalse();\n+        commentChange =\n+                FlinkCatalog.getPropertyChangeFromComment(Optional.of(\""aa\""), Optional.of(\""bb\""));\n+        assertThat(commentChange.isPresent()).isTrue();\n+        commentChange =\n+                FlinkCatalog.getPropertyChangeFromComment(Optional.of(\""aa\""), Optional.empty());\n+        assertThat(commentChange.isPresent()).isFalse();\n+        commentChange =\n+                FlinkCatalog.getPropertyChangeFromComment(Optional.empty(), Optional.of(\""bb\""));\n+        assertThat(commentChange.isPresent()).isTrue();\n+    }\n+\n     @Test\n     public void testCreateTableWithColumnOptions() throws Exception {\n         ResolvedExpression expression =\n\ndiff --git a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\nindex 267bdf0c7100..e3b48f02a696 100644\n--- a/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n+++ b/paimon-hive/paimon-hive-catalog/src/test/java/org/apache/paimon/hive/HiveCatalogTest.java\n@@ -173,6 +173,11 @@ private void testHiveConfDirFromEnvImpl() {\n         assertThat(hiveConf.get(\""hive.metastore.uris\"")).isEqualTo(\""dummy-hms\"");\n     }\n \n+    @Test\n+    public void testAlterDatabase() throws Exception {\n+        this.alterDatabaseWhenSupportAlter();\n+    }\n+\n     @Test\n     public void testAddHiveTableParameters() {\n         try {\n\ndiff --git a/paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala b/paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala\nindex 4ba079ea0bb2..526e24250751 100644\n--- a/paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala\n+++ b/paimon-spark/paimon-spark-ut/src/test/scala/org/apache/paimon/spark/sql/DDLWithHiveCatalogTestBase.scala\n@@ -22,7 +22,7 @@ import org.apache.paimon.hive.HiveMetastoreClient\n import org.apache.paimon.spark.PaimonHiveTestBase\n import org.apache.paimon.table.FileStoreTable\n \n-import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.{AnalysisException, Row, SparkSession}\n import org.junit.jupiter.api.Assertions\n \n abstract class DDLWithHiveCatalogTestBase extends PaimonHiveTestBase {\n@@ -194,6 +194,46 @@ abstract class DDLWithHiveCatalogTestBase extends PaimonHiveTestBase {\n     }\n   }\n \n+  test(\""Paimon DDL with hive catalog: alter database's properties\"") {\n+    Seq(sparkCatalogName, paimonHiveCatalogName).foreach {\n+      catalogName =>\n+        spark.sql(s\""USE $catalogName\"")\n+        val databaseName = \""paimon_db\""\n+        withDatabase(databaseName) {\n+          spark.sql(s\""CREATE DATABASE $databaseName WITH DBPROPERTIES ('k1' = 'v1', 'k2' = 'v2')\"")\n+          var props = getDatabaseProps(databaseName)\n+          Assertions.assertEquals(props(\""k1\""), \""v1\"")\n+          Assertions.assertEquals(props(\""k2\""), \""v2\"")\n+          spark.sql(s\""ALTER DATABASE $databaseName SET DBPROPERTIES ('k1' = 'v11', 'k2' = 'v22')\"")\n+          props = getDatabaseProps(databaseName)\n+          Assertions.assertEquals(props(\""k1\""), \""v11\"")\n+          Assertions.assertEquals(props(\""k2\""), \""v22\"")\n+        }\n+    }\n+  }\n+\n+  test(\""Paimon DDL with hive catalog: alter database location\"") {\n+    Seq(sparkCatalogName, paimonHiveCatalogName).foreach {\n+      catalogName =>\n+        spark.sql(s\""USE $catalogName\"")\n+        val databaseName = \""paimon_db\""\n+        withDatabase(databaseName) {\n+          spark.sql(s\""CREATE DATABASE $databaseName WITH DBPROPERTIES ('k1' = 'v1', 'k2' = 'v2')\"")\n+          withTempDir {\n+            dBLocation =>\n+              try {\n+                spark.sql(\n+                  s\""ALTER DATABASE $databaseName SET LOCATION '${dBLocation.getCanonicalPath}'\"")\n+              } catch {\n+                case e: AnalysisException =>\n+                  Assertions.assertTrue(\n+                    e.getMessage.contains(\""does not support altering database location\""))\n+              }\n+          }\n+        }\n+    }\n+  }\n+\n   test(\""Paimon DDL with hive catalog: set default database\"") {\n     var reusedSpark = spark\n \n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4676"", ""pr_id"": 4676, ""issue_id"": 4540, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Support REST Catalog\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nProvide REST Catalog to implement Catalog. By REST Catalog:\r\n- the user could easily access their catalog system\r\n- less dependency\r\n- customize the server's logic\n\n### Solution\n\n[PIP-28: Introduce REST Catalog](https://cwiki.apache.org/confluence/display/PAIMON/PIP-28%3A+Introduce+REST+Catalog)\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 86, ""test_files_count"": 6, ""non_test_files_count"": 22, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogInternalOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/RESTMessage.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/requests/DropDatabaseRequest.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ConfigResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/CreateDatabaseResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/DatabaseName.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/GetDatabaseResponse.java"", ""paimon-core/src/main/java/org/apache/paimon/rest/responses/ListDatabasesResponse.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java"", ""paimon-open-api/generate.sh"", ""paimon-open-api/rest-catalog-open-api.yaml"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java"", ""paimon-open-api/src/main/java/org/apache/paimon/open/api/config/OpenAPIConfig.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java"", ""paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java""], ""base_commit"": ""f8c33c5d72cbe16977ca842f41bfb8a3e32285f7"", ""head_commit"": ""bfee5baf7674c983f50992e7c7921854da26224b"", ""repo_url"": ""https://github.com/apache/paimon/pull/4676"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4676"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-11T14:18:24.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java b/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\nindex 1a8618c1c603..ce2cbb56ae24 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/DefaultErrorHandler.java\n@@ -18,8 +18,10 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.rest.exceptions.AlreadyExistsException;\n import org.apache.paimon.rest.exceptions.BadRequestException;\n import org.apache.paimon.rest.exceptions.ForbiddenException;\n+import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n import org.apache.paimon.rest.exceptions.NotAuthorizedException;\n import org.apache.paimon.rest.exceptions.RESTException;\n import org.apache.paimon.rest.exceptions.ServiceFailureException;\n@@ -28,6 +30,7 @@\n \n /** Default error handler. */\n public class DefaultErrorHandler extends ErrorHandler {\n+\n     private static final ErrorHandler INSTANCE = new DefaultErrorHandler();\n \n     public static ErrorHandler getInstance() {\n@@ -36,26 +39,32 @@ public static ErrorHandler getInstance() {\n \n     @Override\n     public void accept(ErrorResponse error) {\n-        int code = error.code();\n+        int code = error.getCode();\n+        String message = error.getMessage();\n         switch (code) {\n             case 400:\n-                throw new BadRequestException(\n-                        String.format(\""Malformed request: %s\"", error.message()));\n+                throw new BadRequestException(String.format(\""Malformed request: %s\"", message));\n             case 401:\n-                throw new NotAuthorizedException(\""Not authorized: %s\"", error.message());\n+                throw new NotAuthorizedException(\""Not authorized: %s\"", message);\n             case 403:\n-                throw new ForbiddenException(\""Forbidden: %s\"", error.message());\n+                throw new ForbiddenException(\""Forbidden: %s\"", message);\n+            case 404:\n+                throw new NoSuchResourceException(\""%s\"", message);\n             case 405:\n             case 406:\n                 break;\n+            case 409:\n+                throw new AlreadyExistsException(\""%s\"", message);\n             case 500:\n-                throw new ServiceFailureException(\""Server error: %s\"", error.message());\n+                throw new ServiceFailureException(\""Server error: %s\"", message);\n             case 501:\n-                throw new UnsupportedOperationException(error.message());\n+                throw new UnsupportedOperationException(message);\n             case 503:\n-                throw new ServiceUnavailableException(\""Service unavailable: %s\"", error.message());\n+                throw new ServiceUnavailableException(\""Service unavailable: %s\"", message);\n+            default:\n+                break;\n         }\n \n-        throw new RESTException(\""Unable to process: %s\"", error.message());\n+        throw new RESTException(\""Unable to process: %s\"", message);\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\nindex e092711e5f97..97696aef09ed 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/HttpClient.java\n@@ -95,6 +95,23 @@ public <T extends RESTResponse> T post(\n         }\n     }\n \n+    @Override\n+    public <T extends RESTResponse> T delete(\n+            String path, RESTRequest body, Map<String, String> headers) {\n+        try {\n+            RequestBody requestBody = buildRequestBody(body);\n+            Request request =\n+                    new Request.Builder()\n+                            .url(uri + path)\n+                            .delete(requestBody)\n+                            .headers(Headers.of(headers))\n+                            .build();\n+            return exec(request, null);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n     @Override\n     public void close() throws IOException {\n         okHttpClient.dispatcher().cancelAll();\n@@ -111,10 +128,13 @@ private <T extends RESTResponse> T exec(Request request, Class<T> responseType)\n                                 response.code());\n                 errorHandler.accept(error);\n             }\n-            if (responseBodyStr == null) {\n+            if (responseType != null && responseBodyStr != null) {\n+                return mapper.readValue(responseBodyStr, responseType);\n+            } else if (responseType == null) {\n+                return null;\n+            } else {\n                 throw new RESTException(\""response body is null.\"");\n             }\n-            return mapper.readValue(responseBodyStr, responseType);\n         } catch (Exception e) {\n             throw new RESTException(e, \""rest exception\"");\n         }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\nindex f3007bf4bf02..3c2538df0ca2 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalog.java\n@@ -29,12 +29,21 @@\n import org.apache.paimon.rest.auth.AuthSession;\n import org.apache.paimon.rest.auth.CredentialsProvider;\n import org.apache.paimon.rest.auth.CredentialsProviderFactory;\n+import org.apache.paimon.rest.exceptions.AlreadyExistsException;\n+import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n+import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.DropDatabaseRequest;\n import org.apache.paimon.rest.responses.ConfigResponse;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.DatabaseName;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.table.Table;\n \n import org.apache.paimon.shade.guava30.com.google.common.annotations.VisibleForTesting;\n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n import java.time.Duration;\n@@ -42,6 +51,7 @@\n import java.util.Map;\n import java.util.Optional;\n import java.util.concurrent.ScheduledExecutorService;\n+import java.util.stream.Collectors;\n \n import static org.apache.paimon.utils.ThreadPoolUtils.createScheduledThreadPool;\n \n@@ -113,24 +123,49 @@ public FileIO fileIO() {\n \n     @Override\n     public List<String> listDatabases() {\n-        throw new UnsupportedOperationException();\n+        ListDatabasesResponse response =\n+                client.get(resourcePaths.databases(), ListDatabasesResponse.class, headers());\n+        if (response.getDatabases() != null) {\n+            return response.getDatabases().stream()\n+                    .map(DatabaseName::getName)\n+                    .collect(Collectors.toList());\n+        }\n+        return ImmutableList.of();\n     }\n \n     @Override\n     public void createDatabase(String name, boolean ignoreIfExists, Map<String, String> properties)\n             throws DatabaseAlreadyExistException {\n-        throw new UnsupportedOperationException();\n+        CreateDatabaseRequest request = new CreateDatabaseRequest(name, ignoreIfExists, properties);\n+        try {\n+            client.post(\n+                    resourcePaths.databases(), request, CreateDatabaseResponse.class, headers());\n+        } catch (AlreadyExistsException e) {\n+            throw new DatabaseAlreadyExistException(name);\n+        }\n     }\n \n     @Override\n     public Database getDatabase(String name) throws DatabaseNotExistException {\n-        throw new UnsupportedOperationException();\n+        try {\n+            GetDatabaseResponse response =\n+                    client.get(resourcePaths.database(name), GetDatabaseResponse.class, headers());\n+            return new Database.DatabaseImpl(\n+                    name, response.options(), response.comment().orElseGet(() -> null));\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(name);\n+        }\n     }\n \n     @Override\n     public void dropDatabase(String name, boolean ignoreIfNotExists, boolean cascade)\n             throws DatabaseNotExistException, DatabaseNotEmptyException {\n-        throw new UnsupportedOperationException();\n+        DropDatabaseRequest request = new DropDatabaseRequest(ignoreIfNotExists, cascade);\n+        try {\n+            client.delete(resourcePaths.database(name), request, headers());\n+        } catch (NoSuchResourceException e) {\n+            throw new DatabaseNotExistException(name);\n+        }\n     }\n \n     @Override\n@@ -208,7 +243,7 @@ public void close() throws Exception {\n     Map<String, String> fetchOptionsFromServer(\n             Map<String, String> headers, Map<String, String> clientProperties) {\n         ConfigResponse response =\n-                client.get(ResourcePaths.V1_CONFIG, ConfigResponse.class, headers());\n+                client.get(ResourcePaths.V1_CONFIG, ConfigResponse.class, headers);\n         return response.merge(clientProperties);\n     }\n \n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogInternalOptions.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogInternalOptions.java\nindex 62a8bf134ae5..722010923c46 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogInternalOptions.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTCatalogInternalOptions.java\n@@ -33,4 +33,9 @@ public class RESTCatalogInternalOptions {\n                     .stringType()\n                     .noDefaultValue()\n                     .withDescription(\""REST Catalog auth credentials provider.\"");\n+    public static final ConfigOption<String> DATABASE_COMMENT =\n+            ConfigOptions.key(\""comment\"")\n+                    .stringType()\n+                    .defaultValue(null)\n+                    .withDescription(\""REST Catalog database comment.\"");\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\nindex feeed06a417a..d0244f309ef4 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTClient.java\n@@ -28,4 +28,6 @@ public interface RESTClient extends Closeable {\n \n     <T extends RESTResponse> T post(\n             String path, RESTRequest body, Class<T> responseType, Map<String, String> headers);\n+\n+    <T extends RESTResponse> T delete(String path, RESTRequest body, Map<String, String> headers);\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/RESTMessage.java b/paimon-core/src/main/java/org/apache/paimon/rest/RESTMessage.java\nindex 6cb0b6fa6573..31d46df7ef0f 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/RESTMessage.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/RESTMessage.java\n@@ -18,5 +18,8 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+\n /** Interface to mark both REST requests and responses. */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n public interface RESTMessage {}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\nindex aaca6193802d..a6d0000a225b 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/ResourcePaths.java\n@@ -18,10 +18,13 @@\n \n package org.apache.paimon.rest;\n \n+import java.util.StringJoiner;\n+\n /** Resource paths for REST catalog. */\n public class ResourcePaths {\n \n     public static final String V1_CONFIG = \""/api/v1/config\"";\n+    private static final StringJoiner SLASH = new StringJoiner(\""/\"");\n \n     public static ResourcePaths forCatalogProperties(String prefix) {\n         return new ResourcePaths(prefix);\n@@ -32,4 +35,12 @@ public static ResourcePaths forCatalogProperties(String prefix) {\n     public ResourcePaths(String prefix) {\n         this.prefix = prefix;\n     }\n+\n+    public String databases() {\n+        return SLASH.add(\""api\"").add(\""v1\"").add(prefix).add(\""databases\"").toString();\n+    }\n+\n+    public String database(String databaseName) {\n+        return SLASH.add(\""api\"").add(\""v1\"").add(prefix).add(\""databases\"").add(databaseName).toString();\n+    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java b/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\nindex 74efb8508a06..3ca7590e5f96 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/auth/AuthSession.java\n@@ -33,9 +33,10 @@\n public class AuthSession {\n \n     static final int TOKEN_REFRESH_NUM_RETRIES = 5;\n+    static final long MIN_REFRESH_WAIT_MILLIS = 10;\n+    static final long MAX_REFRESH_WINDOW_MILLIS = 300_000; // 5 minutes\n+\n     private static final Logger log = LoggerFactory.getLogger(AuthSession.class);\n-    private static final long MAX_REFRESH_WINDOW_MILLIS = 300_000; // 5 minutes\n-    private static final long MIN_REFRESH_WAIT_MILLIS = 10;\n     private final CredentialsProvider credentialsProvider;\n     private volatile Map<String, String> headers;\n \n@@ -76,12 +77,38 @@ public Map<String, String> getHeaders() {\n         return headers;\n     }\n \n+    public Boolean refresh() {\n+        if (this.credentialsProvider.supportRefresh()\n+                && this.credentialsProvider.keepRefreshed()\n+                && this.credentialsProvider.expiresInMills().isPresent()) {\n+            boolean isSuccessful = this.credentialsProvider.refresh();\n+            if (isSuccessful) {\n+                Map<String, String> currentHeaders = this.headers;\n+                this.headers =\n+                        RESTUtil.merge(currentHeaders, this.credentialsProvider.authHeader());\n+            }\n+            return isSuccessful;\n+        }\n+\n+        return false;\n+    }\n+\n     @VisibleForTesting\n     static void scheduleTokenRefresh(\n             ScheduledExecutorService executor, AuthSession session, long expiresAtMillis) {\n         scheduleTokenRefresh(executor, session, expiresAtMillis, 0);\n     }\n \n+    @VisibleForTesting\n+    static long getTimeToWaitByExpiresInMills(long expiresInMillis) {\n+        // how much ahead of time to start the refresh to allow it to complete\n+        long refreshWindowMillis = Math.min(expiresInMillis, MAX_REFRESH_WINDOW_MILLIS);\n+        // how much time to wait before expiration\n+        long waitIntervalMillis = expiresInMillis - refreshWindowMillis;\n+        // how much time to actually wait\n+        return Math.max(waitIntervalMillis, MIN_REFRESH_WAIT_MILLIS);\n+    }\n+\n     private static void scheduleTokenRefresh(\n             ScheduledExecutorService executor,\n             AuthSession session,\n@@ -89,12 +116,7 @@ private static void scheduleTokenRefresh(\n             int retryTimes) {\n         if (retryTimes < TOKEN_REFRESH_NUM_RETRIES) {\n             long expiresInMillis = expiresAtMillis - System.currentTimeMillis();\n-            // how much ahead of time to start the refresh to allow it to complete\n-            long refreshWindowMillis = Math.min(expiresInMillis, MAX_REFRESH_WINDOW_MILLIS);\n-            // how much time to wait before expiration\n-            long waitIntervalMillis = expiresInMillis - refreshWindowMillis;\n-            // how much time to actually wait\n-            long timeToWait = Math.max(waitIntervalMillis, MIN_REFRESH_WAIT_MILLIS);\n+            long timeToWait = getTimeToWaitByExpiresInMills(expiresInMillis);\n \n             executor.schedule(\n                     () -> {\n@@ -118,20 +140,4 @@ private static void scheduleTokenRefresh(\n             log.warn(\""Failed to refresh token after {} retries.\"", TOKEN_REFRESH_NUM_RETRIES);\n         }\n     }\n-\n-    public Boolean refresh() {\n-        if (this.credentialsProvider.supportRefresh()\n-                && this.credentialsProvider.keepRefreshed()\n-                && this.credentialsProvider.expiresInMills().isPresent()) {\n-            boolean isSuccessful = this.credentialsProvider.refresh();\n-            if (isSuccessful) {\n-                Map<String, String> currentHeaders = this.headers;\n-                this.headers =\n-                        RESTUtil.merge(currentHeaders, this.credentialsProvider.authHeader());\n-            }\n-            return isSuccessful;\n-        }\n-\n-        return false;\n-    }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java\nnew file mode 100644\nindex 000000000000..8e30c8375bf9\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/AlreadyExistsException.java\n@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.exceptions;\n+\n+/** Exception thrown on HTTP 409 means a resource already exists. */\n+public class AlreadyExistsException extends RESTException {\n+\n+    public AlreadyExistsException(String message, Object... args) {\n+        super(message, args);\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java\nnew file mode 100644\nindex 000000000000..cc4c7881f465\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/exceptions/NoSuchResourceException.java\n@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.exceptions;\n+\n+/** Exception thrown on HTTP 404 means a resource not exists. */\n+public class NoSuchResourceException extends RESTException {\n+\n+    public NoSuchResourceException(String message, Object... args) {\n+        super(message, args);\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java\nnew file mode 100644\nindex 000000000000..6067bf544b87\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/CreateDatabaseRequest.java\n@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Request for creating database. */\n+public class CreateDatabaseRequest implements RESTRequest {\n+\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_IGNORE_IF_EXISTS = \""ignoreIfExists\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+\n+    @JsonProperty(FIELD_NAME)\n+    private String name;\n+\n+    @JsonProperty(FIELD_IGNORE_IF_EXISTS)\n+    private boolean ignoreIfExists;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private Map<String, String> options;\n+\n+    @JsonCreator\n+    public CreateDatabaseRequest(\n+            @JsonProperty(FIELD_NAME) String name,\n+            @JsonProperty(FIELD_IGNORE_IF_EXISTS) boolean ignoreIfExists,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options) {\n+        this.name = name;\n+        this.ignoreIfExists = ignoreIfExists;\n+        this.options = options;\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String getName() {\n+        return name;\n+    }\n+\n+    @JsonGetter(FIELD_IGNORE_IF_EXISTS)\n+    public boolean getIgnoreIfExists() {\n+        return ignoreIfExists;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> getOptions() {\n+        return options;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropDatabaseRequest.java b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropDatabaseRequest.java\nnew file mode 100644\nindex 000000000000..d97f211c1caa\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/requests/DropDatabaseRequest.java\n@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.requests;\n+\n+import org.apache.paimon.rest.RESTRequest;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+/** Request for DropDatabase. */\n+public class DropDatabaseRequest implements RESTRequest {\n+\n+    private static final String FIELD_IGNORE_IF_EXISTS = \""ignoreIfExists\"";\n+    private static final String FIELD_CASCADE = \""cascade\"";\n+\n+    @JsonProperty(FIELD_IGNORE_IF_EXISTS)\n+    private final boolean ignoreIfNotExists;\n+\n+    @JsonProperty(FIELD_CASCADE)\n+    private final boolean cascade;\n+\n+    @JsonCreator\n+    public DropDatabaseRequest(\n+            @JsonProperty(FIELD_IGNORE_IF_EXISTS) boolean ignoreIfNotExists,\n+            @JsonProperty(FIELD_CASCADE) boolean cascade) {\n+        this.ignoreIfNotExists = ignoreIfNotExists;\n+        this.cascade = cascade;\n+    }\n+\n+    @JsonGetter(FIELD_IGNORE_IF_EXISTS)\n+    public boolean getIgnoreIfNotExists() {\n+        return ignoreIfNotExists;\n+    }\n+\n+    @JsonGetter(FIELD_CASCADE)\n+    public boolean getCascade() {\n+        return cascade;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ConfigResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ConfigResponse.java\nindex 903cfc84b46d..e8fff88b09c2 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ConfigResponse.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ConfigResponse.java\n@@ -23,17 +23,16 @@\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableMap;\n import org.apache.paimon.shade.guava30.com.google.common.collect.Maps;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n-import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n-import java.beans.ConstructorProperties;\n import java.util.Map;\n import java.util.Objects;\n \n /** Response for getting config. */\n-@JsonIgnoreProperties(ignoreUnknown = true)\n public class ConfigResponse implements RESTResponse {\n+\n     private static final String FIELD_DEFAULTS = \""defaults\"";\n     private static final String FIELD_OVERRIDES = \""overrides\"";\n \n@@ -43,8 +42,10 @@ public class ConfigResponse implements RESTResponse {\n     @JsonProperty(FIELD_OVERRIDES)\n     private Map<String, String> overrides;\n \n-    @ConstructorProperties({FIELD_DEFAULTS, FIELD_OVERRIDES})\n-    public ConfigResponse(Map<String, String> defaults, Map<String, String> overrides) {\n+    @JsonCreator\n+    public ConfigResponse(\n+            @JsonProperty(FIELD_DEFAULTS) Map<String, String> defaults,\n+            @JsonProperty(FIELD_OVERRIDES) Map<String, String> overrides) {\n         this.defaults = defaults;\n         this.overrides = overrides;\n     }\n@@ -65,12 +66,12 @@ public Map<String, String> merge(Map<String, String> clientProperties) {\n     }\n \n     @JsonGetter(FIELD_DEFAULTS)\n-    public Map<String, String> defaults() {\n+    public Map<String, String> getDefaults() {\n         return defaults;\n     }\n \n     @JsonGetter(FIELD_OVERRIDES)\n-    public Map<String, String> overrides() {\n+    public Map<String, String> getOverrides() {\n         return overrides;\n     }\n }\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/CreateDatabaseResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/CreateDatabaseResponse.java\nnew file mode 100644\nindex 000000000000..43c99254f399\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/CreateDatabaseResponse.java\n@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+\n+/** Response for creating database. */\n+public class CreateDatabaseResponse implements RESTResponse {\n+\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+\n+    @JsonProperty(FIELD_NAME)\n+    private String name;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private Map<String, String> options;\n+\n+    @JsonCreator\n+    public CreateDatabaseResponse(\n+            @JsonProperty(FIELD_NAME) String name,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options) {\n+        this.name = name;\n+        this.options = options;\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String getName() {\n+        return name;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> getOptions() {\n+        return options;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/DatabaseName.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/DatabaseName.java\nnew file mode 100644\nindex 000000000000..9a93b2fd1e3d\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/DatabaseName.java\n@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTMessage;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+/** Class for Database entity. */\n+public class DatabaseName implements RESTMessage {\n+\n+    private static final String FIELD_NAME = \""name\"";\n+\n+    @JsonProperty(FIELD_NAME)\n+    private String name;\n+\n+    @JsonCreator\n+    public DatabaseName(@JsonProperty(FIELD_NAME) String name) {\n+        this.name = name;\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String getName() {\n+        return this.name;\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\nindex 685fe53071b6..d24c8f0f9936 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ErrorResponse.java\n@@ -18,10 +18,12 @@\n \n package org.apache.paimon.rest.responses;\n \n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n \n-import java.beans.ConstructorProperties;\n import java.io.PrintWriter;\n import java.io.StringWriter;\n import java.util.ArrayList;\n@@ -29,7 +31,8 @@\n import java.util.List;\n \n /** Response for error. */\n-public class ErrorResponse {\n+public class ErrorResponse implements RESTResponse {\n+\n     private static final String FIELD_MESSAGE = \""message\"";\n     private static final String FIELD_CODE = \""code\"";\n     private static final String FIELD_STACK = \""stack\"";\n@@ -49,8 +52,11 @@ public ErrorResponse(String message, Integer code) {\n         this.stack = new ArrayList<String>();\n     }\n \n-    @ConstructorProperties({FIELD_MESSAGE, FIELD_CODE, FIELD_STACK})\n-    public ErrorResponse(String message, int code, List<String> stack) {\n+    @JsonCreator\n+    public ErrorResponse(\n+            @JsonProperty(FIELD_MESSAGE) String message,\n+            @JsonProperty(FIELD_CODE) int code,\n+            @JsonProperty(FIELD_STACK) List<String> stack) {\n         this.message = message;\n         this.code = code;\n         this.stack = stack;\n@@ -63,17 +69,17 @@ public ErrorResponse(String message, int code, Throwable throwable) {\n     }\n \n     @JsonGetter(FIELD_MESSAGE)\n-    public String message() {\n+    public String getMessage() {\n         return message;\n     }\n \n     @JsonGetter(FIELD_CODE)\n-    public Integer code() {\n+    public Integer getCode() {\n         return code;\n     }\n \n     @JsonGetter(FIELD_STACK)\n-    public List<String> stack() {\n+    public List<String> getStack() {\n         return stack;\n     }\n \n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetDatabaseResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetDatabaseResponse.java\nnew file mode 100644\nindex 000000000000..f8f7c8794b7b\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/GetDatabaseResponse.java\n@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.catalog.Database;\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static org.apache.paimon.rest.RESTCatalogInternalOptions.DATABASE_COMMENT;\n+\n+/** Response for getting database. */\n+public class GetDatabaseResponse implements RESTResponse, Database {\n+\n+    private static final String FIELD_NAME = \""name\"";\n+    private static final String FIELD_OPTIONS = \""options\"";\n+\n+    @JsonProperty(FIELD_NAME)\n+    private final String name;\n+\n+    @JsonProperty(FIELD_OPTIONS)\n+    private final Map<String, String> options;\n+\n+    @JsonCreator\n+    public GetDatabaseResponse(\n+            @JsonProperty(FIELD_NAME) String name,\n+            @JsonProperty(FIELD_OPTIONS) Map<String, String> options) {\n+        this.name = name;\n+        this.options = options;\n+    }\n+\n+    @JsonGetter(FIELD_NAME)\n+    public String getName() {\n+        return name;\n+    }\n+\n+    @JsonGetter(FIELD_OPTIONS)\n+    public Map<String, String> getOptions() {\n+        return options;\n+    }\n+\n+    @Override\n+    public String name() {\n+        return this.getName();\n+    }\n+\n+    @Override\n+    public Map<String, String> options() {\n+        return this.getOptions();\n+    }\n+\n+    @Override\n+    public Optional<String> comment() {\n+        return Optional.ofNullable(\n+                this.options.getOrDefault(DATABASE_COMMENT.key(), DATABASE_COMMENT.defaultValue()));\n+    }\n+}\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListDatabasesResponse.java b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListDatabasesResponse.java\nnew file mode 100644\nindex 000000000000..38773f354b77\n--- /dev/null\n+++ b/paimon-core/src/main/java/org/apache/paimon/rest/responses/ListDatabasesResponse.java\n@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest.responses;\n+\n+import org.apache.paimon.rest.RESTResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonCreator;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonGetter;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.List;\n+\n+/** Response for listing databases. */\n+public class ListDatabasesResponse implements RESTResponse {\n+    private static final String FIELD_DATABASES = \""databases\"";\n+\n+    @JsonProperty(FIELD_DATABASES)\n+    private List<DatabaseName> databases;\n+\n+    @JsonCreator\n+    public ListDatabasesResponse(@JsonProperty(FIELD_DATABASES) List<DatabaseName> databases) {\n+        this.databases = databases;\n+    }\n+\n+    @JsonGetter(FIELD_DATABASES)\n+    public List<DatabaseName> getDatabases() {\n+        return this.databases;\n+    }\n+}\n\ndiff --git a/paimon-open-api/generate.sh b/paimon-open-api/generate.sh\nindex b63aa538abc4..619b642ab760 100755\n--- a/paimon-open-api/generate.sh\n+++ b/paimon-open-api/generate.sh\n@@ -17,6 +17,7 @@\n \n # Start the application\n cd ..\n+mvn spotless:apply\n mvn clean install -DskipTests\n cd ./paimon-open-api\n mvn spring-boot:run &\n\ndiff --git a/paimon-open-api/rest-catalog-open-api.yaml b/paimon-open-api/rest-catalog-open-api.yaml\nindex 432ee123b8d4..2a5d1dc58418 100644\n--- a/paimon-open-api/rest-catalog-open-api.yaml\n+++ b/paimon-open-api/rest-catalog-open-api.yaml\n@@ -28,6 +28,120 @@ servers:\n   - url: http://localhost:8080\n     description: Server URL in Development environment\n paths:\n+  /api/v1/{prefix}/databases:\n+    get:\n+      tags:\n+        - database\n+      summary: List Databases\n+      operationId: listDatabases\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/ListDatabasesResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+    post:\n+      tags:\n+        - database\n+      summary: Create Databases\n+      operationId: createDatabases\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/CreateDatabaseRequest'\n+      responses:\n+        \""500\"":\n+          description: Internal Server Error\n+        \""409\"":\n+          description: Resource has exist\n+          content:\n+            '*/*':\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/CreateDatabaseResponse'\n+  /api/v1/{prefix}/databases/{database}:\n+    get:\n+      tags:\n+        - database\n+      summary: Get Database\n+      operationId: getDatabases\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        \""200\"":\n+          description: OK\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/GetDatabaseResponse'\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            '*/*':\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n+    delete:\n+      tags:\n+        - database\n+      summary: Drop Database\n+      operationId: dropDatabases\n+      parameters:\n+        - name: prefix\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+        - name: database\n+          in: path\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        content:\n+          application/json:\n+            schema:\n+              $ref: '#/components/schemas/DropDatabaseRequest'\n+      responses:\n+        \""404\"":\n+          description: Resource not found\n+          content:\n+            '*/*':\n+              schema:\n+                $ref: '#/components/schemas/ErrorResponse'\n+        \""500\"":\n+          description: Internal Server Error\n   /api/v1/config:\n     get:\n       tags:\n@@ -37,14 +151,67 @@ paths:\n       responses:\n         \""500\"":\n           description: Internal Server Error\n-        \""201\"":\n-          description: Created\n+        \""200\"":\n+          description: OK\n           content:\n             application/json:\n               schema:\n                 $ref: '#/components/schemas/ConfigResponse'\n components:\n   schemas:\n+    CreateDatabaseRequest:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+        ignoreIfExists:\n+          type: boolean\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+    ErrorResponse:\n+      type: object\n+      properties:\n+        message:\n+          type: string\n+        code:\n+          type: integer\n+          format: int32\n+        stack:\n+          type: array\n+          items:\n+            type: string\n+    CreateDatabaseResponse:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n+    DatabaseName:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+    ListDatabasesResponse:\n+      type: object\n+      properties:\n+        databases:\n+          type: array\n+          items:\n+            $ref: '#/components/schemas/DatabaseName'\n+    GetDatabaseResponse:\n+      type: object\n+      properties:\n+        name:\n+          type: string\n+        options:\n+          type: object\n+          additionalProperties:\n+            type: string\n     ConfigResponse:\n       type: object\n       properties:\n@@ -52,9 +219,14 @@ components:\n           type: object\n           additionalProperties:\n             type: string\n-          writeOnly: true\n         overrides:\n           type: object\n           additionalProperties:\n             type: string\n-          writeOnly: true\n+    DropDatabaseRequest:\n+      type: object\n+      properties:\n+        ignoreIfNotExists:\n+          type: boolean\n+        cascade:\n+          type: boolean\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\nindex b47554057105..364cc5adbb2c 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/RESTCatalogController.java\n@@ -19,17 +19,28 @@\n package org.apache.paimon.open.api;\n \n import org.apache.paimon.rest.ResourcePaths;\n+import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.DropDatabaseRequest;\n import org.apache.paimon.rest.responses.ConfigResponse;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.DatabaseName;\n+import org.apache.paimon.rest.responses.ErrorResponse;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+\n+import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableList;\n \n import io.swagger.v3.oas.annotations.Operation;\n import io.swagger.v3.oas.annotations.media.Content;\n import io.swagger.v3.oas.annotations.media.Schema;\n+import io.swagger.v3.oas.annotations.parameters.RequestBody;\n import io.swagger.v3.oas.annotations.responses.ApiResponse;\n import io.swagger.v3.oas.annotations.responses.ApiResponses;\n-import org.springframework.http.HttpStatus;\n-import org.springframework.http.ResponseEntity;\n import org.springframework.web.bind.annotation.CrossOrigin;\n+import org.springframework.web.bind.annotation.DeleteMapping;\n import org.springframework.web.bind.annotation.GetMapping;\n+import org.springframework.web.bind.annotation.PathVariable;\n+import org.springframework.web.bind.annotation.PostMapping;\n import org.springframework.web.bind.annotation.RestController;\n \n import java.util.HashMap;\n@@ -45,7 +56,7 @@ public class RESTCatalogController {\n             tags = {\""config\""})\n     @ApiResponses({\n         @ApiResponse(\n-                responseCode = \""201\"",\n+                responseCode = \""200\"",\n                 content = {\n                     @Content(\n                             schema = @Schema(implementation = ConfigResponse.class),\n@@ -56,14 +67,99 @@ public class RESTCatalogController {\n                 content = {@Content(schema = @Schema())})\n     })\n     @GetMapping(ResourcePaths.V1_CONFIG)\n-    public ResponseEntity<ConfigResponse> getConfig() {\n-        try {\n-            Map<String, String> defaults = new HashMap<>();\n-            Map<String, String> overrides = new HashMap<>();\n-            ConfigResponse response = new ConfigResponse(defaults, overrides);\n-            return new ResponseEntity<>(response, HttpStatus.CREATED);\n-        } catch (Exception e) {\n-            return new ResponseEntity<>(null, HttpStatus.INTERNAL_SERVER_ERROR);\n-        }\n+    public ConfigResponse getConfig() {\n+        Map<String, String> defaults = new HashMap<>();\n+        Map<String, String> overrides = new HashMap<>();\n+        return new ConfigResponse(defaults, overrides);\n+    }\n+\n+    @Operation(\n+            summary = \""List Databases\"",\n+            tags = {\""database\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(\n+                            schema = @Schema(implementation = ListDatabasesResponse.class),\n+                            mediaType = \""application/json\"")\n+                }),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/api/v1/{prefix}/databases\"")\n+    public ListDatabasesResponse listDatabases(@PathVariable String prefix) {\n+        return new ListDatabasesResponse(ImmutableList.of(new DatabaseName(\""account\"")));\n+    }\n+\n+    @Operation(\n+            summary = \""Create Databases\"",\n+            tags = {\""database\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(\n+                            schema = @Schema(implementation = CreateDatabaseResponse.class),\n+                            mediaType = \""application/json\"")\n+                }),\n+        @ApiResponse(\n+                responseCode = \""409\"",\n+                description = \""Resource has exist\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @PostMapping(\""/api/v1/{prefix}/databases\"")\n+    public CreateDatabaseResponse createDatabases(\n+            @PathVariable String prefix, @RequestBody CreateDatabaseRequest request) {\n+        Map<String, String> properties = new HashMap<>();\n+        return new CreateDatabaseResponse(\""name\"", properties);\n     }\n+\n+    @Operation(\n+            summary = \""Get Database\"",\n+            tags = {\""database\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""200\"",\n+                content = {\n+                    @Content(\n+                            schema = @Schema(implementation = GetDatabaseResponse.class),\n+                            mediaType = \""application/json\"")\n+                }),\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @GetMapping(\""/api/v1/{prefix}/databases/{database}\"")\n+    public GetDatabaseResponse getDatabases(\n+            @PathVariable String prefix, @PathVariable String database) {\n+        Map<String, String> options = new HashMap<>();\n+        return new GetDatabaseResponse(\""name\"", options);\n+    }\n+\n+    @Operation(\n+            summary = \""Drop Database\"",\n+            tags = {\""database\""})\n+    @ApiResponses({\n+        @ApiResponse(\n+                responseCode = \""404\"",\n+                description = \""Resource not found\"",\n+                content = {@Content(schema = @Schema(implementation = ErrorResponse.class))}),\n+        @ApiResponse(\n+                responseCode = \""500\"",\n+                content = {@Content(schema = @Schema())})\n+    })\n+    @DeleteMapping(\""/api/v1/{prefix}/databases/{database}\"")\n+    public void dropDatabases(\n+            @PathVariable String prefix,\n+            @PathVariable String database,\n+            @RequestBody DropDatabaseRequest request) {}\n }\n\ndiff --git a/paimon-open-api/src/main/java/org/apache/paimon/open/api/config/OpenAPIConfig.java b/paimon-open-api/src/main/java/org/apache/paimon/open/api/config/OpenAPIConfig.java\nindex 01234c41bbff..0e28cd95f9d2 100644\n--- a/paimon-open-api/src/main/java/org/apache/paimon/open/api/config/OpenAPIConfig.java\n+++ b/paimon-open-api/src/main/java/org/apache/paimon/open/api/config/OpenAPIConfig.java\n@@ -32,7 +32,6 @@\n /** Config for OpenAPI. */\n @Configuration\n public class OpenAPIConfig {\n-\n     @Value(\""${openapi.url}\"")\n     private String devUrl;\n \n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\nindex 1f1b9c01aace..340e38f6a7f8 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/DefaultErrorHandlerTest.java\n@@ -18,8 +18,10 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.rest.exceptions.AlreadyExistsException;\n import org.apache.paimon.rest.exceptions.BadRequestException;\n import org.apache.paimon.rest.exceptions.ForbiddenException;\n+import org.apache.paimon.rest.exceptions.NoSuchResourceException;\n import org.apache.paimon.rest.exceptions.NotAuthorizedException;\n import org.apache.paimon.rest.exceptions.RESTException;\n import org.apache.paimon.rest.exceptions.ServiceFailureException;\n@@ -54,10 +56,16 @@ public void testHandleErrorResponse() {\n         assertThrows(\n                 ForbiddenException.class,\n                 () -> defaultErrorHandler.accept(generateErrorResponse(403)));\n+        assertThrows(\n+                NoSuchResourceException.class,\n+                () -> defaultErrorHandler.accept(generateErrorResponse(404)));\n         assertThrows(\n                 RESTException.class, () -> defaultErrorHandler.accept(generateErrorResponse(405)));\n         assertThrows(\n                 RESTException.class, () -> defaultErrorHandler.accept(generateErrorResponse(406)));\n+        assertThrows(\n+                AlreadyExistsException.class,\n+                () -> defaultErrorHandler.accept(generateErrorResponse(409)));\n         assertThrows(\n                 ServiceFailureException.class,\n                 () -> defaultErrorHandler.accept(generateErrorResponse(500)));\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\nindex 17c13b932fd2..f12af12a9d35 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/HttpClientTest.java\n@@ -110,6 +110,20 @@ public void testPostFail() {\n         verify(errorHandler, times(1)).accept(any());\n     }\n \n+    @Test\n+    public void testDeleteSuccess() {\n+        mockHttpCallWithCode(mockResponseDataStr, 200);\n+        MockRESTData response = httpClient.delete(MOCK_PATH, mockResponseData, headers);\n+        verify(errorHandler, times(0)).accept(any());\n+    }\n+\n+    @Test\n+    public void testDeleteFail() {\n+        mockHttpCallWithCode(mockResponseDataStr, 400);\n+        httpClient.delete(MOCK_PATH, mockResponseData, headers);\n+        verify(errorHandler, times(1)).accept(any());\n+    }\n+\n     private Map<String, String> headers(String token) {\n         Map<String, String> header = new HashMap<>();\n         header.put(\""Authorization\"", \""Bearer \"" + token);\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\nnew file mode 100644\nindex 000000000000..f111c41f6ada\n--- /dev/null\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/MockRESTMessage.java\n@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.rest;\n+\n+import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.DropDatabaseRequest;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.DatabaseName;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.paimon.rest.RESTCatalogInternalOptions.DATABASE_COMMENT;\n+\n+/** Mock REST message. */\n+public class MockRESTMessage {\n+\n+    public static String databaseName() {\n+        return \""database\"";\n+    }\n+\n+    public static CreateDatabaseRequest createDatabaseRequest(String name) {\n+        boolean ignoreIfExists = true;\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""a\"", \""b\"");\n+        return new CreateDatabaseRequest(name, ignoreIfExists, options);\n+    }\n+\n+    public static DropDatabaseRequest dropDatabaseRequest() {\n+        boolean ignoreIfNotExists = true;\n+        boolean cascade = true;\n+        return new DropDatabaseRequest(ignoreIfNotExists, cascade);\n+    }\n+\n+    public static CreateDatabaseResponse createDatabaseResponse(String name) {\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""a\"", \""b\"");\n+        return new CreateDatabaseResponse(name, options);\n+    }\n+\n+    public static GetDatabaseResponse getDatabaseResponse(String name) {\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""a\"", \""b\"");\n+        options.put(DATABASE_COMMENT.key(), \""comment\"");\n+        return new GetDatabaseResponse(name, options);\n+    }\n+\n+    public static ListDatabasesResponse listDatabasesResponse(String name) {\n+        DatabaseName databaseName = new DatabaseName(name);\n+        List<DatabaseName> databaseNameList = new ArrayList<>();\n+        databaseNameList.add(databaseName);\n+        return new ListDatabasesResponse(databaseNameList);\n+    }\n+}\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\nindex f3f56e97215f..cffac6046623 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTCatalogTest.java\n@@ -18,8 +18,15 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.catalog.Database;\n import org.apache.paimon.options.CatalogOptions;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n+\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.core.JsonProcessingException;\n+import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n import okhttp3.mockwebserver.MockResponse;\n import okhttp3.mockwebserver.MockWebServer;\n@@ -29,14 +36,17 @@\n \n import java.io.IOException;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n \n /** Test for REST Catalog. */\n public class RESTCatalogTest {\n \n+    private ObjectMapper mapper = RESTObjectMapper.create();\n     private MockWebServer mockWebServer;\n     private RESTCatalog restCatalog;\n \n@@ -50,7 +60,11 @@ public void setUp() throws IOException {\n         String initToken = \""init_token\"";\n         options.set(RESTCatalogOptions.TOKEN, initToken);\n         options.set(RESTCatalogOptions.THREAD_POOL_SIZE, 1);\n-        mockOptions(RESTCatalogInternalOptions.PREFIX.key(), \""prefix\"");\n+        String mockResponse =\n+                String.format(\n+                        \""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\""}}\"",\n+                        RESTCatalogInternalOptions.PREFIX.key(), \""prefix\"");\n+        mockResponse(mockResponse);\n         restCatalog = new RESTCatalog(options);\n     }\n \n@@ -70,14 +84,50 @@ public void testInitFailWhenDefineWarehouse() {\n     public void testGetConfig() {\n         String key = \""a\"";\n         String value = \""b\"";\n-        mockOptions(key, value);\n+        String mockResponse = String.format(\""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\""}}\"", key, value);\n+        mockResponse(mockResponse);\n         Map<String, String> header = new HashMap<>();\n         Map<String, String> response = restCatalog.fetchOptionsFromServer(header, new HashMap<>());\n         assertEquals(value, response.get(key));\n     }\n \n-    private void mockOptions(String key, String value) {\n-        String mockResponse = String.format(\""{\\\""defaults\\\"": {\\\""%s\\\"": \\\""%s\\\""}}\"", key, value);\n+    @Test\n+    public void testListDatabases() throws JsonProcessingException {\n+        String name = MockRESTMessage.databaseName();\n+        ListDatabasesResponse response = MockRESTMessage.listDatabasesResponse(name);\n+        mockResponse(mapper.writeValueAsString(response));\n+        List<String> result = restCatalog.listDatabases();\n+        assertEquals(response.getDatabases().size(), result.size());\n+        assertEquals(name, result.get(0));\n+    }\n+\n+    @Test\n+    public void testCreateDatabase() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        CreateDatabaseResponse response = MockRESTMessage.createDatabaseResponse(name);\n+        mockResponse(mapper.writeValueAsString(response));\n+        assertDoesNotThrow(() -> restCatalog.createDatabase(name, false, response.getOptions()));\n+    }\n+\n+    @Test\n+    public void testGetDatabase() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        GetDatabaseResponse response = MockRESTMessage.getDatabaseResponse(name);\n+        mockResponse(mapper.writeValueAsString(response));\n+        Database result = restCatalog.getDatabase(name);\n+        assertEquals(name, result.name());\n+        assertEquals(response.getOptions().size(), result.options().size());\n+        assertEquals(response.comment().get(), result.comment().get());\n+    }\n+\n+    @Test\n+    public void testDropDatabase() {\n+        String name = \""name\"";\n+        mockResponse(\""\"");\n+        assertDoesNotThrow(() -> restCatalog.dropDatabase(name, false, false));\n+    }\n+\n+    private void mockResponse(String mockResponse) {\n         MockResponse mockResponseObj =\n                 new MockResponse()\n                         .setBody(mockResponse)\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\nindex 83a8805d29a0..622a98993692 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/RESTObjectMapperTest.java\n@@ -18,8 +18,13 @@\n \n package org.apache.paimon.rest;\n \n+import org.apache.paimon.rest.requests.CreateDatabaseRequest;\n+import org.apache.paimon.rest.requests.DropDatabaseRequest;\n import org.apache.paimon.rest.responses.ConfigResponse;\n+import org.apache.paimon.rest.responses.CreateDatabaseResponse;\n import org.apache.paimon.rest.responses.ErrorResponse;\n+import org.apache.paimon.rest.responses.GetDatabaseResponse;\n+import org.apache.paimon.rest.responses.ListDatabasesResponse;\n \n import org.apache.paimon.shade.jackson2.com.fasterxml.jackson.databind.ObjectMapper;\n \n@@ -43,7 +48,7 @@ public void configResponseParseTest() throws Exception {\n         ConfigResponse response = new ConfigResponse(conf, conf);\n         String responseStr = mapper.writeValueAsString(response);\n         ConfigResponse parseData = mapper.readValue(responseStr, ConfigResponse.class);\n-        assertEquals(conf.get(confKey), parseData.defaults().get(confKey));\n+        assertEquals(conf.get(confKey), parseData.getDefaults().get(confKey));\n     }\n \n     @Test\n@@ -53,7 +58,60 @@ public void errorResponseParseTest() throws Exception {\n         ErrorResponse response = new ErrorResponse(message, code, new ArrayList<String>());\n         String responseStr = mapper.writeValueAsString(response);\n         ErrorResponse parseData = mapper.readValue(responseStr, ErrorResponse.class);\n-        assertEquals(message, parseData.message());\n-        assertEquals(code, parseData.code());\n+        assertEquals(message, parseData.getMessage());\n+        assertEquals(code, parseData.getCode());\n+    }\n+\n+    @Test\n+    public void createDatabaseRequestParseTest() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        CreateDatabaseRequest request = MockRESTMessage.createDatabaseRequest(name);\n+        String requestStr = mapper.writeValueAsString(request);\n+        CreateDatabaseRequest parseData = mapper.readValue(requestStr, CreateDatabaseRequest.class);\n+        assertEquals(request.getName(), parseData.getName());\n+        assertEquals(request.getIgnoreIfExists(), parseData.getIgnoreIfExists());\n+        assertEquals(request.getOptions().size(), parseData.getOptions().size());\n+    }\n+\n+    @Test\n+    public void dropDatabaseRequestParseTest() throws Exception {\n+        DropDatabaseRequest request = MockRESTMessage.dropDatabaseRequest();\n+        String requestStr = mapper.writeValueAsString(request);\n+        DropDatabaseRequest parseData = mapper.readValue(requestStr, DropDatabaseRequest.class);\n+        assertEquals(request.getIgnoreIfNotExists(), parseData.getIgnoreIfNotExists());\n+        assertEquals(request.getCascade(), parseData.getCascade());\n+    }\n+\n+    @Test\n+    public void createDatabaseResponseParseTest() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        CreateDatabaseResponse response = MockRESTMessage.createDatabaseResponse(name);\n+        String responseStr = mapper.writeValueAsString(response);\n+        CreateDatabaseResponse parseData =\n+                mapper.readValue(responseStr, CreateDatabaseResponse.class);\n+        assertEquals(name, parseData.getName());\n+        assertEquals(response.getOptions().size(), parseData.getOptions().size());\n+    }\n+\n+    @Test\n+    public void getDatabaseResponseParseTest() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        GetDatabaseResponse response = MockRESTMessage.getDatabaseResponse(name);\n+        String responseStr = mapper.writeValueAsString(response);\n+        GetDatabaseResponse parseData = mapper.readValue(responseStr, GetDatabaseResponse.class);\n+        assertEquals(name, parseData.getName());\n+        assertEquals(response.getOptions().size(), parseData.getOptions().size());\n+        assertEquals(response.comment().get(), parseData.comment().get());\n+    }\n+\n+    @Test\n+    public void listDatabaseResponseParseTest() throws Exception {\n+        String name = MockRESTMessage.databaseName();\n+        ListDatabasesResponse response = MockRESTMessage.listDatabasesResponse(name);\n+        String responseStr = mapper.writeValueAsString(response);\n+        ListDatabasesResponse parseData =\n+                mapper.readValue(responseStr, ListDatabasesResponse.class);\n+        assertEquals(response.getDatabases().size(), parseData.getDatabases().size());\n+        assertEquals(name, parseData.getDatabases().get(0).getName());\n     }\n }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\nindex 81b3ea57b703..1f4a48fd5e8c 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/rest/auth/AuthSessionTest.java\n@@ -35,6 +35,8 @@\n import java.util.UUID;\n import java.util.concurrent.ScheduledExecutorService;\n \n+import static org.apache.paimon.rest.auth.AuthSession.MAX_REFRESH_WINDOW_MILLIS;\n+import static org.apache.paimon.rest.auth.AuthSession.MIN_REFRESH_WAIT_MILLIS;\n import static org.apache.paimon.rest.auth.AuthSession.TOKEN_REFRESH_NUM_RETRIES;\n import static org.junit.Assert.assertEquals;\n import static org.mockito.Mockito.verify;\n@@ -121,6 +123,22 @@ public void testRetryWhenRefreshFail() throws Exception {\n         verify(credentialsProvider, Mockito.times(TOKEN_REFRESH_NUM_RETRIES + 1)).refresh();\n     }\n \n+    @Test\n+    public void testGetTimeToWaitByExpiresInMills() {\n+        long expiresInMillis = -100L;\n+        long timeToWait = AuthSession.getTimeToWaitByExpiresInMills(expiresInMillis);\n+        assertEquals(MIN_REFRESH_WAIT_MILLIS, timeToWait);\n+        expiresInMillis = (long) (MAX_REFRESH_WINDOW_MILLIS * 0.5);\n+        timeToWait = AuthSession.getTimeToWaitByExpiresInMills(expiresInMillis);\n+        assertEquals(MIN_REFRESH_WAIT_MILLIS, timeToWait);\n+        expiresInMillis = MAX_REFRESH_WINDOW_MILLIS;\n+        timeToWait = AuthSession.getTimeToWaitByExpiresInMills(expiresInMillis);\n+        assertEquals(timeToWait, MIN_REFRESH_WAIT_MILLIS);\n+        expiresInMillis = MAX_REFRESH_WINDOW_MILLIS * 2L;\n+        timeToWait = AuthSession.getTimeToWaitByExpiresInMills(expiresInMillis);\n+        assertEquals(timeToWait, MAX_REFRESH_WINDOW_MILLIS);\n+    }\n+\n     private Pair<File, String> generateTokenAndWriteToFile(String fileName) throws IOException {\n         File tokenFile = folder.newFile(fileName);\n         String token = UUID.randomUUID().toString();\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4614"", ""pr_id"": 4614, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 11, ""non_test_files_count"": 27, ""pr_changed_files"": [""paimon-flink/paimon-flink-1.20/pom.xml"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SynchronizationActionBase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSource.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSourceFunction.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiAwareBucketTableScan.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiTableScanBase.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiUnawareBucketTableScan.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/UnawareBucketCompactionTopoBuilder.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryFileMonitor.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSourceReader.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BucketUnawareCompactSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/CombinedTableCompactorSourceBuilder.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumState.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumStateSerializer.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumerator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplit.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplitSerializer.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SplitListState.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiTablesReadOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiUnawareTablesReadOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/ReadOperator.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/TestingSourceOperator.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSource.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSourceFunction.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/TestingSourceOperator.java""], ""base_commit"": ""4bf2d9b01d0176bdf52e68dd996cbe79f4304d14"", ""head_commit"": ""681f7bb4379cf8599fea77cdaeeaa4eeffa9efb1"", ""repo_url"": ""https://github.com/apache/paimon/pull/4614"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4614"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-13T09:10:56.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-1.20/pom.xml b/paimon-flink/paimon-flink-1.20/pom.xml\nindex 7cf1d8e98df7..f15792d2bea9 100644\n--- a/paimon-flink/paimon-flink-1.20/pom.xml\n+++ b/paimon-flink/paimon-flink-1.20/pom.xml\n@@ -55,6 +55,20 @@ under the License.\n                 </exclusion>\n             </exclusions>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.flink</groupId>\n+            <artifactId>flink-streaming-java</artifactId>\n+            <version>${flink.version}</version>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.flink</groupId>\n+            <artifactId>flink-table-common</artifactId>\n+            <version>${flink.version}</version>\n+            <scope>provided</scope>\n+        </dependency>\n     </dependencies>\n \n     <build>\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SynchronizationActionBase.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SynchronizationActionBase.java\nindex f103396389e5..a7c770347410 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SynchronizationActionBase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/SynchronizationActionBase.java\n@@ -40,7 +40,6 @@\n import org.apache.flink.configuration.ExecutionOptions;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n-import org.apache.flink.streaming.api.functions.source.SourceFunction;\n \n import java.time.Duration;\n import java.util.HashMap;\n@@ -131,7 +130,7 @@ public void build() throws Exception {\n \n     protected void beforeBuildingSourceSink() throws Exception {}\n \n-    protected Object buildSource() {\n+    protected Source<CdcSourceRecord, ?, ?> buildSource() {\n         return syncJobHandler.provideSource();\n     }\n \n@@ -147,41 +146,32 @@ protected void validateRuntimeExecutionMode() {\n                 \""It's only support STREAMING mode for flink-cdc sync table action.\"");\n     }\n \n-    private DataStreamSource<CdcSourceRecord> buildDataStreamSource(Object source) {\n-        if (source instanceof Source) {\n-            boolean isAutomaticWatermarkCreationEnabled =\n-                    tableConfig.containsKey(CoreOptions.TAG_AUTOMATIC_CREATION.key())\n-                            && Objects.equals(\n-                                    tableConfig.get(CoreOptions.TAG_AUTOMATIC_CREATION.key()),\n-                                    WATERMARK.toString());\n-\n-            Options options = Options.fromMap(tableConfig);\n-            Duration idleTimeout = options.get(SCAN_WATERMARK_IDLE_TIMEOUT);\n-            String watermarkAlignGroup = options.get(SCAN_WATERMARK_ALIGNMENT_GROUP);\n-            WatermarkStrategy<CdcSourceRecord> watermarkStrategy =\n-                    isAutomaticWatermarkCreationEnabled\n-                            ? watermarkAlignGroup != null\n-                                    ? new CdcWatermarkStrategy(createCdcTimestampExtractor())\n-                                            .withWatermarkAlignment(\n-                                                    watermarkAlignGroup,\n-                                                    options.get(SCAN_WATERMARK_ALIGNMENT_MAX_DRIFT),\n-                                                    options.get(\n-                                                            SCAN_WATERMARK_ALIGNMENT_UPDATE_INTERVAL))\n-                                    : new CdcWatermarkStrategy(createCdcTimestampExtractor())\n-                            : WatermarkStrategy.noWatermarks();\n-            if (idleTimeout != null) {\n-                watermarkStrategy = watermarkStrategy.withIdleness(idleTimeout);\n-            }\n-            return env.fromSource(\n-                    (Source<CdcSourceRecord, ?, ?>) source,\n-                    watermarkStrategy,\n-                    syncJobHandler.provideSourceName());\n+    private DataStreamSource<CdcSourceRecord> buildDataStreamSource(\n+            Source<CdcSourceRecord, ?, ?> source) {\n+        boolean isAutomaticWatermarkCreationEnabled =\n+                tableConfig.containsKey(CoreOptions.TAG_AUTOMATIC_CREATION.key())\n+                        && Objects.equals(\n+                                tableConfig.get(CoreOptions.TAG_AUTOMATIC_CREATION.key()),\n+                                WATERMARK.toString());\n+\n+        Options options = Options.fromMap(tableConfig);\n+        Duration idleTimeout = options.get(SCAN_WATERMARK_IDLE_TIMEOUT);\n+        String watermarkAlignGroup = options.get(SCAN_WATERMARK_ALIGNMENT_GROUP);\n+        WatermarkStrategy<CdcSourceRecord> watermarkStrategy =\n+                isAutomaticWatermarkCreationEnabled\n+                        ? watermarkAlignGroup != null\n+                                ? new CdcWatermarkStrategy(createCdcTimestampExtractor())\n+                                        .withWatermarkAlignment(\n+                                                watermarkAlignGroup,\n+                                                options.get(SCAN_WATERMARK_ALIGNMENT_MAX_DRIFT),\n+                                                options.get(\n+                                                        SCAN_WATERMARK_ALIGNMENT_UPDATE_INTERVAL))\n+                                : new CdcWatermarkStrategy(createCdcTimestampExtractor())\n+                        : WatermarkStrategy.noWatermarks();\n+        if (idleTimeout != null) {\n+            watermarkStrategy = watermarkStrategy.withIdleness(idleTimeout);\n         }\n-        if (source instanceof SourceFunction) {\n-            return env.addSource(\n-                    (SourceFunction<CdcSourceRecord>) source, syncJobHandler.provideSourceName());\n-        }\n-        throw new UnsupportedOperationException(\""Unrecognized source type\"");\n+        return env.fromSource(source, watermarkStrategy, syncJobHandler.provideSourceName());\n     }\n \n     protected abstract FlatMapFunction<CdcSourceRecord, RichCdcMultiplexRecord> recordParse();\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiAwareBucketTableScan.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiAwareBucketTableScan.java\nindex 747995d20d67..88730132ef68 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiAwareBucketTableScan.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiAwareBucketTableScan.java\n@@ -32,7 +32,6 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n@@ -52,15 +51,8 @@ public MultiAwareBucketTableScan(\n             Pattern includingPattern,\n             Pattern excludingPattern,\n             Pattern databasePattern,\n-            boolean isStreaming,\n-            AtomicBoolean isRunning) {\n-        super(\n-                catalogLoader,\n-                includingPattern,\n-                excludingPattern,\n-                databasePattern,\n-                isStreaming,\n-                isRunning);\n+            boolean isStreaming) {\n+        super(catalogLoader, includingPattern, excludingPattern, databasePattern, isStreaming);\n         tablesMap = new HashMap<>();\n         scansMap = new HashMap<>();\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiTableScanBase.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiTableScanBase.java\nindex bd4ffe83a4ca..f5940740b691 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiTableScanBase.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiTableScanBase.java\n@@ -26,12 +26,11 @@\n import org.apache.paimon.table.source.EndOfScanException;\n import org.apache.paimon.table.source.Split;\n \n-import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.util.List;\n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.regex.Pattern;\n \n import static org.apache.paimon.flink.utils.MultiTablesCompactorUtil.shouldCompactTable;\n@@ -57,7 +56,6 @@ public abstract class MultiTableScanBase<T> implements AutoCloseable {\n \n     protected transient Catalog catalog;\n \n-    protected AtomicBoolean isRunning;\n     protected boolean isStreaming;\n \n     public MultiTableScanBase(\n@@ -65,14 +63,12 @@ public MultiTableScanBase(\n             Pattern includingPattern,\n             Pattern excludingPattern,\n             Pattern databasePattern,\n-            boolean isStreaming,\n-            AtomicBoolean isRunning) {\n+            boolean isStreaming) {\n         catalog = catalogLoader.load();\n \n         this.includingPattern = includingPattern;\n         this.excludingPattern = excludingPattern;\n         this.databasePattern = databasePattern;\n-        this.isRunning = isRunning;\n         this.isStreaming = isStreaming;\n     }\n \n@@ -104,13 +100,9 @@ protected void updateTableMap()\n         }\n     }\n \n-    public ScanResult scanTable(SourceFunction.SourceContext<T> ctx)\n+    public ScanResult scanTable(ReaderOutput<T> ctx)\n             throws Catalog.TableNotExistException, Catalog.DatabaseNotExistException {\n         try {\n-            if (!isRunning.get()) {\n-                return ScanResult.FINISHED;\n-            }\n-\n             updateTableMap();\n             List<T> tasks = doScan();\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiUnawareBucketTableScan.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiUnawareBucketTableScan.java\nindex 56bf971240e7..da86b93af512 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiUnawareBucketTableScan.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/MultiUnawareBucketTableScan.java\n@@ -29,7 +29,6 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.regex.Pattern;\n \n /**\n@@ -46,15 +45,8 @@ public MultiUnawareBucketTableScan(\n             Pattern includingPattern,\n             Pattern excludingPattern,\n             Pattern databasePattern,\n-            boolean isStreaming,\n-            AtomicBoolean isRunning) {\n-        super(\n-                catalogLoader,\n-                includingPattern,\n-                excludingPattern,\n-                databasePattern,\n-                isStreaming,\n-                isRunning);\n+            boolean isStreaming) {\n+        super(catalogLoader, includingPattern, excludingPattern, databasePattern, isStreaming);\n         tablesMap = new HashMap<>();\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/UnawareBucketCompactionTopoBuilder.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/UnawareBucketCompactionTopoBuilder.java\nindex 8c6ed4c9f59e..a572354e8984 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/UnawareBucketCompactionTopoBuilder.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/UnawareBucketCompactionTopoBuilder.java\n@@ -126,7 +126,7 @@ private DataStreamSource<UnawareAppendCompactionTask> buildSource() {\n                 new BucketUnawareCompactSource(\n                         table, isContinuous, scanInterval, partitionPredicate);\n \n-        return BucketUnawareCompactSource.buildSource(env, source, isContinuous, tableIdentifier);\n+        return BucketUnawareCompactSource.buildSource(env, source, tableIdentifier);\n     }\n \n     private void sinkFromSource(DataStreamSource<UnawareAppendCompactionTask> input) {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryFileMonitor.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryFileMonitor.java\nindex b9776786fa57..6688503778a0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryFileMonitor.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryFileMonitor.java\n@@ -21,6 +21,9 @@\n import org.apache.paimon.CoreOptions;\n import org.apache.paimon.data.BinaryRow;\n import org.apache.paimon.data.InternalRow;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n import org.apache.paimon.flink.utils.InternalTypeInfo;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.table.FileStoreTable;\n@@ -31,11 +34,14 @@\n import org.apache.paimon.table.source.TableRead;\n import org.apache.paimon.table.system.FileMonitorTable;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n \n import java.util.ArrayList;\n import java.util.List;\n@@ -50,19 +56,13 @@\n  *   <li>Assigning them to downstream tasks for further processing.\n  * </ol>\n  */\n-public class QueryFileMonitor extends RichSourceFunction<InternalRow> {\n+public class QueryFileMonitor extends AbstractNonCoordinatedSource<InternalRow> {\n \n     private static final long serialVersionUID = 1L;\n \n     private final Table table;\n     private final long monitorInterval;\n \n-    private transient SourceContext<InternalRow> ctx;\n-    private transient StreamTableScan scan;\n-    private transient TableRead read;\n-\n-    private volatile boolean isRunning = true;\n-\n     public QueryFileMonitor(Table table) {\n         this.table = table;\n         this.monitorInterval =\n@@ -71,65 +71,54 @@ public QueryFileMonitor(Table table) {\n                         .toMillis();\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public Boundedness getBoundedness() {\n+        return Boundedness.CONTINUOUS_UNBOUNDED;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        FileMonitorTable monitorTable = new FileMonitorTable((FileStoreTable) table);\n-        ReadBuilder readBuilder = monitorTable.newReadBuilder().dropStats();\n-        this.scan = readBuilder.newStreamScan();\n-        this.read = readBuilder.newRead();\n+    @Override\n+    public SourceReader<InternalRow, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n     }\n \n-    @Override\n-    public void run(SourceContext<InternalRow> ctx) throws Exception {\n-        this.ctx = ctx;\n-        while (isRunning) {\n-            boolean isEmpty;\n-            synchronized (ctx.getCheckpointLock()) {\n-                if (!isRunning) {\n-                    return;\n-                }\n-                isEmpty = doScan();\n-            }\n+    private class Reader extends AbstractNonCoordinatedSourceReader<InternalRow> {\n+        private transient StreamTableScan scan;\n+        private transient TableRead read;\n+\n+        @Override\n+        public void start() {\n+            FileMonitorTable monitorTable = new FileMonitorTable((FileStoreTable) table);\n+            ReadBuilder readBuilder = monitorTable.newReadBuilder().dropStats();\n+            this.scan = readBuilder.newStreamScan();\n+            this.read = readBuilder.newRead();\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<InternalRow> readerOutput) throws Exception {\n+            boolean isEmpty = doScan(readerOutput);\n \n             if (isEmpty) {\n                 Thread.sleep(monitorInterval);\n             }\n+            return InputStatus.MORE_AVAILABLE;\n         }\n-    }\n \n-    private boolean doScan() throws Exception {\n-        List<InternalRow> records = new ArrayList<>();\n-        read.createReader(scan.plan()).forEachRemaining(records::add);\n-        records.forEach(ctx::collect);\n-        return records.isEmpty();\n-    }\n-\n-    @Override\n-    public void cancel() {\n-        // this is to cover the case where cancel() is called before the run()\n-        if (ctx != null) {\n-            synchronized (ctx.getCheckpointLock()) {\n-                isRunning = false;\n-            }\n-        } else {\n-            isRunning = false;\n+        private boolean doScan(ReaderOutput<InternalRow> readerOutput) throws Exception {\n+            List<InternalRow> records = new ArrayList<>();\n+            read.createReader(scan.plan()).forEachRemaining(records::add);\n+            records.forEach(readerOutput::collect);\n+            return records.isEmpty();\n         }\n     }\n \n     public static DataStream<InternalRow> build(StreamExecutionEnvironment env, Table table) {\n-        return env.addSource(\n-                new QueryFileMonitor(table),\n-                \""FileMonitor-\"" + table.name(),\n-                InternalTypeInfo.fromRowType(FileMonitorTable.getRowType()));\n+        return env.fromSource(\n+                        new QueryFileMonitor(table),\n+                        WatermarkStrategy.noWatermarks(),\n+                        \""FileMonitor-\"" + table.name(),\n+                        InternalTypeInfo.fromRowType(FileMonitorTable.getRowType()))\n+                .setParallelism(1);\n     }\n \n     public static ChannelComputer<InternalRow> createChannelComputer() {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSource.java\nnew file mode 100644\nindex 000000000000..a9a389e837a2\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSource.java\n@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.api.connector.source.Source;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+/** {@link Source} that does not require coordination between JobManager and TaskManagers. */\n+public abstract class AbstractNonCoordinatedSource<T>\n+        implements Source<T, SimpleSourceSplit, NoOpEnumState> {\n+    @Override\n+    public SplitEnumerator<SimpleSourceSplit, NoOpEnumState> createEnumerator(\n+            SplitEnumeratorContext<SimpleSourceSplit> enumContext) {\n+        return new NoOpEnumerator<>();\n+    }\n+\n+    @Override\n+    public SplitEnumerator<SimpleSourceSplit, NoOpEnumState> restoreEnumerator(\n+            SplitEnumeratorContext<SimpleSourceSplit> enumContext, NoOpEnumState checkpoint) {\n+        return new NoOpEnumerator<>();\n+    }\n+\n+    @Override\n+    public SimpleVersionedSerializer<SimpleSourceSplit> getSplitSerializer() {\n+        return new SimpleSourceSplitSerializer();\n+    }\n+\n+    @Override\n+    public SimpleVersionedSerializer<NoOpEnumState> getEnumeratorCheckpointSerializer() {\n+        return new NoOpEnumStateSerializer();\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSourceReader.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSourceReader.java\nnew file mode 100644\nindex 000000000000..18c278868ffa\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AbstractNonCoordinatedSourceReader.java\n@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.api.connector.source.SourceReader;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+/** Abstract {@link SourceReader} for {@link AbstractNonCoordinatedSource}. */\n+public abstract class AbstractNonCoordinatedSourceReader<T>\n+        implements SourceReader<T, SimpleSourceSplit> {\n+    @Override\n+    public void start() {}\n+\n+    @Override\n+    public List<SimpleSourceSplit> snapshotState(long l) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> isAvailable() {\n+        return CompletableFuture.completedFuture(null);\n+    }\n+\n+    @Override\n+    public void addSplits(List<SimpleSourceSplit> list) {}\n+\n+    @Override\n+    public void notifyNoMoreSplits() {}\n+\n+    @Override\n+    public void close() throws Exception {}\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BucketUnawareCompactSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BucketUnawareCompactSource.java\nindex 79ee827fe6e4..7954aad2df0a 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BucketUnawareCompactSource.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/BucketUnawareCompactSource.java\n@@ -21,19 +21,19 @@\n import org.apache.paimon.append.UnawareAppendCompactionTask;\n import org.apache.paimon.append.UnawareAppendTableCompactionCoordinator;\n import org.apache.paimon.flink.sink.CompactionTaskTypeInfo;\n-import org.apache.paimon.flink.utils.RuntimeContextUtils;\n import org.apache.paimon.predicate.Predicate;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.source.EndOfScanException;\n-import org.apache.paimon.utils.Preconditions;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.api.connector.source.Boundedness;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.util.Preconditions;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -42,15 +42,16 @@\n import java.util.List;\n \n /**\n- * Source Function for unaware-bucket Compaction.\n+ * Source for unaware-bucket Compaction.\n  *\n- * <p>Note: The function is the source function of unaware-bucket compactor coordinator. It will\n- * read the latest snapshot continuously by compactionCoordinator, and generate new compaction\n- * tasks. The source function is used in unaware-bucket compaction job (both stand-alone and\n- * write-combined). Besides, we don't need to save state in this function, it will invoke a full\n- * scan when starting up, and scan continuously for the following snapshot.\n+ * <p>Note: The function is the source of unaware-bucket compactor coordinator. It will read the\n+ * latest snapshot continuously by compactionCoordinator, and generate new compaction tasks. The\n+ * source is used in unaware-bucket compaction job (both stand-alone and write-combined). Besides,\n+ * we don't need to save state in this source, it will invoke a full scan when starting up, and scan\n+ * continuously for the following snapshot.\n  */\n-public class BucketUnawareCompactSource extends RichSourceFunction<UnawareAppendCompactionTask> {\n+public class BucketUnawareCompactSource\n+        extends AbstractNonCoordinatedSource<UnawareAppendCompactionTask> {\n \n     private static final Logger LOG = LoggerFactory.getLogger(BucketUnawareCompactSource.class);\n     private static final String COMPACTION_COORDINATOR_NAME = \""Compaction Coordinator\"";\n@@ -59,9 +60,6 @@ public class BucketUnawareCompactSource extends RichSourceFunction<UnawareAppend\n     private final boolean streaming;\n     private final long scanInterval;\n     private final Predicate filter;\n-    private transient UnawareAppendTableCompactionCoordinator compactionCoordinator;\n-    private transient SourceContext<UnawareAppendCompactionTask> ctx;\n-    private volatile boolean isRunning = true;\n \n     public BucketUnawareCompactSource(\n             FileStoreTable table,\n@@ -74,76 +72,64 @@ public BucketUnawareCompactSource(\n         this.filter = filter;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public Boundedness getBoundedness() {\n+        return streaming ? Boundedness.CONTINUOUS_UNBOUNDED : Boundedness.BOUNDED;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        compactionCoordinator =\n-                new UnawareAppendTableCompactionCoordinator(table, streaming, filter);\n+    @Override\n+    public SourceReader<UnawareAppendCompactionTask, SimpleSourceSplit> createReader(\n+            SourceReaderContext readerContext) throws Exception {\n         Preconditions.checkArgument(\n-                RuntimeContextUtils.getNumberOfParallelSubtasks(getRuntimeContext()) == 1,\n+                readerContext.currentParallelism() == 1,\n                 \""Compaction Operator parallelism in paimon MUST be one.\"");\n+        return new BucketUnawareCompactSourceReader(table, streaming, filter, scanInterval);\n     }\n \n-    @Override\n-    public void run(SourceContext<UnawareAppendCompactionTask> sourceContext) throws Exception {\n-        this.ctx = sourceContext;\n-        while (isRunning) {\n+    /** BucketUnawareCompactSourceReader. */\n+    public static class BucketUnawareCompactSourceReader\n+            extends AbstractNonCoordinatedSourceReader<UnawareAppendCompactionTask> {\n+        private final UnawareAppendTableCompactionCoordinator compactionCoordinator;\n+        private final long scanInterval;\n+\n+        public BucketUnawareCompactSourceReader(\n+                FileStoreTable table, boolean streaming, Predicate filter, long scanInterval) {\n+            this.scanInterval = scanInterval;\n+            compactionCoordinator =\n+                    new UnawareAppendTableCompactionCoordinator(table, streaming, filter);\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<UnawareAppendCompactionTask> readerOutput)\n+                throws Exception {\n             boolean isEmpty;\n-            synchronized (ctx.getCheckpointLock()) {\n-                if (!isRunning) {\n-                    return;\n-                }\n-                try {\n-                    // do scan and plan action, emit append-only compaction tasks.\n-                    List<UnawareAppendCompactionTask> tasks = compactionCoordinator.run();\n-                    isEmpty = tasks.isEmpty();\n-                    tasks.forEach(ctx::collect);\n-                } catch (EndOfScanException esf) {\n-                    LOG.info(\""Catching EndOfStreamException, the stream is finished.\"");\n-                    return;\n-                }\n+            try {\n+                // do scan and plan action, emit append-only compaction tasks.\n+                List<UnawareAppendCompactionTask> tasks = compactionCoordinator.run();\n+                isEmpty = tasks.isEmpty();\n+                tasks.forEach(readerOutput::collect);\n+            } catch (EndOfScanException esf) {\n+                LOG.info(\""Catching EndOfStreamException, the stream is finished.\"");\n+                return InputStatus.END_OF_INPUT;\n             }\n \n             if (isEmpty) {\n                 Thread.sleep(scanInterval);\n             }\n-        }\n-    }\n-\n-    @Override\n-    public void cancel() {\n-        if (ctx != null) {\n-            synchronized (ctx.getCheckpointLock()) {\n-                isRunning = false;\n-            }\n-        } else {\n-            isRunning = false;\n+            return InputStatus.MORE_AVAILABLE;\n         }\n     }\n \n     public static DataStreamSource<UnawareAppendCompactionTask> buildSource(\n             StreamExecutionEnvironment env,\n             BucketUnawareCompactSource source,\n-            boolean streaming,\n             String tableIdentifier) {\n-        final StreamSource<UnawareAppendCompactionTask, BucketUnawareCompactSource> sourceOperator =\n-                new StreamSource<>(source);\n         return (DataStreamSource<UnawareAppendCompactionTask>)\n-                new DataStreamSource<>(\n-                                env,\n-                                new CompactionTaskTypeInfo(),\n-                                sourceOperator,\n-                                false,\n+                env.fromSource(\n+                                source,\n+                                WatermarkStrategy.noWatermarks(),\n                                 COMPACTION_COORDINATOR_NAME + \"" : \"" + tableIdentifier,\n-                                streaming ? Boundedness.CONTINUOUS_UNBOUNDED : Boundedness.BOUNDED)\n+                                new CompactionTaskTypeInfo())\n                         .setParallelism(1)\n                         .setMaxParallelism(1);\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/CombinedTableCompactorSourceBuilder.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/CombinedTableCompactorSourceBuilder.java\nindex e5cbbe845ceb..415eddb037df 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/CombinedTableCompactorSourceBuilder.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/CombinedTableCompactorSourceBuilder.java\n@@ -21,10 +21,10 @@\n import org.apache.paimon.append.MultiTableUnawareAppendCompactionTask;\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.flink.LogicalTypeConversion;\n-import org.apache.paimon.flink.source.operator.CombinedAwareBatchSourceFunction;\n-import org.apache.paimon.flink.source.operator.CombinedAwareStreamingSourceFunction;\n-import org.apache.paimon.flink.source.operator.CombinedUnawareBatchSourceFunction;\n-import org.apache.paimon.flink.source.operator.CombinedUnawareStreamingSourceFunction;\n+import org.apache.paimon.flink.source.operator.CombinedAwareBatchSource;\n+import org.apache.paimon.flink.source.operator.CombinedAwareStreamingSource;\n+import org.apache.paimon.flink.source.operator.CombinedUnawareBatchSource;\n+import org.apache.paimon.flink.source.operator.CombinedUnawareStreamingSource;\n import org.apache.paimon.table.system.CompactBucketsTable;\n import org.apache.paimon.types.RowType;\n import org.apache.paimon.utils.Preconditions;\n@@ -87,7 +87,7 @@ public DataStream<RowData> buildAwareBucketTableSource() {\n         Preconditions.checkArgument(env != null, \""StreamExecutionEnvironment should not be null.\"");\n         RowType produceType = CompactBucketsTable.getRowType();\n         if (isContinuous) {\n-            return CombinedAwareStreamingSourceFunction.buildSource(\n+            return CombinedAwareStreamingSource.buildSource(\n                     env,\n                     \""Combine-MultiBucketTables--StreamingCompactorSource\"",\n                     InternalTypeInfo.of(LogicalTypeConversion.toLogicalType(produceType)),\n@@ -97,7 +97,7 @@ public DataStream<RowData> buildAwareBucketTableSource() {\n                     databasePattern,\n                     monitorInterval);\n         } else {\n-            return CombinedAwareBatchSourceFunction.buildSource(\n+            return CombinedAwareBatchSource.buildSource(\n                     env,\n                     \""Combine-MultiBucketTables-BatchCompactorSource\"",\n                     InternalTypeInfo.of(LogicalTypeConversion.toLogicalType(produceType)),\n@@ -112,7 +112,7 @@ public DataStream<RowData> buildAwareBucketTableSource() {\n     public DataStream<MultiTableUnawareAppendCompactionTask> buildForUnawareBucketsTableSource() {\n         Preconditions.checkArgument(env != null, \""StreamExecutionEnvironment should not be null.\"");\n         if (isContinuous) {\n-            return CombinedUnawareStreamingSourceFunction.buildSource(\n+            return CombinedUnawareStreamingSource.buildSource(\n                     env,\n                     \""Combined-UnawareBucketTables-StreamingCompactorSource\"",\n                     catalogLoader,\n@@ -121,7 +121,7 @@ public DataStream<MultiTableUnawareAppendCompactionTask> buildForUnawareBucketsT\n                     databasePattern,\n                     monitorInterval);\n         } else {\n-            return CombinedUnawareBatchSourceFunction.buildSource(\n+            return CombinedUnawareBatchSource.buildSource(\n                     env,\n                     \""Combined-UnawareBucketTables-BatchCompactorSource\"",\n                     catalogLoader,\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\nindex e864ec050045..b85d5274b241 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\n@@ -26,7 +26,7 @@\n import org.apache.paimon.flink.log.LogSourceProvider;\n import org.apache.paimon.flink.sink.FlinkSink;\n import org.apache.paimon.flink.source.align.AlignedContinuousFileStoreSource;\n-import org.apache.paimon.flink.source.operator.MonitorFunction;\n+import org.apache.paimon.flink.source.operator.MonitorSource;\n import org.apache.paimon.flink.utils.TableScanUtils;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.predicate.Predicate;\n@@ -306,7 +306,7 @@ private DataStream<RowData> buildContinuousStreamOperator() {\n                     \""Cannot limit streaming source, please use batch execution mode.\"");\n         }\n         dataStream =\n-                MonitorFunction.buildSource(\n+                MonitorSource.buildSource(\n                         env,\n                         sourceName,\n                         produceTypeInfo(),\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumState.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumState.java\nnew file mode 100644\nindex 000000000000..f07317c155aa\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumState.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+/** The enumerator state class for {@link NoOpEnumerator}. */\n+public class NoOpEnumState {}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumStateSerializer.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumStateSerializer.java\nnew file mode 100644\nindex 000000000000..89c0ad6ac1f1\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumStateSerializer.java\n@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.IOException;\n+\n+/** {@link SimpleVersionedSerializer} for {@link NoOpEnumState}. */\n+public class NoOpEnumStateSerializer implements SimpleVersionedSerializer<NoOpEnumState> {\n+    @Override\n+    public int getVersion() {\n+        return 0;\n+    }\n+\n+    @Override\n+    public byte[] serialize(NoOpEnumState obj) throws IOException {\n+        return new byte[0];\n+    }\n+\n+    @Override\n+    public NoOpEnumState deserialize(int version, byte[] serialized) throws IOException {\n+        return new NoOpEnumState();\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumerator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumerator.java\nnew file mode 100644\nindex 000000000000..f29c6d6db76d\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/NoOpEnumerator.java\n@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.api.connector.source.SourceSplit;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * A {@link SplitEnumerator} that provides no functionality. It is basically used for sources that\n+ * does not require a coordinator.\n+ */\n+public class NoOpEnumerator<SplitT extends SourceSplit>\n+        implements SplitEnumerator<SplitT, NoOpEnumState> {\n+    @Override\n+    public void start() {}\n+\n+    @Override\n+    public void handleSplitRequest(int subtaskId, @Nullable String requesterHostname) {}\n+\n+    @Override\n+    public void addSplitsBack(List<SplitT> splits, int subtaskId) {}\n+\n+    @Override\n+    public void addReader(int subtaskId) {}\n+\n+    @Override\n+    public NoOpEnumState snapshotState(long checkpointId) throws Exception {\n+        return new NoOpEnumState();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {}\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplit.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplit.java\nnew file mode 100644\nindex 000000000000..2db0868f8e34\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplit.java\n@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.api.connector.source.SourceSplit;\n+\n+import java.util.UUID;\n+\n+/** A {@link SourceSplit} that provides basic information through splitId. */\n+public class SimpleSourceSplit implements SourceSplit {\n+    private final String splitId;\n+    private final String value;\n+\n+    public SimpleSourceSplit() {\n+        this(\""\"");\n+    }\n+\n+    public SimpleSourceSplit(String value) {\n+        this(UUID.randomUUID().toString(), value);\n+    }\n+\n+    public SimpleSourceSplit(String splitId, String value) {\n+        this.splitId = splitId;\n+        this.value = value;\n+    }\n+\n+    @Override\n+    public String splitId() {\n+        return splitId;\n+    }\n+\n+    public String value() {\n+        return value;\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplitSerializer.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplitSerializer.java\nnew file mode 100644\nindex 000000000000..3387afed1c2a\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SimpleSourceSplitSerializer.java\n@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+/** {@link SimpleVersionedSerializer} for {@link SimpleSourceSplit}. */\n+public class SimpleSourceSplitSerializer implements SimpleVersionedSerializer<SimpleSourceSplit> {\n+\n+    @Override\n+    public int getVersion() {\n+        return 0;\n+    }\n+\n+    @Override\n+    public byte[] serialize(SimpleSourceSplit split) throws IOException {\n+        if (split.splitId() == null) {\n+            return new byte[0];\n+        }\n+\n+        try (final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+                final DataOutputStream out = new DataOutputStream(baos)) {\n+            writeString(out, split.splitId());\n+            writeString(out, split.value());\n+            return baos.toByteArray();\n+        }\n+    }\n+\n+    @Override\n+    public SimpleSourceSplit deserialize(int version, byte[] serialized) throws IOException {\n+        if (serialized.length == 0) {\n+            return new SimpleSourceSplit();\n+        }\n+\n+        try (final ByteArrayInputStream bais = new ByteArrayInputStream(serialized);\n+                final DataInputStream in = new DataInputStream(bais)) {\n+            String splitId = readString(in);\n+            String value = readString(in);\n+            return new SimpleSourceSplit(splitId, value);\n+        }\n+    }\n+\n+    private void writeString(DataOutputStream out, String str) throws IOException {\n+        byte[] bytes = str.getBytes();\n+        out.writeInt(bytes.length);\n+        out.write(str.getBytes());\n+    }\n+\n+    private String readString(DataInputStream in) throws IOException {\n+        int length = in.readInt();\n+        byte[] bytes = new byte[length];\n+        in.readFully(bytes);\n+        return new String(bytes);\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SplitListState.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SplitListState.java\nnew file mode 100644\nindex 000000000000..0049bdf284e3\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/SplitListState.java\n@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source;\n+\n+import org.apache.paimon.utils.Preconditions;\n+\n+import org.apache.flink.api.common.state.ListState;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Utility class to provide {@link ListState}-like experience for sources that use {@link\n+ * SimpleSourceSplit}.\n+ */\n+public class SplitListState<T> implements ListState<T> {\n+    private final String splitPrefix;\n+    private final List<T> values;\n+    private final Function<T, String> serializer;\n+    private final Function<String, T> deserializer;\n+\n+    public SplitListState(\n+            String identifier, Function<T, String> serializer, Function<String, T> deserializer) {\n+        Preconditions.checkArgument(\n+                !Character.isDigit(identifier.charAt(0)),\n+                String.format(\""Identifier %s should not start with digits.\"", identifier));\n+        this.splitPrefix = identifier.length() + identifier;\n+        this.serializer = serializer;\n+        this.deserializer = deserializer;\n+        this.values = new ArrayList<>();\n+    }\n+\n+    @Override\n+    public void add(T value) {\n+        values.add(value);\n+    }\n+\n+    @Override\n+    public List<T> get() {\n+        return new ArrayList<>(values);\n+    }\n+\n+    @Override\n+    public void update(List<T> values) {\n+        this.values.clear();\n+        this.values.addAll(values);\n+    }\n+\n+    @Override\n+    public void addAll(List<T> values) throws Exception {\n+        this.values.addAll(values);\n+    }\n+\n+    @Override\n+    public void clear() {\n+        values.clear();\n+    }\n+\n+    public List<SimpleSourceSplit> snapshotState() {\n+        return values.stream()\n+                .map(x -> new SimpleSourceSplit(splitPrefix + serializer.apply(x)))\n+                .collect(Collectors.toList());\n+    }\n+\n+    public void restoreState(List<SimpleSourceSplit> splits) {\n+        values.clear();\n+        splits.stream()\n+                .map(SimpleSourceSplit::value)\n+                .filter(x -> x.startsWith(splitPrefix))\n+                .map(x -> x.substring(splitPrefix.length()))\n+                .map(this.deserializer)\n+                .forEach(values::add);\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSourceFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSource.java\nsimilarity index 66%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSourceFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSource.java\nindex 2157be51aee4..c3a1258bb176 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSourceFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareBatchSource.java\n@@ -21,21 +21,23 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.flink.compact.MultiAwareBucketTableScan;\n import org.apache.paimon.flink.compact.MultiTableScanBase;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n import org.apache.paimon.flink.utils.JavaTypeInfo;\n import org.apache.paimon.table.source.DataSplit;\n import org.apache.paimon.table.source.Split;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n import org.apache.flink.table.data.RowData;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -47,15 +49,11 @@\n import static org.apache.paimon.flink.compact.MultiTableScanBase.ScanResult.IS_EMPTY;\n \n /** It is responsible for monitoring compactor source of aware bucket table in batch mode. */\n-public class CombinedAwareBatchSourceFunction\n-        extends CombinedCompactorSourceFunction<Tuple2<Split, String>> {\n+public class CombinedAwareBatchSource extends CombinedCompactorSource<Tuple2<Split, String>> {\n \n-    private static final Logger LOGGER =\n-            LoggerFactory.getLogger(CombinedAwareBatchSourceFunction.class);\n+    private static final Logger LOGGER = LoggerFactory.getLogger(CombinedAwareBatchSource.class);\n \n-    private MultiTableScanBase<Tuple2<Split, String>> tableScan;\n-\n-    public CombinedAwareBatchSourceFunction(\n+    public CombinedAwareBatchSource(\n             Catalog.Loader catalogLoader,\n             Pattern includingPattern,\n             Pattern excludingPattern,\n@@ -63,34 +61,33 @@ public CombinedAwareBatchSourceFunction(\n         super(catalogLoader, includingPattern, excludingPattern, databasePattern, false);\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public SourceReader<Tuple2<Split, String>, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        super.open(parameters);\n-        tableScan =\n-                new MultiAwareBucketTableScan(\n-                        catalogLoader,\n-                        includingPattern,\n-                        excludingPattern,\n-                        databasePattern,\n-                        isStreaming,\n-                        isRunning);\n-    }\n+    private class Reader extends AbstractNonCoordinatedSourceReader<Tuple2<Split, String>> {\n+        private MultiTableScanBase<Tuple2<Split, String>> tableScan;\n \n-    @Override\n-    void scanTable() throws Exception {\n-        if (isRunning.get()) {\n-            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(ctx);\n+        @Override\n+        public void start() {\n+            super.start();\n+            tableScan =\n+                    new MultiAwareBucketTableScan(\n+                            catalogLoader,\n+                            includingPattern,\n+                            excludingPattern,\n+                            databasePattern,\n+                            isStreaming);\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<Tuple2<Split, String>> readerOutput)\n+                throws Exception {\n+            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(readerOutput);\n             if (scanResult == FINISHED) {\n-                return;\n+                return InputStatus.END_OF_INPUT;\n             }\n             if (scanResult == IS_EMPTY) {\n                 // Currently, in the combined mode, there are two scan tasks for the table of two\n@@ -99,6 +96,15 @@ void scanTable() throws Exception {\n                 // should not be thrown exception here.\n                 LOGGER.info(\""No file were collected for the table of aware-bucket\"");\n             }\n+            return InputStatus.END_OF_INPUT;\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            super.close();\n+            if (tableScan != null) {\n+                tableScan.close();\n+            }\n         }\n     }\n \n@@ -111,15 +117,14 @@ public static DataStream<RowData> buildSource(\n             Pattern excludingPattern,\n             Pattern databasePattern,\n             Duration partitionIdleTime) {\n-        CombinedAwareBatchSourceFunction function =\n-                new CombinedAwareBatchSourceFunction(\n+        CombinedAwareBatchSource source =\n+                new CombinedAwareBatchSource(\n                         catalogLoader, includingPattern, excludingPattern, databasePattern);\n-        StreamSource<Tuple2<Split, String>, ?> sourceOperator = new StreamSource<>(function);\n         TupleTypeInfo<Tuple2<Split, String>> tupleTypeInfo =\n                 new TupleTypeInfo<>(\n                         new JavaTypeInfo<>(Split.class), BasicTypeInfo.STRING_TYPE_INFO);\n-        return new DataStreamSource<>(\n-                        env, tupleTypeInfo, sourceOperator, false, name, Boundedness.BOUNDED)\n+\n+        return env.fromSource(source, WatermarkStrategy.noWatermarks(), name, tupleTypeInfo)\n                 .forceNonParallel()\n                 .partitionCustom(\n                         (key, numPartitions) -> key % numPartitions,\n@@ -129,12 +134,4 @@ public static DataStream<RowData> buildSource(\n                         typeInfo,\n                         new MultiTablesReadOperator(catalogLoader, false, partitionIdleTime));\n     }\n-\n-    @Override\n-    public void close() throws Exception {\n-        super.close();\n-        if (tableScan != null) {\n-            tableScan.close();\n-        }\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSourceFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSource.java\nsimilarity index 63%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSourceFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSource.java\nindex 01e0127e9fda..9bd4a84f571c 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSourceFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedAwareStreamingSource.java\n@@ -21,21 +21,23 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.flink.compact.MultiAwareBucketTableScan;\n import org.apache.paimon.flink.compact.MultiTableScanBase;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n import org.apache.paimon.flink.utils.JavaTypeInfo;\n import org.apache.paimon.table.source.DataSplit;\n import org.apache.paimon.table.source.Split;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n import org.apache.flink.table.data.RowData;\n \n import java.util.regex.Pattern;\n@@ -44,13 +46,11 @@\n import static org.apache.paimon.flink.compact.MultiTableScanBase.ScanResult.IS_EMPTY;\n \n /** It is responsible for monitoring compactor source of multi bucket table in stream mode. */\n-public class CombinedAwareStreamingSourceFunction\n-        extends CombinedCompactorSourceFunction<Tuple2<Split, String>> {\n+public class CombinedAwareStreamingSource extends CombinedCompactorSource<Tuple2<Split, String>> {\n \n     private final long monitorInterval;\n-    private transient MultiTableScanBase<Tuple2<Split, String>> tableScan;\n \n-    public CombinedAwareStreamingSourceFunction(\n+    public CombinedAwareStreamingSource(\n             Catalog.Loader catalogLoader,\n             Pattern includingPattern,\n             Pattern excludingPattern,\n@@ -60,39 +60,46 @@ public CombinedAwareStreamingSourceFunction(\n         this.monitorInterval = monitorInterval;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public SourceReader<Tuple2<Split, String>, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        super.open(parameters);\n-        tableScan =\n-                new MultiAwareBucketTableScan(\n-                        catalogLoader,\n-                        includingPattern,\n-                        excludingPattern,\n-                        databasePattern,\n-                        isStreaming,\n-                        isRunning);\n-    }\n+    private class Reader extends AbstractNonCoordinatedSourceReader<Tuple2<Split, String>> {\n+        private transient MultiTableScanBase<Tuple2<Split, String>> tableScan;\n \n-    @SuppressWarnings(\""BusyWait\"")\n-    @Override\n-    void scanTable() throws Exception {\n-        while (isRunning.get()) {\n-            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(ctx);\n+        @Override\n+        public void start() {\n+            super.start();\n+            tableScan =\n+                    new MultiAwareBucketTableScan(\n+                            catalogLoader,\n+                            includingPattern,\n+                            excludingPattern,\n+                            databasePattern,\n+                            isStreaming);\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<Tuple2<Split, String>> readerOutput)\n+                throws Exception {\n+            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(readerOutput);\n             if (scanResult == FINISHED) {\n-                return;\n+                return InputStatus.END_OF_INPUT;\n             }\n             if (scanResult == IS_EMPTY) {\n                 Thread.sleep(monitorInterval);\n             }\n+            return InputStatus.MORE_AVAILABLE;\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            super.close();\n+            if (tableScan != null) {\n+                tableScan.close();\n+            }\n         }\n     }\n \n@@ -106,37 +113,22 @@ public static DataStream<RowData> buildSource(\n             Pattern databasePattern,\n             long monitorInterval) {\n \n-        CombinedAwareStreamingSourceFunction function =\n-                new CombinedAwareStreamingSourceFunction(\n+        CombinedAwareStreamingSource source =\n+                new CombinedAwareStreamingSource(\n                         catalogLoader,\n                         includingPattern,\n                         excludingPattern,\n                         databasePattern,\n                         monitorInterval);\n-        StreamSource<Tuple2<Split, String>, ?> sourceOperator = new StreamSource<>(function);\n-        boolean isParallel = false;\n         TupleTypeInfo<Tuple2<Split, String>> tupleTypeInfo =\n                 new TupleTypeInfo<>(\n                         new JavaTypeInfo<>(Split.class), BasicTypeInfo.STRING_TYPE_INFO);\n-        return new DataStreamSource<>(\n-                        env,\n-                        tupleTypeInfo,\n-                        sourceOperator,\n-                        isParallel,\n-                        name,\n-                        Boundedness.CONTINUOUS_UNBOUNDED)\n+\n+        return env.fromSource(source, WatermarkStrategy.noWatermarks(), name, tupleTypeInfo)\n                 .forceNonParallel()\n                 .partitionCustom(\n                         (key, numPartitions) -> key % numPartitions,\n                         split -> ((DataSplit) split.f0).bucket())\n                 .transform(name, typeInfo, new MultiTablesReadOperator(catalogLoader, true));\n     }\n-\n-    @Override\n-    public void close() throws Exception {\n-        super.close();\n-        if (tableScan != null) {\n-            tableScan.close();\n-        }\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSourceFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSource.java\nsimilarity index 63%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSourceFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSource.java\nindex 02bb8786505d..f58d86cdd65e 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSourceFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedCompactorSource.java\n@@ -20,13 +20,11 @@\n \n import org.apache.paimon.append.UnawareAppendCompactionTask;\n import org.apache.paimon.catalog.Catalog;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n import org.apache.paimon.table.source.Split;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.flink.api.connector.source.Boundedness;\n \n-import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.regex.Pattern;\n \n /**\n@@ -45,8 +43,7 @@\n  * <p>Currently, only dedicated compaction job for multi-tables rely on this monitor. This is the\n  * single (non-parallel) monitoring task, it is responsible for the new Paimon table.\n  */\n-public abstract class CombinedCompactorSourceFunction<T> extends RichSourceFunction<T> {\n-\n+public abstract class CombinedCompactorSource<T> extends AbstractNonCoordinatedSource<T> {\n     private static final long serialVersionUID = 2L;\n \n     protected final Catalog.Loader catalogLoader;\n@@ -55,10 +52,7 @@ public abstract class CombinedCompactorSourceFunction<T> extends RichSourceFunct\n     protected final Pattern databasePattern;\n     protected final boolean isStreaming;\n \n-    protected transient AtomicBoolean isRunning;\n-    protected transient SourceContext<T> ctx;\n-\n-    public CombinedCompactorSourceFunction(\n+    public CombinedCompactorSource(\n             Catalog.Loader catalogLoader,\n             Pattern includingPattern,\n             Pattern excludingPattern,\n@@ -71,37 +65,8 @@ public CombinedCompactorSourceFunction(\n         this.isStreaming = isStreaming;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n-    }\n-\n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        isRunning = new AtomicBoolean(true);\n-    }\n-\n     @Override\n-    public void run(SourceContext<T> sourceContext) throws Exception {\n-        this.ctx = sourceContext;\n-        scanTable();\n+    public Boundedness getBoundedness() {\n+        return isStreaming ? Boundedness.CONTINUOUS_UNBOUNDED : Boundedness.BOUNDED;\n     }\n-\n-    @Override\n-    public void cancel() {\n-        // this is to cover the case where cancel() is called before the run()\n-        if (ctx != null) {\n-            synchronized (ctx.getCheckpointLock()) {\n-                isRunning.set(false);\n-            }\n-        } else {\n-            isRunning.set(false);\n-        }\n-    }\n-\n-    abstract void scanTable() throws Exception;\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSourceFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSource.java\nsimilarity index 71%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSourceFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSource.java\nindex 6a40f10ada61..64f0c38f5a11 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSourceFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareBatchSource.java\n@@ -25,18 +25,20 @@\n import org.apache.paimon.flink.compact.MultiTableScanBase;\n import org.apache.paimon.flink.compact.MultiUnawareBucketTableScan;\n import org.apache.paimon.flink.sink.MultiTableCompactionTaskTypeInfo;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.Table;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n-import org.apache.flink.api.connector.source.Boundedness;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -55,14 +57,12 @@\n  * It is responsible for the batch compactor source of the table with unaware bucket in combined\n  * mode.\n  */\n-public class CombinedUnawareBatchSourceFunction\n-        extends CombinedCompactorSourceFunction<MultiTableUnawareAppendCompactionTask> {\n+public class CombinedUnawareBatchSource\n+        extends CombinedCompactorSource<MultiTableUnawareAppendCompactionTask> {\n \n-    private static final Logger LOGGER =\n-            LoggerFactory.getLogger(CombinedUnawareBatchSourceFunction.class);\n-    private transient MultiTableScanBase<MultiTableUnawareAppendCompactionTask> tableScan;\n+    private static final Logger LOGGER = LoggerFactory.getLogger(CombinedUnawareBatchSource.class);\n \n-    public CombinedUnawareBatchSourceFunction(\n+    public CombinedUnawareBatchSource(\n             Catalog.Loader catalogLoader,\n             Pattern includingPattern,\n             Pattern excludingPattern,\n@@ -70,34 +70,34 @@ public CombinedUnawareBatchSourceFunction(\n         super(catalogLoader, includingPattern, excludingPattern, databasePattern, false);\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public SourceReader<MultiTableUnawareAppendCompactionTask, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        super.open(parameters);\n-        tableScan =\n-                new MultiUnawareBucketTableScan(\n-                        catalogLoader,\n-                        includingPattern,\n-                        excludingPattern,\n-                        databasePattern,\n-                        isStreaming,\n-                        isRunning);\n-    }\n+    private class Reader\n+            extends AbstractNonCoordinatedSourceReader<MultiTableUnawareAppendCompactionTask> {\n+        private transient MultiTableScanBase<MultiTableUnawareAppendCompactionTask> tableScan;\n+\n+        @Override\n+        public void start() {\n+            super.start();\n+            tableScan =\n+                    new MultiUnawareBucketTableScan(\n+                            catalogLoader,\n+                            includingPattern,\n+                            excludingPattern,\n+                            databasePattern,\n+                            isStreaming);\n+        }\n \n-    @Override\n-    void scanTable() throws Exception {\n-        if (isRunning.get()) {\n-            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(ctx);\n+        @Override\n+        public InputStatus pollNext(\n+                ReaderOutput<MultiTableUnawareAppendCompactionTask> readerOutput) throws Exception {\n+            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(readerOutput);\n             if (scanResult == FINISHED) {\n-                return;\n+                return InputStatus.END_OF_INPUT;\n             }\n             if (scanResult == IS_EMPTY) {\n                 // Currently, in the combined mode, there are two scan tasks for the table of two\n@@ -106,6 +106,15 @@ void scanTable() throws Exception {\n                 // should not be thrown exception here.\n                 LOGGER.info(\""No file were collected for the table of unaware-bucket\"");\n             }\n+            return InputStatus.END_OF_INPUT;\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            super.close();\n+            if (tableScan != null) {\n+                tableScan.close();\n+            }\n         }\n     }\n \n@@ -117,22 +126,18 @@ public static DataStream<MultiTableUnawareAppendCompactionTask> buildSource(\n             Pattern excludingPattern,\n             Pattern databasePattern,\n             @Nullable Duration partitionIdleTime) {\n-        CombinedUnawareBatchSourceFunction function =\n-                new CombinedUnawareBatchSourceFunction(\n+        CombinedUnawareBatchSource combinedUnawareBatchSource =\n+                new CombinedUnawareBatchSource(\n                         catalogLoader, includingPattern, excludingPattern, databasePattern);\n-        StreamSource<MultiTableUnawareAppendCompactionTask, CombinedUnawareBatchSourceFunction>\n-                sourceOperator = new StreamSource<>(function);\n         MultiTableCompactionTaskTypeInfo compactionTaskTypeInfo =\n                 new MultiTableCompactionTaskTypeInfo();\n \n         SingleOutputStreamOperator<MultiTableUnawareAppendCompactionTask> source =\n-                new DataStreamSource<>(\n-                                env,\n-                                compactionTaskTypeInfo,\n-                                sourceOperator,\n-                                false,\n+                env.fromSource(\n+                                combinedUnawareBatchSource,\n+                                WatermarkStrategy.noWatermarks(),\n                                 name,\n-                                Boundedness.BOUNDED)\n+                                compactionTaskTypeInfo)\n                         .forceNonParallel();\n \n         if (partitionIdleTime != null) {\n@@ -177,12 +182,4 @@ private static Long getPartitionInfo(\n         }\n         return partitionInfo.get(partition);\n     }\n-\n-    @Override\n-    public void close() throws Exception {\n-        super.close();\n-        if (tableScan != null) {\n-            tableScan.close();\n-        }\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSourceFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSource.java\nsimilarity index 57%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSourceFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSource.java\nindex b64518a7ef60..6ea1ead4db30 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSourceFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/CombinedUnawareStreamingSource.java\n@@ -23,14 +23,16 @@\n import org.apache.paimon.flink.compact.MultiTableScanBase;\n import org.apache.paimon.flink.compact.MultiUnawareBucketTableScan;\n import org.apache.paimon.flink.sink.MultiTableCompactionTaskTypeInfo;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n-import org.apache.flink.api.connector.source.Boundedness;\n-import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n \n import java.util.regex.Pattern;\n \n@@ -40,13 +42,12 @@\n /**\n  * It is responsible for monitoring compactor source in stream mode for the table of unaware bucket.\n  */\n-public class CombinedUnawareStreamingSourceFunction\n-        extends CombinedCompactorSourceFunction<MultiTableUnawareAppendCompactionTask> {\n+public class CombinedUnawareStreamingSource\n+        extends CombinedCompactorSource<MultiTableUnawareAppendCompactionTask> {\n \n     private final long monitorInterval;\n-    private MultiTableScanBase<MultiTableUnawareAppendCompactionTask> tableScan;\n \n-    public CombinedUnawareStreamingSourceFunction(\n+    public CombinedUnawareStreamingSource(\n             Catalog.Loader catalogLoader,\n             Pattern includingPattern,\n             Pattern excludingPattern,\n@@ -56,39 +57,47 @@ public CombinedUnawareStreamingSourceFunction(\n         this.monitorInterval = monitorInterval;\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n-     */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    @Override\n+    public SourceReader<MultiTableUnawareAppendCompactionTask, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n     }\n \n-    /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n-     */\n-    public void open(Configuration parameters) throws Exception {\n-        super.open(parameters);\n-        tableScan =\n-                new MultiUnawareBucketTableScan(\n-                        catalogLoader,\n-                        includingPattern,\n-                        excludingPattern,\n-                        databasePattern,\n-                        isStreaming,\n-                        isRunning);\n-    }\n+    private class Reader\n+            extends AbstractNonCoordinatedSourceReader<MultiTableUnawareAppendCompactionTask> {\n+        private MultiTableScanBase<MultiTableUnawareAppendCompactionTask> tableScan;\n \n-    @SuppressWarnings(\""BusyWait\"")\n-    @Override\n-    void scanTable() throws Exception {\n-        while (isRunning.get()) {\n-            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(ctx);\n+        @Override\n+        public void start() {\n+            super.start();\n+            tableScan =\n+                    new MultiUnawareBucketTableScan(\n+                            catalogLoader,\n+                            includingPattern,\n+                            excludingPattern,\n+                            databasePattern,\n+                            isStreaming);\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(\n+                ReaderOutput<MultiTableUnawareAppendCompactionTask> readerOutput) throws Exception {\n+            MultiTableScanBase.ScanResult scanResult = tableScan.scanTable(readerOutput);\n             if (scanResult == FINISHED) {\n-                return;\n+                return InputStatus.END_OF_INPUT;\n             }\n             if (scanResult == IS_EMPTY) {\n                 Thread.sleep(monitorInterval);\n             }\n+            return InputStatus.MORE_AVAILABLE;\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            super.close();\n+            if (tableScan != null) {\n+                tableScan.close();\n+            }\n         }\n     }\n \n@@ -101,33 +110,18 @@ public static DataStream<MultiTableUnawareAppendCompactionTask> buildSource(\n             Pattern databasePattern,\n             long monitorInterval) {\n \n-        CombinedUnawareStreamingSourceFunction function =\n-                new CombinedUnawareStreamingSourceFunction(\n+        CombinedUnawareStreamingSource source =\n+                new CombinedUnawareStreamingSource(\n                         catalogLoader,\n                         includingPattern,\n                         excludingPattern,\n                         databasePattern,\n                         monitorInterval);\n-        StreamSource<MultiTableUnawareAppendCompactionTask, CombinedUnawareStreamingSourceFunction>\n-                sourceOperator = new StreamSource<>(function);\n-        boolean isParallel = false;\n         MultiTableCompactionTaskTypeInfo compactionTaskTypeInfo =\n                 new MultiTableCompactionTaskTypeInfo();\n-        return new DataStreamSource<>(\n-                        env,\n-                        compactionTaskTypeInfo,\n-                        sourceOperator,\n-                        isParallel,\n-                        name,\n-                        Boundedness.CONTINUOUS_UNBOUNDED)\n-                .forceNonParallel();\n-    }\n \n-    @Override\n-    public void close() throws Exception {\n-        super.close();\n-        if (tableScan != null) {\n-            tableScan.close();\n-        }\n+        return env.fromSource(\n+                        source, WatermarkStrategy.noWatermarks(), name, compactionTaskTypeInfo)\n+                .forceNonParallel();\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorFunction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorSource.java\nsimilarity index 53%\nrename from paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorFunction.java\nrename to paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorSource.java\nindex 3805f6f8c536..4ec0a4f99d9f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorFunction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MonitorSource.java\n@@ -18,6 +18,10 @@\n \n package org.apache.paimon.flink.source.operator;\n \n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n+import org.apache.paimon.flink.source.SplitListState;\n import org.apache.paimon.flink.utils.JavaTypeInfo;\n import org.apache.paimon.table.BucketMode;\n import org.apache.paimon.table.sink.ChannelComputer;\n@@ -27,22 +31,18 @@\n import org.apache.paimon.table.source.Split;\n import org.apache.paimon.table.source.StreamTableScan;\n \n-import org.apache.flink.api.common.state.CheckpointListener;\n-import org.apache.flink.api.common.state.ListState;\n-import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.eventtime.Watermark;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.common.typeutils.TypeSerializer;\n-import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n-import org.apache.flink.runtime.state.FunctionInitializationContext;\n-import org.apache.flink.runtime.state.FunctionSnapshotContext;\n-import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n-import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.util.Preconditions;\n import org.slf4j.Logger;\n@@ -71,33 +71,23 @@\n  * <p>Currently, there are two features that rely on this monitor:\n  *\n  * <ol>\n- *   <li>Consumer-id: rely on this function to do aligned snapshot consumption, and ensure that all\n+ *   <li>Consumer-id: rely on this source to do aligned snapshot consumption, and ensure that all\n  *       data in a snapshot is consumed within each checkpoint.\n  *   <li>Snapshot-watermark: when there is no watermark definition, the default Paimon table will\n  *       pass the watermark recorded in the snapshot.\n  * </ol>\n  */\n-public class MonitorFunction extends RichSourceFunction<Split>\n-        implements CheckpointedFunction, CheckpointListener {\n+public class MonitorSource extends AbstractNonCoordinatedSource<Split> {\n \n     private static final long serialVersionUID = 1L;\n \n-    private static final Logger LOG = LoggerFactory.getLogger(MonitorFunction.class);\n+    private static final Logger LOG = LoggerFactory.getLogger(MonitorSource.class);\n \n     private final ReadBuilder readBuilder;\n     private final long monitorInterval;\n     private final boolean emitSnapshotWatermark;\n \n-    private volatile boolean isRunning = true;\n-\n-    private transient StreamTableScan scan;\n-    private transient SourceContext<Split> ctx;\n-\n-    private transient ListState<Long> checkpointState;\n-    private transient ListState<Tuple2<Long, Long>> nextSnapshotState;\n-    private transient TreeMap<Long, Long> nextSnapshotPerCheckpoint;\n-\n-    public MonitorFunction(\n+    public MonitorSource(\n             ReadBuilder readBuilder, long monitorInterval, boolean emitSnapshotWatermark) {\n         this.readBuilder = readBuilder;\n         this.monitorInterval = monitorInterval;\n@@ -105,40 +95,74 @@ public MonitorFunction(\n     }\n \n     @Override\n-    public void initializeState(FunctionInitializationContext context) throws Exception {\n-        this.scan = readBuilder.newStreamScan();\n-\n-        this.checkpointState =\n-                context.getOperatorStateStore()\n-                        .getListState(\n-                                new ListStateDescriptor<>(\n-                                        \""next-snapshot\"", LongSerializer.INSTANCE));\n-\n-        @SuppressWarnings(\""unchecked\"")\n-        final Class<Tuple2<Long, Long>> typedTuple =\n-                (Class<Tuple2<Long, Long>>) (Class<?>) Tuple2.class;\n-        this.nextSnapshotState =\n-                context.getOperatorStateStore()\n-                        .getListState(\n-                                new ListStateDescriptor<>(\n-                                        \""next-snapshot-per-checkpoint\"",\n-                                        new TupleSerializer<>(\n-                                                typedTuple,\n-                                                new TypeSerializer[] {\n-                                                    LongSerializer.INSTANCE, LongSerializer.INSTANCE\n-                                                })));\n-\n-        this.nextSnapshotPerCheckpoint = new TreeMap<>();\n-\n-        if (context.isRestored()) {\n-            LOG.info(\""Restoring state for the {}.\"", getClass().getSimpleName());\n+    public Boundedness getBoundedness() {\n+        return Boundedness.CONTINUOUS_UNBOUNDED;\n+    }\n+\n+    @Override\n+    public SourceReader<Split, SimpleSourceSplit> createReader(\n+            SourceReaderContext sourceReaderContext) throws Exception {\n+        return new Reader();\n+    }\n+\n+    private class Reader extends AbstractNonCoordinatedSourceReader<Split> {\n+        private static final String CHECKPOINT_STATE = \""CS\"";\n+        private static final String NEXT_SNAPSHOT_STATE = \""NSS\"";\n+\n+        private final StreamTableScan scan = readBuilder.newStreamScan();\n+        private final SplitListState<Long> checkpointState =\n+                new SplitListState<>(CHECKPOINT_STATE, x -> Long.toString(x), Long::parseLong);\n+        private final SplitListState<Tuple2<Long, Long>> nextSnapshotState =\n+                new SplitListState<>(\n+                        NEXT_SNAPSHOT_STATE,\n+                        x -> x.f0 + \"":\"" + x.f1,\n+                        x ->\n+                                Tuple2.of(\n+                                        Long.parseLong(x.split(\"":\"")[0]),\n+                                        Long.parseLong(x.split(\"":\"")[1])));\n+        private final TreeMap<Long, Long> nextSnapshotPerCheckpoint = new TreeMap<>();\n+\n+        @Override\n+        public void notifyCheckpointComplete(long checkpointId) {\n+            NavigableMap<Long, Long> nextSnapshots =\n+                    nextSnapshotPerCheckpoint.headMap(checkpointId, true);\n+            OptionalLong max = nextSnapshots.values().stream().mapToLong(Long::longValue).max();\n+            max.ifPresent(scan::notifyCheckpointComplete);\n+            nextSnapshots.clear();\n+        }\n \n-            List<Long> retrievedStates = new ArrayList<>();\n-            for (Long entry : this.checkpointState.get()) {\n-                retrievedStates.add(entry);\n+        @Override\n+        public List<SimpleSourceSplit> snapshotState(long checkpointId) {\n+            this.checkpointState.clear();\n+            Long nextSnapshot = this.scan.checkpoint();\n+            if (nextSnapshot != null) {\n+                this.checkpointState.add(nextSnapshot);\n+                this.nextSnapshotPerCheckpoint.put(checkpointId, nextSnapshot);\n             }\n \n-            // given that the parallelism of the function is 1, we can only have 1 retrieved items.\n+            List<Tuple2<Long, Long>> nextSnapshots = new ArrayList<>();\n+            this.nextSnapshotPerCheckpoint.forEach((k, v) -> nextSnapshots.add(new Tuple2<>(k, v)));\n+            this.nextSnapshotState.update(nextSnapshots);\n+\n+            if (LOG.isDebugEnabled()) {\n+                LOG.debug(\""{} checkpoint {}.\"", getClass().getSimpleName(), nextSnapshot);\n+            }\n+\n+            List<SimpleSourceSplit> results = new ArrayList<>();\n+            results.addAll(checkpointState.snapshotState());\n+            results.addAll(nextSnapshotState.snapshotState());\n+            return results;\n+        }\n+\n+        @Override\n+        public void addSplits(List<SimpleSourceSplit> list) {\n+            LOG.info(\""Restoring state for the {}.\"", getClass().getSimpleName());\n+            checkpointState.restoreState(list);\n+            nextSnapshotState.restoreState(list);\n+\n+            List<Long> retrievedStates = checkpointState.get();\n+\n+            // given that the parallelism of the source is 1, we can only have 1 retrieved items.\n             Preconditions.checkArgument(\n                     retrievedStates.size() <= 1,\n                     getClass().getSimpleName() + \"" retrieved invalid state.\"");\n@@ -150,80 +174,31 @@ public void initializeState(FunctionInitializationContext context) throws Except\n             for (Tuple2<Long, Long> tuple2 : nextSnapshotState.get()) {\n                 nextSnapshotPerCheckpoint.put(tuple2.f0, tuple2.f1);\n             }\n-        } else {\n-            LOG.info(\""No state to restore for the {}.\"", getClass().getSimpleName());\n         }\n-    }\n-\n-    @Override\n-    public void snapshotState(FunctionSnapshotContext ctx) throws Exception {\n-        this.checkpointState.clear();\n-        Long nextSnapshot = this.scan.checkpoint();\n-        if (nextSnapshot != null) {\n-            this.checkpointState.add(nextSnapshot);\n-            this.nextSnapshotPerCheckpoint.put(ctx.getCheckpointId(), nextSnapshot);\n-        }\n-\n-        List<Tuple2<Long, Long>> nextSnapshots = new ArrayList<>();\n-        this.nextSnapshotPerCheckpoint.forEach((k, v) -> nextSnapshots.add(new Tuple2<>(k, v)));\n-        this.nextSnapshotState.update(nextSnapshots);\n \n-        if (LOG.isDebugEnabled()) {\n-            LOG.debug(\""{} checkpoint {}.\"", getClass().getSimpleName(), nextSnapshot);\n-        }\n-    }\n-\n-    @SuppressWarnings(\""BusyWait\"")\n-    @Override\n-    public void run(SourceContext<Split> ctx) throws Exception {\n-        this.ctx = ctx;\n-        while (isRunning) {\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<Split> readerOutput) throws Exception {\n             boolean isEmpty;\n-            synchronized (ctx.getCheckpointLock()) {\n-                if (!isRunning) {\n-                    return;\n-                }\n-                try {\n-                    List<Split> splits = scan.plan().splits();\n-                    isEmpty = splits.isEmpty();\n-                    splits.forEach(ctx::collect);\n-\n-                    if (emitSnapshotWatermark) {\n-                        Long watermark = scan.watermark();\n-                        if (watermark != null) {\n-                            ctx.emitWatermark(new Watermark(watermark));\n-                        }\n+            try {\n+                List<Split> splits = scan.plan().splits();\n+                isEmpty = splits.isEmpty();\n+                splits.forEach(readerOutput::collect);\n+\n+                if (emitSnapshotWatermark) {\n+                    Long watermark = scan.watermark();\n+                    if (watermark != null) {\n+                        readerOutput.emitWatermark(new Watermark(watermark));\n                     }\n-                } catch (EndOfScanException esf) {\n-                    LOG.info(\""Catching EndOfStreamException, the stream is finished.\"");\n-                    return;\n                 }\n+            } catch (EndOfScanException esf) {\n+                LOG.info(\""Catching EndOfStreamException, the stream is finished.\"");\n+                return InputStatus.END_OF_INPUT;\n             }\n \n             if (isEmpty) {\n                 Thread.sleep(monitorInterval);\n             }\n-        }\n-    }\n-\n-    @Override\n-    public void notifyCheckpointComplete(long checkpointId) {\n-        NavigableMap<Long, Long> nextSnapshots =\n-                nextSnapshotPerCheckpoint.headMap(checkpointId, true);\n-        OptionalLong max = nextSnapshots.values().stream().mapToLong(Long::longValue).max();\n-        max.ifPresent(scan::notifyCheckpointComplete);\n-        nextSnapshots.clear();\n-    }\n-\n-    @Override\n-    public void cancel() {\n-        // this is to cover the case where cancel() is called before the run()\n-        if (ctx != null) {\n-            synchronized (ctx.getCheckpointLock()) {\n-                isRunning = false;\n-            }\n-        } else {\n-            isRunning = false;\n+            return InputStatus.MORE_AVAILABLE;\n         }\n     }\n \n@@ -237,9 +212,10 @@ public static DataStream<RowData> buildSource(\n             boolean shuffleBucketWithPartition,\n             BucketMode bucketMode) {\n         SingleOutputStreamOperator<Split> singleOutputStreamOperator =\n-                env.addSource(\n-                                new MonitorFunction(\n+                env.fromSource(\n+                                new MonitorSource(\n                                         readBuilder, monitorInterval, emitSnapshotWatermark),\n+                                WatermarkStrategy.noWatermarks(),\n                                 name + \""-Monitor\"",\n                                 new JavaTypeInfo<>(Split.class))\n                         .forceNonParallel();\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiTablesReadOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiTablesReadOperator.java\nindex 73d46ae1e3f1..fbc8bb9d756a 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiTablesReadOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiTablesReadOperator.java\n@@ -52,9 +52,8 @@\n \n /**\n  * The operator that reads the Tuple2<{@link Split}, String> received from the preceding {@link\n- * CombinedAwareBatchSourceFunction} or {@link CombinedAwareStreamingSourceFunction}. Contrary to\n- * the {@link CombinedCompactorSourceFunction} which has a parallelism of 1, this operator can have\n- * DOP > 1.\n+ * CombinedAwareBatchSource} or {@link CombinedAwareStreamingSource}. Contrary to the {@link\n+ * CombinedCompactorSource} which has a parallelism of 1, this operator can have DOP > 1.\n  */\n public class MultiTablesReadOperator extends AbstractStreamOperator<RowData>\n         implements OneInputStreamOperator<Tuple2<Split, String>, RowData> {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiUnawareTablesReadOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiUnawareTablesReadOperator.java\nindex c501c2519b41..0864741a178f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiUnawareTablesReadOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/MultiUnawareTablesReadOperator.java\n@@ -44,7 +44,7 @@\n /**\n  * The operator is used for historical partition compaction. It reads {@link\n  * MultiTableUnawareAppendCompactionTask} received from the preceding {@link\n- * CombinedUnawareBatchSourceFunction} and filter partitions which is not historical.\n+ * CombinedUnawareBatchSource} and filter partitions which is not historical.\n  */\n public class MultiUnawareTablesReadOperator\n         extends AbstractStreamOperator<MultiTableUnawareAppendCompactionTask>\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/ReadOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/ReadOperator.java\nindex 80c85f7cdb35..6caf4544e514 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/ReadOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/operator/ReadOperator.java\n@@ -38,8 +38,8 @@\n \n /**\n  * The operator that reads the {@link Split splits} received from the preceding {@link\n- * MonitorFunction}. Contrary to the {@link MonitorFunction} which has a parallelism of 1, this\n- * operator can have DOP > 1.\n+ * MonitorSource}. Contrary to the {@link MonitorSource} which has a parallelism of 1, this operator\n+ * can have DOP > 1.\n  */\n public class ReadOperator extends AbstractStreamOperator<RowData>\n         implements OneInputStreamOperator<Split, RowData> {\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\nindex 723f57a30e3f..ab81e37c7d04 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n@@ -26,7 +26,6 @@\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.source.ParallelSourceFunction;\n import org.apache.flink.streaming.api.transformations.OneInputTransformation;\n import org.apache.flink.streaming.api.transformations.PartitionTransformation;\n import org.junit.jupiter.api.Test;\n@@ -45,14 +44,7 @@ public void testTransformationParallelism() {\n         env.setParallelism(8);\n         int inputParallelism = ThreadLocalRandom.current().nextInt(8) + 1;\n         DataStreamSource<CdcMultiplexRecord> input =\n-                env.addSource(\n-                                new ParallelSourceFunction<CdcMultiplexRecord>() {\n-                                    @Override\n-                                    public void run(SourceContext<CdcMultiplexRecord> ctx) {}\n-\n-                                    @Override\n-                                    public void cancel() {}\n-                                })\n+                env.fromData(CdcMultiplexRecord.class, new CdcMultiplexRecord(\""\"", \""\"", null))\n                         .setParallelism(inputParallelism);\n \n         FlinkCdcMultiTableSink sink =\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\nindex a7c6b2cb6323..28b137a93ed9 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkITCase.java\n@@ -42,6 +42,7 @@\n import org.apache.paimon.utils.FailingFileIO;\n import org.apache.paimon.utils.TraceableFileIO;\n \n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.junit.jupiter.api.Test;\n@@ -154,8 +155,9 @@ private void innerTestRandomCdcEvents(Supplier<Integer> bucket, boolean unawareB\n                         .allowRestart(enableFailure)\n                         .build();\n \n-        TestCdcSourceFunction sourceFunction = new TestCdcSourceFunction(events);\n-        DataStreamSource<TestCdcEvent> source = env.addSource(sourceFunction);\n+        TestCdcSource testCdcSource = new TestCdcSource(events);\n+        DataStreamSource<TestCdcEvent> source =\n+                env.fromSource(testCdcSource, WatermarkStrategy.noWatermarks(), \""TestCdcSource\"");\n         source.setParallelism(2);\n \n         Options catalogOptions = new Options();\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java\nindex 081bd7d073d7..8b19391f3eda 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncTableSinkITCase.java\n@@ -43,6 +43,7 @@\n import org.apache.paimon.utils.FailingFileIO;\n import org.apache.paimon.utils.TraceableFileIO;\n \n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.junit.jupiter.api.Disabled;\n@@ -151,8 +152,9 @@ private void innerTestRandomCdcEvents(\n                         .allowRestart(enableFailure)\n                         .build();\n \n-        TestCdcSourceFunction sourceFunction = new TestCdcSourceFunction(testTable.events());\n-        DataStreamSource<TestCdcEvent> source = env.addSource(sourceFunction);\n+        TestCdcSource testCdcSource = new TestCdcSource(testTable.events());\n+        DataStreamSource<TestCdcEvent> source =\n+                env.fromSource(testCdcSource, WatermarkStrategy.noWatermarks(), \""TestCdcSource\"");\n         source.setParallelism(2);\n \n         Options catalogOptions = new Options();\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSource.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSource.java\nnew file mode 100644\nindex 000000000000..b45983000a23\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSource.java\n@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink.cdc;\n+\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n+import org.apache.paimon.flink.source.SplitListState;\n+\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Testing parallel {@link org.apache.flink.api.connector.source.Source} to produce {@link\n+ * TestCdcEvent}. {@link TestCdcEvent}s with the same key will be produced by the same parallelism.\n+ */\n+public class TestCdcSource extends AbstractNonCoordinatedSource<TestCdcEvent> {\n+\n+    private static final long serialVersionUID = 1L;\n+    private final LinkedList<TestCdcEvent> events;\n+\n+    public TestCdcSource(Collection<TestCdcEvent> events) {\n+        this.events = new LinkedList<>(events);\n+    }\n+\n+    @Override\n+    public Boundedness getBoundedness() {\n+        return Boundedness.CONTINUOUS_UNBOUNDED;\n+    }\n+\n+    @Override\n+    public SourceReader<TestCdcEvent, SimpleSourceSplit> createReader(SourceReaderContext context) {\n+        return new Reader(\n+                context.getIndexOfSubtask(),\n+                context.currentParallelism(),\n+                new LinkedList<>(events));\n+    }\n+\n+    private static class Reader extends AbstractNonCoordinatedSourceReader<TestCdcEvent> {\n+        private final int subtaskId;\n+        private final int totalSubtasks;\n+\n+        private final LinkedList<TestCdcEvent> events;\n+        private final SplitListState<Integer> remainingEventsCount =\n+                new SplitListState<>(\""events\"", x -> Integer.toString(x), Integer::parseInt);\n+\n+        private final int numRecordsPerCheckpoint;\n+        private final AtomicInteger recordsThisCheckpoint;\n+\n+        private Reader(int subtaskId, int totalSubtasks, LinkedList<TestCdcEvent> events) {\n+            this.subtaskId = subtaskId;\n+            this.totalSubtasks = totalSubtasks;\n+            this.events = events;\n+            numRecordsPerCheckpoint =\n+                    events.size() / ThreadLocalRandom.current().nextInt(10, 20) + 1;\n+            recordsThisCheckpoint = new AtomicInteger(0);\n+        }\n+\n+        @Override\n+        public InputStatus pollNext(ReaderOutput<TestCdcEvent> readerOutput) throws Exception {\n+            if (events.isEmpty()) {\n+                return InputStatus.END_OF_INPUT;\n+            }\n+\n+            if (recordsThisCheckpoint.get() >= numRecordsPerCheckpoint) {\n+                Thread.sleep(10);\n+                return InputStatus.MORE_AVAILABLE;\n+            }\n+\n+            TestCdcEvent event = events.poll();\n+            if (event.records() != null) {\n+                if (Math.abs(event.hashCode()) % totalSubtasks != subtaskId) {\n+                    return InputStatus.MORE_AVAILABLE;\n+                }\n+            }\n+            readerOutput.collect(event);\n+            recordsThisCheckpoint.incrementAndGet();\n+            return InputStatus.MORE_AVAILABLE;\n+        }\n+\n+        @Override\n+        public List<SimpleSourceSplit> snapshotState(long l) {\n+            recordsThisCheckpoint.set(0);\n+            remainingEventsCount.clear();\n+            remainingEventsCount.add(events.size());\n+            return remainingEventsCount.snapshotState();\n+        }\n+\n+        @Override\n+        public void addSplits(List<SimpleSourceSplit> list) {\n+            remainingEventsCount.restoreState(list);\n+            int count = 0;\n+            for (int c : remainingEventsCount.get()) {\n+                count += c;\n+            }\n+            while (events.size() > count) {\n+                events.poll();\n+            }\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSourceFunction.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSourceFunction.java\ndeleted file mode 100644\nindex 4e03256a5253..000000000000\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/TestCdcSourceFunction.java\n+++ /dev/null\n@@ -1,107 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \""License\""); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \""AS IS\"" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.paimon.flink.sink.cdc;\n-\n-import org.apache.flink.api.common.state.ListState;\n-import org.apache.flink.api.common.state.ListStateDescriptor;\n-import org.apache.flink.runtime.state.FunctionInitializationContext;\n-import org.apache.flink.runtime.state.FunctionSnapshotContext;\n-import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n-import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n-\n-import java.util.Collection;\n-import java.util.LinkedList;\n-import java.util.concurrent.ThreadLocalRandom;\n-import java.util.concurrent.atomic.AtomicInteger;\n-\n-/**\n- * Testing {@link RichParallelSourceFunction} to produce {@link TestCdcEvent}. {@link TestCdcEvent}s\n- * with the same key will be produced by the same parallelism.\n- */\n-public class TestCdcSourceFunction extends RichParallelSourceFunction<TestCdcEvent>\n-        implements CheckpointedFunction {\n-\n-    private static final long serialVersionUID = 1L;\n-\n-    private final LinkedList<TestCdcEvent> events;\n-\n-    private volatile boolean isRunning = true;\n-    private transient int numRecordsPerCheckpoint;\n-    private transient AtomicInteger recordsThisCheckpoint;\n-    private transient ListState<Integer> remainingEventsCount;\n-\n-    public TestCdcSourceFunction(Collection<TestCdcEvent> events) {\n-        this.events = new LinkedList<>(events);\n-    }\n-\n-    @Override\n-    public void initializeState(FunctionInitializationContext context) throws Exception {\n-        numRecordsPerCheckpoint = events.size() / ThreadLocalRandom.current().nextInt(10, 20) + 1;\n-        recordsThisCheckpoint = new AtomicInteger(0);\n-\n-        remainingEventsCount =\n-                context.getOperatorStateStore()\n-                        .getListState(new ListStateDescriptor<>(\""count\"", Integer.class));\n-\n-        if (context.isRestored()) {\n-            int count = 0;\n-            for (int c : remainingEventsCount.get()) {\n-                count += c;\n-            }\n-            while (events.size() > count) {\n-                events.poll();\n-            }\n-        }\n-    }\n-\n-    @Override\n-    public void snapshotState(FunctionSnapshotContext context) throws Exception {\n-        recordsThisCheckpoint.set(0);\n-        remainingEventsCount.clear();\n-        remainingEventsCount.add(events.size());\n-    }\n-\n-    @Override\n-    public void run(SourceContext<TestCdcEvent> ctx) throws Exception {\n-        while (isRunning && !events.isEmpty()) {\n-            if (recordsThisCheckpoint.get() >= numRecordsPerCheckpoint) {\n-                Thread.sleep(10);\n-                continue;\n-            }\n-\n-            synchronized (ctx.getCheckpointLock()) {\n-                TestCdcEvent event = events.poll();\n-                if (event.records() != null) {\n-                    int subtaskId = getRuntimeContext().getIndexOfThisSubtask();\n-                    int totalSubtasks = getRuntimeContext().getNumberOfParallelSubtasks();\n-                    if (Math.abs(event.hashCode()) % totalSubtasks != subtaskId) {\n-                        continue;\n-                    }\n-                }\n-                ctx.collect(event);\n-                recordsThisCheckpoint.incrementAndGet();\n-            }\n-        }\n-    }\n-\n-    @Override\n-    public void cancel() {\n-        isRunning = false;\n-    }\n-}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java\nindex 6a2c7b071d2d..5245114e80ee 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileStoreITCase.java\n@@ -36,6 +36,7 @@\n import org.apache.paimon.utils.BranchManager;\n import org.apache.paimon.utils.FailingFileIO;\n \n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.api.common.functions.MapFunction;\n import org.apache.flink.api.connector.source.Boundedness;\n import org.apache.flink.api.dag.Transformation;\n@@ -450,7 +451,12 @@ private void sinkAndValidate(\n             throw new UnsupportedOperationException();\n         }\n         DataStreamSource<RowData> source =\n-                env.addSource(new FiniteTestSource<>(src, true), InternalTypeInfo.of(TABLE_TYPE));\n+                env.fromSource(\n+                        new FiniteTestSource<>(src, true),\n+                        WatermarkStrategy.noWatermarks(),\n+                        \""FiniteTestSource\"",\n+                        InternalTypeInfo.of(TABLE_TYPE));\n+        source.forceNonParallel();\n         new FlinkSinkBuilder(table).forRowData(source).build();\n         env.execute();\n         assertThat(iterator.collect(expected.length)).containsExactlyInAnyOrder(expected);\n@@ -521,9 +527,13 @@ public static DataStreamSource<RowData> buildTestSource(\n             StreamExecutionEnvironment env, boolean isBatch) {\n         return isBatch\n                 ? env.fromCollection(SOURCE_DATA, InternalTypeInfo.of(TABLE_TYPE))\n-                : env.addSource(\n-                        new FiniteTestSource<>(SOURCE_DATA, false),\n-                        InternalTypeInfo.of(TABLE_TYPE));\n+                : (DataStreamSource<RowData>)\n+                        env.fromSource(\n+                                        new FiniteTestSource<>(SOURCE_DATA, false),\n+                                        WatermarkStrategy.noWatermarks(),\n+                                        \""FiniteTestSource\"",\n+                                        InternalTypeInfo.of(TABLE_TYPE))\n+                                .forceNonParallel();\n     }\n \n     public static List<Row> executeAndCollect(DataStream<RowData> source) throws Exception {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java\nindex 9c5254d6283b..6691b9c09514 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FiniteTestSource.java\n@@ -18,16 +18,18 @@\n \n package org.apache.paimon.flink;\n \n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n+import org.apache.paimon.flink.source.SplitListState;\n import org.apache.paimon.utils.Preconditions;\n \n-import org.apache.flink.api.common.state.CheckpointListener;\n-import org.apache.flink.api.common.state.ListState;\n-import org.apache.flink.api.common.state.ListStateDescriptor;\n-import org.apache.flink.api.common.typeutils.base.IntSerializer;\n-import org.apache.flink.runtime.state.FunctionInitializationContext;\n-import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n-import org.apache.flink.streaming.api.functions.source.SourceFunction;\n \n import java.util.ArrayList;\n import java.util.List;\n@@ -39,8 +41,7 @@\n  *\n  * <p>The reason this class is rewritten is to support {@link CheckpointedFunction}.\n  */\n-public class FiniteTestSource<T>\n-        implements SourceFunction<T>, CheckpointedFunction, CheckpointListener {\n+public class FiniteTestSource<T> extends AbstractNonCoordinatedSource<T> {\n \n     private static final long serialVersionUID = 1L;\n \n@@ -48,27 +49,78 @@ public class FiniteTestSource<T>\n \n     private final boolean emitOnce;\n \n-    private volatile boolean running = true;\n-\n-    private transient int numCheckpointsComplete;\n-\n-    private transient ListState<Integer> checkpointedState;\n-\n-    private volatile int numTimesEmitted;\n-\n     public FiniteTestSource(List<T> elements, boolean emitOnce) {\n         this.elements = elements;\n         this.emitOnce = emitOnce;\n     }\n \n     @Override\n-    public void initializeState(FunctionInitializationContext context) throws Exception {\n-        this.checkpointedState =\n-                context.getOperatorStateStore()\n-                        .getListState(\n-                                new ListStateDescriptor<>(\""emit-times\"", IntSerializer.INSTANCE));\n+    public Boundedness getBoundedness() {\n+        return Boundedness.BOUNDED;\n+    }\n+\n+    @Override\n+    public SourceReader<T, SimpleSourceSplit> createReader(SourceReaderContext sourceReaderContext)\n+            throws Exception {\n+        return new Reader<>(elements, emitOnce);\n+    }\n+\n+    private static class Reader<T> extends AbstractNonCoordinatedSourceReader<T> {\n+\n+        private final List<T> elements;\n+\n+        private final boolean emitOnce;\n+\n+        private final SplitListState<Integer> checkpointedState =\n+                new SplitListState<>(\""emit-times\"", x -> Integer.toString(x), Integer::parseInt);\n+\n+        private int numTimesEmitted = 0;\n+\n+        private int numCheckpointsComplete;\n+\n+        private Integer checkpointToAwait;\n+\n+        private Reader(List<T> elements, boolean emitOnce) {\n+            this.elements = elements;\n+            this.emitOnce = emitOnce;\n+            this.numCheckpointsComplete = 0;\n+        }\n+\n+        @Override\n+        public synchronized InputStatus pollNext(ReaderOutput<T> readerOutput) {\n+            if (checkpointToAwait == null) {\n+                checkpointToAwait = numCheckpointsComplete + 2;\n+            }\n+            switch (numTimesEmitted) {\n+                case 0:\n+                    emitElements(readerOutput, false);\n+                    if (numCheckpointsComplete < checkpointToAwait) {\n+                        return InputStatus.MORE_AVAILABLE;\n+                    }\n+                    emitElements(readerOutput, true);\n+                    if (numCheckpointsComplete < checkpointToAwait + 2) {\n+                        return InputStatus.MORE_AVAILABLE;\n+                    }\n+                    break;\n+                case 1:\n+                    emitElements(readerOutput, true);\n+                    if (numCheckpointsComplete < checkpointToAwait) {\n+                        return InputStatus.MORE_AVAILABLE;\n+                    }\n+                    break;\n+                case 2:\n+                    // Maybe missed notifyCheckpointComplete, wait next notifyCheckpointComplete\n+                    if (numCheckpointsComplete < checkpointToAwait) {\n+                        return InputStatus.MORE_AVAILABLE;\n+                    }\n+                    break;\n+            }\n+            return InputStatus.END_OF_INPUT;\n+        }\n \n-        if (context.isRestored()) {\n+        @Override\n+        public void addSplits(List<SimpleSourceSplit> list) {\n+            checkpointedState.restoreState(list);\n             List<Integer> retrievedStates = new ArrayList<>();\n             for (Integer entry : this.checkpointedState.get()) {\n                 retrievedStates.add(entry);\n@@ -85,76 +137,27 @@ public void initializeState(FunctionInitializationContext context) throws Except\n                     getClass().getSimpleName()\n                             + \"" retrieved invalid numTimesEmitted: \""\n                             + numTimesEmitted);\n-        } else {\n-            this.numTimesEmitted = 0;\n         }\n-    }\n \n-    @Override\n-    public void run(SourceContext<T> ctx) throws Exception {\n-        switch (numTimesEmitted) {\n-            case 0:\n-                emitElementsAndWaitForCheckpoints(ctx, false);\n-                emitElementsAndWaitForCheckpoints(ctx, true);\n-                break;\n-            case 1:\n-                emitElementsAndWaitForCheckpoints(ctx, true);\n-                break;\n-            case 2:\n-                // Maybe missed notifyCheckpointComplete, wait next notifyCheckpointComplete\n-                final Object lock = ctx.getCheckpointLock();\n-                synchronized (lock) {\n-                    int checkpointToAwait = numCheckpointsComplete + 2;\n-                    while (running && numCheckpointsComplete < checkpointToAwait) {\n-                        lock.wait(1);\n-                    }\n-                }\n-                break;\n+        @Override\n+        public List<SimpleSourceSplit> snapshotState(long l) {\n+            this.checkpointedState.clear();\n+            this.checkpointedState.add(this.numTimesEmitted);\n+            return this.checkpointedState.snapshotState();\n         }\n-    }\n \n-    private void emitElementsAndWaitForCheckpoints(SourceContext<T> ctx, boolean isSecond)\n-            throws InterruptedException {\n-        final Object lock = ctx.getCheckpointLock();\n+        @Override\n+        public void notifyCheckpointComplete(long checkpointId) {\n+            numCheckpointsComplete++;\n+        }\n \n-        final int checkpointToAwait;\n-        synchronized (lock) {\n-            checkpointToAwait = numCheckpointsComplete + 2;\n+        private void emitElements(ReaderOutput<T> readerOutput, boolean isSecond) {\n             if (!isSecond || !emitOnce) {\n                 for (T t : elements) {\n-                    ctx.collect(t);\n+                    readerOutput.collect(t);\n                 }\n             }\n             numTimesEmitted++;\n         }\n-\n-        synchronized (lock) {\n-            while (running && numCheckpointsComplete < checkpointToAwait) {\n-                lock.wait(1);\n-            }\n-        }\n-    }\n-\n-    @Override\n-    public void cancel() {\n-        running = false;\n-    }\n-\n-    @Override\n-    public void notifyCheckpointComplete(long checkpointId) {\n-        numCheckpointsComplete++;\n-    }\n-\n-    @Override\n-    public void notifyCheckpointAborted(long checkpointId) {}\n-\n-    @Override\n-    public void snapshotState(FunctionSnapshotContext context) throws Exception {\n-        Preconditions.checkState(\n-                this.checkpointedState != null,\n-                \""The \"" + getClass().getSimpleName() + \"" has not been properly initialized.\"");\n-\n-        this.checkpointedState.clear();\n-        this.checkpointedState.add(this.numTimesEmitted);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java\nindex 594affc124eb..75b96cbe02eb 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/SerializableRowData.java\n@@ -47,8 +47,10 @@ public SerializableRowData(RowData row, TypeSerializer<RowData> serializer) {\n         this.serializer = serializer;\n     }\n \n-    private void writeObject(ObjectOutputStream out) throws IOException {\n+    private synchronized void writeObject(ObjectOutputStream out) throws IOException {\n         out.defaultWriteObject();\n+        // This following invocation needs to be synchronized to avoid racing problems when the\n+        // serializer is reused across multiple subtasks.\n         serializer.serialize(row, new DataOutputViewStreamWrapper(out));\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\nindex 6ca78b088fb7..fb8bee5d5962 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\n@@ -20,7 +20,9 @@\n \n import org.apache.paimon.Snapshot;\n import org.apache.paimon.data.InternalRow;\n-import org.apache.paimon.flink.utils.RuntimeContextUtils;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSource;\n+import org.apache.paimon.flink.source.AbstractNonCoordinatedSourceReader;\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.fs.local.LocalFileIO;\n import org.apache.paimon.reader.RecordReader;\n@@ -30,9 +32,14 @@\n import org.apache.paimon.utils.FailingFileIO;\n import org.apache.paimon.utils.TimeUtils;\n \n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.ReaderOutput;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.core.io.InputStatus;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n import org.apache.flink.table.planner.factories.TestValuesTableFactory;\n import org.apache.flink.types.Row;\n@@ -380,7 +387,12 @@ public void testStatelessWriter() throws Exception {\n                         .checkpointIntervalMs(500)\n                         .build();\n         DataStream<Integer> source =\n-                env.addSource(new TestStatelessWriterSource(table)).setParallelism(2).forward();\n+                env.fromSource(\n+                                new TestStatelessWriterSource(table),\n+                                WatermarkStrategy.noWatermarks(),\n+                                \""TestStatelessWriterSource\"")\n+                        .setParallelism(2)\n+                        .forward();\n \n         StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);\n         tEnv.registerCatalog(\""mycat\"", sEnv.getCatalog(\""PAIMON\"").get());\n@@ -392,46 +404,59 @@ public void testStatelessWriter() throws Exception {\n                 .containsExactlyInAnyOrder(Row.of(1, \""test\""), Row.of(2, \""test\""));\n     }\n \n-    private static class TestStatelessWriterSource extends RichParallelSourceFunction<Integer> {\n+    private static class TestStatelessWriterSource extends AbstractNonCoordinatedSource<Integer> {\n \n         private final FileStoreTable table;\n \n-        private volatile boolean isRunning = true;\n-\n         private TestStatelessWriterSource(FileStoreTable table) {\n             this.table = table;\n         }\n \n         @Override\n-        public void run(SourceContext<Integer> sourceContext) throws Exception {\n-            int taskId = RuntimeContextUtils.getIndexOfThisSubtask(getRuntimeContext());\n-            // wait some time in parallelism #2,\n-            // so that it does not commit in the same checkpoint with parallelism #1\n-            int waitCount = (taskId == 0 ? 0 : 10);\n-\n-            while (isRunning) {\n-                synchronized (sourceContext.getCheckpointLock()) {\n-                    if (taskId == 0) {\n+        public Boundedness getBoundedness() {\n+            return Boundedness.CONTINUOUS_UNBOUNDED;\n+        }\n+\n+        @Override\n+        public SourceReader<Integer, SimpleSourceSplit> createReader(\n+                SourceReaderContext sourceReaderContext) throws Exception {\n+            return new Reader(sourceReaderContext.getIndexOfSubtask());\n+        }\n+\n+        private class Reader extends AbstractNonCoordinatedSourceReader<Integer> {\n+            private final int taskId;\n+            private int waitCount;\n+\n+            private Reader(int taskId) {\n+                this.taskId = taskId;\n+                this.waitCount = (taskId == 0 ? 0 : 10);\n+            }\n+\n+            @Override\n+            public InputStatus pollNext(ReaderOutput<Integer> readerOutput) throws Exception {\n+                if (taskId == 0) {\n+                    if (waitCount == 0) {\n+                        readerOutput.collect(1);\n+                    } else if (countNumRecords() >= 1) {\n+                        // wait for the record to commit before exiting\n+                        Thread.sleep(1000);\n+                        return InputStatus.END_OF_INPUT;\n+                    }\n+                } else {\n+                    int numRecords = countNumRecords();\n+                    if (numRecords >= 1) {\n                         if (waitCount == 0) {\n-                            sourceContext.collect(1);\n-                        } else if (countNumRecords() >= 1) {\n-                            // wait for the record to commit before exiting\n-                            break;\n-                        }\n-                    } else {\n-                        int numRecords = countNumRecords();\n-                        if (numRecords >= 1) {\n-                            if (waitCount == 0) {\n-                                sourceContext.collect(2);\n-                            } else if (countNumRecords() >= 2) {\n-                                // make sure the next checkpoint is successful\n-                                break;\n-                            }\n+                            readerOutput.collect(2);\n+                        } else if (countNumRecords() >= 2) {\n+                            // make sure the next checkpoint is successful\n+                            Thread.sleep(1000);\n+                            return InputStatus.END_OF_INPUT;\n                         }\n                     }\n-                    waitCount--;\n                 }\n+                waitCount--;\n                 Thread.sleep(1000);\n+                return InputStatus.MORE_AVAILABLE;\n             }\n         }\n \n@@ -447,11 +472,6 @@ private int countNumRecords() throws Exception {\n             }\n             return ret;\n         }\n-\n-        @Override\n-        public void cancel() {\n-            isRunning = false;\n-        }\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java\nindex 61a03a29a21b..b1e0fb83610e 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/OperatorSourceTest.java\n@@ -33,12 +33,17 @@\n import org.apache.paimon.table.source.TableRead;\n import org.apache.paimon.types.DataTypes;\n \n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n-import org.apache.flink.streaming.api.functions.source.SourceFunction;\n-import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.api.operators.SourceOperator;\n import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.io.PushingAsyncDataInput;\n+import org.apache.flink.streaming.runtime.streamrecord.LatencyMarker;\n+import org.apache.flink.streaming.runtime.streamrecord.RecordAttributes;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.watermarkstatus.WatermarkStatus;\n import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n import org.apache.flink.table.data.GenericRowData;\n@@ -46,6 +51,7 @@\n import org.apache.flink.table.runtime.typeutils.InternalSerializers;\n import org.apache.flink.table.types.logical.IntType;\n import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.util.CloseableIterator;\n import org.apache.flink.util.function.SupplierWithException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n@@ -58,11 +64,13 @@\n import java.util.List;\n import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n \n import static org.apache.paimon.CoreOptions.CONSUMER_ID;\n import static org.assertj.core.api.Assertions.assertThat;\n \n-/** Test for {@link MonitorFunction} and {@link ReadOperator}. */\n+/** Test for {@link MonitorSource} and {@link ReadOperator}. */\n public class OperatorSourceTest {\n \n     @TempDir Path tempDir;\n@@ -114,28 +122,39 @@ private List<List<Integer>> readSplit(Split split) throws IOException {\n     }\n \n     @Test\n-    public void testMonitorFunction() throws Exception {\n+    public void testMonitorSource() throws Exception {\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n         // 1. run first\n         OperatorSubtaskState snapshot;\n         {\n-            MonitorFunction function = new MonitorFunction(table.newReadBuilder(), 10, false);\n-            StreamSource<Split, MonitorFunction> src = new StreamSource<>(function);\n+            MonitorSource source = new MonitorSource(table.newReadBuilder(), 10, false);\n+            TestingSourceOperator<Split> operator =\n+                    (TestingSourceOperator<Split>)\n+                            TestingSourceOperator.createTestOperator(\n+                                    source.createReader(null),\n+                                    WatermarkStrategy.noWatermarks(),\n+                                    false);\n             AbstractStreamOperatorTestHarness<Split> testHarness =\n-                    new AbstractStreamOperatorTestHarness<>(src, 1, 1, 0);\n+                    new AbstractStreamOperatorTestHarness<>(operator, 1, 1, 0);\n             testHarness.open();\n-            snapshot = testReadSplit(function, () -> testHarness.snapshot(0, 0), 1, 1, 1);\n+            snapshot = testReadSplit(operator, () -> testHarness.snapshot(0, 0), 1, 1, 1);\n         }\n \n         // 2. restore from state\n         {\n-            MonitorFunction functionCopy1 = new MonitorFunction(table.newReadBuilder(), 10, false);\n-            StreamSource<Split, MonitorFunction> srcCopy1 = new StreamSource<>(functionCopy1);\n+            MonitorSource sourceCopy1 = new MonitorSource(table.newReadBuilder(), 10, false);\n+            TestingSourceOperator<Split> operatorCopy1 =\n+                    (TestingSourceOperator<Split>)\n+                            TestingSourceOperator.createTestOperator(\n+                                    sourceCopy1.createReader(null),\n+                                    WatermarkStrategy.noWatermarks(),\n+                                    false);\n             AbstractStreamOperatorTestHarness<Split> testHarnessCopy1 =\n-                    new AbstractStreamOperatorTestHarness<>(srcCopy1, 1, 1, 0);\n+                    new AbstractStreamOperatorTestHarness<>(operatorCopy1, 1, 1, 0);\n             testHarnessCopy1.initializeState(snapshot);\n             testHarnessCopy1.open();\n             testReadSplit(\n-                    functionCopy1,\n+                    operatorCopy1,\n                     () -> {\n                         testHarnessCopy1.snapshot(1, 1);\n                         testHarnessCopy1.notifyOfCompletedCheckpoint(1);\n@@ -148,12 +167,17 @@ public void testMonitorFunction() throws Exception {\n \n         // 3. restore from consumer id\n         {\n-            MonitorFunction functionCopy2 = new MonitorFunction(table.newReadBuilder(), 10, false);\n-            StreamSource<Split, MonitorFunction> srcCopy2 = new StreamSource<>(functionCopy2);\n+            MonitorSource sourceCopy2 = new MonitorSource(table.newReadBuilder(), 10, false);\n+            TestingSourceOperator<Split> operatorCopy2 =\n+                    (TestingSourceOperator<Split>)\n+                            TestingSourceOperator.createTestOperator(\n+                                    sourceCopy2.createReader(null),\n+                                    WatermarkStrategy.noWatermarks(),\n+                                    false);\n             AbstractStreamOperatorTestHarness<Split> testHarnessCopy2 =\n-                    new AbstractStreamOperatorTestHarness<>(srcCopy2, 1, 1, 0);\n+                    new AbstractStreamOperatorTestHarness<>(operatorCopy2, 1, 1, 0);\n             testHarnessCopy2.open();\n-            testReadSplit(functionCopy2, () -> null, 3, 3, 3);\n+            testReadSplit(operatorCopy2, () -> null, 3, 3, 3);\n         }\n     }\n \n@@ -231,7 +255,7 @@ public void testReadOperatorMetricsRegisterAndUpdate() throws Exception {\n     }\n \n     private <T> T testReadSplit(\n-            MonitorFunction function,\n+            SourceOperator<Split, ?> operator,\n             SupplierWithException<T, Exception> beforeClose,\n             int a,\n             int b,\n@@ -239,20 +263,36 @@ private <T> T testReadSplit(\n             throws Exception {\n         Throwable[] error = new Throwable[1];\n         ArrayBlockingQueue<Split> queue = new ArrayBlockingQueue<>(10);\n+        AtomicReference<CloseableIterator<Split>> iteratorRef = new AtomicReference<>();\n \n-        DummySourceContext sourceContext =\n-                new DummySourceContext() {\n+        PushingAsyncDataInput.DataOutput<Split> output =\n+                new PushingAsyncDataInput.DataOutput<Split>() {\n                     @Override\n-                    public void collect(Split element) {\n-                        queue.add(element);\n+                    public void emitRecord(StreamRecord<Split> streamRecord) {\n+                        queue.add(streamRecord.getValue());\n                     }\n+\n+                    @Override\n+                    public void emitWatermark(Watermark watermark) {}\n+\n+                    @Override\n+                    public void emitWatermarkStatus(WatermarkStatus watermarkStatus) {}\n+\n+                    @Override\n+                    public void emitLatencyMarker(LatencyMarker latencyMarker) {}\n+\n+                    @Override\n+                    public void emitRecordAttributes(RecordAttributes recordAttributes) {}\n                 };\n \n+        AtomicBoolean isRunning = new AtomicBoolean(true);\n         Thread runner =\n                 new Thread(\n                         () -> {\n                             try {\n-                                function.run(sourceContext);\n+                                while (isRunning.get()) {\n+                                    operator.emitNext(output);\n+                                }\n                             } catch (Throwable t) {\n                                 t.printStackTrace();\n                                 error[0] = t;\n@@ -266,34 +306,15 @@ public void collect(Split element) {\n         assertThat(readSplit(split)).containsExactlyInAnyOrder(Arrays.asList(a, b, c));\n \n         T t = beforeClose.get();\n-        function.cancel();\n+        CloseableIterator<Split> iterator = iteratorRef.get();\n+        if (iterator != null) {\n+            iterator.close();\n+        }\n+        isRunning.set(false);\n         runner.join();\n \n         assertThat(error[0]).isNull();\n \n         return t;\n     }\n-\n-    private abstract static class DummySourceContext\n-            implements SourceFunction.SourceContext<Split> {\n-\n-        private final Object lock = new Object();\n-\n-        @Override\n-        public void collectWithTimestamp(Split element, long timestamp) {}\n-\n-        @Override\n-        public void emitWatermark(Watermark mark) {}\n-\n-        @Override\n-        public void markAsTemporarilyIdle() {}\n-\n-        @Override\n-        public Object getCheckpointLock() {\n-            return lock;\n-        }\n-\n-        @Override\n-        public void close() {}\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/TestingSourceOperator.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/TestingSourceOperator.java\nnew file mode 100644\nindex 000000000000..77b44d5b0e5c\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/operator/TestingSourceOperator.java\n@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.source.operator;\n+\n+import org.apache.paimon.flink.source.SimpleSourceSplit;\n+import org.apache.paimon.flink.source.SimpleSourceSplitSerializer;\n+\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.common.state.OperatorStateStore;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.fs.CloseableRegistry;\n+import org.apache.flink.runtime.execution.Environment;\n+import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.operators.coordination.OperatorEvent;\n+import org.apache.flink.runtime.operators.coordination.OperatorEventGateway;\n+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n+import org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder;\n+import org.apache.flink.runtime.state.AbstractStateBackend;\n+import org.apache.flink.runtime.state.OperatorStateBackendParametersImpl;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateInitializationContextImpl;\n+import org.apache.flink.runtime.state.hashmap.HashMapStateBackend;\n+import org.apache.flink.streaming.api.operators.SourceOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask;\n+import org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService;\n+import org.apache.flink.streaming.util.MockOutput;\n+import org.apache.flink.streaming.util.MockStreamConfig;\n+import org.apache.flink.streaming.util.MockStreamingRuntimeContext;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+\n+/**\n+ * A SourceOperator extension to simplify test setup.\n+ *\n+ * <p>This class is implemented in reference to {@link\n+ * org.apache.flink.streaming.api.operators.source.TestingSourceOperator}.\n+ *\n+ * <p>See <a\n+ * href=\""https://github.com/apache/flink/pull/12306/files#diff-bb7687690ffa79fd86950aa23171431fcf707246ca4620d79361a6612ba7b828\"">Flink\n+ * PR that introduced this class</a>\n+ */\n+public class TestingSourceOperator<T> extends SourceOperator<T, SimpleSourceSplit> {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    private final int subtaskIndex;\n+    private final int parallelism;\n+\n+    public TestingSourceOperator(\n+            StreamOperatorParameters<T> parameters,\n+            SourceReader<T, SimpleSourceSplit> reader,\n+            WatermarkStrategy<T> watermarkStrategy,\n+            ProcessingTimeService timeService,\n+            boolean emitProgressiveWatermarks) {\n+\n+        this(\n+                parameters,\n+                reader,\n+                watermarkStrategy,\n+                timeService,\n+                new TestingOperatorEventGateway(),\n+                1,\n+                5,\n+                emitProgressiveWatermarks);\n+    }\n+\n+    public TestingSourceOperator(\n+            StreamOperatorParameters<T> parameters,\n+            SourceReader<T, SimpleSourceSplit> reader,\n+            WatermarkStrategy<T> watermarkStrategy,\n+            ProcessingTimeService timeService,\n+            OperatorEventGateway eventGateway,\n+            int subtaskIndex,\n+            int parallelism,\n+            boolean emitProgressiveWatermarks) {\n+\n+        super(\n+                (context) -> reader,\n+                eventGateway,\n+                new SimpleSourceSplitSerializer(),\n+                watermarkStrategy,\n+                timeService,\n+                new Configuration(),\n+                \""localhost\"",\n+                emitProgressiveWatermarks,\n+                () -> false);\n+\n+        this.subtaskIndex = subtaskIndex;\n+        this.parallelism = parallelism;\n+        this.metrics = UnregisteredMetricGroups.createUnregisteredOperatorMetricGroup();\n+        initSourceMetricGroup();\n+\n+        // unchecked wrapping is okay to keep tests simpler\n+        try {\n+            initReader();\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+\n+        setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());\n+    }\n+\n+    @Override\n+    public StreamingRuntimeContext getRuntimeContext() {\n+        return new MockStreamingRuntimeContext(false, parallelism, subtaskIndex);\n+    }\n+\n+    // this is overridden to avoid complex mock injection through the \""containingTask\""\n+    @Override\n+    public ExecutionConfig getExecutionConfig() {\n+        ExecutionConfig cfg = new ExecutionConfig();\n+        cfg.setAutoWatermarkInterval(100);\n+        return cfg;\n+    }\n+\n+    public static <T> SourceOperator<T, SimpleSourceSplit> createTestOperator(\n+            SourceReader<T, SimpleSourceSplit> reader,\n+            WatermarkStrategy<T> watermarkStrategy,\n+            boolean emitProgressiveWatermarks)\n+            throws Exception {\n+\n+        AbstractStateBackend abstractStateBackend = new HashMapStateBackend();\n+        Environment env = new MockEnvironmentBuilder().build();\n+        CloseableRegistry cancelStreamRegistry = new CloseableRegistry();\n+        final OperatorStateStore operatorStateStore =\n+                abstractStateBackend.createOperatorStateBackend(\n+                        new OperatorStateBackendParametersImpl(\n+                                env,\n+                                \""test-operator\"",\n+                                Collections.emptyList(),\n+                                cancelStreamRegistry));\n+\n+        final StateInitializationContext stateContext =\n+                new StateInitializationContextImpl(null, operatorStateStore, null, null, null);\n+\n+        TestProcessingTimeService timeService = new TestProcessingTimeService();\n+        timeService.setCurrentTime(Integer.MAX_VALUE); // start somewhere that is not zero\n+\n+        final SourceOperator<T, SimpleSourceSplit> sourceOperator =\n+                new TestingSourceOperator<>(\n+                        new StreamOperatorParameters<>(\n+                                new SourceOperatorStreamTask<Integer>(new DummyEnvironment()),\n+                                new MockStreamConfig(new Configuration(), 1),\n+                                new MockOutput<>(new ArrayList<>()),\n+                                null,\n+                                null,\n+                                null),\n+                        reader,\n+                        watermarkStrategy,\n+                        timeService,\n+                        emitProgressiveWatermarks);\n+        sourceOperator.initializeState(stateContext);\n+        sourceOperator.open();\n+\n+        return sourceOperator;\n+    }\n+\n+    private static class TestingOperatorEventGateway implements OperatorEventGateway {\n+        @Override\n+        public void sendEventToCoordinator(OperatorEvent event) {}\n+    }\n+}\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4612"", ""pr_id"": 4612, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 2, ""non_test_files_count"": 14, ""pr_changed_files"": [""paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java"", ""paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java"", ""paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java"", ""paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java"", ""paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java"", ""paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java"", ""paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java"", ""paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/action/CloneAction.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryAddressRegister.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryService.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java""], ""base_commit"": ""e72c06ce489e54272f02dbaa555a5c31d7c36c40"", ""head_commit"": ""ba10887c1a5b7c0e9b9b9d0ee2d67fe67a6df1a8"", ""repo_url"": ""https://github.com/apache/paimon/pull/4612"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4612"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-02T07:46:40.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\nnew file mode 100644\nindex 000000000000..563dbbe75e7e\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.connector.sink2;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface WriterInitContext extends org.apache.flink.api.connector.sink2.Sink.InitContext {}\n\ndiff --git a/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\nnew file mode 100644\nindex 000000000000..98aaf6418ff7\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\n@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.sink.v2;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A special sink that ignores all elements.\n+ *\n+ * @param <IN> The type of elements received by the sink.\n+ */\n+@PublicEvolving\n+public class DiscardingSink<IN> implements Sink<IN> {\n+    private static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public SinkWriter<IN> createWriter(InitContext context) throws IOException {\n+        return new DiscardingElementWriter();\n+    }\n+\n+    private class DiscardingElementWriter implements SinkWriter<IN> {\n+\n+        @Override\n+        public void write(IN element, Context context) throws IOException, InterruptedException {\n+            // discard it.\n+        }\n+\n+        @Override\n+        public void flush(boolean endOfInput) throws IOException, InterruptedException {\n+            // this writer has no pending data.\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            // do nothing.\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\nnew file mode 100644\nindex 000000000000..563dbbe75e7e\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.connector.sink2;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface WriterInitContext extends org.apache.flink.api.connector.sink2.Sink.InitContext {}\n\ndiff --git a/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\nnew file mode 100644\nindex 000000000000..98aaf6418ff7\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\n@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.sink.v2;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A special sink that ignores all elements.\n+ *\n+ * @param <IN> The type of elements received by the sink.\n+ */\n+@PublicEvolving\n+public class DiscardingSink<IN> implements Sink<IN> {\n+    private static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public SinkWriter<IN> createWriter(InitContext context) throws IOException {\n+        return new DiscardingElementWriter();\n+    }\n+\n+    private class DiscardingElementWriter implements SinkWriter<IN> {\n+\n+        @Override\n+        public void write(IN element, Context context) throws IOException, InterruptedException {\n+            // discard it.\n+        }\n+\n+        @Override\n+        public void flush(boolean endOfInput) throws IOException, InterruptedException {\n+            // this writer has no pending data.\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            // do nothing.\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\nnew file mode 100644\nindex 000000000000..db4500042572\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\n@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.connector.sink2;\n+\n+import org.apache.flink.annotation.Public;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+@Public\n+public interface WriterInitContext extends org.apache.flink.api.connector.sink2.Sink.InitContext {}\n\ndiff --git a/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\nnew file mode 100644\nindex 000000000000..fc7eb0d48356\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\n@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.sink.v2;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.common.SupportsConcurrentExecutionAttempts;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A special sink that ignores all elements.\n+ *\n+ * @param <IN> The type of elements received by the sink.\n+ */\n+@PublicEvolving\n+public class DiscardingSink<IN> implements Sink<IN>, SupportsConcurrentExecutionAttempts {\n+    private static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public SinkWriter<IN> createWriter(InitContext context) throws IOException {\n+        return new DiscardingElementWriter();\n+    }\n+\n+    private class DiscardingElementWriter implements SinkWriter<IN> {\n+\n+        @Override\n+        public void write(IN element, Context context) throws IOException, InterruptedException {\n+            // discard it.\n+        }\n+\n+        @Override\n+        public void flush(boolean endOfInput) throws IOException, InterruptedException {\n+            // this writer has no pending data.\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            // do nothing.\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\nnew file mode 100644\nindex 000000000000..563dbbe75e7e\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/connector/sink2/WriterInitContext.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.connector.sink2;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface WriterInitContext extends org.apache.flink.api.connector.sink2.Sink.InitContext {}\n\ndiff --git a/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\nnew file mode 100644\nindex 000000000000..fc7eb0d48356\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/streaming/api/functions/sink/v2/DiscardingSink.java\n@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.sink.v2;\n+\n+import org.apache.flink.annotation.PublicEvolving;\n+import org.apache.flink.api.common.SupportsConcurrentExecutionAttempts;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * A special sink that ignores all elements.\n+ *\n+ * @param <IN> The type of elements received by the sink.\n+ */\n+@PublicEvolving\n+public class DiscardingSink<IN> implements Sink<IN>, SupportsConcurrentExecutionAttempts {\n+    private static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public SinkWriter<IN> createWriter(InitContext context) throws IOException {\n+        return new DiscardingElementWriter();\n+    }\n+\n+    private class DiscardingElementWriter implements SinkWriter<IN> {\n+\n+        @Override\n+        public void write(IN element, Context context) throws IOException, InterruptedException {\n+            // discard it.\n+        }\n+\n+        @Override\n+        public void flush(boolean endOfInput) throws IOException, InterruptedException {\n+            // this writer has no pending data.\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            // do nothing.\n+        }\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\nindex f9b7bbc6b910..1688d4deb088 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n@@ -40,7 +40,7 @@\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n@@ -134,7 +134,7 @@ public DataStreamSink<?> sinkFrom(\n                                         createCommittableStateManager()))\n                         .setParallelism(input.getParallelism());\n         configureGlobalCommitter(committed, commitCpuCores, commitHeapMemory);\n-        return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n+        return committed.sinkTo(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n     protected OneInputStreamOperatorFactory<CdcMultiplexRecord, MultiTableCommittable>\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/action/CloneAction.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/action/CloneAction.java\nindex 2f90147eeb2a..bac030dd0496 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/action/CloneAction.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/action/CloneAction.java\n@@ -32,7 +32,7 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n \n import java.util.HashMap;\n import java.util.Map;\n@@ -141,7 +141,7 @@ copyFiles, new SnapshotHintChannelComputer(), parallelism)\n                                 new SnapshotHintOperator(targetCatalogConfig))\n                         .setParallelism(parallelism);\n \n-        snapshotHintOperator.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n+        snapshotHintOperator.sinkTo(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryAddressRegister.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryAddressRegister.java\nindex 524f2e5f01c1..00d527506cfe 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryAddressRegister.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryAddressRegister.java\n@@ -23,10 +23,9 @@\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.Table;\n \n-import org.apache.flink.api.common.functions.OpenContext;\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n-import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.api.connector.sink2.Sink;\n+import org.apache.flink.api.connector.sink2.SinkWriter;\n+import org.apache.flink.api.connector.sink2.WriterInitContext;\n \n import java.net.InetSocketAddress;\n import java.util.TreeMap;\n@@ -34,57 +33,68 @@\n import static org.apache.paimon.service.ServiceManager.PRIMARY_KEY_LOOKUP;\n \n /** Operator for address server to register addresses to {@link ServiceManager}. */\n-public class QueryAddressRegister extends RichSinkFunction<InternalRow> {\n-\n+public class QueryAddressRegister implements Sink<InternalRow> {\n     private final ServiceManager serviceManager;\n \n-    private transient int numberExecutors;\n-    private transient TreeMap<Integer, InetSocketAddress> executors;\n-\n     public QueryAddressRegister(Table table) {\n         this.serviceManager = ((FileStoreTable) table).store().newServiceManager();\n     }\n \n     /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n      */\n-    public void open(OpenContext openContext) throws Exception {\n-        open(new Configuration());\n+    public SinkWriter<InternalRow> createWriter(InitContext context) {\n+        return new QueryAddressRegisterSinkWriter(serviceManager);\n     }\n \n     /**\n-     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n      */\n-    public void open(Configuration parameters) throws Exception {\n-        this.executors = new TreeMap<>();\n+    public SinkWriter<InternalRow> createWriter(WriterInitContext context) {\n+        return new QueryAddressRegisterSinkWriter(serviceManager);\n     }\n \n-    @Override\n-    public void invoke(InternalRow row, SinkFunction.Context context) {\n-        int numberExecutors = row.getInt(0);\n-        if (this.numberExecutors != 0 && this.numberExecutors != numberExecutors) {\n-            throw new IllegalArgumentException(\n-                    String.format(\n-                            \""Number Executors can not be changed! Old %s , New %s .\"",\n-                            this.numberExecutors, numberExecutors));\n-        }\n-        this.numberExecutors = numberExecutors;\n+    private static class QueryAddressRegisterSinkWriter implements SinkWriter<InternalRow> {\n+        private final ServiceManager serviceManager;\n \n-        int executorId = row.getInt(1);\n-        String hostname = row.getString(2).toString();\n-        int port = row.getInt(3);\n+        private final TreeMap<Integer, InetSocketAddress> executors;\n \n-        executors.put(executorId, new InetSocketAddress(hostname, port));\n+        private int numberExecutors;\n \n-        if (executors.size() == numberExecutors) {\n-            serviceManager.resetService(\n-                    PRIMARY_KEY_LOOKUP, executors.values().toArray(new InetSocketAddress[0]));\n+        private QueryAddressRegisterSinkWriter(ServiceManager serviceManager) {\n+            this.serviceManager = serviceManager;\n+            this.executors = new TreeMap<>();\n         }\n-    }\n \n-    @Override\n-    public void close() throws Exception {\n-        super.close();\n-        serviceManager.deleteService(PRIMARY_KEY_LOOKUP);\n+        @Override\n+        public void write(InternalRow row, Context context) {\n+            int numberExecutors = row.getInt(0);\n+            if (this.numberExecutors != 0 && this.numberExecutors != numberExecutors) {\n+                throw new IllegalArgumentException(\n+                        String.format(\n+                                \""Number Executors can not be changed! Old %s , New %s .\"",\n+                                this.numberExecutors, numberExecutors));\n+            }\n+            this.numberExecutors = numberExecutors;\n+\n+            int executorId = row.getInt(1);\n+            String hostname = row.getString(2).toString();\n+            int port = row.getInt(3);\n+\n+            executors.put(executorId, new InetSocketAddress(hostname, port));\n+\n+            if (executors.size() == numberExecutors) {\n+                serviceManager.resetService(\n+                        PRIMARY_KEY_LOOKUP, executors.values().toArray(new InetSocketAddress[0]));\n+            }\n+        }\n+\n+        @Override\n+        public void flush(boolean endOfInput) {}\n+\n+        @Override\n+        public void close() {\n+            serviceManager.deleteService(PRIMARY_KEY_LOOKUP);\n+        }\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryService.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryService.java\nindex bd433fe0f00d..752d54cff5a0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryService.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/service/QueryService.java\n@@ -62,7 +62,7 @@ public static void build(StreamExecutionEnvironment env, Table table, int parall\n                                 InternalTypeInfo.fromRowType(QueryExecutorOperator.outputType()),\n                                 executorOperator)\n                         .setParallelism(parallelism)\n-                        .addSink(new QueryAddressRegister(table))\n+                        .sinkTo(new QueryAddressRegister(table))\n                         .setParallelism(1);\n \n         sink.getTransformation().setMaxParallelism(1);\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\nindex c2b4cc0f87e6..25f76ce97683 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\n@@ -32,7 +32,7 @@\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.table.data.RowData;\n \n@@ -171,7 +171,7 @@ protected DataStreamSink<?> doCommit(\n         if (!options.get(SINK_COMMITTER_OPERATOR_CHAINING)) {\n             committed = committed.startNewChain();\n         }\n-        return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n+        return committed.sinkTo(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n     // TODO:refactor FlinkSink to adopt this sink\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\nindex 8d6c3554c76f..002f5887b5f0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n@@ -43,7 +43,7 @@\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.table.api.config.ExecutionConfigOptions;\n \n@@ -316,7 +316,7 @@ protected DataStreamSink<?> doCommit(DataStream<Committable> written, String com\n         }\n         configureGlobalCommitter(\n                 committed, options.get(SINK_COMMITTER_CPU), options.get(SINK_COMMITTER_MEMORY));\n-        return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n+        return committed.sinkTo(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n     public static void configureGlobalCommitter(\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\nindex e1bd112ca751..723f57a30e3f 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n@@ -22,11 +22,11 @@\n import org.apache.paimon.flink.FlinkConnectorOptions;\n import org.apache.paimon.options.Options;\n \n+import org.apache.flink.api.dag.Transformation;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.source.ParallelSourceFunction;\n-import org.apache.flink.streaming.api.transformations.LegacySinkTransformation;\n import org.apache.flink.streaming.api.transformations.OneInputTransformation;\n import org.apache.flink.streaming.api.transformations.PartitionTransformation;\n import org.junit.jupiter.api.Test;\n@@ -64,8 +64,7 @@ public void cancel() {}\n         DataStreamSink<?> dataStreamSink = sink.sinkFrom(input);\n \n         // check the transformation graph\n-        LegacySinkTransformation<?> end =\n-                (LegacySinkTransformation<?>) dataStreamSink.getTransformation();\n+        Transformation<?> end = dataStreamSink.getTransformation();\n         assertThat(end.getName()).isEqualTo(\""end\"");\n \n         OneInputTransformation<?, ?> committer =\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java\nindex a4605b830918..24fb529b59ea 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java\n@@ -173,7 +173,7 @@ private List<JobVertex> verifyChaining(\n \n         List<JobVertex> vertices = new ArrayList<>();\n         env.getStreamGraph().getJobGraph().getVertices().forEach(vertices::add);\n-        JobVertex vertex = findVertex(vertices, \""Writer\"");\n+        JobVertex vertex = findVertex(vertices, \""Writer(write-only)\"");\n \n         if (isWriterChainedWithUpstream) {\n             assertThat(vertex.toString()).contains(\""Source\"");\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4611"", ""pr_id"": 4611, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 2, ""non_test_files_count"": 6, ""pr_changed_files"": [""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/DataCatalogTable.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkGenericCatalog.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/SystemCatalogTable.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkCatalogPropertiesUtil.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkDescriptorProperties.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java""], ""base_commit"": ""475e48791b873c516c4c26774cda3b45a268cd70"", ""head_commit"": ""c9d638043039b6a09c43a51f4f28031e1391d5b5"", ""repo_url"": ""https://github.com/apache/paimon/pull/4611"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4611"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-02T13:11:24.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/DataCatalogTable.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/DataCatalogTable.java\nindex 019d7bd6892f..e141581b476b 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/DataCatalogTable.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/DataCatalogTable.java\n@@ -23,33 +23,55 @@\n import org.apache.paimon.types.DataField;\n \n import org.apache.flink.table.api.Schema;\n-import org.apache.flink.table.api.TableColumn;\n-import org.apache.flink.table.api.TableSchema;\n-import org.apache.flink.table.api.constraints.UniqueConstraint;\n import org.apache.flink.table.catalog.CatalogBaseTable;\n import org.apache.flink.table.catalog.CatalogTable;\n-import org.apache.flink.table.catalog.CatalogTableImpl;\n \n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.stream.Collectors;\n \n-/** A {@link CatalogTableImpl} to wrap {@link FileStoreTable}. */\n-public class DataCatalogTable extends CatalogTableImpl {\n+import static org.apache.flink.util.Preconditions.checkArgument;\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/** A {@link CatalogTable} to wrap {@link FileStoreTable}. */\n+public class DataCatalogTable implements CatalogTable {\n+    // Schema of the table (column names and types)\n+    private final Schema schema;\n+\n+    // Partition keys if this is a partitioned table. It's an empty set if the table is not\n+    // partitioned\n+    private final List<String> partitionKeys;\n+\n+    // Properties of the table\n+    private final Map<String, String> options;\n+\n+    // Comment of the table\n+    private final String comment;\n \n     private final Table table;\n     private final Map<String, String> nonPhysicalColumnComments;\n \n     public DataCatalogTable(\n             Table table,\n-            TableSchema tableSchema,\n+            Schema resolvedSchema,\n             List<String> partitionKeys,\n-            Map<String, String> properties,\n+            Map<String, String> options,\n             String comment,\n             Map<String, String> nonPhysicalColumnComments) {\n-        super(tableSchema, partitionKeys, properties, comment);\n+        this.schema = resolvedSchema;\n+        this.partitionKeys = checkNotNull(partitionKeys, \""partitionKeys cannot be null\"");\n+        this.options = checkNotNull(options, \""options cannot be null\"");\n+\n+        checkArgument(\n+                options.entrySet().stream()\n+                        .allMatch(e -> e.getKey() != null && e.getValue() != null),\n+                \""properties cannot have null keys or values\"");\n+\n+        this.comment = comment;\n+\n         this.table = table;\n         this.nonPhysicalColumnComments = nonPhysicalColumnComments;\n     }\n@@ -66,32 +88,30 @@ public Schema getUnresolvedSchema() {\n                         .filter(dataField -> dataField.description() != null)\n                         .collect(Collectors.toMap(DataField::name, DataField::description));\n \n-        return toSchema(getSchema(), columnComments);\n+        return toSchema(schema, columnComments);\n     }\n \n-    /** Copied from {@link TableSchema#toSchema(Map)} to support versions lower than 1.17. */\n-    private Schema toSchema(TableSchema tableSchema, Map<String, String> comments) {\n+    private Schema toSchema(Schema tableSchema, Map<String, String> comments) {\n         final Schema.Builder builder = Schema.newBuilder();\n-\n         tableSchema\n-                .getTableColumns()\n+                .getColumns()\n                 .forEach(\n                         column -> {\n-                            if (column instanceof TableColumn.PhysicalColumn) {\n-                                final TableColumn.PhysicalColumn c =\n-                                        (TableColumn.PhysicalColumn) column;\n-                                builder.column(c.getName(), c.getType());\n-                            } else if (column instanceof TableColumn.MetadataColumn) {\n-                                final TableColumn.MetadataColumn c =\n-                                        (TableColumn.MetadataColumn) column;\n+                            if (column instanceof Schema.UnresolvedPhysicalColumn) {\n+                                final Schema.UnresolvedPhysicalColumn c =\n+                                        (Schema.UnresolvedPhysicalColumn) column;\n+                                builder.column(c.getName(), c.getDataType());\n+                            } else if (column instanceof Schema.UnresolvedMetadataColumn) {\n+                                final Schema.UnresolvedMetadataColumn c =\n+                                        (Schema.UnresolvedMetadataColumn) column;\n                                 builder.columnByMetadata(\n                                         c.getName(),\n-                                        c.getType(),\n-                                        c.getMetadataAlias().orElse(null),\n+                                        c.getDataType(),\n+                                        c.getMetadataKey(),\n                                         c.isVirtual());\n-                            } else if (column instanceof TableColumn.ComputedColumn) {\n-                                final TableColumn.ComputedColumn c =\n-                                        (TableColumn.ComputedColumn) column;\n+                            } else if (column instanceof Schema.UnresolvedComputedColumn) {\n+                                final Schema.UnresolvedComputedColumn c =\n+                                        (Schema.UnresolvedComputedColumn) column;\n                                 builder.columnByExpression(c.getName(), c.getExpression());\n                             } else {\n                                 throw new IllegalArgumentException(\n@@ -104,19 +124,16 @@ private Schema toSchema(TableSchema tableSchema, Map<String, String> comments) {\n                                 builder.withComment(nonPhysicalColumnComments.get(colName));\n                             }\n                         });\n-\n         tableSchema\n                 .getWatermarkSpecs()\n                 .forEach(\n                         spec ->\n                                 builder.watermark(\n-                                        spec.getRowtimeAttribute(), spec.getWatermarkExpr()));\n-\n+                                        spec.getColumnName(), spec.getWatermarkExpression()));\n         if (tableSchema.getPrimaryKey().isPresent()) {\n-            UniqueConstraint primaryKey = tableSchema.getPrimaryKey().get();\n-            builder.primaryKeyNamed(primaryKey.getName(), primaryKey.getColumns());\n+            Schema.UnresolvedPrimaryKey primaryKey = tableSchema.getPrimaryKey().get();\n+            builder.primaryKeyNamed(primaryKey.getConstraintName(), primaryKey.getColumnNames());\n         }\n-\n         return builder.build();\n     }\n \n@@ -124,7 +141,7 @@ private Schema toSchema(TableSchema tableSchema, Map<String, String> comments) {\n     public CatalogBaseTable copy() {\n         return new DataCatalogTable(\n                 table,\n-                getSchema().copy(),\n+                schema,\n                 new ArrayList<>(getPartitionKeys()),\n                 new HashMap<>(getOptions()),\n                 getComment(),\n@@ -135,10 +152,40 @@ public CatalogBaseTable copy() {\n     public CatalogTable copy(Map<String, String> options) {\n         return new DataCatalogTable(\n                 table,\n-                getSchema(),\n+                schema,\n                 getPartitionKeys(),\n                 options,\n                 getComment(),\n                 nonPhysicalColumnComments);\n     }\n+\n+    @Override\n+    public Optional<String> getDescription() {\n+        return Optional.of(getComment());\n+    }\n+\n+    @Override\n+    public Optional<String> getDetailedDescription() {\n+        return Optional.of(\""This is a catalog table in an im-memory catalog\"");\n+    }\n+\n+    @Override\n+    public boolean isPartitioned() {\n+        return !partitionKeys.isEmpty();\n+    }\n+\n+    @Override\n+    public List<String> getPartitionKeys() {\n+        return partitionKeys;\n+    }\n+\n+    @Override\n+    public Map<String, String> getOptions() {\n+        return options;\n+    }\n+\n+    @Override\n+    public String getComment() {\n+        return comment != null ? comment : \""\"";\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\nindex 09fc0328ef65..ff6013e3fed0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkCatalog.java\n@@ -24,6 +24,7 @@\n import org.apache.paimon.catalog.Identifier;\n import org.apache.paimon.flink.procedure.ProcedureUtil;\n import org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil;\n+import org.apache.paimon.flink.utils.FlinkDescriptorProperties;\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.operation.FileStoreCommit;\n@@ -46,7 +47,6 @@\n import org.apache.paimon.view.View;\n import org.apache.paimon.view.ViewImpl;\n \n-import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.catalog.AbstractCatalog;\n import org.apache.flink.table.catalog.CatalogBaseTable;\n import org.apache.flink.table.catalog.CatalogDatabase;\n@@ -96,7 +96,6 @@\n import org.apache.flink.table.catalog.exceptions.TableNotPartitionedException;\n import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;\n import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n-import org.apache.flink.table.descriptors.DescriptorProperties;\n import org.apache.flink.table.expressions.Expression;\n import org.apache.flink.table.factories.Factory;\n import org.apache.flink.table.procedures.Procedure;\n@@ -121,13 +120,6 @@\n import java.util.function.Function;\n import java.util.stream.Collectors;\n \n-import static org.apache.flink.table.descriptors.DescriptorProperties.COMMENT;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.NAME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_ROWTIME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_EXPR;\n-import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n import static org.apache.flink.table.factories.FactoryUtil.CONNECTOR;\n import static org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType;\n import static org.apache.flink.table.utils.EncodingUtils.decodeBase64ToBytes;\n@@ -152,11 +144,18 @@\n import static org.apache.paimon.flink.LogicalTypeConversion.toLogicalType;\n import static org.apache.paimon.flink.log.LogStoreRegister.registerLogSystem;\n import static org.apache.paimon.flink.log.LogStoreRegister.unRegisterLogSystem;\n+import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.SCHEMA;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.compoundKey;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.deserializeWatermarkSpec;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.nonPhysicalColumnsCount;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.serializeNewWatermarkSpec;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.COMMENT;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.NAME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_ROWTIME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_EXPR;\n import static org.apache.paimon.flink.utils.TableStatsUtil.createTableColumnStats;\n import static org.apache.paimon.flink.utils.TableStatsUtil.createTableStats;\n import static org.apache.paimon.utils.Preconditions.checkArgument;\n@@ -1002,18 +1001,18 @@ private static void validateAlterTable(CatalogBaseTable ct1, CatalogBaseTable ct\n         }\n         // materialized table is not resolved at this time.\n         if (!table1IsMaterialized) {\n-            org.apache.flink.table.api.TableSchema ts1 = ct1.getSchema();\n-            org.apache.flink.table.api.TableSchema ts2 = ct2.getSchema();\n+            org.apache.flink.table.api.Schema ts1 = ct1.getUnresolvedSchema();\n+            org.apache.flink.table.api.Schema ts2 = ct2.getUnresolvedSchema();\n             boolean pkEquality = false;\n \n             if (ts1.getPrimaryKey().isPresent() && ts2.getPrimaryKey().isPresent()) {\n                 pkEquality =\n                         Objects.equals(\n-                                        ts1.getPrimaryKey().get().getType(),\n-                                        ts2.getPrimaryKey().get().getType())\n+                                        ts1.getPrimaryKey().get().getConstraintName(),\n+                                        ts2.getPrimaryKey().get().getConstraintName())\n                                 && Objects.equals(\n-                                        ts1.getPrimaryKey().get().getColumns(),\n-                                        ts2.getPrimaryKey().get().getColumns());\n+                                        ts1.getPrimaryKey().get().getColumnNames(),\n+                                        ts2.getPrimaryKey().get().getColumnNames());\n             } else if (!ts1.getPrimaryKey().isPresent() && !ts2.getPrimaryKey().isPresent()) {\n                 pkEquality = true;\n             }\n@@ -1057,7 +1056,8 @@ public final void close() throws CatalogException {\n     private CatalogBaseTable toCatalogTable(Table table) {\n         Map<String, String> newOptions = new HashMap<>(table.options());\n \n-        TableSchema.Builder builder = TableSchema.builder();\n+        org.apache.flink.table.api.Schema.Builder builder =\n+                org.apache.flink.table.api.Schema.newBuilder();\n         Map<String, String> nonPhysicalColumnComments = new HashMap<>();\n \n         // add columns\n@@ -1072,10 +1072,10 @@ private CatalogBaseTable toCatalogTable(Table table) {\n             if (optionalName == null || physicalColumns.contains(optionalName)) {\n                 // build physical column from table row field\n                 RowType.RowField field = physicalRowFields.get(physicalColumnIndex++);\n-                builder.field(field.getName(), fromLogicalToDataType(field.getType()));\n+                builder.column(field.getName(), fromLogicalToDataType(field.getType()));\n             } else {\n                 // build non-physical column from options\n-                builder.add(deserializeNonPhysicalColumn(newOptions, i));\n+                deserializeNonPhysicalColumn(newOptions, i, builder);\n                 if (newOptions.containsKey(compoundKey(SCHEMA, i, COMMENT))) {\n                     nonPhysicalColumnComments.put(\n                             optionalName, newOptions.get(compoundKey(SCHEMA, i, COMMENT)));\n@@ -1087,22 +1087,18 @@ private CatalogBaseTable toCatalogTable(Table table) {\n         // extract watermark information\n         if (newOptions.keySet().stream()\n                 .anyMatch(key -> key.startsWith(compoundKey(SCHEMA, WATERMARK)))) {\n-            builder.watermark(deserializeWatermarkSpec(newOptions));\n+            deserializeWatermarkSpec(newOptions, builder);\n         }\n \n         // add primary keys\n         if (table.primaryKeys().size() > 0) {\n-            builder.primaryKey(\n-                    table.primaryKeys().stream().collect(Collectors.joining(\""_\"", \""PK_\"", \""\"")),\n-                    table.primaryKeys().toArray(new String[0]));\n+            builder.primaryKey(table.primaryKeys());\n         }\n \n-        TableSchema schema = builder.build();\n+        org.apache.flink.table.api.Schema schema = builder.build();\n \n         // remove schema from options\n-        DescriptorProperties removeProperties = new DescriptorProperties(false);\n-        removeProperties.putTableSchema(SCHEMA, schema);\n-        removeProperties.asMap().keySet().forEach(newOptions::remove);\n+        FlinkDescriptorProperties.removeSchemaKeys(SCHEMA, schema, newOptions);\n \n         Options options = Options.fromMap(newOptions);\n         if (TableType.MATERIALIZED_TABLE == options.get(CoreOptions.TYPE)) {\n@@ -1118,7 +1114,10 @@ private CatalogBaseTable toCatalogTable(Table table) {\n     }\n \n     private CatalogMaterializedTable buildMaterializedTable(\n-            Table table, Map<String, String> newOptions, TableSchema schema, Options options) {\n+            Table table,\n+            Map<String, String> newOptions,\n+            org.apache.flink.table.api.Schema schema,\n+            Options options) {\n         String definitionQuery = options.get(MATERIALIZED_TABLE_DEFINITION_QUERY);\n         IntervalFreshness freshness =\n                 IntervalFreshness.of(\n@@ -1142,7 +1141,7 @@ private CatalogMaterializedTable buildMaterializedTable(\n         // remove materialized table related options\n         allMaterializedTableAttributes().forEach(newOptions::remove);\n         return CatalogMaterializedTable.newBuilder()\n-                .schema(schema.toSchema())\n+                .schema(schema)\n                 .comment(table.comment().orElse(\""\""))\n                 .partitionKeys(table.partitionKeys())\n                 .options(newOptions)\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkGenericCatalog.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkGenericCatalog.java\nindex 37bed2d0480f..75af5917bb49 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkGenericCatalog.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkGenericCatalog.java\n@@ -48,7 +48,6 @@\n import org.apache.flink.table.expressions.Expression;\n import org.apache.flink.table.factories.Factory;\n import org.apache.flink.table.factories.FunctionDefinitionFactory;\n-import org.apache.flink.table.factories.TableFactory;\n import org.apache.flink.table.procedures.Procedure;\n \n import java.util.List;\n@@ -86,11 +85,6 @@ public Optional<Factory> getFactory() {\n                 new FlinkGenericTableFactory(paimon.getFactory().get(), flink.getFactory().get()));\n     }\n \n-    @Override\n-    public Optional<TableFactory> getTableFactory() {\n-        return flink.getTableFactory();\n-    }\n-\n     @Override\n     public Optional<FunctionDefinitionFactory> getFunctionDefinitionFactory() {\n         return flink.getFunctionDefinitionFactory();\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/SystemCatalogTable.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/SystemCatalogTable.java\nindex d5d843d91bb1..f88a808713c2 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/SystemCatalogTable.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/SystemCatalogTable.java\n@@ -22,7 +22,6 @@\n import org.apache.paimon.table.system.AuditLogTable;\n \n import org.apache.flink.table.api.Schema;\n-import org.apache.flink.table.api.WatermarkSpec;\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.types.utils.TypeConversions;\n \n@@ -32,11 +31,11 @@\n import java.util.Map;\n import java.util.Optional;\n \n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK;\n-import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n import static org.apache.paimon.flink.LogicalTypeConversion.toLogicalType;\n+import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.SCHEMA;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.compoundKey;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.deserializeWatermarkSpec;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK;\n \n /** A {@link CatalogTable} to represent system table. */\n public class SystemCatalogTable implements CatalogTable {\n@@ -60,11 +59,8 @@ public Schema getUnresolvedSchema() {\n             Map<String, String> newOptions = new HashMap<>(table.options());\n             if (newOptions.keySet().stream()\n                     .anyMatch(key -> key.startsWith(compoundKey(SCHEMA, WATERMARK)))) {\n-                WatermarkSpec watermarkSpec = deserializeWatermarkSpec(newOptions);\n-                return builder.watermark(\n-                                watermarkSpec.getRowtimeAttribute(),\n-                                watermarkSpec.getWatermarkExpr())\n-                        .build();\n+                deserializeWatermarkSpec(newOptions, builder);\n+                return builder.build();\n             }\n         }\n         return builder.build();\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkCatalogPropertiesUtil.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkCatalogPropertiesUtil.java\nindex b0f99a6e89e4..fa84a1ca070d 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkCatalogPropertiesUtil.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkCatalogPropertiesUtil.java\n@@ -20,8 +20,7 @@\n \n import org.apache.paimon.shade.guava30.com.google.common.collect.ImmutableSet;\n \n-import org.apache.flink.table.api.TableColumn;\n-import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.api.Schema;\n import org.apache.flink.table.catalog.Column;\n import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.types.DataType;\n@@ -36,48 +35,23 @@\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n-import static org.apache.flink.table.descriptors.DescriptorProperties.COMMENT;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.DATA_TYPE;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.EXPR;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.METADATA;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.NAME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.VIRTUAL;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_ROWTIME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_EXPR;\n-import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.COMMENT;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.DATA_TYPE;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.EXPR;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.METADATA;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.NAME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.VIRTUAL;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_ROWTIME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_EXPR;\n \n /**\n  * Utilities for ser/deserializing non-physical columns and watermark into/from a map of string\n  * properties.\n  */\n public class FlinkCatalogPropertiesUtil {\n-\n-    public static Map<String, String> serializeNonPhysicalColumns(\n-            Map<String, Integer> indexMap, List<TableColumn> nonPhysicalColumns) {\n-        Map<String, String> serialized = new HashMap<>();\n-        for (TableColumn c : nonPhysicalColumns) {\n-            int index = indexMap.get(c.getName());\n-            serialized.put(compoundKey(SCHEMA, index, NAME), c.getName());\n-            serialized.put(\n-                    compoundKey(SCHEMA, index, DATA_TYPE),\n-                    c.getType().getLogicalType().asSerializableString());\n-            if (c instanceof TableColumn.ComputedColumn) {\n-                TableColumn.ComputedColumn computedColumn = (TableColumn.ComputedColumn) c;\n-                serialized.put(compoundKey(SCHEMA, index, EXPR), computedColumn.getExpression());\n-            } else {\n-                TableColumn.MetadataColumn metadataColumn = (TableColumn.MetadataColumn) c;\n-                serialized.put(\n-                        compoundKey(SCHEMA, index, METADATA),\n-                        metadataColumn.getMetadataAlias().orElse(metadataColumn.getName()));\n-                serialized.put(\n-                        compoundKey(SCHEMA, index, VIRTUAL),\n-                        Boolean.toString(metadataColumn.isVirtual()));\n-            }\n-        }\n-        return serialized;\n-    }\n+    public static final String SCHEMA = \""schema\"";\n \n     /** Serialize non-physical columns of new api. */\n     public static Map<String, String> serializeNonPhysicalNewColumns(ResolvedSchema schema) {\n@@ -119,22 +93,6 @@ public static Map<String, String> serializeNonPhysicalNewColumns(ResolvedSchema\n         return serialized;\n     }\n \n-    public static Map<String, String> serializeWatermarkSpec(WatermarkSpec watermarkSpec) {\n-        Map<String, String> serializedWatermarkSpec = new HashMap<>();\n-        String watermarkPrefix = compoundKey(SCHEMA, WATERMARK, 0);\n-        serializedWatermarkSpec.put(\n-                compoundKey(watermarkPrefix, WATERMARK_ROWTIME),\n-                watermarkSpec.getRowtimeAttribute());\n-        serializedWatermarkSpec.put(\n-                compoundKey(watermarkPrefix, WATERMARK_STRATEGY_EXPR),\n-                watermarkSpec.getWatermarkExpr());\n-        serializedWatermarkSpec.put(\n-                compoundKey(watermarkPrefix, WATERMARK_STRATEGY_DATA_TYPE),\n-                watermarkSpec.getWatermarkExprOutputType().getLogicalType().asSerializableString());\n-\n-        return serializedWatermarkSpec;\n-    }\n-\n     public static Map<String, String> serializeNewWatermarkSpec(\n             org.apache.flink.table.catalog.WatermarkSpec watermarkSpec) {\n         Map<String, String> serializedWatermarkSpec = new HashMap<>();\n@@ -219,7 +177,8 @@ private static boolean isColumnNameKey(String key) {\n                 && SCHEMA_COLUMN_NAME_SUFFIX.matcher(key.substring(SCHEMA.length() + 1)).matches();\n     }\n \n-    public static TableColumn deserializeNonPhysicalColumn(Map<String, String> options, int index) {\n+    public static void deserializeNonPhysicalColumn(\n+            Map<String, String> options, int index, Schema.Builder builder) {\n         String nameKey = compoundKey(SCHEMA, index, NAME);\n         String dataTypeKey = compoundKey(SCHEMA, index, DATA_TYPE);\n         String exprKey = compoundKey(SCHEMA, index, EXPR);\n@@ -227,45 +186,42 @@ public static TableColumn deserializeNonPhysicalColumn(Map<String, String> optio\n         String virtualKey = compoundKey(SCHEMA, index, VIRTUAL);\n \n         String name = options.get(nameKey);\n-        DataType dataType =\n-                TypeConversions.fromLogicalToDataType(\n-                        LogicalTypeParser.parse(options.get(dataTypeKey)));\n \n-        TableColumn column;\n         if (options.containsKey(exprKey)) {\n-            column = TableColumn.computed(name, dataType, options.get(exprKey));\n+            final String expr = options.get(exprKey);\n+            builder.columnByExpression(name, expr);\n         } else if (options.containsKey(metadataKey)) {\n             String metadataAlias = options.get(metadataKey);\n             boolean isVirtual = Boolean.parseBoolean(options.get(virtualKey));\n-            column =\n-                    metadataAlias.equals(name)\n-                            ? TableColumn.metadata(name, dataType, isVirtual)\n-                            : TableColumn.metadata(name, dataType, metadataAlias, isVirtual);\n+            DataType dataType =\n+                    TypeConversions.fromLogicalToDataType(\n+                            LogicalTypeParser.parse(\n+                                    options.get(dataTypeKey),\n+                                    Thread.currentThread().getContextClassLoader()));\n+            if (metadataAlias.equals(name)) {\n+                builder.columnByMetadata(name, dataType, isVirtual);\n+            } else {\n+                builder.columnByMetadata(name, dataType, metadataAlias, isVirtual);\n+            }\n         } else {\n             throw new RuntimeException(\n                     String.format(\n                             \""Failed to build non-physical column. Current index is %s, options are %s\"",\n                             index, options));\n         }\n-\n-        return column;\n     }\n \n-    public static WatermarkSpec deserializeWatermarkSpec(Map<String, String> options) {\n+    public static void deserializeWatermarkSpec(\n+            Map<String, String> options, Schema.Builder builder) {\n         String watermarkPrefixKey = compoundKey(SCHEMA, WATERMARK);\n \n         String rowtimeKey = compoundKey(watermarkPrefixKey, 0, WATERMARK_ROWTIME);\n         String exprKey = compoundKey(watermarkPrefixKey, 0, WATERMARK_STRATEGY_EXPR);\n-        String dataTypeKey = compoundKey(watermarkPrefixKey, 0, WATERMARK_STRATEGY_DATA_TYPE);\n \n         String rowtimeAttribute = options.get(rowtimeKey);\n         String watermarkExpressionString = options.get(exprKey);\n-        DataType watermarkExprOutputType =\n-                TypeConversions.fromLogicalToDataType(\n-                        LogicalTypeParser.parse(options.get(dataTypeKey)));\n \n-        return new WatermarkSpec(\n-                rowtimeAttribute, watermarkExpressionString, watermarkExprOutputType);\n+        builder.watermark(rowtimeAttribute, watermarkExpressionString);\n     }\n \n     public static String compoundKey(Object... components) {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkDescriptorProperties.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkDescriptorProperties.java\nnew file mode 100644\nindex 000000000000..edc73ca7bf41\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/FlinkDescriptorProperties.java\n@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.utils;\n+\n+import org.apache.flink.table.api.Schema;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * Utility class for having a unified string-based representation of Table API related classes such\n+ * as Schema, TypeInformation, etc.\n+ *\n+ * <p>Note to implementers: Please try to reuse key names as much as possible. Key-names should be\n+ * hierarchical and lower case. Use \""-\"" instead of dots or camel case. E.g.,\n+ * connector.schema.start-from = from-earliest. Try not to use the higher level in a key-name. E.g.,\n+ * instead of connector.kafka.kafka-version use connector.kafka.version.\n+ *\n+ * <p>Properties with key normalization enabled contain only lower-case keys.\n+ */\n+public class FlinkDescriptorProperties {\n+\n+    public static final String NAME = \""name\"";\n+\n+    public static final String DATA_TYPE = \""data-type\"";\n+\n+    public static final String EXPR = \""expr\"";\n+\n+    public static final String METADATA = \""metadata\"";\n+\n+    public static final String VIRTUAL = \""virtual\"";\n+\n+    public static final String WATERMARK = \""watermark\"";\n+\n+    public static final String WATERMARK_ROWTIME = \""rowtime\"";\n+\n+    public static final String WATERMARK_STRATEGY = \""strategy\"";\n+\n+    public static final String WATERMARK_STRATEGY_EXPR = WATERMARK_STRATEGY + '.' + EXPR;\n+\n+    public static final String WATERMARK_STRATEGY_DATA_TYPE = WATERMARK_STRATEGY + '.' + DATA_TYPE;\n+\n+    public static final String PRIMARY_KEY_NAME = \""primary-key.name\"";\n+\n+    public static final String PRIMARY_KEY_COLUMNS = \""primary-key.columns\"";\n+\n+    public static final String COMMENT = \""comment\"";\n+\n+    public static void removeSchemaKeys(String key, Schema schema, Map<String, String> options) {\n+        checkNotNull(key);\n+        checkNotNull(schema);\n+\n+        List<String> subKeys = Arrays.asList(NAME, DATA_TYPE, EXPR, METADATA, VIRTUAL);\n+        for (int idx = 0; idx < schema.getColumns().size(); idx++) {\n+            for (String subKey : subKeys) {\n+                options.remove(key + '.' + idx + '.' + subKey);\n+            }\n+        }\n+\n+        if (!schema.getWatermarkSpecs().isEmpty()) {\n+            subKeys =\n+                    Arrays.asList(\n+                            WATERMARK_ROWTIME,\n+                            WATERMARK_STRATEGY_EXPR,\n+                            WATERMARK_STRATEGY_DATA_TYPE);\n+            for (int idx = 0; idx < schema.getWatermarkSpecs().size(); idx++) {\n+                for (String subKey : subKeys) {\n+                    options.remove(key + '.' + WATERMARK + '.' + idx + '.' + subKey);\n+                }\n+            }\n+        }\n+\n+        schema.getPrimaryKey()\n+                .ifPresent(\n+                        pk -> {\n+                            options.remove(key + '.' + PRIMARY_KEY_NAME);\n+                            options.remove(key + '.' + PRIMARY_KEY_COLUMNS);\n+                        });\n+    }\n+}\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java\nindex 9268a236b6cb..e32150b1fe82 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogPropertiesUtilTest.java\n@@ -21,27 +21,35 @@\n import org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil;\n \n import org.apache.flink.table.api.DataTypes;\n-import org.apache.flink.table.api.TableColumn;\n-import org.apache.flink.table.api.WatermarkSpec;\n+import org.apache.flink.table.api.Schema;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n+import org.apache.flink.table.catalog.WatermarkSpec;\n+import org.apache.flink.table.expressions.Expression;\n+import org.apache.flink.table.expressions.ExpressionVisitor;\n+import org.apache.flink.table.expressions.ResolvedExpression;\n+import org.apache.flink.table.expressions.SqlCallExpression;\n+import org.apache.flink.table.types.DataType;\n import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n-import static org.apache.flink.table.descriptors.DescriptorProperties.DATA_TYPE;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.EXPR;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.METADATA;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.NAME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.VIRTUAL;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_ROWTIME;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n-import static org.apache.flink.table.descriptors.DescriptorProperties.WATERMARK_STRATEGY_EXPR;\n-import static org.apache.flink.table.descriptors.Schema.SCHEMA;\n+import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.SCHEMA;\n import static org.apache.paimon.flink.utils.FlinkCatalogPropertiesUtil.compoundKey;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.DATA_TYPE;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.EXPR;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.METADATA;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.NAME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.VIRTUAL;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_ROWTIME;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_DATA_TYPE;\n+import static org.apache.paimon.flink.utils.FlinkDescriptorProperties.WATERMARK_STRATEGY_EXPR;\n import static org.assertj.core.api.Assertions.assertThat;\n \n /** Test for {@link FlinkCatalogPropertiesUtil}. */\n@@ -49,18 +57,27 @@ public class FlinkCatalogPropertiesUtilTest {\n \n     @Test\n     public void testSerDeNonPhysicalColumns() {\n-        Map<String, Integer> indexMap = new HashMap<>();\n-        indexMap.put(\""comp\"", 2);\n-        indexMap.put(\""meta1\"", 3);\n-        indexMap.put(\""meta2\"", 5);\n-        List<TableColumn> columns = new ArrayList<>();\n-        columns.add(TableColumn.computed(\""comp\"", DataTypes.INT(), \""`k` * 2\""));\n-        columns.add(TableColumn.metadata(\""meta1\"", DataTypes.VARCHAR(10)));\n-        columns.add(TableColumn.metadata(\""meta2\"", DataTypes.BIGINT().notNull(), \""price\"", true));\n+        List<Schema.UnresolvedColumn> columns = new ArrayList<>();\n+        columns.add(new Schema.UnresolvedComputedColumn(\""comp\"", new SqlCallExpression(\""`k` * 2\"")));\n+        columns.add(\n+                new Schema.UnresolvedMetadataColumn(\""meta1\"", DataTypes.VARCHAR(10), null, false));\n+        columns.add(\n+                new Schema.UnresolvedMetadataColumn(\n+                        \""meta2\"", DataTypes.BIGINT().notNull(), \""price\"", true, null));\n+\n+        List<Column> resolvedColumns = new ArrayList<>();\n+        resolvedColumns.add(Column.physical(\""phy1\"", DataTypes.INT()));\n+        resolvedColumns.add(Column.physical(\""phy2\"", DataTypes.INT()));\n+        resolvedColumns.add(\n+                Column.computed(\""comp\"", new TestResolvedExpression(\""`k` * 2\"", DataTypes.INT())));\n+        resolvedColumns.add(Column.metadata(\""meta1\"", DataTypes.VARCHAR(10), null, false));\n+        resolvedColumns.add(Column.physical(\""phy3\"", DataTypes.INT()));\n+        resolvedColumns.add(Column.metadata(\""meta2\"", DataTypes.BIGINT().notNull(), \""price\"", true));\n \n         // validate serialization\n         Map<String, String> serialized =\n-                FlinkCatalogPropertiesUtil.serializeNonPhysicalColumns(indexMap, columns);\n+                FlinkCatalogPropertiesUtil.serializeNonPhysicalNewColumns(\n+                        new ResolvedSchema(resolvedColumns, Collections.emptyList(), null));\n \n         Map<String, String> expected = new HashMap<>();\n         expected.put(compoundKey(SCHEMA, 2, NAME), \""comp\"");\n@@ -80,27 +97,26 @@ public void testSerDeNonPhysicalColumns() {\n         assertThat(serialized).containsExactlyInAnyOrderEntriesOf(expected);\n \n         // validate deserialization\n-        List<TableColumn> deserialized = new ArrayList<>();\n-        deserialized.add(FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 2));\n-        deserialized.add(FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 3));\n-        deserialized.add(FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 5));\n+        Schema.Builder builder = Schema.newBuilder();\n+        FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 2, builder);\n+        FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 3, builder);\n+        FlinkCatalogPropertiesUtil.deserializeNonPhysicalColumn(serialized, 5, builder);\n \n-        assertThat(deserialized).isEqualTo(columns);\n-\n-        // validate that\n+        assertThat(builder.build().getColumns())\n+                .containsExactly(columns.toArray(new Schema.UnresolvedColumn[0]));\n     }\n \n     @Test\n     public void testSerDeWatermarkSpec() {\n         WatermarkSpec watermarkSpec =\n-                new WatermarkSpec(\n+                WatermarkSpec.of(\n                         \""test_time\"",\n-                        \""`test_time` - INTERVAL '0.001' SECOND\"",\n-                        DataTypes.TIMESTAMP(3));\n+                        new TestResolvedExpression(\n+                                \""`test_time` - INTERVAL '0.001' SECOND\"", DataTypes.TIMESTAMP(3)));\n \n         // validate serialization\n         Map<String, String> serialized =\n-                FlinkCatalogPropertiesUtil.serializeWatermarkSpec(watermarkSpec);\n+                FlinkCatalogPropertiesUtil.serializeNewWatermarkSpec(watermarkSpec);\n \n         Map<String, String> expected = new HashMap<>();\n         String watermarkPrefix = compoundKey(SCHEMA, WATERMARK, 0);\n@@ -113,9 +129,13 @@ public void testSerDeWatermarkSpec() {\n         assertThat(serialized).containsExactlyInAnyOrderEntriesOf(expected);\n \n         // validate serialization\n-        WatermarkSpec deserialized =\n-                FlinkCatalogPropertiesUtil.deserializeWatermarkSpec(serialized);\n-        assertThat(deserialized).isEqualTo(watermarkSpec);\n+        Schema.Builder builder = Schema.newBuilder();\n+        FlinkCatalogPropertiesUtil.deserializeWatermarkSpec(serialized, builder);\n+        assertThat(builder.build().getWatermarkSpecs()).hasSize(1);\n+        Schema.UnresolvedWatermarkSpec actual = builder.build().getWatermarkSpecs().get(0);\n+        assertThat(actual.getColumnName()).isEqualTo(watermarkSpec.getRowtimeAttribute());\n+        assertThat(actual.getWatermarkExpression().asSummaryString())\n+                .isEqualTo(watermarkSpec.getWatermarkExpression().asSummaryString());\n     }\n \n     @Test\n@@ -150,4 +170,44 @@ public void testNonPhysicalColumnsCount() {\n                                 oldStyleOptions, Arrays.asList(\""phy1\"", \""phy2\"")))\n                 .isEqualTo(3);\n     }\n+\n+    private static class TestResolvedExpression implements ResolvedExpression {\n+        private final String name;\n+        private final DataType outputDataType;\n+\n+        private TestResolvedExpression(String name, DataType outputDataType) {\n+            this.name = name;\n+            this.outputDataType = outputDataType;\n+        }\n+\n+        @Override\n+        public DataType getOutputDataType() {\n+            return outputDataType;\n+        }\n+\n+        @Override\n+        public List<ResolvedExpression> getResolvedChildren() {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public String asSummaryString() {\n+            return new SqlCallExpression(name).asSummaryString();\n+        }\n+\n+        @Override\n+        public String asSerializableString() {\n+            return name;\n+        }\n+\n+        @Override\n+        public List<Expression> getChildren() {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public <R> R accept(ExpressionVisitor<R> expressionVisitor) {\n+            return null;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\nindex 27a89510975f..e4286eb18172 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkCatalogTest.java\n@@ -850,7 +850,7 @@ private static void checkEquals(CatalogBaseTable t1, CatalogBaseTable t2) {\n         assertThat(t2.getComment()).isEqualTo(t1.getComment());\n         assertThat(t2.getOptions()).isEqualTo(t1.getOptions());\n         if (t1.getTableKind() == CatalogBaseTable.TableKind.TABLE) {\n-            assertThat(t2.getSchema()).isEqualTo(t1.getSchema());\n+            assertThat(t2.getUnresolvedSchema()).isEqualTo(t1.getUnresolvedSchema());\n             assertThat(((CatalogTable) (t2)).getPartitionKeys())\n                     .isEqualTo(((CatalogTable) (t1)).getPartitionKeys());\n             assertThat(((CatalogTable) (t2)).isPartitioned())\n@@ -864,7 +864,12 @@ private static void checkEquals(CatalogBaseTable t1, CatalogBaseTable t2) {\n                                             t2.getUnresolvedSchema()\n                                                     .resolve(new TestSchemaResolver()))\n                                     .build())\n-                    .isEqualTo(t1.getSchema().toSchema());\n+                    .isEqualTo(\n+                            Schema.newBuilder()\n+                                    .fromResolvedSchema(\n+                                            t1.getUnresolvedSchema()\n+                                                    .resolve(new TestSchemaResolver()))\n+                                    .build());\n             assertThat(mt2.getPartitionKeys()).isEqualTo(mt1.getPartitionKeys());\n             assertThat(mt2.isPartitioned()).isEqualTo(mt1.isPartitioned());\n             // validate definition query\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4591"", ""pr_id"": 4591, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 15, ""non_test_files_count"": 40, ""pr_changed_files"": [""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketWriteOperator.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcFixedBucketSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperator.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperator.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketWriteOperator.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendBypassCompactWorkerOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendCompactWorkerOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorFactory.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorFactory.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperatorFactory.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactorSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/DynamicBucketRowWriteOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FixedBucketSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSinkBuilder.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/LocalMergeOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTablesStoreCompactOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/PrepareCommitOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RewriteFileIndexSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDynamicBucketSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowUnawareBucketSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCompactOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableWriteOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketCompactionSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/GlobalDynamicBucketSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/IndexBootstrapOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperator.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperatorFactory.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java""], ""base_commit"": ""475e48791b873c516c4c26774cda3b45a268cd70"", ""head_commit"": ""833100e0c7beacfe1dc8fc22fb35e5c36732778c"", ""repo_url"": ""https://github.com/apache/paimon/pull/4591"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4591"", ""dockerfile"": """", ""pr_merged_at"": ""2024-12-01T11:38:54.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketSink.java\nindex 574ff685f3fa..6d9e3a4a7c82 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketSink.java\n@@ -25,7 +25,7 @@\n import org.apache.paimon.table.sink.KeyAndBucketExtractor;\n \n import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n /** {@link CdcDynamicBucketSinkBase} for {@link CdcRecord}. */\n public class CdcDynamicBucketSink extends CdcDynamicBucketSinkBase<CdcRecord> {\n@@ -42,8 +42,8 @@ protected KeyAndBucketExtractor<CdcRecord> createExtractor(TableSchema schema) {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Tuple2<CdcRecord, Integer>, Committable> createWriteOperator(\n-            StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new CdcDynamicBucketWriteOperator(table, writeProvider, commitUser);\n+    protected OneInputStreamOperatorFactory<Tuple2<CdcRecord, Integer>, Committable>\n+            createWriteOperatorFactory(StoreSinkWrite.Provider writeProvider, String commitUser) {\n+        return new CdcDynamicBucketWriteOperator.Factory(table, writeProvider, commitUser);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketWriteOperator.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketWriteOperator.java\nindex b2fbdc3e93ee..b0b135b3610b 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketWriteOperator.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcDynamicBucketWriteOperator.java\n@@ -19,6 +19,7 @@\n package org.apache.paimon.flink.sink.cdc;\n \n import org.apache.paimon.data.GenericRow;\n+import org.apache.paimon.flink.sink.Committable;\n import org.apache.paimon.flink.sink.PrepareCommitOperator;\n import org.apache.paimon.flink.sink.StoreSinkWrite;\n import org.apache.paimon.flink.sink.TableWriteOperator;\n@@ -26,6 +27,9 @@\n \n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n import java.io.IOException;\n@@ -43,11 +47,12 @@ public class CdcDynamicBucketWriteOperator extends TableWriteOperator<Tuple2<Cdc\n \n     private final long retrySleepMillis;\n \n-    public CdcDynamicBucketWriteOperator(\n+    private CdcDynamicBucketWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(table, storeSinkWriteProvider, initialCommitUser);\n+        super(parameters, table, storeSinkWriteProvider, initialCommitUser);\n         this.retrySleepMillis =\n                 table.coreOptions().toConfiguration().get(RETRY_SLEEP_TIME).toMillis();\n     }\n@@ -85,4 +90,30 @@ public void processElement(StreamRecord<Tuple2<CdcRecord, Integer>> element) thr\n             throw new IOException(e);\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link CdcDynamicBucketWriteOperator}. */\n+    public static class Factory extends TableWriteOperator.Factory<Tuple2<CdcRecord, Integer>> {\n+\n+        public Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new CdcDynamicBucketWriteOperator(\n+                            parameters, table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return CdcDynamicBucketWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcFixedBucketSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcFixedBucketSink.java\nindex 59bdb192beea..bec9508888b4 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcFixedBucketSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcFixedBucketSink.java\n@@ -24,7 +24,7 @@\n import org.apache.paimon.flink.sink.StoreSinkWrite;\n import org.apache.paimon.table.FileStoreTable;\n \n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n /**\n  * A {@link FlinkSink} for fixed-bucket table which accepts {@link CdcRecord} and waits for a schema\n@@ -39,8 +39,8 @@ public CdcFixedBucketSink(FileStoreTable table) {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<CdcRecord, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<CdcRecord, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new CdcRecordStoreWriteOperator(table, writeProvider, commitUser);\n+        return new CdcRecordStoreWriteOperator.Factory(table, writeProvider, commitUser);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperator.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperator.java\nindex 7d72fe3e801f..5db111a30047 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperator.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperator.java\n@@ -38,6 +38,9 @@\n \n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n import java.io.IOException;\n@@ -74,12 +77,13 @@ public class CdcRecordStoreMultiWriteOperator\n     private String commitUser;\n     private ExecutorService compactExecutor;\n \n-    public CdcRecordStoreMultiWriteOperator(\n+    private CdcRecordStoreMultiWriteOperator(\n+            StreamOperatorParameters<MultiTableCommittable> parameters,\n             Catalog.Loader catalogLoader,\n             StoreSinkWrite.WithWriteBufferProvider storeSinkWriteProvider,\n             String initialCommitUser,\n             Options options) {\n-        super(options);\n+        super(parameters, options);\n         this.catalogLoader = catalogLoader;\n         this.storeSinkWriteProvider = storeSinkWriteProvider;\n         this.initialCommitUser = initialCommitUser;\n@@ -254,4 +258,42 @@ public Map<Identifier, StoreSinkWrite> writes() {\n     public String commitUser() {\n         return commitUser;\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link CdcRecordStoreMultiWriteOperator}. */\n+    public static class Factory\n+            extends PrepareCommitOperator.Factory<CdcMultiplexRecord, MultiTableCommittable> {\n+        private final StoreSinkWrite.WithWriteBufferProvider storeSinkWriteProvider;\n+        private final String initialCommitUser;\n+        private final Catalog.Loader catalogLoader;\n+\n+        public Factory(\n+                Catalog.Loader catalogLoader,\n+                StoreSinkWrite.WithWriteBufferProvider storeSinkWriteProvider,\n+                String initialCommitUser,\n+                Options options) {\n+            super(options);\n+            this.catalogLoader = catalogLoader;\n+            this.storeSinkWriteProvider = storeSinkWriteProvider;\n+            this.initialCommitUser = initialCommitUser;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<MultiTableCommittable>> T createStreamOperator(\n+                StreamOperatorParameters<MultiTableCommittable> parameters) {\n+            return (T)\n+                    new CdcRecordStoreMultiWriteOperator(\n+                            parameters,\n+                            catalogLoader,\n+                            storeSinkWriteProvider,\n+                            initialCommitUser,\n+                            options);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return CdcRecordStoreMultiWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperator.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperator.java\nindex dd0aa2e5622c..195e683daaf6 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperator.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperator.java\n@@ -19,6 +19,7 @@\n package org.apache.paimon.flink.sink.cdc;\n \n import org.apache.paimon.data.GenericRow;\n+import org.apache.paimon.flink.sink.Committable;\n import org.apache.paimon.flink.sink.PrepareCommitOperator;\n import org.apache.paimon.flink.sink.StoreSinkWrite;\n import org.apache.paimon.flink.sink.TableWriteOperator;\n@@ -27,6 +28,9 @@\n import org.apache.paimon.table.FileStoreTable;\n \n import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n import java.io.IOException;\n@@ -50,11 +54,12 @@ public class CdcRecordStoreWriteOperator extends TableWriteOperator<CdcRecord> {\n \n     private final long retrySleepMillis;\n \n-    public CdcRecordStoreWriteOperator(\n+    protected CdcRecordStoreWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(table, storeSinkWriteProvider, initialCommitUser);\n+        super(parameters, table, storeSinkWriteProvider, initialCommitUser);\n         this.retrySleepMillis =\n                 table.coreOptions().toConfiguration().get(RETRY_SLEEP_TIME).toMillis();\n     }\n@@ -92,4 +97,30 @@ public void processElement(StreamRecord<CdcRecord> element) throws Exception {\n             throw new IOException(e);\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link CdcRecordStoreWriteOperator}. */\n+    public static class Factory extends TableWriteOperator.Factory<CdcRecord> {\n+\n+        public Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new CdcRecordStoreWriteOperator(\n+                            parameters, table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return CdcRecordStoreWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketSink.java\nindex 313f4d013ef8..820ef7728f8c 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketSink.java\n@@ -24,7 +24,7 @@\n import org.apache.paimon.table.FileStoreTable;\n \n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n \n@@ -42,9 +42,9 @@ public CdcUnawareBucketSink(FileStoreTable table, Integer parallelism) {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<CdcRecord, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<CdcRecord, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new CdcUnawareBucketWriteOperator(table, writeProvider, commitUser);\n+        return new CdcUnawareBucketWriteOperator.Factory(table, writeProvider, commitUser);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketWriteOperator.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketWriteOperator.java\nindex c57a40f3f71d..26f65fdd09ce 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketWriteOperator.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/CdcUnawareBucketWriteOperator.java\n@@ -18,21 +18,26 @@\n \n package org.apache.paimon.flink.sink.cdc;\n \n+import org.apache.paimon.flink.sink.Committable;\n import org.apache.paimon.flink.sink.PrepareCommitOperator;\n import org.apache.paimon.flink.sink.StoreSinkWrite;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.types.RowKind;\n \n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n /** A {@link PrepareCommitOperator} to write {@link CdcRecord} to unaware-bucket mode table. */\n public class CdcUnawareBucketWriteOperator extends CdcRecordStoreWriteOperator {\n \n-    public CdcUnawareBucketWriteOperator(\n+    private CdcUnawareBucketWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(table, storeSinkWriteProvider, initialCommitUser);\n+        super(parameters, table, storeSinkWriteProvider, initialCommitUser);\n     }\n \n     @Override\n@@ -42,4 +47,30 @@ public void processElement(StreamRecord<CdcRecord> element) throws Exception {\n             super.processElement(element);\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link CdcUnawareBucketWriteOperator}. */\n+    public static class Factory extends CdcRecordStoreWriteOperator.Factory {\n+\n+        public Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new CdcUnawareBucketWriteOperator(\n+                            parameters, table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return CdcUnawareBucketWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\nindex 55e987c6055f..f9b7bbc6b910 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSink.java\n@@ -21,7 +21,7 @@\n import org.apache.paimon.catalog.Catalog;\n import org.apache.paimon.flink.sink.CommittableStateManager;\n import org.apache.paimon.flink.sink.Committer;\n-import org.apache.paimon.flink.sink.CommitterOperator;\n+import org.apache.paimon.flink.sink.CommitterOperatorFactory;\n import org.apache.paimon.flink.sink.FlinkSink;\n import org.apache.paimon.flink.sink.FlinkStreamPartitioner;\n import org.apache.paimon.flink.sink.MultiTableCommittable;\n@@ -41,7 +41,7 @@\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n \n@@ -63,19 +63,16 @@ public class FlinkCdcMultiTableSink implements Serializable {\n     private final Catalog.Loader catalogLoader;\n     private final double commitCpuCores;\n     @Nullable private final MemorySize commitHeapMemory;\n-    private final boolean commitChaining;\n     private final String commitUser;\n \n     public FlinkCdcMultiTableSink(\n             Catalog.Loader catalogLoader,\n             double commitCpuCores,\n             @Nullable MemorySize commitHeapMemory,\n-            boolean commitChaining,\n             String commitUser) {\n         this.catalogLoader = catalogLoader;\n         this.commitCpuCores = commitCpuCores;\n         this.commitHeapMemory = commitHeapMemory;\n-        this.commitChaining = commitChaining;\n         this.commitUser = commitUser;\n     }\n \n@@ -129,10 +126,9 @@ public DataStreamSink<?> sinkFrom(\n                         .transform(\n                                 GLOBAL_COMMITTER_NAME,\n                                 typeInfo,\n-                                new CommitterOperator<>(\n+                                new CommitterOperatorFactory<>(\n                                         true,\n                                         false,\n-                                        commitChaining,\n                                         commitUser,\n                                         createCommitterFactory(),\n                                         createCommittableStateManager()))\n@@ -141,9 +137,10 @@ public DataStreamSink<?> sinkFrom(\n         return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n-    protected OneInputStreamOperator<CdcMultiplexRecord, MultiTableCommittable> createWriteOperator(\n-            StoreSinkWrite.WithWriteBufferProvider writeProvider, String commitUser) {\n-        return new CdcRecordStoreMultiWriteOperator(\n+    protected OneInputStreamOperatorFactory<CdcMultiplexRecord, MultiTableCommittable>\n+            createWriteOperator(\n+                    StoreSinkWrite.WithWriteBufferProvider writeProvider, String commitUser) {\n+        return new CdcRecordStoreMultiWriteOperator.Factory(\n                 catalogLoader, writeProvider, commitUser, new Options());\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\nindex ed8fdd113389..a9ad66847b4b 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/sink/cdc/FlinkCdcSyncDatabaseSinkBuilder.java\n@@ -66,7 +66,6 @@ public class FlinkCdcSyncDatabaseSinkBuilder<T> {\n     @Nullable private Integer parallelism;\n     private double committerCpu;\n     @Nullable private MemorySize committerMemory;\n-    private boolean commitChaining;\n \n     // Paimon catalog used to check and create tables. There will be two\n     //     places where this catalog is used. 1) in processing function,\n@@ -103,7 +102,6 @@ public FlinkCdcSyncDatabaseSinkBuilder<T> withTableOptions(Options options) {\n         this.parallelism = options.get(FlinkConnectorOptions.SINK_PARALLELISM);\n         this.committerCpu = options.get(FlinkConnectorOptions.SINK_COMMITTER_CPU);\n         this.committerMemory = options.get(FlinkConnectorOptions.SINK_COMMITTER_MEMORY);\n-        this.commitChaining = options.get(FlinkConnectorOptions.SINK_COMMITTER_OPERATOR_CHAINING);\n         this.commitUser = createCommitUser(options);\n         return this;\n     }\n@@ -169,7 +167,7 @@ private void buildCombinedCdcSink() {\n \n         FlinkCdcMultiTableSink sink =\n                 new FlinkCdcMultiTableSink(\n-                        catalogLoader, committerCpu, committerMemory, commitChaining, commitUser);\n+                        catalogLoader, committerCpu, committerMemory, commitUser);\n         sink.sinkFrom(partitioned);\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendBypassCompactWorkerOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendBypassCompactWorkerOperator.java\nindex 92cd31ea8aa2..977511920a06 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendBypassCompactWorkerOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendBypassCompactWorkerOperator.java\n@@ -21,7 +21,9 @@\n import org.apache.paimon.append.UnawareAppendCompactionTask;\n import org.apache.paimon.table.FileStoreTable;\n \n-import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.types.Either;\n \n@@ -29,9 +31,11 @@\n public class AppendBypassCompactWorkerOperator\n         extends AppendCompactWorkerOperator<Either<Committable, UnawareAppendCompactionTask>> {\n \n-    public AppendBypassCompactWorkerOperator(FileStoreTable table, String commitUser) {\n-        super(table, commitUser);\n-        this.chainingStrategy = ChainingStrategy.HEAD;\n+    private AppendBypassCompactWorkerOperator(\n+            StreamOperatorParameters<Committable> parameters,\n+            FileStoreTable table,\n+            String commitUser) {\n+        super(parameters, table, commitUser);\n     }\n \n     @Override\n@@ -49,4 +53,27 @@ public void processElement(\n             unawareBucketCompactor.processElement(element.getValue().right());\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link AppendBypassCompactWorkerOperator}. */\n+    public static class Factory\n+            extends AppendCompactWorkerOperator.Factory<\n+                    Either<Committable, UnawareAppendCompactionTask>> {\n+\n+        public Factory(FileStoreTable table, String initialCommitUser) {\n+            super(table, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T) new AppendBypassCompactWorkerOperator(parameters, table, commitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return AppendBypassCompactWorkerOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendCompactWorkerOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendCompactWorkerOperator.java\nindex 52ab75de6b2c..7a3c0231eb65 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendCompactWorkerOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendCompactWorkerOperator.java\n@@ -27,6 +27,8 @@\n import org.apache.paimon.table.sink.CommitMessage;\n import org.apache.paimon.utils.ExecutorThreadFactory;\n \n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -53,8 +55,11 @@ public abstract class AppendCompactWorkerOperator<IN>\n \n     private transient ExecutorService lazyCompactExecutor;\n \n-    public AppendCompactWorkerOperator(FileStoreTable table, String commitUser) {\n-        super(Options.fromMap(table.options()));\n+    public AppendCompactWorkerOperator(\n+            StreamOperatorParameters<Committable> parameters,\n+            FileStoreTable table,\n+            String commitUser) {\n+        super(parameters, Options.fromMap(table.options()));\n         this.table = table;\n         this.commitUser = commitUser;\n     }\n@@ -101,4 +106,17 @@ public void close() throws Exception {\n             this.unawareBucketCompactor.close();\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link AppendCompactWorkerOperator}. */\n+    protected abstract static class Factory<IN>\n+            extends PrepareCommitOperator.Factory<IN, Committable> {\n+        protected final FileStoreTable table;\n+        protected final String commitUser;\n+\n+        protected Factory(FileStoreTable table, String commitUser) {\n+            super(Options.fromMap(table.options()));\n+            this.table = table;\n+            this.commitUser = commitUser;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperator.java\nindex 15e7b9746fe6..83d51f302e51 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperator.java\n@@ -28,6 +28,9 @@\n import org.apache.paimon.utils.ExceptionUtils;\n import org.apache.paimon.utils.ExecutorThreadFactory;\n \n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -62,9 +65,12 @@ public class AppendOnlyMultiTableCompactionWorkerOperator\n \n     private transient Catalog catalog;\n \n-    public AppendOnlyMultiTableCompactionWorkerOperator(\n-            Catalog.Loader catalogLoader, String commitUser, Options options) {\n-        super(options);\n+    private AppendOnlyMultiTableCompactionWorkerOperator(\n+            StreamOperatorParameters<MultiTableCommittable> parameters,\n+            Catalog.Loader catalogLoader,\n+            String commitUser,\n+            Options options) {\n+        super(parameters, options);\n         this.commitUser = commitUser;\n         this.catalogLoader = catalogLoader;\n     }\n@@ -175,4 +181,34 @@ public void close() throws Exception {\n \n         ExceptionUtils.throwMultiException(exceptions);\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link AppendOnlyMultiTableCompactionWorkerOperator}. */\n+    public static class Factory\n+            extends PrepareCommitOperator.Factory<\n+                    MultiTableUnawareAppendCompactionTask, MultiTableCommittable> {\n+\n+        private final String commitUser;\n+        private final Catalog.Loader catalogLoader;\n+\n+        public Factory(Catalog.Loader catalogLoader, String commitUser, Options options) {\n+            super(options);\n+            this.commitUser = commitUser;\n+            this.catalogLoader = catalogLoader;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<MultiTableCommittable>> T createStreamOperator(\n+                StreamOperatorParameters<MultiTableCommittable> parameters) {\n+            return (T)\n+                    new AppendOnlyMultiTableCompactionWorkerOperator(\n+                            parameters, catalogLoader, commitUser, options);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return AppendOnlyMultiTableCompactionWorkerOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperator.java\nindex 4d0201d32461..917a7f64f1a0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperator.java\n@@ -22,6 +22,9 @@\n import org.apache.paimon.flink.source.BucketUnawareCompactSource;\n import org.apache.paimon.table.FileStoreTable;\n \n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n /**\n@@ -31,12 +34,39 @@\n public class AppendOnlySingleTableCompactionWorkerOperator\n         extends AppendCompactWorkerOperator<UnawareAppendCompactionTask> {\n \n-    public AppendOnlySingleTableCompactionWorkerOperator(FileStoreTable table, String commitUser) {\n-        super(table, commitUser);\n+    private AppendOnlySingleTableCompactionWorkerOperator(\n+            StreamOperatorParameters<Committable> parameters,\n+            FileStoreTable table,\n+            String commitUser) {\n+        super(parameters, table, commitUser);\n     }\n \n     @Override\n     public void processElement(StreamRecord<UnawareAppendCompactionTask> element) throws Exception {\n         this.unawareBucketCompactor.processElement(element.getValue());\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link AppendOnlySingleTableCompactionWorkerOperator}. */\n+    public static class Factory\n+            extends AppendCompactWorkerOperator.Factory<UnawareAppendCompactionTask> {\n+\n+        public Factory(FileStoreTable table, String initialCommitUser) {\n+            super(table, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new AppendOnlySingleTableCompactionWorkerOperator(\n+                            parameters, table, commitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return AppendOnlySingleTableCompactionWorkerOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperator.java\nindex 6d27c6019483..0822f0461241 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperator.java\n@@ -32,18 +32,13 @@\n import org.apache.flink.runtime.checkpoint.CheckpointOptions;\n import org.apache.flink.runtime.jobgraph.OperatorID;\n import org.apache.flink.runtime.state.CheckpointStreamFactory;\n-import org.apache.flink.streaming.api.graph.StreamConfig;\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n-import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n import org.apache.flink.streaming.api.operators.OperatorSnapshotFutures;\n-import org.apache.flink.streaming.api.operators.Output;\n-import org.apache.flink.streaming.api.operators.SetupableStreamOperator;\n import org.apache.flink.streaming.api.operators.StreamTaskStateInitializer;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.LatencyMarker;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n-import org.apache.flink.streaming.runtime.tasks.StreamTask;\n import org.apache.flink.streaming.runtime.watermarkstatus.WatermarkStatus;\n \n import java.time.Duration;\n@@ -58,9 +53,7 @@\n  * time, tags are automatically created for each flink savepoint.\n  */\n public class AutoTagForSavepointCommitterOperator<CommitT, GlobalCommitT>\n-        implements OneInputStreamOperator<CommitT, CommitT>,\n-                SetupableStreamOperator,\n-                BoundedOneInput {\n+        implements OneInputStreamOperator<CommitT, CommitT>, BoundedOneInput {\n     public static final String SAVEPOINT_TAG_PREFIX = \""savepoint-\"";\n \n     private static final long serialVersionUID = 1L;\n@@ -256,19 +249,4 @@ public void setKeyContextElement(StreamRecord<CommitT> record) throws Exception\n     public void endInput() throws Exception {\n         commitOperator.endInput();\n     }\n-\n-    @Override\n-    public void setup(StreamTask containingTask, StreamConfig config, Output output) {\n-        commitOperator.setup(containingTask, config, output);\n-    }\n-\n-    @Override\n-    public ChainingStrategy getChainingStrategy() {\n-        return commitOperator.getChainingStrategy();\n-    }\n-\n-    @Override\n-    public void setChainingStrategy(ChainingStrategy strategy) {\n-        commitOperator.setChainingStrategy(strategy);\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorFactory.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorFactory.java\nnew file mode 100644\nindex 000000000000..1787f8e7adce\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorFactory.java\n@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import org.apache.paimon.operation.TagDeletion;\n+import org.apache.paimon.table.sink.TagCallback;\n+import org.apache.paimon.utils.SerializableSupplier;\n+import org.apache.paimon.utils.SnapshotManager;\n+import org.apache.paimon.utils.TagManager;\n+\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.NavigableSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link org.apache.flink.streaming.api.operators.StreamOperatorFactory} for {@link\n+ * AutoTagForSavepointCommitterOperator}.\n+ */\n+public class AutoTagForSavepointCommitterOperatorFactory<CommitT, GlobalCommitT>\n+        extends AbstractStreamOperatorFactory<CommitT>\n+        implements OneInputStreamOperatorFactory<CommitT, CommitT> {\n+\n+    private final CommitterOperatorFactory<CommitT, GlobalCommitT> commitOperatorFactory;\n+\n+    private final SerializableSupplier<SnapshotManager> snapshotManagerFactory;\n+\n+    private final SerializableSupplier<TagManager> tagManagerFactory;\n+\n+    private final SerializableSupplier<TagDeletion> tagDeletionFactory;\n+\n+    private final SerializableSupplier<List<TagCallback>> callbacksSupplier;\n+\n+    private final NavigableSet<Long> identifiersForTags;\n+\n+    private final Duration tagTimeRetained;\n+\n+    public AutoTagForSavepointCommitterOperatorFactory(\n+            CommitterOperatorFactory<CommitT, GlobalCommitT> commitOperatorFactory,\n+            SerializableSupplier<SnapshotManager> snapshotManagerFactory,\n+            SerializableSupplier<TagManager> tagManagerFactory,\n+            SerializableSupplier<TagDeletion> tagDeletionFactory,\n+            SerializableSupplier<List<TagCallback>> callbacksSupplier,\n+            Duration tagTimeRetained) {\n+        this.commitOperatorFactory = commitOperatorFactory;\n+        this.tagManagerFactory = tagManagerFactory;\n+        this.snapshotManagerFactory = snapshotManagerFactory;\n+        this.tagDeletionFactory = tagDeletionFactory;\n+        this.callbacksSupplier = callbacksSupplier;\n+        this.identifiersForTags = new TreeSet<>();\n+        this.tagTimeRetained = tagTimeRetained;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""unchecked\"")\n+    public <T extends StreamOperator<CommitT>> T createStreamOperator(\n+            StreamOperatorParameters<CommitT> parameters) {\n+        return (T)\n+                new AutoTagForSavepointCommitterOperator<>(\n+                        commitOperatorFactory.createStreamOperator(parameters),\n+                        snapshotManagerFactory,\n+                        tagManagerFactory,\n+                        tagDeletionFactory,\n+                        callbacksSupplier,\n+                        tagTimeRetained);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""rawtypes\"")\n+    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+        return AutoTagForSavepointCommitterOperator.class;\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperator.java\nindex 23202b45077f..1cbcc4b2262f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperator.java\n@@ -28,18 +28,13 @@\n import org.apache.flink.runtime.checkpoint.CheckpointOptions;\n import org.apache.flink.runtime.jobgraph.OperatorID;\n import org.apache.flink.runtime.state.CheckpointStreamFactory;\n-import org.apache.flink.streaming.api.graph.StreamConfig;\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n-import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n import org.apache.flink.streaming.api.operators.OperatorSnapshotFutures;\n-import org.apache.flink.streaming.api.operators.Output;\n-import org.apache.flink.streaming.api.operators.SetupableStreamOperator;\n import org.apache.flink.streaming.api.operators.StreamTaskStateInitializer;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.LatencyMarker;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n-import org.apache.flink.streaming.runtime.tasks.StreamTask;\n import org.apache.flink.streaming.runtime.watermarkstatus.WatermarkStatus;\n \n import java.time.Instant;\n@@ -53,9 +48,7 @@\n  * completed, the corresponding tag is generated.\n  */\n public class BatchWriteGeneratorTagOperator<CommitT, GlobalCommitT>\n-        implements OneInputStreamOperator<CommitT, CommitT>,\n-                SetupableStreamOperator,\n-                BoundedOneInput {\n+        implements OneInputStreamOperator<CommitT, CommitT>, BoundedOneInput {\n \n     private static final String BATCH_WRITE_TAG_PREFIX = \""batch-write-\"";\n \n@@ -250,19 +243,4 @@ public void setKeyContextElement(StreamRecord<CommitT> record) throws Exception\n     public void endInput() throws Exception {\n         commitOperator.endInput();\n     }\n-\n-    @Override\n-    public void setup(StreamTask containingTask, StreamConfig config, Output output) {\n-        commitOperator.setup(containingTask, config, output);\n-    }\n-\n-    @Override\n-    public ChainingStrategy getChainingStrategy() {\n-        return commitOperator.getChainingStrategy();\n-    }\n-\n-    @Override\n-    public void setChainingStrategy(ChainingStrategy strategy) {\n-        commitOperator.setChainingStrategy(strategy);\n-    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorFactory.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorFactory.java\nnew file mode 100644\nindex 000000000000..e3c0e5c49168\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorFactory.java\n@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import org.apache.paimon.table.FileStoreTable;\n+\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+\n+/**\n+ * {@link org.apache.flink.streaming.api.operators.StreamOperatorFactory} for {@link\n+ * BatchWriteGeneratorTagOperator}.\n+ */\n+public class BatchWriteGeneratorTagOperatorFactory<CommitT, GlobalCommitT>\n+        extends AbstractStreamOperatorFactory<CommitT>\n+        implements OneInputStreamOperatorFactory<CommitT, CommitT> {\n+    private final CommitterOperatorFactory<CommitT, GlobalCommitT> commitOperatorFactory;\n+\n+    protected final FileStoreTable table;\n+\n+    public BatchWriteGeneratorTagOperatorFactory(\n+            CommitterOperatorFactory<CommitT, GlobalCommitT> commitOperatorFactory,\n+            FileStoreTable table) {\n+        this.table = table;\n+        this.commitOperatorFactory = commitOperatorFactory;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""unchecked\"")\n+    public <T extends StreamOperator<CommitT>> T createStreamOperator(\n+            StreamOperatorParameters<CommitT> parameters) {\n+        return (T)\n+                new BatchWriteGeneratorTagOperator<>(\n+                        commitOperatorFactory.createStreamOperator(parameters), table);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""rawtypes\"")\n+    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+        return BatchWriteGeneratorTagOperator.class;\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\nindex ce4e37305909..c2b4cc0f87e6 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CombinedTableCompactorSink.java\n@@ -33,7 +33,7 @@\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.table.data.RowData;\n \n import java.io.Serializable;\n@@ -119,7 +119,7 @@ public DataStream<MultiTableCommittable> doWrite(\n                         .transform(\n                                 String.format(\""%s-%s\"", \""Unaware-Bucket-Table\"", WRITER_NAME),\n                                 new MultiTableCommittableTypeInfo(),\n-                                new AppendOnlyMultiTableCompactionWorkerOperator(\n+                                new AppendOnlyMultiTableCompactionWorkerOperator.Factory(\n                                         catalogLoader, commitUser, options))\n                         .setParallelism(unawareBucketTableSource.getParallelism());\n \n@@ -160,26 +160,28 @@ protected DataStreamSink<?> doCommit(\n                         .transform(\n                                 GLOBAL_COMMITTER_NAME,\n                                 new MultiTableCommittableTypeInfo(),\n-                                new CommitterOperator<>(\n+                                new CommitterOperatorFactory<>(\n                                         streamingCheckpointEnabled,\n                                         false,\n-                                        options.get(SINK_COMMITTER_OPERATOR_CHAINING),\n                                         commitUser,\n                                         createCommitterFactory(isStreaming),\n                                         createCommittableStateManager(),\n                                         options.get(END_INPUT_WATERMARK)))\n                         .setParallelism(written.getParallelism());\n+        if (!options.get(SINK_COMMITTER_OPERATOR_CHAINING)) {\n+            committed = committed.startNewChain();\n+        }\n         return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n     }\n \n     // TODO:refactor FlinkSink to adopt this sink\n-    protected OneInputStreamOperator<RowData, MultiTableCommittable>\n+    protected OneInputStreamOperatorFactory<RowData, MultiTableCommittable>\n             combinedMultiComacptionWriteOperator(\n                     CheckpointConfig checkpointConfig,\n                     boolean isStreaming,\n                     boolean fullCompaction,\n                     String commitUser) {\n-        return new MultiTablesStoreCompactOperator(\n+        return new MultiTablesStoreCompactOperator.Factory(\n                 catalogLoader,\n                 commitUser,\n                 checkpointConfig,\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperator.java\nindex 021a5db413d5..383cbcd6ebf7 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperator.java\n@@ -25,8 +25,8 @@\n import org.apache.flink.runtime.state.StateSnapshotContext;\n import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n-import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n@@ -91,26 +91,9 @@ public class CommitterOperator<CommitT, GlobalCommitT> extends AbstractStreamOpe\n     private final Long endInputWatermark;\n \n     public CommitterOperator(\n+            StreamOperatorParameters<CommitT> parameters,\n             boolean streamingCheckpointEnabled,\n             boolean forceSingleParallelism,\n-            boolean chaining,\n-            String initialCommitUser,\n-            Committer.Factory<CommitT, GlobalCommitT> committerFactory,\n-            CommittableStateManager<GlobalCommitT> committableStateManager) {\n-        this(\n-                streamingCheckpointEnabled,\n-                forceSingleParallelism,\n-                chaining,\n-                initialCommitUser,\n-                committerFactory,\n-                committableStateManager,\n-                null);\n-    }\n-\n-    public CommitterOperator(\n-            boolean streamingCheckpointEnabled,\n-            boolean forceSingleParallelism,\n-            boolean chaining,\n             String initialCommitUser,\n             Committer.Factory<CommitT, GlobalCommitT> committerFactory,\n             CommittableStateManager<GlobalCommitT> committableStateManager,\n@@ -122,7 +105,10 @@ public CommitterOperator(\n         this.committerFactory = checkNotNull(committerFactory);\n         this.committableStateManager = committableStateManager;\n         this.endInputWatermark = endInputWatermark;\n-        setChainingStrategy(chaining ? ChainingStrategy.ALWAYS : ChainingStrategy.HEAD);\n+        this.setup(\n+                parameters.getContainingTask(),\n+                parameters.getStreamConfig(),\n+                parameters.getOutput());\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperatorFactory.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperatorFactory.java\nnew file mode 100644\nindex 000000000000..cce3d4e176bf\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommitterOperatorFactory.java\n@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+\n+import java.util.NavigableMap;\n+import java.util.TreeMap;\n+\n+import static org.apache.paimon.utils.Preconditions.checkNotNull;\n+\n+/**\n+ * {@link org.apache.flink.streaming.api.operators.StreamOperatorFactory} for {@link\n+ * CommitterOperator}.\n+ */\n+public class CommitterOperatorFactory<CommitT, GlobalCommitT>\n+        extends AbstractStreamOperatorFactory<CommitT>\n+        implements OneInputStreamOperatorFactory<CommitT, CommitT> {\n+    protected final boolean streamingCheckpointEnabled;\n+\n+    /** Whether to check the parallelism while runtime. */\n+    protected final boolean forceSingleParallelism;\n+    /**\n+     * This commitUser is valid only for new jobs. After the job starts, this commitUser will be\n+     * recorded into the states of write and commit operators. When the job restarts, commitUser\n+     * will be recovered from states and this value is ignored.\n+     */\n+    protected final String initialCommitUser;\n+\n+    /** Group the committable by the checkpoint id. */\n+    protected final NavigableMap<Long, GlobalCommitT> committablesPerCheckpoint;\n+\n+    protected final Committer.Factory<CommitT, GlobalCommitT> committerFactory;\n+\n+    protected final CommittableStateManager<GlobalCommitT> committableStateManager;\n+\n+    /**\n+     * Aggregate committables to global committables and commit the global committables to the\n+     * external system.\n+     */\n+    protected Committer<CommitT, GlobalCommitT> committer;\n+\n+    protected final Long endInputWatermark;\n+\n+    public CommitterOperatorFactory(\n+            boolean streamingCheckpointEnabled,\n+            boolean forceSingleParallelism,\n+            String initialCommitUser,\n+            Committer.Factory<CommitT, GlobalCommitT> committerFactory,\n+            CommittableStateManager<GlobalCommitT> committableStateManager) {\n+        this(\n+                streamingCheckpointEnabled,\n+                forceSingleParallelism,\n+                initialCommitUser,\n+                committerFactory,\n+                committableStateManager,\n+                null);\n+    }\n+\n+    public CommitterOperatorFactory(\n+            boolean streamingCheckpointEnabled,\n+            boolean forceSingleParallelism,\n+            String initialCommitUser,\n+            Committer.Factory<CommitT, GlobalCommitT> committerFactory,\n+            CommittableStateManager<GlobalCommitT> committableStateManager,\n+            Long endInputWatermark) {\n+        this.streamingCheckpointEnabled = streamingCheckpointEnabled;\n+        this.forceSingleParallelism = forceSingleParallelism;\n+        this.initialCommitUser = initialCommitUser;\n+        this.committablesPerCheckpoint = new TreeMap<>();\n+        this.committerFactory = checkNotNull(committerFactory);\n+        this.committableStateManager = committableStateManager;\n+        this.endInputWatermark = endInputWatermark;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""unchecked\"")\n+    public <T extends StreamOperator<CommitT>> T createStreamOperator(\n+            StreamOperatorParameters<CommitT> parameters) {\n+        return (T)\n+                new CommitterOperator<>(\n+                        parameters,\n+                        streamingCheckpointEnabled,\n+                        forceSingleParallelism,\n+                        initialCommitUser,\n+                        committerFactory,\n+                        committableStateManager,\n+                        endInputWatermark);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\""rawtypes\"")\n+    public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+        return CommitterOperator.class;\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactorSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactorSink.java\nindex a0c830d73f58..a9c6031dfa34 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactorSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactorSink.java\n@@ -21,7 +21,7 @@\n import org.apache.paimon.manifest.ManifestCommittable;\n import org.apache.paimon.table.FileStoreTable;\n \n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.table.data.RowData;\n \n /** {@link FlinkSink} for dedicated compact jobs. */\n@@ -37,9 +37,9 @@ public CompactorSink(FileStoreTable table, boolean fullCompaction) {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<RowData, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<RowData, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new StoreCompactOperator(table, writeProvider, commitUser, fullCompaction);\n+        return new StoreCompactOperator.Factory(table, writeProvider, commitUser, fullCompaction);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/DynamicBucketRowWriteOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/DynamicBucketRowWriteOperator.java\nindex 53b9be457c3d..b31a1af05224 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/DynamicBucketRowWriteOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/DynamicBucketRowWriteOperator.java\n@@ -22,6 +22,9 @@\n import org.apache.paimon.table.FileStoreTable;\n \n import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n /**\n@@ -32,11 +35,12 @@ public class DynamicBucketRowWriteOperator\n \n     private static final long serialVersionUID = 1L;\n \n-    public DynamicBucketRowWriteOperator(\n+    private DynamicBucketRowWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(table, storeSinkWriteProvider, initialCommitUser);\n+        super(parameters, table, storeSinkWriteProvider, initialCommitUser);\n     }\n \n     @Override\n@@ -49,4 +53,30 @@ public void processElement(StreamRecord<Tuple2<InternalRow, Integer>> element)\n             throws Exception {\n         write.write(element.getValue().f0, element.getValue().f1);\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link DynamicBucketRowWriteOperator}. */\n+    public static class Factory extends TableWriteOperator.Factory<Tuple2<InternalRow, Integer>> {\n+\n+        public Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new DynamicBucketRowWriteOperator(\n+                            parameters, table, storeSinkWriteProvider, initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return DynamicBucketRowWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FixedBucketSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FixedBucketSink.java\nindex 613bf369b052..402abb4d5aac 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FixedBucketSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FixedBucketSink.java\n@@ -21,7 +21,7 @@\n import org.apache.paimon.data.InternalRow;\n import org.apache.paimon.table.FileStoreTable;\n \n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n \n@@ -43,8 +43,9 @@ public FixedBucketSink(\n     }\n \n     @Override\n-    protected OneInputStreamOperator<InternalRow, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<InternalRow, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new RowDataStoreWriteOperator(table, logSinkFunction, writeProvider, commitUser);\n+        return new RowDataStoreWriteOperator.Factory(\n+                table, logSinkFunction, writeProvider, commitUser);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\nindex dd364c196d8b..8d6c3554c76f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n@@ -44,7 +44,7 @@\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.table.api.config.ExecutionConfigOptions;\n \n import javax.annotation.Nullable;\n@@ -220,7 +220,7 @@ public DataStream<Committable> doWrite(\n                                         + \"" : \""\n                                         + table.name(),\n                                 new CommittableTypeInfo(),\n-                                createWriteOperator(\n+                                createWriteOperatorFactory(\n                                         createWriteProvider(\n                                                 env.getCheckpointConfig(),\n                                                 isStreaming,\n@@ -268,11 +268,10 @@ protected DataStreamSink<?> doCommit(DataStream<Committable> written, String com\n         }\n \n         Options options = Options.fromMap(table.options());\n-        OneInputStreamOperator<Committable, Committable> committerOperator =\n-                new CommitterOperator<>(\n+        OneInputStreamOperatorFactory<Committable, Committable> committerOperator =\n+                new CommitterOperatorFactory<>(\n                         streamingCheckpointEnabled,\n                         true,\n-                        options.get(SINK_COMMITTER_OPERATOR_CHAINING),\n                         commitUser,\n                         createCommitterFactory(),\n                         createCommittableStateManager(),\n@@ -280,8 +279,9 @@ protected DataStreamSink<?> doCommit(DataStream<Committable> written, String com\n \n         if (options.get(SINK_AUTO_TAG_FOR_SAVEPOINT)) {\n             committerOperator =\n-                    new AutoTagForSavepointCommitterOperator<>(\n-                            (CommitterOperator<Committable, ManifestCommittable>) committerOperator,\n+                    new AutoTagForSavepointCommitterOperatorFactory<>(\n+                            (CommitterOperatorFactory<Committable, ManifestCommittable>)\n+                                    committerOperator,\n                             table::snapshotManager,\n                             table::tagManager,\n                             () -> table.store().newTagDeletion(),\n@@ -291,8 +291,9 @@ protected DataStreamSink<?> doCommit(DataStream<Committable> written, String com\n         if (conf.get(ExecutionOptions.RUNTIME_MODE) == RuntimeExecutionMode.BATCH\n                 && table.coreOptions().tagCreationMode() == TagCreationMode.BATCH) {\n             committerOperator =\n-                    new BatchWriteGeneratorTagOperator<>(\n-                            (CommitterOperator<Committable, ManifestCommittable>) committerOperator,\n+                    new BatchWriteGeneratorTagOperatorFactory<>(\n+                            (CommitterOperatorFactory<Committable, ManifestCommittable>)\n+                                    committerOperator,\n                             table);\n         }\n         SingleOutputStreamOperator<?> committed =\n@@ -310,6 +311,9 @@ protected DataStreamSink<?> doCommit(DataStream<Committable> written, String com\n                                     table.name(),\n                                     options.get(SINK_OPERATOR_UID_SUFFIX)));\n         }\n+        if (!options.get(SINK_COMMITTER_OPERATOR_CHAINING)) {\n+            committed = committed.startNewChain();\n+        }\n         configureGlobalCommitter(\n                 committed, options.get(SINK_COMMITTER_CPU), options.get(SINK_COMMITTER_MEMORY));\n         return committed.addSink(new DiscardingSink<>()).name(\""end\"").setParallelism(1);\n@@ -362,7 +366,7 @@ public static void assertBatchAdaptiveParallelism(\n         }\n     }\n \n-    protected abstract OneInputStreamOperator<T, Committable> createWriteOperator(\n+    protected abstract OneInputStreamOperatorFactory<T, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser);\n \n     protected abstract Committer.Factory<Committable, ManifestCommittable> createCommitterFactory();\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSinkBuilder.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSinkBuilder.java\nindex dcccd0a1a988..5703c408243b 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSinkBuilder.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSinkBuilder.java\n@@ -222,7 +222,7 @@ public DataStreamSink<?> build() {\n                             .transform(\n                                     \""local merge\"",\n                                     input.getType(),\n-                                    new LocalMergeOperator(table.schema()))\n+                                    new LocalMergeOperator.Factory(table.schema()))\n                             .setParallelism(input.getParallelism());\n         }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/LocalMergeOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/LocalMergeOperator.java\nindex 6931fe907218..070262147643 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/LocalMergeOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/LocalMergeOperator.java\n@@ -44,10 +44,15 @@\n import org.apache.paimon.utils.UserDefinedSeqComparator;\n \n import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n@@ -76,13 +81,14 @@ public class LocalMergeOperator extends AbstractStreamOperator<InternalRow>\n \n     private transient boolean endOfInput;\n \n-    public LocalMergeOperator(TableSchema schema) {\n+    private LocalMergeOperator(\n+            StreamOperatorParameters<InternalRow> parameters, TableSchema schema) {\n         Preconditions.checkArgument(\n                 schema.primaryKeys().size() > 0,\n                 \""LocalMergeOperator currently only support tables with primary keys\"");\n         this.schema = schema;\n         this.ignoreDelete = CoreOptions.fromMap(schema.options()).ignoreDelete();\n-        setChainingStrategy(ChainingStrategy.ALWAYS);\n+        setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());\n     }\n \n     @Override\n@@ -235,4 +241,28 @@ LocalMerger merger() {\n     void setOutput(Output<StreamRecord<InternalRow>> output) {\n         this.output = output;\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link LocalMergeOperator}. */\n+    public static class Factory extends AbstractStreamOperatorFactory<InternalRow>\n+            implements OneInputStreamOperatorFactory<InternalRow, InternalRow> {\n+        private final TableSchema schema;\n+\n+        public Factory(TableSchema schema) {\n+            this.chainingStrategy = ChainingStrategy.ALWAYS;\n+            this.schema = schema;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<InternalRow>> T createStreamOperator(\n+                StreamOperatorParameters<InternalRow> parameters) {\n+            return (T) new LocalMergeOperator(parameters, schema);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return LocalMergeOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTablesStoreCompactOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTablesStoreCompactOperator.java\nindex 57d2e8413cb5..58f6a3834096 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTablesStoreCompactOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTablesStoreCompactOperator.java\n@@ -33,6 +33,9 @@\n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.runtime.state.StateSnapshotContext;\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.table.data.RowData;\n \n@@ -76,7 +79,8 @@ public class MultiTablesStoreCompactOperator\n     protected Map<Identifier, StoreSinkWrite> writes;\n     protected String commitUser;\n \n-    public MultiTablesStoreCompactOperator(\n+    private MultiTablesStoreCompactOperator(\n+            StreamOperatorParameters<MultiTableCommittable> parameters,\n             Catalog.Loader catalogLoader,\n             String initialCommitUser,\n             CheckpointConfig checkpointConfig,\n@@ -84,7 +88,7 @@ public MultiTablesStoreCompactOperator(\n             boolean ignorePreviousFiles,\n             boolean fullCompaction,\n             Options options) {\n-        super(options);\n+        super(parameters, options);\n         this.catalogLoader = catalogLoader;\n         this.initialCommitUser = initialCommitUser;\n         this.checkpointConfig = checkpointConfig;\n@@ -316,4 +320,54 @@ private StoreSinkWrite.Provider createWriteProvider(\n                         memoryPool,\n                         metricGroup);\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link MultiTablesStoreCompactOperator}. */\n+    public static class Factory\n+            extends PrepareCommitOperator.Factory<RowData, MultiTableCommittable> {\n+        private final Catalog.Loader catalogLoader;\n+        private final CheckpointConfig checkpointConfig;\n+        private final boolean isStreaming;\n+        private final boolean ignorePreviousFiles;\n+        private final boolean fullCompaction;\n+        private final String initialCommitUser;\n+\n+        public Factory(\n+                Catalog.Loader catalogLoader,\n+                String initialCommitUser,\n+                CheckpointConfig checkpointConfig,\n+                boolean isStreaming,\n+                boolean ignorePreviousFiles,\n+                boolean fullCompaction,\n+                Options options) {\n+            super(options);\n+            this.catalogLoader = catalogLoader;\n+            this.initialCommitUser = initialCommitUser;\n+            this.checkpointConfig = checkpointConfig;\n+            this.isStreaming = isStreaming;\n+            this.ignorePreviousFiles = ignorePreviousFiles;\n+            this.fullCompaction = fullCompaction;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<MultiTableCommittable>> T createStreamOperator(\n+                StreamOperatorParameters<MultiTableCommittable> parameters) {\n+            return (T)\n+                    new MultiTablesStoreCompactOperator(\n+                            parameters,\n+                            catalogLoader,\n+                            initialCommitUser,\n+                            checkpointConfig,\n+                            isStreaming,\n+                            ignorePreviousFiles,\n+                            fullCompaction,\n+                            options);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return MultiTablesStoreCompactOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/PrepareCommitOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/PrepareCommitOperator.java\nindex 3668386ddc2d..8b114d3e492f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/PrepareCommitOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/PrepareCommitOperator.java\n@@ -26,10 +26,14 @@\n import org.apache.flink.runtime.memory.MemoryManager;\n import org.apache.flink.streaming.api.graph.StreamConfig;\n import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.streaming.runtime.tasks.StreamTask;\n \n@@ -52,9 +56,9 @@ public abstract class PrepareCommitOperator<IN, OUT> extends AbstractStreamOpera\n     private final Options options;\n     private boolean endOfInput = false;\n \n-    public PrepareCommitOperator(Options options) {\n+    public PrepareCommitOperator(StreamOperatorParameters<OUT> parameters, Options options) {\n         this.options = options;\n-        setChainingStrategy(ChainingStrategy.ALWAYS);\n+        setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());\n     }\n \n     @Override\n@@ -103,4 +107,15 @@ private void emitCommittables(boolean waitCompaction, long checkpointId) throws\n \n     protected abstract List<OUT> prepareCommit(boolean waitCompaction, long checkpointId)\n             throws IOException;\n+\n+    /** {@link StreamOperatorFactory} of {@link PrepareCommitOperator}. */\n+    protected abstract static class Factory<IN, OUT> extends AbstractStreamOperatorFactory<OUT>\n+            implements OneInputStreamOperatorFactory<IN, OUT> {\n+        protected final Options options;\n+\n+        protected Factory(Options options) {\n+            this.options = options;\n+            this.chainingStrategy = ChainingStrategy.ALWAYS;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RewriteFileIndexSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RewriteFileIndexSink.java\nindex 39dcca03c6aa..d9f863c6b919 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RewriteFileIndexSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RewriteFileIndexSink.java\n@@ -45,11 +45,10 @@\n import org.apache.paimon.utils.FileStorePathFactory;\n import org.apache.paimon.utils.Pair;\n \n-import org.apache.flink.streaming.api.graph.StreamConfig;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n-import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n-import org.apache.flink.streaming.runtime.tasks.StreamTask;\n \n import javax.annotation.Nullable;\n \n@@ -76,34 +75,49 @@ public RewriteFileIndexSink(FileStoreTable table) {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<ManifestEntry, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<ManifestEntry, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new FileIndexModificationOperator(table.coreOptions().toConfiguration(), table);\n+        return new FileIndexModificationOperatorFactory(\n+                table.coreOptions().toConfiguration(), table);\n     }\n \n-    /** File index modification operator to rewrite file index. */\n-    private static class FileIndexModificationOperator\n-            extends PrepareCommitOperator<ManifestEntry, Committable> {\n-\n-        private static final long serialVersionUID = 1L;\n-\n+    private static class FileIndexModificationOperatorFactory\n+            extends PrepareCommitOperator.Factory<ManifestEntry, Committable> {\n         private final FileStoreTable table;\n \n-        private transient FileIndexProcessor fileIndexProcessor;\n-        private transient List<CommitMessage> messages;\n-\n-        public FileIndexModificationOperator(Options options, FileStoreTable table) {\n+        public FileIndexModificationOperatorFactory(Options options, FileStoreTable table) {\n             super(options);\n             this.table = table;\n         }\n \n         @Override\n-        public void setup(\n-                StreamTask<?, ?> containingTask,\n-                StreamConfig config,\n-                Output<StreamRecord<Committable>> output) {\n-            super.setup(containingTask, config, output);\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T) new FileIndexModificationOperator(parameters, options, table);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return FileIndexModificationOperator.class;\n+        }\n+    }\n+\n+    /** File index modification operator to rewrite file index. */\n+    private static class FileIndexModificationOperator\n+            extends PrepareCommitOperator<ManifestEntry, Committable> {\n+\n+        private static final long serialVersionUID = 1L;\n+\n+        private final transient FileIndexProcessor fileIndexProcessor;\n+        private final transient List<CommitMessage> messages;\n \n+        private FileIndexModificationOperator(\n+                StreamOperatorParameters<Committable> parameters,\n+                Options options,\n+                FileStoreTable table) {\n+            super(parameters, options);\n             this.fileIndexProcessor = new FileIndexProcessor(table);\n             this.messages = new ArrayList<>();\n         }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\nindex 2b25f074667c..8009bec9677f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDataStoreWriteOperator.java\n@@ -32,13 +32,13 @@\n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.runtime.state.StateSnapshotContext;\n import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n-import org.apache.flink.streaming.api.graph.StreamConfig;\n import org.apache.flink.streaming.api.operators.InternalTimerService;\n-import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n-import org.apache.flink.streaming.runtime.tasks.StreamTask;\n import org.apache.flink.streaming.util.functions.StreamingFunctionUtils;\n \n import javax.annotation.Nullable;\n@@ -61,21 +61,14 @@ public class RowDataStoreWriteOperator extends TableWriteOperator<InternalRow> {\n     /** We listen to this ourselves because we don't have an {@link InternalTimerService}. */\n     private long currentWatermark = Long.MIN_VALUE;\n \n-    public RowDataStoreWriteOperator(\n+    protected RowDataStoreWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             @Nullable LogSinkFunction logSinkFunction,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(table, storeSinkWriteProvider, initialCommitUser);\n+        super(parameters, table, storeSinkWriteProvider, initialCommitUser);\n         this.logSinkFunction = logSinkFunction;\n-    }\n-\n-    @Override\n-    public void setup(\n-            StreamTask<?, ?> containingTask,\n-            StreamConfig config,\n-            Output<StreamRecord<Committable>> output) {\n-        super.setup(containingTask, config, output);\n         if (logSinkFunction != null) {\n             FunctionUtils.setFunctionRuntimeContext(logSinkFunction, getRuntimeContext());\n         }\n@@ -249,4 +242,38 @@ public Long timestamp() {\n             return timestamp;\n         }\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link RowDataStoreWriteOperator}. */\n+    public static class Factory extends TableWriteOperator.Factory<InternalRow> {\n+\n+        @Nullable private final LogSinkFunction logSinkFunction;\n+\n+        public Factory(\n+                FileStoreTable table,\n+                @Nullable LogSinkFunction logSinkFunction,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(table, storeSinkWriteProvider, initialCommitUser);\n+            this.logSinkFunction = logSinkFunction;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new RowDataStoreWriteOperator(\n+                            parameters,\n+                            table,\n+                            logSinkFunction,\n+                            storeSinkWriteProvider,\n+                            initialCommitUser);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return RowDataStoreWriteOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDynamicBucketSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDynamicBucketSink.java\nindex bf6c70f0aa29..1f7e62d74916 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDynamicBucketSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowDynamicBucketSink.java\n@@ -27,7 +27,7 @@\n import org.apache.paimon.utils.SerializableFunction;\n \n import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n \n@@ -60,8 +60,8 @@ protected ChannelComputer<Tuple2<InternalRow, Integer>> channelComputer2() {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Tuple2<InternalRow, Integer>, Committable> createWriteOperator(\n-            StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new DynamicBucketRowWriteOperator(table, writeProvider, commitUser);\n+    protected OneInputStreamOperatorFactory<Tuple2<InternalRow, Integer>, Committable>\n+            createWriteOperatorFactory(StoreSinkWrite.Provider writeProvider, String commitUser) {\n+        return new DynamicBucketRowWriteOperator.Factory(table, writeProvider, commitUser);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowUnawareBucketSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowUnawareBucketSink.java\nindex 1cd10390c1a0..fea8a382a954 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowUnawareBucketSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/RowUnawareBucketSink.java\n@@ -22,7 +22,9 @@\n import org.apache.paimon.table.FileStoreTable;\n \n import org.apache.flink.runtime.state.StateInitializationContext;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n \n import java.util.Map;\n \n@@ -38,25 +40,35 @@ public RowUnawareBucketSink(\n     }\n \n     @Override\n-    protected OneInputStreamOperator<InternalRow, Committable> createWriteOperator(\n+    protected OneInputStreamOperatorFactory<InternalRow, Committable> createWriteOperatorFactory(\n             StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new RowDataStoreWriteOperator(table, logSinkFunction, writeProvider, commitUser) {\n-\n+        return new RowDataStoreWriteOperator.Factory(\n+                table, logSinkFunction, writeProvider, commitUser) {\n             @Override\n-            protected StoreSinkWriteState createState(\n-                    StateInitializationContext context,\n-                    StoreSinkWriteState.StateValueFilter stateFilter)\n-                    throws Exception {\n-                // No conflicts will occur in append only unaware bucket writer, so no state is\n-                // needed.\n-                return new NoopStoreSinkWriteState(stateFilter);\n-            }\n+            public StreamOperator createStreamOperator(StreamOperatorParameters parameters) {\n+                return new RowDataStoreWriteOperator(\n+                        parameters, table, logSinkFunction, writeProvider, commitUser) {\n \n-            @Override\n-            protected String getCommitUser(StateInitializationContext context) throws Exception {\n-                // No conflicts will occur in append only unaware bucket writer, so commitUser does\n-                // not matter.\n-                return commitUser;\n+                    @Override\n+                    protected StoreSinkWriteState createState(\n+                            StateInitializationContext context,\n+                            StoreSinkWriteState.StateValueFilter stateFilter)\n+                            throws Exception {\n+                        // No conflicts will occur in append only unaware bucket writer, so no state\n+                        // is\n+                        // needed.\n+                        return new NoopStoreSinkWriteState(stateFilter);\n+                    }\n+\n+                    @Override\n+                    protected String getCommitUser(StateInitializationContext context)\n+                            throws Exception {\n+                        // No conflicts will occur in append only unaware bucket writer, so\n+                        // commitUser does\n+                        // not matter.\n+                        return commitUser;\n+                    }\n+                };\n             }\n         };\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCompactOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCompactOperator.java\nindex ac10345bc425..1870a0493c2f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCompactOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCompactOperator.java\n@@ -31,6 +31,9 @@\n \n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.table.data.RowData;\n \n@@ -60,12 +63,13 @@ public class StoreCompactOperator extends PrepareCommitOperator<RowData, Committ\n     private transient DataFileMetaSerializer dataFileMetaSerializer;\n     private transient Set<Pair<BinaryRow, Integer>> waitToCompact;\n \n-    public StoreCompactOperator(\n+    private StoreCompactOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser,\n             boolean fullCompaction) {\n-        super(Options.fromMap(table.options()));\n+        super(parameters, Options.fromMap(table.options()));\n         Preconditions.checkArgument(\n                 !table.coreOptions().writeOnly(),\n                 CoreOptions.WRITE_ONLY.key() + \"" should not be true for StoreCompactOperator.\"");\n@@ -163,4 +167,46 @@ public void close() throws Exception {\n         super.close();\n         write.close();\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link StoreCompactOperator}. */\n+    public static class Factory extends PrepareCommitOperator.Factory<RowData, Committable> {\n+        private final FileStoreTable table;\n+        private final StoreSinkWrite.Provider storeSinkWriteProvider;\n+        private final String initialCommitUser;\n+        private final boolean fullCompaction;\n+\n+        public Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser,\n+                boolean fullCompaction) {\n+            super(Options.fromMap(table.options()));\n+            Preconditions.checkArgument(\n+                    !table.coreOptions().writeOnly(),\n+                    CoreOptions.WRITE_ONLY.key() + \"" should not be true for StoreCompactOperator.\"");\n+            this.table = table;\n+            this.storeSinkWriteProvider = storeSinkWriteProvider;\n+            this.initialCommitUser = initialCommitUser;\n+            this.fullCompaction = fullCompaction;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                StreamOperatorParameters<Committable> parameters) {\n+            return (T)\n+                    new StoreCompactOperator(\n+                            parameters,\n+                            table,\n+                            storeSinkWriteProvider,\n+                            initialCommitUser,\n+                            fullCompaction);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return StoreCompactOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableWriteOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableWriteOperator.java\nindex 32fcdd03bdfd..fd876698c094 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableWriteOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/TableWriteOperator.java\n@@ -28,6 +28,8 @@\n \n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.RecordAttributes;\n \n import java.io.IOException;\n@@ -45,10 +47,11 @@ public abstract class TableWriteOperator<IN> extends PrepareCommitOperator<IN, C\n     protected transient StoreSinkWrite write;\n \n     public TableWriteOperator(\n+            StreamOperatorParameters<Committable> parameters,\n             FileStoreTable table,\n             StoreSinkWrite.Provider storeSinkWriteProvider,\n             String initialCommitUser) {\n-        super(Options.fromMap(table.options()));\n+        super(parameters, Options.fromMap(table.options()));\n         this.table = table;\n         this.storeSinkWriteProvider = storeSinkWriteProvider;\n         this.initialCommitUser = initialCommitUser;\n@@ -128,4 +131,22 @@ protected List<Committable> prepareCommit(boolean waitCompaction, long checkpoin\n     public StoreSinkWrite getWrite() {\n         return write;\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link TableWriteOperator}. */\n+    protected abstract static class Factory<IN>\n+            extends PrepareCommitOperator.Factory<IN, Committable> {\n+        protected final FileStoreTable table;\n+        protected final StoreSinkWrite.Provider storeSinkWriteProvider;\n+        protected final String initialCommitUser;\n+\n+        protected Factory(\n+                FileStoreTable table,\n+                StoreSinkWrite.Provider storeSinkWriteProvider,\n+                String initialCommitUser) {\n+            super(Options.fromMap(table.options()));\n+            this.table = table;\n+            this.storeSinkWriteProvider = storeSinkWriteProvider;\n+            this.initialCommitUser = initialCommitUser;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketCompactionSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketCompactionSink.java\nindex da966d5e5156..7a4095f896cc 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketCompactionSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketCompactionSink.java\n@@ -24,7 +24,7 @@\n \n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n /** Compaction Sink for unaware-bucket table. */\n public class UnawareBucketCompactionSink extends FlinkSink<UnawareAppendCompactionTask> {\n@@ -42,9 +42,9 @@ public static DataStreamSink<?> sink(\n     }\n \n     @Override\n-    protected OneInputStreamOperator<UnawareAppendCompactionTask, Committable> createWriteOperator(\n-            StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new AppendOnlySingleTableCompactionWorkerOperator(table, commitUser);\n+    protected OneInputStreamOperatorFactory<UnawareAppendCompactionTask, Committable>\n+            createWriteOperatorFactory(StoreSinkWrite.Provider writeProvider, String commitUser) {\n+        return new AppendOnlySingleTableCompactionWorkerOperator.Factory(table, commitUser);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketSink.java\nindex 98b58aa8e96d..7bc40d4c2080 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/UnawareBucketSink.java\n@@ -74,11 +74,14 @@ public DataStream<Committable> doWrite(\n                                             new CommittableTypeInfo(),\n                                             new CompactionTaskTypeInfo()),\n                                     new AppendBypassCoordinateOperatorFactory<>(table))\n+                            .startNewChain()\n                             .forceNonParallel()\n                             .transform(\n                                     \""Compact Worker: \"" + table.name(),\n                                     new CommittableTypeInfo(),\n-                                    new AppendBypassCompactWorkerOperator(table, initialCommitUser))\n+                                    new AppendBypassCompactWorkerOperator.Factory(\n+                                            table, initialCommitUser))\n+                            .startNewChain()\n                             .setParallelism(written.getParallelism());\n         }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/GlobalDynamicBucketSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/GlobalDynamicBucketSink.java\nindex 26e080c32e83..7022002a43ba 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/GlobalDynamicBucketSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/GlobalDynamicBucketSink.java\n@@ -39,7 +39,7 @@\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n \n import javax.annotation.Nullable;\n \n@@ -63,9 +63,9 @@ public GlobalDynamicBucketSink(\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Tuple2<InternalRow, Integer>, Committable> createWriteOperator(\n-            StoreSinkWrite.Provider writeProvider, String commitUser) {\n-        return new DynamicBucketRowWriteOperator(table, writeProvider, commitUser);\n+    protected OneInputStreamOperatorFactory<Tuple2<InternalRow, Integer>, Committable>\n+            createWriteOperatorFactory(StoreSinkWrite.Provider writeProvider, String commitUser) {\n+        return new DynamicBucketRowWriteOperator.Factory(table, writeProvider, commitUser);\n     }\n \n     public DataStreamSink<?> build(DataStream<InternalRow> input, @Nullable Integer parallelism) {\n@@ -89,7 +89,8 @@ public DataStreamSink<?> build(DataStream<InternalRow> input, @Nullable Integer\n                                 new InternalTypeInfo<>(\n                                         new KeyWithRowSerializer<>(\n                                                 bootstrapSerializer, rowSerializer)),\n-                                new IndexBootstrapOperator<>(new IndexBootstrap(table), r -> r))\n+                                new IndexBootstrapOperator.Factory<>(\n+                                        new IndexBootstrap(table), r -> r))\n                         .setParallelism(input.getParallelism());\n \n         // 1. shuffle by key hash\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/IndexBootstrapOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/IndexBootstrapOperator.java\nindex 5c8ba8f9441f..8136565f98cf 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/IndexBootstrapOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/index/IndexBootstrapOperator.java\n@@ -27,8 +27,13 @@\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.runtime.state.StateInitializationContext;\n import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n \n /** Operator for {@link IndexBootstrap}. */\n@@ -40,11 +45,13 @@ public class IndexBootstrapOperator<T> extends AbstractStreamOperator<Tuple2<Key\n     private final IndexBootstrap bootstrap;\n     private final SerializableFunction<InternalRow, T> converter;\n \n-    public IndexBootstrapOperator(\n-            IndexBootstrap bootstrap, SerializableFunction<InternalRow, T> converter) {\n+    private IndexBootstrapOperator(\n+            StreamOperatorParameters<Tuple2<KeyPartOrRow, T>> parameters,\n+            IndexBootstrap bootstrap,\n+            SerializableFunction<InternalRow, T> converter) {\n         this.bootstrap = bootstrap;\n         this.converter = converter;\n-        setChainingStrategy(ChainingStrategy.ALWAYS);\n+        setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());\n     }\n \n     @Override\n@@ -65,4 +72,30 @@ private void collect(InternalRow row) {\n         output.collect(\n                 new StreamRecord<>(new Tuple2<>(KeyPartOrRow.KEY_PART, converter.apply(row))));\n     }\n+\n+    /** {@link StreamOperatorFactory} of {@link IndexBootstrapOperator}. */\n+    public static class Factory<T> extends AbstractStreamOperatorFactory<Tuple2<KeyPartOrRow, T>>\n+            implements OneInputStreamOperatorFactory<T, Tuple2<KeyPartOrRow, T>> {\n+        private final IndexBootstrap bootstrap;\n+        private final SerializableFunction<InternalRow, T> converter;\n+\n+        public Factory(IndexBootstrap bootstrap, SerializableFunction<InternalRow, T> converter) {\n+            this.chainingStrategy = ChainingStrategy.ALWAYS;\n+            this.bootstrap = bootstrap;\n+            this.converter = converter;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""unchecked\"")\n+        public <OP extends StreamOperator<Tuple2<KeyPartOrRow, T>>> OP createStreamOperator(\n+                StreamOperatorParameters<Tuple2<KeyPartOrRow, T>> parameters) {\n+            return (OP) new IndexBootstrapOperator<>(parameters, bootstrap, converter);\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\""rawtypes\"")\n+        public Class<? extends StreamOperator> getStreamOperatorClass(ClassLoader classLoader) {\n+            return IndexBootstrapOperator.class;\n+        }\n+    }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperator.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperator.java\nindex 45090f7b68b4..b8b0d61e10a9 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperator.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperator.java\n@@ -26,8 +26,8 @@\n \n import org.apache.flink.api.common.operators.ProcessingTimeService.ProcessingTimeCallback;\n import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n-import org.apache.flink.streaming.api.operators.ChainingStrategy;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n import org.apache.flink.types.Either;\n@@ -58,10 +58,12 @@ public class AppendBypassCoordinateOperator<CommitT>\n     private transient LinkedBlockingQueue<UnawareAppendCompactionTask> compactTasks;\n \n     public AppendBypassCoordinateOperator(\n-            FileStoreTable table, ProcessingTimeService processingTimeService) {\n+            StreamOperatorParameters<Either<CommitT, UnawareAppendCompactionTask>> parameters,\n+            FileStoreTable table,\n+            ProcessingTimeService processingTimeService) {\n         this.table = table;\n         this.processingTimeService = processingTimeService;\n-        this.chainingStrategy = ChainingStrategy.HEAD;\n+        setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperatorFactory.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperatorFactory.java\nindex 7c53e01b47e6..a4c51e5b5a9b 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperatorFactory.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/AppendBypassCoordinateOperatorFactory.java\n@@ -45,11 +45,7 @@ T createStreamOperator(\n                     StreamOperatorParameters<Either<CommitT, UnawareAppendCompactionTask>>\n                             parameters) {\n         AppendBypassCoordinateOperator<CommitT> operator =\n-                new AppendBypassCoordinateOperator<>(table, processingTimeService);\n-        operator.setup(\n-                parameters.getContainingTask(),\n-                parameters.getStreamConfig(),\n-                parameters.getOutput());\n+                new AppendBypassCoordinateOperator<>(parameters, table, processingTimeService);\n         return (T) operator;\n     }\n \n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java\nindex 8c78ab853a60..9f35b25026bb 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreMultiWriteOperatorTest.java\n@@ -689,8 +689,8 @@ public void testUsingTheSameCompactExecutor() throws Exception {\n \n     private OneInputStreamOperatorTestHarness<CdcMultiplexRecord, MultiTableCommittable>\n             createTestHarness(Catalog.Loader catalogLoader) throws Exception {\n-        CdcRecordStoreMultiWriteOperator operator =\n-                new CdcRecordStoreMultiWriteOperator(\n+        CdcRecordStoreMultiWriteOperator.Factory operatorFactory =\n+                new CdcRecordStoreMultiWriteOperator.Factory(\n                         catalogLoader,\n                         (t, commitUser, state, ioManager, memoryPoolFactory, metricGroup) ->\n                                 new StoreSinkWriteImpl(\n@@ -709,7 +709,7 @@ public void testUsingTheSameCompactExecutor() throws Exception {\n         TypeSerializer<MultiTableCommittable> outputSerializer =\n                 new MultiTableCommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<CdcMultiplexRecord, MultiTableCommittable> harness =\n-                new OneInputStreamOperatorTestHarness<>(operator, inputSerializer);\n+                new OneInputStreamOperatorTestHarness<>(operatorFactory, inputSerializer);\n         harness.setup(outputSerializer);\n         return harness;\n     }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java\nindex f3693fe405de..f00229d99890 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordStoreWriteOperatorTest.java\n@@ -253,8 +253,8 @@ public void testUpdateColumnType() throws Exception {\n \n     private OneInputStreamOperatorTestHarness<CdcRecord, Committable> createTestHarness(\n             FileStoreTable table) throws Exception {\n-        CdcRecordStoreWriteOperator operator =\n-                new CdcRecordStoreWriteOperator(\n+        CdcRecordStoreWriteOperator.Factory operatorFactory =\n+                new CdcRecordStoreWriteOperator.Factory(\n                         table,\n                         (t, commitUser, state, ioManager, memoryPool, metricGroup) ->\n                                 new StoreSinkWriteImpl(\n@@ -272,7 +272,7 @@ private OneInputStreamOperatorTestHarness<CdcRecord, Committable> createTestHarn\n         TypeSerializer<Committable> outputSerializer =\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<CdcRecord, Committable> harness =\n-                new OneInputStreamOperatorTestHarness<>(operator, inputSerializer);\n+                new OneInputStreamOperatorTestHarness<>(operatorFactory, inputSerializer);\n         harness.setup(outputSerializer);\n         return harness;\n     }\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\nindex fd23e500d5e5..e1bd112ca751 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/FlinkCdcMultiTableSinkTest.java\n@@ -60,7 +60,6 @@ public void cancel() {}\n                         () -> FlinkCatalogFactory.createPaimonCatalog(new Options()),\n                         FlinkConnectorOptions.SINK_COMMITTER_CPU.defaultValue(),\n                         null,\n-                        true,\n                         UUID.randomUUID().toString());\n         DataStreamSink<?> dataStreamSink = sink.sinkFrom(input);\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java\nindex d589459d9b96..949c2c7a66a3 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlyMultiTableCompactionWorkerOperatorTest.java\n@@ -25,7 +25,13 @@\n import org.apache.paimon.table.sink.CommitMessage;\n import org.apache.paimon.table.sink.CommitMessageImpl;\n \n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask;\n+import org.apache.flink.streaming.util.MockOutput;\n+import org.apache.flink.streaming.util.MockStreamConfig;\n import org.assertj.core.api.Assertions;\n import org.junit.jupiter.api.Test;\n \n@@ -45,8 +51,17 @@ public class AppendOnlyMultiTableCompactionWorkerOperatorTest extends TableTestB\n     public void testAsyncCompactionWorks() throws Exception {\n \n         AppendOnlyMultiTableCompactionWorkerOperator workerOperator =\n-                new AppendOnlyMultiTableCompactionWorkerOperator(\n-                        () -> catalog, \""user\"", new Options());\n+                new AppendOnlyMultiTableCompactionWorkerOperator.Factory(\n+                                () -> catalog, \""user\"", new Options())\n+                        .createStreamOperator(\n+                                new StreamOperatorParameters<>(\n+                                        new SourceOperatorStreamTask<Integer>(\n+                                                new DummyEnvironment()),\n+                                        new MockStreamConfig(new Configuration(), 1),\n+                                        new MockOutput<>(new ArrayList<>()),\n+                                        null,\n+                                        null,\n+                                        null));\n \n         List<StreamRecord<MultiTableUnawareAppendCompactionTask>> records = new ArrayList<>();\n         // create table and write\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java\nindex d04032817cf0..6238a9cbf3ea 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AppendOnlySingleTableCompactionWorkerOperatorTest.java\n@@ -32,7 +32,13 @@\n import org.apache.paimon.table.sink.CommitMessageImpl;\n import org.apache.paimon.types.DataTypes;\n \n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask;\n+import org.apache.flink.streaming.util.MockOutput;\n+import org.apache.flink.streaming.util.MockStreamConfig;\n import org.assertj.core.api.Assertions;\n import org.junit.jupiter.api.Test;\n \n@@ -49,7 +55,16 @@ public class AppendOnlySingleTableCompactionWorkerOperatorTest extends TableTest\n     public void testAsyncCompactionWorks() throws Exception {\n         createTableDefault();\n         AppendOnlySingleTableCompactionWorkerOperator workerOperator =\n-                new AppendOnlySingleTableCompactionWorkerOperator(getTableDefault(), \""user\"");\n+                new AppendOnlySingleTableCompactionWorkerOperator.Factory(getTableDefault(), \""user\"")\n+                        .createStreamOperator(\n+                                new StreamOperatorParameters<>(\n+                                        new SourceOperatorStreamTask<Integer>(\n+                                                new DummyEnvironment()),\n+                                        new MockStreamConfig(new Configuration(), 1),\n+                                        new MockOutput<>(new ArrayList<>()),\n+                                        null,\n+                                        null,\n+                                        null));\n \n         // write 200 files\n         List<CommitMessage> commitMessages = writeDataDefault(200, 20);\n@@ -102,7 +117,16 @@ public void testAsyncCompactionWorks() throws Exception {\n     public void testAsyncCompactionFileDeletedWhenShutdown() throws Exception {\n         createTableDefault();\n         AppendOnlySingleTableCompactionWorkerOperator workerOperator =\n-                new AppendOnlySingleTableCompactionWorkerOperator(getTableDefault(), \""user\"");\n+                new AppendOnlySingleTableCompactionWorkerOperator.Factory(getTableDefault(), \""user\"")\n+                        .createStreamOperator(\n+                                new StreamOperatorParameters<>(\n+                                        new SourceOperatorStreamTask<Integer>(\n+                                                new DummyEnvironment()),\n+                                        new MockStreamConfig(new Configuration(), 1),\n+                                        new MockOutput<>(new ArrayList<>()),\n+                                        null,\n+                                        null,\n+                                        null));\n \n         // write 200 files\n         List<CommitMessage> commitMessages = writeDataDefault(200, 40);\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java\nindex 3b58c24d16b1..ee930a06fc3d 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/AutoTagForSavepointCommitterOperatorTest.java\n@@ -32,7 +32,7 @@\n import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n import org.apache.flink.runtime.checkpoint.SavepointType;\n import org.apache.flink.runtime.state.StateInitializationContext;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n import org.junit.jupiter.api.Test;\n \n@@ -198,13 +198,15 @@ private void processCommittable(\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Committable, Committable> createCommitterOperator(\n-            FileStoreTable table,\n-            String commitUser,\n-            CommittableStateManager<ManifestCommittable> committableStateManager) {\n-        return new AutoTagForSavepointCommitterOperator<>(\n-                (CommitterOperator<Committable, ManifestCommittable>)\n-                        super.createCommitterOperator(table, commitUser, committableStateManager),\n+    protected OneInputStreamOperatorFactory<Committable, Committable>\n+            createCommitterOperatorFactory(\n+                    FileStoreTable table,\n+                    String commitUser,\n+                    CommittableStateManager<ManifestCommittable> committableStateManager) {\n+        return new AutoTagForSavepointCommitterOperatorFactory<>(\n+                (CommitterOperatorFactory<Committable, ManifestCommittable>)\n+                        super.createCommitterOperatorFactory(\n+                                table, commitUser, committableStateManager),\n                 table::snapshotManager,\n                 table::tagManager,\n                 () -> table.store().newTagDeletion(),\n@@ -213,14 +215,15 @@ protected OneInputStreamOperator<Committable, Committable> createCommitterOperat\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Committable, Committable> createCommitterOperator(\n-            FileStoreTable table,\n-            String commitUser,\n-            CommittableStateManager<ManifestCommittable> committableStateManager,\n-            ThrowingConsumer<StateInitializationContext, Exception> initializeFunction) {\n-        return new AutoTagForSavepointCommitterOperator<>(\n-                (CommitterOperator<Committable, ManifestCommittable>)\n-                        super.createCommitterOperator(\n+    protected OneInputStreamOperatorFactory<Committable, Committable>\n+            createCommitterOperatorFactory(\n+                    FileStoreTable table,\n+                    String commitUser,\n+                    CommittableStateManager<ManifestCommittable> committableStateManager,\n+                    ThrowingConsumer<StateInitializationContext, Exception> initializeFunction) {\n+        return new AutoTagForSavepointCommitterOperatorFactory<>(\n+                (CommitterOperatorFactory<Committable, ManifestCommittable>)\n+                        super.createCommitterOperatorFactory(\n                                 table, commitUser, committableStateManager, initializeFunction),\n                 table::snapshotManager,\n                 table::tagManager,\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java\nindex 147110637aef..68162832eac9 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/BatchWriteGeneratorTagOperatorTest.java\n@@ -27,13 +27,21 @@\n import org.apache.paimon.utils.SnapshotManager;\n import org.apache.paimon.utils.TagManager;\n \n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask;\n+import org.apache.flink.streaming.util.MockOutput;\n+import org.apache.flink.streaming.util.MockStreamConfig;\n import org.junit.jupiter.api.Test;\n \n import java.time.Instant;\n import java.time.LocalDateTime;\n import java.time.ZoneId;\n import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.Objects;\n \n@@ -54,12 +62,23 @@ public void testBatchWriteGeneratorTag() throws Exception {\n         StreamTableWrite write =\n                 table.newStreamWriteBuilder().withCommitUser(initialCommitUser).newWrite();\n \n-        OneInputStreamOperator<Committable, Committable> committerOperator =\n-                createCommitterOperator(\n+        OneInputStreamOperatorFactory<Committable, Committable> committerOperatorFactory =\n+                createCommitterOperatorFactory(\n                         table,\n                         initialCommitUser,\n                         new RestoreAndFailCommittableStateManager<>(\n                                 ManifestCommittableSerializer::new));\n+\n+        OneInputStreamOperator<Committable, Committable> committerOperator =\n+                committerOperatorFactory.createStreamOperator(\n+                        new StreamOperatorParameters<>(\n+                                new SourceOperatorStreamTask<Integer>(new DummyEnvironment()),\n+                                new MockStreamConfig(new Configuration(), 1),\n+                                new MockOutput<>(new ArrayList<>()),\n+                                null,\n+                                null,\n+                                null));\n+\n         committerOperator.open();\n \n         TableCommitImpl tableCommit = table.newCommit(initialCommitUser);\n@@ -106,13 +125,15 @@ public void testBatchWriteGeneratorTag() throws Exception {\n     }\n \n     @Override\n-    protected OneInputStreamOperator<Committable, Committable> createCommitterOperator(\n-            FileStoreTable table,\n-            String commitUser,\n-            CommittableStateManager<ManifestCommittable> committableStateManager) {\n-        return new BatchWriteGeneratorTagOperator<>(\n-                (CommitterOperator<Committable, ManifestCommittable>)\n-                        super.createCommitterOperator(table, commitUser, committableStateManager),\n+    protected OneInputStreamOperatorFactory<Committable, Committable>\n+            createCommitterOperatorFactory(\n+                    FileStoreTable table,\n+                    String commitUser,\n+                    CommittableStateManager<ManifestCommittable> committableStateManager) {\n+        return new BatchWriteGeneratorTagOperatorFactory<>(\n+                (CommitterOperatorFactory<Committable, ManifestCommittable>)\n+                        super.createCommitterOperatorFactory(\n+                                table, commitUser, committableStateManager),\n                 table);\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java\nindex 668d651236fd..28c93ca79be0 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CommitterOperatorTest.java\n@@ -51,10 +51,13 @@\n import org.apache.flink.runtime.checkpoint.OperatorSubtaskState;\n import org.apache.flink.runtime.jobgraph.OperatorID;\n import org.apache.flink.runtime.state.StateInitializationContext;\n-import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.util.Preconditions;\n import org.assertj.core.api.Assertions;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n@@ -259,8 +262,8 @@ public void testRestoreCommitUser() throws Exception {\n         // 3. Check whether success\n         List<String> actual = new ArrayList<>();\n \n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n                         table,\n                         initialCommitUser,\n                         new NoopCommittableStateManager(),\n@@ -274,7 +277,7 @@ public void testRestoreCommitUser() throws Exception {\n                         });\n \n         OneInputStreamOperatorTestHarness<Committable, Committable> testHarness1 =\n-                createTestHarness(operator);\n+                createTestHarness(operatorFactory);\n         testHarness1.initializeState(snapshot);\n         testHarness1.close();\n \n@@ -315,10 +318,11 @@ public void testRestoreEmptyMarkDoneState() throws Exception {\n     public void testCommitInputEnd() throws Exception {\n         FileStoreTable table = createFileStoreTable();\n         String commitUser = UUID.randomUUID().toString();\n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(table, commitUser, new NoopCommittableStateManager());\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n+                        table, commitUser, new NoopCommittableStateManager());\n         OneInputStreamOperatorTestHarness<Committable, Committable> testHarness =\n-                createTestHarness(operator);\n+                createTestHarness(operatorFactory);\n         testHarness.open();\n         Assertions.assertThatCode(\n                         () -> {\n@@ -378,10 +382,10 @@ public void testCommitInputEnd() throws Exception {\n                         })\n                 .doesNotThrowAnyException();\n \n-        if (operator instanceof CommitterOperator) {\n+        if (operatorFactory instanceof CommitterOperator) {\n             Assertions.assertThat(\n                             ((ManifestCommittable)\n-                                            ((CommitterOperator) operator)\n+                                            ((CommitterOperator) operatorFactory)\n                                                     .committablesPerCheckpoint.get(Long.MAX_VALUE))\n                                     .fileCommittables()\n                                     .size())\n@@ -604,14 +608,14 @@ public void testCalcDataBytesSend() throws Exception {\n     public void testCommitMetrics() throws Exception {\n         FileStoreTable table = createFileStoreTable();\n \n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n                         table,\n                         null,\n                         new RestoreAndFailCommittableStateManager<>(\n                                 ManifestCommittableSerializer::new));\n         OneInputStreamOperatorTestHarness<Committable, Committable> testHarness =\n-                createTestHarness(operator);\n+                createTestHarness(operatorFactory);\n         testHarness.open();\n         long timestamp = 0;\n         StreamTableWrite write =\n@@ -627,7 +631,9 @@ public void testCommitMetrics() throws Exception {\n         testHarness.notifyOfCompletedCheckpoint(cpId);\n \n         MetricGroup commitMetricGroup =\n-                operator.getMetricGroup()\n+                testHarness\n+                        .getOneInputOperator()\n+                        .getMetricGroup()\n                         .addGroup(\""paimon\"")\n                         .addGroup(\""table\"", table.name())\n                         .addGroup(\""commit\"");\n@@ -685,10 +691,11 @@ public void testCommitMetrics() throws Exception {\n     public void testParallelism() throws Exception {\n         FileStoreTable table = createFileStoreTable();\n         String commitUser = UUID.randomUUID().toString();\n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(table, commitUser, new NoopCommittableStateManager());\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n+                        table, commitUser, new NoopCommittableStateManager());\n         try (OneInputStreamOperatorTestHarness<Committable, Committable> testHarness =\n-                createTestHarness(operator, 10, 10, 3)) {\n+                createTestHarness(operatorFactory, 10, 10, 3)) {\n             Assertions.assertThatCode(testHarness::open)\n                     .hasMessage(\""Committer Operator parallelism in paimon MUST be one.\"");\n         }\n@@ -700,13 +707,13 @@ public void testParallelism() throws Exception {\n \n     protected OneInputStreamOperatorTestHarness<Committable, Committable>\n             createRecoverableTestHarness(FileStoreTable table) throws Exception {\n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n                         table,\n                         null,\n                         new RestoreAndFailCommittableStateManager<>(\n                                 ManifestCommittableSerializer::new));\n-        return createTestHarness(operator);\n+        return createTestHarness(operatorFactory);\n     }\n \n     private OneInputStreamOperatorTestHarness<Committable, Committable> createLossyTestHarness(\n@@ -716,18 +723,20 @@ private OneInputStreamOperatorTestHarness<Committable, Committable> createLossyT\n \n     private OneInputStreamOperatorTestHarness<Committable, Committable> createLossyTestHarness(\n             FileStoreTable table, String commitUser) throws Exception {\n-        OneInputStreamOperator<Committable, Committable> operator =\n-                createCommitterOperator(table, commitUser, new NoopCommittableStateManager());\n-        return createTestHarness(operator);\n+        OneInputStreamOperatorFactory<Committable, Committable> operatorFactory =\n+                createCommitterOperatorFactory(\n+                        table, commitUser, new NoopCommittableStateManager());\n+        return createTestHarness(operatorFactory);\n     }\n \n     private OneInputStreamOperatorTestHarness<Committable, Committable> createTestHarness(\n-            OneInputStreamOperator<Committable, Committable> operator) throws Exception {\n-        return createTestHarness(operator, 1, 1, 0);\n+            OneInputStreamOperatorFactory<Committable, Committable> operatorFactory)\n+            throws Exception {\n+        return createTestHarness(operatorFactory, 1, 1, 0);\n     }\n \n     private OneInputStreamOperatorTestHarness<Committable, Committable> createTestHarness(\n-            OneInputStreamOperator<Committable, Committable> operator,\n+            OneInputStreamOperatorFactory<Committable, Committable> operatorFactory,\n             int maxParallelism,\n             int parallelism,\n             int subTaskIndex)\n@@ -736,22 +745,23 @@ private OneInputStreamOperatorTestHarness<Committable, Committable> createTestHa\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<Committable, Committable> harness =\n                 new OneInputStreamOperatorTestHarness<>(\n-                        operator,\n+                        operatorFactory,\n                         maxParallelism,\n                         parallelism,\n                         subTaskIndex,\n-                        serializer,\n                         new OperatorID());\n+        harness.getStreamConfig().setupNetworkInputs(Preconditions.checkNotNull(serializer));\n+        harness.getStreamConfig().serializeAllConfigs();\n         harness.setup(serializer);\n         return harness;\n     }\n \n-    protected OneInputStreamOperator<Committable, Committable> createCommitterOperator(\n-            FileStoreTable table,\n-            String commitUser,\n-            CommittableStateManager<ManifestCommittable> committableStateManager) {\n-        return new CommitterOperator<>(\n-                true,\n+    protected OneInputStreamOperatorFactory<Committable, Committable>\n+            createCommitterOperatorFactory(\n+                    FileStoreTable table,\n+                    String commitUser,\n+                    CommittableStateManager<ManifestCommittable> committableStateManager) {\n+        return new CommitterOperatorFactory<>(\n                 true,\n                 true,\n                 commitUser == null ? initialCommitUser : commitUser,\n@@ -765,13 +775,13 @@ protected OneInputStreamOperator<Committable, Committable> createCommitterOperat\n                 committableStateManager);\n     }\n \n-    protected OneInputStreamOperator<Committable, Committable> createCommitterOperator(\n-            FileStoreTable table,\n-            String commitUser,\n-            CommittableStateManager<ManifestCommittable> committableStateManager,\n-            ThrowingConsumer<StateInitializationContext, Exception> initializeFunction) {\n-        return new CommitterOperator<Committable, ManifestCommittable>(\n-                true,\n+    protected OneInputStreamOperatorFactory<Committable, Committable>\n+            createCommitterOperatorFactory(\n+                    FileStoreTable table,\n+                    String commitUser,\n+                    CommittableStateManager<ManifestCommittable> committableStateManager,\n+                    ThrowingConsumer<StateInitializationContext, Exception> initializeFunction) {\n+        return new CommitterOperatorFactory<Committable, ManifestCommittable>(\n                 true,\n                 true,\n                 commitUser == null ? initialCommitUser : commitUser,\n@@ -784,8 +794,24 @@ protected OneInputStreamOperator<Committable, Committable> createCommitterOperat\n                                 context),\n                 committableStateManager) {\n             @Override\n-            public void initializeState(StateInitializationContext context) throws Exception {\n-                initializeFunction.accept(context);\n+            @SuppressWarnings(\""unchecked\"")\n+            public <T extends StreamOperator<Committable>> T createStreamOperator(\n+                    StreamOperatorParameters<Committable> parameters) {\n+                return (T)\n+                        new CommitterOperator<Committable, ManifestCommittable>(\n+                                parameters,\n+                                streamingCheckpointEnabled,\n+                                forceSingleParallelism,\n+                                initialCommitUser,\n+                                committerFactory,\n+                                committableStateManager,\n+                                endInputWatermark) {\n+                            @Override\n+                            public void initializeState(StateInitializationContext context)\n+                                    throws Exception {\n+                                initializeFunction.accept(context);\n+                            }\n+                        };\n             }\n         };\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java\nindex 42293ca2842e..d487d75925eb 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/CompactorSinkITCase.java\n@@ -254,8 +254,8 @@ private OneInputStreamOperatorTestHarness<RowData, Committable> createTestHarnes\n         return harness;\n     }\n \n-    protected StoreCompactOperator createCompactOperator(FileStoreTable table) {\n-        return new StoreCompactOperator(\n+    protected StoreCompactOperator.Factory createCompactOperator(FileStoreTable table) {\n+        return new StoreCompactOperator.Factory(\n                 table,\n                 (t, commitUser, state, ioManager, memoryPool, metricGroup) ->\n                         new StoreSinkWriteImpl(\n@@ -272,9 +272,9 @@ protected StoreCompactOperator createCompactOperator(FileStoreTable table) {\n                 true);\n     }\n \n-    protected MultiTablesStoreCompactOperator createMultiTablesCompactOperator(\n+    protected MultiTablesStoreCompactOperator.Factory createMultiTablesCompactOperator(\n             Catalog.Loader catalogLoader) throws Exception {\n-        return new MultiTablesStoreCompactOperator(\n+        return new MultiTablesStoreCompactOperator.Factory(\n                 catalogLoader,\n                 commitUser,\n                 new CheckpointConfig(),\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java\nindex c335568344b3..5f21858e61a5 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/FlinkSinkTest.java\n@@ -42,7 +42,7 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.operators.SimpleOperatorFactory;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n import org.apache.flink.streaming.api.transformations.OneInputTransformation;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n import org.junit.jupiter.api.Test;\n@@ -82,20 +82,22 @@ private boolean testSpillable(\n                         Collections.singletonList(GenericRow.of(1, 1)));\n         FlinkSink<InternalRow> flinkSink = new FixedBucketSink(fileStoreTable, null, null);\n         DataStream<Committable> written = flinkSink.doWrite(source, \""123\"", 1);\n-        RowDataStoreWriteOperator operator =\n-                ((RowDataStoreWriteOperator)\n-                        ((SimpleOperatorFactory)\n-                                        ((OneInputTransformation) written.getTransformation())\n-                                                .getOperatorFactory())\n-                                .getOperator());\n+        OneInputStreamOperatorFactory<InternalRow, Committable> operatorFactory =\n+                (OneInputStreamOperatorFactory<InternalRow, Committable>)\n+                        ((OneInputTransformation<InternalRow, Committable>)\n+                                        written.getTransformation())\n+                                .getOperatorFactory();\n \n         TypeSerializer<Committable> serializer =\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<InternalRow, Committable> harness =\n-                new OneInputStreamOperatorTestHarness<>(operator);\n+                new OneInputStreamOperatorTestHarness<>(operatorFactory);\n         harness.setup(serializer);\n         harness.initializeEmptyState();\n \n+        RowDataStoreWriteOperator operator =\n+                (RowDataStoreWriteOperator) harness.getOneInputOperator();\n+\n         return ((KeyValueFileStoreWrite) ((StoreSinkWriteImpl) operator.write).write.getWrite())\n                 .bufferSpillable();\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java\nindex 1162e20b155b..fc45eceb3fd5 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/LocalMergeOperatorTest.java\n@@ -26,12 +26,18 @@\n import org.apache.paimon.types.RowKind;\n import org.apache.paimon.types.RowType;\n \n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n import org.apache.flink.streaming.api.watermark.Watermark;\n import org.apache.flink.streaming.runtime.streamrecord.LatencyMarker;\n import org.apache.flink.streaming.runtime.streamrecord.RecordAttributes;\n import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask;\n import org.apache.flink.streaming.runtime.watermarkstatus.WatermarkStatus;\n+import org.apache.flink.streaming.util.MockOutput;\n+import org.apache.flink.streaming.util.MockStreamConfig;\n import org.apache.flink.util.OutputTag;\n import org.junit.jupiter.api.Test;\n \n@@ -151,7 +157,17 @@ private void prepareHashOperator(Map<String, String> options) throws Exception {\n                         Collections.singletonList(\""f0\""),\n                         options,\n                         null);\n-        operator = new LocalMergeOperator(schema);\n+        operator =\n+                new LocalMergeOperator.Factory(schema)\n+                        .createStreamOperator(\n+                                new StreamOperatorParameters<>(\n+                                        new SourceOperatorStreamTask<Integer>(\n+                                                new DummyEnvironment()),\n+                                        new MockStreamConfig(new Configuration(), 1),\n+                                        new MockOutput<>(new ArrayList<>()),\n+                                        null,\n+                                        null,\n+                                        null));\n         operator.open();\n         assertThat(operator.merger()).isInstanceOf(HashMapLocalMerger.class);\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java\nindex f8387e1fc41a..3740033e025e 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreCompactOperatorTest.java\n@@ -48,8 +48,8 @@ public void testCompactExactlyOnce(boolean streamingMode) throws Exception {\n \n         CompactRememberStoreWrite compactRememberStoreWrite =\n                 new CompactRememberStoreWrite(streamingMode);\n-        StoreCompactOperator operator =\n-                new StoreCompactOperator(\n+        StoreCompactOperator.Factory operatorFactory =\n+                new StoreCompactOperator.Factory(\n                         getTableDefault(),\n                         (table, commitUser, state, ioManager, memoryPool, metricGroup) ->\n                                 compactRememberStoreWrite,\n@@ -59,7 +59,7 @@ public void testCompactExactlyOnce(boolean streamingMode) throws Exception {\n         TypeSerializer<Committable> serializer =\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<RowData, Committable> harness =\n-                new OneInputStreamOperatorTestHarness<>(operator);\n+                new OneInputStreamOperatorTestHarness<>(operatorFactory);\n         harness.setup(serializer);\n         harness.initializeEmptyState();\n         harness.open();\n@@ -70,7 +70,7 @@ public void testCompactExactlyOnce(boolean streamingMode) throws Exception {\n         harness.processElement(new StreamRecord<>(data(1)));\n         harness.processElement(new StreamRecord<>(data(2)));\n \n-        operator.prepareCommit(true, 1);\n+        ((StoreCompactOperator) harness.getOneInputOperator()).prepareCommit(true, 1);\n         Assertions.assertThat(compactRememberStoreWrite.compactTime).isEqualTo(3);\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\nindex 10e432f3c8c2..752679fb5903 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/StoreMultiCommitterTest.java\n@@ -645,11 +645,10 @@ public void testCommitMetrics() throws Exception {\n \n     private OneInputStreamOperatorTestHarness<MultiTableCommittable, MultiTableCommittable>\n             createRecoverableTestHarness() throws Exception {\n-        CommitterOperator<MultiTableCommittable, WrappedManifestCommittable> operator =\n-                new CommitterOperator<>(\n+        CommitterOperatorFactory<MultiTableCommittable, WrappedManifestCommittable> operator =\n+                new CommitterOperatorFactory<>(\n                         true,\n                         false,\n-                        true,\n                         initialCommitUser,\n                         context -> new StoreMultiCommitter(catalogLoader, context),\n                         new RestoreAndFailCommittableStateManager<>(\n@@ -659,11 +658,10 @@ public void testCommitMetrics() throws Exception {\n \n     private OneInputStreamOperatorTestHarness<MultiTableCommittable, MultiTableCommittable>\n             createLossyTestHarness() throws Exception {\n-        CommitterOperator<MultiTableCommittable, WrappedManifestCommittable> operator =\n-                new CommitterOperator<>(\n+        CommitterOperatorFactory<MultiTableCommittable, WrappedManifestCommittable> operator =\n+                new CommitterOperatorFactory<>(\n                         true,\n                         false,\n-                        true,\n                         initialCommitUser,\n                         context -> new StoreMultiCommitter(catalogLoader, context),\n                         new CommittableStateManager<WrappedManifestCommittable>() {\n@@ -682,12 +680,13 @@ public void snapshotState(\n \n     private OneInputStreamOperatorTestHarness<MultiTableCommittable, MultiTableCommittable>\n             createTestHarness(\n-                    CommitterOperator<MultiTableCommittable, WrappedManifestCommittable> operator)\n+                    CommitterOperatorFactory<MultiTableCommittable, WrappedManifestCommittable>\n+                            operatorFactory)\n                     throws Exception {\n         TypeSerializer<MultiTableCommittable> serializer =\n                 new MultiTableCommittableTypeInfo().createSerializer(new ExecutionConfig());\n         OneInputStreamOperatorTestHarness<MultiTableCommittable, MultiTableCommittable> harness =\n-                new OneInputStreamOperatorTestHarness<>(operator, serializer);\n+                new OneInputStreamOperatorTestHarness<>(operatorFactory, serializer);\n         harness.setup(serializer);\n         return harness;\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java\nnew file mode 100644\nindex 000000000000..a4605b830918\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterChainingStrategyTest.java\n@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.paimon.flink.sink;\n+\n+import org.apache.flink.api.dag.Transformation;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.jobgraph.JobVertex;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.CompiledPlan;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.internal.CompiledPlanUtils;\n+import org.apache.flink.util.TimeUtils;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+/**\n+ * Tests for {@link org.apache.flink.streaming.api.operators.ChainingStrategy} of writer operators.\n+ */\n+public class WriterChainingStrategyTest {\n+    private static final String TABLE_NAME = \""paimon_table\"";\n+\n+    @TempDir java.nio.file.Path tempDir;\n+\n+    private StreamTableEnvironment tEnv;\n+\n+    @BeforeEach\n+    public void beforeEach() {\n+        Configuration config = new Configuration();\n+        config.setString(\n+                \""execution.checkpointing.interval\"",\n+                TimeUtils.formatWithHighestUnit(Duration.ofMillis(500)));\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);\n+        tEnv = StreamTableEnvironment.create(env);\n+\n+        String catalog = \""PAIMON\"";\n+        Map<String, String> options = new HashMap<>();\n+        options.put(\""type\"", \""paimon\"");\n+        options.put(\""warehouse\"", tempDir.toString());\n+        tEnv.executeSql(\n+                String.format(\n+                        \""CREATE CATALOG %s WITH ( %s )\"",\n+                        catalog,\n+                        options.entrySet().stream()\n+                                .map(e -> String.format(\""'%s'='%s'\"", e.getKey(), e.getValue()))\n+                                .collect(Collectors.joining(\"",\""))));\n+        tEnv.useCatalog(catalog);\n+    }\n+\n+    @Test\n+    public void testAppendTable() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING) \""\n+                                        + \""WITH ('bucket' = '1', 'bucket-key'='id', 'write-only' = 'true')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        verifyChaining(false, true);\n+    }\n+\n+    @Test\n+    public void testAppendTableWithUnawareBucket() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING) \""\n+                                        + \""WITH ('bucket' = '-1', 'write-only' = 'true')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        verifyChaining(true, true);\n+    }\n+\n+    @Test\n+    public void testPrimaryKeyTable() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING, PRIMARY KEY (id) NOT ENFORCED) \""\n+                                        + \""WITH ('bucket' = '1', 'bucket-key'='id', 'write-only' = 'true')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        verifyChaining(false, true);\n+    }\n+\n+    @Test\n+    public void testPrimaryKeyTableWithDynamicBucket() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING, PRIMARY KEY (id) NOT ENFORCED) \""\n+                                        + \""WITH ('bucket' = '-1', 'write-only' = 'true')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        verifyChaining(false, true);\n+    }\n+\n+    @Test\n+    public void testPrimaryKeyTableWithMultipleWriter() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING, PRIMARY KEY (id) NOT ENFORCED) \""\n+                                        + \""WITH ('bucket' = '1', 'bucket-key'='id', 'write-only' = 'true', 'sink.parallelism' = '2')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        verifyChaining(false, false);\n+    }\n+\n+    @Test\n+    public void testPrimaryKeyTableWithCrossPartitionUpdate() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING, PRIMARY KEY (id) NOT ENFORCED) \""\n+                                        + \""PARTITIONED BY ( dt ) WITH ('bucket' = '-1', 'write-only' = 'true')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        List<JobVertex> vertices = verifyChaining(false, true);\n+        JobVertex vertex = findVertex(vertices, \""INDEX_BOOTSTRAP\"");\n+        assertThat(vertex.toString()).contains(\""Source\"");\n+    }\n+\n+    @Test\n+    public void testPrimaryKeyTableWithLocalMerge() throws Exception {\n+        tEnv.executeSql(\n+                        String.format(\n+                                \""CREATE TABLE %s (id INT, data STRING, dt STRING, PRIMARY KEY (id) NOT ENFORCED) \""\n+                                        + \""WITH ('bucket' = '-1', 'write-only' = 'true', 'local-merge-buffer-size' = '1MB')\"",\n+                                TABLE_NAME))\n+                .await();\n+\n+        List<JobVertex> vertices = verifyChaining(false, true);\n+        JobVertex vertex = findVertex(vertices, \""local merge\"");\n+        assertThat(vertex.toString()).contains(\""Source\"");\n+    }\n+\n+    private List<JobVertex> verifyChaining(\n+            boolean isWriterChainedWithUpstream, boolean isWriterChainedWithDownStream) {\n+        CompiledPlan plan =\n+                tEnv.compilePlanSql(\n+                        String.format(\n+                                \""INSERT INTO %s VALUES (1, 'AAA', ''), (2, 'BBB', '')\"",\n+                                TABLE_NAME));\n+        List<Transformation<?>> transformations = CompiledPlanUtils.toTransformations(tEnv, plan);\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        transformations.forEach(env::addOperator);\n+\n+        List<JobVertex> vertices = new ArrayList<>();\n+        env.getStreamGraph().getJobGraph().getVertices().forEach(vertices::add);\n+        JobVertex vertex = findVertex(vertices, \""Writer\"");\n+\n+        if (isWriterChainedWithUpstream) {\n+            assertThat(vertex.toString()).contains(\""Source\"");\n+        } else {\n+            assertThat(vertex.toString()).doesNotContain(\""Source\"");\n+        }\n+\n+        if (isWriterChainedWithDownStream) {\n+            assertThat(vertex.toString()).contains(\""Committer\"");\n+        } else {\n+            assertThat(vertex.toString()).doesNotContain(\""Committer\"");\n+        }\n+\n+        return vertices;\n+    }\n+\n+    private JobVertex findVertex(List<JobVertex> vertices, String key) {\n+        for (JobVertex vertex : vertices) {\n+            if (vertex.toString().contains(key)) {\n+                return vertex;\n+            }\n+        }\n+        throw new IllegalStateException(\n+                String.format(\n+                        \""Cannot find vertex with keyword %s among job vertices %s\"", key, vertices));\n+    }\n+}\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java\nindex 3a8c1557122f..83af15745078 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WriterOperatorTest.java\n@@ -115,9 +115,10 @@ public void testAppendOnlyTableMetrics() throws Exception {\n \n     private void testMetricsImpl(FileStoreTable fileStoreTable) throws Exception {\n         String tableName = tablePath.getName();\n-        RowDataStoreWriteOperator operator = getStoreSinkWriteOperator(fileStoreTable);\n+        RowDataStoreWriteOperator.Factory operatorFactory =\n+                getStoreSinkWriteOperatorFactory(fileStoreTable);\n         OneInputStreamOperatorTestHarness<InternalRow, Committable> harness =\n-                createHarness(operator);\n+                createHarness(operatorFactory);\n \n         TypeSerializer<Committable> serializer =\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n@@ -133,7 +134,7 @@ private void testMetricsImpl(FileStoreTable fileStoreTable) throws Exception {\n         harness.snapshot(1, 2);\n         harness.notifyOfCompletedCheckpoint(1);\n \n-        OperatorMetricGroup metricGroup = operator.getMetricGroup();\n+        OperatorMetricGroup metricGroup = harness.getOneInputOperator().getMetricGroup();\n         MetricGroup writerBufferMetricGroup =\n                 metricGroup\n                         .addGroup(\""paimon\"")\n@@ -173,9 +174,10 @@ public void testAsyncLookupWithFailure() throws Exception {\n                         rowType, Arrays.asList(\""pt\"", \""k\""), Collections.singletonList(\""k\""), options);\n \n         // we don't wait for compaction because this is async lookup test\n-        RowDataStoreWriteOperator operator = getAsyncLookupWriteOperator(fileStoreTable, false);\n+        RowDataStoreWriteOperator.Factory operatorFactory =\n+                getAsyncLookupWriteOperatorFactory(fileStoreTable, false);\n         OneInputStreamOperatorTestHarness<InternalRow, Committable> harness =\n-                createHarness(operator);\n+                createHarness(operatorFactory);\n \n         TableCommitImpl commit = fileStoreTable.newCommit(commitUser);\n \n@@ -205,8 +207,8 @@ public void testAsyncLookupWithFailure() throws Exception {\n         harness.close();\n \n         // re-create operator from state, this time wait for compaction to check result\n-        operator = getAsyncLookupWriteOperator(fileStoreTable, true);\n-        harness = createHarness(operator);\n+        operatorFactory = getAsyncLookupWriteOperatorFactory(fileStoreTable, true);\n+        harness = createHarness(operatorFactory);\n         harness.setup(serializer);\n         harness.initializeState(state);\n         harness.open();\n@@ -263,9 +265,10 @@ private void testChangelog(boolean insertOnly) throws Exception {\n         FileStoreTable fileStoreTable =\n                 createFileStoreTable(\n                         rowType, Arrays.asList(\""pt\"", \""k\""), Collections.singletonList(\""k\""), options);\n-        RowDataStoreWriteOperator operator = getStoreSinkWriteOperator(fileStoreTable);\n+        RowDataStoreWriteOperator.Factory operatorFactory =\n+                getStoreSinkWriteOperatorFactory(fileStoreTable);\n         OneInputStreamOperatorTestHarness<InternalRow, Committable> harness =\n-                createHarness(operator);\n+                createHarness(operatorFactory);\n \n         TableCommitImpl commit = fileStoreTable.newCommit(commitUser);\n \n@@ -277,7 +280,7 @@ private void testChangelog(boolean insertOnly) throws Exception {\n         if (insertOnly) {\n             Field field = TableWriteOperator.class.getDeclaredField(\""write\"");\n             field.setAccessible(true);\n-            StoreSinkWrite write = (StoreSinkWrite) field.get(operator);\n+            StoreSinkWrite write = (StoreSinkWrite) field.get(harness.getOneInputOperator());\n             write.withInsertOnly(true);\n         }\n \n@@ -339,17 +342,17 @@ public void testNumWritersMetric() throws Exception {\n                         options);\n         TableCommitImpl commit = fileStoreTable.newCommit(commitUser);\n \n-        RowDataStoreWriteOperator rowDataStoreWriteOperator =\n-                getStoreSinkWriteOperator(fileStoreTable);\n+        RowDataStoreWriteOperator.Factory operatorFactory =\n+                getStoreSinkWriteOperatorFactory(fileStoreTable);\n         OneInputStreamOperatorTestHarness<InternalRow, Committable> harness =\n-                createHarness(rowDataStoreWriteOperator);\n+                createHarness(operatorFactory);\n \n         TypeSerializer<Committable> serializer =\n                 new CommittableTypeInfo().createSerializer(new ExecutionConfig());\n         harness.setup(serializer);\n         harness.open();\n \n-        OperatorMetricGroup metricGroup = rowDataStoreWriteOperator.getMetricGroup();\n+        OperatorMetricGroup metricGroup = harness.getOneInputOperator().getMetricGroup();\n         MetricGroup writerBufferMetricGroup =\n                 metricGroup\n                         .addGroup(\""paimon\"")\n@@ -408,8 +411,9 @@ public void testNumWritersMetric() throws Exception {\n     //  Test utils\n     // ------------------------------------------------------------------------\n \n-    private RowDataStoreWriteOperator getStoreSinkWriteOperator(FileStoreTable fileStoreTable) {\n-        return new RowDataStoreWriteOperator(\n+    private RowDataStoreWriteOperator.Factory getStoreSinkWriteOperatorFactory(\n+            FileStoreTable fileStoreTable) {\n+        return new RowDataStoreWriteOperator.Factory(\n                 fileStoreTable,\n                 null,\n                 (table, commitUser, state, ioManager, memoryPool, metricGroup) ->\n@@ -426,9 +430,9 @@ private RowDataStoreWriteOperator getStoreSinkWriteOperator(FileStoreTable fileS\n                 commitUser);\n     }\n \n-    private RowDataStoreWriteOperator getAsyncLookupWriteOperator(\n+    private RowDataStoreWriteOperator.Factory getAsyncLookupWriteOperatorFactory(\n             FileStoreTable fileStoreTable, boolean waitCompaction) {\n-        return new RowDataStoreWriteOperator(\n+        return new RowDataStoreWriteOperator.Factory(\n                 fileStoreTable,\n                 null,\n                 (table, commitUser, state, ioManager, memoryPool, metricGroup) ->\n@@ -471,10 +475,11 @@ private FileStoreTable createFileStoreTable(\n     }\n \n     private OneInputStreamOperatorTestHarness<InternalRow, Committable> createHarness(\n-            RowDataStoreWriteOperator operator) throws Exception {\n+            RowDataStoreWriteOperator.Factory operatorFactory) throws Exception {\n         InternalTypeInfo<InternalRow> internalRowInternalTypeInfo =\n                 new InternalTypeInfo<>(new InternalRowTypeSerializer(RowType.builder().build()));\n         return new OneInputStreamOperatorTestHarness<>(\n-                operator, internalRowInternalTypeInfo.createSerializer(new ExecutionConfig()));\n+                operatorFactory,\n+                internalRowInternalTypeInfo.createSerializer(new ExecutionConfig()));\n     }\n }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4590"", ""pr_id"": 4590, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 4, ""non_test_files_count"": 3, ""pr_changed_files"": [""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkFileIO.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FileStoreSourceReader.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedSourceReader.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java""], ""base_commit"": ""a79b08bc668c16e143e3f0126d59dea2a7253b11"", ""head_commit"": ""9fc1aec0f314180c25926a4397fe2d4f6d6ff361"", ""repo_url"": ""https://github.com/apache/paimon/pull/4590"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4590"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-27T09:46:13.000Z"", ""patch"": ""diff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkFileIO.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkFileIO.java\nindex 74512409bfc8..617d25125f37 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkFileIO.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/FlinkFileIO.java\n@@ -29,10 +29,10 @@\n import org.apache.flink.core.fs.FSDataOutputStream;\n import org.apache.flink.core.fs.FileSystem;\n import org.apache.flink.core.fs.FileSystem.WriteMode;\n-import org.apache.flink.core.fs.FileSystemKind;\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.util.Locale;\n \n /** Flink {@link FileIO} to use {@link FileSystem}. */\n public class FlinkFileIO implements FileIO {\n@@ -48,7 +48,27 @@ public FlinkFileIO(Path path) {\n     @Override\n     public boolean isObjectStore() {\n         try {\n-            return path.getFileSystem().getKind() != FileSystemKind.FILE_SYSTEM;\n+            FileSystem fs = path.getFileSystem();\n+            String scheme = fs.getUri().getScheme().toLowerCase(Locale.US);\n+\n+            if (scheme.startsWith(\""s3\"")\n+                    || scheme.startsWith(\""emr\"")\n+                    || scheme.startsWith(\""oss\"")\n+                    || scheme.startsWith(\""wasb\"")\n+                    || scheme.startsWith(\""gs\"")) {\n+                // the Amazon S3 storage or Aliyun OSS storage or Azure Blob Storage\n+                // or Google Cloud Storage\n+                return true;\n+            } else if (scheme.startsWith(\""http\"") || scheme.startsWith(\""ftp\"")) {\n+                // file servers instead of file systems\n+                // they might actually be consistent, but we have no hard guarantees\n+                // currently to rely on that\n+                return true;\n+            } else {\n+                // the remainder should include hdfs, kosmos, ceph, ...\n+                // this also includes federated HDFS (viewfs).\n+                return false;\n+            }\n         } catch (IOException e) {\n             throw new UncheckedIOException(e);\n         }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FileStoreSourceReader.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FileStoreSourceReader.java\nindex 92adf5e04998..8fc78c868ba5 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FileStoreSourceReader.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FileStoreSourceReader.java\n@@ -25,9 +25,7 @@\n \n import org.apache.flink.api.connector.source.SourceReader;\n import org.apache.flink.api.connector.source.SourceReaderContext;\n-import org.apache.flink.connector.base.source.reader.RecordsWithSplitIds;\n import org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase;\n-import org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue;\n import org.apache.flink.connector.file.src.reader.BulkFormat.RecordIterator;\n import org.apache.flink.table.data.RowData;\n \n@@ -64,27 +62,6 @@ public FileStoreSourceReader(\n         this.ioManager = ioManager;\n     }\n \n-    public FileStoreSourceReader(\n-            SourceReaderContext readerContext,\n-            TableRead tableRead,\n-            FileStoreSourceReaderMetrics metrics,\n-            IOManager ioManager,\n-            @Nullable Long limit,\n-            FutureCompletingBlockingQueue<RecordsWithSplitIds<RecordIterator<RowData>>>\n-                    elementsQueue) {\n-        super(\n-                elementsQueue,\n-                () ->\n-                        new FileStoreSourceSplitReader(\n-                                tableRead, RecordLimiter.create(limit), metrics),\n-                (element, output, state) ->\n-                        FlinkRecordsWithSplitIds.emitRecord(\n-                                readerContext, element, output, state, metrics),\n-                readerContext.getConfiguration(),\n-                readerContext);\n-        this.ioManager = ioManager;\n-    }\n-\n     @Override\n     public void start() {\n         // we request a split only if we did not get splits during the checkpoint restore\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedSourceReader.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedSourceReader.java\nindex 1f0bbca314b6..a8ffe3de561f 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedSourceReader.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedSourceReader.java\n@@ -58,7 +58,7 @@ public AlignedSourceReader(\n             @Nullable Long limit,\n             FutureCompletingBlockingQueue<RecordsWithSplitIds<BulkFormat.RecordIterator<RowData>>>\n                     elementsQueue) {\n-        super(readerContext, tableRead, metrics, ioManager, limit, elementsQueue);\n+        super(readerContext, tableRead, metrics, ioManager, limit);\n         this.elementsQueue = elementsQueue;\n         this.nextCheckpointId = null;\n     }\n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java\nindex 2a855796d8d4..96334de3f87b 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/CatalogTableITCase.java\n@@ -251,17 +251,20 @@ public void testSchemasTable() {\n         sql(\""ALTER TABLE T SET ('snapshot.num-retained.min' = '18')\"");\n         sql(\""ALTER TABLE T SET ('manifest.format' = 'avro')\"");\n \n-        assertThat(sql(\""SHOW CREATE TABLE T$schemas\"").toString())\n-                .isEqualTo(\n-                        \""[+I[CREATE TABLE `PAIMON`.`default`.`T$schemas` (\\n\""\n-                                + \""  `schema_id` BIGINT NOT NULL,\\n\""\n-                                + \""  `fields` VARCHAR(2147483647) NOT NULL,\\n\""\n-                                + \""  `partition_keys` VARCHAR(2147483647) NOT NULL,\\n\""\n-                                + \""  `primary_keys` VARCHAR(2147483647) NOT NULL,\\n\""\n-                                + \""  `options` VARCHAR(2147483647) NOT NULL,\\n\""\n-                                + \""  `comment` VARCHAR(2147483647),\\n\""\n-                                + \""  `update_time` TIMESTAMP(3) NOT NULL\\n\""\n-                                + \"") ]]\"");\n+        String actualResult = sql(\""SHOW CREATE TABLE T$schemas\"").toString();\n+        String expectedResult =\n+                \""[+I[CREATE TABLE `PAIMON`.`default`.`T$schemas` (\\n\""\n+                        + \""  `schema_id` BIGINT NOT NULL,\\n\""\n+                        + \""  `fields` VARCHAR(2147483647) NOT NULL,\\n\""\n+                        + \""  `partition_keys` VARCHAR(2147483647) NOT NULL,\\n\""\n+                        + \""  `primary_keys` VARCHAR(2147483647) NOT NULL,\\n\""\n+                        + \""  `options` VARCHAR(2147483647) NOT NULL,\\n\""\n+                        + \""  `comment` VARCHAR(2147483647),\\n\""\n+                        + \""  `update_time` TIMESTAMP(3) NOT NULL\\n\""\n+                        + \"") ]]\"";\n+        actualResult = actualResult.replace(\"" \"", \""\"").replace(\""\\n\"", \""\"");\n+        expectedResult = expectedResult.replace(\"" \"", \""\"").replace(\""\\n\"", \""\"");\n+        assertThat(actualResult).isEqualTo(expectedResult);\n \n         List<Row> result =\n                 sql(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java\nindex 8404d994fa9f..0c5d485af7bc 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/source/IteratorSourcesITCase.java\n@@ -18,10 +18,10 @@\n \n package org.apache.paimon.flink.source;\n \n+import org.apache.commons.collections.IteratorUtils;\n import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.test.util.MiniClusterWithClientResource;\n@@ -67,7 +67,7 @@ public void testParallelSourceExecution() throws Exception {\n                         \""iterator source\"");\n \n         final List<RowData> result =\n-                DataStreamUtils.collectBoundedStream(stream, \""Iterator Source Test\"");\n+                IteratorUtils.toList(stream.executeAndCollect(\""Iterator Source Test\""));\n \n         verifySequence(result, 1L, 1_000L);\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java\nindex cfc23a0a44d8..39939f78670b 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/MiniClusterWithClientExtension.java\n@@ -29,7 +29,6 @@\n import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n import org.apache.flink.streaming.util.TestStreamEnvironment;\n import org.apache.flink.test.junit5.InjectClusterClient;\n-import org.apache.flink.test.util.TestEnvironment;\n import org.junit.jupiter.api.extension.AfterAllCallback;\n import org.junit.jupiter.api.extension.AfterEachCallback;\n import org.junit.jupiter.api.extension.BeforeAllCallback;\n@@ -167,17 +166,12 @@ private void registerEnv(InternalMiniClusterExtension internalMiniClusterExtensi\n                         .getOptional(CoreOptions.DEFAULT_PARALLELISM)\n                         .orElse(internalMiniClusterExtension.getNumberSlots());\n \n-        TestEnvironment executionEnvironment =\n-                new TestEnvironment(\n-                        internalMiniClusterExtension.getMiniCluster(), defaultParallelism, false);\n-        executionEnvironment.setAsContext();\n         TestStreamEnvironment.setAsContext(\n                 internalMiniClusterExtension.getMiniCluster(), defaultParallelism);\n     }\n \n     private void unregisterEnv(InternalMiniClusterExtension internalMiniClusterExtension) {\n         TestStreamEnvironment.unsetAsContext();\n-        TestEnvironment.unsetAsContext();\n     }\n \n     private MiniClusterClient createMiniClusterClient(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\nindex 9c3170f9a96b..0eac2ed2936e 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\n@@ -23,7 +23,6 @@\n import org.apache.paimon.utils.BlockingIterator;\n \n import org.apache.flink.api.common.RuntimeExecutionMode;\n-import org.apache.flink.api.common.time.Time;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.RestartStrategyOptions;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n@@ -37,6 +36,7 @@\n import javax.annotation.Nullable;\n \n import java.nio.file.Paths;\n+import java.time.Duration;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.HashMap;\n@@ -53,7 +53,7 @@\n /** Test util for {@link ReadWriteTableITCase}. */\n public class ReadWriteTableTestUtil {\n \n-    private static final Time TIME_OUT = Time.seconds(10);\n+    private static final Duration TIME_OUT = Duration.ofSeconds(10);\n \n     public static final int DEFAULT_PARALLELISM = 2;\n \n@@ -278,7 +278,7 @@ public static void testBatchRead(String query, List<Row> expected) throws Except\n         try (BlockingIterator<Row, Row> iterator = BlockingIterator.of(resultItr)) {\n             if (!expected.isEmpty()) {\n                 List<Row> result =\n-                        iterator.collect(expected.size(), TIME_OUT.getSize(), TIME_OUT.getUnit());\n+                        iterator.collect(expected.size(), TIME_OUT.getSeconds(), TimeUnit.SECONDS);\n                 assertThat(toInsertOnlyRows(result))\n                         .containsExactlyInAnyOrderElementsOf(toInsertOnlyRows(expected));\n             }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4584"", ""pr_id"": 4584, ""issue_id"": 4442, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] Avoid deprecated APIs removed in Flink 2.0 Preview\n### Search before asking\r\n\r\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\r\n\r\n\r\n### Motivation\r\n\r\nFlink 2.0 Preview has been released, and Paimon needs to make itself compatible with 2.0 Preview now, to be better prepared for the Flink 2.0 in future.\r\n\r\n### Solution\r\n\r\nA bunch of public APIs that had been marked as `@Deprecated` are now removed in Flink 2.0 Preview. Paimon needs to replace usages of these deprecated APIs to the latest alternatives.\r\n\r\nSubtasks | Related PR | Status\r\n-- | -- | --\r\n[hotfix] Wait for consumer reset before job close | #4578 | Merged\r\n[cdc] Update flink dependency to 1.20 | #4580 | Merged\r\nAdopt open(OpenContext) in RichFunction | #4581 | Merged\r\nAdopt getTaskInfo() when acquiring parallelism info | #4583 | Merged\r\nAvoid deprecated usages about Configuration | #4584 | Merged\r\nAvoid deprecated DataStreamUtils | #4590 | Merged\r\nRemove deprecated TestEnvironment | #4590 | Merged\r\nReplace deprecated flink Time with java Duration | #4590 | Merged\r\nAvoid deprecated SingleThreadMultiplexSourceReaderBase constructor | #4590 | Merged\r\nAvoid deprecated FileSystem#getKind | #4590 | Merged\r\nAvoid deprecated SetupableStreamOperator | #4591 | Merged\r\nAvoid deprecated usage on Table API, including TableSchema, DataType and DescriptorProperties | #4611 | Merged\r\nReplace legacy SinkFunction with v2 Sink | #4612 | Merged\r\nReplace legacy SourceFunction with v2 Source | #4614 | Under Review\r\nAvoid relying on format of table description | #4590 | Merged\r\nAvoid deprecated sql syntax | \u00a0 | Waiting for external bugfix: [FLINK-36783](https://issues.apache.org/jira/browse/FLINK-36783)\r\nChange MultipleParameterTool package name | \u00a0 | Waiting for Flink 2.0 formal release\r\nCreate profile for Flink 2.0 using Java 11 | \u00a0 | Waiting for Flink 2.0 formal release\r\nAvoid external legacy SourceFunction/SinkFunction in Flink Kafka Connector | \u00a0 | Waiting for external work: Flink Kafka Connector avoids legacy sink function\r\n\r\n### Anything else?\r\n\r\nFlink 2.0 still have more work to do with its internal implementation, and only dealt with its public APIs in 2.0 Preview. So if Paimon has been using Flink's public methods that are not marks as `@Public` or `@PublicEvolving`, the usages might still be compatible with 2.0 Preview for now, but should also be updated to alternatives as early as possible.\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 360, ""test_files_count"": 11, ""non_test_files_count"": 23, ""pr_changed_files"": [""paimon-benchmark/paimon-cluster-benchmark/src/main/java/org/apache/paimon/benchmark/QueryRunner.java"", ""paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java"", ""paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java"", ""paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java"", ""paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java"", ""paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java"", ""paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java"", ""paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java"", ""paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/kafka/KafkaDebeziumAvroDeserializationSchema.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mongodb/strategy/MongoVersionStrategy.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mysql/MySqlRecordParser.java"", ""paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/pulsar/PulsarDebeziumAvroDeserializationSchema.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/changelog/ChangelogTaskTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommittableTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactionTaskTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCommittableTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCompactionTaskTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/InternalTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/JavaTypeInfo.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java"", ""paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java""], ""pr_changed_test_files"": [""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java"", ""paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java"", ""paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java""], ""base_commit"": ""ee466bcac14bd7f1229beeaf4e405da0956792ca"", ""head_commit"": ""f94a1d80abfde350e655935a7c456ab17125e8c7"", ""repo_url"": ""https://github.com/apache/paimon/pull/4584"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4584"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-26T06:27:22.000Z"", ""patch"": ""diff --git a/paimon-benchmark/paimon-cluster-benchmark/src/main/java/org/apache/paimon/benchmark/QueryRunner.java b/paimon-benchmark/paimon-cluster-benchmark/src/main/java/org/apache/paimon/benchmark/QueryRunner.java\nindex b07cdef8465e..8bfe4b6c9c03 100644\n--- a/paimon-benchmark/paimon-cluster-benchmark/src/main/java/org/apache/paimon/benchmark/QueryRunner.java\n+++ b/paimon-benchmark/paimon-cluster-benchmark/src/main/java/org/apache/paimon/benchmark/QueryRunner.java\n@@ -77,7 +77,7 @@ public Result run() {\n \n             String sinkPathConfig =\n                     BenchmarkGlobalConfiguration.loadConfiguration()\n-                            .getString(BenchmarkOptions.SINK_PATH);\n+                            .get(BenchmarkOptions.SINK_PATH);\n             if (sinkPathConfig == null) {\n                 throw new IllegalArgumentException(\n                         BenchmarkOptions.SINK_PATH.key() + \"" must be set\"");\n\ndiff --git a/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\nnew file mode 100644\nindex 000000000000..16987469a948\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\nnew file mode 100644\nindex 000000000000..374d33f6500d\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.15/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public class SerializerConfigImpl implements SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\nnew file mode 100644\nindex 000000000000..16987469a948\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\nnew file mode 100644\nindex 000000000000..374d33f6500d\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.16/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public class SerializerConfigImpl implements SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\nnew file mode 100644\nindex 000000000000..16987469a948\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\nnew file mode 100644\nindex 000000000000..374d33f6500d\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.17/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public class SerializerConfigImpl implements SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\nnew file mode 100644\nindex 000000000000..16987469a948\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfig.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public interface SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\nnew file mode 100644\nindex 000000000000..374d33f6500d\n--- /dev/null\n+++ b/paimon-flink/paimon-flink-1.18/src/main/java/org/apache/flink/api/common/serialization/SerializerConfigImpl.java\n@@ -0,0 +1,22 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \""License\""); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \""AS IS\"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.api.common.serialization;\n+\n+/** Placeholder class to resolve compatibility issues. */\n+public class SerializerConfigImpl implements SerializerConfig {}\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/kafka/KafkaDebeziumAvroDeserializationSchema.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/kafka/KafkaDebeziumAvroDeserializationSchema.java\nindex fc672b9dc0ab..eea364d460de 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/kafka/KafkaDebeziumAvroDeserializationSchema.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/kafka/KafkaDebeziumAvroDeserializationSchema.java\n@@ -48,7 +48,7 @@ public class KafkaDebeziumAvroDeserializationSchema\n \n     public KafkaDebeziumAvroDeserializationSchema(Configuration cdcSourceConfig) {\n         this.topic = KafkaActionUtils.findOneTopic(cdcSourceConfig);\n-        this.schemaRegistryUrl = cdcSourceConfig.getString(SCHEMA_REGISTRY_URL);\n+        this.schemaRegistryUrl = cdcSourceConfig.get(SCHEMA_REGISTRY_URL);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mongodb/strategy/MongoVersionStrategy.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mongodb/strategy/MongoVersionStrategy.java\nindex 64f127571134..df288a4150e6 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mongodb/strategy/MongoVersionStrategy.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mongodb/strategy/MongoVersionStrategy.java\n@@ -83,7 +83,7 @@ default Map<String, String> getExtractRow(\n             Configuration mongodbConfig)\n             throws JsonProcessingException {\n         SchemaAcquisitionMode mode =\n-                SchemaAcquisitionMode.valueOf(mongodbConfig.getString(START_MODE).toUpperCase());\n+                SchemaAcquisitionMode.valueOf(mongodbConfig.get(START_MODE).toUpperCase());\n         ObjectNode objectNode =\n                 JsonSerdeUtil.asSpecificNodeType(jsonNode.asText(), ObjectNode.class);\n         JsonNode idNode = objectNode.get(ID_FIELD);\n@@ -92,7 +92,7 @@ default Map<String, String> getExtractRow(\n                     \""The provided MongoDB JSON document does not contain an _id field.\"");\n         }\n         JsonNode document =\n-                mongodbConfig.getBoolean(DEFAULT_ID_GENERATION)\n+                mongodbConfig.get(DEFAULT_ID_GENERATION)\n                         ? objectNode.set(\n                                 ID_FIELD,\n                                 idNode.get(OID_FIELD) == null ? idNode : idNode.get(OID_FIELD))\n@@ -101,8 +101,8 @@ default Map<String, String> getExtractRow(\n             case SPECIFIED:\n                 return parseFieldsFromJsonRecord(\n                         document.toString(),\n-                        mongodbConfig.getString(PARSER_PATH),\n-                        mongodbConfig.getString(FIELD_NAME),\n+                        mongodbConfig.get(PARSER_PATH),\n+                        mongodbConfig.get(FIELD_NAME),\n                         computedColumns,\n                         rowTypeBuilder);\n             case DYNAMIC:\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mysql/MySqlRecordParser.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mysql/MySqlRecordParser.java\nindex 502e6237a477..26579e718f56 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mysql/MySqlRecordParser.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/mysql/MySqlRecordParser.java\n@@ -45,6 +45,8 @@\n import org.apache.flink.api.common.functions.FlatMapFunction;\n import org.apache.flink.cdc.connectors.mysql.source.config.MySqlSourceOptions;\n import org.apache.flink.cdc.debezium.table.DebeziumOptions;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.util.Collector;\n import org.slf4j.Logger;\n@@ -99,11 +101,14 @@ public MySqlRecordParser(\n                 .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n         String stringifyServerTimeZone = mySqlConfig.get(MySqlSourceOptions.SERVER_TIME_ZONE);\n \n-        this.isDebeziumSchemaCommentsEnabled =\n-                mySqlConfig.getBoolean(\n-                        DebeziumOptions.DEBEZIUM_OPTIONS_PREFIX\n-                                + RelationalDatabaseConnectorConfig.INCLUDE_SCHEMA_COMMENTS.name(),\n-                        false);\n+        ConfigOption<Boolean> includeSchemaCommentsConfig =\n+                ConfigOptions.key(\n+                                DebeziumOptions.DEBEZIUM_OPTIONS_PREFIX\n+                                        + RelationalDatabaseConnectorConfig.INCLUDE_SCHEMA_COMMENTS\n+                                                .name())\n+                        .booleanType()\n+                        .defaultValue(false);\n+        this.isDebeziumSchemaCommentsEnabled = mySqlConfig.get(includeSchemaCommentsConfig);\n         this.serverTimeZone =\n                 stringifyServerTimeZone == null\n                         ? ZoneId.systemDefault()\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/pulsar/PulsarDebeziumAvroDeserializationSchema.java b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/pulsar/PulsarDebeziumAvroDeserializationSchema.java\nindex b0d1d1bf620f..f45ee034bec8 100644\n--- a/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/pulsar/PulsarDebeziumAvroDeserializationSchema.java\n+++ b/paimon-flink/paimon-flink-cdc/src/main/java/org/apache/paimon/flink/action/cdc/pulsar/PulsarDebeziumAvroDeserializationSchema.java\n@@ -46,7 +46,7 @@ public class PulsarDebeziumAvroDeserializationSchema\n \n     public PulsarDebeziumAvroDeserializationSchema(Configuration cdcSourceConfig) {\n         this.topic = PulsarActionUtils.findOneTopic(cdcSourceConfig);\n-        this.schemaRegistryUrl = cdcSourceConfig.getString(SCHEMA_REGISTRY_URL);\n+        this.schemaRegistryUrl = cdcSourceConfig.get(SCHEMA_REGISTRY_URL);\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/changelog/ChangelogTaskTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/changelog/ChangelogTaskTypeInfo.java\nindex 5cae899a0704..a529e6764fae 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/changelog/ChangelogTaskTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/compact/changelog/ChangelogTaskTypeInfo.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.flink.sink.NoneCopyVersionedSerializerTypeSerializerProxy;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n \n@@ -56,7 +57,17 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<ChangelogCompactTask> createSerializer(\n+            SerializerConfig serializerConfig) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<ChangelogCompactTask> createSerializer(ExecutionConfig config) {\n         // we don't need copy for task\n         return new NoneCopyVersionedSerializerTypeSerializerProxy<ChangelogCompactTask>(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommittableTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommittableTypeInfo.java\nindex dcb87238b833..92e826a91379 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommittableTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CommittableTypeInfo.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.table.sink.CommitMessageSerializer;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n \n@@ -57,7 +58,16 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<Committable> createSerializer(SerializerConfig config) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<Committable> createSerializer(ExecutionConfig config) {\n         // no copy, so that data from writer is directly going into committer while chaining\n         return new NoneCopyVersionedSerializerTypeSerializerProxy<Committable>(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactionTaskTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactionTaskTypeInfo.java\nindex 47defa61a971..6510a85b800a 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactionTaskTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/CompactionTaskTypeInfo.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.table.sink.CompactionTaskSerializer;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n \n@@ -58,7 +59,16 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<UnawareAppendCompactionTask> createSerializer(SerializerConfig config) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<UnawareAppendCompactionTask> createSerializer(ExecutionConfig config) {\n         // we don't need copy for task\n         return new NoneCopyVersionedSerializerTypeSerializerProxy<UnawareAppendCompactionTask>(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\nindex 59f2f4b1035f..dd364c196d8b 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/FlinkSink.java\n@@ -42,7 +42,6 @@\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n-import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n@@ -338,13 +337,11 @@ public static void assertStreamingConfiguration(StreamExecutionEnvironment env)\n         checkArgument(\n                 !env.getCheckpointConfig().isUnalignedCheckpointsEnabled(),\n                 \""Paimon sink currently does not support unaligned checkpoints. Please set \""\n-                        + ExecutionCheckpointingOptions.ENABLE_UNALIGNED.key()\n-                        + \"" to false.\"");\n+                        + \""execution.checkpointing.unaligned.enabled to false.\"");\n         checkArgument(\n                 env.getCheckpointConfig().getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE,\n                 \""Paimon sink currently only supports EXACTLY_ONCE checkpoint mode. Please set \""\n-                        + ExecutionCheckpointingOptions.CHECKPOINTING_MODE.key()\n-                        + \"" to exactly-once\"");\n+                        + \""execution.checkpointing.mode to exactly-once\"");\n     }\n \n     public static void assertBatchAdaptiveParallelism(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCommittableTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCommittableTypeInfo.java\nindex f82f08209867..7da0ae0e2078 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCommittableTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCommittableTypeInfo.java\n@@ -21,6 +21,7 @@\n import org.apache.paimon.table.sink.CommitMessageSerializer;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n \n@@ -57,7 +58,16 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<MultiTableCommittable> createSerializer(SerializerConfig config) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<MultiTableCommittable> createSerializer(ExecutionConfig config) {\n         // no copy, so that data from writer is directly going into committer while chaining\n         return new NoneCopyVersionedSerializerTypeSerializerProxy<MultiTableCommittable>(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCompactionTaskTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCompactionTaskTypeInfo.java\nindex f27f29f87fe7..0116ff198811 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCompactionTaskTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/MultiTableCompactionTaskTypeInfo.java\n@@ -23,6 +23,7 @@\n import org.apache.paimon.table.sink.MultiTableCompactionTaskSerializer;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n import org.apache.flink.core.io.SimpleVersionedSerializerTypeSerializerProxy;\n@@ -60,7 +61,17 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<MultiTableUnawareAppendCompactionTask> createSerializer(\n+            SerializerConfig serializerConfig) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<MultiTableUnawareAppendCompactionTask> createSerializer(\n             ExecutionConfig executionConfig) {\n         return new SimpleVersionedSerializerTypeSerializerProxy<\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\nindex b3dcd4840cc1..e864ec050045 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/FlinkSourceBuilder.java\n@@ -46,7 +46,6 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.CheckpointConfig;\n-import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n@@ -331,30 +330,25 @@ private void assertStreamingConfigurationForAlignMode(StreamExecutionEnvironment\n         checkArgument(\n                 checkpointConfig.isCheckpointingEnabled(),\n                 \""The align mode of paimon source is only supported when checkpoint enabled. Please set \""\n-                        + ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL.key()\n-                        + \""larger than 0\"");\n+                        + \""execution.checkpointing.interval larger than 0\"");\n         checkArgument(\n                 checkpointConfig.getMaxConcurrentCheckpoints() == 1,\n                 \""The align mode of paimon source supports at most one ongoing checkpoint at the same time. Please set \""\n-                        + ExecutionCheckpointingOptions.MAX_CONCURRENT_CHECKPOINTS.key()\n-                        + \"" to 1\"");\n+                        + \""execution.checkpointing.max-concurrent-checkpoints to 1\"");\n         checkArgument(\n                 checkpointConfig.getCheckpointTimeout()\n                         > conf.get(FlinkConnectorOptions.SOURCE_CHECKPOINT_ALIGN_TIMEOUT)\n                                 .toMillis(),\n                 \""The align mode of paimon source requires that the timeout of checkpoint is greater than the timeout of the source's snapshot alignment. Please increase \""\n-                        + ExecutionCheckpointingOptions.CHECKPOINTING_TIMEOUT.key()\n-                        + \"" or decrease \""\n+                        + \""execution.checkpointing.timeout or decrease \""\n                         + FlinkConnectorOptions.SOURCE_CHECKPOINT_ALIGN_TIMEOUT.key());\n         checkArgument(\n                 !env.getCheckpointConfig().isUnalignedCheckpointsEnabled(),\n                 \""The align mode of paimon source currently does not support unaligned checkpoints. Please set \""\n-                        + ExecutionCheckpointingOptions.ENABLE_UNALIGNED.key()\n-                        + \"" to false.\"");\n+                        + \""execution.checkpointing.unaligned.enabled to false.\"");\n         checkArgument(\n                 env.getCheckpointConfig().getCheckpointingMode() == CheckpointingMode.EXACTLY_ONCE,\n                 \""The align mode of paimon source currently only supports EXACTLY_ONCE checkpoint mode. Please set \""\n-                        + ExecutionCheckpointingOptions.CHECKPOINTING_MODE.key()\n-                        + \"" to exactly-once\"");\n+                        + \""execution.checkpointing.mode to exactly-once\"");\n     }\n }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\nindex d6b7060763ac..705e1d9a7a4c 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/source/align/AlignedContinuousFileStoreSource.java\n@@ -73,7 +73,7 @@ public SourceReader<RowData, FileStoreSourceSplit> createReader(SourceReaderCont\n                 limit,\n                 new FutureCompletingBlockingQueue<>(\n                         context.getConfiguration()\n-                                .getInteger(SourceReaderOptions.ELEMENT_QUEUE_CAPACITY)));\n+                                .get(SourceReaderOptions.ELEMENT_QUEUE_CAPACITY)));\n     }\n \n     @Override\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/InternalTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/InternalTypeInfo.java\nindex 4ea5db9f34d4..60898421ddea 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/InternalTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/InternalTypeInfo.java\n@@ -22,6 +22,7 @@\n import org.apache.paimon.types.RowType;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeSerializer;\n \n@@ -73,8 +74,17 @@ public boolean isKeyType() {\n         return false;\n     }\n \n-    @Override\n-    public TypeSerializer<T> createSerializer(ExecutionConfig config) {\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<T> createSerializer(SerializerConfig config) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n+    public TypeSerializer<T> createSerializer(ExecutionConfig executionConfig) {\n         return serializer.duplicate();\n     }\n \n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/JavaTypeInfo.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/JavaTypeInfo.java\nindex a36243c5bdac..4aea809b51bc 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/JavaTypeInfo.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/utils/JavaTypeInfo.java\n@@ -20,6 +20,7 @@\n \n import org.apache.flink.annotation.PublicEvolving;\n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n import org.apache.flink.api.common.typeinfo.AtomicType;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeutils.TypeComparator;\n@@ -78,7 +79,16 @@ public boolean isKeyType() {\n         return Comparable.class.isAssignableFrom(typeClass);\n     }\n \n-    @Override\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 1.18-.\n+     */\n+    public TypeSerializer<T> createSerializer(SerializerConfig config) {\n+        return this.createSerializer((ExecutionConfig) null);\n+    }\n+\n+    /**\n+     * Do not annotate with <code>@override</code> here to maintain compatibility with Flink 2.0+.\n+     */\n     public TypeSerializer<T> createSerializer(ExecutionConfig config) {\n         return new JavaSerializer<>(this.typeClass);\n     }\n@@ -91,7 +101,9 @@ public TypeComparator<T> createComparator(\n             @SuppressWarnings(\""rawtypes\"")\n             GenericTypeComparator comparator =\n                     new GenericTypeComparator(\n-                            sortOrderAscending, createSerializer(executionConfig), this.typeClass);\n+                            sortOrderAscending,\n+                            new JavaSerializer<>(this.typeClass),\n+                            this.typeClass);\n             return (TypeComparator<T>) comparator;\n         }\n \n"", ""test_patch"": ""diff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java\nindex 394cdd1f149b..f0328b566324 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mongodb/MongodbSchemaITCase.java\n@@ -76,13 +76,12 @@ public static void initMongoDB() {\n     @Test\n     public void testCreateSchemaFromValidConfig() {\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n-        mongodbConfig.setString(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n-        mongodbConfig.setString(\n-                MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n-        mongodbConfig.setString(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n+        mongodbConfig.set(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n+        mongodbConfig.set(MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n+        mongodbConfig.set(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n+        mongodbConfig.set(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n+        mongodbConfig.set(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n         Schema schema = MongodbSchemaUtils.getMongodbSchema(mongodbConfig);\n         assertNotNull(schema);\n     }\n@@ -90,13 +89,12 @@ public void testCreateSchemaFromValidConfig() {\n     @Test\n     public void testCreateSchemaFromInvalidHost() {\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, \""127.0.0.1:12345\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n-        mongodbConfig.setString(\n-                MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n-        mongodbConfig.setString(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, \""127.0.0.1:12345\"");\n+        mongodbConfig.set(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n+        mongodbConfig.set(MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n+        mongodbConfig.set(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n+        mongodbConfig.set(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n+        mongodbConfig.set(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n \n         assertThrows(\n                 RuntimeException.class, () -> MongodbSchemaUtils.getMongodbSchema(mongodbConfig));\n@@ -106,7 +104,7 @@ public void testCreateSchemaFromInvalidHost() {\n     public void testCreateSchemaFromIncompleteConfig() {\n         // Create a Configuration object with missing necessary settings\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n         // Expect an exception to be thrown due to missing necessary settings\n         assertThrows(\n                 NullPointerException.class,\n@@ -117,13 +115,12 @@ public void testCreateSchemaFromIncompleteConfig() {\n     public void testCreateSchemaFromDynamicConfig() {\n         // Create a Configuration object with the necessary settings\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n-        mongodbConfig.setString(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n-        mongodbConfig.setString(\n-                MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n-        mongodbConfig.setString(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n+        mongodbConfig.set(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n+        mongodbConfig.set(MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n+        mongodbConfig.set(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n+        mongodbConfig.set(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n+        mongodbConfig.set(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n \n         // Call the method and check the results\n         Schema schema = MongodbSchemaUtils.getMongodbSchema(mongodbConfig);\n@@ -142,13 +139,12 @@ public void testCreateSchemaFromDynamicConfig() {\n     @Test\n     public void testCreateSchemaFromInvalidDatabase() {\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n-        mongodbConfig.setString(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n-        mongodbConfig.setString(\n-                MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n-        mongodbConfig.setString(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.DATABASE, \""invalidDatabase\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n+        mongodbConfig.set(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n+        mongodbConfig.set(MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n+        mongodbConfig.set(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n+        mongodbConfig.set(MongoDBSourceOptions.DATABASE, \""invalidDatabase\"");\n+        mongodbConfig.set(MongoDBSourceOptions.COLLECTION, \""testCollection\"");\n \n         assertThrows(\n                 RuntimeException.class, () -> MongodbSchemaUtils.getMongodbSchema(mongodbConfig));\n@@ -157,13 +153,12 @@ public void testCreateSchemaFromInvalidDatabase() {\n     @Test\n     public void testCreateSchemaFromInvalidCollection() {\n         Configuration mongodbConfig = new Configuration();\n-        mongodbConfig.setString(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n-        mongodbConfig.setString(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n-        mongodbConfig.setString(\n-                MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n-        mongodbConfig.setString(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n-        mongodbConfig.setString(MongoDBSourceOptions.COLLECTION, \""invalidCollection\"");\n+        mongodbConfig.set(MongoDBSourceOptions.HOSTS, MONGODB_CONTAINER.getHostAndPort());\n+        mongodbConfig.set(MongoDBSourceOptions.USERNAME, MongoDBContainer.PAIMON_USER);\n+        mongodbConfig.set(MongoDBSourceOptions.PASSWORD, MongoDBContainer.PAIMON_USER_PASSWORD);\n+        mongodbConfig.set(MongoDBSourceOptions.CONNECTION_OPTIONS, \""authSource=admin\"");\n+        mongodbConfig.set(MongoDBSourceOptions.DATABASE, \""testDatabase\"");\n+        mongodbConfig.set(MongoDBSourceOptions.COLLECTION, \""invalidCollection\"");\n \n         assertThrows(\n                 RuntimeException.class, () -> MongodbSchemaUtils.getMongodbSchema(mongodbConfig));\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java\nindex bdeab07a746c..febbe4e1deaa 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/action/cdc/mysql/MySqlSyncTableActionITCase.java\n@@ -31,7 +31,8 @@\n import org.apache.paimon.utils.CommonTestUtils;\n import org.apache.paimon.utils.JsonSerdeUtil;\n \n-import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.RestartStrategyOptions;\n import org.apache.flink.core.execution.JobClient;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.junit.jupiter.api.BeforeAll;\n@@ -1285,8 +1286,11 @@ public void testDefaultCheckpointInterval() throws Exception {\n         mySqlConfig.put(\""database-name\"", \""default_checkpoint\"");\n         mySqlConfig.put(\""table-name\"", \""t\"");\n \n-        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-        env.setRestartStrategy(RestartStrategies.noRestart());\n+        // Using `none` to avoid compatibility issues with Flink 1.18-.\n+        Configuration configuration = new Configuration();\n+        configuration.set(RestartStrategyOptions.RESTART_STRATEGY, \""none\"");\n+        StreamExecutionEnvironment env =\n+                StreamExecutionEnvironment.getExecutionEnvironment(configuration);\n \n         MySqlSyncTableAction action = syncTableActionBuilder(mySqlConfig).build();\n         action.withStreamExecutionEnvironment(env);\n\ndiff --git a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java\nindex 698900436e8d..b202ca53c9cc 100644\n--- a/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java\n+++ b/paimon-flink/paimon-flink-cdc/src/test/java/org/apache/paimon/flink/sink/cdc/CdcRecordSerializeITCase.java\n@@ -25,6 +25,8 @@\n import org.apache.paimon.types.VarCharType;\n \n import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfigImpl;\n import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;\n import org.apache.flink.core.memory.DataInputView;\n import org.apache.flink.core.memory.DataOutputView;\n@@ -35,6 +37,8 @@\n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n import java.io.IOException;\n+import java.lang.reflect.Constructor;\n+import java.lang.reflect.InvocationTargetException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.HashMap;\n@@ -49,7 +53,7 @@\n public class CdcRecordSerializeITCase {\n \n     @Test\n-    public void testCdcRecordKryoSerialize() throws IOException {\n+    public void testCdcRecordKryoSerialize() throws Exception {\n         KryoSerializer<RichCdcMultiplexRecord> kr =\n                 createFlinkKryoSerializer(RichCdcMultiplexRecord.class);\n         RowType.Builder rowType = RowType.builder();\n@@ -78,7 +82,7 @@ public void testCdcRecordKryoSerialize() throws IOException {\n     }\n \n     @Test\n-    public void testUnmodifiableListKryoSerialize() throws IOException {\n+    public void testUnmodifiableListKryoSerialize() throws Exception {\n         KryoSerializer<List> kryoSerializer = createFlinkKryoSerializer(List.class);\n         RowType.Builder rowType = RowType.builder();\n         rowType.field(\""id\"", new BigIntType());\n@@ -101,8 +105,24 @@ public void testUnmodifiableListKryoSerialize() throws IOException {\n         assertThat(deserializeRecord).isEqualTo(fields);\n     }\n \n-    public static <T> KryoSerializer<T> createFlinkKryoSerializer(Class<T> type) {\n-        return new KryoSerializer<>(type, new ExecutionConfig());\n+    @SuppressWarnings({\""unchecked\"", \""rawtypes\""})\n+    public static <T> KryoSerializer<T> createFlinkKryoSerializer(Class<T> type)\n+            throws NoSuchMethodException, InvocationTargetException, InstantiationException,\n+                    IllegalAccessException {\n+        try {\n+            Constructor<KryoSerializer> constructor =\n+                    KryoSerializer.class.getConstructor(Class.class, SerializerConfig.class);\n+            return (KryoSerializer<T>) constructor.newInstance(type, new SerializerConfigImpl());\n+        } catch (NoSuchMethodException\n+                | InvocationTargetException\n+                | IllegalAccessException\n+                | InstantiationException e) {\n+            // to stay compatible with Flink 1.18-\n+        }\n+\n+        Constructor<KryoSerializer> constructor =\n+                KryoSerializer.class.getConstructor(Class.class, ExecutionConfig.class);\n+        return (KryoSerializer<T>) constructor.newInstance(type, new ExecutionConfig());\n     }\n \n     private static final class TestOutputView extends DataOutputStream implements DataOutputView {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java\nindex 239043ff79e1..915c93680a0d 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FileSystemCatalogITCase.java\n@@ -27,7 +27,6 @@\n import org.apache.paimon.fs.Path;\n import org.apache.paimon.utils.BlockingIterator;\n \n-import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.CloseableIterator;\n@@ -60,7 +59,7 @@ public void setup() {\n                 tableEnvironmentBuilder()\n                         .streamingMode()\n                         .parallelism(1)\n-                        .setConf(ExecutionCheckpointingOptions.ENABLE_UNALIGNED, false)\n+                        .setString(\""execution.checkpointing.unaligned.enabled\"", \""false\"")\n                         .build();\n         path = getTempDirPath();\n         tEnv.executeSql(\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java\nindex c46c4c358922..8df379a71b78 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/FlinkJobRecoveryITCase.java\n@@ -65,7 +65,7 @@ public void before() throws IOException {\n                 .set(\n                         CheckpointingOptions.EXTERNALIZED_CHECKPOINT_RETENTION,\n                         ExternalizedCheckpointRetention.RETAIN_ON_CANCELLATION)\n-                .removeConfig(CheckpointingOptions.CHECKPOINTING_INTERVAL);\n+                .removeKey(\""execution.checkpointing.interval\"");\n \n         // insert source data\n         batchSql(\""INSERT INTO source_table1 VALUES (1, 'test-1', '20241030')\"");\n@@ -219,10 +219,9 @@ private void testRecoverFromSavepoint(\n             batchSql(sql);\n         }\n \n-        Configuration config =\n-                sEnv.getConfig()\n-                        .getConfiguration()\n-                        .set(StateRecoveryOptions.SAVEPOINT_PATH, checkpointPath);\n+        Configuration config = sEnv.getConfig().getConfiguration();\n+        // use config string to stay compatible with flink 1.19-\n+        config.setString(\""execution.state-recovery.path\"", checkpointPath);\n         for (Map.Entry<String, String> entry : recoverOptions.entrySet()) {\n             config.setString(entry.getKey(), entry.getValue());\n         }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java\nindex 08969bddfdb3..d5747d2e28d4 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/RescaleBucketITCase.java\n@@ -26,7 +26,6 @@\n \n import org.apache.flink.core.execution.JobClient;\n import org.apache.flink.core.execution.SavepointFormatType;\n-import org.apache.flink.runtime.jobgraph.SavepointConfigOptions;\n import org.apache.flink.types.Row;\n import org.junit.jupiter.api.Test;\n \n@@ -106,9 +105,10 @@ public void testSuspendAndRecoverAfterRescaleOverwrite() throws Exception {\n         assertThat(batchSql(\""SELECT * FROM T3\"")).containsExactlyInAnyOrderElementsOf(committedData);\n \n         // step5: resume streaming job\n+        // use config string to stay compatible with flink 1.19-\n         sEnv.getConfig()\n                 .getConfiguration()\n-                .set(SavepointConfigOptions.SAVEPOINT_PATH, savepointPath);\n+                .setString(\""execution.state-recovery.path\"", savepointPath);\n         JobClient resumedJobClient =\n                 startJobAndCommitSnapshot(streamSql, snapshotAfterRescale.id());\n         // stop job\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\nindex cb323542d4c1..efe948d4ecae 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/UnawareBucketAppendOnlyTableITCase.java\n@@ -27,6 +27,7 @@\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.FileStoreTableFactory;\n import org.apache.paimon.utils.FailingFileIO;\n+import org.apache.paimon.utils.TimeUtils;\n \n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n@@ -49,7 +50,6 @@\n import java.util.List;\n import java.util.Random;\n \n-import static org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n@@ -202,7 +202,11 @@ public void testCompactionInStreamingMode() throws Exception {\n         batchSql(\""ALTER TABLE append_table SET ('compaction.early-max.file-num' = '4')\"");\n         batchSql(\""ALTER TABLE append_table SET ('continuous.discovery-interval' = '1 s')\"");\n \n-        sEnv.getConfig().getConfiguration().set(CHECKPOINTING_INTERVAL, Duration.ofMillis(500));\n+        sEnv.getConfig()\n+                .getConfiguration()\n+                .setString(\n+                        \""execution.checkpointing.interval\"",\n+                        TimeUtils.formatWithHighestUnit(Duration.ofMillis(500)));\n         sEnv.executeSql(\n                 \""CREATE TEMPORARY TABLE Orders_in (\\n\""\n                         + \""    f0        INT,\\n\""\n@@ -223,7 +227,11 @@ public void testCompactionInStreamingModeWithMaxWatermark() throws Exception {\n         batchSql(\""ALTER TABLE append_table SET ('compaction.early-max.file-num' = '4')\"");\n         batchSql(\""ALTER TABLE append_table SET ('continuous.discovery-interval' = '1 s')\"");\n \n-        sEnv.getConfig().getConfiguration().set(CHECKPOINTING_INTERVAL, Duration.ofMillis(500));\n+        sEnv.getConfig()\n+                .getConfiguration()\n+                .setString(\n+                        \""execution.checkpointing.interval\"",\n+                        TimeUtils.formatWithHighestUnit(Duration.ofMillis(500)));\n         sEnv.executeSql(\n                 \""CREATE TEMPORARY TABLE Orders_in (\\n\""\n                         + \""    f0        INT,\\n\""\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java\nindex 6b912d2e57fe..b1486deacb0c 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/SinkSavepointITCase.java\n@@ -137,7 +137,7 @@ private JobClient runRecoverFromSavepointJob(String failingPath, String savepoin\n                         .parallelism(1)\n                         .allowRestart()\n                         .setConf(conf)\n-                        .setConf(StateBackendOptions.STATE_BACKEND, \""filesystem\"")\n+                        .setConf(StateBackendOptions.STATE_BACKEND, \""hashmap\"")\n                         .setConf(\n                                 CheckpointingOptions.CHECKPOINTS_DIRECTORY,\n                                 \""file://\"" + path + \""/checkpoint\"")\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java\nindex ce0017eb1874..ee838ed68255 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/AbstractTestBase.java\n@@ -19,6 +19,7 @@\n package org.apache.paimon.flink.util;\n \n import org.apache.paimon.utils.FileIOUtils;\n+import org.apache.paimon.utils.TimeUtils;\n \n import org.apache.flink.api.common.RuntimeExecutionMode;\n import org.apache.flink.api.dag.Transformation;\n@@ -29,7 +30,6 @@\n import org.apache.flink.runtime.client.JobStatusMessage;\n import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n import org.apache.flink.streaming.api.CheckpointingMode;\n-import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n import org.apache.flink.table.api.TableEnvironment;\n@@ -164,6 +164,11 @@ public <T> TableEnvironmentBuilder setConf(ConfigOption<T> option, T value) {\n             return this;\n         }\n \n+        public TableEnvironmentBuilder setString(String key, String value) {\n+            conf.setString(key, value);\n+            return this;\n+        }\n+\n         public TableEnvironmentBuilder setConf(Configuration conf) {\n             this.conf.addAll(conf);\n             return this;\n@@ -182,9 +187,10 @@ public TableEnvironment build() {\n                 if (checkpointIntervalMs != null) {\n                     tEnv.getConfig()\n                             .getConfiguration()\n-                            .set(\n-                                    ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,\n-                                    Duration.ofMillis(checkpointIntervalMs));\n+                            .setString(\n+                                    \""execution.checkpointing.interval\"",\n+                                    TimeUtils.formatWithHighestUnit(\n+                                            Duration.ofMillis(checkpointIntervalMs)));\n                 }\n             } else {\n                 tEnv =\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\nindex 86b0014eb39c..9c3170f9a96b 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/util/ReadWriteTableTestUtil.java\n@@ -23,8 +23,9 @@\n import org.apache.paimon.utils.BlockingIterator;\n \n import org.apache.flink.api.common.RuntimeExecutionMode;\n-import org.apache.flink.api.common.restartstrategy.RestartStrategies;\n import org.apache.flink.api.common.time.Time;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.RestartStrategyOptions;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.api.EnvironmentSettings;\n import org.apache.flink.table.api.TableEnvironment;\n@@ -75,12 +76,11 @@ public static void init(String warehouse) {\n     }\n \n     public static void init(String warehouse, int parallelism) {\n-        StreamExecutionEnvironment sExeEnv = buildStreamEnv(parallelism);\n-        sExeEnv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n+        // Using `none` to avoid compatibility issues with Flink 1.18-.\n+        StreamExecutionEnvironment sExeEnv = buildStreamEnv(parallelism, \""none\"");\n         sEnv = StreamTableEnvironment.create(sExeEnv);\n \n-        bExeEnv = buildBatchEnv(parallelism);\n-        bExeEnv.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n+        bExeEnv = buildBatchEnv(parallelism, \""none\"");\n         bEnv = StreamTableEnvironment.create(bExeEnv, EnvironmentSettings.inBatchMode());\n \n         ReadWriteTableTestUtil.warehouse = warehouse;\n@@ -95,16 +95,24 @@ public static void init(String warehouse, int parallelism) {\n         bEnv.useCatalog(catalog);\n     }\n \n-    public static StreamExecutionEnvironment buildStreamEnv(int parallelism) {\n-        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    public static StreamExecutionEnvironment buildStreamEnv(\n+            int parallelism, String restartStrategy) {\n+        Configuration configuration = new Configuration();\n+        configuration.set(RestartStrategyOptions.RESTART_STRATEGY, restartStrategy);\n+        final StreamExecutionEnvironment env =\n+                StreamExecutionEnvironment.getExecutionEnvironment(configuration);\n         env.setRuntimeMode(RuntimeExecutionMode.STREAMING);\n         env.enableCheckpointing(100);\n         env.setParallelism(parallelism);\n         return env;\n     }\n \n-    public static StreamExecutionEnvironment buildBatchEnv(int parallelism) {\n-        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    public static StreamExecutionEnvironment buildBatchEnv(\n+            int parallelism, String restartStrategy) {\n+        Configuration configuration = new Configuration();\n+        configuration.set(RestartStrategyOptions.RESTART_STRATEGY, restartStrategy);\n+        final StreamExecutionEnvironment env =\n+                StreamExecutionEnvironment.getExecutionEnvironment(configuration);\n         env.setRuntimeMode(RuntimeExecutionMode.BATCH);\n         env.setParallelism(parallelism);\n         return env;\n\ndiff --git a/paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java b/paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java\nindex 74d2d7e1c343..2266a8484d9d 100644\n--- a/paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java\n+++ b/paimon-hive/paimon-hive-connector-common/src/test/java/org/apache/paimon/hive/HiveCatalogITCaseBase.java\n@@ -31,12 +31,12 @@\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.utils.IOUtils;\n+import org.apache.paimon.utils.TimeUtils;\n \n import com.klarna.hiverunner.HiveShell;\n import com.klarna.hiverunner.annotations.HiveSQL;\n import org.apache.flink.core.fs.FSDataInputStream;\n import org.apache.flink.core.fs.Path;\n-import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n import org.apache.flink.table.api.EnvironmentSettings;\n import org.apache.flink.table.api.TableEnvironment;\n import org.apache.flink.table.api.TableResult;\n@@ -139,7 +139,9 @@ private void registerHiveCatalog(String catalogName, Map<String, String> catalog\n                         EnvironmentSettings.newInstance().inStreamingMode().build());\n         sEnv.getConfig()\n                 .getConfiguration()\n-                .set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(1));\n+                .setString(\n+                        \""execution.checkpointing.interval\"",\n+                        TimeUtils.formatWithHighestUnit(Duration.ofSeconds(1)));\n         sEnv.getConfig().set(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM, 1);\n \n         tEnv.executeSql(\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4516"", ""pr_id"": 4516, ""issue_id"": 4515, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Bug] unaware bucket mode does not support multiple writers writing to the same kafka partition when using kafka as logSystem.\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Paimon version\n\npaimon-1.0-snapshot\n\n### Compute Engine\n\nflink-1.17\n\n### Minimal reproduce step\n\n1. create an append-only table and use kafka as the logsystem.\r\n2. use flink to write to the table and set the write parallelism to greater than 1.\n\n### What doesn't meet your expectations?\n\n```\r\nCaused by: java.lang.RuntimeException: bucket-0 appears multiple times, which is not possible.\r\n\tat org.apache.paimon.manifest.ManifestCommittable.addLogOffset(ManifestCommittable.java:68)\r\n\tat org.apache.paimon.flink.sink.StoreCommitter.combine(StoreCommitter.java:97)\r\n\tat org.apache.paimon.flink.sink.StoreCommitter.combine(StoreCommitter.java:79)\r\n\tat org.apache.paimon.flink.sink.StoreCommitter.combine(StoreCommitter.java:42)\r\n```\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] I'm willing to submit a PR!"", ""issue_word_count"": 164, ""test_files_count"": 3, ""non_test_files_count"": 3, ""pr_changed_files"": [""paimon-core/src/main/java/org/apache/paimon/manifest/ManifestCommittable.java"", ""paimon-core/src/test/java/org/apache/paimon/TestFileStore.java"", ""paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCommitter.java"", ""paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/TestFileStore.java"", ""paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java"", ""paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java""], ""base_commit"": ""2b94a33bc1dc841476b8970d9a0bb69a7e85502e"", ""head_commit"": ""3483d5ad8e074b049e5bca4cf99a337cfca53687"", ""repo_url"": ""https://github.com/apache/paimon/pull/4516"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4516"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-13T09:04:39.000Z"", ""patch"": ""diff --git a/paimon-core/src/main/java/org/apache/paimon/manifest/ManifestCommittable.java b/paimon-core/src/main/java/org/apache/paimon/manifest/ManifestCommittable.java\nindex 61c4619bd6d6..b4abd0e9ec0e 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/manifest/ManifestCommittable.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/manifest/ManifestCommittable.java\n@@ -62,13 +62,14 @@ public void addFileCommittable(CommitMessage commitMessage) {\n         commitMessages.add(commitMessage);\n     }\n \n-    public void addLogOffset(int bucket, long offset) {\n-        if (logOffsets.containsKey(bucket)) {\n+    public void addLogOffset(int bucket, long offset, boolean allowDuplicate) {\n+        if (!allowDuplicate && logOffsets.containsKey(bucket)) {\n             throw new RuntimeException(\n                     String.format(\n                             \""bucket-%d appears multiple times, which is not possible.\"", bucket));\n         }\n-        logOffsets.put(bucket, offset);\n+        long newOffset = Math.max(logOffsets.getOrDefault(bucket, offset), offset);\n+        logOffsets.put(bucket, newOffset);\n     }\n \n     public long identifier() {\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCommitter.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCommitter.java\nindex d237f4da56cf..4908b99317ba 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCommitter.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreCommitter.java\n@@ -23,6 +23,7 @@\n import org.apache.paimon.flink.sink.partition.PartitionListeners;\n import org.apache.paimon.io.DataFileMeta;\n import org.apache.paimon.manifest.ManifestCommittable;\n+import org.apache.paimon.table.BucketMode;\n import org.apache.paimon.table.FileStoreTable;\n import org.apache.paimon.table.sink.CommitMessage;\n import org.apache.paimon.table.sink.CommitMessageImpl;\n@@ -44,6 +45,7 @@ public class StoreCommitter implements Committer<Committable, ManifestCommittabl\n     private final TableCommitImpl commit;\n     @Nullable private final CommitterMetrics committerMetrics;\n     private final PartitionListeners partitionListeners;\n+    private final boolean allowLogOffsetDuplicate;\n \n     public StoreCommitter(FileStoreTable table, TableCommit commit, Context context) {\n         this.commit = (TableCommitImpl) commit;\n@@ -60,6 +62,7 @@ public StoreCommitter(FileStoreTable table, TableCommit commit, Context context)\n         } catch (Exception e) {\n             throw new RuntimeException(e);\n         }\n+        allowLogOffsetDuplicate = table.bucketMode() == BucketMode.BUCKET_UNAWARE;\n     }\n \n     @VisibleForTesting\n@@ -94,7 +97,8 @@ public ManifestCommittable combine(\n                 case LOG_OFFSET:\n                     LogOffsetCommittable offset =\n                             (LogOffsetCommittable) committable.wrappedCommittable();\n-                    manifestCommittable.addLogOffset(offset.bucket(), offset.offset());\n+                    manifestCommittable.addLogOffset(\n+                            offset.bucket(), offset.offset(), allowLogOffsetDuplicate);\n                     break;\n             }\n         }\n@@ -138,6 +142,10 @@ public void close() throws Exception {\n         partitionListeners.close();\n     }\n \n+    public boolean allowLogOffsetDuplicate() {\n+        return allowLogOffsetDuplicate;\n+    }\n+\n     private void calcNumBytesAndRecordsOut(List<ManifestCommittable> committables) {\n         if (committerMetrics == null) {\n             return;\n\ndiff --git a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\nindex aeb3e1857b9b..537a98f97fb0 100644\n--- a/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\n+++ b/paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/sink/StoreMultiCommitter.java\n@@ -92,11 +92,11 @@ public WrappedManifestCommittable combine(\n             WrappedManifestCommittable wrappedManifestCommittable,\n             List<MultiTableCommittable> committables) {\n         for (MultiTableCommittable committable : committables) {\n+            Identifier identifier =\n+                    Identifier.create(committable.getDatabase(), committable.getTable());\n             ManifestCommittable manifestCommittable =\n                     wrappedManifestCommittable.computeCommittableIfAbsent(\n-                            Identifier.create(committable.getDatabase(), committable.getTable()),\n-                            checkpointId,\n-                            watermark);\n+                            identifier, checkpointId, watermark);\n \n             switch (committable.kind()) {\n                 case FILE:\n@@ -106,7 +106,9 @@ public WrappedManifestCommittable combine(\n                 case LOG_OFFSET:\n                     LogOffsetCommittable offset =\n                             (LogOffsetCommittable) committable.wrappedCommittable();\n-                    manifestCommittable.addLogOffset(offset.bucket(), offset.offset());\n+                    StoreCommitter committer = tableCommitters.get(identifier);\n+                    manifestCommittable.addLogOffset(\n+                            offset.bucket(), offset.offset(), committer.allowLogOffsetDuplicate());\n                     break;\n             }\n         }\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/TestFileStore.java b/paimon-core/src/test/java/org/apache/paimon/TestFileStore.java\nindex 303879337780..5218a515a337 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/TestFileStore.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/TestFileStore.java\n@@ -222,7 +222,8 @@ public List<Snapshot> commitData(\n                 null,\n                 Collections.emptyList(),\n                 (commit, committable) -> {\n-                    logOffsets.forEach(committable::addLogOffset);\n+                    logOffsets.forEach(\n+                            (bucket, offset) -> committable.addLogOffset(bucket, offset, false));\n                     commit.commit(committable, Collections.emptyMap());\n                 });\n     }\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java b/paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java\nindex c179a2c0a789..8de8309bc8fb 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/manifest/ManifestCommittableSerializerTest.java\n@@ -83,7 +83,7 @@ private static void addFileCommittables(\n \n         if (!committable.logOffsets().containsKey(bucket)) {\n             int offset = ID.incrementAndGet();\n-            committable.addLogOffset(bucket, offset);\n+            committable.addLogOffset(bucket, offset, false);\n             assertThat(committable.logOffsets().get(bucket)).isEqualTo(offset);\n         }\n     }\n\ndiff --git a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java\nindex 298f3155ba34..b0aa76f157ac 100644\n--- a/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java\n+++ b/paimon-flink/paimon-flink-common/src/test/java/org/apache/paimon/flink/sink/WrappedManifestCommittableSerializerTest.java\n@@ -98,7 +98,7 @@ public static void addFileCommittables(\n \n         if (!committable.logOffsets().containsKey(bucket)) {\n             int offset = ID.incrementAndGet();\n-            committable.addLogOffset(bucket, offset);\n+            committable.addLogOffset(bucket, offset, false);\n             assertThat(committable.logOffsets().get(bucket)).isEqualTo(offset);\n         }\n     }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"
"{""instance_id"": ""apache__paimon-4427"", ""pr_id"": 4427, ""issue_id"": 4205, ""repo"": ""apache/paimon"", ""problem_statement"": ""[Feature] In paimon catalog, add partition  query and cache\n### Search before asking\n\n- [X] I searched in the [issues](https://github.com/apache/paimon/issues) and found nothing similar.\n\n\n### Motivation\n\nIn the OLAP scenario, it is necessary to obtain the maximum partition of the table.Currently, the partition is obtained through createReader, which takes about 500ms. Therefore, it is necessary to add partition query and cache in paimon catalog\n\n### Solution\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] I'm willing to submit a PR!"", ""issue_word_count"": 89, ""test_files_count"": 2, ""non_test_files_count"": 3, ""pr_changed_files"": [""docs/layouts/shortcodes/generated/catalog_configuration.html"", ""paimon-common/src/main/java/org/apache/paimon/options/CatalogOptions.java"", ""paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java""], ""pr_changed_test_files"": [""paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java"", ""paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java""], ""base_commit"": ""38fc71793b3097378754e537ec367039c96ac7d5"", ""head_commit"": ""76b703a5c26ad596070ec948db0b9a7e2de9486c"", ""repo_url"": ""https://github.com/apache/paimon/pull/4427"", ""swe_url"": ""https://swe-bench-plus.turing.com/repos/apache__paimon/4427"", ""dockerfile"": """", ""pr_merged_at"": ""2024-11-05T05:31:27.000Z"", ""patch"": ""diff --git a/docs/layouts/shortcodes/generated/catalog_configuration.html b/docs/layouts/shortcodes/generated/catalog_configuration.html\nindex b9d74c8124dd..94f11c6c484a 100644\n--- a/docs/layouts/shortcodes/generated/catalog_configuration.html\n+++ b/docs/layouts/shortcodes/generated/catalog_configuration.html\n@@ -44,6 +44,12 @@\n             <td>Duration</td>\n             <td>Controls the duration for which databases and tables in the catalog are cached.</td>\n         </tr>\n+        <tr>\n+            <td><h5>cache.partition.max-num</h5></td>\n+            <td style=\""word-wrap: break-word;\"">0</td>\n+            <td>Long</td>\n+            <td>Controls the max number for which partitions in the catalog are cached.</td>\n+        </tr>\n         <tr>\n             <td><h5>cache.manifest.max-memory</h5></td>\n             <td style=\""word-wrap: break-word;\"">(none)</td>\n\ndiff --git a/paimon-common/src/main/java/org/apache/paimon/options/CatalogOptions.java b/paimon-common/src/main/java/org/apache/paimon/options/CatalogOptions.java\nindex 081668675dd9..0d8a9290a53f 100644\n--- a/paimon-common/src/main/java/org/apache/paimon/options/CatalogOptions.java\n+++ b/paimon-common/src/main/java/org/apache/paimon/options/CatalogOptions.java\n@@ -98,6 +98,13 @@ public class CatalogOptions {\n                     .withDescription(\n                             \""Controls the duration for which databases and tables in the catalog are cached.\"");\n \n+    public static final ConfigOption<Long> CACHE_PARTITION_MAX_NUM =\n+            key(\""cache.partition.max-num\"")\n+                    .longType()\n+                    .defaultValue(0L)\n+                    .withDescription(\n+                            \""Controls the max number for which partitions in the catalog are cached.\"");\n+\n     public static final ConfigOption<MemorySize> CACHE_MANIFEST_SMALL_FILE_MEMORY =\n             key(\""cache.manifest.small-file-memory\"")\n                     .memoryType()\n\ndiff --git a/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java b/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\nindex 0777759456f8..444a828af43e 100644\n--- a/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\n+++ b/paimon-core/src/main/java/org/apache/paimon/catalog/CachingCatalog.java\n@@ -19,6 +19,7 @@\n package org.apache.paimon.catalog;\n \n import org.apache.paimon.fs.Path;\n+import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.MemorySize;\n import org.apache.paimon.options.Options;\n import org.apache.paimon.schema.SchemaChange;\n@@ -52,6 +53,7 @@\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_MAX_MEMORY;\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_SMALL_FILE_MEMORY;\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_SMALL_FILE_THRESHOLD;\n+import static org.apache.paimon.options.CatalogOptions.CACHE_PARTITION_MAX_NUM;\n import static org.apache.paimon.table.system.SystemTableLoader.SYSTEM_TABLES;\n \n /** A {@link Catalog} to cache databases and tables and manifests. */\n@@ -61,26 +63,31 @@ public class CachingCatalog extends DelegateCatalog {\n \n     protected final Cache<String, Map<String, String>> databaseCache;\n     protected final Cache<Identifier, Table> tableCache;\n+    @Nullable protected final Cache<Identifier, List<PartitionEntry>> partitionCache;\n     @Nullable protected final SegmentsCache<Path> manifestCache;\n+    private final long cachedPartitionMaxNum;\n \n     public CachingCatalog(Catalog wrapped) {\n         this(\n                 wrapped,\n                 CACHE_EXPIRATION_INTERVAL_MS.defaultValue(),\n                 CACHE_MANIFEST_SMALL_FILE_MEMORY.defaultValue(),\n-                CACHE_MANIFEST_SMALL_FILE_THRESHOLD.defaultValue().getBytes());\n+                CACHE_MANIFEST_SMALL_FILE_THRESHOLD.defaultValue().getBytes(),\n+                CACHE_PARTITION_MAX_NUM.defaultValue());\n     }\n \n     public CachingCatalog(\n             Catalog wrapped,\n             Duration expirationInterval,\n             MemorySize manifestMaxMemory,\n-            long manifestCacheThreshold) {\n+            long manifestCacheThreshold,\n+            long cachedPartitionMaxNum) {\n         this(\n                 wrapped,\n                 expirationInterval,\n                 manifestMaxMemory,\n                 manifestCacheThreshold,\n+                cachedPartitionMaxNum,\n                 Ticker.systemTicker());\n     }\n \n@@ -89,6 +96,7 @@ public CachingCatalog(\n             Duration expirationInterval,\n             MemorySize manifestMaxMemory,\n             long manifestCacheThreshold,\n+            long cachedPartitionMaxNum,\n             Ticker ticker) {\n         super(wrapped);\n         if (expirationInterval.isZero() || expirationInterval.isNegative()) {\n@@ -111,7 +119,19 @@ public CachingCatalog(\n                         .expireAfterAccess(expirationInterval)\n                         .ticker(ticker)\n                         .build();\n+        this.partitionCache =\n+                cachedPartitionMaxNum == 0\n+                        ? null\n+                        : Caffeine.newBuilder()\n+                                .softValues()\n+                                .executor(Runnable::run)\n+                                .expireAfterAccess(expirationInterval)\n+                                .weigher(this::weigh)\n+                                .maximumWeight(cachedPartitionMaxNum)\n+                                .ticker(ticker)\n+                                .build();\n         this.manifestCache = SegmentsCache.create(manifestMaxMemory, manifestCacheThreshold);\n+        this.cachedPartitionMaxNum = cachedPartitionMaxNum;\n     }\n \n     public static Catalog tryToCreate(Catalog catalog, Options options) {\n@@ -131,7 +151,8 @@ public static Catalog tryToCreate(Catalog catalog, Options options) {\n                 catalog,\n                 options.get(CACHE_EXPIRATION_INTERVAL_MS),\n                 manifestMaxMemory,\n-                manifestThreshold);\n+                manifestThreshold,\n+                options.get(CACHE_PARTITION_MAX_NUM));\n     }\n \n     @Override\n@@ -227,6 +248,51 @@ private void putTableCache(Identifier identifier, Table table) {\n         tableCache.put(identifier, table);\n     }\n \n+    public List<PartitionEntry> getPartitions(Identifier identifier) throws TableNotExistException {\n+        Table table = this.getTable(identifier);\n+        if (partitionCacheEnabled(table)) {\n+            List<PartitionEntry> partitions;\n+            partitions = partitionCache.getIfPresent(identifier);\n+            if (partitions == null || partitions.isEmpty()) {\n+                partitions = this.refreshPartitions(identifier);\n+            }\n+            return partitions;\n+        }\n+        return ((FileStoreTable) table).newSnapshotReader().partitionEntries();\n+    }\n+\n+    public List<PartitionEntry> refreshPartitions(Identifier identifier)\n+            throws TableNotExistException {\n+        Table table = this.getTable(identifier);\n+        List<PartitionEntry> partitions =\n+                ((FileStoreTable) table).newSnapshotReader().partitionEntries();\n+        if (partitionCacheEnabled(table)\n+                && partitionCache.asMap().values().stream().mapToInt(List::size).sum()\n+                        < this.cachedPartitionMaxNum) {\n+            partitionCache.put(identifier, partitions);\n+        }\n+        return partitions;\n+    }\n+\n+    private boolean partitionCacheEnabled(Table table) {\n+        return partitionCache != null\n+                && table instanceof FileStoreTable\n+                && !table.partitionKeys().isEmpty();\n+    }\n+\n+    private int weigh(Identifier identifier, List<PartitionEntry> partitions) {\n+        return partitions.size();\n+    }\n+\n+    @Override\n+    public void dropPartition(Identifier identifier, Map<String, String> partitions)\n+            throws TableNotExistException, PartitionNotExistException {\n+        wrapped.dropPartition(identifier, partitions);\n+        if (partitionCache != null) {\n+            partitionCache.invalidate(identifier);\n+        }\n+    }\n+\n     private class TableInvalidatingRemovalListener implements RemovalListener<Identifier, Table> {\n         @Override\n         public void onRemoval(Identifier identifier, Table table, @NonNull RemovalCause cause) {\n"", ""test_patch"": ""diff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java b/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\nindex d1f7eeb8a56d..d645c46bf656 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/CachingCatalogTest.java\n@@ -20,8 +20,10 @@\n \n import org.apache.paimon.data.GenericRow;\n import org.apache.paimon.fs.Path;\n+import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.MemorySize;\n import org.apache.paimon.options.Options;\n+import org.apache.paimon.schema.Schema;\n import org.apache.paimon.schema.SchemaChange;\n import org.apache.paimon.table.Table;\n import org.apache.paimon.table.sink.BatchTableCommit;\n@@ -32,6 +34,8 @@\n import org.apache.paimon.table.source.TableScan;\n import org.apache.paimon.table.system.SystemTableLoader;\n import org.apache.paimon.types.DataTypes;\n+import org.apache.paimon.types.RowType;\n+import org.apache.paimon.types.VarCharType;\n import org.apache.paimon.utils.FakeTicker;\n \n import org.apache.paimon.shade.caffeine2.com.github.benmanes.caffeine.cache.Cache;\n@@ -44,6 +48,7 @@\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Optional;\n import java.util.concurrent.ExecutorService;\n@@ -51,6 +56,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n \n+import static java.util.Collections.emptyList;\n+import static java.util.Collections.singletonList;\n import static org.apache.paimon.data.BinaryString.fromString;\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_MAX_MEMORY;\n import static org.apache.paimon.options.CatalogOptions.CACHE_MANIFEST_SMALL_FILE_MEMORY;\n@@ -113,15 +120,15 @@ public void testTableExpiresAfterInterval() throws Exception {\n         Table table = catalog.getTable(tableIdent);\n \n         // Ensure table is cached with full ttl remaining upon creation\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.remainingAgeFor(tableIdent)).isPresent().get().isEqualTo(EXPIRATION_TTL);\n \n         ticker.advance(HALF_OF_EXPIRATION);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent)).isPresent().get().isEqualTo(HALF_OF_EXPIRATION);\n \n         ticker.advance(HALF_OF_EXPIRATION.plus(Duration.ofSeconds(10)));\n-        assertThat(catalog.cache().asMap()).doesNotContainKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).doesNotContainKey(tableIdent);\n         assertThat(catalog.getTable(tableIdent))\n                 .as(\""CachingCatalog should return a new instance after expiration\"")\n                 .isNotSameAs(table);\n@@ -135,11 +142,11 @@ public void testCatalogExpirationTtlRefreshesAfterAccessViaCatalog() throws Exce\n         Identifier tableIdent = new Identifier(\""db\"", \""tbl\"");\n         catalog.createTable(tableIdent, DEFAULT_TABLE_SCHEMA, false);\n         catalog.getTable(tableIdent);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent)).isPresent().get().isEqualTo(Duration.ZERO);\n \n         ticker.advance(HALF_OF_EXPIRATION);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent)).isPresent().get().isEqualTo(HALF_OF_EXPIRATION);\n         assertThat(catalog.remainingAgeFor(tableIdent))\n                 .isPresent()\n@@ -148,7 +155,7 @@ public void testCatalogExpirationTtlRefreshesAfterAccessViaCatalog() throws Exce\n \n         Duration oneMinute = Duration.ofMinutes(1L);\n         ticker.advance(oneMinute);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent))\n                 .isPresent()\n                 .get()\n@@ -175,17 +182,17 @@ public void testCacheExpirationEagerlyRemovesSysTables() throws Exception {\n         Identifier tableIdent = new Identifier(\""db\"", \""tbl\"");\n         catalog.createTable(tableIdent, DEFAULT_TABLE_SCHEMA, false);\n         Table table = catalog.getTable(tableIdent);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent)).get().isEqualTo(Duration.ZERO);\n \n         ticker.advance(HALF_OF_EXPIRATION);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         assertThat(catalog.ageOf(tableIdent)).get().isEqualTo(HALF_OF_EXPIRATION);\n \n         for (Identifier sysTable : sysTables(tableIdent)) {\n             catalog.getTable(sysTable);\n         }\n-        assertThat(catalog.cache().asMap()).containsKeys(sysTables(tableIdent));\n+        assertThat(catalog.tableCache().asMap()).containsKeys(sysTables(tableIdent));\n         assertThat(Arrays.stream(sysTables(tableIdent)).map(catalog::ageOf))\n                 .isNotEmpty()\n                 .allMatch(age -> age.isPresent() && age.get().equals(Duration.ZERO));\n@@ -209,17 +216,39 @@ public void testCacheExpirationEagerlyRemovesSysTables() throws Exception {\n \n         // Move time forward so the data table drops.\n         ticker.advance(HALF_OF_EXPIRATION);\n-        assertThat(catalog.cache().asMap()).doesNotContainKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).doesNotContainKey(tableIdent);\n \n         Arrays.stream(sysTables(tableIdent))\n                 .forEach(\n                         sysTable ->\n-                                assertThat(catalog.cache().asMap())\n+                                assertThat(catalog.tableCache().asMap())\n                                         .as(\n                                                 \""When a data table expires, its sys tables should expire regardless of age\"")\n                                         .doesNotContainKeys(sysTable));\n     }\n \n+    @Test\n+    public void testPartitionCache() throws Exception {\n+        TestableCachingCatalog catalog =\n+                new TestableCachingCatalog(this.catalog, EXPIRATION_TTL, ticker);\n+\n+        Identifier tableIdent = new Identifier(\""db\"", \""tbl\"");\n+        Schema schema =\n+                new Schema(\n+                        RowType.of(VarCharType.STRING_TYPE, VarCharType.STRING_TYPE).getFields(),\n+                        singletonList(\""f0\""),\n+                        emptyList(),\n+                        Collections.emptyMap(),\n+                        \""\"");\n+        catalog.createTable(tableIdent, schema, false);\n+        List<PartitionEntry> partitionEntryList = catalog.getPartitions(tableIdent);\n+        assertThat(catalog.partitionCache().asMap()).containsKey(tableIdent);\n+        List<PartitionEntry> partitionEntryListFromCache =\n+                catalog.partitionCache().getIfPresent(tableIdent);\n+        assertThat(partitionEntryListFromCache).isNotNull();\n+        assertThat(partitionEntryListFromCache).containsAll(partitionEntryList);\n+    }\n+\n     @Test\n     public void testDeadlock() throws Exception {\n         Catalog underlyCatalog = this.catalog;\n@@ -233,7 +262,7 @@ public void testDeadlock() throws Exception {\n             createdTables.add(tableIdent);\n         }\n \n-        Cache<Identifier, Table> cache = catalog.cache();\n+        Cache<Identifier, Table> cache = catalog.tableCache();\n         AtomicInteger cacheGetCount = new AtomicInteger(0);\n         AtomicInteger cacheCleanupCount = new AtomicInteger(0);\n         ExecutorService executor = Executors.newFixedThreadPool(numThreads);\n@@ -288,10 +317,10 @@ public void testInvalidateTableForChainedCachingCatalogs() throws Exception {\n         Identifier tableIdent = new Identifier(\""db\"", \""tbl\"");\n         catalog.createTable(tableIdent, DEFAULT_TABLE_SCHEMA, false);\n         catalog.getTable(tableIdent);\n-        assertThat(catalog.cache().asMap()).containsKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).containsKey(tableIdent);\n         catalog.dropTable(tableIdent, false);\n-        assertThat(catalog.cache().asMap()).doesNotContainKey(tableIdent);\n-        assertThat(wrappedCatalog.cache().asMap()).doesNotContainKey(tableIdent);\n+        assertThat(catalog.tableCache().asMap()).doesNotContainKey(tableIdent);\n+        assertThat(wrappedCatalog.tableCache().asMap()).doesNotContainKey(tableIdent);\n     }\n \n     public static Identifier[] sysTables(Identifier tableIdent) {\n@@ -313,7 +342,8 @@ private void innerTestManifestCache(long manifestCacheThreshold) throws Exceptio\n                         this.catalog,\n                         Duration.ofSeconds(10),\n                         MemorySize.ofMebiBytes(1),\n-                        manifestCacheThreshold);\n+                        manifestCacheThreshold,\n+                        0L);\n         Identifier tableIdent = new Identifier(\""db\"", \""tbl\"");\n         catalog.dropTable(tableIdent, true);\n         catalog.createTable(tableIdent, DEFAULT_TABLE_SCHEMA, false);\n\ndiff --git a/paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java b/paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java\nindex 159f5edaef1f..4c70a0232c44 100644\n--- a/paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java\n+++ b/paimon-core/src/test/java/org/apache/paimon/catalog/TestableCachingCatalog.java\n@@ -18,6 +18,7 @@\n \n package org.apache.paimon.catalog;\n \n+import org.apache.paimon.manifest.PartitionEntry;\n import org.apache.paimon.options.MemorySize;\n import org.apache.paimon.table.Table;\n \n@@ -25,6 +26,7 @@\n import org.apache.paimon.shade.caffeine2.com.github.benmanes.caffeine.cache.Ticker;\n \n import java.time.Duration;\n+import java.util.List;\n import java.util.Optional;\n \n /**\n@@ -36,18 +38,22 @@ public class TestableCachingCatalog extends CachingCatalog {\n     private final Duration cacheExpirationInterval;\n \n     public TestableCachingCatalog(Catalog catalog, Duration expirationInterval, Ticker ticker) {\n-        super(catalog, expirationInterval, MemorySize.ZERO, Long.MAX_VALUE, ticker);\n+        super(catalog, expirationInterval, MemorySize.ZERO, Long.MAX_VALUE, Long.MAX_VALUE, ticker);\n         this.cacheExpirationInterval = expirationInterval;\n     }\n \n-    public Cache<Identifier, Table> cache() {\n+    public Cache<Identifier, Table> tableCache() {\n         // cleanUp must be called as tests apply assertions directly on the underlying map, but\n-        // metadata\n-        // table map entries are cleaned up asynchronously.\n+        // metadata table map entries are cleaned up asynchronously.\n         tableCache.cleanUp();\n         return tableCache;\n     }\n \n+    public Cache<Identifier, List<PartitionEntry>> partitionCache() {\n+        partitionCache.cleanUp();\n+        return partitionCache;\n+    }\n+\n     public Optional<Duration> ageOf(Identifier identifier) {\n         return tableCache.policy().expireAfterAccess().get().ageOf(identifier);\n     }\n"", ""agent_patch"": null, ""FAIL_TO_PASS"": [], ""PASS_TO_PASS"": [], ""test_output_before"": null, ""errors_before"": [], ""failed_before"": [], ""test_output_after"": null, ""errors_after"": [], ""failed_after"": []}"

[
  {
    "instance_id": "prestodb__presto-25445",
    "pr_id": 25445,
    "issue_id": 22690,
    "repo": "prestodb/presto",
    "problem_statement": "Negative array subscript being pushdown as subfield pruning and worker is not checking against it\nWe are pushing down negative array subscript as subfield pruning (https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java) and worker is not sanity checking against it (https://github.com/prestodb/presto/blob/master/presto-orc/src/main/java/com/facebook/presto/orc/reader/ListSelectiveStreamReader.java\r\n).  With -1 pushdown this happens to work because `ELEMENT_LENGTH_UNBOUNDED = -1` but -2 is resulting in `GENERIC_INTERNAL_ERROR`:\r\n```\r\njava.lang.IllegalArgumentException: Offset is not monotonically ascending. offsets[0]=0, offsets[1]=-2\r\n\tat com.facebook.presto.common.block.ArrayBlock.fromElementBlock(ArrayBlock.java:58)\r\n\tat com.facebook.presto.orc.reader.ListSelectiveStreamReader.getBlock(ListSelectiveStreamReader.java:530)\r\n\tat com.facebook.presto.orc.OrcSelectiveRecordReader$OrcBlockLoader.load(OrcSelectiveRecordReader.java:941)\r\n\tat com.facebook.presto.orc.OrcSelectiveRecordReader$OrcBlockLoader.load(OrcSelectiveRecordReader.java:900)\r\n\tat com.facebook.presto.common.block.LazyBlock.assureLoaded(LazyBlock.java:313)\r\n\tat com.facebook.presto.common.block.LazyBlock.getLoadedBlock(LazyBlock.java:304)\r\n\tat com.facebook.presto.operator.ScanFilterAndProjectOperator$RecordingLazyBlockLoader.load(ScanFilterAndProjectOperator.java:335)\r\n\tat com.facebook.presto.operator.ScanFilterAndProjectOperator$RecordingLazyBlockLoader.load(ScanFilterAndProjectOperator.java:321)\r\n\tat com.facebook.presto.common.block.LazyBlock.assureLoaded(LazyBlock.java:313)\r\n\tat com.facebook.presto.common.block.LazyBlock.getLoadedBlock(LazyBlock.java:304)\r\n\tat com.facebook.presto.operator.project.DictionaryAwarePageProjection$DictionaryAwarePageProjectionWork.<init>(DictionaryAwarePageProjection.java:92)\r\n\tat com.facebook.presto.operator.project.DictionaryAwarePageProjection.project(DictionaryAwarePageProjection.java:70)\r\n\tat com.facebook.presto.operator.project.PageProjectionWithOutputs.project(PageProjectionWithOutputs.java:56)\r\n\tat com.facebook.presto.operator.project.PageProcessor$ProjectSelectedPositions.processBatch(PageProcessor.java:327)\r\n\tat com.facebook.presto.operator.project.PageProcessor$ProjectSelectedPositions.process(PageProcessor.java:201)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$ProcessWorkProcessor.process(WorkProcessorUtils.java:315)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$YieldingIterator.computeNext(WorkProcessorUtils.java:79)\r\n\tat com.facebook.presto.operator.WorkProcessorUtils$YieldingIterator.computeNext(WorkProcessorUtils.java:65)\r\n\tat com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:141)\r\n\tat com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:136)\r\n\tat com.facebook.presto.operator.project.MergingPageOutput.getOutput(MergingPageOutput.java:128)\r\n\tat com.facebook.presto.operator.ScanFilterAndProjectOperator.processPageSource(ScanFilterAndProjectOperator.java:316)\r\n\tat com.facebook.presto.operator.ScanFilterAndProjectOperator.getOutput(ScanFilterAndProjectOperator.java:260)\r\n\tat com.facebook.presto.operator.Driver.processInternal(Driver.java:441)\r\n\tat com.facebook.presto.operator.Driver.lambda$processFor$10(Driver.java:324)\r\n\tat com.facebook.presto.operator.Driver.tryWithLock(Driver.java:750)\r\n\tat com.facebook.presto.operator.Driver.processFor(Driver.java:317)\r\n\tat com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1079)\r\n\tat com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:165)\r\n\tat com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:621)\r\n\tat com.facebook.presto.$gen.Presto_0_288_edge1_2_SNAPSHOT_$_git_commit_id_abbrev___0_288_edge1_2____20240504_213846_1.run(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n```\r\nWe should stop push down non-positive array subscripts.",
    "issue_word_count": 482,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java"
    ],
    "base_commit": "73c3d4ab4c441c42d6c8fd592be3413c5f1b2f85",
    "head_commit": "7e4a4abc4d03ba73690e4e8c02f91af9eb911f58",
    "repo_url": "https://github.com/prestodb/presto/pull/25445",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25445",
    "dockerfile": "",
    "pr_merged_at": "2025-06-26T21:56:54.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java\nindex c463c8d38f27..f3ff6423807f 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/PushdownSubfields.java\n@@ -638,7 +638,7 @@ private static Optional<List<Subfield>> toSubfield(\n                         if (index instanceof Number) {\n                             //Fix for issue https://github.com/prestodb/presto/issues/22690\n                             //Avoid negative index pushdown\n-                            if (((Number) index).longValue() < 0) {\n+                            if (((Number) index).longValue() < 0 && arguments.get(0).getType() instanceof ArrayType) {\n                                 return Optional.empty();\n                             }\n \n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\nindex 04fbd820fe37..a9d23bac9269 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\n@@ -1462,6 +1462,18 @@ public void testPushdownSubfields()\n         assertUpdate(\"DROP TABLE test_pushdown_struct_subfields\");\n     }\n \n+    @Test\n+    public void testPushdownNegativeSubfiels()\n+    {\n+        assertUpdate(\"CREATE TABLE test_pushdown_subfields_negative_key(id bigint, arr array(bigint), mp map(integer, varchar))\");\n+        assertPushdownSubfields(\"select element_at(arr, -1) from test_pushdown_subfields_negative_key\", \"test_pushdown_subfields_negative_key\", ImmutableMap.of(\"arr\", toSubfields()));\n+        assertPushdownSubfields(\"select element_at(mp, -1) from test_pushdown_subfields_negative_key\", \"test_pushdown_subfields_negative_key\", ImmutableMap.of(\"mp\", toSubfields(\"mp[-1]\")));\n+        assertPushdownSubfields(\"select element_at(arr, -1), element_at(arr, 2) from test_pushdown_subfields_negative_key\", \"test_pushdown_subfields_negative_key\", ImmutableMap.of(\"arr\", toSubfields()));\n+        assertPushdownSubfields(\"select element_at(mp, -1), element_at(mp, 2) from test_pushdown_subfields_negative_key\", \"test_pushdown_subfields_negative_key\", ImmutableMap.of(\"mp\", toSubfields(\"mp[-1]\", \"mp[2]\")));\n+\n+        assertUpdate(\"DROP TABLE test_pushdown_subfields_negative_key\");\n+    }\n+\n     @Test\n     public void testPushdownSubfieldsForMapSubset()\n     {\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java\nindex 5b0844835b8a..1e0f61f2becd 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePushdownFilterQueries.java\n@@ -1163,6 +1163,32 @@ public void testArraySubscriptPushdown()\n         }\n     }\n \n+    @Test\n+    public void testMapSubscriptPushdown()\n+    {\n+        Session session = enablePushdownFilterAndSubfield(getQueryRunner().getDefaultSession());\n+        getQueryRunner().execute(session,\n+                \"CREATE TABLE test_neg_map_sub_pushdown AS \\n\" +\n+                        \"select map(ARRAY[-10,20,30,0], array['a', 'b', 'c', 'd']) numbers\");\n+\n+        try {\n+            assertQuery(\"select element_at(numbers,-10) as number from test_neg_map_sub_pushdown\", \"SELECT 'a'\");\n+            assertQuery(\"select element_at(numbers,20) as number from test_neg_map_sub_pushdown\", \"SELECT 'b'\");\n+            assertQuery(\"select element_at(numbers,30) as number from test_neg_map_sub_pushdown\", \"SELECT 'c'\");\n+            assertQuery(\"select element_at(numbers,0) as number from test_neg_map_sub_pushdown\", \"SELECT 'd'\");\n+            assertQuery(\"select element_at(numbers,40) as number from test_neg_map_sub_pushdown\", \"SELECT cast(NULL as varchar)\");\n+\n+            assertQuery(\"select numbers[-10] as number from test_neg_map_sub_pushdown\", \"SELECT 'a'\");\n+            assertQuery(\"select numbers[20] as number from test_neg_map_sub_pushdown\", \"SELECT 'b'\");\n+            assertQuery(\"select numbers[30] as number from test_neg_map_sub_pushdown\", \"SELECT 'c'\");\n+            assertQuery(\"select numbers[0] as number from test_neg_map_sub_pushdown\", \"SELECT 'd'\");\n+            assertQueryFails(\"select numbers[40] as number from test_neg_map_sub_pushdown\", \"Key not present in map: 40\");\n+        }\n+        finally {\n+            getQueryRunner().execute(\"DROP TABLE test_neg_map_sub_pushdown\");\n+        }\n+    }\n+\n     @Test\n     public void testArraySubscriptPushdownEmptyArray()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25421",
    "pr_id": 25421,
    "issue_id": 25407,
    "repo": "prestodb/presto",
    "problem_statement": "[native] TestPrestoNativeAsyncDataCacheCleanupAPI.testAsyncDataCacheCleanup failing\nCI https://github.com/prestodb/presto/actions/workflows/prestocpp-linux-build-and-unit-test.yml?page=2\n\nSeems to have started here \nhttps://github.com/prestodb/presto/pull/25367",
    "issue_word_count": 33,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java"
    ],
    "base_commit": "8d5e52df798f3f261df66cab6c1cc17e4713942f",
    "head_commit": "4d411c1466663571dda7f595be2ddd85188cfd4b",
    "repo_url": "https://github.com/prestodb/presto/pull/25421",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25421",
    "dockerfile": "",
    "pr_merged_at": "2025-06-24T15:12:12.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java\nindex 8c2b25121512..314144c322fc 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeAsyncDataCacheCleanupAPI.java\n@@ -67,7 +67,7 @@ protected void createTables()\n         createCustomer(queryRunner);\n     }\n \n-    @Test(groups = {\"async_data_cache\"})\n+    @Test(groups = {\"async_data_cache\"}, enabled = false)\n     public void testAsyncDataCacheCleanup() throws Exception\n     {\n         Session session = Session.builder(super.getSession())\n@@ -193,7 +193,7 @@ private Set<InternalNode> getWorkerNodes(DistributedQueryRunner queryRunner)\n                 .collect(Collectors.toSet());\n     }\n \n-    @Test(groups = {\"async_data_cache\"})\n+    @Test(groups = {\"async_data_cache\"}, enabled = false)\n     public void testAsyncDataCacheCleanupApiFormat()\n     {\n         QueryRunner queryRunner = getQueryRunner();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25420",
    "pr_id": 25420,
    "issue_id": 25404,
    "repo": "prestodb/presto",
    "problem_statement": "Presto Fails to Propagate Configs When Using CopyOnFirstWriteConfiguration\n<!--- Provide a general summary of the issue in the Title above -->\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\nThe introduction of `WrapperJobConf` and `CopyOnFirstWriteConfiguration` in [PR #17625](https://github.com/prestodb/presto/pull/17625) introduced several compatibility issues, especially when the configuration property `hive.copy-on-first-write-configuration-enabled` is set to true (which is the default).\n\n`CopyOnFirstWriteConfiguration` wraps the actual Hadoop `Configuration` object, extending it and introducing a layer of indirection. Many third-party libraries that Presto integrates with rely on Hadoop’s `Configuration` copy constructor ([source](https://github.com/apache/hadoop/blob/02a9190af5f8264e25966a80c8f9ea9bb6677899/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L844-L875)). This constructor is unaware of the wrapped configuration inside `CopyOnFirstWriteConfiguration`, leading to silent failures where key configuration properties are not propagated correctly.\n\nA recent example of this problem is described in [issue #25306](https://github.com/prestodb/presto/issues/25306), which was addressed by [PR #25307](https://github.com/prestodb/presto/pull/25307). We have also fixed several other similar bugs in the past.\n\nBecause `CopyOnFirstWriteConfiguration` is enabled by default, users may encounter these issues out-of-the-box, even without any custom configuration. While some of these issues have already been addressed, there are still remaining cases where Presto passes wrapped configuration objects to third-party libraries, which need to be reviewed and fixed.\n\nGiven the scope of the impact, it may be worth considering setting `hive.copy-on-first-write-configuration-enabled=false` as the default until all such compatibility issues are fully resolved.\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used: 0.294-SNAPSHOT\n* Storage (HDFS/S3/GCS..): S3\n* Data source and connector used: Any Lakehouse Connector (Hive, Iceberg, Delta, Hudi)",
    "issue_word_count": 313,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java"
    ],
    "base_commit": "b38dcc0e13c1c8739fe6058cf4236bc11912a096",
    "head_commit": "5203be4497e315f551a174d5738026f93f0908f1",
    "repo_url": "https://github.com/prestodb/presto/pull/25420",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25420",
    "dockerfile": "",
    "pr_merged_at": "2025-06-24T21:42:25.000Z",
    "patch": "diff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java b/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java\nindex 136c1c293e26..dfad3fb97611 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientConfig.java\n@@ -202,7 +202,7 @@ public class HiveClientConfig\n     private Protocol thriftProtocol = Protocol.BINARY;\n     private DataSize thriftBufferSize = new DataSize(128, BYTE);\n \n-    private boolean copyOnFirstWriteConfigurationEnabled = true;\n+    private boolean copyOnFirstWriteConfigurationEnabled;\n \n     private boolean partitionFilteringFromMetastoreEnabled = true;\n \n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java\nindex 2d446e260027..88e0bf914e82 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientConfig.java\n@@ -152,7 +152,7 @@ public void testDefaults()\n                 .setHudiTablesUseMergedView(null)\n                 .setThriftProtocol(Protocol.BINARY)\n                 .setThriftBufferSize(new DataSize(128, BYTE))\n-                .setCopyOnFirstWriteConfigurationEnabled(true)\n+                .setCopyOnFirstWriteConfigurationEnabled(false)\n                 .setPartitionFilteringFromMetastoreEnabled(true)\n                 .setParallelParsingOfPartitionValuesEnabled(false)\n                 .setMaxParallelParsingConcurrency(100)\n@@ -280,7 +280,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"hive.hudi-tables-use-merged-view\", \"default.user\")\n                 .put(\"hive.internal-communication.thrift-transport-protocol\", \"COMPACT\")\n                 .put(\"hive.internal-communication.thrift-transport-buffer-size\", \"256B\")\n-                .put(\"hive.copy-on-first-write-configuration-enabled\", \"false\")\n+                .put(\"hive.copy-on-first-write-configuration-enabled\", \"true\")\n                 .put(\"hive.partition-filtering-from-metastore-enabled\", \"false\")\n                 .put(\"hive.parallel-parsing-of-partition-values-enabled\", \"true\")\n                 .put(\"hive.max-parallel-parsing-concurrency\", \"200\")\n@@ -404,7 +404,7 @@ public void testExplicitPropertyMappings()\n                 .setHudiTablesUseMergedView(\"default.user\")\n                 .setThriftProtocol(Protocol.COMPACT)\n                 .setThriftBufferSize(new DataSize(256, BYTE))\n-                .setCopyOnFirstWriteConfigurationEnabled(false)\n+                .setCopyOnFirstWriteConfigurationEnabled(true)\n                 .setPartitionFilteringFromMetastoreEnabled(false)\n                 .setParallelParsingOfPartitionValuesEnabled(true)\n                 .setMaxParallelParsingConcurrency(200)\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25308",
    "pr_id": 25308,
    "issue_id": 25287,
    "repo": "prestodb/presto",
    "problem_statement": "Thrift protocol generation issues for TimeZoneKey and HostAddress\nWe were looking at the new thrift protocol generation.\n\nWe found at least two issues upon inspection.\n\n1. TimeZoneKey is a struct with 2 members but the manual conversion (serialize/deserialize) does not implement it correctly for it. It only has the key converted to a local value (which had the wrong type).\n\nThe assumption is that when this value is sent by the Java coordinator that contains the value for both members (id and key) it will not be correctly handled. The Java code for TimeZoneKey.java does indicate that there are two fields but it is unclear how they are serialized/deserialized.\n\nIssue is in the manual implementation in `TimeZoneKey.cpp.inc`.\nWe think that the presto_protocol_core.h should define TimeZoneKey as a std::string to handle both members similar to HostAddress. From the Java implementation we can't tell how this would look like serialized (aka what is the separator between id and key).\n\n2. We looked at HostAddress to get an idea where also two members are used for host and port which are a string and an integer.\n\nWe then looked at the implementation for the serialization and deserialization. but the code there didn't make sense in terms of what the separator is. One of the functions splits the value using `:` (in the deserialization) while the serialization uses `.` as the separator. \n\nhttps://github.com/prestodb/presto/blob/b1bddba5666a79422c3d141ffc3a935d293183d4/presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc#L17\nhttps://github.com/prestodb/presto/blob/b1bddba5666a79422c3d141ffc3a935d293183d4/presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc#L25\n\nThat seems incorrect.\n\nIn addition, neither `.` nor `:` should be used as the field separator in the serialized string value because`:` appears in IPv6 addresses while `.` appears in IPv4 addresses in the host string member.\n\nA different separator is needed which also affects the Java side.\n\n<!--- Provide a general summary of the issue in the Title above -->\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used:\n* Storage (HDFS/S3/GCS..):\n* Data source and connector used:\n* Deployment (Cloud or On-prem):\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\n\n## Expected Behavior\n<!--- Tell us what should happen -->\n\n## Current Behavior\n<!--- Tell us what happens instead of the expected behavior -->\n\n## Possible Solution\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\n\n## Steps to Reproduce\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\n1.\n2.\n3.\n4.\n\n## Screenshots (if appropriate)\n\n## Context\n<!--- How has this issue affected you? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\n\n",
    "issue_word_count": 475,
    "test_files_count": 1,
    "non_test_files_count": 6,
    "pr_changed_files": [
      "presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp",
      "presto-native-execution/presto_cpp/main/thrift/ProtocolToThrift.cpp",
      "presto-native-execution/presto_cpp/main/thrift/presto_protocol-to-thrift-json.yml",
      "presto-native-execution/presto_cpp/main/thrift/presto_thrift.thrift",
      "presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc",
      "presto-native-execution/presto_cpp/main/thrift/special/TimeZoneKey.cpp.inc",
      "presto-native-execution/presto_cpp/main/thrift/thrift2json.py"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp"
    ],
    "base_commit": "00a30bd74c717690e2742fcfb7ecb965d015de97",
    "head_commit": "30f6642255f3a3789ddaab59ef6fa8d26499f636",
    "repo_url": "https://github.com/prestodb/presto/pull/25308",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25308",
    "dockerfile": "",
    "pr_merged_at": "2025-06-17T21:29:35.000Z",
    "patch": "diff --git a/presto-native-execution/presto_cpp/main/thrift/ProtocolToThrift.cpp b/presto-native-execution/presto_cpp/main/thrift/ProtocolToThrift.cpp\nindex df6dc9eabae2b..f347e088a4b7a 100644\n--- a/presto-native-execution/presto_cpp/main/thrift/ProtocolToThrift.cpp\n+++ b/presto-native-execution/presto_cpp/main/thrift/ProtocolToThrift.cpp\n@@ -427,19 +427,12 @@ void fromThrift(\n void toThrift(\n     const facebook::presto::protocol::HostAddress& proto,\n     HostAddress& thrift) {\n-  std::vector<std::string> parts;\n-  folly::split(\":\", proto, parts);\n-  if (parts.size() == 2) {\n-    thrift.host_ref() = parts[0];\n-    thrift.port_ref() = std::stoi(parts[1]);\n-  }\n+  toThrift(proto, *thrift.hostPortString_ref());\n }\n void fromThrift(\n     const HostAddress& thrift,\n     facebook::presto::protocol::HostAddress& proto) {\n-  std::string hostAddressStr =\n-      *thrift.host_ref() + \".\" + std::to_string(*thrift.port_ref());\n-  fromThrift(hostAddressStr, proto);\n+  fromThrift(*thrift.hostPortString_ref(), proto);\n }\n void toThrift(\n     const facebook::presto::protocol::OutputBufferId& proto,\n@@ -635,12 +628,12 @@ void fromThrift(\n void toThrift(\n     const facebook::presto::protocol::TimeZoneKey& proto,\n     TimeZoneKey& thrift) {\n-  toThrift(proto, *thrift.key_ref());\n+  toThrift(proto, *thrift.timeZoneKey_ref());\n }\n void fromThrift(\n     const TimeZoneKey& thrift,\n     facebook::presto::protocol::TimeZoneKey& proto) {\n-  fromThrift(*thrift.key_ref(), proto);\n+  fromThrift(*thrift.timeZoneKey_ref(), proto);\n }\n void toThrift(\n     const facebook::presto::protocol::ResourceEstimates& proto,\n\ndiff --git a/presto-native-execution/presto_cpp/main/thrift/presto_protocol-to-thrift-json.yml b/presto-native-execution/presto_cpp/main/thrift/presto_protocol-to-thrift-json.yml\nindex 08d809b945417..b4e44171674e0 100644\n--- a/presto-native-execution/presto_cpp/main/thrift/presto_protocol-to-thrift-json.yml\n+++ b/presto-native-execution/presto_cpp/main/thrift/presto_protocol-to-thrift-json.yml\n@@ -49,9 +49,10 @@ WrapperStruct:\n   TransactionId:\n   RuntimeStats:\n   SqlFunctionId:\n+  HostAddress:\n+  TimeZoneKey:\n \n Special:\n-  HostAddress:\n   QualifiedObjectName:\n   MetadataUpdatesWrapper:\n   OperatorInfoUnion:\n@@ -59,6 +60,5 @@ Special:\n   SplitWrapper:\n   TableWriteInfoWrapper:\n   TaskId:\n-  TimeZoneKey:\n   Type:\n   TypeSignature:\n\ndiff --git a/presto-native-execution/presto_cpp/main/thrift/presto_thrift.thrift b/presto-native-execution/presto_cpp/main/thrift/presto_thrift.thrift\nindex e9a1fe2c6e1dd..2623225df0c11 100644\n--- a/presto-native-execution/presto_cpp/main/thrift/presto_thrift.thrift\n+++ b/presto-native-execution/presto_cpp/main/thrift/presto_thrift.thrift\n@@ -98,8 +98,7 @@ struct ErrorLocation {\n   2: i32 columnNumber;\n }\n struct HostAddress {\n-  1: string host;\n-  2: i32 port;\n+  1: string hostPortString;\n }\n struct StageId {\n   1: string queryId;\n@@ -237,8 +236,7 @@ struct TransactionId {\n   1: string uuid;\n }\n struct TimeZoneKey {\n-  1: string id;\n-  2: i16 key;\n+  1: i16 timeZoneKey;\n }\n struct ResourceEstimates {\n   1: optional double executionTime;\n\ndiff --git a/presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc b/presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc\ndeleted file mode 100644\nindex 640836b32a1db..0000000000000\n--- a/presto-native-execution/presto_cpp/main/thrift/special/HostAddress.cpp.inc\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-void toThrift(const facebook::presto::protocol::HostAddress& proto, HostAddress& thrift) {\n-  std::vector<std::string> parts;\n-  folly::split(\":\", proto, parts);\n-  if (parts.size() == 2) {\n-    thrift.host_ref() = parts[0];\n-    thrift.port_ref() = std::stoi(parts[1]);\n-  }\n-}\n-void fromThrift(const HostAddress& thrift, facebook::presto::protocol::HostAddress& proto) {\n-  std::string hostAddressStr =\n-    *thrift.host_ref() + \".\" +\n-    std::to_string(*thrift.port_ref());\n-  fromThrift(hostAddressStr, proto);\n-}\n\ndiff --git a/presto-native-execution/presto_cpp/main/thrift/special/TimeZoneKey.cpp.inc b/presto-native-execution/presto_cpp/main/thrift/special/TimeZoneKey.cpp.inc\ndeleted file mode 100644\nindex 99f51c98ee086..0000000000000\n--- a/presto-native-execution/presto_cpp/main/thrift/special/TimeZoneKey.cpp.inc\n+++ /dev/null\n@@ -1,20 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-void toThrift(const facebook::presto::protocol::TimeZoneKey& proto, TimeZoneKey& thrift) {\n-  toThrift(proto, *thrift.key_ref());\n-}\n-void fromThrift(const TimeZoneKey& thrift, facebook::presto::protocol::TimeZoneKey& proto) {\n-  fromThrift(*thrift.key_ref(), proto);\n-}\n\ndiff --git a/presto-native-execution/presto_cpp/main/thrift/thrift2json.py b/presto-native-execution/presto_cpp/main/thrift/thrift2json.py\nindex 8dd1a305d5d28..8560f037c68f5 100755\n--- a/presto-native-execution/presto_cpp/main/thrift/thrift2json.py\n+++ b/presto-native-execution/presto_cpp/main/thrift/thrift2json.py\n@@ -1,4 +1,4 @@\n-#!/Library/Developer/CommandLineTools/usr/bin/python3\n+#!/usr/bin/env python3\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n # You may obtain a copy of the License at\n",
    "test_patch": "diff --git a/presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp b/presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp\nindex 5f375ce45c75a..d74d1e9ed89ea 100644\n--- a/presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp\n+++ b/presto-native-execution/presto_cpp/main/tests/TaskStatusTest.cpp\n@@ -79,8 +79,7 @@ TEST_F(TaskStatusTest, executionFailureInfoOptionalFieldsEmpty) {\n \n   ASSERT_EQ(thriftExecutionFailureInfo.type(), \"type\");\n   ASSERT_EQ(thriftExecutionFailureInfo.errorLocation()->columnNumber(), 2);\n-  ASSERT_EQ(thriftExecutionFailureInfo.remoteHost()->host(), \"localhost\");\n-  ASSERT_EQ(thriftExecutionFailureInfo.remoteHost()->port(), 8080);\n+  ASSERT_EQ(thriftExecutionFailureInfo.remoteHost()->hostPortString(), \"localhost:8080\");\n   ASSERT_EQ(thriftExecutionFailureInfo.errorCode()->type(), facebook::presto::thrift::ErrorType::INTERNAL_ERROR);\n   ASSERT_EQ(thriftExecutionFailureInfo.errorCode()->retriable(), false);\n   ASSERT_EQ(thriftExecutionFailureInfo.errorCause(), facebook::presto::thrift::ErrorCause::EXCEEDS_BROADCAST_MEMORY_LIMIT);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25299",
    "pr_id": 25299,
    "issue_id": 25290,
    "repo": "prestodb/presto",
    "problem_statement": "[native] Make the prestocpp CI tests required\nThe following prestocpp CI tests must be made required.\n```\nprestocpp-format-and-header-check\nprestocpp-macos-build\nprestocpp-linux-build\nprestocpp-linux-build-and-unit-test\n```",
    "issue_word_count": 33,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      ".github/workflows/prestocpp-linux-adapters-build.yml",
      ".github/workflows/prestocpp-linux-build-and-unit-test.yml",
      ".github/workflows/prestocpp-linux-build.yml",
      ".github/workflows/prestocpp-macos-build.yml"
    ],
    "pr_changed_test_files": [
      ".github/workflows/prestocpp-linux-build-and-unit-test.yml"
    ],
    "base_commit": "db3db7f0b40c611b0f93b9537882b1ce4176a19c",
    "head_commit": "317ed3a576d53133ba8ccbd5a464e105b8685420",
    "repo_url": "https://github.com/prestodb/presto/pull/25299",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25299",
    "dockerfile": "",
    "pr_merged_at": "2025-06-13T15:10:00.000Z",
    "patch": "diff --git a/.github/workflows/prestocpp-linux-adapters-build.yml b/.github/workflows/prestocpp-linux-adapters-build.yml\nindex 130065fe5a791..dfde8686553fc 100644\n--- a/.github/workflows/prestocpp-linux-adapters-build.yml\n+++ b/.github/workflows/prestocpp-linux-adapters-build.yml\n@@ -7,9 +7,6 @@ on:\n   # - use smaller image - in the works\n   # - remove the adapters downloaded files after install - JWT needs fixing because of the cmake files end up\n   #pull_request:\n-  #  paths:\n-  #    - 'presto-native-execution/scripts/**'\n-  #    - '.github/workflows/prestocpp-linux-adapters-build.yml'\n \n jobs:\n   prestocpp-linux-adapters-build:\n\ndiff --git a/.github/workflows/prestocpp-linux-build.yml b/.github/workflows/prestocpp-linux-build.yml\nindex c887a434344bf..c0c70289a1d0b 100644\n--- a/.github/workflows/prestocpp-linux-build.yml\n+++ b/.github/workflows/prestocpp-linux-build.yml\n@@ -3,30 +3,6 @@ name: prestocpp-linux-build\n on:\n   workflow_dispatch:\n   pull_request:\n-    paths:\n-      - 'presto-native-execution/**'\n-      - '.github/workflows/prestocpp-linux-build.yml'\n-      # Build also changes to files that can change the protocol and are referenced in the protocol yaml:\n-      # protocol_core\n-      - 'presto-spi/src/main/java/com/facebook/presto/spi/**'\n-      - 'presto-common/src/main/java/com/facebook/presto/**'\n-      - 'presto-main/src/main/java/com/facebook/presto/**'\n-      - 'presto-client/src/main/java/com/facebook/presto/client/**'\n-      - 'presto-spark-base/src/main/java/com/facebook/presto/spark/execution/**'\n-      - 'presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/nativechecker/**'\n-      - 'presto-function-namespace-managers-common/src/main/java/com/facebook/presto/functionNamespace/**'\n-      - 'presto-hdfs-core/src/main/java/com/facebook/presto/hive/**'\n-      - 'presto-verifier/src/main/java/com/facebook/presto/verifier/framework/**'\n-      # arrow-flight\n-      - 'presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/**'\n-      # hive\n-      - 'presto-hive-metastore/src/main/java/com/facebook/presto/hive/**'\n-      - 'presto-hive-common/src/main/java/com/facebook/presto/hive/**'\n-      - 'presto-hive/src/main/java/com/facebook/presto/hive/**'\n-      # iceberg\n-      - 'presto-iceberg/src/main/java/com/facebook/presto/iceberg/**'\n-      # tpch\n-      - 'presto-tpch/src/main/java/com/facebook/presto/tpch/**'\n \n jobs:\n   prestocpp-linux-build-engine:\n@@ -38,28 +14,28 @@ jobs:\n       CC: /usr/bin/clang-15\n       CXX: /usr/bin/clang++-15\n       BUILD_SCRIPT: |\n-          cd presto-native-execution\n-          cmake \\\n-            -B _build/debug \\\n-            -GNinja \\\n-            -DTREAT_WARNINGS_AS_ERRORS=1 \\\n-            -DENABLE_ALL_WARNINGS=1 \\\n-            -DCMAKE_BUILD_TYPE=Debug \\\n-            -DPRESTO_ENABLE_PARQUET=ON \\\n-            -DPRESTO_ENABLE_S3=ON \\\n-            -DPRESTO_ENABLE_GCS=ON \\\n-            -DPRESTO_ENABLE_ABFS=OFF \\\n-            -DPRESTO_ENABLE_HDFS=ON \\\n-            -DPRESTO_ENABLE_REMOTE_FUNCTIONS=ON \\\n-            -DPRESTO_ENABLE_JWT=ON \\\n-            -DPRESTO_STATS_REPORTER_TYPE=PROMETHEUS \\\n-            -DPRESTO_MEMORY_CHECKER_TYPE=LINUX_MEMORY_CHECKER \\\n-            -DPRESTO_ENABLE_TESTING=OFF \\\n-            -DCMAKE_PREFIX_PATH=/usr/local \\\n-            -DThrift_ROOT=/usr/local \\\n-            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n-            -DMAX_LINK_JOBS=4\n-          ninja -C _build/debug -j 4\n+        cd presto-native-execution\n+        cmake \\\n+          -B _build/debug \\\n+          -GNinja \\\n+          -DTREAT_WARNINGS_AS_ERRORS=1 \\\n+          -DENABLE_ALL_WARNINGS=1 \\\n+          -DCMAKE_BUILD_TYPE=Debug \\\n+          -DPRESTO_ENABLE_PARQUET=ON \\\n+          -DPRESTO_ENABLE_S3=ON \\\n+          -DPRESTO_ENABLE_GCS=ON \\\n+          -DPRESTO_ENABLE_ABFS=OFF \\\n+          -DPRESTO_ENABLE_HDFS=ON \\\n+          -DPRESTO_ENABLE_REMOTE_FUNCTIONS=ON \\\n+          -DPRESTO_ENABLE_JWT=ON \\\n+          -DPRESTO_STATS_REPORTER_TYPE=PROMETHEUS \\\n+          -DPRESTO_MEMORY_CHECKER_TYPE=LINUX_MEMORY_CHECKER \\\n+          -DPRESTO_ENABLE_TESTING=OFF \\\n+          -DCMAKE_PREFIX_PATH=/usr/local \\\n+          -DThrift_ROOT=/usr/local \\\n+          -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n+          -DMAX_LINK_JOBS=4\n+        ninja -C _build/debug -j 4\n \n     steps:\n       - uses: actions/checkout@v4\n\ndiff --git a/.github/workflows/prestocpp-macos-build.yml b/.github/workflows/prestocpp-macos-build.yml\nindex 85a0b5b0351b9..913e792481e07 100644\n--- a/.github/workflows/prestocpp-macos-build.yml\n+++ b/.github/workflows/prestocpp-macos-build.yml\n@@ -3,9 +3,6 @@ name: prestocpp-macos-build\n on:\n   workflow_dispatch:\n   pull_request:\n-    paths:\n-      - 'presto-native-execution/**'\n-      - '.github/workflows/prestocpp-macos-build.yml'\n \n jobs:\n   prestocpp-macos-build-engine:\n",
    "test_patch": "diff --git a/.github/workflows/prestocpp-linux-build-and-unit-test.yml b/.github/workflows/prestocpp-linux-build-and-unit-test.yml\nindex 18e37e01cc512..e76f47f9690b6 100644\n--- a/.github/workflows/prestocpp-linux-build-and-unit-test.yml\n+++ b/.github/workflows/prestocpp-linux-build-and-unit-test.yml\n@@ -3,10 +3,6 @@ name: prestocpp-linux-build-and-unit-test\n on:\n   workflow_dispatch:\n   pull_request:\n-    paths:\n-      - 'presto-native-execution/**'\n-      - 'presto-native-sidecar-plugin/**'\n-      - '.github/workflows/prestocpp-linux-build-and-unit-test.yml'\n   push:\n     branches:\n       - master\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25293",
    "pr_id": 25293,
    "issue_id": 24629,
    "repo": "prestodb/presto",
    "problem_statement": "【Iceberg equality scan】fix duplicate key issue when querying metadata column \"$data_sequence_number\" from an iceberg table within equality deletes\n<!--- Provide a general summary of the issue in the Title above -->\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used: 0.291\n* Storage (HDFS/S3/GCS..): local file\n* Data source and connector used: IcebergConnecntor\n* Deployment (Cloud or On-prem): run IcebergQueryRunner on local env\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\n\n## Expected Behavior\n<!--- Tell us what should happen -->\nSelect the hidden column \"$data_sequence_number\" successfully \n\n## Current Behavior\n<!--- Tell us what happens instead of the expected behavior -->\nQuery failed, error messages as follow:\n\nQuery 20250226_023830_00016_bkm44 failed: Duplicate key $data_sequence_number\n\n## Possible Solution\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\nWhen update the new TableScanNode, check whether need to add the extra metadata column to the assignments and outputs.\n\n## Steps to Reproduce\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\n1. Create an iceberg table with equality deletes by flink cdc\n2. Configure the IcebergQueryRunner, the properties are:\n\n![Image](https://github.com/user-attachments/assets/b8b521f3-1e74-4c08-b102-e6a3c91d282e)\n\n3. Running the IcebergQueryRunner and query with the sql:\nselect \"$data_sequence_number\", * from tbl_cdc_with_equality;\nor\nselect \"$data_sequence_number\" from tbl_cdc_with_equality;\n4. Query failed, and receive the error\n\n## Screenshots (if appropriate)\n\n![Image](https://github.com/user-attachments/assets/d8222c2b-f40d-43bc-a889-5c469faf9fe2)\n\n## Context\n<!--- How has this issue affected you? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\n\n",
    "issue_word_count": 288,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java"
    ],
    "base_commit": "3dea808c726cd21e3e9e7b3d368d58c96e4224a8",
    "head_commit": "96bf02e6734f11f858308512e43d8f660e3763f2",
    "repo_url": "https://github.com/prestodb/presto/pull/25293",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25293",
    "dockerfile": "",
    "pr_merged_at": "2025-06-15T01:44:22.000Z",
    "patch": "diff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\nindex 18635bb4ce806..8a2bbba1c2423 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\n@@ -266,9 +266,10 @@ public PlanNode visitTableScan(TableScanNode node, RewriteContext<Void> context)\n                     new SpecialFormExpression(SpecialFormExpression.Form.IS_NULL, BooleanType.BOOLEAN,\n                             new SpecialFormExpression(SpecialFormExpression.Form.COALESCE, BigintType.BIGINT, deleteVersionColumns)));\n \n+            boolean hasExplicitDataSequenceNumberCol = node.getAssignments().containsValue(DATA_SEQUENCE_NUMBER_COLUMN_HANDLE);\n             Assignments.Builder assignmentsBuilder = Assignments.builder();\n             filter.getOutputVariables().stream()\n-                    .filter(variableReferenceExpression -> !variableReferenceExpression.getName().startsWith(DATA_SEQUENCE_NUMBER_COLUMN_HANDLE.getName()))\n+                    .filter(variableReferenceExpression -> hasExplicitDataSequenceNumberCol || !variableReferenceExpression.getName().startsWith(DATA_SEQUENCE_NUMBER_COLUMN_HANDLE.getName()))\n                     .forEach(variableReferenceExpression -> assignmentsBuilder.put(variableReferenceExpression, variableReferenceExpression));\n             return new ProjectNode(Optional.empty(), idAllocator.getNextId(), filter, assignmentsBuilder.build(), ProjectNode.Locality.LOCAL);\n         }\n@@ -368,12 +369,12 @@ private TableScanNode createNewRoot(TableScanNode node, IcebergTableHandle icebe\n \n             VariableReferenceExpression dataSequenceNumberVariableReference = toVariableReference(DATA_SEQUENCE_NUMBER_COLUMN_HANDLE);\n             ImmutableMap.Builder<VariableReferenceExpression, ColumnHandle> assignmentsBuilder = ImmutableMap.<VariableReferenceExpression, ColumnHandle>builder()\n-                    .put(dataSequenceNumberVariableReference, DATA_SEQUENCE_NUMBER_COLUMN_HANDLE)\n                     .putAll(unselectedAssignments)\n                     .putAll(node.getAssignments());\n             ImmutableList.Builder<VariableReferenceExpression> outputsBuilder = ImmutableList.builder();\n             outputsBuilder.addAll(node.getOutputVariables());\n-            if (!node.getAssignments().containsKey(dataSequenceNumberVariableReference)) {\n+            if (!node.getAssignments().containsValue(DATA_SEQUENCE_NUMBER_COLUMN_HANDLE)) {\n+                assignmentsBuilder.put(dataSequenceNumberVariableReference, DATA_SEQUENCE_NUMBER_COLUMN_HANDLE);\n                 outputsBuilder.add(dataSequenceNumberVariableReference);\n             }\n             outputsBuilder.addAll(unselectedAssignments.keySet());\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\nindex ed91212089c3a..5e30f9bdaab72 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n@@ -1423,6 +1423,36 @@ public void testEqualityDeletesWithHiddenPartitionsEvolution(String fileFormat,\n         assertQuery(session, \"SELECT * FROM \" + tableName, \"VALUES (1, '1001', NULL, NULL), (3, '1003', NULL, NULL), (6, '1004', 1, NULL), (6, '1006', 2, 'th002')\");\n     }\n \n+    @Test(dataProvider = \"equalityDeleteOptions\")\n+    public void testEqualityDeletesWithDataSequenceNumber(String fileFormat, boolean joinRewriteEnabled)\n+            throws Exception\n+    {\n+        Session session = deleteAsJoinEnabled(joinRewriteEnabled);\n+        String tableName = \"test_v2_row_delete_\" + randomTableSuffix();\n+        String tableName2 = \"test_v2_row_delete_2_\" + randomTableSuffix();\n+        assertUpdate(\"CREATE TABLE \" + tableName + \"(id int, data varchar) WITH (\\\"write.format.default\\\" = '\" + fileFormat + \"')\");\n+        assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+\n+        assertUpdate(\"CREATE TABLE \" + tableName2 + \"(id int, data varchar) WITH (\\\"write.format.default\\\" = '\" + fileFormat + \"')\");\n+        assertUpdate(\"INSERT INTO \" + tableName2 + \" VALUES (1, 'a')\", 1);\n+\n+        Table icebergTable = updateTable(tableName);\n+        writeEqualityDeleteToNationTable(icebergTable, ImmutableMap.of(\"id\", 1));\n+\n+        Table icebergTable2 = updateTable(tableName2);\n+        writeEqualityDeleteToNationTable(icebergTable2, ImmutableMap.of(\"id\", 1));\n+\n+        assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'b'), (2, 'a'), (3, 'a')\", 3);\n+        assertUpdate(\"INSERT INTO \" + tableName2 + \" VALUES (1, 'b'), (2, 'a'), (3, 'a')\", 3);\n+\n+        assertQuery(session, \"SELECT * FROM \" + tableName, \"VALUES (1, 'b'), (2, 'a'), (3, 'a')\");\n+\n+        assertQuery(session, \"SELECT \\\"$data_sequence_number\\\", * FROM \" + tableName, \"VALUES (3, 1, 'b'), (3, 2, 'a'), (3, 3, 'a')\");\n+\n+        assertQuery(session, \"SELECT a.\\\"$data_sequence_number\\\", b.\\\"$data_sequence_number\\\" from \" + tableName + \" as a, \" + tableName2 + \" as b where a.id = b.id\",\n+                \"VALUES (3, 3), (3, 3), (3, 3)\");\n+    }\n+\n     @Test\n     public void testPartShowStatsWithFilters()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25280",
    "pr_id": 25280,
    "issue_id": 24733,
    "repo": "prestodb/presto",
    "problem_statement": "Support more Iceberg Metadata columns\nhttps://prestodb.io/docs/current/connector/iceberg.html#extra-hidden-metadata-columns\nSo far Presto only supports querying the $path and $data_sequence_number metadata columns. We will need to add more columns. For more see https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html. \nThe first several we should support\n\n- [DELETE_FILE_PATH](https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html#DELETE_FILE_PATH)\n- [IS_DELETED](https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html#IS_DELETED)\n\n## Context\nThese columns are important for debugging ",
    "issue_word_count": 95,
    "test_files_count": 1,
    "non_test_files_count": 11,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataColumn.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSourceProvider.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUpdateablePageSource.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFilter.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/EqualityDeleteFilter.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/PositionDeleteFilter.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/RowPredicate.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java"
    ],
    "base_commit": "0a504d20e69db9d82b28fe475f623c13ac149765",
    "head_commit": "8ad17c79cb15fd65231da3dc451a8dbafe6809fe",
    "repo_url": "https://github.com/prestodb/presto/pull/25280",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25280",
    "dockerfile": "",
    "pr_merged_at": "2025-06-25T21:50:19.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex ef5692f41408d..722f17862f03d 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -675,6 +675,42 @@ The Iceberg data sequence number in which this row was added.\n      ----------------------------------+------------\n                   2                    | 3\n \n+``$deleted`` column\n+^^^^^^^^^^^^^^^^^^^\n+Whether this row is a deleted row. When this column is used, deleted rows\n+from delete files will be marked as ``true`` instead of being filtered out of the results.\n+\n+.. code-block:: sql\n+\n+    DELETE FROM \"ctas_nation\" WHERE regionkey = 0;\n+\n+    SELECT \"$deleted\", regionkey FROM \"ctas_nation\";\n+\n+.. code-block:: text\n+\n+     $deleted | regionkey\n+    ----------+-----------\n+     true     |         0\n+     false    |         1\n+\n+``$delete_file_path`` column\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+The path of the delete file corresponding to a deleted row, or NULL if the row was not deleted.\n+When this column is used, deleted rows will not be filtered out of the results.\n+\n+.. code-block:: sql\n+\n+    DELETE FROM \"ctas_nation\" WHERE regionkey = 0;\n+\n+    SELECT \"$delete_file_path\", regionkey FROM \"ctas_nation\";\n+\n+.. code-block:: text\n+\n+                                     $delete_file_path                                 | regionkey\n+    -----------------------------------------------------------------------------------+-----------\n+     file:/path/to/table/data/delete_file_d8510b3e-510a-4fc2-b2b2-e59ead7fd386.parquet |         0\n+     NULL                                                                              |         1\n+\n Presto C++ Support\n ^^^^^^^^^^^^^^^^^^\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex b2e6b7431a4f6..1166a0ef4ab85 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -130,12 +130,18 @@\n import static com.facebook.presto.iceberg.ExpressionConverter.toIcebergExpression;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.DATA_SEQUENCE_NUMBER_COLUMN_HANDLE;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.DATA_SEQUENCE_NUMBER_COLUMN_METADATA;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.DELETE_FILE_PATH_COLUMN_HANDLE;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.DELETE_FILE_PATH_COLUMN_METADATA;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.IS_DELETED_COLUMN_HANDLE;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.IS_DELETED_COLUMN_METADATA;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.PATH_COLUMN_HANDLE;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.PATH_COLUMN_METADATA;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_COMMIT_ERROR;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_INVALID_SNAPSHOT_ID;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.DATA_SEQUENCE_NUMBER;\n+import static com.facebook.presto.iceberg.IcebergMetadataColumn.DELETE_FILE_PATH;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.FILE_PATH;\n+import static com.facebook.presto.iceberg.IcebergMetadataColumn.IS_DELETED;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.UPDATE_ROW_DATA;\n import static com.facebook.presto.iceberg.IcebergPartitionType.ALL;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.getCompressionCodec;\n@@ -431,6 +437,8 @@ protected ConnectorTableMetadata getTableOrViewMetadata(ConnectorSession session\n             else {\n                 columns.add(PATH_COLUMN_METADATA);\n                 columns.add(DATA_SEQUENCE_NUMBER_COLUMN_METADATA);\n+                columns.add(IS_DELETED_COLUMN_METADATA);\n+                columns.add(DELETE_FILE_PATH_COLUMN_METADATA);\n             }\n             return new ConnectorTableMetadata(table, columns.build(), createMetadataProperties(icebergTable, session), getTableComment(icebergTable));\n         }\n@@ -932,6 +940,8 @@ public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, Conn\n         if (table.getIcebergTableName().getTableType() != CHANGELOG) {\n             columnHandles.put(FILE_PATH.getColumnName(), PATH_COLUMN_HANDLE);\n             columnHandles.put(DATA_SEQUENCE_NUMBER.getColumnName(), DATA_SEQUENCE_NUMBER_COLUMN_HANDLE);\n+            columnHandles.put(IS_DELETED.getColumnName(), IS_DELETED_COLUMN_HANDLE);\n+            columnHandles.put(DELETE_FILE_PATH.getColumnName(), DELETE_FILE_PATH_COLUMN_HANDLE);\n         }\n         return columnHandles.build();\n     }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java\nindex fa912f60a3f8c..62fafea0d48d8 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java\n@@ -34,7 +34,9 @@\n import static com.facebook.presto.iceberg.ColumnIdentity.createColumnIdentity;\n import static com.facebook.presto.iceberg.ColumnIdentity.primitiveColumnIdentity;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.DATA_SEQUENCE_NUMBER;\n+import static com.facebook.presto.iceberg.IcebergMetadataColumn.DELETE_FILE_PATH;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.FILE_PATH;\n+import static com.facebook.presto.iceberg.IcebergMetadataColumn.IS_DELETED;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.UPDATE_ROW_DATA;\n import static com.facebook.presto.iceberg.TypeConverter.toPrestoType;\n import static com.google.common.base.Preconditions.checkArgument;\n@@ -50,6 +52,10 @@ public class IcebergColumnHandle\n     public static final ColumnMetadata PATH_COLUMN_METADATA = getColumnMetadata(FILE_PATH);\n     public static final IcebergColumnHandle DATA_SEQUENCE_NUMBER_COLUMN_HANDLE = getIcebergColumnHandle(DATA_SEQUENCE_NUMBER);\n     public static final ColumnMetadata DATA_SEQUENCE_NUMBER_COLUMN_METADATA = getColumnMetadata(DATA_SEQUENCE_NUMBER);\n+    public static final IcebergColumnHandle IS_DELETED_COLUMN_HANDLE = getIcebergColumnHandle(IS_DELETED);\n+    public static final ColumnMetadata IS_DELETED_COLUMN_METADATA = getColumnMetadata(IS_DELETED);\n+    public static final IcebergColumnHandle DELETE_FILE_PATH_COLUMN_HANDLE = getIcebergColumnHandle(DELETE_FILE_PATH);\n+    public static final ColumnMetadata DELETE_FILE_PATH_COLUMN_METADATA = getColumnMetadata(DELETE_FILE_PATH);\n \n     private final ColumnIdentity columnIdentity;\n     private final Type type;\n@@ -180,6 +186,16 @@ public boolean isDataSequenceNumberColumn()\n         return getColumnIdentity().getId() == DATA_SEQUENCE_NUMBER.getId();\n     }\n \n+    public boolean isDeletedColumn()\n+    {\n+        return getColumnIdentity().getId() == IS_DELETED.getId();\n+    }\n+\n+    public boolean isDeleteFilePathColumn()\n+    {\n+        return getColumnIdentity().getId() == DELETE_FILE_PATH.getId();\n+    }\n+\n     public static IcebergColumnHandle primitiveIcebergColumnHandle(int id, String name, Type type, Optional<String> comment)\n     {\n         return new IcebergColumnHandle(primitiveColumnIdentity(id, name), type, comment, REGULAR);\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataColumn.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataColumn.java\nindex 9758f325338c7..5862fba4975f8 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataColumn.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataColumn.java\n@@ -22,6 +22,7 @@\n import java.util.stream.Stream;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n import static com.facebook.presto.common.type.UnknownType.UNKNOWN;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.iceberg.ColumnIdentity.TypeCategory.PRIMITIVE;\n@@ -32,6 +33,8 @@ public enum IcebergMetadataColumn\n {\n     FILE_PATH(MetadataColumns.FILE_PATH.fieldId(), \"$path\", VARCHAR, PRIMITIVE),\n     DATA_SEQUENCE_NUMBER(Integer.MAX_VALUE - 1001, \"$data_sequence_number\", BIGINT, PRIMITIVE),\n+    IS_DELETED(MetadataColumns.IS_DELETED.fieldId(), \"$deleted\", BOOLEAN, PRIMITIVE),\n+    DELETE_FILE_PATH(MetadataColumns.DELETE_FILE_PATH.fieldId(), \"$delete_file_path\", VARCHAR, PRIMITIVE),\n     /**\n      * Iceberg reserved row ids begin at INTEGER.MAX_VALUE and count down. Starting with MIN_VALUE here to avoid conflicts.\n      * Inner type for row is not known until runtime.\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSourceProvider.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSourceProvider.java\nindex ca982e29cfc54..07b4079ecf0d7 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSourceProvider.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSourceProvider.java\n@@ -78,7 +78,6 @@\n import com.facebook.presto.spi.SplitContext;\n import com.facebook.presto.spi.connector.ConnectorPageSourceProvider;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n-import com.google.common.base.Suppliers;\n import com.google.common.base.VerifyException;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n@@ -145,6 +144,7 @@\n import static com.facebook.presto.hive.parquet.ParquetPageSourceFactory.createDecryptor;\n import static com.facebook.presto.iceberg.FileContent.EQUALITY_DELETES;\n import static com.facebook.presto.iceberg.FileContent.POSITION_DELETES;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.DELETE_FILE_PATH_COLUMN_HANDLE;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.getPushedDownSubfield;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.isPushedDownSubfield;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_BAD_DATA;\n@@ -176,6 +176,7 @@\n import static com.facebook.presto.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n import static com.google.common.base.Predicates.not;\n+import static com.google.common.base.Suppliers.memoize;\n import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n@@ -858,24 +859,26 @@ else if (icebergColumn.isDataSequenceNumberColumn()) {\n                 session,\n                 split.getPath(),\n                 split.getFileFormat());\n-        Supplier<Optional<RowPredicate>> deletePredicate = Suppliers.memoize(() -> {\n+        boolean storeDeleteFilePath = icebergColumns.contains(DELETE_FILE_PATH_COLUMN_HANDLE);\n+        Supplier<List<DeleteFilter>> deleteFilters = memoize(() -> {\n             // If equality deletes are optimized into a join they don't need to be applied here\n             List<DeleteFile> deletesToApply = split\n                     .getDeletes()\n                     .stream()\n                     .filter(deleteFile -> deleteFile.content() == POSITION_DELETES || equalityDeletesRequired)\n                     .collect(toImmutableList());\n-            List<DeleteFilter> deleteFilters = readDeletes(\n+            return readDeletes(\n                     session,\n                     tableSchema,\n                     split.getPath(),\n                     deletesToApply,\n                     partitionInsertingPageSource.getRowPositionDelegate().getStartRowPosition(),\n-                    partitionInsertingPageSource.getRowPositionDelegate().getEndRowPosition());\n-            return deleteFilters.stream()\n-                    .map(filter -> filter.createPredicate(delegateColumns))\n-                    .reduce(RowPredicate::and);\n+                    partitionInsertingPageSource.getRowPositionDelegate().getEndRowPosition(),\n+                    storeDeleteFilePath);\n         });\n+        Supplier<Optional<RowPredicate>> deletePredicate = memoize(() -> deleteFilters.get().stream()\n+                .map(filter -> filter.createPredicate(delegateColumns))\n+                .reduce(RowPredicate::and));\n         Table icebergTable = getShallowWrappedIcebergTable(\n                 tableSchema,\n                 partitionSpec,\n@@ -905,6 +908,7 @@ else if (icebergColumn.isDataSequenceNumberColumn()) {\n                 delegateColumns,\n                 deleteSinkSupplier,\n                 deletePredicate,\n+                deleteFilters,\n                 updatedRowPageSinkSupplier,\n                 table.getUpdatedColumns(),\n                 updateRow);\n@@ -940,7 +944,8 @@ private List<DeleteFilter> readDeletes(\n             String dataFilePath,\n             List<DeleteFile> deleteFiles,\n             Optional<Long> startRowPosition,\n-            Optional<Long> endRowPosition)\n+            Optional<Long> endRowPosition,\n+            boolean storeDeleteFilePath)\n     {\n         verify(startRowPosition.isPresent() == endRowPosition.isPresent(), \"startRowPosition and endRowPosition must be specified together\");\n \n@@ -981,6 +986,10 @@ private List<DeleteFilter> readDeletes(\n                 catch (IOException e) {\n                     throw new PrestoException(ICEBERG_CANNOT_OPEN_SPLIT, format(\"Cannot open Iceberg delete file: %s\", delete.path()), e);\n                 }\n+                if (storeDeleteFilePath) {\n+                    filters.add(new PositionDeleteFilter(deletedRows, delete.path()));\n+                    deletedRows = new Roaring64Bitmap(); // Reset the deleted rows for the next file\n+                }\n             }\n             else if (delete.content() == EQUALITY_DELETES) {\n                 List<Integer> fieldIds = delete.equalityFieldIds();\n@@ -990,7 +999,7 @@ else if (delete.content() == EQUALITY_DELETES) {\n                         .collect(toImmutableList());\n \n                 try (ConnectorPageSource pageSource = openDeletes(session, delete, columns, TupleDomain.all())) {\n-                    filters.add(readEqualityDeletes(pageSource, columns, schema));\n+                    filters.add(readEqualityDeletes(pageSource, columns, storeDeleteFilePath ? delete.path() : null));\n                 }\n                 catch (IOException e) {\n                     throw new PrestoException(ICEBERG_CANNOT_OPEN_SPLIT, format(\"Cannot open Iceberg delete file: %s\", delete.path()), e);\n@@ -1001,8 +1010,8 @@ else if (delete.content() == EQUALITY_DELETES) {\n             }\n         }\n \n-        if (!deletedRows.isEmpty()) {\n-            filters.add(new PositionDeleteFilter(deletedRows));\n+        if (!deletedRows.isEmpty() && !storeDeleteFilePath) {\n+            filters.add(new PositionDeleteFilter(deletedRows, null));\n         }\n \n         return filters;\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUpdateablePageSource.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUpdateablePageSource.java\nindex 16cc207683a9b..7d8d4bb1500fd 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUpdateablePageSource.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUpdateablePageSource.java\n@@ -15,9 +15,12 @@\n \n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.Block;\n+import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.block.ColumnarRow;\n import com.facebook.presto.common.block.RowBlock;\n+import com.facebook.presto.common.block.RunLengthEncodedBlock;\n import com.facebook.presto.hive.HivePartitionKey;\n+import com.facebook.presto.iceberg.delete.DeleteFilter;\n import com.facebook.presto.iceberg.delete.IcebergDeletePageSink;\n import com.facebook.presto.iceberg.delete.RowPredicate;\n import com.facebook.presto.spi.ConnectorPageSource;\n@@ -25,8 +28,10 @@\n import com.facebook.presto.spi.UpdatablePageSource;\n import com.google.common.collect.ImmutableList;\n import io.airlift.slice.Slice;\n+import io.airlift.slice.Slices;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.Pair;\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n@@ -34,14 +39,19 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.CompletableFuture;\n import java.util.function.Consumer;\n+import java.util.function.Predicate;\n import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n import static com.facebook.presto.common.block.ColumnarRow.toColumnarRow;\n+import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n+import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_BAD_DATA;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_MISSING_COLUMN;\n import static com.google.common.base.Throwables.throwIfInstanceOf;\n@@ -64,12 +74,14 @@ public class IcebergUpdateablePageSource\n     private final Supplier<IcebergDeletePageSink> deleteSinkSupplier;\n     private IcebergDeletePageSink positionDeleteSink;\n     private final Supplier<Optional<RowPredicate>> deletePredicate;\n+    private final Supplier<List<DeleteFilter>> deleteFilters;\n \n     private final List<IcebergColumnHandle> columns;\n     /**\n      * Columns actually updated in the query\n      */\n     private final List<IcebergColumnHandle> updatedColumns;\n+    private final List<IcebergColumnHandle> delegateColumns;\n     private final Schema tableSchema;\n     private final Supplier<IcebergPageSink> updatedRowPageSinkSupplier;\n     private IcebergPageSink updatedRowPageSink;\n@@ -83,6 +95,8 @@ public class IcebergUpdateablePageSource\n     // Maps the Iceberg field ids of modified columns to their indexes in the updatedColumns columnValueAndRowIdChannels array\n     private final Map<ColumnIdentity, Integer> columnIdentityToUpdatedColumnIndex = new HashMap<>();\n     private final int[] outputColumnToDelegateMapping;\n+    private final int isDeletedColumnId;\n+    private final int deleteFilePathColumnId;\n \n     public IcebergUpdateablePageSource(\n             Schema tableSchema,\n@@ -95,6 +109,7 @@ public IcebergUpdateablePageSource(\n             List<IcebergColumnHandle> delegateColumns,\n             Supplier<IcebergDeletePageSink> deleteSinkSupplier,\n             Supplier<Optional<RowPredicate>> deletePredicate,\n+            Supplier<List<DeleteFilter>> deleteFilters,\n             Supplier<IcebergPageSink> updatedRowPageSinkSupplier,\n             // the columns that this page source is supposed to update\n             List<IcebergColumnHandle> updatedColumns,\n@@ -104,9 +119,11 @@ public IcebergUpdateablePageSource(\n         this.tableSchema = requireNonNull(tableSchema, \"tableSchema is null\");\n         this.columns = requireNonNull(outputColumns, \"columns is null\");\n         this.delegate = requireNonNull(delegate, \"delegate is null\");\n+        this.delegateColumns = requireNonNull(delegateColumns, \"delegateColumns is null\");\n         // information for deletes\n         this.deleteSinkSupplier = deleteSinkSupplier;\n         this.deletePredicate = requireNonNull(deletePredicate, \"deletePredicate is null\");\n+        this.deleteFilters = requireNonNull(deleteFilters, \"deleteFilters is null\");\n         // information for updates\n         this.updatedRowPageSinkSupplier = requireNonNull(updatedRowPageSinkSupplier, \"updatedRowPageSinkSupplier is null\");\n         this.updatedColumns = requireNonNull(updatedColumns, \"updatedColumns is null\");\n@@ -145,6 +162,8 @@ public IcebergUpdateablePageSource(\n                 outputColumnToDelegateMapping[i] = columnToIndex.get(outputColumns.get(i).getColumnIdentity());\n             }\n         }\n+        this.isDeletedColumnId = getDelegateColumnId(IcebergColumnHandle::isDeletedColumn);\n+        this.deleteFilePathColumnId = getDelegateColumnId(IcebergColumnHandle::isDeleteFilePathColumn);\n     }\n \n     @Override\n@@ -191,7 +210,22 @@ public Page getNextPage()\n             }\n \n             Optional<RowPredicate> deleteFilterPredicate = deletePredicate.get();\n-            if (deleteFilterPredicate.isPresent()) {\n+            if (isDeletedColumnId != -1 || deleteFilePathColumnId != -1) {\n+                if (isDeletedColumnId != -1) {\n+                    if (deleteFilterPredicate.isPresent()) {\n+                        // Instead of filtering rows, we mark whether the row is deleted in the $deleted column\n+                        dataPage = deleteFilterPredicate.get().markDeleted(dataPage, isDeletedColumnId);\n+                    }\n+                    else {\n+                        Block allFalseBlock = RunLengthEncodedBlock.create(BOOLEAN, false, dataPage.getPositionCount());\n+                        dataPage = dataPage.replaceColumn(isDeletedColumnId, allFalseBlock);\n+                    }\n+                }\n+                if (deleteFilePathColumnId != -1) {\n+                    dataPage = markDeleteFilePath(dataPage, deleteFilePathColumnId);\n+                }\n+            }\n+            else if (deleteFilterPredicate.isPresent()) {\n                 dataPage = deleteFilterPredicate.get().filterPage(dataPage);\n             }\n \n@@ -311,6 +345,87 @@ private Page setUpdateRowIdBlock(Page page)\n         return new Page(page.getPositionCount(), fullPage);\n     }\n \n+    private int getDelegateColumnId(Predicate<IcebergColumnHandle> columnPredicate)\n+    {\n+        int targetColumnId = -1;\n+        for (int i = 0; i < columns.size(); i++) {\n+            if (columnPredicate.test(columns.get(i))) {\n+                targetColumnId = i;\n+                break;\n+            }\n+        }\n+        if (targetColumnId == -1) {\n+            return -1;\n+        }\n+        return outputColumnToDelegateMapping[targetColumnId];\n+    }\n+\n+    private Page markDeleteFilePath(Page page, int deleteFilePathDelegateColumnId)\n+    {\n+        List<Pair<DeleteFilter, RowPredicate>> filterPredicates = deleteFilters.get().stream()\n+                .map(filter -> Pair.of(filter, filter.createPredicate(delegateColumns)))\n+                .collect(Collectors.toList());\n+\n+        int positionCount = page.getPositionCount();\n+        if (positionCount == 0) {\n+            return page;\n+        }\n+\n+        boolean allSameValues = true;\n+        Optional<String> firstValue = getDeleteFilePath(page, 0, filterPredicates);\n+        BlockBuilder blockBuilder = null;\n+        // Build the varchar block with the deleted file path or null if the row isn't deleted\n+        for (int position = 1; position < positionCount; position++) {\n+            Optional<String> deleteFilePath = getDeleteFilePath(page, position, filterPredicates);\n+            if (allSameValues && !Objects.equals(firstValue.orElse(null), deleteFilePath.orElse(null))) {\n+                blockBuilder = VARCHAR.createBlockBuilder(null, positionCount);\n+                for (int idx = 0; idx < position; idx++) {\n+                    writeStringOrNull(blockBuilder, firstValue);\n+                }\n+                writeStringOrNull(blockBuilder, deleteFilePath);\n+                allSameValues = false;\n+            }\n+            else if (!allSameValues) {\n+                writeStringOrNull(blockBuilder, deleteFilePath);\n+            }\n+        }\n+\n+        Block block;\n+        if (blockBuilder != null) {\n+            block = blockBuilder.build();\n+        }\n+        else {\n+            Slice slice = firstValue.map(Slices::utf8Slice).orElse(null);\n+            block = RunLengthEncodedBlock.create(VARCHAR, slice, positionCount);\n+        }\n+\n+        return page.replaceColumn(deleteFilePathDelegateColumnId, block);\n+    }\n+\n+    private void writeStringOrNull(BlockBuilder blockBuilder, Optional<String> toWrite)\n+    {\n+        if (toWrite.isPresent()) {\n+            VARCHAR.writeString(blockBuilder, toWrite.get());\n+        }\n+        else {\n+            blockBuilder.appendNull();\n+        }\n+    }\n+\n+    private Optional<String> getDeleteFilePath(Page page, int position, List<Pair<DeleteFilter, RowPredicate>> filterPredicates)\n+    {\n+        for (Pair<DeleteFilter, RowPredicate> pair : filterPredicates) {\n+            boolean deleted = !pair.second().test(page, position);\n+            if (deleted) {\n+                String path = pair.first().getDeleteFilePath().orElse(null);\n+                if (path != null) {\n+                    return Optional.of(path);\n+                }\n+            }\n+        }\n+        return Optional.empty();\n+    }\n+\n     @Override\n     public void close()\n     {\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFilter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFilter.java\nindex 42122d8103920..eb3f77bbf1f7b 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFilter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFilter.java\n@@ -16,8 +16,11 @@\n import com.facebook.presto.iceberg.IcebergColumnHandle;\n \n import java.util.List;\n+import java.util.Optional;\n \n public interface DeleteFilter\n {\n     RowPredicate createPredicate(List<IcebergColumnHandle> columns);\n+\n+    Optional<String> getDeleteFilePath();\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/EqualityDeleteFilter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/EqualityDeleteFilter.java\nindex c86f0dae9a28d..3d2cd9a5b6bba 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/EqualityDeleteFilter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/EqualityDeleteFilter.java\n@@ -22,7 +22,10 @@\n import org.apache.iceberg.util.StructLikeSet;\n import org.apache.iceberg.util.StructProjection;\n \n+import javax.annotation.Nullable;\n+\n import java.util.List;\n+import java.util.Optional;\n \n import static com.facebook.presto.iceberg.IcebergUtil.schemaFromHandles;\n import static java.util.Objects.requireNonNull;\n@@ -32,11 +35,14 @@ public final class EqualityDeleteFilter\n {\n     private final Schema schema;\n     private final StructLikeSet deleteSet;\n+    @Nullable\n+    private final String deleteFilePath;\n \n-    private EqualityDeleteFilter(Schema schema, StructLikeSet deleteSet)\n+    private EqualityDeleteFilter(Schema schema, StructLikeSet deleteSet, @Nullable String deleteFilePath)\n     {\n         this.schema = requireNonNull(schema, \"schema is null\");\n         this.deleteSet = requireNonNull(deleteSet, \"deleteSet is null\");\n+        this.deleteFilePath = deleteFilePath;\n     }\n \n     @Override\n@@ -55,7 +61,13 @@ public RowPredicate createPredicate(List<IcebergColumnHandle> columns)\n         };\n     }\n \n-    public static DeleteFilter readEqualityDeletes(ConnectorPageSource pageSource, List<IcebergColumnHandle> columns, Schema tableSchema)\n+    @Override\n+    public Optional<String> getDeleteFilePath()\n+    {\n+        return Optional.ofNullable(deleteFilePath);\n+    }\n+\n+    public static DeleteFilter readEqualityDeletes(ConnectorPageSource pageSource, List<IcebergColumnHandle> columns, String deleteFilePath)\n     {\n         Type[] types = columns.stream()\n                 .map(IcebergColumnHandle::getType)\n@@ -75,6 +87,6 @@ public static DeleteFilter readEqualityDeletes(ConnectorPageSource pageSource, L\n             }\n         }\n \n-        return new EqualityDeleteFilter(deleteSchema, deleteSet);\n+        return new EqualityDeleteFilter(deleteSchema, deleteSet, deleteFilePath);\n     }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/PositionDeleteFilter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/PositionDeleteFilter.java\nindex f9fca85d170b2..99a28d016fdb0 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/PositionDeleteFilter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/PositionDeleteFilter.java\n@@ -21,7 +21,10 @@\n import org.roaringbitmap.longlong.ImmutableLongBitmapDataProvider;\n import org.roaringbitmap.longlong.LongBitmapDataProvider;\n \n+import javax.annotation.Nullable;\n+\n import java.util.List;\n+import java.util.Optional;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n@@ -32,10 +35,13 @@ public final class PositionDeleteFilter\n         implements DeleteFilter\n {\n     private final ImmutableLongBitmapDataProvider deletedRows;\n+    @Nullable\n+    private final String deleteFilePath;\n \n-    public PositionDeleteFilter(ImmutableLongBitmapDataProvider deletedRows)\n+    public PositionDeleteFilter(ImmutableLongBitmapDataProvider deletedRows, @Nullable String deleteFilePath)\n     {\n         this.deletedRows = requireNonNull(deletedRows, \"deletedRows is null\");\n+        this.deleteFilePath = deleteFilePath;\n     }\n \n     @Override\n@@ -48,6 +54,11 @@ public RowPredicate createPredicate(List<IcebergColumnHandle> columns)\n         };\n     }\n \n+    public Optional<String> getDeleteFilePath()\n+    {\n+        return Optional.ofNullable(deleteFilePath);\n+    }\n+\n     private static int rowPositionChannel(List<IcebergColumnHandle> columns)\n     {\n         for (int i = 0; i < columns.size(); i++) {\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/RowPredicate.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/RowPredicate.java\nindex 450eccb3f8314..e61bc49294c26 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/RowPredicate.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/RowPredicate.java\n@@ -14,7 +14,11 @@\n package com.facebook.presto.iceberg.delete;\n \n import com.facebook.presto.common.Page;\n+import com.facebook.presto.common.block.Block;\n+import com.facebook.presto.common.block.BlockBuilder;\n+import com.facebook.presto.common.block.RunLengthEncodedBlock;\n \n+import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n import static java.util.Objects.requireNonNull;\n \n public interface RowPredicate\n@@ -43,4 +47,40 @@ default Page filterPage(Page page)\n         }\n         return page.getPositions(retained, 0, retainedCount);\n     }\n+\n+    default Page markDeleted(Page page, int deletedDelegateColumnId)\n+    {\n+        int positionCount = page.getPositionCount();\n+        if (positionCount == 0) {\n+            return page;\n+        }\n+\n+        boolean allSameValues = true;\n+        boolean firstValue = !test(page, 0);\n+        BlockBuilder blockBuilder = null;\n+        for (int position = 1; position < positionCount; position++) {\n+            boolean deleted = !test(page, position);\n+            if (allSameValues && deleted != firstValue) {\n+                blockBuilder = BOOLEAN.createFixedSizeBlockBuilder(positionCount);\n+                for (int idx = 0; idx < position; idx++) {\n+                    BOOLEAN.writeBoolean(blockBuilder, firstValue);\n+                }\n+                BOOLEAN.writeBoolean(blockBuilder, deleted);\n+                allSameValues = false;\n+            }\n+            else if (!allSameValues) {\n+                BOOLEAN.writeBoolean(blockBuilder, deleted);\n+            }\n+        }\n+\n+        Block block;\n+        if (blockBuilder != null) {\n+            block = blockBuilder.build();\n+        }\n+        else {\n+            block = RunLengthEncodedBlock.create(BOOLEAN, firstValue, positionCount);\n+        }\n+\n+        return page.replaceColumn(deletedDelegateColumnId, block);\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\nindex 8a2bbba1c2423..caf9c6075c0c3 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/optimizer/IcebergEqualityDeleteAsJoin.java\n@@ -83,6 +83,8 @@\n import static com.facebook.presto.iceberg.FileContent.EQUALITY_DELETES;\n import static com.facebook.presto.iceberg.FileContent.fromIcebergFileContent;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.DATA_SEQUENCE_NUMBER_COLUMN_HANDLE;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.DELETE_FILE_PATH_COLUMN_HANDLE;\n+import static com.facebook.presto.iceberg.IcebergColumnHandle.IS_DELETED_COLUMN_HANDLE;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.DATA_SEQUENCE_NUMBER;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.isDeleteToJoinPushdownEnabled;\n@@ -175,6 +177,11 @@ public PlanNode visitTableScan(TableScanNode node, RewriteContext<Void> context)\n                 return node;\n             }\n \n+            if (node.getAssignments().containsValue(IS_DELETED_COLUMN_HANDLE) || node.getAssignments().containsValue(DELETE_FILE_PATH_COLUMN_HANDLE)) {\n+                // Skip this optimization if metadata columns `$deleted` or `$delete_file_path` exist\n+                return node;\n+            }\n+\n             IcebergAbstractMetadata metadata = (IcebergAbstractMetadata) transactionManager.get(table.getTransaction());\n             Table icebergTable = getIcebergTable(metadata, session, icebergTableHandle.getSchemaTableName());\n \n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\nindex 5e30f9bdaab72..dc45f9a8030e7 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n@@ -2046,6 +2046,60 @@ public void testHiddenColumns()\n         testDataSequenceNumberHiddenColumn();\n     }\n \n+    @Test\n+    public void testDeletedHiddenColumn()\n+    {\n+        assertUpdate(\"DROP TABLE IF EXISTS test_deleted\");\n+        assertUpdate(\"CREATE TABLE test_deleted AS SELECT * FROM tpch.tiny.region WHERE regionkey=0\", 1);\n+        assertUpdate(\"INSERT INTO test_deleted SELECT * FROM tpch.tiny.region WHERE regionkey=1\", 1);\n+\n+        assertQuery(\"SELECT \\\"$deleted\\\" FROM test_deleted\", format(\"VALUES %s, %s\", \"false\", \"false\"));\n+\n+        assertUpdate(\"DELETE FROM test_deleted WHERE regionkey=1\", 1);\n+        assertEquals(computeActual(\"SELECT * FROM test_deleted\").getRowCount(), 1);\n+        assertQuery(\"SELECT \\\"$deleted\\\" FROM test_deleted ORDER BY \\\"$deleted\\\"\", format(\"VALUES %s, %s\", \"false\", \"true\"));\n+    }\n+\n+    @Test\n+    public void testDeleteFilePathHiddenColumn()\n+    {\n+        assertUpdate(\"DROP TABLE IF EXISTS test_delete_file_path\");\n+        assertUpdate(\"CREATE TABLE test_delete_file_path AS SELECT * FROM tpch.tiny.region WHERE regionkey=0\", 1);\n+        assertUpdate(\"INSERT INTO test_delete_file_path SELECT * FROM tpch.tiny.region WHERE regionkey=1\", 1);\n+\n+        assertQuery(\"SELECT \\\"$delete_file_path\\\" FROM test_delete_file_path\", format(\"VALUES %s, %s\", \"NULL\", \"NULL\"));\n+\n+        assertUpdate(\"DELETE FROM test_delete_file_path WHERE regionkey=1\", 1);\n+        assertEquals(computeActual(\"SELECT * FROM test_delete_file_path\").getRowCount(), 1);\n+        assertEquals(computeActual(\"SELECT \\\"$delete_file_path\\\" FROM test_delete_file_path\").getRowCount(), 2);\n+\n+        assertUpdate(\"DELETE FROM test_delete_file_path WHERE regionkey=0\", 1);\n+        computeActual(\"SELECT \\\"$delete_file_path\\\" FROM test_delete_file_path\").getMaterializedRows().forEach(row -> {\n+            assertEquals(row.getFieldCount(), 1);\n+            assertNotNull(row.getField(0));\n+        });\n+    }\n+\n+    @Test(dataProvider = \"equalityDeleteOptions\")\n+    public void testEqualityDeletesWithDeletedHiddenColumn(String fileFormat, boolean joinRewriteEnabled)\n+            throws Exception\n+    {\n+        Session session = deleteAsJoinEnabled(joinRewriteEnabled);\n+        String tableName = \"test_v2_row_delete_\" + randomTableSuffix();\n+        assertUpdate(\"CREATE TABLE \" + tableName + \"(id int, data varchar) WITH (\\\"write.format.default\\\" = '\" + fileFormat + \"')\");\n+        assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+\n+        Table icebergTable = updateTable(tableName);\n+        writeEqualityDeleteToNationTable(icebergTable, ImmutableMap.of(\"id\", 1));\n+\n+        assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'b'), (2, 'a'), (3, 'a')\", 3);\n+\n+        assertQuery(session, \"SELECT * FROM \" + tableName, \"VALUES (1, 'b'), (2, 'a'), (3, 'a')\");\n+\n+        assertQuery(session, \"SELECT \\\"$deleted\\\", * FROM \" + tableName,\n+                \"VALUES (true, 1, 'a'), (false, 1, 'b'), (false, 2, 'a'), (false, 3, 'a')\");\n+    }\n+\n     @DataProvider(name = \"pushdownFilterEnabled\")\n     public Object[][] pushdownFilterEnabledProvider()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25270",
    "pr_id": 25270,
    "issue_id": 23448,
    "repo": "prestodb/presto",
    "problem_statement": "TestEventListener.testPrepareAndExecute is flaky\nExample failure: https://github.com/prestodb/presto/actions/runs/10393975339/job/28782708706?pr=23447\r\n```\r\nError:  Tests run: 76, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 350.615 s <<< FAILURE! - in TestSuite\r\nError:  com.facebook.presto.execution.TestEventListener.testPrepareAndExecute  Time elapsed: 1.419 s  <<< FAILURE!\r\njava.lang.AssertionError: expected object to not be null\r\n\tat org.testng.Assert.fail(Assert.java:110)\r\n\tat org.testng.Assert.assertNotNull(Assert.java:1319)\r\n\tat org.testng.Assert.assertNotNull(Assert.java:1303)\r\n\tat com.facebook.presto.execution.TestEventListener.testPrepareAndExecute(TestEventListener.java:227)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```",
    "issue_word_count": 232,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java"
    ],
    "pr_changed_test_files": [
      "presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java"
    ],
    "base_commit": "cfbf6f301e39d7c8aeef8a364503e6cc576ca849",
    "head_commit": "9a04da8b9e7ddf78c10edf09ffe77da8d6385eef",
    "repo_url": "https://github.com/prestodb/presto/pull/25270",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25270",
    "dockerfile": "",
    "pr_merged_at": "2025-06-09T23:25:49.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java b/presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java\nindex 8137940444408..b6a31b743cf10 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/execution/TestEventListener.java\n@@ -340,7 +340,7 @@ public static class EventsBuilder\n         private ImmutableList.Builder<QueryCreatedEvent> queryCreatedEvents;\n         private ImmutableList.Builder<QueryCompletedEvent> queryCompletedEvents;\n         private ImmutableList.Builder<SplitCompletedEvent> splitCompletedEvents;\n-        private QueryProgressEvent queryProgressEvent;\n+        private volatile QueryProgressEvent queryProgressEvent;\n \n         private CountDownLatch eventsLatch;\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25261",
    "pr_id": 25261,
    "issue_id": 24733,
    "repo": "prestodb/presto",
    "problem_statement": "Support more Iceberg Metadata columns\nhttps://prestodb.io/docs/current/connector/iceberg.html#extra-hidden-metadata-columns\nSo far Presto only supports querying the $path and $data_sequence_number metadata columns. We will need to add more columns. For more see https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html. \nThe first several we should support\n\n- [DELETE_FILE_PATH](https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html#DELETE_FILE_PATH)\n- [IS_DELETED](https://iceberg.apache.org/javadoc/1.8.1/org/apache/iceberg/MetadataColumns.html#IS_DELETED)\n\n## Context\nThese columns are important for debugging ",
    "issue_word_count": 95,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-common/src/main/java/com/facebook/presto/common/Page.java",
      "presto-common/src/test/java/com/facebook/presto/common/TestPage.java"
    ],
    "pr_changed_test_files": [
      "presto-common/src/test/java/com/facebook/presto/common/TestPage.java"
    ],
    "base_commit": "6c7de58678b3ec0df8729156d2f9a735d8f01079",
    "head_commit": "c41746c69d303d3968cacfaef6825cec8eaf8357",
    "repo_url": "https://github.com/prestodb/presto/pull/25261",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25261",
    "dockerfile": "",
    "pr_merged_at": "2025-06-07T09:28:50.000Z",
    "patch": "diff --git a/presto-common/src/main/java/com/facebook/presto/common/Page.java b/presto-common/src/main/java/com/facebook/presto/common/Page.java\nindex 2e941461dca9f..8eb4fe7550cc5 100644\n--- a/presto-common/src/main/java/com/facebook/presto/common/Page.java\n+++ b/presto-common/src/main/java/com/facebook/presto/common/Page.java\n@@ -471,7 +471,7 @@ public Page replaceColumn(int channelIndex, Block column)\n \n         Block[] newBlocks = Arrays.copyOf(blocks, blocks.length);\n         newBlocks[channelIndex] = column;\n-        return Page.wrapBlocksWithoutCopy(newBlocks.length, newBlocks);\n+        return Page.wrapBlocksWithoutCopy(positionCount, newBlocks);\n     }\n \n     private static class DictionaryBlockIndexes\n",
    "test_patch": "diff --git a/presto-common/src/test/java/com/facebook/presto/common/TestPage.java b/presto-common/src/test/java/com/facebook/presto/common/TestPage.java\nindex 5a5deb8fbc09e..c7b048a157763 100644\n--- a/presto-common/src/test/java/com/facebook/presto/common/TestPage.java\n+++ b/presto-common/src/test/java/com/facebook/presto/common/TestPage.java\n@@ -182,6 +182,7 @@ public void testReplaceColumn()\n         Page newPage = page.replaceColumn(1, newBlock);\n \n         assertEquals(newPage.getChannelCount(), 3);\n+        assertEquals(newPage.getPositionCount(), entries);\n         assertEquals(newPage.getBlock(1).getLong(0), 0);\n         assertEquals(newPage.getBlock(1).getLong(1), -1);\n     }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25253",
    "pr_id": 25253,
    "issue_id": 25108,
    "repo": "prestodb/presto",
    "problem_statement": "[Native] Change PrestoIcebergPartitionSpec for better code reuse in velox\n<!--- Provide a general summary of the issue in the Title above -->\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used:\n* Storage (HDFS/S3/GCS..):\n* Data source and connector used:\n* Deployment (Cloud or On-prem):\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\n\n## Expected Behavior\n<!--- Tell us what should happen -->\n\n## Current Behavior\n<!--- Tell us what happens instead of the expected behavior -->\n\n## Possible Solution\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\n\n## Steps to Reproduce\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\n1.\n2.\n3.\n4.\n\n## Screenshots (if appropriate)\n\n## Context\n<!--- How has this issue affected you? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\n\nThe existing PrestoIcebergPartitionSpec class stores the iceberg table partition information as a string. But actually before constructing this object the partition transform has been parsed and converted to org.apache.iceberg.PartitionSpec. And in Java, it converts org.apache.iceberg.PartitionSpec to string again, and when this fields been passed to velox, it needs to be passed again (to construct the partition type, column etc.).\n \nWe can simply pass the column name, column Id, partition type and schema to velox.\n\n\n",
    "issue_word_count": 248,
    "test_files_count": 2,
    "non_test_files_count": 8,
    "pr_changed_files": [
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPartitionField.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionFields.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionSpecConverter.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionTransformType.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java",
      "presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.cpp",
      "presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.h",
      "presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.yml"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java"
    ],
    "base_commit": "f19fee9c66cc984b8b2cf09d610c0f212dc55713",
    "head_commit": "1260f9f22fce3cd44720a39727b0a2328afbe9f9",
    "repo_url": "https://github.com/prestodb/presto/pull/25253",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25253",
    "dockerfile": "",
    "pr_merged_at": "2025-06-19T09:24:51.000Z",
    "patch": "diff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPartitionField.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPartitionField.java\nnew file mode 100644\nindex 0000000000000..9fd48b4474488\n--- /dev/null\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPartitionField.java\n@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.facebook.presto.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Objects;\n+import java.util.OptionalInt;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergPartitionField\n+{\n+    private final int sourceId;\n+    private final int fieldId;\n+    private final OptionalInt parameter;\n+    private final PartitionTransformType transform;\n+    private final String name;\n+\n+    @JsonCreator\n+    public IcebergPartitionField(\n+            @JsonProperty(\"sourceId\") int sourceId,\n+            @JsonProperty(\"fieldId\") int fieldId,\n+            @JsonProperty(\"parameter\") OptionalInt parameter,\n+            @JsonProperty(\"transform\") PartitionTransformType transform,\n+            @JsonProperty(\"name\") String name)\n+    {\n+        this.sourceId = sourceId;\n+        this.fieldId = fieldId;\n+        this.parameter = requireNonNull(parameter, \"parameter is null\");\n+        this.transform = requireNonNull(transform, \"transform is null\");\n+        this.name = requireNonNull(name, \"name is null\");\n+    }\n+\n+    @JsonProperty\n+    public int getSourceId()\n+    {\n+        return sourceId;\n+    }\n+\n+    @JsonProperty\n+    public int getFieldId()\n+    {\n+        return fieldId;\n+    }\n+\n+    @JsonProperty\n+    public OptionalInt getParameter()\n+    {\n+        return parameter;\n+    }\n+\n+    @JsonProperty\n+    public PartitionTransformType getTransform()\n+    {\n+        return transform;\n+    }\n+\n+    @JsonProperty\n+    public String getName()\n+    {\n+        return name;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o)\n+    {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        IcebergPartitionField that = (IcebergPartitionField) o;\n+        return transform == that.transform &&\n+                Objects.equals(name, that.name) &&\n+                sourceId == that.sourceId &&\n+                fieldId == that.fieldId &&\n+                parameter == that.parameter;\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(sourceId, fieldId, parameter, transform, name);\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return \"IcebergPartitionField{\" +\n+                \"sourceId=\" + sourceId +\n+                \", fieldId=\" + fieldId +\n+                \", parameter=\" + (parameter.isPresent() ? String.valueOf(parameter.getAsInt()) : \"null\") +\n+                \", transform=\" + transform +\n+                \", name='\" + name + '\\'' +\n+                '}';\n+    }\n+\n+    public static Builder builder()\n+    {\n+        return new Builder();\n+    }\n+\n+    public static class Builder\n+    {\n+        private int sourceId;\n+        private int fieldId;\n+        private OptionalInt parameter;\n+        private PartitionTransformType transform;\n+        private String name;\n+\n+        public Builder setSourceId(int sourceId)\n+        {\n+            this.sourceId = sourceId;\n+            return this;\n+        }\n+\n+        public Builder setFieldId(int fieldId)\n+        {\n+            this.fieldId = fieldId;\n+            return this;\n+        }\n+\n+        public Builder setTransform(PartitionTransformType transform)\n+        {\n+            this.transform = transform;\n+            return this;\n+        }\n+\n+        public Builder setName(String name)\n+        {\n+            this.name = name;\n+            return this;\n+        }\n+\n+        public Builder setParameter(OptionalInt parameter)\n+        {\n+            this.parameter = parameter;\n+            return this;\n+        }\n+\n+        public IcebergPartitionField build()\n+        {\n+            return new IcebergPartitionField(sourceId, fieldId, parameter == null ? OptionalInt.empty() : parameter, transform, name);\n+        }\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionFields.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionFields.java\nindex eb6a2463a50ff..7668efea0f791 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionFields.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionFields.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.iceberg;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.PartitionField;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n@@ -22,6 +23,7 @@\n \n import java.util.List;\n import java.util.Optional;\n+import java.util.OptionalInt;\n import java.util.function.Consumer;\n import java.util.regex.MatchResult;\n import java.util.regex.Matcher;\n@@ -78,12 +80,25 @@ public static PartitionSpec parsePartitionFields(Schema schema, List<String> fie\n                 .orElseGet(() -> PartitionSpec.builderFor(schema));\n \n         for (String field : fields) {\n-            parsePartitionField(builder, field);\n+            buildPartitionField(builder, field);\n         }\n         return builder.build();\n     }\n \n-    public static void parsePartitionField(PartitionSpec.Builder builder, String field)\n+    public static PartitionSpec parseIcebergPartitionFields(Schema schema, List<IcebergPartitionField> fields, @Nullable Integer specId)\n+    {\n+        PartitionSpec.Builder builder = Optional.ofNullable(specId)\n+                .map(id -> PartitionSpec.builderFor(schema).withSpecId(id))\n+                .orElseGet(() -> PartitionSpec.builderFor(schema));\n+\n+        for (IcebergPartitionField field : fields) {\n+            buildPartitionSpec(builder, field);\n+        }\n+        return builder.build();\n+    }\n+\n+    @VisibleForTesting\n+    static void buildPartitionField(PartitionSpec.Builder builder, String field)\n     {\n         @SuppressWarnings(\"PointlessBooleanExpression\")\n         boolean matched = false ||\n@@ -99,6 +114,35 @@ public static void parsePartitionField(PartitionSpec.Builder builder, String fie\n         }\n     }\n \n+    private static void buildPartitionSpec(PartitionSpec.Builder builder, IcebergPartitionField partitionField)\n+    {\n+        String field = partitionField.getName();\n+        PartitionTransformType type = partitionField.getTransform();\n+        OptionalInt parameter = partitionField.getParameter();\n+        switch (type) {\n+            case IDENTITY:\n+                builder.identity(field);\n+                break;\n+            case YEAR:\n+                builder.year(field);\n+                break;\n+            case MONTH:\n+                builder.month(field);\n+                break;\n+            case DAY:\n+                builder.day(field);\n+                break;\n+            case HOUR:\n+                builder.hour(field);\n+                break;\n+            case BUCKET:\n+                builder.bucket(field, parameter.getAsInt());\n+                break;\n+            case TRUNCATE:\n+                builder.truncate(field, parameter.getAsInt());\n+        }\n+    }\n+\n     private static boolean tryMatch(CharSequence value, Pattern pattern, Consumer<MatchResult> match)\n     {\n         Matcher matcher = pattern.matcher(value);\n@@ -116,6 +160,41 @@ public static List<String> toPartitionFields(PartitionSpec spec)\n                 .collect(toImmutableList());\n     }\n \n+    private static String toPartitionField(PartitionSpec spec, PartitionField field)\n+    {\n+        String name = spec.schema().findColumnName(field.sourceId());\n+        String transform = field.transform().toString();\n+\n+        switch (transform) {\n+            case \"identity\":\n+                return name;\n+            case \"year\":\n+            case \"month\":\n+            case \"day\":\n+            case \"hour\":\n+                return format(\"%s(%s)\", transform, name);\n+        }\n+\n+        Matcher matcher = ICEBERG_BUCKET_PATTERN.matcher(transform);\n+        if (matcher.matches()) {\n+            return format(\"bucket(%s, %s)\", name, matcher.group(1));\n+        }\n+\n+        matcher = ICEBERG_TRUNCATE_PATTERN.matcher(transform);\n+        if (matcher.matches()) {\n+            return format(\"truncate(%s, %s)\", name, matcher.group(1));\n+        }\n+\n+        throw new UnsupportedOperationException(\"Unsupported partition transform: \" + field);\n+    }\n+\n+    public static List<IcebergPartitionField> toIcebergPartitionFields(PartitionSpec spec)\n+    {\n+        return spec.fields().stream()\n+                .map(field -> toIcebergPartitionField(spec, field))\n+                .collect(toImmutableList());\n+    }\n+\n     // Keep consistency with PartitionSpec.Builder\n     protected static String getPartitionColumnName(String columnName, String transform)\n     {\n@@ -170,32 +249,23 @@ protected static Term getTransformTerm(String columnName, String transform)\n         throw new UnsupportedOperationException(\"Unknown partition transform: \" + transform);\n     }\n \n-    private static String toPartitionField(PartitionSpec spec, PartitionField field)\n+    private static IcebergPartitionField toIcebergPartitionField(PartitionSpec spec, PartitionField field)\n     {\n         String name = spec.schema().findColumnName(field.sourceId());\n         String transform = field.transform().toString();\n-\n-        switch (transform) {\n-            case \"identity\":\n-                return name;\n-            case \"year\":\n-            case \"month\":\n-            case \"day\":\n-            case \"hour\":\n-                return format(\"%s(%s)\", transform, name);\n-        }\n-\n+        IcebergPartitionField.Builder builder = IcebergPartitionField.builder();\n+        builder.setTransform(PartitionTransformType.fromStringOrFail(transform)).setFieldId(field.fieldId()).setSourceId(field.sourceId()).setName(name);\n         Matcher matcher = ICEBERG_BUCKET_PATTERN.matcher(transform);\n         if (matcher.matches()) {\n-            return format(\"bucket(%s, %s)\", name, matcher.group(1));\n+            builder.setParameter(OptionalInt.of(Integer.parseInt(matcher.group(1))));\n+            return builder.build();\n         }\n-\n         matcher = ICEBERG_TRUNCATE_PATTERN.matcher(transform);\n         if (matcher.matches()) {\n-            return format(\"truncate(%s, %s)\", name, matcher.group(1));\n+            builder.setParameter(OptionalInt.of(Integer.parseInt(matcher.group(1))));\n+            return builder.build();\n         }\n-\n-        throw new UnsupportedOperationException(\"Unsupported partition transform: \" + field);\n+        return builder.build();\n     }\n \n     public static String quotedName(String name)\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionSpecConverter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionSpecConverter.java\nindex 921168870dc9e..cc191ca6f1be0 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionSpecConverter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionSpecConverter.java\n@@ -16,8 +16,8 @@\n import com.facebook.presto.common.type.TypeManager;\n import org.apache.iceberg.PartitionSpec;\n \n-import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n-import static com.facebook.presto.iceberg.PartitionFields.toPartitionFields;\n+import static com.facebook.presto.iceberg.PartitionFields.parseIcebergPartitionFields;\n+import static com.facebook.presto.iceberg.PartitionFields.toIcebergPartitionFields;\n import static com.facebook.presto.iceberg.SchemaConverter.toIcebergSchema;\n import static com.facebook.presto.iceberg.SchemaConverter.toPrestoSchema;\n \n@@ -30,12 +30,12 @@ public static PrestoIcebergPartitionSpec toPrestoPartitionSpec(PartitionSpec spe\n         return new PrestoIcebergPartitionSpec(\n                 spec.specId(),\n                 toPrestoSchema(spec.schema(), typeManager),\n-                toPartitionFields(spec));\n+                toIcebergPartitionFields(spec));\n     }\n \n     public static PartitionSpec toIcebergPartitionSpec(PrestoIcebergPartitionSpec spec)\n     {\n-        return parsePartitionFields(\n+        return parseIcebergPartitionFields(\n                 toIcebergSchema(spec.getSchema()),\n                 spec.getFields(),\n                 spec.getSpecId());\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionTransformType.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionTransformType.java\nnew file mode 100644\nindex 0000000000000..f72c8dd061158\n--- /dev/null\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionTransformType.java\n@@ -0,0 +1,101 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+public enum PartitionTransformType\n+{\n+    IDENTITY(\"identity\", 0),\n+    YEAR(\"year\", 1),\n+    MONTH(\"month\", 2),\n+    DAY(\"day\", 3),\n+    HOUR(\"hour\", 4),\n+    BUCKET(\"bucket\", 5),\n+    TRUNCATE(\"truncate\", 6);\n+\n+    private static final Map<String, PartitionTransformType> TRANSFORM_MAP = new HashMap<>();\n+    private static final Map<Integer, PartitionTransformType> CODE_MAP = new HashMap<>();\n+\n+    static {\n+        for (PartitionTransformType type : values()) {\n+            TRANSFORM_MAP.put(type.transform, type);\n+            CODE_MAP.put(type.code, type);\n+        }\n+    }\n+\n+    private final String transform;\n+    private final int code;\n+\n+    PartitionTransformType(String transform, int code)\n+    {\n+        this.transform = transform;\n+        this.code = code;\n+    }\n+\n+    public String getTransform()\n+    {\n+        return transform;\n+    }\n+\n+    public int getCode()\n+    {\n+        return code;\n+    }\n+\n+    public static Optional<PartitionTransformType> fromString(String transform)\n+    {\n+        if (transform == null) {\n+            return Optional.empty();\n+        }\n+\n+        PartitionTransformType type = TRANSFORM_MAP.get(transform);\n+        if (type != null) {\n+            return Optional.of(type);\n+        }\n+\n+        // Handle bucket and truncate transforms with parameters\n+        if (transform.startsWith(BUCKET.transform + \"[\")) {\n+            return Optional.of(BUCKET);\n+        }\n+        if (transform.startsWith(TRUNCATE.transform + \"[\")) {\n+            return Optional.of(TRUNCATE);\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    public static PartitionTransformType fromCode(int code)\n+    {\n+        PartitionTransformType type = CODE_MAP.get(code);\n+        if (type == null) {\n+            throw new IllegalArgumentException(\"Unknown transform code: \" + code);\n+        }\n+        return type;\n+    }\n+\n+    public static PartitionTransformType fromStringOrFail(String transform)\n+    {\n+        return fromString(transform)\n+                .orElseThrow(() -> new IllegalArgumentException(\"Unsupported transform type: \" + transform));\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return transform;\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java\nindex 119a819868f2f..38039b2643568 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java\n@@ -26,13 +26,13 @@ public class PrestoIcebergPartitionSpec\n {\n     private final int specId;\n     private final PrestoIcebergSchema schema;\n-    private final List<String> fields;\n+    private final List<IcebergPartitionField> fields;\n \n     @JsonCreator\n     public PrestoIcebergPartitionSpec(\n             @JsonProperty(\"specId\") int specId,\n             @JsonProperty(\"schema\") PrestoIcebergSchema schema,\n-            @JsonProperty(\"fields\") List<String> fields)\n+            @JsonProperty(\"fields\") List<IcebergPartitionField> fields)\n     {\n         this.specId = specId;\n         this.schema = requireNonNull(schema, \"schema is null\");\n@@ -52,7 +52,7 @@ public PrestoIcebergSchema getSchema()\n     }\n \n     @JsonProperty\n-    public List<String> getFields()\n+    public List<IcebergPartitionField> getFields()\n     {\n         return fields;\n     }\n\ndiff --git a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.cpp b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.cpp\nindex fdfed953d088c..c8e6cfef182f3 100644\n--- a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.cpp\n+++ b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.cpp\n@@ -504,6 +504,88 @@ void from_json(const json& j, IcebergTableName& p) {\n }\n } // namespace facebook::presto::protocol::iceberg\n namespace facebook::presto::protocol::iceberg {\n+// Loosly copied this here from NLOHMANN_JSON_SERIALIZE_ENUM()\n+\n+// NOLINTNEXTLINE: cppcoreguidelines-avoid-c-arrays\n+static const std::pair<PartitionTransformType, json>\n+    PartitionTransformType_enum_table[] =\n+        { // NOLINT: cert-err58-cpp\n+            {PartitionTransformType::IDENTITY, \"IDENTITY\"},\n+            {PartitionTransformType::YEAR, \"YEAR\"},\n+            {PartitionTransformType::MONTH, \"MONTH\"},\n+            {PartitionTransformType::DAY, \"DAY\"},\n+            {PartitionTransformType::HOUR, \"HOUR\"},\n+            {PartitionTransformType::BUCKET, \"BUCKET\"},\n+            {PartitionTransformType::TRUNCATE, \"TRUNCATE\"}};\n+void to_json(json& j, const PartitionTransformType& e) {\n+  static_assert(\n+      std::is_enum<PartitionTransformType>::value,\n+      \"PartitionTransformType must be an enum!\");\n+  const auto* it = std::find_if(\n+      std::begin(PartitionTransformType_enum_table),\n+      std::end(PartitionTransformType_enum_table),\n+      [e](const std::pair<PartitionTransformType, json>& ej_pair) -> bool {\n+        return ej_pair.first == e;\n+      });\n+  j = ((it != std::end(PartitionTransformType_enum_table))\n+           ? it\n+           : std::begin(PartitionTransformType_enum_table))\n+          ->second;\n+}\n+void from_json(const json& j, PartitionTransformType& e) {\n+  static_assert(\n+      std::is_enum<PartitionTransformType>::value,\n+      \"PartitionTransformType must be an enum!\");\n+  const auto* it = std::find_if(\n+      std::begin(PartitionTransformType_enum_table),\n+      std::end(PartitionTransformType_enum_table),\n+      [&j](const std::pair<PartitionTransformType, json>& ej_pair) -> bool {\n+        return ej_pair.second == j;\n+      });\n+  e = ((it != std::end(PartitionTransformType_enum_table))\n+           ? it\n+           : std::begin(PartitionTransformType_enum_table))\n+          ->first;\n+}\n+} // namespace facebook::presto::protocol::iceberg\n+namespace facebook::presto::protocol::iceberg {\n+\n+void to_json(json& j, const IcebergPartitionField& p) {\n+  j = json::object();\n+  to_json_key(\n+      j, \"sourceId\", p.sourceId, \"IcebergPartitionField\", \"int\", \"sourceId\");\n+  to_json_key(\n+      j, \"fieldId\", p.fieldId, \"IcebergPartitionField\", \"int\", \"fieldId\");\n+  to_json_key(\n+      j, \"parameter\", p.parameter, \"IcebergPartitionField\", \"int\", \"parameter\");\n+  to_json_key(\n+      j,\n+      \"transform\",\n+      p.transform,\n+      \"IcebergPartitionField\",\n+      \"PartitionTransformType\",\n+      \"transform\");\n+  to_json_key(j, \"name\", p.name, \"IcebergPartitionField\", \"String\", \"name\");\n+}\n+\n+void from_json(const json& j, IcebergPartitionField& p) {\n+  from_json_key(\n+      j, \"sourceId\", p.sourceId, \"IcebergPartitionField\", \"int\", \"sourceId\");\n+  from_json_key(\n+      j, \"fieldId\", p.fieldId, \"IcebergPartitionField\", \"int\", \"fieldId\");\n+  from_json_key(\n+      j, \"parameter\", p.parameter, \"IcebergPartitionField\", \"int\", \"parameter\");\n+  from_json_key(\n+      j,\n+      \"transform\",\n+      p.transform,\n+      \"IcebergPartitionField\",\n+      \"PartitionTransformType\",\n+      \"transform\");\n+  from_json_key(j, \"name\", p.name, \"IcebergPartitionField\", \"String\", \"name\");\n+}\n+} // namespace facebook::presto::protocol::iceberg\n+namespace facebook::presto::protocol::iceberg {\n \n void to_json(json& j, const PrestoIcebergNestedField& p) {\n   j = json::object();\n@@ -634,7 +716,7 @@ void to_json(json& j, const PrestoIcebergPartitionSpec& p) {\n       \"fields\",\n       p.fields,\n       \"PrestoIcebergPartitionSpec\",\n-      \"List<String>\",\n+      \"List<IcebergPartitionField>\",\n       \"fields\");\n }\n \n@@ -653,7 +735,7 @@ void from_json(const json& j, PrestoIcebergPartitionSpec& p) {\n       \"fields\",\n       p.fields,\n       \"PrestoIcebergPartitionSpec\",\n-      \"List<String>\",\n+      \"List<IcebergPartitionField>\",\n       \"fields\");\n }\n } // namespace facebook::presto::protocol::iceberg\n\ndiff --git a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.h b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.h\nindex 76193419e26d2..a4048e0f6707c 100644\n--- a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.h\n+++ b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.h\n@@ -129,6 +129,30 @@ void to_json(json& j, const IcebergTableName& p);\n void from_json(const json& j, IcebergTableName& p);\n } // namespace facebook::presto::protocol::iceberg\n namespace facebook::presto::protocol::iceberg {\n+enum class PartitionTransformType {\n+  IDENTITY,\n+  YEAR,\n+  MONTH,\n+  DAY,\n+  HOUR,\n+  BUCKET,\n+  TRUNCATE\n+};\n+extern void to_json(json& j, const PartitionTransformType& e);\n+extern void from_json(const json& j, PartitionTransformType& e);\n+} // namespace facebook::presto::protocol::iceberg\n+namespace facebook::presto::protocol::iceberg {\n+struct IcebergPartitionField {\n+  int sourceId = {};\n+  int fieldId = {};\n+  std::shared_ptr<int> parameter = {};\n+  PartitionTransformType transform = {};\n+  String name = {};\n+};\n+void to_json(json& j, const IcebergPartitionField& p);\n+void from_json(const json& j, IcebergPartitionField& p);\n+} // namespace facebook::presto::protocol::iceberg\n+namespace facebook::presto::protocol::iceberg {\n struct PrestoIcebergNestedField {\n   bool optional = {};\n   int id = {};\n@@ -154,7 +178,7 @@ namespace facebook::presto::protocol::iceberg {\n struct PrestoIcebergPartitionSpec {\n   int specId = {};\n   PrestoIcebergSchema schema = {};\n-  List<String> fields = {};\n+  List<IcebergPartitionField> fields = {};\n };\n void to_json(json& j, const PrestoIcebergPartitionSpec& p);\n void from_json(const json& j, PrestoIcebergPartitionSpec& p);\n\ndiff --git a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.yml b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.yml\nindex 197b713b25ae0..c66f1294de180 100644\n--- a/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.yml\n+++ b/presto-native-execution/presto_cpp/presto_protocol/connector/iceberg/presto_protocol_iceberg.yml\n@@ -69,6 +69,8 @@ JavaClasses:\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergInsertTableHandle.java\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/ColumnIdentity.java\n+  - presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPartitionField.java\n+  - presto-iceberg/src/main/java/com/facebook/presto/iceberg/PartitionTransformType.java\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergPartitionSpec.java\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/PrestoIcebergNestedField.java\n   - presto-iceberg/src/main/java/com/facebook/presto/iceberg/delete/DeleteFile.java\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java\nindex 1f3a859e2a342..29f9705d40408 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionFields.java\n@@ -26,10 +26,8 @@\n \n import java.util.function.Consumer;\n \n-import static com.facebook.presto.iceberg.PartitionFields.parsePartitionField;\n-import static com.facebook.presto.iceberg.PartitionFields.toPartitionFields;\n+import static com.facebook.presto.iceberg.PartitionFields.buildPartitionField;\n import static com.facebook.presto.testing.assertions.Assert.assertEquals;\n-import static com.google.common.collect.Iterables.getOnlyElement;\n import static org.assertj.core.api.AssertionsForClassTypes.assertThatThrownBy;\n \n public class TestPartitionFields\n@@ -60,7 +58,6 @@ private static void assertParse(String value, PartitionSpec expected)\n     {\n         assertEquals(expected.fields().size(), 1);\n         assertEquals(parseField(value), expected);\n-        assertEquals(getOnlyElement(toPartitionFields(expected)), value);\n     }\n \n     private static void assertInvalid(String value, String message)\n@@ -75,7 +72,7 @@ private static void assertInvalid(String value, String message)\n \n     private static PartitionSpec parseField(String value)\n     {\n-        return partitionSpec(builder -> parsePartitionField(builder, value));\n+        return partitionSpec(builder -> buildPartitionField(builder, value));\n     }\n \n     private static PartitionSpec partitionSpec(Consumer<PartitionSpec.Builder> consumer)\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java\nindex 41b3556956820..938819231b4f4 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestPartitionSpecConverter.java\n@@ -20,13 +20,13 @@\n \n import java.util.ArrayList;\n import java.util.List;\n+import java.util.OptionalInt;\n \n import static com.facebook.presto.iceberg.PartitionSpecConverter.toIcebergPartitionSpec;\n import static com.facebook.presto.iceberg.PartitionSpecConverter.toPrestoPartitionSpec;\n import static com.facebook.presto.iceberg.TestSchemaConverter.prestoIcebergSchema;\n import static com.facebook.presto.iceberg.TestSchemaConverter.schema;\n import static com.facebook.presto.metadata.FunctionAndTypeManager.createTestFunctionAndTypeManager;\n-import static java.lang.String.format;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertNotNull;\n import static org.testng.Assert.assertTrue;\n@@ -46,26 +46,6 @@ public static Object[][] testAllTransforms()\n         };\n     }\n \n-    @Test(dataProvider = \"allTransforms\")\n-    public void testToPrestoPartitionSpec(String transform, String name)\n-    {\n-        // Create a test TypeManager\n-        TypeManager typeManager = createTestFunctionAndTypeManager();\n-\n-        // Create a mock PartitionSpec\n-        PartitionSpec partitionSpec = partitionSpec(transform, name);\n-\n-        PrestoIcebergPartitionSpec expectedPrestoPartitionSpec = prestoIcebergPartitionSpec(transform, name, typeManager);\n-\n-        // Convert Iceberg PartitionSpec to Presto Iceberg Partition Spec\n-        PrestoIcebergPartitionSpec prestoIcebergPartitionSpec = toPrestoPartitionSpec(partitionSpec, typeManager);\n-\n-        // Check that the result is not null\n-        assertNotNull(prestoIcebergPartitionSpec);\n-\n-        assertEquals(prestoIcebergPartitionSpec, expectedPrestoPartitionSpec);\n-    }\n-\n     @Test(dataProvider = \"allTransforms\")\n     public void testToIcebergPartitionSpec(String transform, String name)\n     {\n@@ -110,23 +90,35 @@ public void validateConversion(String transform, String name)\n \n     private static PrestoIcebergPartitionSpec prestoIcebergPartitionSpec(String transform, String name, TypeManager typeManager)\n     {\n-        List<String> fields = new ArrayList<>();\n-\n+        List<IcebergPartitionField> fields = new ArrayList<>();\n+        IcebergPartitionField.Builder builder = IcebergPartitionField.builder();\n+        builder.setName(name);\n         switch (transform) {\n             case \"identity\":\n-                fields.add(name);\n+                builder.setTransform(PartitionTransformType.IDENTITY);\n                 break;\n             case \"year\":\n+                builder.setTransform(PartitionTransformType.YEAR);\n+                break;\n             case \"month\":\n+                builder.setTransform(PartitionTransformType.MONTH);\n+                break;\n             case \"day\":\n-                fields.add(format(\"%s(%s)\", transform, name));\n+                builder.setTransform(PartitionTransformType.DAY);\n+                break;\n+            case \"hour\":\n+                builder.setTransform(PartitionTransformType.HOUR);\n                 break;\n             case \"bucket\":\n+                builder.setTransform(PartitionTransformType.BUCKET)\n+                        .setParameter(OptionalInt.of(3));\n+                break;\n             case \"truncate\":\n-                fields.add(format(\"%s(%s, 3)\", transform, name));\n+                builder.setTransform(PartitionTransformType.TRUNCATE)\n+                        .setParameter(OptionalInt.of(3));\n                 break;\n         }\n-\n+        fields.add(builder.build());\n         return new PrestoIcebergPartitionSpec(0, prestoIcebergSchema(typeManager), fields);\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25219",
    "pr_id": 25219,
    "issue_id": 25218,
    "repo": "prestodb/presto",
    "problem_statement": "Session is missing in some abstract methods in BaseArrowFlightClientHandler.java\nSession is missing in some abstract methods in BaseArrowFlightClientHandler.java\n\n## Expected Behavior or Use Case\nWe have an implementation of arrow flight server connector, which implements the presto-base-arrow-flight module. In that, I want to attach the presto QueryId to the request I am sending to the flight server. \nThis can be done easily by taking the queryId from session, if the session is available there.\n`session.getQueryId()`\n\nIn most of the methods in BaseArrowFlightClientHandler, session is passed as an argument.\nBut the session is not passed in these two methods : \n1. protected abstract FlightDescriptor getFlightDescriptorForSchema(String schemaName, String tableName);\n2. protected abstract FlightDescriptor getFlightDescriptorForTableScan(ArrowTableLayoutHandle tableLayoutHandle);\n\n\n## Presto Component, Service, or Connector\n<!--- Tell us to which service or component this request is related to -->\n\nIt is related to the arrow module. \npresto-base-arrow-flight\n\n## Possible Implementation\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\n\nThis can be introduced in a PR. I will raise the PR. \n\nI have tried this in our internal repo in dev branches, and it is working fine.\n\n## Example Screenshots (if appropriate):\n\n## Context\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\nI need this feature to improve the logging for our service. Specifically, the logs in an arrow flight server.\n\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 249,
    "test_files_count": 1,
    "non_test_files_count": 4,
    "pr_changed_files": [
      "presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowMetadata.java",
      "presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowPageSource.java",
      "presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowSplitManager.java",
      "presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/BaseArrowFlightClientHandler.java",
      "presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java"
    ],
    "pr_changed_test_files": [
      "presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java"
    ],
    "base_commit": "d9f71c41dca2e7454eb8d2d5bce1b0dd2c1a24bf",
    "head_commit": "a081f0c54867ff29377c63a516a5b828d17aaea8",
    "repo_url": "https://github.com/prestodb/presto/pull/25219",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25219",
    "dockerfile": "",
    "pr_merged_at": "2025-06-02T13:27:39.000Z",
    "patch": "diff --git a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowMetadata.java b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowMetadata.java\nindex d3ef4a8a3b8a7..fd530b21a718d 100644\n--- a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowMetadata.java\n+++ b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowMetadata.java\n@@ -84,10 +84,10 @@ public ConnectorTableHandle getTableHandle(ConnectorSession session, SchemaTable\n         return new ArrowTableHandle(tableName.getSchemaName(), tableName.getTableName());\n     }\n \n-    public List<Field> getColumnsList(String schema, String table, ConnectorSession connectorSession)\n+    public List<Field> getColumnsList(ConnectorSession connectorSession, String schema, String table)\n     {\n         try {\n-            Schema flightSchema = clientHandler.getSchemaForTable(schema, table, connectorSession);\n+            Schema flightSchema = clientHandler.getSchemaForTable(connectorSession, schema, table);\n             return flightSchema.getFields();\n         }\n         catch (Exception e) {\n@@ -102,7 +102,7 @@ public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, Conn\n \n         String schemaValue = ((ArrowTableHandle) tableHandle).getSchema();\n         String tableValue = ((ArrowTableHandle) tableHandle).getTable();\n-        List<Field> columnList = getColumnsList(schemaValue, tableValue, session);\n+        List<Field> columnList = getColumnsList(session, schemaValue, tableValue);\n \n         for (Field field : columnList) {\n             String columnName = field.getName();\n@@ -143,7 +143,7 @@ public ConnectorTableLayout getTableLayout(ConnectorSession session, ConnectorTa\n     public ConnectorTableMetadata getTableMetadata(ConnectorSession session, ConnectorTableHandle table)\n     {\n         List<ColumnMetadata> meta = new ArrayList<>();\n-        List<Field> columnList = getColumnsList(((ArrowTableHandle) table).getSchema(), ((ArrowTableHandle) table).getTable(), session);\n+        List<Field> columnList = getColumnsList(session, ((ArrowTableHandle) table).getSchema(), ((ArrowTableHandle) table).getTable());\n \n         for (Field field : columnList) {\n             String columnName = field.getName();\n\ndiff --git a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowPageSource.java b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowPageSource.java\nindex fcacb204da7d4..1fdbd03f87669 100644\n--- a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowPageSource.java\n+++ b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowPageSource.java\n@@ -48,7 +48,7 @@ public ArrowPageSource(\n         this.columnHandles = requireNonNull(columnHandles, \"columnHandles is null\");\n         requireNonNull(clientHandler, \"clientHandler is null\");\n         this.arrowBlockBuilder = requireNonNull(arrowBlockBuilder, \"arrowBlockBuilder is null\");\n-        this.flightStreamAndClient = clientHandler.getFlightStream(split, connectorSession);\n+        this.flightStreamAndClient = clientHandler.getFlightStream(connectorSession, split);\n     }\n \n     @Override\n\ndiff --git a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowSplitManager.java b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowSplitManager.java\nindex 129a5e11f6355..8e1893917becf 100644\n--- a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowSplitManager.java\n+++ b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/ArrowSplitManager.java\n@@ -44,7 +44,7 @@ public ConnectorSplitSource getSplits(ConnectorTransactionHandle transactionHand\n     {\n         ArrowTableLayoutHandle tableLayoutHandle = (ArrowTableLayoutHandle) layout;\n         ArrowTableHandle tableHandle = tableLayoutHandle.getTable();\n-        FlightInfo flightInfo = clientHandler.getFlightInfoForTableScan(tableLayoutHandle, session);\n+        FlightInfo flightInfo = clientHandler.getFlightInfoForTableScan(session, tableLayoutHandle);\n         List<ArrowSplit> splits = flightInfo.getEndpoints()\n                 .stream()\n                 .map(info -> new ArrowSplit(\n\ndiff --git a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/BaseArrowFlightClientHandler.java b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/BaseArrowFlightClientHandler.java\nindex e8fde1ec39521..304bf4c953ff7 100644\n--- a/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/BaseArrowFlightClientHandler.java\n+++ b/presto-base-arrow-flight/src/main/java/com/facebook/plugin/arrow/BaseArrowFlightClientHandler.java\n@@ -87,7 +87,7 @@ protected FlightClient createFlightClient(Location location)\n \n     public abstract CallOption[] getCallOptions(ConnectorSession connectorSession);\n \n-    protected FlightInfo getFlightInfo(FlightDescriptor flightDescriptor, ConnectorSession connectorSession)\n+    protected FlightInfo getFlightInfo(ConnectorSession connectorSession, FlightDescriptor flightDescriptor)\n     {\n         try (FlightClient client = createFlightClient()) {\n             CallOption[] callOptions = getCallOptions(connectorSession);\n@@ -98,7 +98,7 @@ protected FlightInfo getFlightInfo(FlightDescriptor flightDescriptor, ConnectorS\n         }\n     }\n \n-    protected ClientClosingFlightStream getFlightStream(ArrowSplit split, ConnectorSession connectorSession)\n+    protected ClientClosingFlightStream getFlightStream(ConnectorSession connectorSession, ArrowSplit split)\n     {\n         ByteBuffer endpointBytes = ByteBuffer.wrap(split.getFlightEndpointBytes());\n         try {\n@@ -116,7 +116,7 @@ protected ClientClosingFlightStream getFlightStream(ArrowSplit split, ConnectorS\n         }\n     }\n \n-    public Schema getSchema(FlightDescriptor flightDescriptor, ConnectorSession connectorSession)\n+    public Schema getSchema(ConnectorSession connectorSession, FlightDescriptor flightDescriptor)\n     {\n         try (FlightClient client = createFlightClient()) {\n             CallOption[] callOptions = this.getCallOptions(connectorSession);\n@@ -131,19 +131,19 @@ public Schema getSchema(FlightDescriptor flightDescriptor, ConnectorSession conn\n \n     public abstract List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName);\n \n-    protected abstract FlightDescriptor getFlightDescriptorForSchema(String schemaName, String tableName);\n+    protected abstract FlightDescriptor getFlightDescriptorForSchema(ConnectorSession session, String schemaName, String tableName);\n \n-    protected abstract FlightDescriptor getFlightDescriptorForTableScan(ArrowTableLayoutHandle tableLayoutHandle);\n+    protected abstract FlightDescriptor getFlightDescriptorForTableScan(ConnectorSession session, ArrowTableLayoutHandle tableLayoutHandle);\n \n-    public Schema getSchemaForTable(String schemaName, String tableName, ConnectorSession connectorSession)\n+    public Schema getSchemaForTable(ConnectorSession connectorSession, String schemaName, String tableName)\n     {\n-        FlightDescriptor flightDescriptor = getFlightDescriptorForSchema(schemaName, tableName);\n-        return getSchema(flightDescriptor, connectorSession);\n+        FlightDescriptor flightDescriptor = getFlightDescriptorForSchema(connectorSession, schemaName, tableName);\n+        return getSchema(connectorSession, flightDescriptor);\n     }\n \n-    public FlightInfo getFlightInfoForTableScan(ArrowTableLayoutHandle tableLayoutHandle, ConnectorSession session)\n+    public FlightInfo getFlightInfoForTableScan(ConnectorSession session, ArrowTableLayoutHandle tableLayoutHandle)\n     {\n-        FlightDescriptor flightDescriptor = getFlightDescriptorForTableScan(tableLayoutHandle);\n-        return getFlightInfo(flightDescriptor, session);\n+        FlightDescriptor flightDescriptor = getFlightDescriptorForTableScan(session, tableLayoutHandle);\n+        return getFlightInfo(session, flightDescriptor);\n     }\n }\n",
    "test_patch": "diff --git a/presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java b/presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java\nindex 4d35cab2cc1bb..d8e9d1e5929a4 100644\n--- a/presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java\n+++ b/presto-base-arrow-flight/src/test/java/com/facebook/plugin/arrow/testingConnector/TestingArrowFlightClientHandler.java\n@@ -74,7 +74,7 @@ public CallOption[] getCallOptions(ConnectorSession connectorSession)\n     }\n \n     @Override\n-    public FlightDescriptor getFlightDescriptorForSchema(String schemaName, String tableName)\n+    public FlightDescriptor getFlightDescriptorForSchema(ConnectorSession session, String schemaName, String tableName)\n     {\n         TestingArrowFlightRequest request = TestingArrowFlightRequest.createDescribeTableRequest(schemaName, tableName);\n         return FlightDescriptor.command(requestCodec.toBytes(request));\n@@ -139,7 +139,7 @@ public List<SchemaTableName> listTables(ConnectorSession session, Optional<Strin\n     }\n \n     @Override\n-    public FlightDescriptor getFlightDescriptorForTableScan(ArrowTableLayoutHandle tableLayoutHandle)\n+    public FlightDescriptor getFlightDescriptorForTableScan(ConnectorSession session, ArrowTableLayoutHandle tableLayoutHandle)\n     {\n         ArrowTableHandle tableHandle = tableLayoutHandle.getTable();\n         String query = new TestingArrowQueryBuilder().buildSql(\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25213",
    "pr_id": 25213,
    "issue_id": 22848,
    "repo": "prestodb/presto",
    "problem_statement": "IcebergDistributedSmokeTestBase.testRegisterTable is flaky\nError:  Failures: \r\nError:    TestIcebergSmokeNessie>IcebergDistributedSmokeTestBase.testRegisterTable:1378->AbstractTestQueryFramework.assertQuery:164 For query: \r\n SELECT * FROM register_new\r\nnot equal\r\nExpected rows (1 of 1 missing rows shown, 1 rows in total):\r\n    [1, 1]\r\n\r\nSeen here https://github.com/prestodb/presto/actions/runs/9260568635/job/25474586005?pr=22843",
    "issue_word_count": 49,
    "test_files_count": 2,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java"
    ],
    "base_commit": "d9f71c41dca2e7454eb8d2d5bce1b0dd2c1a24bf",
    "head_commit": "59d628a47eb3326e6ebc30a53c190f24ce66da1b",
    "repo_url": "https://github.com/prestodb/presto/pull/25213",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25213",
    "dockerfile": "",
    "pr_merged_at": "2025-05-31T00:12:23.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\nindex 70f771882736e..b8766417432be 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n@@ -1588,6 +1588,9 @@ public void testRegisterTable()\n     {\n         String schemaName = getSession().getSchema().get();\n         String tableName = \"register\";\n+        // Create a `noise` table in the same schema to test that the `getLocation` method finds and returns the right metadata location.\n+        String noiseTableName = \"register1\";\n+        assertUpdate(\"CREATE TABLE \" + noiseTableName + \" (id integer, value integer)\");\n         assertUpdate(\"CREATE TABLE \" + tableName + \" (id integer, value integer)\");\n         assertUpdate(\"INSERT INTO \" + tableName + \" VALUES(1, 1)\", 1);\n \n@@ -1599,6 +1602,7 @@ public void testRegisterTable()\n \n         unregisterTable(schemaName, newTableName);\n         dropTable(getSession(), tableName);\n+        dropTable(getSession(), noiseTableName);\n     }\n \n     @Test\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\nindex 064679730c9e2..9c4faeb7546d4 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\n@@ -82,7 +82,7 @@ protected String getLocation(String schema, String table)\n         Path dataDirectory = ((DistributedQueryRunner) queryRunner).getCoordinator().getDataDirectory();\n         Path icebergDataDirectory = getIcebergDataDirectoryPath(dataDirectory, NESSIE.name(), new IcebergConfig().getFileFormat(), false);\n         Optional<File> tempTableLocation = Arrays.stream(requireNonNull(icebergDataDirectory.resolve(schema).toFile().listFiles()))\n-                .filter(file -> file.toURI().toString().contains(table)).findFirst();\n+                .filter(file -> endsWithTableUUID(table, file.toURI().toString())).findFirst();\n \n         String dataLocation = icebergDataDirectory.toFile().toURI().toString();\n         String relativeTableLocation = tempTableLocation.get().toURI().toString().replace(dataLocation, \"\");\n@@ -119,4 +119,9 @@ protected Table getIcebergTable(ConnectorSession session, String schema, String\n                 session,\n                 SchemaTableName.valueOf(schema + \".\" + tableName));\n     }\n+\n+    private static boolean endsWithTableUUID(String tableName, String tablePath)\n+    {\n+        return tablePath.matches(format(\".*%s_[-a-fA-F0-9]{36}/$\", tableName));\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25168",
    "pr_id": 25168,
    "issue_id": 25134,
    "repo": "prestodb/presto",
    "problem_statement": "TestOutputColumnTypes.testOutputColumnsForInsertAsSelect is flaky\nExample failure: https://github.com/prestodb/presto/actions/runs/15061285470/job/42336793411?pr=25106\n\n```\nError:  Tests run: 4477, Failures: 1, Errors: 0, Skipped: 27, Time elapsed: 2,243.501 s <<< FAILURE! - in TestSuite\nError:  com.facebook.presto.iceberg.TestOutputColumnTypes.testOutputColumnsForInsertAsSelect  Time elapsed: 0.958 s  <<< FAILURE!\njava.lang.IllegalArgumentException: expected one element but was: <com.facebook.presto.spi.eventlistener.QueryCompletedEvent@2ad44d54, com.facebook.presto.spi.eventlistener.QueryCompletedEvent@64f2bd95>\n\tat com.google.common.collect.Iterators.getOnlyElement(Iterators.java:323)\n\tat com.google.common.collect.Iterables.getOnlyElement(Iterables.java:263)\n\tat com.facebook.presto.iceberg.TestOutputColumnTypes.testOutputColumnsForInsertAsSelect(TestOutputColumnTypes.java:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n```",
    "issue_word_count": 242,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java"
    ],
    "base_commit": "b566980241fe4ef65f0f3db3e0c00148033136ef",
    "head_commit": "fc804e7dc3bc2e51d17472a82d152634d1178368",
    "repo_url": "https://github.com/prestodb/presto/pull/25168",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25168",
    "dockerfile": "",
    "pr_merged_at": "2025-05-22T18:27:15.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java\nindex a9b16de301b6e..8f247835e7418 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java\n@@ -97,14 +97,14 @@ private MaterializedResult runQueryAndWaitForEvents(@Language(\"SQL\") String sql,\n     public void testOutputColumnsForInsertAsSelect()\n             throws Exception\n     {\n-        runQueryAndWaitForEvents(\"CREATE TABLE create_insert_table1 AS SELECT clerk, orderkey, totalprice FROM orders\", 3);\n-        runQueryAndWaitForEvents(\"INSERT INTO create_insert_table1 SELECT clerk, orderkey, totalprice FROM orders\", 3);\n+        runQueryAndWaitForEvents(\"CREATE TABLE create_insert_table1 AS SELECT clerk, orderkey, totalprice FROM orders\", 2);\n+        runQueryAndWaitForEvents(\"INSERT INTO create_insert_table1 SELECT clerk, orderkey, totalprice FROM orders\", 2);\n         QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n \n         assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n         assertThat(event.getIoMetadata().getOutput().get().getSchema()).isEqualTo(\"tpch\");\n         assertThat(event.getIoMetadata().getOutput().get().getTable()).isEqualTo(\"create_insert_table1\");\n-        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"CREATE TABLE\");\n+        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"INSERT\");\n \n         assertThat(event.getIoMetadata().getOutput().get().getColumns().get())\n                 .containsExactly(\n@@ -114,11 +114,10 @@ public void testOutputColumnsForInsertAsSelect()\n     }\n \n     @Test\n-    public void testOutputColumnsForUpdate()\n+    public void testOutputColumnsForCreateTableAS()\n             throws Exception\n     {\n-        runQueryAndWaitForEvents(\"CREATE TABLE create_update_table AS SELECT * FROM orders \", 3);\n-        runQueryAndWaitForEvents(\"UPDATE create_update_table SET clerk = 're-reset'\", 3);\n+        runQueryAndWaitForEvents(\"CREATE TABLE create_update_table AS SELECT * FROM orders \", 2);\n         QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n \n         assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n@@ -139,26 +138,6 @@ public void testOutputColumnsForUpdate()\n                         new Column(\"comment\", \"varchar\"));\n     }\n \n-    @Test\n-    public void testOutputColumnsForDeleteWithWhere()\n-            throws Exception\n-    {\n-        runQueryAndWaitForEvents(\"CREATE TABLE create_del_table AS SELECT clerk, orderkey, totalprice FROM orders \", 3);\n-        runQueryAndWaitForEvents(\"DELETE FROM create_del_table WHERE orderkey = 1\", 3);\n-        QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n-\n-        assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n-        assertThat(event.getIoMetadata().getOutput().get().getSchema()).isEqualTo(\"tpch\");\n-        assertThat(event.getIoMetadata().getOutput().get().getTable()).isEqualTo(\"create_del_table\");\n-        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"CREATE TABLE\");\n-\n-        assertThat(event.getIoMetadata().getOutput().get().getColumns().get())\n-                .containsExactly(\n-                        new Column(\"clerk\", \"varchar\"),\n-                        new Column(\"orderkey\", \"bigint\"),\n-                        new Column(\"totalprice\", \"double\"));\n-    }\n-\n     static class TestingEventListenerPlugin\n             implements Plugin\n     {\n@@ -247,7 +226,7 @@ public synchronized void initialize(int numEvents)\n         public void waitForEvents(Duration duration)\n                 throws InterruptedException\n         {\n-            eventsLatch.await(duration.getNano(), NANOSECONDS);\n+            eventsLatch.await(duration.toNanos(), NANOSECONDS);\n         }\n \n         public synchronized void addQueryCreated(QueryCreatedEvent event)\n@@ -265,7 +244,6 @@ public synchronized void addQueryCompleted(QueryCompletedEvent event)\n         public synchronized void addSplitCompleted(SplitCompletedEvent event)\n         {\n             splitCompletedEvents.add(event);\n-            eventsLatch.countDown();\n         }\n \n         public List<QueryCompletedEvent> getQueryCompletedEvents()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-25116",
    "pr_id": 25116,
    "issue_id": 25118,
    "repo": "prestodb/presto",
    "problem_statement": "TestNativeSidecarPlugin.testLambdaFunctions fails because the exception message isn't matching\nTestNativeSidecarPlugin.testLambdaFunctions fails because the exception message isn't matching.\n\nExample failure:\n\n```\nError:  com.facebook.presto.sidecar.TestNativeSidecarPlugin.testLambdaFunctions  Time elapsed: 2.711 s  <<< FAILURE!\njava.lang.AssertionError: \nExpected exception message 'Failed to find matching function signature for array_sort, matching failures: \n Exception 1: line 1:31: Expected a lambda that takes 2 argument(s) but got 3\n Exception 2: line 1:31: Expected a lambda that takes 1 argument(s) but got 3\n' to match '\\QFailed to find matching function signature for array_sort, matching failures: \n Exception 1: line 1:31: Expected a lambda that takes 1 argument(s) but got 3\n Exception 2: line 1:31: Expected a lambda that takes 2 argument(s) but got 3\n\\E' for query: SELECT array_sort(quantities, (x, y, z) -> if (x < y + z, cast(1 as bigint), if (x > y + z, cast(-1 as bigint), cast(0 as bigint)))) FROM orders_ex\n\tat org.testng.Assert.fail(Assert.java:98)\n\tat com.facebook.presto.tests.QueryAssertions.assertExceptionMessage(QueryAssertions.java:387)\n\tat com.facebook.presto.tests.QueryAssertions.assertQueryFails(QueryAssertions.java:350)\n\tat com.facebook.presto.tests.AbstractTestQueryFramework.assertQueryFails(AbstractTestQueryFramework.java:311)\n\tat com.facebook.presto.sidecar.TestNativeSidecarPlugin.testLambdaFunctions(TestNativeSidecarPlugin.java:205)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.RuntimeException: Failed to find matching function signature for array_sort, matching failures: \n Exception 1: line 1:31: Expected a lambda that takes 2 argument(s) but got 3\n Exception 2: line 1:31: Expected a lambda that takes 1 argument(s) but got 3\n```\n\nFor more context: https://github.com/prestodb/presto/actions/runs/15005461339/job/42163438139",
    "issue_word_count": 406,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java"
    ],
    "pr_changed_test_files": [
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java"
    ],
    "base_commit": "3da0e14058470c483fd88720f25ca4defb089ed7",
    "head_commit": "ce1c448ed0b956425fcb94808f74ecdc881eb172",
    "repo_url": "https://github.com/prestodb/presto/pull/25116",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/25116",
    "dockerfile": "",
    "pr_merged_at": "2025-05-15T16:28:15.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\nindex 7f97726169ae4..0d37043b630e4 100644\n--- a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n@@ -204,9 +204,9 @@ public void testLambdaFunctions()\n         assertQuery(\"SELECT transform(array[1, 2, 3], x -> x * regionkey + nationkey) FROM nation\");\n         assertQueryFails(\n                 \"SELECT array_sort(quantities, (x, y, z) -> if (x < y + z, cast(1 as bigint), if (x > y + z, cast(-1 as bigint), cast(0 as bigint)))) FROM orders_ex\",\n-                Pattern.quote(\"Failed to find matching function signature for array_sort, matching failures: \\n\" +\n-                        \" Exception 1: line 1:31: Expected a lambda that takes 1 argument(s) but got 3\\n\" +\n-                        \" Exception 2: line 1:31: Expected a lambda that takes 2 argument(s) but got 3\\n\"));\n+                \"Failed to find matching function signature for array_sort, matching failures: \\n\" +\n+                        \" Exception 1: line 1:31: Expected a lambda that takes ([12])\" + Pattern.quote(\" argument(s) but got 3\\n\") +\n+                        \" Exception 2: line 1:31: Expected a lambda that takes ([12])\" + Pattern.quote(\" argument(s) but got 3\\n\"));\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24978",
    "pr_id": 24978,
    "issue_id": 24964,
    "repo": "prestodb/presto",
    "problem_statement": "Productionize native-sidecar feature\nWe are working to run native-side feature with worker as sidecar option. This can be enabled when [native-sidecar plugin](https://github.com/prestodb/presto/tree/master/presto-native-sidecar-plugin) is in plugins directory and following two configs need to be set:\n\n1. In coordinator config properties: `\"coordinator-sidecar-enabled\": \"true\"`\n2. In worker config properties: `\"native-sidecar\": \"true\"`\n\nWhile deploying it in Meta internal systems, we are finding improvement opportunities which will be listed here. To start with we are able to build and package the plugin. Now we saw 2 issues.\n\n1. coordinator always loads the plugin even if `\"coordinator-sidecar-enabled\": \"false\"`, this makes coordinator crash\n```\n025-04-23T12:55:49.520-0700\tERROR\tmain\tcom.facebook.presto.server.PrestoServer\tSystem session property provider is already registered for property provider : native-worker\njava.lang.IllegalArgumentException: System session property provider is already registered for property provider : native-worker\n\tat com.facebook.presto.metadata.SessionPropertyManager.loadSessionPropertyProvider(SessionPropertyManager.java:147)\n\tat com.facebook.presto.metadata.SessionPropertyManager.loadSessionPropertyProviders(SessionPropertyManager.java:154)\n\tat com.facebook.presto.server.PrestoServer.run(PrestoServer.java:193)\n```\nSince this plugin will always be included in the build, we need to conditionally load it given native worker may not use this plugin at runtime, or we disable it with config for java workers at runtime. We won't be able to control the build scenario as build will unconditionally include the plugins.\n2. When the plugin runs, we failed to run any query when coordinator and worker are in https mode.\n```\ncom.facebook.presto.spi.PrestoException: Failed to get session properties from sidecar.\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.fetchSessionProperties(NativeSystemSessionPropertyProvider.java:105)\n\tat com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:261)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.getSessionProperties(NativeSystemSessionPropertyProvider.java:89)\n\tat com.facebook.presto.metadata.SessionPropertyManager.lambda$getWorkerSessionProperties$0(SessionPropertyManager.java:216)\n\tat java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:271)\n\tat java.base/java.util.concurrent.ConcurrentHashMap$ValueSpliterator.forEachRemaining(ConcurrentHashMap.java:3605)\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\n\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\n\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)\n\tat com.facebook.presto.metadata.SessionPropertyManager.getWorkerSessionProperties(SessionPropertyManager.java:217)\n\tat com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:261)\n\tat com.facebook.presto.metadata.SessionPropertyManager.getSystemSessionPropertyMetadata(SessionPropertyManager.java:196)\n\tat com.facebook.presto.metadata.SessionPropertyManager.validateSystemSessionProperty(SessionPropertyManager.java:303)\n\tat com.facebook.presto.Session.beginTransactionId(Session.java:378)\n\tat com.facebook.presto.execution.QueryStateMachine.beginWithTicker(QueryStateMachine.java:261)\n\tat com.facebook.presto.execution.QueryStateMachine.begin(QueryStateMachine.java:226)\n\tat com.facebook.presto.dispatcher.LocalDispatchQueryFactory.createDispatchQuery(LocalDispatchQueryFactory.java:147)\n\tat com.facebook.presto.dispatcher.DispatchManager.createQueryInternal(DispatchManager.java:322)\n\tat com.facebook.presto.dispatcher.DispatchManager.lambda$createQuery$0(DispatchManager.java:249)\n\tat com.facebook.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: host starts with a bracket\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:143)\n\tat com.facebook.airlift.http.client.HttpUriBuilder.host(HttpUriBuilder.java:85)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.getSidecarLocation(NativeSystemSessionPropertyProvider.java:144)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.fetchSessionProperties(NativeSystemSessionPropertyProvider.java:96)\n\t... 24 more\n```\n\nWe will add more improvements as we try it and once above two is fixed.",
    "issue_word_count": 612,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/functionNamespace/NativeFunctionDefinitionProvider.java",
      "presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestURIGenerationForSidecarEndpoints.java"
    ],
    "pr_changed_test_files": [
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestURIGenerationForSidecarEndpoints.java"
    ],
    "base_commit": "ee6bc00c62b370967a580aeb23db5ae3a72d862c",
    "head_commit": "d2359371c75c04ce212ee799bac777e900b9bc56",
    "repo_url": "https://github.com/prestodb/presto/pull/24978",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24978",
    "dockerfile": "",
    "pr_merged_at": "2025-04-25T21:41:35.000Z",
    "patch": "diff --git a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/functionNamespace/NativeFunctionDefinitionProvider.java b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/functionNamespace/NativeFunctionDefinitionProvider.java\nindex 310cd9ebda2e6..f11a60ec6255f 100644\n--- a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/functionNamespace/NativeFunctionDefinitionProvider.java\n+++ b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/functionNamespace/NativeFunctionDefinitionProvider.java\n@@ -73,10 +73,8 @@ public UdfFunctionSignatureMap getUdfDefinition(NodeManager nodeManager)\n     private URI getSidecarLocation()\n     {\n         Node sidecarNode = nodeManager.getSidecarNode();\n-        return HttpUriBuilder.uriBuilder()\n-                .scheme(\"http\")\n-                .host(sidecarNode.getHost())\n-                .port(sidecarNode.getHostAndPort().getPort())\n+        return HttpUriBuilder\n+                .uriBuilderFrom(sidecarNode.getHttpUri())\n                 .appendPath(FUNCTION_SIGNATURES_ENDPOINT)\n                 .build();\n     }\n\ndiff --git a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\nindex 424d81f4b2efb..a23d28febaf05 100644\n--- a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\n+++ b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\n@@ -139,10 +139,8 @@ else if (type == TINYINT) {\n     private URI getSidecarLocation()\n     {\n         Node sidecarNode = nodeManager.getSidecarNode();\n-        return HttpUriBuilder.uriBuilder()\n-                .scheme(\"http\")\n-                .host(sidecarNode.getHost())\n-                .port(sidecarNode.getHostAndPort().getPort())\n+        return HttpUriBuilder\n+                .uriBuilderFrom(sidecarNode.getHttpUri())\n                 .appendPath(SESSION_PROPERTIES_ENDPOINT)\n                 .build();\n     }\n",
    "test_patch": "diff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestURIGenerationForSidecarEndpoints.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestURIGenerationForSidecarEndpoints.java\nnew file mode 100644\nindex 0000000000000..91aefc61ba17d\n--- /dev/null\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestURIGenerationForSidecarEndpoints.java\n@@ -0,0 +1,78 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sidecar;\n+\n+import com.facebook.airlift.http.client.HttpUriBuilder;\n+import com.facebook.presto.client.NodeVersion;\n+import com.facebook.presto.metadata.InMemoryNodeManager;\n+import com.facebook.presto.metadata.InternalNode;\n+import com.facebook.presto.nodeManager.PluginNodeManager;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.Node;\n+import com.facebook.presto.spi.PrestoException;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.net.URI;\n+\n+import static com.facebook.presto.spi.StandardErrorCode.NO_NODES_AVAILABLE;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestURIGenerationForSidecarEndpoints\n+{\n+    private InMemoryNodeManager inMemoryNodeManager;\n+    private PluginNodeManager pluginNodeManager;\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        // Initialize the InMemoryNodeManager and PluginNodeManager before each test.\n+        inMemoryNodeManager = new InMemoryNodeManager();\n+        pluginNodeManager = new PluginNodeManager(inMemoryNodeManager, \"test-env\");\n+    }\n+\n+    @Test(dataProvider = \"testDifferentIPs\")\n+    public void testURIGenerationForSidecarEndpoints(String nodeIdentifier, String url)\n+    {\n+        String testEndpoint = \"/v1/test\";\n+        ConnectorId connectorId = new ConnectorId(\"test\");\n+        InternalNode node = new InternalNode(nodeIdentifier, URI.create(url), new NodeVersion(\"1\"), false, false, false, true);\n+        inMemoryNodeManager.addNode(connectorId, node);\n+        Node sidecarNode = pluginNodeManager.getAllNodes()\n+                .stream()\n+                .filter((node1 -> node1.getNodeIdentifier().equals(nodeIdentifier))).findFirst()\n+                .orElseThrow(() ->\n+                        new PrestoException(\n+                                NO_NODES_AVAILABLE, format(\"Failed to find node with nodeIdentifier %s\", nodeIdentifier)));\n+        URI expectedURI = URI.create(url + testEndpoint);\n+        URI actualURI = HttpUriBuilder\n+                .uriBuilderFrom(sidecarNode.getHttpUri())\n+                .appendPath(testEndpoint)\n+                .build();\n+        assertEquals(actualURI, expectedURI);\n+        assertEquals(actualURI.getScheme(), expectedURI.getScheme());\n+    }\n+\n+    @DataProvider(name = \"testDifferentIPs\")\n+    public static Object[][] testDifferentIPs()\n+    {\n+        return new Object[][] {\n+                {\"activeNode1\", \"https://[::1]:8080\"},\n+                {\"activeNode2\", \"http://[::1]:8080\"},\n+                {\"activeNode3\", \"http://example1.com:8081\"},\n+                {\"activeNode4\", \"https://example1.com:8081\"}};\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24965",
    "pr_id": 24965,
    "issue_id": 24964,
    "repo": "prestodb/presto",
    "problem_statement": "Productionize native-sidecar feature\nWe are working to run native-side feature with worker as sidecar option. This can be enabled when [native-sidecar plugin](https://github.com/prestodb/presto/tree/master/presto-native-sidecar-plugin) is in plugins directory and following two configs need to be set:\n\n1. In coordinator config properties: `\"coordinator-sidecar-enabled\": \"true\"`\n2. In worker config properties: `\"native-sidecar\": \"true\"`\n\nWhile deploying it in Meta internal systems, we are finding improvement opportunities which will be listed here. To start with we are able to build and package the plugin. Now we saw 2 issues.\n\n1. coordinator always loads the plugin even if `\"coordinator-sidecar-enabled\": \"false\"`, this makes coordinator crash\n```\n025-04-23T12:55:49.520-0700\tERROR\tmain\tcom.facebook.presto.server.PrestoServer\tSystem session property provider is already registered for property provider : native-worker\njava.lang.IllegalArgumentException: System session property provider is already registered for property provider : native-worker\n\tat com.facebook.presto.metadata.SessionPropertyManager.loadSessionPropertyProvider(SessionPropertyManager.java:147)\n\tat com.facebook.presto.metadata.SessionPropertyManager.loadSessionPropertyProviders(SessionPropertyManager.java:154)\n\tat com.facebook.presto.server.PrestoServer.run(PrestoServer.java:193)\n```\nSince this plugin will always be included in the build, we need to conditionally load it given native worker may not use this plugin at runtime, or we disable it with config for java workers at runtime. We won't be able to control the build scenario as build will unconditionally include the plugins.\n2. When the plugin runs, we failed to run any query when coordinator and worker are in https mode.\n```\ncom.facebook.presto.spi.PrestoException: Failed to get session properties from sidecar.\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.fetchSessionProperties(NativeSystemSessionPropertyProvider.java:105)\n\tat com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:261)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.getSessionProperties(NativeSystemSessionPropertyProvider.java:89)\n\tat com.facebook.presto.metadata.SessionPropertyManager.lambda$getWorkerSessionProperties$0(SessionPropertyManager.java:216)\n\tat java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:271)\n\tat java.base/java.util.concurrent.ConcurrentHashMap$ValueSpliterator.forEachRemaining(ConcurrentHashMap.java:3605)\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\n\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\n\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)\n\tat com.facebook.presto.metadata.SessionPropertyManager.getWorkerSessionProperties(SessionPropertyManager.java:217)\n\tat com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:261)\n\tat com.facebook.presto.metadata.SessionPropertyManager.getSystemSessionPropertyMetadata(SessionPropertyManager.java:196)\n\tat com.facebook.presto.metadata.SessionPropertyManager.validateSystemSessionProperty(SessionPropertyManager.java:303)\n\tat com.facebook.presto.Session.beginTransactionId(Session.java:378)\n\tat com.facebook.presto.execution.QueryStateMachine.beginWithTicker(QueryStateMachine.java:261)\n\tat com.facebook.presto.execution.QueryStateMachine.begin(QueryStateMachine.java:226)\n\tat com.facebook.presto.dispatcher.LocalDispatchQueryFactory.createDispatchQuery(LocalDispatchQueryFactory.java:147)\n\tat com.facebook.presto.dispatcher.DispatchManager.createQueryInternal(DispatchManager.java:322)\n\tat com.facebook.presto.dispatcher.DispatchManager.lambda$createQuery$0(DispatchManager.java:249)\n\tat com.facebook.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: host starts with a bracket\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:143)\n\tat com.facebook.airlift.http.client.HttpUriBuilder.host(HttpUriBuilder.java:85)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.getSidecarLocation(NativeSystemSessionPropertyProvider.java:144)\n\tat com.facebook.presto.sidecar.sessionpropertyproviders.NativeSystemSessionPropertyProvider.fetchSessionProperties(NativeSystemSessionPropertyProvider.java:96)\n\t... 24 more\n```\n\nWe will add more improvements as we try it and once above two is fixed.",
    "issue_word_count": 612,
    "test_files_count": 5,
    "non_test_files_count": 10,
    "pr_changed_files": [
      "presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionAndTypeManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyProviderConfig.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStore.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStoreConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/metadata/TestSessionPropertyProviderConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/metadata/TestStaticTypeManagerStoreConfig.java",
      "presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java",
      "presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java",
      "presto-native-sidecar-plugin/pom.xml",
      "presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPluginWithoutLoadingFunctionalities.java",
      "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkModule.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/test/java/com/facebook/presto/metadata/TestSessionPropertyProviderConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/metadata/TestStaticTypeManagerStoreConfig.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPluginWithoutLoadingFunctionalities.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java"
    ],
    "base_commit": "dbe3d365c07bc20b5006b420f124f4317eb7fe2d",
    "head_commit": "1d1fe4aaa4248828ac340d2488c3d67f40afe840",
    "repo_url": "https://github.com/prestodb/presto/pull/24965",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24965",
    "dockerfile": "",
    "pr_merged_at": "2025-05-29T16:53:17.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionAndTypeManager.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionAndTypeManager.java\nindex 4ea19e25698cd..9f469711cd8d4 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionAndTypeManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionAndTypeManager.java\n@@ -418,13 +418,6 @@ public void loadTypeManager(String typeManagerName)\n         servingTypeManagerParametricTypesSupplier.set(this::getServingTypeManagerParametricTypes);\n     }\n \n-    public void loadTypeManagers()\n-    {\n-        for (String typeManagerName : typeManagerFactories.keySet()) {\n-            loadTypeManager(typeManagerName);\n-        }\n-    }\n-\n     public void addTypeManagerFactory(TypeManagerFactory factory)\n     {\n         if (typeManagerFactories.putIfAbsent(factory.getName(), factory) != null) {\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyManager.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyManager.java\nindex 3e9e23e1a6e9a..a37187441158c 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyManager.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.airlift.json.JsonCodecFactory;\n+import com.facebook.airlift.log.Logger;\n import com.facebook.presto.Session;\n import com.facebook.presto.SystemSessionProperties;\n import com.facebook.presto.common.block.BlockBuilder;\n@@ -52,6 +53,8 @@\n import javax.annotation.Nullable;\n import javax.inject.Inject;\n \n+import java.io.File;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n@@ -59,14 +62,18 @@\n import java.util.TreeMap;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.function.Supplier;\n \n import static com.facebook.presto.common.type.TypeUtils.writeNativeValue;\n import static com.facebook.presto.spi.StandardErrorCode.INVALID_SESSION_PROPERTY;\n import static com.facebook.presto.sql.planner.ExpressionInterpreter.evaluateConstantExpression;\n+import static com.facebook.presto.util.PropertiesUtil.loadProperties;\n import static com.google.common.base.MoreObjects.firstNonNull;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.io.Files.getNameWithoutExtension;\n import static java.lang.String.format;\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.TimeUnit.HOURS;\n@@ -74,6 +81,9 @@\n public final class SessionPropertyManager\n {\n     private static final JsonCodecFactory JSON_CODEC_FACTORY = new JsonCodecFactory();\n+    private static final Logger log = Logger.get(SessionPropertyManager.class);\n+    private static final String SESSION_PROPERTY_PROVIDER_NAME = \"session-property-provider.name\";\n+\n     private final ConcurrentMap<String, PropertyMetadata<?>> systemSessionProperties = new ConcurrentHashMap<>();\n     private final ConcurrentMap<ConnectorId, Map<String, PropertyMetadata<?>>> connectorSessionProperties = new ConcurrentHashMap<>();\n     private final Map<String, WorkerSessionPropertyProvider> workerSessionPropertyProviders;\n@@ -81,28 +91,33 @@ public final class SessionPropertyManager\n     private final Supplier<Map<String, PropertyMetadata<?>>> memoizedWorkerSessionProperties;\n     private final Optional<NodeManager> nodeManager;\n     private final Optional<TypeManager> functionAndTypeManager;\n+    private final File configDir;\n+    private final AtomicBoolean sessionPropertyProvidersLoading = new AtomicBoolean();\n \n     @Inject\n     public SessionPropertyManager(\n             SystemSessionProperties systemSessionProperties,\n             Map<String, WorkerSessionPropertyProvider> workerSessionPropertyProviders,\n             FunctionAndTypeManager functionAndTypeManager,\n-            NodeManager nodeManager)\n+            NodeManager nodeManager,\n+            SessionPropertyProviderConfig config)\n     {\n-        this(systemSessionProperties.getSessionProperties(), workerSessionPropertyProviders, Optional.ofNullable(functionAndTypeManager), Optional.ofNullable(nodeManager));\n+        this(systemSessionProperties.getSessionProperties(), workerSessionPropertyProviders, Optional.ofNullable(functionAndTypeManager), Optional.ofNullable(nodeManager), config);\n     }\n \n     public SessionPropertyManager(\n             List<PropertyMetadata<?>> sessionProperties,\n             Map<String, WorkerSessionPropertyProvider> workerSessionPropertyProviders,\n             Optional<TypeManager> functionAndTypeManager,\n-            Optional<NodeManager> nodeManager)\n+            Optional<NodeManager> nodeManager,\n+            SessionPropertyProviderConfig config)\n     {\n         this.nodeManager = requireNonNull(nodeManager, \"nodeManager is null\");\n         this.functionAndTypeManager = requireNonNull(functionAndTypeManager, \"functionAndTypeManager is null\");\n         this.memoizedWorkerSessionProperties = Suppliers.memoizeWithExpiration(this::getWorkerSessionProperties,\n                 1, HOURS);\n         this.workerSessionPropertyProviders = new ConcurrentHashMap<>(workerSessionPropertyProviders);\n+        this.configDir = requireNonNull(config, \"config is null\").getSessionPropertyProvidersConfigurationDir();\n         addSystemSessionProperties(sessionProperties);\n     }\n \n@@ -135,24 +150,42 @@ public static SessionPropertyManager createTestingSessionPropertyManager(\n                                 javaFeaturesConfig,\n                                 nodeSpillConfig)),\n                 Optional.empty(),\n-                Optional.empty());\n+                Optional.empty(),\n+                new SessionPropertyProviderConfig());\n     }\n \n-    public void loadSessionPropertyProvider(String sessionPropertyProviderName, Optional<TypeManager> typeManager, Optional<NodeManager> nodeManager)\n+    public void loadSessionPropertyProviders()\n+            throws Exception\n     {\n+        if (!sessionPropertyProvidersLoading.compareAndSet(false, true)) {\n+            return;\n+        }\n+\n+        for (File file : listFiles(configDir)) {\n+            if (file.isFile() && file.getName().endsWith(\".properties\")) {\n+                String sessionPropertyProviderName = getNameWithoutExtension(file.getName());\n+                Map<String, String> properties = loadProperties(file);\n+                checkState(!isNullOrEmpty(properties.get(SESSION_PROPERTY_PROVIDER_NAME)),\n+                        \"Session property manager configuration %s does not contain %s\",\n+                        file.getAbsoluteFile(),\n+                        SESSION_PROPERTY_PROVIDER_NAME);\n+                properties = new HashMap<>(properties);\n+                properties.remove(SESSION_PROPERTY_PROVIDER_NAME);\n+                loadSessionPropertyProvider(sessionPropertyProviderName, properties, functionAndTypeManager, nodeManager);\n+            }\n+        }\n+    }\n+\n+    public void loadSessionPropertyProvider(String sessionPropertyProviderName, Map<String, String> properties, Optional<TypeManager> typeManager, Optional<NodeManager> nodeManager)\n+    {\n+        log.info(\"-- Loading %s session property provider --\", sessionPropertyProviderName);\n         WorkerSessionPropertyProviderFactory factory = workerSessionPropertyProviderFactories.get(sessionPropertyProviderName);\n         checkState(factory != null, \"No factory for session property provider : \" + sessionPropertyProviderName);\n         WorkerSessionPropertyProvider sessionPropertyProvider = factory.create(new SessionPropertyContext(typeManager, nodeManager));\n         if (workerSessionPropertyProviders.putIfAbsent(sessionPropertyProviderName, sessionPropertyProvider) != null) {\n             throw new IllegalArgumentException(\"System session property provider is already registered for property provider : \" + sessionPropertyProviderName);\n         }\n-    }\n-\n-    public void loadSessionPropertyProviders()\n-    {\n-        for (String sessionPropertyProviderName : workerSessionPropertyProviderFactories.keySet()) {\n-            loadSessionPropertyProvider(sessionPropertyProviderName, functionAndTypeManager, nodeManager);\n-        }\n+        log.info(\"-- Added session property provider [%s] --\", sessionPropertyProviderName);\n     }\n \n     public void addSessionPropertyProviderFactory(WorkerSessionPropertyProviderFactory factory)\n@@ -224,6 +257,17 @@ private Map<String, PropertyMetadata<?>> getWorkerSessionProperties()\n         return workerSessionProperties;\n     }\n \n+    private static List<File> listFiles(File dir)\n+    {\n+        if (dir != null && dir.isDirectory()) {\n+            File[] files = dir.listFiles();\n+            if (files != null) {\n+                return ImmutableList.copyOf(files);\n+            }\n+        }\n+        return ImmutableList.of();\n+    }\n+\n     public List<SessionPropertyValue> getAllSessionProperties(Session session, Map<String, ConnectorId> catalogs)\n     {\n         requireNonNull(session, \"session is null\");\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyProviderConfig.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyProviderConfig.java\nnew file mode 100644\nindex 0000000000000..2464ab7d3becb\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/SessionPropertyProviderConfig.java\n@@ -0,0 +1,38 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.facebook.airlift.configuration.Config;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import java.io.File;\n+\n+public class SessionPropertyProviderConfig\n+{\n+    private File sessionPropertyProvidersConfigurationDir = new File(\"etc/session-property-providers/\");\n+\n+    @NotNull\n+    public File getSessionPropertyProvidersConfigurationDir()\n+    {\n+        return sessionPropertyProvidersConfigurationDir;\n+    }\n+\n+    @Config(\"session-property-provider.config-dir\")\n+    public SessionPropertyProviderConfig setSessionPropertyProvidersConfigurationDir(File dir)\n+    {\n+        this.sessionPropertyProvidersConfigurationDir = dir;\n+        return this;\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStore.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStore.java\nnew file mode 100644\nindex 0000000000000..9e62ec064550e\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStore.java\n@@ -0,0 +1,93 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.facebook.airlift.log.Logger;\n+import com.google.common.collect.ImmutableList;\n+\n+import javax.inject.Inject;\n+\n+import java.io.File;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.facebook.presto.util.PropertiesUtil.loadProperties;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n+import static com.google.common.io.Files.getNameWithoutExtension;\n+\n+public class StaticTypeManagerStore\n+{\n+    private static final Logger log = Logger.get(StaticTypeManagerStore.class);\n+    private static final String TYPE_MANAGER_NAME = \"type-manager.name\";\n+    private final FunctionAndTypeManager functionAndTypeManager;\n+    private final File configDir;\n+    private final AtomicBoolean typeManagersLoading = new AtomicBoolean();\n+\n+    @Inject\n+    public StaticTypeManagerStore(FunctionAndTypeManager functionAndTypeManager, StaticTypeManagerStoreConfig config)\n+    {\n+        this.functionAndTypeManager = functionAndTypeManager;\n+        this.configDir = config.getTypeManagerConfigurationDir();\n+    }\n+\n+    public void loadTypeManagers()\n+            throws Exception\n+    {\n+        if (!typeManagersLoading.compareAndSet(false, true)) {\n+            return;\n+        }\n+\n+        for (File file : listFiles(configDir)) {\n+            if (file.isFile() && file.getName().endsWith(\".properties\")) {\n+                String catalogName = getNameWithoutExtension(file.getName());\n+                Map<String, String> properties = loadProperties(file);\n+                checkState(!isNullOrEmpty(properties.get(TYPE_MANAGER_NAME)),\n+                        \"Type manager configuration %s does not contain %s\",\n+                        file.getAbsoluteFile(),\n+                        TYPE_MANAGER_NAME);\n+                loadTypeManager(catalogName, properties);\n+            }\n+        }\n+    }\n+\n+    public void loadTypeManagers(Map<String, Map<String, String>> catalogProperties)\n+    {\n+        catalogProperties.entrySet().stream()\n+                .forEach(entry -> loadTypeManager(entry.getKey(), entry.getValue()));\n+    }\n+\n+    private void loadTypeManager(String catalogName, Map<String, String> properties)\n+    {\n+        log.info(\"-- Loading %s type manager --\", catalogName);\n+        properties = new HashMap<>(properties);\n+        String typeManagerName = properties.remove(TYPE_MANAGER_NAME);\n+        checkState(!isNullOrEmpty(typeManagerName), \"%s property must be present\", TYPE_MANAGER_NAME);\n+        functionAndTypeManager.loadTypeManager(typeManagerName);\n+        log.info(\"-- Added type manager [%s] --\", catalogName);\n+    }\n+\n+    private static List<File> listFiles(File dir)\n+    {\n+        if (dir != null && dir.isDirectory()) {\n+            File[] files = dir.listFiles();\n+            if (files != null) {\n+                return ImmutableList.copyOf(files);\n+            }\n+        }\n+        return ImmutableList.of();\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStoreConfig.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStoreConfig.java\nnew file mode 100644\nindex 0000000000000..de29a50089b0b\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/StaticTypeManagerStoreConfig.java\n@@ -0,0 +1,38 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.facebook.airlift.configuration.Config;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import java.io.File;\n+\n+public class StaticTypeManagerStoreConfig\n+{\n+    private File typeManagerConfigurationDir = new File(\"etc/type-managers/\");\n+\n+    @NotNull\n+    public File getTypeManagerConfigurationDir()\n+    {\n+        return typeManagerConfigurationDir;\n+    }\n+\n+    @Config(\"type-manager.config-dir\")\n+    public StaticTypeManagerStoreConfig setTypeManagerConfigurationDir(File dir)\n+    {\n+        this.typeManagerConfigurationDir = dir;\n+        return this;\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java b/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\nindex fac8bc79e06b5..552b6da5d812e 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\n@@ -43,11 +43,11 @@\n import com.facebook.presto.metadata.Catalog;\n import com.facebook.presto.metadata.CatalogManager;\n import com.facebook.presto.metadata.DiscoveryNodeManager;\n-import com.facebook.presto.metadata.FunctionAndTypeManager;\n import com.facebook.presto.metadata.InternalNodeManager;\n import com.facebook.presto.metadata.SessionPropertyManager;\n import com.facebook.presto.metadata.StaticCatalogStore;\n import com.facebook.presto.metadata.StaticFunctionNamespaceStore;\n+import com.facebook.presto.metadata.StaticTypeManagerStore;\n import com.facebook.presto.nodeManager.PluginNodeManager;\n import com.facebook.presto.security.AccessControlManager;\n import com.facebook.presto.security.AccessControlModule;\n@@ -175,6 +175,7 @@ public void run()\n                     injector.getInstance(DriftServer.class));\n \n             injector.getInstance(StaticFunctionNamespaceStore.class).loadFunctionNamespaceManagers();\n+            injector.getInstance(StaticTypeManagerStore.class).loadTypeManagers();\n             injector.getInstance(SessionPropertyDefaults.class).loadConfigurationManager();\n             injector.getInstance(ResourceGroupManager.class).loadConfigurationManager();\n             if (!serverConfig.isResourceManager()) {\n@@ -191,7 +192,6 @@ public void run()\n             injector.getInstance(NodeStatusNotificationManager.class).loadNodeStatusNotificationProvider();\n             injector.getInstance(GracefulShutdownHandler.class).loadNodeStatusNotification();\n             injector.getInstance(SessionPropertyManager.class).loadSessionPropertyProviders();\n-            injector.getInstance(FunctionAndTypeManager.class).loadTypeManagers();\n             PlanCheckerProviderManager planCheckerProviderManager = injector.getInstance(PlanCheckerProviderManager.class);\n             InternalNodeManager nodeManager = injector.getInstance(DiscoveryNodeManager.class);\n             NodeInfo nodeInfo = injector.getInstance(NodeInfo.class);\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java b/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\nindex 7c9f8742ccea7..130d4fcdd89cc 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\n@@ -100,11 +100,14 @@\n import com.facebook.presto.metadata.MetadataUpdates;\n import com.facebook.presto.metadata.SchemaPropertyManager;\n import com.facebook.presto.metadata.SessionPropertyManager;\n+import com.facebook.presto.metadata.SessionPropertyProviderConfig;\n import com.facebook.presto.metadata.Split;\n import com.facebook.presto.metadata.StaticCatalogStore;\n import com.facebook.presto.metadata.StaticCatalogStoreConfig;\n import com.facebook.presto.metadata.StaticFunctionNamespaceStore;\n import com.facebook.presto.metadata.StaticFunctionNamespaceStoreConfig;\n+import com.facebook.presto.metadata.StaticTypeManagerStore;\n+import com.facebook.presto.metadata.StaticTypeManagerStoreConfig;\n import com.facebook.presto.metadata.TablePropertyManager;\n import com.facebook.presto.nodeManager.PluginNodeManager;\n import com.facebook.presto.operator.ExchangeClientConfig;\n@@ -644,6 +647,9 @@ public ListeningExecutorService createResourceManagerExecutor(ResourceManagerCon\n         configBinder(binder).bindConfig(StaticCatalogStoreConfig.class);\n         binder.bind(StaticFunctionNamespaceStore.class).in(Scopes.SINGLETON);\n         configBinder(binder).bindConfig(StaticFunctionNamespaceStoreConfig.class);\n+        binder.bind(StaticTypeManagerStore.class).in(Scopes.SINGLETON);\n+        configBinder(binder).bindConfig(StaticTypeManagerStoreConfig.class);\n+        configBinder(binder).bindConfig(SessionPropertyProviderConfig.class);\n         binder.bind(FunctionAndTypeManager.class).in(Scopes.SINGLETON);\n         binder.bind(MetadataManager.class).in(Scopes.SINGLETON);\n \n\ndiff --git a/presto-native-sidecar-plugin/pom.xml b/presto-native-sidecar-plugin/pom.xml\nindex f6b20a1b32d4b..4e11d78ada815 100644\n--- a/presto-native-sidecar-plugin/pom.xml\n+++ b/presto-native-sidecar-plugin/pom.xml\n@@ -267,6 +267,18 @@\n                     </ignoredUnusedDeclaredDependencies>\n                 </configuration>\n             </plugin>\n+            <plugin>\n+                <groupId>org.apache.maven.plugins</groupId>\n+                <artifactId>maven-surefire-plugin</artifactId>\n+                <configuration>\n+                    <forkCount>1</forkCount>\n+                    <reuseForks>false</reuseForks>\n+                    <systemPropertyVariables>\n+                        <PRESTO_SERVER>/root/project/build/debug/presto_cpp/main/presto_server</PRESTO_SERVER>\n+                        <DATA_DIR>/tmp/velox</DATA_DIR>\n+                    </systemPropertyVariables>\n+                </configuration>\n+            </plugin>\n         </plugins>\n     </build>\n </project>\n\ndiff --git a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\nindex a23d28febaf05..3f64aa5247a02 100644\n--- a/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\n+++ b/presto-native-sidecar-plugin/src/main/java/com/facebook/presto/sidecar/sessionpropertyproviders/NativeSystemSessionPropertyProvider.java\n@@ -77,7 +77,7 @@ public NativeSystemSessionPropertyProvider(\n         this.nativeSessionPropertiesJsonCodec = requireNonNull(nativeSessionPropertiesJsonCodec, \"nativeSessionPropertiesJsonCodec is null\");\n         this.nodeManager = requireNonNull(nodeManager, \"nodeManager is null\");\n         this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n-        this.httpClient = requireNonNull(httpClient, \"typeManager is null\");\n+        this.httpClient = requireNonNull(httpClient, \"httpClient is null\");\n         requireNonNull(config, \"config is null\");\n         this.memoizedSessionPropertiesSupplier =\n                 Suppliers.memoizeWithExpiration(this::fetchSessionProperties, config.getSessionPropertiesCacheExpiration().toMillis(), MILLISECONDS);\n\ndiff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkModule.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkModule.java\nindex 6c666961fcd51..f41f6365e1d37 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkModule.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkModule.java\n@@ -76,10 +76,13 @@\n import com.facebook.presto.metadata.MetadataManager;\n import com.facebook.presto.metadata.SchemaPropertyManager;\n import com.facebook.presto.metadata.SessionPropertyManager;\n+import com.facebook.presto.metadata.SessionPropertyProviderConfig;\n import com.facebook.presto.metadata.StaticCatalogStore;\n import com.facebook.presto.metadata.StaticCatalogStoreConfig;\n import com.facebook.presto.metadata.StaticFunctionNamespaceStore;\n import com.facebook.presto.metadata.StaticFunctionNamespaceStoreConfig;\n+import com.facebook.presto.metadata.StaticTypeManagerStore;\n+import com.facebook.presto.metadata.StaticTypeManagerStoreConfig;\n import com.facebook.presto.metadata.TablePropertyManager;\n import com.facebook.presto.nodeManager.PluginNodeManager;\n import com.facebook.presto.operator.FileFragmentResultCacheConfig;\n@@ -276,6 +279,8 @@ protected void setup(Binder binder)\n         configBinder(binder).bindConfig(CompilerConfig.class);\n         configBinder(binder).bindConfig(SqlEnvironmentConfig.class);\n         configBinder(binder).bindConfig(StaticFunctionNamespaceStoreConfig.class);\n+        configBinder(binder).bindConfig(StaticTypeManagerStoreConfig.class);\n+        configBinder(binder).bindConfig(SessionPropertyProviderConfig.class);\n         configBinder(binder).bindConfig(PrestoSparkConfig.class);\n         configBinder(binder).bindConfig(TracingConfig.class);\n         configBinder(binder).bindConfig(NativeExecutionVeloxConfig.class);\n@@ -370,6 +375,7 @@ protected void setup(Binder binder)\n         binder.bind(MetadataManager.class).in(Scopes.SINGLETON);\n         binder.bind(Metadata.class).to(MetadataManager.class).in(Scopes.SINGLETON);\n         binder.bind(StaticFunctionNamespaceStore.class).in(Scopes.SINGLETON);\n+        binder.bind(StaticTypeManagerStore.class).in(Scopes.SINGLETON);\n \n         // type\n         newSetBinder(binder, Type.class);\n",
    "test_patch": "diff --git a/presto-main-base/src/test/java/com/facebook/presto/metadata/TestSessionPropertyProviderConfig.java b/presto-main-base/src/test/java/com/facebook/presto/metadata/TestSessionPropertyProviderConfig.java\nnew file mode 100644\nindex 0000000000000..cfa205bb637e3\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/metadata/TestSessionPropertyProviderConfig.java\n@@ -0,0 +1,47 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.util.Map;\n+\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertFullMapping;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertRecordedDefaults;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.recordDefaults;\n+\n+public class TestSessionPropertyProviderConfig\n+{\n+    @Test\n+    public void testDefaults()\n+    {\n+        assertRecordedDefaults(recordDefaults(SessionPropertyProviderConfig.class)\n+                .setSessionPropertyProvidersConfigurationDir(new File(\"etc/session-property-providers\")));\n+    }\n+\n+    @Test\n+    public void testExplicitPropertyMappings()\n+    {\n+        Map<String, String> properties = new ImmutableMap.Builder<String, String>()\n+                .put(\"session-property-provider.config-dir\", \"/foo\")\n+                .build();\n+\n+        SessionPropertyProviderConfig expected = new SessionPropertyProviderConfig()\n+                .setSessionPropertyProvidersConfigurationDir(new File(\"/foo\"));\n+\n+        assertFullMapping(properties, expected);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/metadata/TestStaticTypeManagerStoreConfig.java b/presto-main-base/src/test/java/com/facebook/presto/metadata/TestStaticTypeManagerStoreConfig.java\nnew file mode 100644\nindex 0000000000000..424078a6b4b29\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/metadata/TestStaticTypeManagerStoreConfig.java\n@@ -0,0 +1,47 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.util.Map;\n+\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertFullMapping;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertRecordedDefaults;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.recordDefaults;\n+\n+public class TestStaticTypeManagerStoreConfig\n+{\n+    @Test\n+    public void testDefaults()\n+    {\n+        assertRecordedDefaults(recordDefaults(StaticTypeManagerStoreConfig.class)\n+                .setTypeManagerConfigurationDir(new File(\"etc/type-managers\")));\n+    }\n+\n+    @Test\n+    public void testExplicitPropertyMappings()\n+    {\n+        Map<String, String> properties = new ImmutableMap.Builder<String, String>()\n+                .put(\"type-manager.config-dir\", \"/foo\")\n+                .build();\n+\n+        StaticTypeManagerStoreConfig expected = new StaticTypeManagerStoreConfig()\n+                .setTypeManagerConfigurationDir(new File(\"/foo\"));\n+\n+        assertFullMapping(properties, expected);\n+    }\n+}\n\ndiff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\nindex 1a22c05bdff4a..7bab4909d10a7 100644\n--- a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n@@ -40,7 +40,6 @@\n import static org.testng.Assert.assertFalse;\n import static org.testng.Assert.fail;\n \n-@Test(singleThreaded = true)\n public class TestNativeSidecarPlugin\n         extends AbstractTestQueryFramework\n {\n\ndiff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPluginWithoutLoadingFunctionalities.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPluginWithoutLoadingFunctionalities.java\nnew file mode 100644\nindex 0000000000000..607d81c81c312\n--- /dev/null\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPluginWithoutLoadingFunctionalities.java\n@@ -0,0 +1,79 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sidecar;\n+\n+import com.facebook.presto.nativeworker.PrestoNativeQueryRunnerUtils;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.AbstractTestQueryFramework;\n+import com.facebook.presto.tests.DistributedQueryRunner;\n+import org.testng.annotations.Test;\n+\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitem;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createNation;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createOrders;\n+\n+public class TestNativeSidecarPluginWithoutLoadingFunctionalities\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected void createTables()\n+    {\n+        QueryRunner queryRunner = (QueryRunner) getExpectedQueryRunner();\n+        createLineitem(queryRunner);\n+        createNation(queryRunner);\n+        createOrders(queryRunner);\n+    }\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) PrestoNativeQueryRunnerUtils.createQueryRunner(true, false, false, false);\n+        // Installing the native sidecar plugin on a native cluster does not load the plugin functionalities because\n+        // we aren't loading the individual functionalities.\n+        queryRunner.installCoordinatorPlugin(new NativeSidecarPlugin());\n+        return queryRunner;\n+    }\n+\n+    @Override\n+    protected QueryRunner createExpectedQueryRunner()\n+            throws Exception\n+    {\n+        DistributedQueryRunner queryRunner = (DistributedQueryRunner) PrestoNativeQueryRunnerUtils.createJavaQueryRunner();\n+        // Installing the native sidecar plugin on a Java cluster, does not load the plugin functionalities because\n+        // we aren't loading the individual functionalities.\n+        queryRunner.installCoordinatorPlugin(new NativeSidecarPlugin());\n+        return queryRunner;\n+    }\n+\n+    @Test\n+    public void testBasicQueries()\n+    {\n+        assertQuery(\"SELECT ARRAY['abc']\");\n+        assertQuery(\"SELECT ARRAY[1, 2, 3]\");\n+        assertQuery(\"SELECT substr(comment, 1, 10), length(comment), trim(comment) FROM orders\");\n+        assertQuery(\"SELECT substr(comment, 1, 10), length(comment), rtrim(comment) FROM orders\");\n+        assertQuery(\"select lower(comment) from nation\");\n+        assertQuery(\"SELECT mod(orderkey, linenumber) FROM lineitem\");\n+        assertQuery(\"select corr(nationkey, nationkey) from nation\");\n+        assertQuery(\"select count(comment) from orders\");\n+        assertQuery(\"select count(*) from nation\");\n+        assertQuery(\"select count(abs(orderkey) between 1 and 60000) from orders group by orderkey\");\n+        assertQuery(\"SELECT count(orderkey) FROM orders WHERE orderkey < 0 GROUP BY GROUPING SETS (())\");\n+        // This query will work when sidecar is enabled, should fail without it.\n+        assertQueryFails(\n+                \"select array_sort(array[row('apples', 23), row('bananas', 12), row('grapes', 44)], x -> x[2])\",\n+                \"line 1:84: Expected a lambda that takes 2 argument\\\\(s\\\\) but got 1\");\n+    }\n+}\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java b/presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java\nindex 6fe6d1ec81717..6b49e14b1d34b 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/DistributedQueryRunner.java\n@@ -989,6 +989,7 @@ public void loadSessionPropertyProvider(String sessionPropertyProviderName)\n         for (TestingPrestoServer server : servers) {\n             server.getMetadata().getSessionPropertyManager().loadSessionPropertyProvider(\n                     sessionPropertyProviderName,\n+                    ImmutableMap.of(),\n                     Optional.ofNullable(server.getMetadata().getFunctionAndTypeManager()),\n                     Optional.ofNullable(server.getPluginNodeManager()));\n         }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24920",
    "pr_id": 24920,
    "issue_id": 24919,
    "repo": "prestodb/presto",
    "problem_statement": "Timeout reading delta tables after incremental updates with null values\n<!--- Provide a general summary of the issue in the Title above -->\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used: 0.289, 0.292\n* Data source and connector used: Delta\n\n## Expected Behavior\n<!--- Tell us what should happen -->\nThe query should end and return data\n\n## Current Behavior\n<!--- Tell us what happens instead of the expected behavior -->\nThe query stays executing forever. This is due to an error processing columnar batches when the splits are processed (getNextBatch). With the new delta kernel library, incremental changes inserting null values can cause a batch to return empty iterators.\n\n## Possible Solution\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\nChange the code to handle the case explained in the previous section\n\n## Steps to Reproduce\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\n1. Create a delta table pointing to a lakehouse location (S3, Azure, HDFS...)\n2. Execute incremental updates over the table from another tool that suports them (merge)\n3. Query the table (it should timeout)\n\n## Workarrounds\nVACUUM OPTIMIZE the delta table seems to solve the problem\n",
    "issue_word_count": 227,
    "test_files_count": 20,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-delta/src/main/java/com/facebook/presto/delta/DeltaExpressionUtils.java",
      "presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java",
      "presto-delta/src/test/java/com/facebook/presto/delta/IncrementalUpdateQueriesTest.java",
      "presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000000.json.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000001.json.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000000.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000001.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/.part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/.part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/.part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/.part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/.part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet"
    ],
    "pr_changed_test_files": [
      "presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java",
      "presto-delta/src/test/java/com/facebook/presto/delta/IncrementalUpdateQueriesTest.java",
      "presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000000.json.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000001.json.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000000.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000001.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.json",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/.part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/.part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/.part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/.part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/.part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet.crc",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet",
      "presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet"
    ],
    "base_commit": "b2919faba781a0aeea2eb52d44bd807579fdcd41",
    "head_commit": "50f541164895999bfb5dfcf7ac3a6dcef8342305",
    "repo_url": "https://github.com/prestodb/presto/pull/24920",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24920",
    "dockerfile": "",
    "pr_merged_at": "2025-05-09T18:10:09.000Z",
    "patch": "diff --git a/presto-delta/src/main/java/com/facebook/presto/delta/DeltaExpressionUtils.java b/presto-delta/src/main/java/com/facebook/presto/delta/DeltaExpressionUtils.java\nindex 8ea44c160a591..878bb594cef68 100644\n--- a/presto-delta/src/main/java/com/facebook/presto/delta/DeltaExpressionUtils.java\n+++ b/presto-delta/src/main/java/com/facebook/presto/delta/DeltaExpressionUtils.java\n@@ -14,7 +14,6 @@\n package com.facebook.presto.delta;\n \n import com.facebook.airlift.log.Logger;\n-import com.facebook.presto.common.GenericInternalException;\n import com.facebook.presto.common.predicate.Domain;\n import com.facebook.presto.common.predicate.TupleDomain;\n import com.facebook.presto.common.predicate.ValueSet;\n@@ -25,6 +24,7 @@\n import com.facebook.presto.spi.PrestoException;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Streams;\n import io.airlift.slice.Slice;\n import io.delta.kernel.data.FilteredColumnarBatch;\n import io.delta.kernel.data.Row;\n@@ -34,10 +34,12 @@\n import java.io.IOException;\n import java.sql.Date;\n import java.sql.Timestamp;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.NoSuchElementException;\n import java.util.Optional;\n+import java.util.function.Predicate;\n \n import static com.facebook.presto.delta.DeltaColumnHandle.ColumnType.PARTITION;\n import static com.facebook.presto.delta.DeltaErrorCode.DELTA_INVALID_PARTITION_VALUE;\n@@ -123,59 +125,26 @@ private static TupleDomain<String> extractPartitionColumnsPredicate(TupleDomain<\n                 });\n     }\n \n-    private static class AllFilesIterator\n+    private static class NoneFilesIterator\n             implements CloseableIterator<Row>\n     {\n         private final CloseableIterator<FilteredColumnarBatch> inputIterator;\n-        private Row nextItem;\n-        private boolean rowsRemaining;\n-        private CloseableIterator<Row> row;\n \n-        public AllFilesIterator(CloseableIterator<FilteredColumnarBatch> inputIterator)\n+        NoneFilesIterator(CloseableIterator<FilteredColumnarBatch> inputIterator)\n         {\n             this.inputIterator = inputIterator;\n         }\n+\n         @Override\n         public boolean hasNext()\n         {\n-            if (nextItem != null) {\n-                return true;\n-            }\n-\n-            if (!rowsRemaining) {\n-                if (!inputIterator.hasNext()) {\n-                    return false;\n-                }\n-                FilteredColumnarBatch nextFile = inputIterator.next();\n-                row = nextFile.getRows();\n-            }\n-            Row nextRow;\n-            rowsRemaining = false;\n-            if (row.hasNext()) {\n-                nextRow = row.next();\n-                nextItem = nextRow;\n-                rowsRemaining = true;\n-            }\n-            if (!rowsRemaining) {\n-                try {\n-                    row.close();\n-                }\n-                catch (IOException e) {\n-                    throw new GenericInternalException(\"Could not close row batch\", e);\n-                }\n-            }\n-            return nextItem != null;\n+            return false;\n         }\n \n         @Override\n         public Row next()\n         {\n-            if (!hasNext()) {\n-                throw new NoSuchElementException(\"There are no more files\");\n-            }\n-            Row toReturn = nextItem;\n-            nextItem = null;\n-            return toReturn;\n+            throw new NoSuchElementException();\n         }\n \n         @Override\n@@ -186,109 +155,78 @@ public void close()\n         }\n     }\n \n-    private static class NoneFilesIterator\n+    private static class BatchRowIterator\n             implements CloseableIterator<Row>\n     {\n         private final CloseableIterator<FilteredColumnarBatch> inputIterator;\n+        private final Iterator<Row> rows;\n+        private CloseableIterator<Row> prev;\n \n-        NoneFilesIterator(CloseableIterator<FilteredColumnarBatch> inputIterator)\n+        public BatchRowIterator(CloseableIterator<FilteredColumnarBatch> inputIterator,\n+                Optional<Predicate<Row>> rowFilter)\n         {\n             this.inputIterator = inputIterator;\n+            this.rows = Streams.stream(inputIterator)\n+                    .flatMap(batch -> {\n+                        if (prev != null) {\n+                            try {\n+                                prev.close();\n+                            }\n+                            catch (IOException e) {\n+                                throw new RuntimeException(\"Failed to close previous row batch\", e);\n+                            }\n+                        }\n+                        prev = batch.getRows();\n+                        return Streams.stream(prev);\n+                    })\n+                    // if there is a filter to be applied, it applies it\n+                    .filter(row -> rowFilter.map(predicate -> predicate.test(row)).orElse(true))\n+                    .iterator();\n         }\n \n         @Override\n         public boolean hasNext()\n         {\n-            return false;\n+            return rows.hasNext();\n         }\n \n         @Override\n         public Row next()\n         {\n-            throw new NoSuchElementException();\n+            return rows.next();\n         }\n \n         @Override\n-        public void close()\n-                throws IOException\n+        public void close() throws IOException\n         {\n-            inputIterator.close();\n-        }\n-    }\n-\n-    private static class FilteredByPredicateIterator\n-            implements CloseableIterator<Row>\n-    {\n-        private final CloseableIterator<FilteredColumnarBatch> inputIterator;\n-        private final TupleDomain<String> partitionPredicate;\n-        private final List<DeltaColumnHandle> partitionColumns;\n-        private final TypeManager typeManager;\n-        private Row nextItem;\n-        private boolean rowsRemaining;\n-        private CloseableIterator<Row> row;\n-\n-        public FilteredByPredicateIterator(CloseableIterator<FilteredColumnarBatch> inputIterator,\n-                                           TupleDomain<String> partitionPredicate,\n-                                           List<DeltaColumnHandle> partitionColumns, TypeManager typeManager)\n-        {\n-            this.inputIterator = inputIterator;\n-            this.partitionPredicate = partitionPredicate;\n-            this.partitionColumns = partitionColumns;\n-            this.typeManager = typeManager;\n-        }\n-\n-        @Override\n-        public boolean hasNext()\n-        {\n-            if (nextItem != null) {\n-                return true;\n-            }\n-\n-            if (!rowsRemaining) {\n-                if (!inputIterator.hasNext()) {\n-                    return false;\n-                }\n-                FilteredColumnarBatch nextFile = inputIterator.next();\n-                row = nextFile.getRows();\n-            }\n-            Row nextRow;\n-            rowsRemaining = false;\n-            while (row.hasNext()) {\n-                nextRow = row.next();\n-                if (evaluatePartitionPredicate(partitionPredicate, partitionColumns, typeManager,\n-                        nextRow)) {\n-                    nextItem = nextRow;\n-                    rowsRemaining = true;\n-                    break;\n-                }\n+            if (prev != null) {\n+                prev.close();\n             }\n-            if (!rowsRemaining) {\n-                try {\n-                    row.close();\n-                }\n-                catch (IOException e) {\n-                    throw new GenericInternalException(\"Cloud not close row batch\", e);\n-                }\n+            if (inputIterator != null) {\n+                inputIterator.close();\n             }\n-            return nextItem != null;\n         }\n+    }\n \n-        @Override\n-        public Row next()\n+    private static class AllFilesIterator\n+            extends BatchRowIterator\n+    {\n+        public AllFilesIterator(CloseableIterator<FilteredColumnarBatch> inputIterator)\n         {\n-            if (!hasNext()) {\n-                throw new NoSuchElementException(\"There are no more files\");\n-            }\n-            Row toReturn = nextItem;\n-            nextItem = null;\n-            return toReturn;\n+            super(inputIterator, Optional.empty());\n         }\n+    }\n \n-        @Override\n-        public void close()\n-                throws IOException\n+    private static class FilteredByPredicateIterator\n+            extends BatchRowIterator\n+    {\n+        public FilteredByPredicateIterator(CloseableIterator<FilteredColumnarBatch> inputIterator,\n+                TupleDomain<String> partitionPredicate,\n+                List<DeltaColumnHandle> partitionColumns,\n+                TypeManager typeManager)\n         {\n-            inputIterator.close();\n+            super(inputIterator,\n+                    Optional.of(row -> evaluatePartitionPredicate(partitionPredicate, partitionColumns, typeManager, row)));\n         }\n \n         private static boolean evaluatePartitionPredicate(\n",
    "test_patch": "diff --git a/presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java b/presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java\nindex ec77098e36312..367f7a09db711 100644\n--- a/presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java\n+++ b/presto-delta/src/test/java/com/facebook/presto/delta/AbstractDeltaDistributedQueryTestBase.java\n@@ -188,7 +188,7 @@ private static DistributedQueryRunner createDeltaQueryRunner(Map<String, String>\n      * @param deltaTableName Name of the delta table which is on the classpath.\n      * @param hiveTableName Name of the Hive table that the Delta table is to be registered as in HMS\n      */\n-    private static void registerDeltaTableInHMS(QueryRunner queryRunner, String deltaTableName, String hiveTableName)\n+    protected static void registerDeltaTableInHMS(QueryRunner queryRunner, String deltaTableName, String hiveTableName)\n     {\n         queryRunner.execute(format(\n                 \"CREATE TABLE %s.\\\"%s\\\".\\\"%s\\\" (dummyColumn INT) WITH (external_location = '%s')\",\n\ndiff --git a/presto-delta/src/test/java/com/facebook/presto/delta/IncrementalUpdateQueriesTest.java b/presto-delta/src/test/java/com/facebook/presto/delta/IncrementalUpdateQueriesTest.java\nnew file mode 100644\nindex 0000000000000..dff308db40c6d\n--- /dev/null\n+++ b/presto-delta/src/test/java/com/facebook/presto/delta/IncrementalUpdateQueriesTest.java\n@@ -0,0 +1,150 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.delta;\n+\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.MaterializedRow;\n+import com.facebook.presto.testing.QueryRunner;\n+import org.testng.annotations.Test;\n+\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+\n+import static com.facebook.presto.testing.assertions.Assert.assertEquals;\n+import static java.lang.String.format;\n+import static org.testng.Assert.assertNull;\n+import static org.testng.Assert.assertTrue;\n+\n+public class IncrementalUpdateQueriesTest\n+        extends AbstractDeltaDistributedQueryTestBase\n+{\n+    private final String version = \"delta_v3\";\n+    private final String controlTableName = \"deltatbl-partition-prune\";\n+    private final String targetTableName = controlTableName + \"-incremental\";\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        QueryRunner queryRunner = super.createQueryRunner();\n+        Path path = Paths.get(version);\n+        registerDeltaTableInHMS(queryRunner,\n+                path.resolve(targetTableName).toString(),\n+                path.resolve(targetTableName).toString());\n+        return queryRunner;\n+    }\n+\n+    private void checkQueryOutputOnIncrementalWithNullRows(String controlTableQuery, String testTableQuery)\n+    {\n+        MaterializedResult expectedResult = getQueryRunner().execute(controlTableQuery);\n+        MaterializedResult testResult = getQueryRunner().execute(testTableQuery);\n+        assertTrue(testResult.getRowCount() > expectedResult.getRowCount());\n+        // check that the non-null elements are equal in both tables\n+        for (int i = 0; i < expectedResult.getRowCount(); i++) {\n+            assertEquals(expectedResult.getMaterializedRows().get(i),\n+                    testResult.getMaterializedRows().get(i));\n+        }\n+        // check that the remaining elements in the test table are null\n+        for (int i = expectedResult.getRowCount(); i < testResult.getRowCount(); i++) {\n+            MaterializedRow row = testResult.getMaterializedRows().get(i);\n+            for (Object field : row.getFields()) {\n+                assertNull(field);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void readTableAllColumnsAfterIncrementalUpdateTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT * FROM \\\"%s\\\".\\\"%s\\\" order by date, city asc\", PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT * FROM \\\"%s\\\".\\\"%s\\\" order by date, city asc\", PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                controlTableName));\n+        checkQueryOutputOnIncrementalWithNullRows(controlTableQuery, testTableQuery);\n+    }\n+\n+    @Test\n+    public void readTableAllColumnsAfterIncrementalUpdateFilteringNullsTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT * FROM \\\"%s\\\".\\\"%s\\\" where name is not null order by date, city asc\",\n+                        PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT * FROM \\\"%s\\\".\\\"%s\\\" where name is not null order by date, city asc\",\n+                        PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        controlTableName));\n+        MaterializedResult expectedResult = getQueryRunner().execute(controlTableQuery);\n+        MaterializedResult testResult = getQueryRunner().execute(testTableQuery);\n+        assertEquals(testResult.getMaterializedRows(), expectedResult.getMaterializedRows());\n+    }\n+\n+    @Test\n+    public void readTableNonePartitionedColumnAfterIncrementalUpdateTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT name, cnt FROM \\\"%s\\\".\\\"%s\\\" order by name, cnt asc\", PATH_SCHEMA,\n+                        goldenTablePathWithPrefix(version, targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT name, cnt FROM \\\"%s\\\".\\\"%s\\\" order by name, cnt asc\", PATH_SCHEMA,\n+                        goldenTablePathWithPrefix(version, controlTableName));\n+        checkQueryOutputOnIncrementalWithNullRows(controlTableQuery, testTableQuery);\n+    }\n+\n+    @Test\n+    public void readTableNonePartitionedColumnAfterIncrementalUpdateFilteringNullsTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT name, cnt FROM \\\"%s\\\".\\\"%s\\\" where name is not null and \" +\n+                 \"cnt is not null order by name, cnt asc\", PATH_SCHEMA,\n+                 goldenTablePathWithPrefix(version, targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT name, cnt FROM \\\"%s\\\".\\\"%s\\\" where name is not null and \" +\n+                 \"cnt is not null order by name, cnt asc\", PATH_SCHEMA,\n+                 goldenTablePathWithPrefix(version, controlTableName));\n+        MaterializedResult expectedResult = getQueryRunner().execute(controlTableQuery);\n+        MaterializedResult testResult = getQueryRunner().execute(testTableQuery);\n+        assertEquals(testResult.getMaterializedRows(), expectedResult.getMaterializedRows());\n+    }\n+\n+    @Test\n+    public void readTablePartitionedColumnAfterIncrementalUpdateTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT date, city FROM \\\"%s\\\".\\\"%s\\\" order by date, city asc\", PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT date, city FROM \\\"%s\\\".\\\"%s\\\" order by date, city asc\", PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        controlTableName));\n+        checkQueryOutputOnIncrementalWithNullRows(controlTableQuery, testTableQuery);\n+    }\n+\n+    @Test\n+    public void readTablePartitionedColumnFilteringNullValuesAfterIncrementalUpdateTest()\n+    {\n+        String testTableQuery =\n+                format(\"SELECT date FROM \\\"%s\\\".\\\"%s\\\" where date is not null order by date asc\",\n+                PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        targetTableName));\n+        String controlTableQuery =\n+                format(\"SELECT date FROM \\\"%s\\\".\\\"%s\\\" where date is not null order by date asc\",\n+                PATH_SCHEMA, goldenTablePathWithPrefix(version,\n+                        controlTableName));\n+        MaterializedResult expectedResult = getQueryRunner().execute(controlTableQuery);\n+        MaterializedResult testResult = getQueryRunner().execute(testTableQuery);\n+        assertEquals(testResult.getMaterializedRows(), expectedResult.getMaterializedRows());\n+    }\n+}\n\ndiff --git a/presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java b/presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java\nindex 02fe5c36aacd5..97c312d5877f3 100644\n--- a/presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java\n+++ b/presto-delta/src/test/java/com/facebook/presto/delta/TestUppercasePartitionColumns.java\n@@ -23,7 +23,6 @@\n import com.google.common.collect.ImmutableMap;\n import org.testng.annotations.Test;\n \n-import java.nio.file.FileSystems;\n import java.nio.file.Path;\n import java.util.Map;\n \n@@ -35,24 +34,6 @@\n public class TestUppercasePartitionColumns\n         extends AbstractDeltaDistributedQueryTestBase\n {\n-    private static final String[] DELTA_TEST_TABLE_NAMES_LIST = {\n-            \"test-lowercase\",\n-            \"test-uppercase\",\n-            \"test-partitions-lowercase\",\n-            \"test-partitions-uppercase\"\n-    };\n-\n-    private static final String[] DELTA_TEST_TABLE_LIST = new String[DELTA_TEST_TABLE_NAMES_LIST.length *\n-            DELTA_VERSIONS.length];\n-    static {\n-        for (int i = 0; i < DELTA_VERSIONS.length; i++) {\n-            for (int j = 0; j < DELTA_TEST_TABLE_NAMES_LIST.length; j++) {\n-                DELTA_TEST_TABLE_LIST[i * DELTA_TEST_TABLE_NAMES_LIST.length + j] = DELTA_VERSIONS[i] +\n-                        FileSystems.getDefault().getSeparator() + DELTA_TEST_TABLE_NAMES_LIST[j];\n-            }\n-        }\n-    }\n-\n     @Override\n     protected QueryRunner createQueryRunner()\n             throws Exception\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000000.json.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000000.json.crc\nnew file mode 100644\nindex 0000000000000..3623b38b1e09d\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000000.json.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000001.json.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000001.json.crc\nnew file mode 100644\nindex 0000000000000..0eb7428340537\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/.00000000000000000001.json.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000000.json b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000000.json\nnew file mode 100644\nindex 0000000000000..a1b055c531a98\n--- /dev/null\n+++ b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000000.json\n@@ -0,0 +1,8 @@\n+{\"commitInfo\":{\"timestamp\":1713956585297,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[\\\"date\\\",\\\"city\\\"]\"},\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numFiles\":\"5\",\"numOutputRows\":\"5\",\"numOutputBytes\":\"3450\"},\"engineInfo\":\"Apache-Spark/3.5.1 Delta-Lake/3.1.0\",\"txnId\":\"5c66bd9e-7b46-4342-ae0a-6b53bc451626\"}}\n+{\"metaData\":{\"id\":\"eec3641b-b02a-4da0-b502-eb4149c4ddb1\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"cnt\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[\"date\",\"city\"],\"configuration\":{},\"createdTime\":1713956583685}}\n+{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n+{\"add\":{\"path\":\"date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180520\",\"city\":\"bj\"},\"size\":697,\"modificationTime\":1713956584979,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Trump\\\",\\\"cnt\\\":1},\\\"maxValues\\\":{\\\"name\\\":\\\"Trump\\\",\\\"cnt\\\":1},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"}}\n+{\"add\":{\"path\":\"date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20181212\",\"city\":\"sz\"},\"size\":697,\"modificationTime\":1713956584979,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Linda\\\",\\\"cnt\\\":8},\\\"maxValues\\\":{\\\"name\\\":\\\"Linda\\\",\\\"cnt\\\":8},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"}}\n+{\"add\":{\"path\":\"date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180718\",\"city\":\"hz\"},\"size\":690,\"modificationTime\":1713956584979,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jone\\\",\\\"cnt\\\":7},\\\"maxValues\\\":{\\\"name\\\":\\\"Jone\\\",\\\"cnt\\\":7},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"}}\n+{\"add\":{\"path\":\"date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180520\",\"city\":\"hz\"},\"size\":683,\"modificationTime\":1713956584979,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jim\\\",\\\"cnt\\\":3},\\\"maxValues\\\":{\\\"name\\\":\\\"Jim\\\",\\\"cnt\\\":3},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"}}\n+{\"add\":{\"path\":\"date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180512\",\"city\":\"sh\"},\"size\":683,\"modificationTime\":1713956584979,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jay\\\",\\\"cnt\\\":4},\\\"maxValues\\\":{\\\"name\\\":\\\"Jay\\\",\\\"cnt\\\":4},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"}}\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000001.json b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000001.json\nnew file mode 100644\nindex 0000000000000..1f6c828d7ee45\n--- /dev/null\n+++ b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000001.json\n@@ -0,0 +1,3 @@\n+{\"commitInfo\":{\"timestamp\":1713956588418,\"operation\":\"SET TBLPROPERTIES\",\"operationParameters\":{\"properties\":\"{\\\"delta.minReaderVersion\\\":\\\"3\\\",\\\"delta.minWriterVersion\\\":\\\"7\\\"}\"},\"readVersion\":0,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":true,\"operationMetrics\":{},\"engineInfo\":\"Apache-Spark/3.5.1 Delta-Lake/3.1.0\",\"txnId\":\"00d3656c-4f8a-4b56-8fb6-8c31266936bd\"}}\n+{\"metaData\":{\"id\":\"eec3641b-b02a-4da0-b502-eb4149c4ddb1\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"cnt\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[\"date\",\"city\"],\"configuration\":{},\"createdTime\":1713956583685}}\n+{\"protocol\":{\"minReaderVersion\":3,\"minWriterVersion\":7,\"readerFeatures\":[],\"writerFeatures\":[\"appendOnly\",\"invariants\"]}}\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.crc\nnew file mode 100644\nindex 0000000000000..bbd9f6f76d443\n--- /dev/null\n+++ b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.crc\n@@ -0,0 +1 @@\n+{\"txnId\":\"5f659bfa-ef01-49c2-a286-1c4479e6e7bd\",\"tableSizeBytes\":4230,\"numFiles\":6,\"numMetadata\":1,\"numProtocol\":1,\"setTransactions\":[],\"domainMetadata\":[],\"metadata\":{\"id\":\"eec3641b-b02a-4da0-b502-eb4149c4ddb1\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"date\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"cnt\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\"partitionColumns\":[\"date\",\"city\"],\"configuration\":{},\"createdTime\":1713956583685},\"protocol\":{\"minReaderVersion\":3,\"minWriterVersion\":7,\"readerFeatures\":[],\"writerFeatures\":[\"appendOnly\",\"invariants\"]},\"histogramOpt\":{\"sortedBinBoundaries\":[0,8192,16384,32768,65536,131072,262144,524288,1048576,2097152,4194304,8388608,12582912,16777216,20971520,25165824,29360128,33554432,37748736,41943040,50331648,58720256,67108864,75497472,83886080,92274688,100663296,109051904,117440512,125829120,130023424,134217728,138412032,142606336,146800640,150994944,167772160,184549376,201326592,218103808,234881024,251658240,268435456,285212672,301989888,318767104,335544320,352321536,369098752,385875968,402653184,419430400,436207616,452984832,469762048,486539264,503316480,520093696,536870912,553648128,570425344,587202560,603979776,671088640,738197504,805306368,872415232,939524096,1006632960,1073741824,1140850688,1207959552,1275068416,1342177280,1409286144,1476395008,1610612736,1744830464,1879048192,2013265920,2147483648,2415919104,2684354560,2952790016,3221225472,3489660928,3758096384,4026531840,4294967296,8589934592,17179869184,34359738368,68719476736,137438953472,274877906944],\"fileCounts\":[6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"totalBytes\":[4230,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},\"allFiles\":[{\"path\":\"date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180512\",\"city\":\"sh\"},\"size\":683,\"modificationTime\":1713956584979,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jay\\\",\\\"cnt\\\":4},\\\"maxValues\\\":{\\\"name\\\":\\\"Jay\\\",\\\"cnt\\\":4},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"},{\"path\":\"date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180718\",\"city\":\"hz\"},\"size\":690,\"modificationTime\":1713956584979,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jone\\\",\\\"cnt\\\":7},\\\"maxValues\\\":{\\\"name\\\":\\\"Jone\\\",\\\"cnt\\\":7},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"},{\"path\":\"date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180520\",\"city\":\"bj\"},\"size\":697,\"modificationTime\":1713956584979,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Trump\\\",\\\"cnt\\\":1},\\\"maxValues\\\":{\\\"name\\\":\\\"Trump\\\",\\\"cnt\\\":1},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"},{\"path\":\"date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20180520\",\"city\":\"hz\"},\"size\":683,\"modificationTime\":1713956584979,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Jim\\\",\\\"cnt\\\":3},\\\"maxValues\\\":{\\\"name\\\":\\\"Jim\\\",\\\"cnt\\\":3},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"},{\"path\":\"date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"20181212\",\"city\":\"sz\"},\"size\":697,\"modificationTime\":1713956584979,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"name\\\":\\\"Linda\\\",\\\"cnt\\\":8},\\\"maxValues\\\":{\\\"name\\\":\\\"Linda\\\",\\\"cnt\\\":8},\\\"nullCount\\\":{\\\"name\\\":0,\\\"cnt\\\":0}}\"},{\"path\":\"date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet\",\"partitionValues\":{\"date\":null,\"city\":null},\"size\":780,\"modificationTime\":1744617327000,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":10,\\\"minValues\\\":{},\\\"maxValues\\\":{},\\\"nullCount\\\":{\\\"name\\\":10,\\\"cnt\\\":10}}\",\"tags\":{\"INSERTION_TIME\":\"1744617327000000\",\"MIN_INSERTION_TIME\":\"1744617327000000\",\"MAX_INSERTION_TIME\":\"1744617327000000\",\"OPTIMIZE_TARGET_SIZE\":\"268435456\"}}]}\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.json b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.json\nnew file mode 100644\nindex 0000000000000..d13c1e61a9af4\n--- /dev/null\n+++ b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/_delta_log/00000000000000000002.json\n@@ -0,0 +1,2 @@\n+{\"commitInfo\":{\"timestamp\":1744617328664,\"userId\":\"4331016811320670\",\"userName\":\"mblanco@denodo.com\",\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":\"[\\\"null\\\"]\",\"clusterBy\":\"[]\",\"matchedPredicates\":\"[]\",\"statsOnLoad\":false,\"notMatchedBySourcePredicates\":\"[]\",\"notMatchedPredicates\":\"[{\\\"actionType\\\":\\\"insert\\\"}]\"},\"readVersion\":1,\"isolationLevel\":\"WriteSerializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numTargetRowsCopied\":\"0\",\"numTargetRowsDeleted\":\"0\",\"numTargetFilesAdded\":\"1\",\"numTargetBytesAdded\":\"780\",\"numTargetBytesRemoved\":\"0\",\"numTargetDeletionVectorsAdded\":\"0\",\"numTargetRowsMatchedUpdated\":\"0\",\"executionTimeMs\":\"6833\",\"materializeSourceTimeMs\":\"10\",\"numTargetRowsInserted\":\"10\",\"numTargetRowsMatchedDeleted\":\"0\",\"numTargetDeletionVectorsUpdated\":\"0\",\"scanTimeMs\":\"0\",\"numTargetRowsUpdated\":\"0\",\"numOutputRows\":\"10\",\"numTargetDeletionVectorsRemoved\":\"0\",\"numTargetRowsNotMatchedBySourceUpdated\":\"0\",\"numTargetChangeFilesAdded\":\"0\",\"numSourceRows\":\"10\",\"numTargetFilesRemoved\":\"0\",\"numTargetRowsNotMatchedBySourceDeleted\":\"0\",\"rewriteTimeMs\":\"6756\"},\"tags\":{\"noRowsCopied\":\"true\",\"delta.rowTracking.preserved\":\"false\",\"restoresDeletedRows\":\"false\"},\"engineInfo\":\"Databricks-Runtime/16.2.x-photon-scala2.12\",\"txnId\":\"5f659bfa-ef01-49c2-a286-1c4479e6e7bd\"}}\n+{\"add\":{\"path\":\"date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet\",\"partitionValues\":{\"date\":null,\"city\":null},\"size\":780,\"modificationTime\":1744617327000,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":10,\\\"minValues\\\":{},\\\"maxValues\\\":{},\\\"nullCount\\\":{\\\"name\\\":10,\\\"cnt\\\":10}}\",\"tags\":{\"INSERTION_TIME\":\"1744617327000000\",\"MIN_INSERTION_TIME\":\"1744617327000000\",\"MAX_INSERTION_TIME\":\"1744617327000000\",\"OPTIMIZE_TARGET_SIZE\":\"268435456\"}}}\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/.part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/.part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet.crc\nnew file mode 100644\nindex 0000000000000..175c150206759\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/.part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..f275079cd30f8\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180512/city=sh/part-00004-4d39fa0a-62f7-4045-b2c4-1e860e6379e7.c000.snappy.parquet differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/.part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/.part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet.crc\nnew file mode 100644\nindex 0000000000000..2157ba0fe3ea8\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/.part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..242ded658f82a\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=bj/part-00000-fe79b4e1-aaca-4bef-9a72-00c6def5b90c.c000.snappy.parquet differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/.part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/.part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet.crc\nnew file mode 100644\nindex 0000000000000..153bb6311d9e8\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/.part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..7eb41c4261745\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180520/city=hz/part-00003-6dbf3cc5-0294-49d5-a40f-79c05a093c66.c000.snappy.parquet differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/.part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/.part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet.crc\nnew file mode 100644\nindex 0000000000000..1010a4c01f819\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/.part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..81c723dbbb3db\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20180718/city=hz/part-00002-f7dbf1cd-7f74-40c2-b097-832286326b3e.c000.snappy.parquet differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/.part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet.crc b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/.part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet.crc\nnew file mode 100644\nindex 0000000000000..669f327ffb054\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/.part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet.crc differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..b27667999a7fc\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=20181212/city=sz/part-00001-b2e29782-7e1e-4819-a241-b5a9b5e910e8.c000.snappy.parquet differ\n\ndiff --git a/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet\nnew file mode 100644\nindex 0000000000000..1042d9383cc2f\nBinary files /dev/null and b/presto-delta/src/test/resources/delta_v3/deltatbl-partition-prune-incremental/date=__HIVE_DEFAULT_PARTITION__/city=__HIVE_DEFAULT_PARTITION__/part-00000-8b6b5718-0044-45c5-8ed6-fc1abcdd5bd4.c000.snappy.parquet differ\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24898",
    "pr_id": 24898,
    "issue_id": 24888,
    "repo": "prestodb/presto",
    "problem_statement": "Set column details in output field\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\n\nWhile extracting the output table details , the column details are missing. Implement changes for set the column details  to output field.\n\n## Expected Behavior or Use Case\n<!--- Tell us how it should work -->\nIt should populate the column name and its type in the output field.\n\n## Presto Component, Service, or Connector\n<!--- Tell us to which service or component this request is related to -->\npresto-main, presto-spi\n\n## Possible Implementation\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\n\n## Example Screenshots (if appropriate):\n\n## Context\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 159,
    "test_files_count": 3,
    "non_test_files_count": 8,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java",
      "presto-main-base/src/main/java/com/facebook/presto/event/QueryMonitor.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/Output.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/QueryStateMachine.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/OutputExtractor.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/Column.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryOutputMetadata.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java"
    ],
    "base_commit": "1a3a4f7e9d8d7bad088c8d6d16403364a876eeb3",
    "head_commit": "91619b6702c4cd0f7f5d6a06aab3c3c13085c743",
    "repo_url": "https://github.com/prestodb/presto/pull/24898",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24898",
    "dockerfile": "",
    "pr_merged_at": "2025-05-07T19:12:46.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/event/QueryMonitor.java b/presto-main-base/src/main/java/com/facebook/presto/event/QueryMonitor.java\nindex fa87e067dd831..73f35985a3a97 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/event/QueryMonitor.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/event/QueryMonitor.java\n@@ -28,7 +28,6 @@\n import com.facebook.presto.cost.PlanNodeStatsEstimate;\n import com.facebook.presto.cost.StatsAndCosts;\n import com.facebook.presto.eventlistener.EventListenerManager;\n-import com.facebook.presto.execution.Column;\n import com.facebook.presto.execution.ExecutionFailureInfo;\n import com.facebook.presto.execution.Input;\n import com.facebook.presto.execution.QueryInfo;\n@@ -49,6 +48,7 @@\n import com.facebook.presto.server.BasicQueryStats;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.QueryId;\n+import com.facebook.presto.spi.eventlistener.Column;\n import com.facebook.presto.spi.eventlistener.OperatorStatistics;\n import com.facebook.presto.spi.eventlistener.QueryCompletedEvent;\n import com.facebook.presto.spi.eventlistener.QueryContext;\n@@ -167,7 +167,8 @@ public void queryCreatedEvent(BasicQueryInfo queryInfo)\n                                 Optional.empty(),\n                                 Optional.empty(),\n                                 ImmutableList.of(),\n-                                queryInfo.getSession().getTraceToken())));\n+                                queryInfo.getSession().getTraceToken(),\n+                                Optional.empty())));\n     }\n \n     public void queryUpdatedEvent(QueryInfo queryInfo)\n@@ -192,7 +193,8 @@ public void publishQueryProgressEvent(long monotonicallyIncreasingEventId, Basic\n                         Optional.empty(),\n                         Optional.empty(),\n                         ImmutableList.of(),\n-                        queryInfo.getSession().getTraceToken()),\n+                        queryInfo.getSession().getTraceToken(),\n+                        Optional.empty()),\n                 createQueryStatistics(queryInfo),\n                 createQueryContext(queryInfo.getSession(), queryInfo.getResourceGroupId()),\n                 queryInfo.getQueryType(),\n@@ -215,7 +217,8 @@ public void queryImmediateFailureEvent(BasicQueryInfo queryInfo, ExecutionFailur\n                         Optional.empty(),\n                         Optional.empty(),\n                         ImmutableList.of(),\n-                        queryInfo.getSession().getTraceToken()),\n+                        queryInfo.getSession().getTraceToken(),\n+                        Optional.empty()),\n                 new QueryStatistics(\n                         ofMillis(0),\n                         ofMillis(0),\n@@ -359,7 +362,8 @@ private QueryMetadata createQueryMetadata(QueryInfo queryInfo)\n                 queryInfo.getRuntimeOptimizedStages().orElse(ImmutableList.of()).stream()\n                         .map(stageId -> String.valueOf(stageId.getId()))\n                         .collect(toImmutableList()),\n-                queryInfo.getSession().getTraceToken());\n+                queryInfo.getSession().getTraceToken(),\n+                Optional.ofNullable(queryInfo.getUpdateType()));\n     }\n \n     private List<OperatorStatistics> createOperatorStatistics(QueryInfo queryInfo)\n@@ -582,7 +586,8 @@ private static QueryIOMetadata getQueryIOMetadata(QueryInfo queryInfo)\n                     input.getSchema(),\n                     input.getTable(),\n                     input.getColumns().stream()\n-                            .map(Column::getName).collect(Collectors.toList()),\n+                            .map(column -> new Column(column.getName(), column.getType()))\n+                            .collect(Collectors.toList()),\n                     input.getConnectorInfo(),\n                     input.getStatistics(),\n                     input.getSerializedCommitOutput()));\n@@ -596,6 +601,13 @@ private static QueryIOMetadata getQueryIOMetadata(QueryInfo queryInfo)\n                     .map(TableFinishInfo.class::cast)\n                     .findFirst();\n \n+            Optional<List<Column>> outputColumns = queryInfo.getOutput().get().getColumns()\n+                    .map(columns -> columns.stream()\n+                            .map(column -> new Column(\n+                                    column.getName(),\n+                                    column.getType()))\n+                            .collect(toImmutableList()));\n+\n             output = Optional.of(\n                     new QueryOutputMetadata(\n                             queryInfo.getOutput().get().getConnectorId().getCatalogName(),\n@@ -603,8 +615,10 @@ private static QueryIOMetadata getQueryIOMetadata(QueryInfo queryInfo)\n                             queryInfo.getOutput().get().getTable(),\n                             tableFinishInfo.map(TableFinishInfo::getSerializedConnectorOutputMetadata),\n                             tableFinishInfo.map(TableFinishInfo::isJsonLengthLimitExceeded),\n-                            queryInfo.getOutput().get().getSerializedCommitOutput()));\n+                            queryInfo.getOutput().get().getSerializedCommitOutput(),\n+                            outputColumns));\n         }\n+\n         return new QueryIOMetadata(inputs.build(), output);\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/Output.java b/presto-main-base/src/main/java/com/facebook/presto/execution/Output.java\nindex 4868de139da79..74f633b871407 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/execution/Output.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/Output.java\n@@ -16,10 +16,13 @@\n import com.facebook.presto.spi.ConnectorId;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n \n import javax.annotation.concurrent.Immutable;\n \n+import java.util.List;\n import java.util.Objects;\n+import java.util.Optional;\n \n import static java.util.Objects.requireNonNull;\n \n@@ -30,18 +33,21 @@ public final class Output\n     private final String schema;\n     private final String table;\n     private final String serializedCommitOutput;\n+    private final Optional<List<Column>> columns;\n \n     @JsonCreator\n     public Output(\n             @JsonProperty(\"connectorId\") ConnectorId connectorId,\n             @JsonProperty(\"schema\") String schema,\n             @JsonProperty(\"table\") String table,\n-            @JsonProperty(\"serializedCommitOutput\") String serializedCommitOutput)\n+            @JsonProperty(\"serializedCommitOutput\") String serializedCommitOutput,\n+            @JsonProperty(\"columns\") Optional<List<Column>> columns)\n     {\n         this.connectorId = requireNonNull(connectorId, \"connectorId is null\");\n         this.schema = requireNonNull(schema, \"schema is null\");\n         this.table = requireNonNull(table, \"table is null\");\n         this.serializedCommitOutput = requireNonNull(serializedCommitOutput, \"connectorCommitOutput is null\");\n+        this.columns = columns.map(ImmutableList::copyOf);\n     }\n \n     @JsonProperty\n@@ -68,6 +74,12 @@ public String getSerializedCommitOutput()\n         return serializedCommitOutput;\n     }\n \n+    @JsonProperty\n+    public Optional<List<Column>> getColumns()\n+    {\n+        return columns;\n+    }\n+\n     @Override\n     public boolean equals(Object o)\n     {\n@@ -81,12 +93,13 @@ public boolean equals(Object o)\n         return Objects.equals(connectorId, output.connectorId) &&\n                 Objects.equals(schema, output.schema) &&\n                 Objects.equals(table, output.table) &&\n-                Objects.equals(serializedCommitOutput, output.serializedCommitOutput);\n+                Objects.equals(serializedCommitOutput, output.serializedCommitOutput) &&\n+                Objects.equals(columns, output.columns);\n     }\n \n     @Override\n     public int hashCode()\n     {\n-        return Objects.hash(connectorId, schema, table, serializedCommitOutput);\n+        return Objects.hash(connectorId, schema, table, serializedCommitOutput, columns);\n     }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/QueryStateMachine.java b/presto-main-base/src/main/java/com/facebook/presto/execution/QueryStateMachine.java\nindex 010489c76b14e..01830e0f733e6 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/execution/QueryStateMachine.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/QueryStateMachine.java\n@@ -606,7 +606,8 @@ private void addSerializedCommitOutputToOutput(ConnectorCommitHandle commitHandl\n                 outputInfo.getConnectorId(),\n                 outputInfo.getSchema(),\n                 outputInfo.getTable(),\n-                commitHandle.getSerializedCommitOutputForWrite(table))));\n+                commitHandle.getSerializedCommitOutputForWrite(table),\n+                outputInfo.getColumns())));\n     }\n \n     private void addSerializedCommitOutputToInputs(List<?> commitHandles)\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/OutputExtractor.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/OutputExtractor.java\nindex 196124e3f79f7..bd4418d337e1f 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/OutputExtractor.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/OutputExtractor.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.sql.planner;\n \n+import com.facebook.presto.execution.Column;\n import com.facebook.presto.execution.Output;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.SchemaTableName;\n@@ -21,10 +22,14 @@\n import com.facebook.presto.sql.planner.plan.InternalPlanVisitor;\n import com.facebook.presto.sql.planner.plan.SequenceNode;\n import com.google.common.base.VerifyException;\n+import com.google.common.collect.ImmutableList;\n \n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.Optional;\n \n import static com.facebook.presto.spi.connector.ConnectorCommitHandle.EMPTY_COMMIT_OUTPUT;\n+import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n \n public class OutputExtractor\n@@ -37,12 +42,12 @@ public Optional<Output> extractOutput(PlanNode root)\n         if (visitor.getConnectorId() == null) {\n             return Optional.empty();\n         }\n-\n         return Optional.of(new Output(\n                 visitor.getConnectorId(),\n                 visitor.getSchemaTableName().getSchemaName(),\n                 visitor.getSchemaTableName().getTableName(),\n-                EMPTY_COMMIT_OUTPUT));\n+                EMPTY_COMMIT_OUTPUT,\n+                Optional.of(ImmutableList.copyOf(visitor.getColumns()))));\n     }\n \n     private class Visitor\n@@ -50,6 +55,7 @@ private class Visitor\n     {\n         private ConnectorId connectorId;\n         private SchemaTableName schemaTableName;\n+        private List<Column> columns = new ArrayList<>();\n \n         @Override\n         public Void visitTableWriter(TableWriterNode node, Void context)\n@@ -59,6 +65,11 @@ public Void visitTableWriter(TableWriterNode node, Void context)\n             checkState(schemaTableName == null || schemaTableName.equals(writerTarget.getSchemaTableName()),\n                     \"cannot have more than a single create, insert or delete in a query\");\n             schemaTableName = writerTarget.getSchemaTableName();\n+\n+            checkArgument(node.getColumnNames().size() == node.getColumns().size(), \"Column names and columns sizes must be equal\");\n+            for (int i = 0; i < node.getColumnNames().size(); i++) {\n+                columns.add(new Column(node.getColumnNames().get(i), node.getColumns().get(i).getType().toString()));\n+            }\n             return null;\n         }\n \n@@ -88,5 +99,10 @@ public SchemaTableName getSchemaTableName()\n         {\n             return schemaTableName;\n         }\n+\n+        public List<Column> getColumns()\n+        {\n+            return columns;\n+        }\n     }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/Column.java b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/Column.java\nnew file mode 100644\nindex 0000000000000..9e3c9b69726b5\n--- /dev/null\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/Column.java\n@@ -0,0 +1,71 @@\n+package com.facebook.presto.spi.eventlistener;\n+\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public final class Column\n+{\n+    private final String name;\n+    private final String type;\n+\n+    @JsonCreator\n+    public Column(\n+            @JsonProperty(\"name\") String name,\n+            @JsonProperty(\"type\") String type)\n+    {\n+        this.name = requireNonNull(name, \"name is null\");\n+        this.type = requireNonNull(type, \"type is null\");\n+    }\n+\n+    @JsonProperty\n+    public String getName()\n+    {\n+        return name;\n+    }\n+\n+    @JsonProperty\n+    public String getType()\n+    {\n+        return type;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o)\n+    {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+\n+        Column that = (Column) o;\n+\n+        return Objects.equals(this.name, that.name) &&\n+                Objects.equals(this.type, that.type);\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(name, type);\n+    }\n+}\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java\nindex c00c1ee3d7819..425c37e2c3115 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryInputMetadata.java\n@@ -16,8 +16,10 @@\n import com.facebook.presto.spi.statistics.TableStatistics;\n import com.fasterxml.jackson.annotation.JsonProperty;\n \n+import java.util.Collections;\n import java.util.List;\n import java.util.Optional;\n+import java.util.stream.Collectors;\n \n import static java.util.Objects.requireNonNull;\n \n@@ -26,17 +28,17 @@ public class QueryInputMetadata\n     private final String catalogName;\n     private final String schema;\n     private final String table;\n-    private final List<String> columns;\n+    private final List<Column> columnObjects;\n     private final Optional<Object> connectorInfo;\n     private final Optional<TableStatistics> statistics;\n     private final String serializedCommitOutput;\n \n-    public QueryInputMetadata(String catalogName, String schema, String table, List<String> columns, Optional<Object> connectorInfo, Optional<TableStatistics> statistics, String serializedCommitOutput)\n+    public QueryInputMetadata(String catalogName, String schema, String table, List<Column> columnObjects, Optional<Object> connectorInfo, Optional<TableStatistics> statistics, String serializedCommitOutput)\n     {\n         this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n         this.schema = requireNonNull(schema, \"schema is null\");\n         this.table = requireNonNull(table, \"table is null\");\n-        this.columns = requireNonNull(columns, \"columns is null\");\n+        this.columnObjects = requireNonNull(columnObjects, \"columns is null\");\n         this.connectorInfo = requireNonNull(connectorInfo, \"connectorInfo is null\");\n         this.statistics = requireNonNull(statistics, \"table statistics is null\");\n         this.serializedCommitOutput = requireNonNull(serializedCommitOutput, \"serializedCommitOutput is null\");\n@@ -63,7 +65,13 @@ public String getTable()\n     @JsonProperty\n     public List<String> getColumns()\n     {\n-        return columns;\n+        return Collections.unmodifiableList(columnObjects.stream().map(Column::getName).collect(Collectors.toList()));\n+    }\n+\n+    @JsonProperty\n+    public List<Column> getColumnObjects()\n+    {\n+        return columnObjects;\n     }\n \n     @JsonProperty\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryMetadata.java\nindex ca96cdba0bb44..1fe654baf6c57 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryMetadata.java\n@@ -42,6 +42,7 @@ public class QueryMetadata\n     private final Optional<String> payload;\n \n     private final List<String> runtimeOptimizedStages;\n+    private final Optional<String> updateQueryType;\n \n     public QueryMetadata(\n             String queryId,\n@@ -56,7 +57,8 @@ public QueryMetadata(\n             Optional<String> graphvizPlan,\n             Optional<String> payload,\n             List<String> runtimeOptimizedStages,\n-            Optional<String> tracingId)\n+            Optional<String> tracingId,\n+            Optional<String> updateQueryType)\n     {\n         this.queryId = requireNonNull(queryId, \"queryId is null\");\n         this.transactionId = requireNonNull(transactionId, \"transactionId is null\");\n@@ -71,6 +73,7 @@ public QueryMetadata(\n         this.payload = requireNonNull(payload, \"payload is null\");\n         this.runtimeOptimizedStages = requireNonNull(runtimeOptimizedStages, \"runtimeOptimizedStages is null\");\n         this.tracingId = requireNonNull(tracingId, \"tracingId is null\");\n+        this.updateQueryType = requireNonNull(updateQueryType, \"updateQueryType is null\");\n     }\n \n     @JsonProperty\n@@ -150,4 +153,9 @@ public Optional<String> getTracingId()\n     {\n         return tracingId;\n     }\n+\n+    public Optional<String> getUpdateQueryType()\n+    {\n+        return updateQueryType;\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryOutputMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryOutputMetadata.java\nindex ac09f44e09a48..22b33bc695670 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryOutputMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/eventlistener/QueryOutputMetadata.java\n@@ -15,6 +15,7 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n \n+import java.util.List;\n import java.util.Optional;\n \n import static java.util.Objects.requireNonNull;\n@@ -29,6 +30,7 @@ public class QueryOutputMetadata\n     private final Optional<Boolean> jsonLengthLimitExceeded;\n \n     private final String serializedCommitOutput;\n+    private final Optional<List<Column>> columns;\n \n     public QueryOutputMetadata(\n             String catalogName,\n@@ -36,7 +38,8 @@ public QueryOutputMetadata(\n             String table,\n             Optional<String> connectorOutputMetadata,\n             Optional<Boolean> jsonLengthLimitExceeded,\n-            String serializedCommitOutput)\n+            String serializedCommitOutput,\n+            Optional<List<Column>> columns)\n     {\n         this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n         this.schema = requireNonNull(schema, \"schema is null\");\n@@ -44,6 +47,7 @@ public QueryOutputMetadata(\n         this.connectorOutputMetadata = requireNonNull(connectorOutputMetadata, \"connectorOutputMetadata is null\");\n         this.jsonLengthLimitExceeded = requireNonNull(jsonLengthLimitExceeded, \"jsonLengthLimitExceeded is null\");\n         this.serializedCommitOutput = requireNonNull(serializedCommitOutput, \"connectorCommitHandle is null\");\n+        this.columns = requireNonNull(columns, \"columns is null\");\n     }\n \n     @JsonProperty\n@@ -81,4 +85,10 @@ public String getSerializedCommitOutput()\n     {\n         return serializedCommitOutput;\n     }\n+\n+    @JsonProperty\n+    public Optional<List<Column>> getColumns()\n+    {\n+        return columns;\n+    }\n }\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java\nnew file mode 100644\nindex 0000000000000..a9b16de301b6e\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestOutputColumnTypes.java\n@@ -0,0 +1,276 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.spi.Plugin;\n+import com.facebook.presto.spi.eventlistener.Column;\n+import com.facebook.presto.spi.eventlistener.EventListener;\n+import com.facebook.presto.spi.eventlistener.EventListenerFactory;\n+import com.facebook.presto.spi.eventlistener.QueryCompletedEvent;\n+import com.facebook.presto.spi.eventlistener.QueryCreatedEvent;\n+import com.facebook.presto.spi.eventlistener.SplitCompletedEvent;\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.AbstractTestQueryFramework;\n+import com.google.common.collect.ImmutableList;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+@Test(singleThreaded = true)\n+public class TestOutputColumnTypes\n+        extends AbstractTestQueryFramework\n+{\n+    public static final String ICEBERG_CATALOG = \"iceberg\";\n+    private static final Duration EVENT_TIMEOUT = Duration.ofSeconds(10);\n+\n+    private QueryRunner queryRunner;\n+    private final CatalogType catalogType;\n+    private final EventsBuilder generatedEvents = new EventsBuilder();\n+    private final Session session;\n+\n+    public TestOutputColumnTypes()\n+            throws Exception\n+    {\n+        this.catalogType = CatalogType.HIVE;\n+        this.queryRunner = createQueryRunner();\n+        this.queryRunner.installPlugin(new TestingEventListenerPlugin(generatedEvents));\n+        session = testSessionBuilder()\n+                .setCatalog(ICEBERG_CATALOG)\n+                .setSchema(\"tpch\")\n+                .build();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    private void tearDown()\n+    {\n+        queryRunner.close();\n+        queryRunner = null;\n+    }\n+\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return IcebergQueryRunner.builder().build().getQueryRunner();\n+    }\n+\n+    private MaterializedResult runQueryAndWaitForEvents(@Language(\"SQL\") String sql, int numEventsExpected)\n+            throws Exception\n+    {\n+        return runQueryAndWaitForEvents(sql, numEventsExpected, session);\n+    }\n+\n+    private MaterializedResult runQueryAndWaitForEvents(@Language(\"SQL\") String sql, int numEventsExpected, Session session)\n+            throws Exception\n+    {\n+        generatedEvents.initialize(numEventsExpected);\n+        MaterializedResult result = queryRunner.execute(session, sql);\n+        generatedEvents.waitForEvents(EVENT_TIMEOUT);\n+        return result;\n+    }\n+\n+    @Test\n+    public void testOutputColumnsForInsertAsSelect()\n+            throws Exception\n+    {\n+        runQueryAndWaitForEvents(\"CREATE TABLE create_insert_table1 AS SELECT clerk, orderkey, totalprice FROM orders\", 3);\n+        runQueryAndWaitForEvents(\"INSERT INTO create_insert_table1 SELECT clerk, orderkey, totalprice FROM orders\", 3);\n+        QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n+        assertThat(event.getIoMetadata().getOutput().get().getSchema()).isEqualTo(\"tpch\");\n+        assertThat(event.getIoMetadata().getOutput().get().getTable()).isEqualTo(\"create_insert_table1\");\n+        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"CREATE TABLE\");\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getColumns().get())\n+                .containsExactly(\n+                        new Column(\"clerk\", \"varchar\"),\n+                        new Column(\"orderkey\", \"bigint\"),\n+                        new Column(\"totalprice\", \"double\"));\n+    }\n+\n+    @Test\n+    public void testOutputColumnsForUpdate()\n+            throws Exception\n+    {\n+        runQueryAndWaitForEvents(\"CREATE TABLE create_update_table AS SELECT * FROM orders \", 3);\n+        runQueryAndWaitForEvents(\"UPDATE create_update_table SET clerk = 're-reset'\", 3);\n+        QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n+        assertThat(event.getIoMetadata().getOutput().get().getSchema()).isEqualTo(\"tpch\");\n+        assertThat(event.getIoMetadata().getOutput().get().getTable()).isEqualTo(\"create_update_table\");\n+        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"CREATE TABLE\");\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getColumns().get())\n+                .containsExactly(\n+                        new Column(\"orderkey\", \"bigint\"),\n+                        new Column(\"custkey\", \"bigint\"),\n+                        new Column(\"orderstatus\", \"varchar\"),\n+                        new Column(\"totalprice\", \"double\"),\n+                        new Column(\"orderdate\", \"date\"),\n+                        new Column(\"orderpriority\", \"varchar\"),\n+                        new Column(\"clerk\", \"varchar\"),\n+                        new Column(\"shippriority\", \"integer\"),\n+                        new Column(\"comment\", \"varchar\"));\n+    }\n+\n+    @Test\n+    public void testOutputColumnsForDeleteWithWhere()\n+            throws Exception\n+    {\n+        runQueryAndWaitForEvents(\"CREATE TABLE create_del_table AS SELECT clerk, orderkey, totalprice FROM orders \", 3);\n+        runQueryAndWaitForEvents(\"DELETE FROM create_del_table WHERE orderkey = 1\", 3);\n+        QueryCompletedEvent event = getOnlyElement(generatedEvents.getQueryCompletedEvents());\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getCatalogName()).isEqualTo(\"iceberg\");\n+        assertThat(event.getIoMetadata().getOutput().get().getSchema()).isEqualTo(\"tpch\");\n+        assertThat(event.getIoMetadata().getOutput().get().getTable()).isEqualTo(\"create_del_table\");\n+        assertThat(event.getMetadata().getUpdateQueryType().get()).isEqualTo(\"CREATE TABLE\");\n+\n+        assertThat(event.getIoMetadata().getOutput().get().getColumns().get())\n+                .containsExactly(\n+                        new Column(\"clerk\", \"varchar\"),\n+                        new Column(\"orderkey\", \"bigint\"),\n+                        new Column(\"totalprice\", \"double\"));\n+    }\n+\n+    static class TestingEventListenerPlugin\n+            implements Plugin\n+    {\n+        private final EventsBuilder eventsBuilder;\n+\n+        public TestingEventListenerPlugin(EventsBuilder eventsBuilder)\n+        {\n+            this.eventsBuilder = requireNonNull(eventsBuilder, \"eventsBuilder is null\");\n+        }\n+\n+        @Override\n+        public Iterable<EventListenerFactory> getEventListenerFactories()\n+        {\n+            return ImmutableList.of(new TestingEventListenerFactory(eventsBuilder));\n+        }\n+    }\n+\n+    private static class TestingEventListenerFactory\n+            implements EventListenerFactory\n+    {\n+        private final EventsBuilder eventsBuilder;\n+\n+        public TestingEventListenerFactory(EventsBuilder eventsBuilder)\n+        {\n+            this.eventsBuilder = eventsBuilder;\n+        }\n+\n+        @Override\n+        public String getName()\n+        {\n+            return \"test\";\n+        }\n+\n+        @Override\n+        public EventListener create(Map<String, String> config)\n+        {\n+            return new TestingEventListener(eventsBuilder);\n+        }\n+    }\n+\n+    private static class TestingEventListener\n+            implements EventListener\n+    {\n+        private final EventsBuilder eventsBuilder;\n+\n+        public TestingEventListener(EventsBuilder eventsBuilder)\n+        {\n+            this.eventsBuilder = eventsBuilder;\n+        }\n+\n+        @Override\n+        public void queryCreated(QueryCreatedEvent queryCreatedEvent)\n+        {\n+            eventsBuilder.addQueryCreated(queryCreatedEvent);\n+        }\n+\n+        @Override\n+        public void queryCompleted(QueryCompletedEvent queryCompletedEvent)\n+        {\n+            eventsBuilder.addQueryCompleted(queryCompletedEvent);\n+        }\n+\n+        @Override\n+        public void splitCompleted(SplitCompletedEvent splitCompletedEvent)\n+        {\n+            eventsBuilder.addSplitCompleted(splitCompletedEvent);\n+        }\n+    }\n+\n+    static class EventsBuilder\n+    {\n+        private ImmutableList.Builder<QueryCreatedEvent> queryCreatedEvents;\n+        private ImmutableList.Builder<QueryCompletedEvent> queryCompletedEvents;\n+        private ImmutableList.Builder<SplitCompletedEvent> splitCompletedEvents;\n+\n+        private CountDownLatch eventsLatch;\n+\n+        public synchronized void initialize(int numEvents)\n+        {\n+            queryCreatedEvents = ImmutableList.builder();\n+            queryCompletedEvents = ImmutableList.builder();\n+            splitCompletedEvents = ImmutableList.builder();\n+\n+            eventsLatch = new CountDownLatch(numEvents);\n+        }\n+        public void waitForEvents(Duration duration)\n+                throws InterruptedException\n+        {\n+            eventsLatch.await(duration.getNano(), NANOSECONDS);\n+        }\n+\n+        public synchronized void addQueryCreated(QueryCreatedEvent event)\n+        {\n+            queryCreatedEvents.add(event);\n+            eventsLatch.countDown();\n+        }\n+\n+        public synchronized void addQueryCompleted(QueryCompletedEvent event)\n+        {\n+            queryCompletedEvents.add(event);\n+            eventsLatch.countDown();\n+        }\n+\n+        public synchronized void addSplitCompleted(SplitCompletedEvent event)\n+        {\n+            splitCompletedEvents.add(event);\n+            eventsLatch.countDown();\n+        }\n+\n+        public List<QueryCompletedEvent> getQueryCompletedEvents()\n+        {\n+            return queryCompletedEvents.build();\n+        }\n+    }\n+}\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java b/presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java\nindex 6de37ad4205e9..a427cf4f4878b 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/TestOutput.java\n@@ -15,8 +15,11 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.spi.ConnectorId;\n+import com.google.common.collect.ImmutableList;\n import org.testng.annotations.Test;\n \n+import java.util.Optional;\n+\n import static com.facebook.presto.spi.connector.ConnectorCommitHandle.EMPTY_COMMIT_OUTPUT;\n import static org.testng.Assert.assertEquals;\n \n@@ -31,7 +34,10 @@ public void testRoundTrip()\n                 new ConnectorId(\"connectorId\"),\n                 \"schema\",\n                 \"table\",\n-                EMPTY_COMMIT_OUTPUT);\n+                EMPTY_COMMIT_OUTPUT,\n+                Optional.of(\n+                        ImmutableList.of(\n+                                new Column(\"column\", \"type\"))));\n \n         String json = codec.toJson(expected);\n         Output actual = codec.fromJson(json);\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java\nindex ac7179172f08c..cccb271166fb7 100644\n--- a/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java\n+++ b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java\n@@ -20,6 +20,7 @@\n import com.facebook.presto.common.resourceGroups.QueryType;\n import com.facebook.presto.spi.PrestoWarning;\n import com.facebook.presto.spi.eventlistener.CTEInformation;\n+import com.facebook.presto.spi.eventlistener.Column;\n import com.facebook.presto.spi.eventlistener.EventListener;\n import com.facebook.presto.spi.eventlistener.EventListenerFactory;\n import com.facebook.presto.spi.eventlistener.OperatorStatistics;\n@@ -302,6 +303,7 @@ private static QueryMetadata createDummyQueryMetadata()\n         Optional<String> payload = Optional.of(\"dummy-payload\");\n         List<String> runtimeOptimizedStages = new ArrayList<>(Arrays.asList(\"stage1\", \"stage2\"));\n         Optional<String> tracingId = Optional.of(\"dummy-tracing-id\");\n+        Optional<String> updateType = Optional.of(\"dummy-type\");\n \n         return new QueryMetadata(\n                 queryId,\n@@ -316,7 +318,8 @@ private static QueryMetadata createDummyQueryMetadata()\n                 graphvizPlan,\n                 payload,\n                 runtimeOptimizedStages,\n-                tracingId);\n+                tracingId,\n+                updateType);\n     }\n \n     private static QueryContext createDummyQueryContext()\n@@ -371,13 +374,18 @@ private static QueryIOMetadata createDummyQueryIoMetadata()\n         List<QueryInputMetadata> inputs = new ArrayList<>();\n         QueryInputMetadata queryInputMetadata = getQueryInputMetadata();\n         inputs.add(queryInputMetadata);\n+        Column column1 = new Column(\"column1\", \"int\");\n+        Column column2 = new Column(\"column2\", \"varchar\");\n+        Column column3 = new Column(\"column3\", \"varchar\");\n+        List<Column> columns = Arrays.asList(column1, column2, column3);\n         QueryOutputMetadata outputMetadata = new QueryOutputMetadata(\n                 \"dummyCatalog\",\n                 \"dummySchema\",\n                 \"dummyTable\",\n                 Optional.of(\"dummyConnectorMetadata\"),\n                 Optional.of(true),\n-                \"dummySerializedCommitOutput\");\n+                \"dummySerializedCommitOutput\",\n+                Optional.of(columns));\n         return new QueryIOMetadata(inputs, Optional.of(outputMetadata));\n     }\n \n@@ -387,7 +395,10 @@ private static QueryInputMetadata getQueryInputMetadata()\n         String schema = \"dummySchema\";\n         String table = \"dummyTable\";\n         String serializedCommitOutput = \"commitOutputDummy\";\n-        List<String> columns = new ArrayList<>(Arrays.asList(\"column1\", \"column2\", \"column3\"));\n+        Column column1 = new Column(\"column1\", \"int\");\n+        Column column2 = new Column(\"column2\", \"varchar\");\n+        Column column3 = new Column(\"column3\", \"varchar\");\n+        List<Column> columns = Arrays.asList(column1, column2, column3);\n         Optional<Object> connectorInfo = Optional.of(new Object());\n         return new QueryInputMetadata(\n                 catalogName,\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24884",
    "pr_id": 24884,
    "issue_id": 24510,
    "repo": "prestodb/presto",
    "problem_statement": "Add distinct if needed to the right side of a semijoin\nLooks like SetBuilder works best when you give it distinct elements. So when we see something like:\n\nselect ... from t1 \nwhere id in (select id from t2)\n\nwe should distinct id in t2",
    "issue_word_count": 44,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/LeftJoinNullFilterToSemiJoin.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "base_commit": "899f740a311e94ac7f60aff6e2e0aff3f643a0e8",
    "head_commit": "a239977ee7c8832a310b30c100543ad26b74d057",
    "repo_url": "https://github.com/prestodb/presto/pull/24884",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24884",
    "dockerfile": "",
    "pr_merged_at": "2025-04-10T18:41:36.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/LeftJoinNullFilterToSemiJoin.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/LeftJoinNullFilterToSemiJoin.java\nindex b252f7c8845fd..39849c62eb0f4 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/LeftJoinNullFilterToSemiJoin.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/LeftJoinNullFilterToSemiJoin.java\n@@ -18,6 +18,7 @@\n import com.facebook.presto.matching.Captures;\n import com.facebook.presto.matching.Pattern;\n import com.facebook.presto.metadata.FunctionAndTypeManager;\n+import com.facebook.presto.spi.plan.AggregationNode;\n import com.facebook.presto.spi.plan.Assignments;\n import com.facebook.presto.spi.plan.FilterNode;\n import com.facebook.presto.spi.plan.JoinNode;\n@@ -29,7 +30,9 @@\n import com.facebook.presto.spi.relation.SpecialFormExpression;\n import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.sql.planner.iterative.Rule;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n \n import java.util.List;\n import java.util.Map;\n@@ -136,7 +139,33 @@ public Result apply(FilterNode filterNode, Captures captures, Context context)\n         }\n \n         VariableReferenceExpression semiJoinOutput = context.getVariableAllocator().newVariable(\"semiJoinOutput\", BOOLEAN);\n-        SemiJoinNode semiJoinNode = new SemiJoinNode(filterNode.getSourceLocation(), context.getIdAllocator().getNextId(), joinNode.getLeft(), joinNode.getRight(),\n+\n+        PlanNode semiJoinFilteringSource = joinNode.getRight();\n+        // TODO(sreeni) Check if this is alreay distinct (visitng the children to see if it's an agg and/or look at properties)\n+        // for now we unconditionally add distinct\n+\n+        // Skip adding distinct node if a distinct is already applied.\n+        if (!((semiJoinFilteringSource instanceof AggregationNode\n+                && AggregationNode.isDistinct((AggregationNode) semiJoinFilteringSource))\n+                || (semiJoinFilteringSource instanceof ProjectNode &&\n+                ((ProjectNode) semiJoinFilteringSource).getSource() instanceof AggregationNode &&\n+                AggregationNode.isDistinct((AggregationNode) ((ProjectNode) semiJoinFilteringSource).getSource())))) {\n+            AggregationNode.GroupingSetDescriptor groupingSetDescriptor = new AggregationNode.GroupingSetDescriptor(ImmutableList.of(semiJoinFilteringSource.getOutputVariables().get(0)), 1, ImmutableSet.of());\n+            semiJoinFilteringSource = new AggregationNode(\n+                    semiJoinFilteringSource.getSourceLocation(),\n+                    context.getIdAllocator().getNextId(),\n+                    Optional.empty(),\n+                    semiJoinFilteringSource,\n+                    ImmutableMap.of(),\n+                    groupingSetDescriptor,\n+                    ImmutableList.of(),\n+                    AggregationNode.Step.SINGLE,\n+                    Optional.empty(),\n+                    Optional.empty(),\n+                    Optional.empty());\n+        }\n+\n+        SemiJoinNode semiJoinNode = new SemiJoinNode(filterNode.getSourceLocation(), context.getIdAllocator().getNextId(), joinNode.getLeft(), semiJoinFilteringSource,\n                 leftKey, rightKey, semiJoinOutput, Optional.empty(), Optional.empty(), Optional.empty(), ImmutableMap.of());\n         RowExpression filterExpression = not(functionAndTypeManager, coalesceNullToFalse(semiJoinOutput));\n         FilterNode semiFilterNode = new FilterNode(filterNode.getSourceLocation(), context.getIdAllocator().getNextId(), semiJoinNode, filterExpression);\n",
    "test_patch": "diff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java\nindex bedc64ed1479b..360edfef42856 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestLeftJoinNullFilterToSemiJoin.java\n@@ -13,15 +13,22 @@\n  */\n package com.facebook.presto.sql.planner.iterative.rule;\n \n+import com.facebook.presto.spi.plan.AggregationNode;\n import com.facebook.presto.spi.plan.EquiJoinClause;\n import com.facebook.presto.spi.plan.JoinType;\n import com.facebook.presto.sql.planner.iterative.rule.test.BaseRuleTest;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import org.testng.annotations.Test;\n \n+import java.util.Optional;\n+\n import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.aggregation;\n import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.filter;\n import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.project;\n import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.semiJoin;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.singleGroupingSet;\n import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.values;\n \n public class TestLeftJoinNullFilterToSemiJoin\n@@ -52,7 +59,7 @@ public void testTrigger()\n                                                 \"right_k1\",\n                                                 \"semijoinoutput\",\n                                                 values(\"left_k1\", \"left_k2\"),\n-                                                values(\"right_k1\")))));\n+                                                aggregation(singleGroupingSet(ImmutableList.of(\"right_k1\")), ImmutableMap.of(), ImmutableMap.of(), Optional.empty(), AggregationNode.Step.SINGLE, values(\"right_k1\"))))));\n     }\n \n     @Test\n@@ -192,6 +199,6 @@ public void testTriggerForFilterWithAnd()\n                                                         \"right_k1\",\n                                                         \"semijoinoutput\",\n                                                         values(\"left_k1\", \"left_k2\"),\n-                                                        values(\"right_k1\"))))));\n+                                                        aggregation(singleGroupingSet(ImmutableList.of(\"right_k1\")), ImmutableMap.of(), ImmutableMap.of(), Optional.empty(), AggregationNode.Step.SINGLE, values(\"right_k1\")))))));\n     }\n }\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\nindex f303dd07651b3..faa4d6a884778 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n@@ -7426,10 +7426,26 @@ public void testLeftJoinNullFilterToSemiJoin()\n         // null in both sides\n         sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3, null) t(k)), t2 as (select * from (values 1, 1, 2, 2, null) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n         assertQuery(enableOptimization, sql, \"values 3, 3, null\");\n+        sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3, null) t(k)), t2 as (select * from (values 1, 1, 2, 2, null, null, null, null) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n+        assertQuery(enableOptimization, sql, \"values 3, 3, null\");\n+        assertNotEquals(computeActual(\"EXPLAIN(TYPE DISTRIBUTED) \" + sql).getOnlyValue().toString().indexOf(\"Aggregate\"), -1);\n \n         // null in right side\n         sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3) t(k)), t2 as (select * from (values 1, 1, 2, 2, null) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n         assertQuery(enableOptimization, sql, \"values 3, 3\");\n+        sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3) t(k)), t2 as (select * from (values 1, 1, 2, 2, null, null, null, null) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n+        assertQuery(enableOptimization, sql, \"values 3, 3\");\n+        assertNotEquals(computeActual(\"EXPLAIN(TYPE DISTRIBUTED) \" + sql).getOnlyValue().toString().indexOf(\"Aggregate\"), -1);\n+\n+        // right side already has distinct should apply no more distinct\n+        sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3) t(k)), t2 as (select distinct(k) from (values 1, 1, 2, 2, null) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n+        assertQuery(enableOptimization, sql, \"values 3, 3\");\n+        assertNotEquals(computeActual(\"EXPLAIN(TYPE DISTRIBUTED) \" + sql).getOnlyValue().toString().indexOf(\"Aggregate\"), -1);\n+\n+        // right side has group by but not distinct\n+        sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3) t(k)), t2 as (select k from (values 1, 1, 2, 2, null) t(k) group by k having count(1) > 1) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n+        assertQuery(enableOptimization, sql, \"values 3, 3\");\n+        assertNotEquals(computeActual(\"EXPLAIN(TYPE DISTRIBUTED) \" + sql).getOnlyValue().toString().indexOf(\"Aggregate\"), -1);\n \n         // null in left side\n         sql = \"with t1 as (select * from (values 1, 1, 2, 2, 3, 3, null) t(k)), t2 as (select * from (values 1, 1, 2, 2) t(k)) select t1.* from t1 left join t2 on t1.k=t2.k where t2.k is null\";\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24857",
    "pr_id": 24857,
    "issue_id": 24807,
    "repo": "prestodb/presto",
    "problem_statement": "Remove use of TableWriteInfo::DeleteScanInfo\nCurrently, TableWriteInfo::DeleteScanInfo provides a TableHandle instance that can override the TableScanNode TableHandle in LocalExecutionPlanner.  It looks like the original intent of this was for ConnectorMetadata::beginDelete() implementations to be able to return a modified TableHandle for use in the TableScanOperator or ScanFilterAndProjectOperator that are generated.  However, none of the existing ConnectorMetadata::beginDelete() implementations seem to be using this.  It's not clear when this would be needed and the TableScan's TableHandle should not be used.\n\nIn #24528 the return type of beginDelete() was changed to DeleteTableHandle, making it incompatible with the TableScan TableHandle, so DeleteScanInfo was changed to use the TableScan's TableHandle.  However, the use of DeleteScanInfo for overriding the TableHandle instance now has no effect, since the TableHandle should match what is already in the TableScan.\n\nUnless this feature is actually used somewhere, we should clean up DeleteScanInfo and all the references to it, in order to simplify the existing code, and clarify the contract for DELETE queries.  Where necessary, a ConnectorOptimizer can still rewrite TableScanNodes for DELETEs.\n",
    "issue_word_count": 177,
    "test_files_count": 14,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-main-base/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/SplitSourceFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java",
      "presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java",
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java",
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java",
      "presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.cpp",
      "presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.h",
      "presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java",
      "presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java",
      "presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java",
      "presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java",
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java",
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java",
      "presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java",
      "presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java",
      "presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java"
    ],
    "base_commit": "899f740a311e94ac7f60aff6e2e0aff3f643a0e8",
    "head_commit": "1fc32d2420cda7215d3168869009f8f311851f24",
    "repo_url": "https://github.com/prestodb/presto/pull/24857",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24857",
    "dockerfile": "",
    "pr_merged_at": "2025-04-10T19:21:27.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java b/presto-main-base/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\nindex 793b6b8f451e0..63d952c1c0ce1 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\n@@ -15,29 +15,16 @@\n package com.facebook.presto.execution.scheduler;\n \n import com.facebook.presto.Session;\n-import com.facebook.presto.common.predicate.TupleDomain;\n import com.facebook.presto.metadata.AnalyzeTableHandle;\n import com.facebook.presto.metadata.Metadata;\n-import com.facebook.presto.metadata.TableLayoutResult;\n-import com.facebook.presto.spi.ColumnHandle;\n-import com.facebook.presto.spi.Constraint;\n-import com.facebook.presto.spi.TableHandle;\n-import com.facebook.presto.spi.plan.DeleteNode;\n-import com.facebook.presto.spi.plan.FilterNode;\n-import com.facebook.presto.spi.plan.JoinNode;\n import com.facebook.presto.spi.plan.PlanNode;\n-import com.facebook.presto.spi.plan.PlanNodeId;\n-import com.facebook.presto.spi.plan.ProjectNode;\n-import com.facebook.presto.spi.plan.SemiJoinNode;\n import com.facebook.presto.spi.plan.TableFinishNode;\n-import com.facebook.presto.spi.plan.TableScanNode;\n import com.facebook.presto.spi.plan.TableWriterNode;\n import com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher;\n import com.facebook.presto.sql.planner.plan.StatisticsWriterNode;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.base.VerifyException;\n-import com.google.common.collect.ImmutableSet;\n \n import java.util.Collection;\n import java.util.List;\n@@ -55,35 +42,29 @@ public class TableWriteInfo\n {\n     private final Optional<ExecutionWriterTarget> writerTarget;\n     private final Optional<AnalyzeTableHandle> analyzeTableHandle;\n-    private final Optional<DeleteScanInfo> deleteScanInfo;\n \n     @JsonCreator\n     public TableWriteInfo(\n             @JsonProperty(\"writerTarget\") Optional<ExecutionWriterTarget> writerTarget,\n-            @JsonProperty(\"analyzeTableHandle\") Optional<AnalyzeTableHandle> analyzeTableHandle,\n-            @JsonProperty(\"deleteScanInfo\") Optional<DeleteScanInfo> deleteScanInfo)\n+            @JsonProperty(\"analyzeTableHandle\") Optional<AnalyzeTableHandle> analyzeTableHandle)\n     {\n         this.writerTarget = requireNonNull(writerTarget, \"writerTarget is null\");\n         this.analyzeTableHandle = requireNonNull(analyzeTableHandle, \"analyzeTableHandle is null\");\n-        this.deleteScanInfo = requireNonNull(deleteScanInfo, \"deleteScanInfo is null\");\n-        checkArgument(!analyzeTableHandle.isPresent() || !writerTarget.isPresent() && !deleteScanInfo.isPresent(), \"analyzeTableHandle is present, so no other fields should be present\");\n-        checkArgument(!deleteScanInfo.isPresent() || writerTarget.isPresent(), \"deleteScanInfo is present, but writerTarget is not present\");\n+        checkArgument(!analyzeTableHandle.isPresent() || !writerTarget.isPresent(), \"analyzeTableHandle is present, so no other fields should be present\");\n     }\n \n     public static TableWriteInfo createTableWriteInfo(StreamingSubPlan plan, Metadata metadata, Session session)\n     {\n         Optional<ExecutionWriterTarget> writerTarget = createWriterTarget(plan, metadata, session);\n         Optional<AnalyzeTableHandle> analyzeTableHandle = createAnalyzeTableHandle(plan, metadata, session);\n-        Optional<DeleteScanInfo> deleteScanInfo = createDeleteScanInfo(plan, writerTarget, metadata, session);\n-        return new TableWriteInfo(writerTarget, analyzeTableHandle, deleteScanInfo);\n+        return new TableWriteInfo(writerTarget, analyzeTableHandle);\n     }\n \n     public static TableWriteInfo createTableWriteInfo(PlanNode planNode, Metadata metadata, Session session)\n     {\n         Optional<ExecutionWriterTarget> writerTarget = createWriterTarget(planNode, metadata, session);\n         Optional<AnalyzeTableHandle> analyzeTableHandle = createAnalyzeTableHandle(planNode, metadata, session);\n-        Optional<DeleteScanInfo> deleteScanInfo = createDeleteScanInfo(planNode, writerTarget, metadata, session);\n-        return new TableWriteInfo(writerTarget, analyzeTableHandle, deleteScanInfo);\n+        return new TableWriteInfo(writerTarget, analyzeTableHandle);\n     }\n \n     private static Optional<ExecutionWriterTarget> createWriterTarget(Optional<TableFinishNode> finishNodeOptional, Metadata metadata, Session session)\n@@ -141,37 +122,6 @@ private static Optional<AnalyzeTableHandle> createAnalyzeTableHandle(Optional<St\n         return statisticsWriterNodeOptional.map(node -> metadata.beginStatisticsCollection(session, node.getTableHandle()));\n     }\n \n-    private static Optional<DeleteScanInfo> createDeleteScanInfo(StreamingSubPlan plan, Optional<ExecutionWriterTarget> writerTarget, Metadata metadata, Session session)\n-    {\n-        if (writerTarget.isPresent() && writerTarget.get() instanceof ExecutionWriterTarget.DeleteHandle) {\n-            DeleteNode delete = getOnlyElement(findPlanNodes(plan, DeleteNode.class));\n-            return createDeleteScanInfo(delete, metadata, session);\n-        }\n-        return Optional.empty();\n-    }\n-\n-    private static Optional<DeleteScanInfo> createDeleteScanInfo(PlanNode planNode, Optional<ExecutionWriterTarget> writerTarget, Metadata metadata, Session session)\n-    {\n-        if (writerTarget.isPresent() && writerTarget.get() instanceof ExecutionWriterTarget.DeleteHandle) {\n-            DeleteNode delete = findSinglePlanNode(planNode, DeleteNode.class).get();\n-            return createDeleteScanInfo(delete, metadata, session);\n-        }\n-        return Optional.empty();\n-    }\n-\n-    private static Optional<DeleteScanInfo> createDeleteScanInfo(DeleteNode delete, Metadata metadata, Session session)\n-    {\n-        TableScanNode tableScan = getDeleteTableScan(delete);\n-        TupleDomain<ColumnHandle> originalEnforcedConstraint = tableScan.getEnforcedConstraint();\n-        TableLayoutResult layoutResult = metadata.getLayout(\n-                session,\n-                tableScan.getTable(),\n-                new Constraint<>(originalEnforcedConstraint),\n-                Optional.of(ImmutableSet.copyOf(tableScan.getAssignments().values())));\n-\n-        return Optional.of(new DeleteScanInfo(tableScan.getId(), layoutResult.getLayout().getNewTableHandle()));\n-    }\n-\n     private static <T extends PlanNode> Optional<T> findSinglePlanNode(PlanNode planNode, Class<T> clazz)\n     {\n         return PlanNodeSearcher\n@@ -203,30 +153,6 @@ private static <T extends PlanNode> List<T> findPlanNodes(StreamingSubPlan plan,\n                 .collect(toImmutableList());\n     }\n \n-    private static TableScanNode getDeleteTableScan(PlanNode node)\n-    {\n-        if (node instanceof TableScanNode) {\n-            return (TableScanNode) node;\n-        }\n-        if (node instanceof DeleteNode) {\n-            return getDeleteTableScan(((DeleteNode) node).getSource());\n-        }\n-        if (node instanceof FilterNode) {\n-            return getDeleteTableScan(((FilterNode) node).getSource());\n-        }\n-        if (node instanceof ProjectNode) {\n-            return getDeleteTableScan(((ProjectNode) node).getSource());\n-        }\n-        if (node instanceof SemiJoinNode) {\n-            return getDeleteTableScan(((SemiJoinNode) node).getSource());\n-        }\n-        if (node instanceof JoinNode) {\n-            JoinNode joinNode = (JoinNode) node;\n-            return getDeleteTableScan(joinNode.getLeft());\n-        }\n-        throw new IllegalArgumentException(\"Invalid descendant for DeleteNode: \" + node.getClass().getName());\n-    }\n-\n     @JsonProperty\n     public Optional<ExecutionWriterTarget> getWriterTarget()\n     {\n@@ -238,35 +164,4 @@ public Optional<AnalyzeTableHandle> getAnalyzeTableHandle()\n     {\n         return analyzeTableHandle;\n     }\n-\n-    @JsonProperty\n-    public Optional<DeleteScanInfo> getDeleteScanInfo()\n-    {\n-        return deleteScanInfo;\n-    }\n-\n-    public static class DeleteScanInfo\n-    {\n-        private final PlanNodeId id;\n-        private final TableHandle tableHandle;\n-\n-        @JsonCreator\n-        public DeleteScanInfo(@JsonProperty(\"id\") PlanNodeId id, @JsonProperty(\"tableHandle\") TableHandle tableHandle)\n-        {\n-            this.id = id;\n-            this.tableHandle = tableHandle;\n-        }\n-\n-        @JsonProperty\n-        public PlanNodeId getId()\n-        {\n-            return id;\n-        }\n-\n-        @JsonProperty\n-        public TableHandle getTableHandle()\n-        {\n-            return tableHandle;\n-        }\n-    }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\nindex d3acdd0e9bf50..6c46f02d91576 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\n@@ -40,7 +40,6 @@\n import com.facebook.presto.execution.scheduler.ExecutionWriterTarget.RefreshMaterializedViewHandle;\n import com.facebook.presto.execution.scheduler.ExecutionWriterTarget.UpdateHandle;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n-import com.facebook.presto.execution.scheduler.TableWriteInfo.DeleteScanInfo;\n import com.facebook.presto.expressions.DynamicFilters;\n import com.facebook.presto.expressions.DynamicFilters.DynamicFilterExtractResult;\n import com.facebook.presto.expressions.DynamicFilters.DynamicFilterPlaceholder;\n@@ -1453,13 +1452,7 @@ private PhysicalOperation visitScanFilterAndProject(\n             PhysicalOperation source = null;\n             if (sourceNode instanceof TableScanNode && locality.equals(LOCAL)) {\n                 TableScanNode tableScanNode = (TableScanNode) sourceNode;\n-                Optional<DeleteScanInfo> deleteScanInfo = context.getTableWriteInfo().getDeleteScanInfo();\n-                if (deleteScanInfo.isPresent() && deleteScanInfo.get().getId() == tableScanNode.getId()) {\n-                    table = deleteScanInfo.get().getTableHandle();\n-                }\n-                else {\n-                    table = tableScanNode.getTable();\n-                }\n+                table = tableScanNode.getTable();\n \n                 // extract the column handles and channel to type mapping\n                 sourceLayout = new LinkedHashMap<>();\n@@ -1613,14 +1606,7 @@ public PhysicalOperation visitTableScan(TableScanNode node, LocalExecutionPlanCo\n                 columns.add(node.getAssignments().get(variable));\n             }\n \n-            TableHandle tableHandle;\n-            Optional<DeleteScanInfo> deleteScanInfo = context.getTableWriteInfo().getDeleteScanInfo();\n-            if (deleteScanInfo.isPresent() && deleteScanInfo.get().getId() == node.getId()) {\n-                tableHandle = deleteScanInfo.get().getTableHandle();\n-            }\n-            else {\n-                tableHandle = node.getTable();\n-            }\n+            TableHandle tableHandle = node.getTable();\n             OperatorFactory operatorFactory = new TableScanOperatorFactory(context.getNextOperatorId(), node.getId(), pageSourceProvider, tableHandle, columns);\n             return new PhysicalOperation(operatorFactory, makeLayout(node), context, stageExecutionDescriptor.isScanGroupedExecution(node.getId()) ? GROUPED_EXECUTION : UNGROUPED_EXECUTION);\n         }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/SplitSourceFactory.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/SplitSourceFactory.java\nindex c4707b487744e..b51bb17a8b56b 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/SplitSourceFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/SplitSourceFactory.java\n@@ -16,7 +16,6 @@\n import com.facebook.airlift.log.Logger;\n import com.facebook.presto.Session;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n-import com.facebook.presto.execution.scheduler.TableWriteInfo.DeleteScanInfo;\n import com.facebook.presto.spi.TableHandle;\n import com.facebook.presto.spi.WarningCollector;\n import com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy;\n@@ -67,7 +66,6 @@\n \n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n import java.util.function.Supplier;\n \n import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.GROUPED_SCHEDULING;\n@@ -146,14 +144,7 @@ public Map<PlanNodeId, SplitSource> visitExplainAnalyze(ExplainAnalyzeNode node,\n         public Map<PlanNodeId, SplitSource> visitTableScan(TableScanNode node, Context context)\n         {\n             // get dataSource for table\n-            TableHandle table;\n-            Optional<DeleteScanInfo> deleteScanInfo = context.getTableWriteInfo().getDeleteScanInfo();\n-            if (deleteScanInfo.isPresent() && deleteScanInfo.get().getId() == node.getId()) {\n-                table = deleteScanInfo.get().getTableHandle();\n-            }\n-            else {\n-                table = node.getTable();\n-            }\n+            TableHandle table = node.getTable();\n             Supplier<SplitSource> splitSourceSupplier = () -> splitSourceProvider.getSplits(\n                     session,\n                     table,\n\ndiff --git a/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.cpp b/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.cpp\nindex cf0adb97c15c1..2fec6870ec913 100644\n--- a/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.cpp\n+++ b/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.cpp\n@@ -2298,130 +2298,6 @@ void from_json(const json& j, SessionRepresentation& p) {\n       \"sessionFunctions\");\n }\n } // namespace facebook::presto::protocol\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-namespace facebook::presto::protocol {\n-void to_json(json& j, const std::shared_ptr<ConnectorTableLayoutHandle>& p) {\n-  if (p == nullptr) {\n-    return;\n-  }\n-  String type = p->_type;\n-  getConnectorProtocol(type).to_json(j, p);\n-}\n-\n-void from_json(const json& j, std::shared_ptr<ConnectorTableLayoutHandle>& p) {\n-  String type;\n-  try {\n-    type = p->getSubclassKey(j);\n-  } catch (json::parse_error& e) {\n-    throw ParseError(\n-        std::string(e.what()) +\n-        \" ConnectorTableLayoutHandle  ConnectorTableLayoutHandle\");\n-  }\n-  getConnectorProtocol(type).from_json(j, p);\n-}\n-} // namespace facebook::presto::protocol\n-namespace facebook::presto::protocol {\n-\n-void to_json(json& j, const TableHandle& p) {\n-  j = json::object();\n-  to_json_key(\n-      j,\n-      \"connectorId\",\n-      p.connectorId,\n-      \"TableHandle\",\n-      \"ConnectorId\",\n-      \"connectorId\");\n-  to_json_key(\n-      j,\n-      \"connectorHandle\",\n-      p.connectorHandle,\n-      \"TableHandle\",\n-      \"ConnectorTableHandle\",\n-      \"connectorHandle\");\n-  to_json_key(\n-      j,\n-      \"transaction\",\n-      p.transaction,\n-      \"TableHandle\",\n-      \"ConnectorTransactionHandle\",\n-      \"transaction\");\n-  to_json_key(\n-      j,\n-      \"connectorTableLayout\",\n-      p.connectorTableLayout,\n-      \"TableHandle\",\n-      \"ConnectorTableLayoutHandle\",\n-      \"connectorTableLayout\");\n-}\n-\n-void from_json(const json& j, TableHandle& p) {\n-  from_json_key(\n-      j,\n-      \"connectorId\",\n-      p.connectorId,\n-      \"TableHandle\",\n-      \"ConnectorId\",\n-      \"connectorId\");\n-  from_json_key(\n-      j,\n-      \"connectorHandle\",\n-      p.connectorHandle,\n-      \"TableHandle\",\n-      \"ConnectorTableHandle\",\n-      \"connectorHandle\");\n-  from_json_key(\n-      j,\n-      \"transaction\",\n-      p.transaction,\n-      \"TableHandle\",\n-      \"ConnectorTransactionHandle\",\n-      \"transaction\");\n-  from_json_key(\n-      j,\n-      \"connectorTableLayout\",\n-      p.connectorTableLayout,\n-      \"TableHandle\",\n-      \"ConnectorTableLayoutHandle\",\n-      \"connectorTableLayout\");\n-}\n-} // namespace facebook::presto::protocol\n-namespace facebook::presto::protocol {\n-\n-void to_json(json& j, const DeleteScanInfo& p) {\n-  j = json::object();\n-  to_json_key(j, \"id\", p.id, \"DeleteScanInfo\", \"PlanNodeId\", \"id\");\n-  to_json_key(\n-      j,\n-      \"tableHandle\",\n-      p.tableHandle,\n-      \"DeleteScanInfo\",\n-      \"TableHandle\",\n-      \"tableHandle\");\n-}\n-\n-void from_json(const json& j, DeleteScanInfo& p) {\n-  from_json_key(j, \"id\", p.id, \"DeleteScanInfo\", \"PlanNodeId\", \"id\");\n-  from_json_key(\n-      j,\n-      \"tableHandle\",\n-      p.tableHandle,\n-      \"DeleteScanInfo\",\n-      \"TableHandle\",\n-      \"tableHandle\");\n-}\n-} // namespace facebook::presto::protocol\n namespace facebook::presto::protocol {\n void to_json(json& j, const std::shared_ptr<ExecutionWriterTarget>& p) {\n   if (p == nullptr) {\n@@ -2505,13 +2381,6 @@ void to_json(json& j, const TableWriteInfo& p) {\n       \"TableWriteInfo\",\n       \"AnalyzeTableHandle\",\n       \"analyzeTableHandle\");\n-  to_json_key(\n-      j,\n-      \"deleteScanInfo\",\n-      p.deleteScanInfo,\n-      \"TableWriteInfo\",\n-      \"DeleteScanInfo\",\n-      \"deleteScanInfo\");\n }\n \n void from_json(const json& j, TableWriteInfo& p) {\n@@ -2529,13 +2398,6 @@ void from_json(const json& j, TableWriteInfo& p) {\n       \"TableWriteInfo\",\n       \"AnalyzeTableHandle\",\n       \"analyzeTableHandle\");\n-  from_json_key(\n-      j,\n-      \"deleteScanInfo\",\n-      p.deleteScanInfo,\n-      \"TableWriteInfo\",\n-      \"DeleteScanInfo\",\n-      \"deleteScanInfo\");\n }\n } // namespace facebook::presto::protocol\n /*\n@@ -6001,6 +5863,105 @@ void from_json(const json& j, std::shared_ptr<ColumnHandle>& p) {\n   getConnectorProtocol(type).from_json(j, p);\n }\n } // namespace facebook::presto::protocol\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+namespace facebook::presto::protocol {\n+void to_json(json& j, const std::shared_ptr<ConnectorTableLayoutHandle>& p) {\n+  if (p == nullptr) {\n+    return;\n+  }\n+  String type = p->_type;\n+  getConnectorProtocol(type).to_json(j, p);\n+}\n+\n+void from_json(const json& j, std::shared_ptr<ConnectorTableLayoutHandle>& p) {\n+  String type;\n+  try {\n+    type = p->getSubclassKey(j);\n+  } catch (json::parse_error& e) {\n+    throw ParseError(\n+        std::string(e.what()) +\n+        \" ConnectorTableLayoutHandle  ConnectorTableLayoutHandle\");\n+  }\n+  getConnectorProtocol(type).from_json(j, p);\n+}\n+} // namespace facebook::presto::protocol\n+namespace facebook::presto::protocol {\n+\n+void to_json(json& j, const TableHandle& p) {\n+  j = json::object();\n+  to_json_key(\n+      j,\n+      \"connectorId\",\n+      p.connectorId,\n+      \"TableHandle\",\n+      \"ConnectorId\",\n+      \"connectorId\");\n+  to_json_key(\n+      j,\n+      \"connectorHandle\",\n+      p.connectorHandle,\n+      \"TableHandle\",\n+      \"ConnectorTableHandle\",\n+      \"connectorHandle\");\n+  to_json_key(\n+      j,\n+      \"transaction\",\n+      p.transaction,\n+      \"TableHandle\",\n+      \"ConnectorTransactionHandle\",\n+      \"transaction\");\n+  to_json_key(\n+      j,\n+      \"connectorTableLayout\",\n+      p.connectorTableLayout,\n+      \"TableHandle\",\n+      \"ConnectorTableLayoutHandle\",\n+      \"connectorTableLayout\");\n+}\n+\n+void from_json(const json& j, TableHandle& p) {\n+  from_json_key(\n+      j,\n+      \"connectorId\",\n+      p.connectorId,\n+      \"TableHandle\",\n+      \"ConnectorId\",\n+      \"connectorId\");\n+  from_json_key(\n+      j,\n+      \"connectorHandle\",\n+      p.connectorHandle,\n+      \"TableHandle\",\n+      \"ConnectorTableHandle\",\n+      \"connectorHandle\");\n+  from_json_key(\n+      j,\n+      \"transaction\",\n+      p.transaction,\n+      \"TableHandle\",\n+      \"ConnectorTransactionHandle\",\n+      \"transaction\");\n+  from_json_key(\n+      j,\n+      \"connectorTableLayout\",\n+      p.connectorTableLayout,\n+      \"TableHandle\",\n+      \"ConnectorTableLayoutHandle\",\n+      \"connectorTableLayout\");\n+}\n+} // namespace facebook::presto::protocol\n namespace facebook::presto::protocol {\n IndexSourceNode::IndexSourceNode() noexcept {\n   _type = \".IndexSourceNode\";\n\ndiff --git a/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.h b/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.h\nindex a219909495d40..a5bd2d50a874c 100644\n--- a/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.h\n+++ b/presto-native-execution/presto_cpp/presto_protocol/core/presto_protocol_core.h\n@@ -293,11 +293,6 @@ void to_json(json& j, const std::shared_ptr<ConnectorTransactionHandle>& p);\n void from_json(const json& j, std::shared_ptr<ConnectorTransactionHandle>& p);\n } // namespace facebook::presto::protocol\n namespace facebook::presto::protocol {\n-struct ConnectorTableLayoutHandle : public JsonEncodedSubclass {};\n-void to_json(json& j, const std::shared_ptr<ConnectorTableLayoutHandle>& p);\n-void from_json(const json& j, std::shared_ptr<ConnectorTableLayoutHandle>& p);\n-} // namespace facebook::presto::protocol\n-namespace facebook::presto::protocol {\n struct ExecutionWriterTarget : public JsonEncodedSubclass {};\n void to_json(json& j, const std::shared_ptr<ExecutionWriterTarget>& p);\n void from_json(const json& j, std::shared_ptr<ExecutionWriterTarget>& p);\n@@ -347,6 +342,11 @@ void to_json(json& j, const std::shared_ptr<ColumnHandle>& p);\n void from_json(const json& j, std::shared_ptr<ColumnHandle>& p);\n } // namespace facebook::presto::protocol\n namespace facebook::presto::protocol {\n+struct ConnectorTableLayoutHandle : public JsonEncodedSubclass {};\n+void to_json(json& j, const std::shared_ptr<ConnectorTableLayoutHandle>& p);\n+void from_json(const json& j, std::shared_ptr<ConnectorTableLayoutHandle>& p);\n+} // namespace facebook::presto::protocol\n+namespace facebook::presto::protocol {\n struct ConnectorInsertTableHandle : public JsonEncodedSubclass {};\n void to_json(json& j, const std::shared_ptr<ConnectorInsertTableHandle>& p);\n void from_json(const json& j, std::shared_ptr<ConnectorInsertTableHandle>& p);\n@@ -769,28 +769,9 @@ void to_json(json& j, const SessionRepresentation& p);\n void from_json(const json& j, SessionRepresentation& p);\n } // namespace facebook::presto::protocol\n namespace facebook::presto::protocol {\n-struct TableHandle {\n-  ConnectorId connectorId = {};\n-  std::shared_ptr<ConnectorTableHandle> connectorHandle = {};\n-  std::shared_ptr<ConnectorTransactionHandle> transaction = {};\n-  std::shared_ptr<ConnectorTableLayoutHandle> connectorTableLayout = {};\n-};\n-void to_json(json& j, const TableHandle& p);\n-void from_json(const json& j, TableHandle& p);\n-} // namespace facebook::presto::protocol\n-namespace facebook::presto::protocol {\n-struct DeleteScanInfo {\n-  PlanNodeId id = {};\n-  TableHandle tableHandle = {};\n-};\n-void to_json(json& j, const DeleteScanInfo& p);\n-void from_json(const json& j, DeleteScanInfo& p);\n-} // namespace facebook::presto::protocol\n-namespace facebook::presto::protocol {\n struct TableWriteInfo {\n   std::shared_ptr<ExecutionWriterTarget> writerTarget = {};\n   std::shared_ptr<AnalyzeTableHandle> analyzeTableHandle = {};\n-  std::shared_ptr<DeleteScanInfo> deleteScanInfo = {};\n };\n void to_json(json& j, const TableWriteInfo& p);\n void from_json(const json& j, TableWriteInfo& p);\n@@ -1519,6 +1500,16 @@ struct IndexJoinNode : public PlanNode {\n void to_json(json& j, const IndexJoinNode& p);\n void from_json(const json& j, IndexJoinNode& p);\n } // namespace facebook::presto::protocol\n+namespace facebook::presto::protocol {\n+struct TableHandle {\n+  ConnectorId connectorId = {};\n+  std::shared_ptr<ConnectorTableHandle> connectorHandle = {};\n+  std::shared_ptr<ConnectorTransactionHandle> transaction = {};\n+  std::shared_ptr<ConnectorTableLayoutHandle> connectorTableLayout = {};\n+};\n+void to_json(json& j, const TableHandle& p);\n+void from_json(const json& j, TableHandle& p);\n+} // namespace facebook::presto::protocol\n /*\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n",
    "test_patch": "diff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java b/presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java\nindex 78fb461381d43..cc810353617d3 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/MockRemoteTaskFactory.java\n@@ -146,7 +146,7 @@ public MockRemoteTask createTableScanTask(TaskId taskId, InternalNode newNode, L\n                 createInitialEmptyOutputBuffers(BROADCAST),\n                 nodeStatsTracker,\n                 true,\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()),\n+                new TableWriteInfo(Optional.empty(), Optional.empty()),\n                 SchedulerStatsTracker.NOOP);\n     }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java b/presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\nindex 92d32cd9cecf8..e6282d6a05a25 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\n@@ -196,7 +196,7 @@ public static LocalExecutionPlanner createTestingPlanner()\n \n     public static TaskInfo updateTask(SqlTask sqlTask, List<TaskSource> taskSources, OutputBuffers outputBuffers)\n     {\n-        return sqlTask.updateTask(TEST_SESSION, Optional.of(PLAN_FRAGMENT), taskSources, outputBuffers, Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+        return sqlTask.updateTask(TEST_SESSION, Optional.of(PLAN_FRAGMENT), taskSources, outputBuffers, Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n     }\n \n     public static SplitMonitor createTestSplitMonitor()\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java\nindex 00c8d0afd4d7c..bce1acccd12f3 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlStageExecution.java\n@@ -109,7 +109,7 @@ private void testFinalStageInfoInternal()\n                 executor,\n                 new NoOpFailureDetector(),\n                 new SplitSchedulerStats(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()));\n+                new TableWriteInfo(Optional.empty(), Optional.empty()));\n         stage.setOutputBuffers(createInitialEmptyOutputBuffers(ARBITRARY));\n \n         // add listener that fetches stage info when the final status is available\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java\nindex 2a7d731e59df4..311c7081a76ad 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskManager.java\n@@ -349,7 +349,7 @@ private TaskInfo createTask(SqlTaskManager sqlTaskManager, TaskId taskId, Immuta\n                 Optional.of(PLAN_FRAGMENT),\n                 ImmutableList.of(new TaskSource(TABLE_SCAN_NODE_ID, splits, true)),\n                 outputBuffers,\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n     }\n \n     private TaskInfo createTask(SqlTaskManager sqlTaskManager, TaskId taskId, OutputBuffers outputBuffers)\n@@ -370,7 +370,7 @@ private TaskInfo createTask(SqlTaskManager sqlTaskManager, TaskId taskId, Output\n                 Optional.of(PLAN_FRAGMENT),\n                 ImmutableList.of(),\n                 outputBuffers,\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n     }\n \n     public static class MockExchangeClientSupplier\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java b/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java\nindex 34a707792fccd..8f50e98cd8d02 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestAdaptivePhasedExecutionPolicy.java\n@@ -143,7 +143,7 @@ private StageExecutionAndScheduler getStageExecutionAndScheduler(int stage, Plan\n                 newDirectExecutorService(),\n                 new NoOpFailureDetector(),\n                 new SplitSchedulerStats(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()));\n+                new TableWriteInfo(Optional.empty(), Optional.empty()));\n         StageLinkage stageLinkage = new StageLinkage(fragmentId, (id, tasks, noMoreExchangeLocations) -> {}, ImmutableSet.of());\n         StageScheduler stageScheduler = new FixedCountScheduler(stageExecution, ImmutableList.of());\n         StageExecutionAndScheduler scheduler = new StageExecutionAndScheduler(stageExecution, stageLinkage, stageScheduler);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java b/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java\nindex 00c4483a755c5..5ea371568b9c3 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/scheduler/TestSourcePartitionedScheduler.java\n@@ -546,7 +546,7 @@ private SqlStageExecution createSqlStageExecution(SubPlan tableScanPlan, NodeTas\n                 queryExecutor,\n                 new NoOpFailureDetector(),\n                 new SplitSchedulerStats(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()));\n+                new TableWriteInfo(Optional.empty(), Optional.empty()));\n \n         stage.setOutputBuffers(createInitialEmptyOutputBuffers(PARTITIONED)\n                 .withBuffer(OUT, 0)\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java\nindex 1047d45985fe2..5422304b52935 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestLocalExecutionPlanner.java\n@@ -199,7 +199,7 @@ private LocalExecutionPlan getLocalExecutionPlan(Session session, PlanNode plan,\n                 testFragment,\n                 new TestingOutputBuffer(),\n                 new TestingRemoteSourceFactory(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()),\n+                new TableWriteInfo(Optional.empty(), Optional.empty()),\n                 customPlanTranslators);\n     }\n \n@@ -218,7 +218,7 @@ private LocalExecutionPlan getLocalExecutionPlan(Session session)\n                 leafFragment,\n                 new TestingOutputBuffer(),\n                 new TestingRemoteSourceFactory(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()));\n+                new TableWriteInfo(Optional.empty(), Optional.empty()));\n     }\n \n     private static class CustomNodeA\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java b/presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java\nindex ff737c3567901..08c5992dd1b7a 100644\n--- a/presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java\n+++ b/presto-main/src/test/java/com/facebook/presto/execution/TestSqlTask.java\n@@ -123,7 +123,7 @@ public void testEmptyQuery()\n                 ImmutableList.of(),\n                 createInitialEmptyOutputBuffers(PARTITIONED)\n                         .withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.RUNNING);\n \n         taskInfo = sqlTask.getTaskInfo();\n@@ -134,7 +134,7 @@ public void testEmptyQuery()\n                 ImmutableList.of(new TaskSource(TABLE_SCAN_NODE_ID, ImmutableSet.of(), true)),\n                 createInitialEmptyOutputBuffers(PARTITIONED)\n                         .withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.FINISHED);\n \n         taskInfo = sqlTask.getTaskInfo();\n@@ -151,7 +151,7 @@ public void testSimpleQuery()\n                 Optional.of(PLAN_FRAGMENT),\n                 ImmutableList.of(new TaskSource(TABLE_SCAN_NODE_ID, ImmutableSet.of(SPLIT), true)),\n                 createInitialEmptyOutputBuffers(PARTITIONED).withBuffer(OUT, 0).withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.RUNNING);\n \n         taskInfo = sqlTask.getTaskInfo();\n@@ -201,7 +201,7 @@ public void testCancel()\n                 createInitialEmptyOutputBuffers(PARTITIONED)\n                         .withBuffer(OUT, 0)\n                         .withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.RUNNING);\n         assertEquals(taskInfo.getStats().getEndTimeInMillis(), 0);\n \n@@ -228,7 +228,7 @@ public void testAbort()\n                 Optional.of(PLAN_FRAGMENT),\n                 ImmutableList.of(new TaskSource(TABLE_SCAN_NODE_ID, ImmutableSet.of(SPLIT), true)),\n                 createInitialEmptyOutputBuffers(PARTITIONED).withBuffer(OUT, 0).withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.RUNNING);\n \n         taskInfo = sqlTask.getTaskInfo();\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java b/presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java\nindex 30016db2fd94f..c809c5de26445 100644\n--- a/presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java\n+++ b/presto-main/src/test/java/com/facebook/presto/memory/TestHighMemoryTaskKiller.java\n@@ -128,7 +128,7 @@ public void updateTaskMemory(SqlTask sqlTask, long systemMemory)\n                 ImmutableList.of(),\n                 createInitialEmptyOutputBuffers(PARTITIONED)\n                         .withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         assertEquals(taskInfo.getTaskStatus().getState(), TaskState.RUNNING);\n \n         TaskContext taskContext = sqlTask.getTaskContext().get();\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\nindex 8d741787312e5..6c93591583411 100644\n--- a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\n+++ b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\n@@ -330,7 +330,7 @@ private RemoteTask createRemoteTask(HttpRemoteTaskFactory httpRemoteTaskFactory)\n                 createInitialEmptyOutputBuffers(OutputBuffers.BufferType.BROADCAST),\n                 new NodeTaskMap.NodeStatsTracker(i -> {}, i -> {}, (age, i) -> {}),\n                 true,\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()),\n+                new TableWriteInfo(Optional.empty(), Optional.empty()),\n                 SchedulerStatsTracker.NOOP);\n     }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java\nindex 517da0b593fb4..d301958eec80c 100644\n--- a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java\n+++ b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTaskWithEventLoop.java\n@@ -338,7 +338,7 @@ private RemoteTask createRemoteTask(HttpRemoteTaskFactory httpRemoteTaskFactory)\n                 createInitialEmptyOutputBuffers(OutputBuffers.BufferType.BROADCAST),\n                 new NodeTaskMap.NodeStatsTracker(i -> {}, i -> {}, (age, i) -> {}),\n                 true,\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()),\n+                new TableWriteInfo(Optional.empty(), Optional.empty()),\n                 SchedulerStatsTracker.NOOP);\n     }\n \n\ndiff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java\nindex 2cedbc7eefe76..fda1cee85153c 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/execution/TestBatchTaskUpdateRequest.java\n@@ -99,7 +99,7 @@ public void testJsonConversion()\n                 Optional.of(createPlanFragment().bytesForTaskSerialization(PLAN_FRAGMENT_JSON_CODEC)),\n                 sources,\n                 createInitialEmptyOutputBuffers(PARTITIONED),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n         String shuffleWriteInfo = \"dummy-shuffle-write-info\";\n         BatchTaskUpdateRequest batchUpdateRequest = new BatchTaskUpdateRequest(updateRequest, Optional.of(shuffleWriteInfo), Optional.empty());\n         JsonCodec<BatchTaskUpdateRequest> batchTaskUpdateRequestJsonCodec = getJsonCodec();\n\ndiff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java\nindex e085715d7c097..485e08a7ea9d1 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/task/TestPrestoSparkTaskExecution.java\n@@ -98,7 +98,7 @@ public void setUp()\n                 planFragment,\n                 new TestingOutputBuffer(),\n                 new TestingRemoteSourceFactory(),\n-                new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty()),\n+                new TableWriteInfo(Optional.empty(), Optional.empty()),\n                 new ArrayList<>());\n     }\n \n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java b/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\nindex 110bc5e520596..84a699714938f 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\n@@ -712,7 +712,7 @@ private TestOperatorContext createTestingOperatorContexts(SqlTask sqlTask, Strin\n                 Optional.of(PLAN_FRAGMENT),\n                 ImmutableList.of(new TaskSource(TABLE_SCAN_NODE_ID, ImmutableSet.of(SPLIT), false)),\n                 createInitialEmptyOutputBuffers(PARTITIONED).withBuffer(OUT, 0).withNoMoreBufferIds(),\n-                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty(), Optional.empty())));\n+                Optional.of(new TableWriteInfo(Optional.empty(), Optional.empty())));\n \n         // use implicitly created task context from updateTask. It should be the only task in this QueryContext's tasks\n         TaskContext taskContext = sqlTask.getQueryContext().getTaskContextByTaskId(sqlTask.getTaskId());\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24817",
    "pr_id": 24817,
    "issue_id": 24775,
    "repo": "prestodb/presto",
    "problem_statement": "CI test (17.0.13, :presto-hive) frequently fails due to java.lang.OutOfMemoryError: Java heap space\nRecently, I have frequently encountered CI test (17.0.13, :presto-hive) failures due to `java.lang.OutOfMemoryError: Java heap space`. The latest one can be found here: https://github.com/prestodb/presto/actions/runs/14004593199/job/39216718757?pr=24524.\n\nHowever, CI test (8, :presto-hive) seems not encountered this problem.",
    "issue_word_count": 68,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-hive/pom.xml",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java"
    ],
    "base_commit": "718cd5d704d428aa5cbaa133417e0e42a9611cd6",
    "head_commit": "d7d0ab82a4116e4ce858e2ebb4374faae899caa2",
    "repo_url": "https://github.com/prestodb/presto/pull/24817",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24817",
    "dockerfile": "",
    "pr_merged_at": "2025-03-27T17:52:45.000Z",
    "patch": "diff --git a/presto-hive/pom.xml b/presto-hive/pom.xml\nindex 00629aaf8643b..e1d3f66ec06cb 100644\n--- a/presto-hive/pom.xml\n+++ b/presto-hive/pom.xml\n@@ -14,6 +14,7 @@\n \n     <properties>\n         <air.main.basedir>${project.parent.basedir}</air.main.basedir>\n+        <air.test.jvmsize>5g</air.test.jvmsize>\n     </properties>\n \n     <dependencies>\n@@ -630,9 +631,6 @@\n         </profile>\n         <profile>\n             <id>test-hive-pushdown-filter-queries-basic</id>\n-            <properties>\n-                <air.test.jvmsize>5g</air.test.jvmsize>\n-            </properties>\n             <build>\n                 <plugins>\n                     <plugin>\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java\nindex 66d21ef864300..55379ae2492d2 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveIntegrationSmokeTest.java\n@@ -2382,23 +2382,23 @@ private void testWriteSortedTable(Session session)\n                     session,\n                     \"CREATE TABLE create_partitioned_sorted_table (orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus)\\n\" +\n                             \"WITH (partitioned_by = ARRAY['orderstatus'], bucketed_by = ARRAY['custkey'], bucket_count = 11, sorted_by = ARRAY['orderkey']) AS\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM create_partitioned_sorted_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n \n             assertUpdate(\n                     session,\n                     \"CREATE TABLE create_unpartitioned_sorted_table (orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus)\\n\" +\n                             \"WITH (bucketed_by = ARRAY['custkey'], bucket_count = 11, sorted_by = ARRAY['orderkey']) AS\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM create_unpartitioned_sorted_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n \n             // insert\n             assertUpdate(\n@@ -2409,12 +2409,12 @@ private void testWriteSortedTable(Session session)\n             assertUpdate(\n                     session,\n                     \"INSERT INTO insert_partitioned_sorted_table\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM insert_partitioned_sorted_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n         }\n         finally {\n             assertUpdate(session, \"DROP TABLE IF EXISTS create_partitioned_sorted_table\");\n@@ -2441,23 +2441,23 @@ public void testWritePreferredOrderingTable(Session session)\n                     session,\n                     \"CREATE TABLE create_partitioned_ordering_table (orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus)\\n\" +\n                             \"WITH (partitioned_by = ARRAY['orderstatus'], preferred_ordering_columns = ARRAY['orderkey']) AS\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM create_partitioned_ordering_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n \n             assertUpdate(\n                     session,\n                     \"CREATE TABLE create_unpartitioned_ordering_table (orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus)\\n\" +\n                             \"WITH (preferred_ordering_columns = ARRAY['orderkey']) AS\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM create_unpartitioned_ordering_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n \n             // insert\n             assertUpdate(\n@@ -2468,12 +2468,12 @@ public void testWritePreferredOrderingTable(Session session)\n             assertUpdate(\n                     session,\n                     \"INSERT INTO insert_partitioned_ordering_table\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n             assertQuery(\n                     session,\n                     \"SELECT count(*) FROM insert_partitioned_ordering_table\",\n-                    \"SELECT count(*) * 100 FROM orders\");\n+                    \"SELECT count(*) FROM orders\");\n \n             // invalid\n             assertQueryFails(\n@@ -2644,8 +2644,8 @@ public void testFileRenamingForPartitionedTable()\n                             .build(),\n                     \"CREATE TABLE partitioned_ordering_table (orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus)\\n\" +\n                             \"WITH (partitioned_by = ARRAY['orderstatus'], preferred_ordering_columns = ARRAY['orderkey']) AS\\n\" +\n-                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                            \"SELECT orderkey, custkey, totalprice, orderdate, orderpriority, clerk, shippriority, comment, orderstatus FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n \n             // Collect all file names\n             Map<String, List<Integer>> partitionFileNamesMap = new HashMap<>();\n@@ -2683,8 +2683,8 @@ public void testFileRenamingForUnpartitionedTable()\n                             .setSystemProperty(\"writer_min_size\", \"1MB\")\n                             .setSystemProperty(\"task_writer_count\", \"1\")\n                             .build(),\n-                    \"CREATE TABLE unpartitioned_ordering_table AS SELECT * FROM tpch.sf1.orders\",\n-                    (long) computeActual(\"SELECT count(*) FROM tpch.sf1.orders\").getOnlyValue());\n+                    \"CREATE TABLE unpartitioned_ordering_table AS SELECT * FROM tpch.tiny.orders\",\n+                    (long) computeActual(\"SELECT count(*) FROM tpch.tiny.orders\").getOnlyValue());\n \n             // Collect file names of the table\n             List<Integer> fileNames = new ArrayList<>();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24808",
    "pr_id": 24808,
    "issue_id": 22125,
    "repo": "prestodb/presto",
    "problem_statement": "Flaky TestBrutalShutdown.testQueryRetryOnShutdown\nAnother test that failed on a completely unrelated change (a config change to a different CI):\r\n\r\nhttps://github.com/prestodb/presto/actions/runs/8203469130/job/22436277814?pr=22124\r\n\r\nLife cycle stopping...\r\n2024-03-08T13:29:55.2222787Z [INFO] \r\n2024-03-08T13:29:55.2223309Z [INFO] Results:\r\n2024-03-08T13:29:55.2223782Z [INFO] \r\n2024-03-08T13:29:55.2224208Z [ERROR] Failures: \r\n2024-03-08T13:29:55.2228422Z [ERROR]   TestBrutalShutdown.testQueryRetryOnShutdown:83 expected [5] but found [2]\r\n2024-03-08T13:29:55.2229497Z [INFO] \r\n2024-03-08T13:29:55.2231113Z [ERROR] Tests run: 3107, Failures: 1, Errors: 0, Skipped: 0\r\n2024-03-08T13:29:55.2231792Z [INFO] \r\n2024-03-08T13:29:55.2268645Z [INFO] ----------------\r\n\r\nThree things that might help here:\r\n\r\n1. Track and report the state of unsuccessful queries when this method fails\r\n2. Make the test single threaded\r\n3. Change the setUp and shutdown methods to BeforeMethod and AfterMethod instead of BeforeClass and AfterClass \r\n",
    "issue_word_count": 157,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java"
    ],
    "pr_changed_test_files": [
      "presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java"
    ],
    "base_commit": "718cd5d704d428aa5cbaa133417e0e42a9611cd6",
    "head_commit": "29362f542ea06746ba6a68bdf25f5a384fd6e812",
    "repo_url": "https://github.com/prestodb/presto/pull/24808",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24808",
    "dockerfile": "",
    "pr_merged_at": "2025-03-27T22:48:34.000Z",
    "patch": "diff --git a/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java b/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\nindex 49cf6e0989fe7..4de55e4aa38d6 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\n@@ -87,7 +87,7 @@ public enum StandardErrorCode\n     REMOTE_TASK_MISMATCH(0x0001_0008, INTERNAL_ERROR),\n     SERVER_SHUTTING_DOWN(0x0001_0009, INTERNAL_ERROR),\n     FUNCTION_IMPLEMENTATION_MISSING(0x0001_000A, INTERNAL_ERROR),\n-    REMOTE_BUFFER_CLOSE_FAILED(0x0001_000B, INTERNAL_ERROR),\n+    REMOTE_BUFFER_CLOSE_FAILED(0x0001_000B, INTERNAL_ERROR, true),\n     SERVER_STARTING_UP(0x0001_000C, INTERNAL_ERROR),\n     FUNCTION_IMPLEMENTATION_ERROR(0x0001_000D, INTERNAL_ERROR),\n     INVALID_PROCEDURE_DEFINITION(0x0001_000E, INTERNAL_ERROR),\n",
    "test_patch": "diff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java\nindex 5f94d5a88e52e..992c2cd7a2217 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestBrutalShutdown.java\n@@ -13,12 +13,15 @@\n  */\n package com.facebook.presto.tests;\n \n+import com.facebook.airlift.log.Logger;\n import com.facebook.presto.Session;\n+import com.facebook.presto.common.ErrorCode;\n import com.facebook.presto.execution.TaskManager;\n import com.facebook.presto.server.BasicQueryInfo;\n import com.facebook.presto.server.testing.TestingPrestoServer;\n import com.facebook.presto.tpch.TpchPlugin;\n import com.facebook.presto.transaction.TransactionInfo;\n+import com.google.common.base.Joiner;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.ListeningExecutorService;\n@@ -30,10 +33,12 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n \n import static com.facebook.airlift.testing.Assertions.assertLessThanOrEqual;\n import static com.facebook.presto.execution.QueryState.FINISHED;\n import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static java.lang.String.format;\n import static java.util.concurrent.Executors.newCachedThreadPool;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n import static org.testng.Assert.assertEquals;\n@@ -42,6 +47,7 @@\n @Test(singleThreaded = true)\n public class TestBrutalShutdown\n {\n+    private static final Logger LOG = Logger.get(TestBrutalShutdown.class);\n     private static final long SHUTDOWN_TIMEOUT_MILLIS = 600_000;\n     private static final Session TINY_SESSION = testSessionBuilder()\n             .setCatalog(\"tpch\")\n@@ -80,6 +86,13 @@ public void testQueryRetryOnShutdown()\n                     totalSuccessfulQueries++;\n                 }\n             }\n+            if (totalQueries != totalSuccessfulQueries) {\n+                LOG.error(Joiner.on(\"\\n\").join(queryInfos.stream()\n+                        .filter(queryInfo -> queryInfo.getState() != FINISHED)\n+                        .map(queryInfo -> format(\"query %s should have been successful but is in state: %s. Error: %s, retriable: %s\",\n+                                queryInfo.getQueryId(), queryInfo.getState(), queryInfo.getErrorCode(), Optional.ofNullable(queryInfo.getErrorCode()).map(ErrorCode::isRetriable).orElse(null)))\n+                        .iterator()));\n+            }\n             assertEquals(totalSuccessfulQueries, totalQueries);\n         }\n     }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24802",
    "pr_id": 24802,
    "issue_id": 24801,
    "repo": "prestodb/presto",
    "problem_statement": "Update the version of MySql used in product test in order to support arm64 architecture\nThe current MySQL version used in the product tests, \"mysql:5.7\", does not support ARM64 architecture. To enable support, we need to upgrade the version to \"mysql:8.0\".\n\n\n",
    "issue_word_count": 45,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml"
    ],
    "pr_changed_test_files": [
      "presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml"
    ],
    "base_commit": "266069c7792e6d2c50f3c3d54cda345d9b6e58c2",
    "head_commit": "dff479aaca3ca5e4c2ab5bc5e672f867dfecbd8f",
    "repo_url": "https://github.com/prestodb/presto/pull/24802",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24802",
    "dockerfile": "",
    "pr_merged_at": "2025-03-27T06:41:22.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml b/presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml\nindex f16e312dac8e1..33f84e6082dc5 100644\n--- a/presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml\n+++ b/presto-product-tests/conf/docker/singlenode-mysql/docker-compose.yml\n@@ -2,7 +2,7 @@ services:\n \n   mysql:\n     hostname: mysql\n-    image: 'mysql:5.7'\n+    image: 'mysql:8.0'\n     ports:\n       - '13306:13306'\n     command:\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24766",
    "pr_id": 24766,
    "issue_id": 24691,
    "repo": "prestodb/presto",
    "problem_statement": "TestMemoryRevokingScheduler.testTaskRevokingOrderForRevocableBytes flakes\nSee: https://github.com/prestodb/presto/actions/runs/13736459275/job/38420745838?pr=24524\n\nError stack:\n```\nError:  Tests run: 8601, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 1,077.076 s <<< FAILURE! - in TestSuite\nError:  com.facebook.presto.execution.TestMemoryRevokingScheduler.testTaskRevokingOrderForRevocableBytes  Time elapsed: 0.01 s  <<< FAILURE!\njava.lang.AssertionError: expected [operator2] but found [operator1]\n\tat org.testng.Assert.fail(Assert.java:110)\n\tat org.testng.Assert.failNotEquals(Assert.java:1413)\n\tat org.testng.Assert.assertEqualsImpl(Assert.java:149)\n\tat org.testng.Assert.assertEquals(Assert.java:131)\n\tat org.testng.Assert.assertEquals(Assert.java:655)\n\tat org.testng.Assert.assertEquals(Assert.java:665)\n\tat com.facebook.presto.execution.TestMemoryRevokingScheduler.testTaskRevokingOrderForRevocableBytes(TestMemoryRevokingScheduler.java:355)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n```",
    "issue_word_count": 273,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/execution/MemoryRevokingScheduler.java",
      "presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java"
    ],
    "pr_changed_test_files": [
      "presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java"
    ],
    "base_commit": "94d6defaa88d0f447290b49edebb6b5f0636486b",
    "head_commit": "a535020177e01ea777bb46611e468037e660662c",
    "repo_url": "https://github.com/prestodb/presto/pull/24766",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24766",
    "dockerfile": "",
    "pr_merged_at": "2025-03-28T01:21:28.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/execution/MemoryRevokingScheduler.java b/presto-main/src/main/java/com/facebook/presto/execution/MemoryRevokingScheduler.java\nindex 03e92faf17878..fc28963d525ec 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/MemoryRevokingScheduler.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/MemoryRevokingScheduler.java\n@@ -151,6 +151,12 @@ void awaitAsynchronousCallbacksRun()\n         memoryRevocationExecutor.invokeAll(singletonList((Callable<?>) () -> null));\n     }\n \n+    @VisibleForTesting\n+    void submitAsynchronousCallable(Callable<?> callable)\n+    {\n+        memoryRevocationExecutor.submit(callable);\n+    }\n+\n     private void onMemoryReserved(MemoryPool memoryPool, QueryId queryId, long queryMemoryReservation)\n     {\n         try {\n",
    "test_patch": "diff --git a/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java b/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\nindex c1ded6db0a813..110bc5e520596 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/execution/TestMemoryRevokingScheduler.java\n@@ -56,6 +56,7 @@\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.ScheduledExecutorService;\n@@ -344,12 +345,20 @@ public void testTaskRevokingOrderForRevocableBytes()\n         try {\n             scheduler.start();\n \n+            // Waiting for all existing tasks in scheduler's memoryRevocationExecutor to complete\n             scheduler.awaitAsynchronousCallbacksRun();\n             assertMemoryRevokingNotRequested();\n \n+            CompletableFuture<Void> future = new CompletableFuture<>();\n+            // Submit a task that will only be completed after the following two memory reserving actions have occurred.\n+            // It can make sure that no asynchronous memory revoking task occurs between the two memory reserving actions,\n+            // since `memoryRevocationExecutor` of the scheduler where all these tasks run is a single threaded pool\n+            scheduler.submitAsynchronousCallable(() -> future.get());\n             operatorContext1.localRevocableMemoryContext().setBytes(11);\n             operatorContext2.localRevocableMemoryContext().setBytes(12);\n+            future.complete(null);\n \n+            // Waiting for all existing tasks in scheduler's memoryRevocationExecutor to complete\n             scheduler.awaitAsynchronousCallbacksRun();\n             assertMemoryRevokingRequestedFor(operatorContext1, operatorContext2);\n             assertEquals(TestOperatorContext.firstOperator, \"operator2\"); // operator2 should revoke first since it (and it's encompassing task) has allocated more bytes\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24738",
    "pr_id": 24738,
    "issue_id": 24731,
    "repo": "prestodb/presto",
    "problem_statement": "Missing data type information on iceberg partition transforms\n<!--Tips before filing an issue -->\n<!-- Join the Presto Slack Channel to engage in conversations and get faster support at https://https://prestodb.io/slack. -->\n<!-- If you have triaged this as a bug, then file an [BUG](https://github.com/prestodb/presto/issues/new/choose) directly. -->\n\n## Describe the problem you faced\nIn this [section](https://prestodb.io/docs/current/connector/iceberg.html#partition-column-transform) , it introduces the basic concepts of Iceberg partition transform, but lack the detail of which data type is/are supported for each transform and also which data types are not supported.\nFor example, UUID is a supported data type by presto, and according to this [table](https://prestodb.io/docs/current/connector/iceberg.html#prestodb-to-iceberg-type-mapping) there is a corresponding data type from iceberge.\nAnd according to iceberg partition transform spec UUID is supported column data type by `bucket` transform. But presto iceberg connector doesn't support this data type.\n\n`Query 20250314_205658_00005_7rdtf failed: Unsupported type for 'bucket': 1000: c_uuid_bucket: bucket[3](1)\n`\n* A clear and concise description of the problem.\n* Was this working before or is this a first try?\n* If this worked before, then what has changed that recently?  \n* Provide table DDLs and `` EXPLAIN ANALYZE `` for your Presto queries.\n\n## Environment Description\n\n* Presto version used: 0.92\n* Storage (HDFS/S3/GCS..):\n* Data source and connectors or catalogs used: iceberg\n* Deployment (Cloud or On-prem): on-prem\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\n\n## Steps To Reproduce\n\nSteps to reproduce the behavior:\n1. presto:iceberg> create table bucket_t1(c_uuid uuid, c_decimal DECIMAL(23,3), c_TIMESTAMP TIMESTAMP) with (format='PARQUET', partitioning=ARRAY['bucket(c_uuid,3)', 'bucket(c_decimal,5)', 'bucket(c_timestamp, 7)']);\n2. presto:iceberg> insert into bucket_t1 values(uuid(), cast('123456789.124' AS decimal(22,3)), TIMESTAMP '2025-03-13 08:09:11');\n3.\n4.\n\n## Expected behavior\n\n<!--- A clear and concise description of what you expected to happen.-->\n\n\n## Additional context\n\n<!--- Add any other context about the problem here. What is your use case? What are you trying to accomplish? Providing context helps us come up with a solution that is most useful in the real world.-->\n\n## Stacktrace\n\n<!---Add the complete stacktrace of the error.-->\n",
    "issue_word_count": 370,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java"
    ],
    "base_commit": "446afa2cf0d8d32f2e2cd41603b070c70beb01d4",
    "head_commit": "d2336c0d357484190c2e02fe8ef8f96cc7ac034f",
    "repo_url": "https://github.com/prestodb/presto/pull/24738",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24738",
    "dockerfile": "",
    "pr_merged_at": "2025-03-18T00:43:44.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 8ffda4a7e85d2..68c74720c37fc 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -1213,6 +1213,30 @@ Create an Iceberg table partitioned by ``ts``::\n         partitioning = ARRAY['hour(ts)']\n     );\n \n+Types for partition transforms\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The list of supported types in Presto for each partition transform is as follows:\n+\n+===================== =======================================================================\n+Transform Name        Source Types\n+===================== =======================================================================\n+``Identity``          ``boolean``, ``int``, ``bigint``, ``real``, ``double``, ``decimal``,\n+                      ``varchar``, ``varbinary``, ``date``, ``time``, ``timestamp``\n+\n+``Bucket``            ``int``, ``bigint``, ``decimal``, ``varchar``, ``varbinary``, ``date``\n+\n+``Truncate``          ``int``, ``bigint``, ``decimal``, ``varchar``, ``varbinary``\n+\n+``Year``              ``date``, ``timestamp``\n+\n+``Month``             ``date``, ``timestamp``\n+\n+``Day``               ``date``, ``timestamp``\n+\n+``Hour``              ``timestamp``\n+===================== =======================================================================\n+\n CREATE VIEW\n ^^^^^^^^^^^\n \n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\nindex b9cd7baf0849d..3e219d3b60748 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n@@ -1577,6 +1577,100 @@ public void testTimestampPartitionedByHour(String zoneId, boolean legacyTimestam\n         }\n     }\n \n+    @Test\n+    public void testPartitionTransformOnTimestampWithTimeZone()\n+    {\n+        //TODO: Not yet support identity transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_identity_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['col_timestamp_tz'])\");\n+        assertQueryFails(\"insert into test_identity_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Type not supported as partition column: timestamp with time zone\");\n+        assertUpdate(\"drop table if exists test_identity_transform_timestamp_tz\");\n+\n+        //TODO: Not yet support bucket transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_bucket_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['bucket(col_timestamp_tz, 2)'])\");\n+        assertQueryFails(\"insert into test_bucket_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Unsupported type for 'bucket': 1000: col_timestamp_tz_bucket: bucket\\\\[2\\\\]\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_bucket_transform_timestamp_tz\");\n+\n+        //TODO: Not yet support year transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_year_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['year(col_timestamp_tz)'])\");\n+        assertQueryFails(\"insert into test_year_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Unsupported type for 'year': 1000: col_timestamp_tz_year: year\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_year_transform_timestamp_tz\");\n+\n+        //TODO: Not yet support month transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_month_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['month(col_timestamp_tz)'])\");\n+        assertQueryFails(\"insert into test_month_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Unsupported type for 'month': 1000: col_timestamp_tz_month: month\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_month_transform_timestamp_tz\");\n+\n+        //TODO: Not yet support day transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_day_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['day(col_timestamp_tz)'])\");\n+        assertQueryFails(\"insert into test_day_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Unsupported type for 'day': 1000: col_timestamp_tz_day: day\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_day_transform_timestamp_tz\");\n+\n+        //TODO: Not yet support hour transform for timestamp with time zone, which was supported by Iceberg\n+        assertUpdate(\"create table test_hour_transform_timestamp_tz(col_timestamp_tz timestamp with time zone)\" +\n+                \"with (partitioning = ARRAY['hour(col_timestamp_tz)'])\");\n+        assertQueryFails(\"insert into test_hour_transform_timestamp_tz \" +\n+                        \"values(CAST('2023-01-01 00:00:00.000 UTC' AS TIMESTAMP WITH TIME ZONE))\",\n+                \"Unsupported type for 'hour': 1000: col_timestamp_tz_hour: hour\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_hour_transform_timestamp_tz\");\n+    }\n+\n+    @Test\n+    public void testPartitionTransformOnUUID()\n+    {\n+        //TODO: Not yet support identity transform for uuid, which was supported by Iceberg\n+        assertUpdate(\"create table test_identity_transform_uuid(col_uuid uuid)\" +\n+                \"with (partitioning = ARRAY['col_uuid'])\");\n+        assertQueryFails(\"insert into test_identity_transform_uuid \" +\n+                        \"values(cast ('d2177dd0-eaa2-11de-a572-001b779c76e1' as uuid))\",\n+                \"Type not supported as partition column: uuid\");\n+        assertUpdate(\"drop table if exists test_identity_transform_uuid\");\n+\n+        //TODO: Not yet support bucket transform for uuid, which was supported by Iceberg\n+        assertUpdate(\"create table test_bucket_transform_uuid(col_uuid uuid)\" +\n+                \"with (partitioning = ARRAY['bucket(col_uuid, 2)'])\");\n+        assertQueryFails(\"insert into test_bucket_transform_uuid \" +\n+                        \"values(cast ('d2177dd0-eaa2-11de-a572-001b779c76e1' as uuid))\",\n+                \"Unsupported type for 'bucket': 1000: col_uuid_bucket: bucket\\\\[2\\\\]\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_bucket_transform_uuid\");\n+    }\n+\n+    @Test\n+    public void testBucketTransformOnTime()\n+    {\n+        //TODO: Not yet support bucket transform for time, which was supported by Iceberg\n+        assertUpdate(\"create table test_bucket_transform_time(col_time time)\" +\n+                \"with (partitioning = ARRAY['bucket(col_time, 2)'])\");\n+        assertQueryFails(\"insert into test_bucket_transform_time values(time '01:02:03.123')\",\n+                \"Unsupported type for 'bucket': 1000: col_time_bucket: bucket\\\\[2\\\\]\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_bucket_transform_time\");\n+    }\n+\n+    @Test\n+    public void testBucketTransformOnTimestamp()\n+    {\n+        //TODO: Not yet support bucket transform for timestamp, which was supported by Iceberg\n+        assertUpdate(\"create table test_bucket_transform_timestamp(col_timestamp timestamp)\" +\n+                \"with (partitioning = ARRAY['bucket(col_timestamp, 2)'])\");\n+        assertQueryFails(\"insert into test_bucket_transform_timestamp values(timestamp '1984-01-08 01:02:03.123')\",\n+                \"Unsupported type for 'bucket': 1000: col_timestamp_bucket: bucket\\\\[2\\\\]\\\\(1\\\\)\");\n+        assertUpdate(\"drop table if exists test_bucket_transform_timestamp\");\n+    }\n+\n     @Test\n     public void testRegisterTable()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24715",
    "pr_id": 24715,
    "issue_id": 24698,
    "repo": "prestodb/presto",
    "problem_statement": "doc-only PR seems to not follow the doc-only PR CI workflow\n\n## Your Environment\nGitHub Presto PRs. \n\n## Expected Behavior\nThe doc-only CI workflow should complete all checks. \n\n## Current Behavior\nTwo different open doc-only PRs - #24696 and #24687 - appear to be stuck and not following the doc-only CI workflow. \n\n## Possible Solution\n\n## Steps to Reproduce\n\nSee #24696 and #24687 . \n\n## Screenshots (if appropriate)\n\n## Context\nDoc-only PRs that do not complete the checks cannot be merged. \n",
    "issue_word_count": 77,
    "test_files_count": 5,
    "non_test_files_count": 2,
    "pr_changed_files": [
      ".github/workflows/hive-tests.yml",
      ".github/workflows/kudu.yml",
      ".github/workflows/product-tests-basic-environment.yml",
      ".github/workflows/product-tests-specific-environment.yml",
      ".github/workflows/singlestore-tests.yml",
      ".github/workflows/spark-integration.yml",
      ".github/workflows/test-other-modules.yml"
    ],
    "pr_changed_test_files": [
      ".github/workflows/hive-tests.yml",
      ".github/workflows/product-tests-basic-environment.yml",
      ".github/workflows/product-tests-specific-environment.yml",
      ".github/workflows/singlestore-tests.yml",
      ".github/workflows/test-other-modules.yml"
    ],
    "base_commit": "06c8e35938b5001428ab94c86d3f99c4d29d4bee",
    "head_commit": "19d108714878b6639f74958a4bc8a38dc559bff1",
    "repo_url": "https://github.com/prestodb/presto/pull/24715",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24715",
    "dockerfile": "",
    "pr_merged_at": "2025-03-14T20:27:18.000Z",
    "patch": "diff --git a/.github/workflows/kudu.yml b/.github/workflows/kudu.yml\nindex 8d1c4151ef809..cc741b8066a7d 100644\n--- a/.github/workflows/kudu.yml\n+++ b/.github/workflows/kudu.yml\n@@ -35,27 +35,31 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-kudu-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Maven Install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am --no-transfer-progress -pl presto-kudu\n       - name: Kudu Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           presto-kudu/bin/run_kudu_tests.sh 3 null\n           presto-kudu/bin/run_kudu_tests.sh 1 \"\"\n\ndiff --git a/.github/workflows/spark-integration.yml b/.github/workflows/spark-integration.yml\nindex 005b18231c1b1..3929e081c5f74 100644\n--- a/.github/workflows/spark-integration.yml\n+++ b/.github/workflows/spark-integration.yml\n@@ -36,25 +36,29 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-spark-integration-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Maven Install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl presto-spark-launcher,presto-spark-package,presto-spark-testing\n       - name: Maven Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         run: ./mvnw test ${MAVEN_TEST} -pl :presto-spark-testing -P test-presto-spark-integration-smoke-test\n",
    "test_patch": "diff --git a/.github/workflows/hive-tests.yml b/.github/workflows/hive-tests.yml\nindex b97a1f3cf24c4..7118bc411a74d 100644\n--- a/.github/workflows/hive-tests.yml\n+++ b/.github/workflows/hive-tests.yml\n@@ -37,29 +37,34 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-hive-tests-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Install Hive Module\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl :presto-hive-hadoop2\n       - name: Run Hive Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         run: presto-hive-hadoop2/bin/run_hive_tests.sh\n       - name: Run Hive S3 Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           AWS_ACCESS_KEY_ID: ${{ secrets.HIVE_AWS_ACCESSKEY }}\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.HIVE_AWS_SECRETKEY }}\n@@ -70,6 +75,7 @@ jobs:\n               presto-hive-hadoop2/bin/run_hive_s3_tests.sh\n           fi\n       - name: Run Hive Glue Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           AWS_ACCESS_KEY_ID: ${{ secrets.HIVE_AWS_ACCESSKEY }}\n           AWS_SECRET_ACCESS_KEY: ${{ secrets.HIVE_AWS_SECRETKEY }}\n@@ -85,25 +91,29 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 20\n     concurrency:\n       group: ${{ github.workflow }}-hive-dockerized-tests-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Install Hive Module\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl :presto-hive\n       - name: Run Hive Dockerized Tests\n-        run: ./mvnw test ${MAVEN_TEST} -pl :presto-hive -P test-hive-insert-overwrite\n\\ No newline at end of file\n+        if: needs.changes.outputs.codechange == 'true'\n+        run: ./mvnw test ${MAVEN_TEST} -pl :presto-hive -P test-hive-insert-overwrite\n\ndiff --git a/.github/workflows/product-tests-basic-environment.yml b/.github/workflows/product-tests-basic-environment.yml\nindex 5d44eba7bb1f2..6c76b0d355e34 100644\n--- a/.github/workflows/product-tests-basic-environment.yml\n+++ b/.github/workflows/product-tests-basic-environment.yml\n@@ -35,33 +35,38 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-product-tests-basic-environment-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - name: Free Disk Space\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           df -h\n           sudo apt-get clean\n           rm -rf /opt/hostedtoolcache\n           df -h\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Maven install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl '!presto-docs,!presto-spark-package,!presto-spark-launcher,!presto-spark-testing,!presto-test-coverage'\n       - name: Run Product Tests Basic Environment\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh multinode -x quarantine,big_query,storage_formats,profile_specific_tests,tpcds,cassandra,mysql_connector,postgresql_connector,mysql,kafka,avro\n\ndiff --git a/.github/workflows/product-tests-specific-environment.yml b/.github/workflows/product-tests-specific-environment.yml\nindex 0a29cec164958..2d625d6a17ff4 100644\n--- a/.github/workflows/product-tests-specific-environment.yml\n+++ b/.github/workflows/product-tests-specific-environment.yml\n@@ -35,37 +35,43 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-product-tests-specific-environment1-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - name: Free Disk Space\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           df -h\n           sudo apt-get clean\n           rm -rf /opt/hostedtoolcache\n           df -h\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Maven install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl '!presto-docs,!presto-spark-package,!presto-spark-launcher,!presto-spark-testing,!presto-test-coverage'\n       - name: Product Tests Specific 1.1\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode -g hdfs_no_impersonation,avro\n       - name: Product Tests Specific 1.2\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-kerberos-hdfs-no-impersonation -g hdfs_no_impersonation\n@@ -73,14 +79,17 @@ jobs:\n       # - name: Product Tests Specific 1.3\n       #  run: presto-product-tests/bin/run_on_docker.sh singlenode-hdfs-impersonation -g storage_formats,cli,hdfs_impersonation\n       - name: Product Tests Specific 1.4\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-kerberos-hdfs-impersonation -g storage_formats,cli,hdfs_impersonation,authorization,hive_file_header\n       - name: Product Tests Specific 1.5\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-kerberos-hdfs-impersonation-cross-realm -g storage_formats,cli,hdfs_impersonation\n       - name: Product Tests Specific 1.6\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh multinode-tls-kerberos -g cli,group-by,join,tls\n@@ -92,49 +101,58 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-product-tests-specific-environment2-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - name: Free Disk Space\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           df -h\n           sudo apt-get clean\n           rm -rf /opt/hostedtoolcache\n           df -h\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Maven install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am -pl '!presto-docs,!presto-spark-package,!presto-spark-launcher,!presto-spark-testing,!presto-test-coverage'\n       - name: Product Tests Specific 2.1\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-ldap -g ldap -x simba_jdbc\n       - name: Product Tests Specific 2.2\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh multinode-tls -g smoke,cli,group-by,join,tls\n       - name: Product Tests Specific 2.3\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-mysql -g mysql_connector,mysql\n       - name: Product Tests Specific 2.4\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-postgresql -g postgresql_connector\n       - name: Product Tests Specific 2.5\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-cassandra -g cassandra\n@@ -142,10 +160,12 @@ jobs:\n       # - name: Product Tests Specific 2.6\n       #  run: presto-product-tests/bin/run_on_docker.sh singlenode-kerberos-hdfs-impersonation-with-wire-encryption -g storage_formats,cli,hdfs_impersonation,authorization\n       - name: Product Tests Specific 2.7\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-kafka -g kafka\n       - name: Product Tests Specific 2.8\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           OVERRIDE_JDK_DIR: ${{ env.JAVA_HOME }}\n         run: presto-product-tests/bin/run_on_docker.sh singlenode-sqlserver -g sqlserver\n\ndiff --git a/.github/workflows/singlestore-tests.yml b/.github/workflows/singlestore-tests.yml\nindex cd4b2c91b7660..ada2f4026f43d 100644\n--- a/.github/workflows/singlestore-tests.yml\n+++ b/.github/workflows/singlestore-tests.yml\n@@ -36,17 +36,17 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n-\n     timeout-minutes: 30\n     concurrency:\n       group: ${{ github.workflow }}-singlestore-dockerized-tests-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - name: Remove unnecessary pre-installed toolchains for free disk spaces\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           echo \"=== BEFORE ===\"\n           df -h\n@@ -58,17 +58,21 @@ jobs:\n           echo \"=== AFTER ===\"\n           df -h\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       - name: Install SingleStore Module\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -am  --no-transfer-progress -pl :presto-singlestore\n       - name: Run SingleStore Dockerized Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         env:\n           SINGLESTORE_LICENSE: ${{ secrets.SINGLESTORE_LICENSE }}\n-        run: ./mvnw test ${MAVEN_TEST} -pl :presto-singlestore\n\\ No newline at end of file\n+        run: ./mvnw test ${MAVEN_TEST} -pl :presto-singlestore\n\ndiff --git a/.github/workflows/test-other-modules.yml b/.github/workflows/test-other-modules.yml\nindex 131a8f4a99ded..afb84cfd7bf5d 100644\n--- a/.github/workflows/test-other-modules.yml\n+++ b/.github/workflows/test-other-modules.yml\n@@ -36,30 +36,35 @@ jobs:\n         java: [ 8, 17.0.13 ]\n     runs-on: ubuntu-latest\n     needs: changes\n-    if: needs.changes.outputs.codechange == 'true'\n     timeout-minutes: 60\n     concurrency:\n       group: ${{ github.workflow }}-test-other-modules-${{ github.event.pull_request.number }}-${{ matrix.java }}\n       cancel-in-progress: true\n     steps:\n       - uses: actions/checkout@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           show-progress: false\n       - uses: actions/setup-java@v4\n+        if: needs.changes.outputs.codechange == 'true'\n         with:\n           distribution: 'temurin'\n           java-version: ${{ matrix.java }}\n           cache: 'maven'\n       - name: Download nodejs to maven cache\n+        if: needs.changes.outputs.codechange == 'true'\n         run: .github/bin/download_nodejs\n       # Workaround for Ubuntu 24 and mysql to find the dependent library (https://github.com/prestodb/presto/issues/24327)\n       - name: Create symlink for libaio.so.1\n+        if: needs.changes.outputs.codechange == 'true'\n         run: sudo ln -s /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1\n       - name: Maven Install\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           export MAVEN_OPTS=\"${MAVEN_INSTALL_OPTS}\"\n           ./mvnw install ${MAVEN_FAST_INSTALL} -pl '!:presto-docs,!:presto-server,!presto-test-coverage'\n       - name: Maven Tests\n+        if: needs.changes.outputs.codechange == 'true'\n         run: |\n           ./mvnw test -T 1 ${MAVEN_TEST} -pl '\n             !presto-tests, \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24670",
    "pr_id": 24670,
    "issue_id": 24597,
    "repo": "prestodb/presto",
    "problem_statement": "Support choosing compression codecs for exchange\nCompression might benefit queries with large amount of data bring shuffled, like TPCH Q9. However, \n\n1) Presto defaults to LZ4 and users don't have a way to specify other codecs. See https://github.com/prestodb/presto/blob/a766a3a1ecd5fa45426977bc05e707da5d1c138c/presto-main/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java#L66\n\n2) Velox defaults to none and no compression is made at all. See PartitionedOutput.cpp\n```\nif (serde_->kind() == VectorSerde::Kind::kPresto) {\n      serializer::presto::PrestoVectorSerde::PrestoOptions options;\n      options.compressionKind =\n          OutputBufferManager::getInstance().lock()->compressionKind();\n      options.minCompressionRatio = PartitionedOutput::minCompressionRatio();\n      current_->createStreamTree(rowType, rowsInCurrent_, &options);\n    }\n```\nHere OutputBufferManager::getInstance() uses the default Option that doesn't compress. \n```\nstruct Options {\n    common::CompressionKind compressionKind{\n        common::CompressionKind::CompressionKind_NONE};\n  };\n```\n\n3) THere are not enough tests for different compression codecs for exchange.\n\n## Expected Behavior or Use Case\nCreate a config property \"exchange.compression-codec\" that chooses from an enum. This will replace the current Presto \"exchange.compression-enabled\". Introduce the same for Presto CPP and pass it to Velox.\n\n## Presto Component, Service, or Connector\nExchange\n\n## Possible Implementation\nJust straightforward implementation\n\n## Example Screenshots (if appropriate):\n\n## Context\nThis is one part of the exchange optimization. When working on it I discovered the missing functionality.",
    "issue_word_count": 200,
    "test_files_count": 20,
    "non_test_files_count": 24,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/admin/properties.rst",
      "presto-docs/src/main/sphinx/admin/spill.rst",
      "presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java",
      "presto-main-base/src/main/java/com/facebook/presto/CompressionCodec.java",
      "presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftCompressorAdapter.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftDecompressorAdapter.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipCompressor.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipDecompressor.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibCompressor.java",
      "presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibDecompressor.java",
      "presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheConfig.java",
      "presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/server/protocol/Query.java",
      "presto-main-base/src/main/java/com/facebook/presto/spiller/FileSingleStreamSpillerFactory.java",
      "presto-main-base/src/main/java/com/facebook/presto/spiller/NodeSpillConfig.java",
      "presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageSingleStreamSpillerFactory.java",
      "presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageStandaloneSpillerFactory.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/HttpRemoteSourceFactory.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java",
      "presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java",
      "presto-main/src/test/java/com/facebook/presto/server/TestServer.java",
      "presto-native-execution/presto_cpp/main/QueryContextManager.cpp",
      "presto-native-execution/presto_cpp/main/SessionProperties.h",
      "presto-spi/src/main/java/com/facebook/presto/spi/page/PagesSerde.java"
    ],
    "pr_changed_test_files": [
      "presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java",
      "presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java",
      "presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java",
      "presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java",
      "presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java",
      "presto-main/src/test/java/com/facebook/presto/server/TestServer.java"
    ],
    "base_commit": "45e0e21ef65a28380d984b36200075566fe2830d",
    "head_commit": "08eb063bc1da8d770186e2becac34eef7ab1c49b",
    "repo_url": "https://github.com/prestodb/presto/pull/24670",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24670",
    "dockerfile": "",
    "pr_merged_at": "2025-04-04T22:07:47.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/admin/properties.rst b/presto-docs/src/main/sphinx/admin/properties.rst\nindex 83505c0e0fb5e..36b411c6e3a46 100644\n--- a/presto-docs/src/main/sphinx/admin/properties.rst\n+++ b/presto-docs/src/main/sphinx/admin/properties.rst\n@@ -388,13 +388,14 @@ Limit for memory used for unspilling a single aggregation operator instance.\n \n The corresponding session property is :ref:`admin/properties-session:\\`\\`aggregation_operator_unspill_memory_limit\\`\\``. \n \n-``experimental.spill-compression-enabled``\n-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+``experimental.spill-compression-codec``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n-* **Type:** ``boolean``\n-* **Default value:** ``false``\n+* **Type:** ``string``\n+* **Allowed value:** ``SNAPPY``, ``NONE``, ``GZIP``, ``LZ4``, ``LZO``,, ``ZLIB`` ``ZSTD``\n+* **Default value:** ``NONE``\n \n-Enables data compression for pages spilled to disk.\n+The data compression codec to be used for pages spilled to disk.\n \n ``experimental.spill-encryption-enabled``\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ndiff --git a/presto-docs/src/main/sphinx/admin/spill.rst b/presto-docs/src/main/sphinx/admin/spill.rst\nindex b186dd8283f98..3dd6f1dec22de 100644\n--- a/presto-docs/src/main/sphinx/admin/spill.rst\n+++ b/presto-docs/src/main/sphinx/admin/spill.rst\n@@ -93,10 +93,10 @@ there is no need to use RAID for spill.\n Spill Compression\n -----------------\n \n-When spill compression is enabled (``spill-compression-enabled`` property in\n-:ref:`tuning-spilling`), spilled pages will be compressed using the same\n-implementation as exchange compression when they are sufficiently compressible.\n-Enabling this feature can reduce the amount of disk IO at the cost\n+When :ref:`admin/properties:\\`\\`experimental.spill-compression-codec\\`\\`` is \n+configured, spilled pages are compressed using \n+the configured codec implementation when they are sufficiently compressible.\n+This feature can reduce the amount of disk IO at the cost\n of extra CPU load to compress and decompress spilled pages.\n \n Spill Encryption\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/CompressionCodec.java b/presto-main-base/src/main/java/com/facebook/presto/CompressionCodec.java\nnew file mode 100644\nindex 0000000000000..f3982379b84f2\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/CompressionCodec.java\n@@ -0,0 +1,18 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto;\n+\n+public enum CompressionCodec {\n+    GZIP, LZ4, LZO, SNAPPY, ZLIB, ZSTD, NONE\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java b/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\nindex 0e548c5ef5b4a..fe1b35cc3c368 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\n@@ -160,7 +160,7 @@ public final class SystemSessionProperties\n     public static final String ITERATIVE_OPTIMIZER_TIMEOUT = \"iterative_optimizer_timeout\";\n     public static final String QUERY_ANALYZER_TIMEOUT = \"query_analyzer_timeout\";\n     public static final String RUNTIME_OPTIMIZER_ENABLED = \"runtime_optimizer_enabled\";\n-    public static final String EXCHANGE_COMPRESSION = \"exchange_compression\";\n+    public static final String EXCHANGE_COMPRESSION_CODEC = \"exchange_compression_codec\";\n     public static final String EXCHANGE_CHECKSUM = \"exchange_checksum\";\n     public static final String LEGACY_TIMESTAMP = \"legacy_timestamp\";\n     public static final String ENABLE_INTERMEDIATE_AGGREGATIONS = \"enable_intermediate_aggregations\";\n@@ -840,11 +840,15 @@ public SystemSessionProperties(\n                         \"Experimental: enable runtime optimizer\",\n                         featuresConfig.isRuntimeOptimizerEnabled(),\n                         false),\n-                booleanProperty(\n-                        EXCHANGE_COMPRESSION,\n-                        \"Enable compression in exchanges\",\n-                        featuresConfig.isExchangeCompressionEnabled(),\n-                        false),\n+                new PropertyMetadata<>(\n+                        EXCHANGE_COMPRESSION_CODEC,\n+                        \"Exchange compression codec\",\n+                        VARCHAR,\n+                        CompressionCodec.class,\n+                        featuresConfig.getExchangeCompressionCodec(),\n+                        false,\n+                        value -> CompressionCodec.valueOf(((String) value).toUpperCase()),\n+                        CompressionCodec::name),\n                 booleanProperty(\n                         EXCHANGE_CHECKSUM,\n                         \"Enable checksum in exchanges\",\n@@ -2291,9 +2295,9 @@ public static Duration getQueryAnalyzerTimeout(Session session)\n         return session.getSystemProperty(QUERY_ANALYZER_TIMEOUT, Duration.class);\n     }\n \n-    public static boolean isExchangeCompressionEnabled(Session session)\n+    public static CompressionCodec getExchangeCompressionCodec(Session session)\n     {\n-        return session.getSystemProperty(EXCHANGE_COMPRESSION, Boolean.class);\n+        return session.getSystemProperty(EXCHANGE_COMPRESSION_CODEC, CompressionCodec.class);\n     }\n \n     public static boolean isExchangeChecksumEnabled(Session session)\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftCompressorAdapter.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftCompressorAdapter.java\nnew file mode 100644\nindex 0000000000000..330786ed1ea06\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftCompressorAdapter.java\n@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import com.facebook.presto.spi.page.PageCompressor;\n+import io.airlift.compress.Compressor;\n+\n+import java.nio.ByteBuffer;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class AirliftCompressorAdapter\n+        implements PageCompressor\n+{\n+    private final Compressor compressor;\n+\n+    public AirliftCompressorAdapter(Compressor compressor)\n+    {\n+        this.compressor = requireNonNull(compressor, \"compressor is null\");\n+    }\n+\n+    @Override\n+    public int maxCompressedLength(int uncompressedSize)\n+    {\n+        return compressor.maxCompressedLength(uncompressedSize);\n+    }\n+\n+    @Override\n+    public int compress(\n+            byte[] input,\n+            int inputOffset,\n+            int inputLength,\n+            byte[] output,\n+            int outputOffset,\n+            int maxOutputLength)\n+    {\n+        return compressor.compress(input, inputOffset, inputLength, output, outputOffset, maxOutputLength);\n+    }\n+\n+    @Override\n+    public void compress(ByteBuffer input, ByteBuffer output)\n+    {\n+        compressor.compress(input, output);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftDecompressorAdapter.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftDecompressorAdapter.java\nnew file mode 100644\nindex 0000000000000..9d82e6adc428e\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/AirliftDecompressorAdapter.java\n@@ -0,0 +1,50 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import com.facebook.presto.spi.page.PageDecompressor;\n+import io.airlift.compress.Decompressor;\n+\n+import java.nio.ByteBuffer;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class AirliftDecompressorAdapter\n+        implements PageDecompressor\n+{\n+    private final Decompressor decompressor;\n+\n+    public AirliftDecompressorAdapter(Decompressor decompressor)\n+    {\n+        this.decompressor = requireNonNull(decompressor, \"decompressor is null\");\n+    }\n+\n+    @Override\n+    public int decompress(\n+            byte[] input,\n+            int inputOffset,\n+            int inputLength,\n+            byte[] output,\n+            int outputOffset,\n+            int maxOutputLength)\n+    {\n+        return decompressor.decompress(input, inputOffset, inputLength, output, outputOffset, maxOutputLength);\n+    }\n+\n+    @Override\n+    public void decompress(ByteBuffer input, ByteBuffer output)\n+    {\n+        decompressor.decompress(input, output);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipCompressor.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipCompressor.java\nnew file mode 100644\nindex 0000000000000..a04ef14c68012\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipCompressor.java\n@@ -0,0 +1,68 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import io.airlift.compress.Compressor;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.zip.GZIPOutputStream;\n+\n+public class GzipCompressor\n+        implements Compressor\n+{\n+    private static final int EXTRA_COMPRESSION_SPACE = 16;\n+\n+    @Override\n+    public int maxCompressedLength(int uncompressedSize)\n+    {\n+        // From Mark Adler's post http://stackoverflow.com/questions/1207877/java-size-of-compression-output-bytearray\n+        return uncompressedSize + ((uncompressedSize + 7) >> 3) + ((uncompressedSize + 63) >> 6) + 5 + EXTRA_COMPRESSION_SPACE;\n+    }\n+\n+    @Override\n+    public int compress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset, int maxOutputLength)\n+    {\n+        try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+                GZIPOutputStream gzipOutputStream = new GZIPOutputStream(byteArrayOutputStream)) {\n+            gzipOutputStream.write(input, inputOffset, inputLength);\n+            gzipOutputStream.finish();\n+            byte[] compressed = byteArrayOutputStream.toByteArray();\n+            if (compressed.length > maxOutputLength) {\n+                throw new IllegalArgumentException(\"maxCompressedLength formula is incorrect, because gzip produced more data\");\n+            }\n+            System.arraycopy(compressed, 0, output, outputOffset, compressed.length);\n+            return compressed.length;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void compress(ByteBuffer input, ByteBuffer output)\n+    {\n+        if (input.isDirect() || output.isDirect() || !input.hasArray() || !output.hasArray()) {\n+            throw new IllegalArgumentException(\"Non-direct byte buffer backed by byte array required\");\n+        }\n+        int inputOffset = input.arrayOffset() + input.position();\n+        int outputOffset = output.arrayOffset() + output.position();\n+\n+        int written = compress(input.array(), inputOffset, input.remaining(), output.array(), outputOffset, output.remaining());\n+        ((Buffer) output).position(output.position() + written);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipDecompressor.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipDecompressor.java\nnew file mode 100644\nindex 0000000000000..daf8ca2bb41ce\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/GzipDecompressor.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import io.airlift.compress.Decompressor;\n+import io.airlift.compress.MalformedInputException;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.zip.GZIPInputStream;\n+\n+public class GzipDecompressor\n+        implements Decompressor\n+{\n+    @Override\n+    public int decompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset, int maxOutputLength) throws MalformedInputException\n+    {\n+        try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(input, inputOffset, inputLength);\n+                GZIPInputStream gzipInputStream = new GZIPInputStream(byteArrayInputStream)) {\n+            int totalRead = 0;\n+            int bytesRead;\n+            while (totalRead < maxOutputLength) {\n+                bytesRead = gzipInputStream.read(output, outputOffset + totalRead, maxOutputLength - totalRead);\n+                if (bytesRead == -1) {\n+                    break;\n+                }\n+                totalRead += bytesRead;\n+            }\n+            if (totalRead >= maxOutputLength && gzipInputStream.read() != -1) {\n+                throw new IllegalArgumentException(\"maxOutputLength is incorrect, there is more data to be decompressed\");\n+            }\n+            return totalRead;\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void decompress(ByteBuffer input, ByteBuffer output) throws MalformedInputException\n+    {\n+        int inputOffset = input.arrayOffset() + input.position();\n+        int outputOffset = output.arrayOffset() + output.position();\n+        int written = decompress(input.array(), inputOffset, input.remaining(), output.array(), outputOffset, output.remaining());\n+        ((Buffer) output).position(output.position() + written);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java\nindex b3150f552325d..71d43ef6f4db6 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/PagesSerdeFactory.java\n@@ -13,36 +13,41 @@\n  */\n package com.facebook.presto.execution.buffer;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.block.BlockEncodingSerde;\n import com.facebook.presto.spi.page.PageCompressor;\n import com.facebook.presto.spi.page.PageDecompressor;\n import com.facebook.presto.spi.page.PagesSerde;\n import com.facebook.presto.spi.spiller.SpillCipher;\n-import io.airlift.compress.Compressor;\n-import io.airlift.compress.Decompressor;\n import io.airlift.compress.lz4.Lz4Compressor;\n import io.airlift.compress.lz4.Lz4Decompressor;\n+import io.airlift.compress.lzo.LzoCompressor;\n+import io.airlift.compress.lzo.LzoDecompressor;\n+import io.airlift.compress.snappy.SnappyCompressor;\n+import io.airlift.compress.snappy.SnappyDecompressor;\n+import io.airlift.compress.zstd.ZstdCompressor;\n+import io.airlift.compress.zstd.ZstdDecompressor;\n \n-import java.nio.ByteBuffer;\n import java.util.Optional;\n+import java.util.OptionalInt;\n \n import static java.util.Objects.requireNonNull;\n \n public class PagesSerdeFactory\n {\n     private final BlockEncodingSerde blockEncodingSerde;\n-    private final boolean compressionEnabled;\n+    private final CompressionCodec compressionCodec;\n     private final boolean checksumEnabled;\n \n-    public PagesSerdeFactory(BlockEncodingSerde blockEncodingSerde, boolean compressionEnabled)\n+    public PagesSerdeFactory(BlockEncodingSerde blockEncodingSerde, CompressionCodec compressionCodec)\n     {\n-        this(blockEncodingSerde, compressionEnabled, false);\n+        this(blockEncodingSerde, compressionCodec, false);\n     }\n \n-    public PagesSerdeFactory(BlockEncodingSerde blockEncodingSerde, boolean compressionEnabled, boolean checksumEnabled)\n+    public PagesSerdeFactory(BlockEncodingSerde blockEncodingSerde, CompressionCodec compressionCodec, boolean checksumEnabled)\n     {\n         this.blockEncodingSerde = requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\");\n-        this.compressionEnabled = compressionEnabled;\n+        this.compressionCodec = requireNonNull(compressionCodec, \"compressionCodec is null\");\n         this.checksumEnabled = checksumEnabled;\n     }\n \n@@ -58,60 +63,48 @@ public PagesSerde createPagesSerdeForSpill(Optional<SpillCipher> spillCipher)\n \n     private PagesSerde createPagesSerdeInternal(Optional<SpillCipher> spillCipher)\n     {\n-        if (compressionEnabled) {\n-            return new PagesSerde(\n-                    blockEncodingSerde,\n-                    Optional.of(new PageCompressor()\n-                    {\n-                        Compressor compressor = new Lz4Compressor();\n-                        @Override\n-                        public int maxCompressedLength(int uncompressedSize)\n-                        {\n-                            return compressor.maxCompressedLength(uncompressedSize);\n-                        }\n-\n-                        @Override\n-                        public int compress(\n-                                byte[] input,\n-                                int inputOffset,\n-                                int inputLength,\n-                                byte[] output,\n-                                int outputOffset,\n-                                int maxOutputLength)\n-                        {\n-                            return compressor.compress(input, inputOffset, inputLength, output, outputOffset, maxOutputLength);\n-                        }\n-\n-                        @Override\n-                        public void compress(ByteBuffer input, ByteBuffer output)\n-                        {\n-                            compressor.compress(input, output);\n-                        }\n-                    }),\n-                    Optional.of(new PageDecompressor()\n-                    {\n-                        Decompressor decompressor = new Lz4Decompressor();\n-                        @Override\n-                        public int decompress(\n-                                byte[] input,\n-                                int inputOffset,\n-                                int inputLength,\n-                                byte[] output,\n-                                int outputOffset,\n-                                int maxOutputLength)\n-                        {\n-                            return decompressor.decompress(input, inputOffset, inputLength, output, outputOffset, maxOutputLength);\n-                        }\n+        return new PagesSerde(blockEncodingSerde, getPageCompressor(), getPageDecompressor(), spillCipher, checksumEnabled);\n+    }\n \n-                        @Override\n-                        public void decompress(ByteBuffer input, ByteBuffer output)\n-                        {\n-                            decompressor.decompress(input, output);\n-                        }\n-                    }),\n-                    spillCipher, checksumEnabled);\n+    private Optional<PageCompressor> getPageCompressor()\n+    {\n+        switch (compressionCodec) {\n+            case GZIP:\n+                return Optional.of(new AirliftCompressorAdapter(new GzipCompressor()));\n+            case LZ4:\n+                return Optional.of(new AirliftCompressorAdapter(new Lz4Compressor()));\n+            case LZO:\n+                return Optional.of(new AirliftCompressorAdapter(new LzoCompressor()));\n+            case SNAPPY:\n+                return Optional.of(new AirliftCompressorAdapter(new SnappyCompressor()));\n+            case ZLIB:\n+                return Optional.of(new AirliftCompressorAdapter(new ZlibCompressor(OptionalInt.empty())));\n+            case ZSTD:\n+                return Optional.of(new AirliftCompressorAdapter(new ZstdCompressor()));\n+            case NONE:\n+            default:\n+                return Optional.empty();\n         }\n+    }\n \n-        return new PagesSerde(blockEncodingSerde, Optional.empty(), Optional.empty(), spillCipher, checksumEnabled);\n+    private Optional<PageDecompressor> getPageDecompressor()\n+    {\n+        switch (compressionCodec) {\n+            case GZIP:\n+                return Optional.of(new AirliftDecompressorAdapter(new GzipDecompressor()));\n+            case LZ4:\n+                return Optional.of(new AirliftDecompressorAdapter(new Lz4Decompressor()));\n+            case LZO:\n+                return Optional.of(new AirliftDecompressorAdapter(new LzoDecompressor()));\n+            case SNAPPY:\n+                return Optional.of(new AirliftDecompressorAdapter(new SnappyDecompressor()));\n+            case ZLIB:\n+                return Optional.of(new AirliftDecompressorAdapter(new ZlibDecompressor()));\n+            case ZSTD:\n+                return Optional.of(new AirliftDecompressorAdapter(new ZstdDecompressor()));\n+            case NONE:\n+            default:\n+                return Optional.empty();\n+        }\n     }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibCompressor.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibCompressor.java\nnew file mode 100644\nindex 0000000000000..77f4c364e5a48\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibCompressor.java\n@@ -0,0 +1,83 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import io.airlift.compress.Compressor;\n+\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.OptionalInt;\n+import java.util.zip.Deflater;\n+\n+import static java.util.Objects.requireNonNull;\n+import static java.util.zip.Deflater.FULL_FLUSH;\n+\n+public class ZlibCompressor\n+        implements Compressor\n+{\n+    private static final int EXTRA_COMPRESSION_SPACE = 16;\n+    private static final int DEFAULT_COMPRESSION_LEVEL = 4;\n+\n+    private final int compressionLevel;\n+\n+    public ZlibCompressor(OptionalInt compressionLevel)\n+    {\n+        requireNonNull(compressionLevel, \"compressionLevel is null\");\n+        this.compressionLevel = compressionLevel.orElse(DEFAULT_COMPRESSION_LEVEL);\n+    }\n+\n+    @Override\n+    public int maxCompressedLength(int uncompressedSize)\n+    {\n+        // From Mark Adler's post http://stackoverflow.com/questions/1207877/java-size-of-compression-output-bytearray\n+        return uncompressedSize + ((uncompressedSize + 7) >> 3) + ((uncompressedSize + 63) >> 6) + 5 + EXTRA_COMPRESSION_SPACE;\n+    }\n+\n+    @Override\n+    public int compress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset, int maxOutputLength)\n+    {\n+        int maxCompressedLength = maxCompressedLength(inputLength);\n+        if (maxOutputLength < maxCompressedLength) {\n+            throw new IllegalArgumentException(\"Output buffer must be at least \" + maxCompressedLength + \" bytes\");\n+        }\n+\n+        Deflater deflater = new Deflater(compressionLevel, false);\n+        try {\n+            deflater.setInput(input, inputOffset, inputLength);\n+            deflater.finish();\n+\n+            int compressedDataLength = deflater.deflate(output, outputOffset, maxOutputLength, FULL_FLUSH);\n+            if (!deflater.finished()) {\n+                throw new IllegalArgumentException(\"maxCompressedLength formula is incorrect, because deflate produced more data\");\n+            }\n+            return compressedDataLength;\n+        }\n+        finally {\n+            deflater.end();\n+        }\n+    }\n+\n+    @Override\n+    public void compress(ByteBuffer input, ByteBuffer output)\n+    {\n+        if (input.isDirect() || output.isDirect() || !input.hasArray() || !output.hasArray()) {\n+            throw new IllegalArgumentException(\"Non-direct byte buffer backed by byte array required\");\n+        }\n+        int inputOffset = input.arrayOffset() + input.position();\n+        int outputOffset = output.arrayOffset() + output.position();\n+\n+        int written = compress(input.array(), inputOffset, input.remaining(), output.array(), outputOffset, output.remaining());\n+        ((Buffer) output).position(output.position() + written);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibDecompressor.java b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibDecompressor.java\nnew file mode 100644\nindex 0000000000000..0a0076c39551b\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/execution/buffer/ZlibDecompressor.java\n@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution.buffer;\n+\n+import io.airlift.compress.Decompressor;\n+import io.airlift.compress.MalformedInputException;\n+\n+import java.nio.Buffer;\n+import java.nio.ByteBuffer;\n+import java.util.zip.DataFormatException;\n+import java.util.zip.Inflater;\n+\n+public class ZlibDecompressor\n+        implements Decompressor\n+{\n+    @Override\n+    public int decompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset, int maxOutputLength)\n+            throws MalformedInputException\n+    {\n+        Inflater inflater = new Inflater(false);\n+        inflater.setInput(input, inputOffset, inputLength);\n+        int uncompressedLength = 0;\n+        try {\n+            uncompressedLength = inflater.inflate(output, outputOffset, maxOutputLength);\n+            if (!inflater.finished()) {\n+                throw new IllegalArgumentException(\"maxOutputLength is incorrect, there is more data to be decompressed\");\n+            }\n+        }\n+        catch (DataFormatException e) {\n+            throw new MalformedInputException(inputOffset, e.getMessage());\n+        }\n+        finally {\n+            inflater.end();\n+        }\n+        return uncompressedLength;\n+    }\n+\n+    @Override\n+    public void decompress(ByteBuffer input, ByteBuffer output)\n+            throws MalformedInputException\n+    {\n+        if (input.isDirect() || output.isDirect() || !input.hasArray() || !output.hasArray()) {\n+            throw new IllegalArgumentException(\"Non-direct byte buffer backed by byte array required\");\n+        }\n+        int inputOffset = input.arrayOffset() + input.position();\n+        int outputOffset = output.arrayOffset() + output.position();\n+\n+        int written = decompress(input.array(), inputOffset, input.remaining(), output.array(), outputOffset, output.remaining());\n+        ((Buffer) output).position(output.position() + written);\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheConfig.java b/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheConfig.java\nindex b8a3af93004d5..c900ba362f2b9 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheConfig.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheConfig.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import com.facebook.presto.CompressionCodec;\n import io.airlift.units.DataSize;\n import io.airlift.units.Duration;\n import io.airlift.units.MinDataSize;\n@@ -32,7 +33,7 @@ public class FileFragmentResultCacheConfig\n {\n     private boolean cachingEnabled;\n     private URI baseDirectory;\n-    private boolean blockEncodingCompressionEnabled;\n+    private CompressionCodec blockEncodingCompressionCodec = CompressionCodec.NONE;\n \n     private int maxCachedEntries = 10_000;\n     private Duration cacheTtl = new Duration(2, DAYS);\n@@ -68,16 +69,16 @@ public FileFragmentResultCacheConfig setBaseDirectory(URI baseDirectory)\n         return this;\n     }\n \n-    public boolean isBlockEncodingCompressionEnabled()\n+    public CompressionCodec getBlockEncodingCompressionCodec()\n     {\n-        return blockEncodingCompressionEnabled;\n+        return blockEncodingCompressionCodec;\n     }\n \n-    @Config(\"fragment-result-cache.block-encoding-compression-enabled\")\n-    @ConfigDescription(\"Enable compression for block encoding\")\n-    public FileFragmentResultCacheConfig setBlockEncodingCompressionEnabled(boolean blockEncodingCompressionEnabled)\n+    @Config(\"fragment-result-cache.block-encoding-compression-codec\")\n+    @ConfigDescription(\"Compression codec for block encoding\")\n+    public FileFragmentResultCacheConfig setBlockEncodingCompressionCodec(CompressionCodec blockEncodingCompressionCodec)\n     {\n-        this.blockEncodingCompressionEnabled = blockEncodingCompressionEnabled;\n+        this.blockEncodingCompressionCodec = blockEncodingCompressionCodec;\n         return this;\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheManager.java b/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheManager.java\nindex 54a4063a081d0..18a4e7d026d62 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/operator/FileFragmentResultCacheManager.java\n@@ -96,7 +96,7 @@ public FileFragmentResultCacheManager(\n         this.maxSinglePagesBytes = cacheConfig.getMaxSinglePagesSize().toBytes();\n         this.maxCacheBytes = cacheConfig.getMaxCacheSize().toBytes();\n         // pagesSerde is not thread safe\n-        this.pagesSerdeFactory = new PagesSerdeFactory(blockEncodingSerde, cacheConfig.isBlockEncodingCompressionEnabled());\n+        this.pagesSerdeFactory = new PagesSerdeFactory(blockEncodingSerde, cacheConfig.getBlockEncodingCompressionCodec());\n         this.fragmentCacheStats = requireNonNull(fragmentCacheStats, \"fragmentCacheStats is null\");\n         this.flushExecutor = requireNonNull(flushExecutor, \"flushExecutor is null\");\n         this.removalExecutor = requireNonNull(removalExecutor, \"removalExecutor is null\");\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/server/protocol/Query.java b/presto-main-base/src/main/java/com/facebook/presto/server/protocol/Query.java\nindex c944f0cd468b6..d9c398636df14 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/server/protocol/Query.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/server/protocol/Query.java\n@@ -75,11 +75,11 @@\n import java.util.concurrent.ScheduledExecutorService;\n \n import static com.facebook.airlift.concurrent.MoreFutures.addTimeout;\n+import static com.facebook.presto.SystemSessionProperties.getExchangeCompressionCodec;\n import static com.facebook.presto.SystemSessionProperties.getQueryRetryLimit;\n import static com.facebook.presto.SystemSessionProperties.getQueryRetryMaxExecutionTime;\n import static com.facebook.presto.SystemSessionProperties.getTargetResultSize;\n import static com.facebook.presto.SystemSessionProperties.isExchangeChecksumEnabled;\n-import static com.facebook.presto.SystemSessionProperties.isExchangeCompressionEnabled;\n import static com.facebook.presto.SystemSessionProperties.retryQueryWithHistoryBasedOptimizationEnabled;\n import static com.facebook.presto.SystemSessionProperties.trackHistoryBasedPlanStatisticsEnabled;\n import static com.facebook.presto.SystemSessionProperties.useHistoryBasedPlanStatisticsEnabled;\n@@ -231,7 +231,7 @@ private Query(\n         this.resultsProcessorExecutor = resultsProcessorExecutor;\n         this.timeoutExecutor = timeoutExecutor;\n \n-        this.serde = new PagesSerdeFactory(blockEncodingSerde, isExchangeCompressionEnabled(session), isExchangeChecksumEnabled(session)).createPagesSerde();\n+        this.serde = new PagesSerdeFactory(blockEncodingSerde, getExchangeCompressionCodec(session), isExchangeChecksumEnabled(session)).createPagesSerde();\n         this.retryCircuitBreaker = retryCircuitBreaker;\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/spiller/FileSingleStreamSpillerFactory.java b/presto-main-base/src/main/java/com/facebook/presto/spiller/FileSingleStreamSpillerFactory.java\nindex 6640baddc3540..6d685b3cee0cf 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/spiller/FileSingleStreamSpillerFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/spiller/FileSingleStreamSpillerFactory.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.spiller;\n \n import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.block.BlockEncodingSerde;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.execution.buffer.PagesSerdeFactory;\n@@ -79,7 +80,7 @@ public FileSingleStreamSpillerFactory(BlockEncodingSerde blockEncodingSerde, Spi\n                 spillerStats,\n                 requireNonNull(featuresConfig, \"featuresConfig is null\").getSpillerSpillPaths(),\n                 requireNonNull(featuresConfig, \"featuresConfig is null\").getSpillMaxUsedSpaceThreshold(),\n-                requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").isSpillCompressionEnabled(),\n+                requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").getSpillCompressionCodec(),\n                 requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").isSpillEncryptionEnabled());\n     }\n \n@@ -90,10 +91,10 @@ public FileSingleStreamSpillerFactory(\n             SpillerStats spillerStats,\n             List<Path> spillPaths,\n             double maxUsedSpaceThreshold,\n-            boolean spillCompressionEnabled,\n+            CompressionCodec spillCompressionCodec,\n             boolean spillEncryptionEnabled)\n     {\n-        this.serdeFactory = new PagesSerdeFactory(requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\"), spillCompressionEnabled);\n+        this.serdeFactory = new PagesSerdeFactory(requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\"), spillCompressionCodec);\n         this.executor = requireNonNull(executor, \"executor is null\");\n         this.spillerStats = requireNonNull(spillerStats, \"spillerStats can not be null\");\n         requireNonNull(spillPaths, \"spillPaths is null\");\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/spiller/NodeSpillConfig.java b/presto-main-base/src/main/java/com/facebook/presto/spiller/NodeSpillConfig.java\nindex 5b8d2afa7e232..4c7816a428b07 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/spiller/NodeSpillConfig.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/spiller/NodeSpillConfig.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.spiller;\n \n import com.facebook.airlift.configuration.Config;\n+import com.facebook.presto.CompressionCodec;\n import io.airlift.units.DataSize;\n \n import javax.validation.constraints.NotNull;\n@@ -25,7 +26,7 @@ public class NodeSpillConfig\n     private DataSize queryMaxSpillPerNode = new DataSize(100, DataSize.Unit.GIGABYTE);\n     private DataSize tempStorageBufferSize = new DataSize(4, DataSize.Unit.KILOBYTE);\n \n-    private boolean spillCompressionEnabled;\n+    private CompressionCodec spillCompressionCodec = CompressionCodec.NONE;\n     private boolean spillEncryptionEnabled;\n \n     @NotNull\n@@ -67,15 +68,15 @@ public NodeSpillConfig setMaxRevocableMemoryPerNode(DataSize maxRevocableMemoryP\n         return this;\n     }\n \n-    public boolean isSpillCompressionEnabled()\n+    public CompressionCodec getSpillCompressionCodec()\n     {\n-        return spillCompressionEnabled;\n+        return spillCompressionCodec;\n     }\n \n-    @Config(\"experimental.spill-compression-enabled\")\n-    public NodeSpillConfig setSpillCompressionEnabled(boolean spillCompressionEnabled)\n+    @Config(\"experimental.spill-compression-codec\")\n+    public NodeSpillConfig setSpillCompressionCodec(CompressionCodec spillCompressionCodec)\n     {\n-        this.spillCompressionEnabled = spillCompressionEnabled;\n+        this.spillCompressionCodec = spillCompressionCodec;\n         return this;\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageSingleStreamSpillerFactory.java b/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageSingleStreamSpillerFactory.java\nindex 28a7ad350d015..c5eceeb68203e 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageSingleStreamSpillerFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageSingleStreamSpillerFactory.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.block.BlockEncodingSerde;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.execution.buffer.PagesSerdeFactory;\n@@ -61,7 +62,7 @@ public TempStorageSingleStreamSpillerFactory(\n                         daemonThreadsNamed(\"binary-spiller-%s\"))),\n                 blockEncodingSerde,\n                 spillerStats,\n-                requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").isSpillCompressionEnabled(),\n+                requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").getSpillCompressionCodec(),\n                 requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\").isSpillEncryptionEnabled(),\n                 requireNonNull(featuresConfig, \"featuresConfig is null\").getSpillerTempStorage());\n     }\n@@ -72,7 +73,7 @@ public TempStorageSingleStreamSpillerFactory(\n             ListeningExecutorService executor,\n             BlockEncodingSerde blockEncodingSerde,\n             SpillerStats spillerStats,\n-            boolean spillCompressionEnabled,\n+            CompressionCodec spillCompressionEnabled,\n             boolean spillEncryptionEnabled,\n             String tempStorageName)\n     {\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageStandaloneSpillerFactory.java b/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageStandaloneSpillerFactory.java\nindex ad25975e916ad..fc13d4ebf45c0 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageStandaloneSpillerFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/spiller/TempStorageStandaloneSpillerFactory.java\n@@ -48,7 +48,7 @@ public TempStorageStandaloneSpillerFactory(\n         this.tempStorageManager = requireNonNull(tempStorageManager, \"tempStorageManager is null\");\n         requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\");\n         requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\");\n-        this.serdeFactory = new PagesSerdeFactory(blockEncodingSerde, nodeSpillConfig.isSpillCompressionEnabled());\n+        this.serdeFactory = new PagesSerdeFactory(blockEncodingSerde, nodeSpillConfig.getSpillCompressionCodec());\n         this.tempStorageName = requireNonNull(featuresConfig, \"featuresConfig is null\").getSpillerTempStorage();\n         this.spillerStats = requireNonNull(spillerStats, \"spillerStats can not be null\");\n     }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\nindex 8d24e8673e35e..a962fb66004e7 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n@@ -17,6 +17,7 @@\n import com.facebook.airlift.configuration.ConfigDescription;\n import com.facebook.airlift.configuration.DefunctConfig;\n import com.facebook.airlift.configuration.LegacyConfig;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.function.OperatorType;\n import com.facebook.presto.spi.function.FunctionMetadata;\n import com.facebook.presto.sql.tree.CreateView;\n@@ -116,7 +117,7 @@ public class FeaturesConfig\n     private boolean enableIntermediateAggregations;\n     private boolean optimizeCaseExpressionPredicate;\n     private boolean pushTableWriteThroughUnion = true;\n-    private boolean exchangeCompressionEnabled;\n+    private CompressionCodec exchangeCompressionCodec = CompressionCodec.NONE;\n     private boolean exchangeChecksumEnabled;\n     private boolean optimizeMixedDistinctAggregations;\n     private boolean forceSingleNodeOutput = true;\n@@ -1490,9 +1491,9 @@ public FeaturesConfig setOptimizeMixedDistinctAggregations(boolean value)\n         return this;\n     }\n \n-    public boolean isExchangeCompressionEnabled()\n+    public CompressionCodec getExchangeCompressionCodec()\n     {\n-        return exchangeCompressionEnabled;\n+        return exchangeCompressionCodec;\n     }\n \n     public boolean isExchangeChecksumEnabled()\n@@ -1500,10 +1501,10 @@ public boolean isExchangeChecksumEnabled()\n         return exchangeChecksumEnabled;\n     }\n \n-    @Config(\"exchange.compression-enabled\")\n-    public FeaturesConfig setExchangeCompressionEnabled(boolean exchangeCompressionEnabled)\n+    @Config(\"exchange.compression-codec\")\n+    public FeaturesConfig setExchangeCompressionCodec(CompressionCodec exchangeCompressionCodec)\n     {\n-        this.exchangeCompressionEnabled = exchangeCompressionEnabled;\n+        this.exchangeCompressionCodec = exchangeCompressionCodec;\n         return this;\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/HttpRemoteSourceFactory.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/HttpRemoteSourceFactory.java\nindex 4918f14574050..49ca8773217de 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/HttpRemoteSourceFactory.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/HttpRemoteSourceFactory.java\n@@ -27,8 +27,8 @@\n \n import java.util.List;\n \n+import static com.facebook.presto.SystemSessionProperties.getExchangeCompressionCodec;\n import static com.facebook.presto.SystemSessionProperties.isExchangeChecksumEnabled;\n-import static com.facebook.presto.SystemSessionProperties.isExchangeCompressionEnabled;\n import static java.util.Objects.requireNonNull;\n \n public class HttpRemoteSourceFactory\n@@ -52,7 +52,7 @@ public SourceOperatorFactory createRemoteSource(Session session, int operatorId,\n                 operatorId,\n                 planNodeId,\n                 taskExchangeClientManager,\n-                new PagesSerdeFactory(blockEncodingSerde, isExchangeCompressionEnabled(session), isExchangeChecksumEnabled(session)));\n+                new PagesSerdeFactory(blockEncodingSerde, getExchangeCompressionCodec(session), isExchangeChecksumEnabled(session)));\n     }\n \n     @Override\n@@ -69,7 +69,7 @@ public SourceOperatorFactory createMergeRemoteSource(\n                 operatorId,\n                 planNodeId,\n                 taskExchangeClientManager,\n-                new PagesSerdeFactory(blockEncodingSerde, isExchangeCompressionEnabled(session), isExchangeChecksumEnabled(session)),\n+                new PagesSerdeFactory(blockEncodingSerde, getExchangeCompressionCodec(session), isExchangeChecksumEnabled(session)),\n                 orderingCompiler,\n                 types,\n                 outputChannels,\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\nindex 2667c097010be..d3acdd0e9bf50 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/LocalExecutionPlanner.java\n@@ -260,6 +260,7 @@\n import static com.facebook.presto.SystemSessionProperties.getDynamicFilteringMaxPerDriverRowCount;\n import static com.facebook.presto.SystemSessionProperties.getDynamicFilteringMaxPerDriverSize;\n import static com.facebook.presto.SystemSessionProperties.getDynamicFilteringRangeRowLimitPerDriver;\n+import static com.facebook.presto.SystemSessionProperties.getExchangeCompressionCodec;\n import static com.facebook.presto.SystemSessionProperties.getFilterAndProjectMinOutputPageRowCount;\n import static com.facebook.presto.SystemSessionProperties.getFilterAndProjectMinOutputPageSize;\n import static com.facebook.presto.SystemSessionProperties.getIndexLoaderTimeout;\n@@ -269,7 +270,6 @@\n import static com.facebook.presto.SystemSessionProperties.isAdaptivePartialAggregationEnabled;\n import static com.facebook.presto.SystemSessionProperties.isEnableDynamicFiltering;\n import static com.facebook.presto.SystemSessionProperties.isExchangeChecksumEnabled;\n-import static com.facebook.presto.SystemSessionProperties.isExchangeCompressionEnabled;\n import static com.facebook.presto.SystemSessionProperties.isJoinSpillingEnabled;\n import static com.facebook.presto.SystemSessionProperties.isNativeExecutionEnabled;\n import static com.facebook.presto.SystemSessionProperties.isOptimizeCommonSubExpressions;\n@@ -641,7 +641,7 @@ public LocalExecutionPlan plan(\n                                 outputTypes,\n                                 pagePreprocessor,\n                                 outputPartitioning,\n-                                new PagesSerdeFactory(blockEncodingSerde, isExchangeCompressionEnabled(session), isExchangeChecksumEnabled(session))))\n+                                new PagesSerdeFactory(blockEncodingSerde, getExchangeCompressionCodec(session), isExchangeChecksumEnabled(session))))\n                         .build(),\n                 context.getDriverInstanceCount(),\n                 physicalOperation.getPipelineExecutionStrategy(),\n\ndiff --git a/presto-native-execution/presto_cpp/main/QueryContextManager.cpp b/presto-native-execution/presto_cpp/main/QueryContextManager.cpp\nindex b784fce193349..94e946d6cbd90 100644\n--- a/presto-native-execution/presto_cpp/main/QueryContextManager.cpp\n+++ b/presto-native-execution/presto_cpp/main/QueryContextManager.cpp\n@@ -212,19 +212,17 @@ QueryContextManager::toVeloxConfigs(\n       traceFragmentId = it.second;\n     } else if (it.first == SessionProperties::kQueryTraceShardId) {\n       traceShardId = it.second;\n-    } else if (it.first == SessionProperties::kShuffleCompressionEnabled) {\n-      if (it.second == \"true\") {\n-        // NOTE: Presto java only support lz4 compression so configure the same\n-        // compression kind on velox.\n-        configs[core::QueryConfig::kShuffleCompressionKind] =\n-            velox::common::compressionKindToString(\n-                velox::common::CompressionKind_LZ4);\n-      } else {\n-        VELOX_USER_CHECK_EQ(it.second, \"false\");\n-        configs[core::QueryConfig::kShuffleCompressionKind] =\n-            velox::common::compressionKindToString(\n-                velox::common::CompressionKind_NONE);\n-      }\n+    } else if (it.first == SessionProperties::kShuffleCompressionCodec) {\n+      auto compression = it.second;\n+      std::transform(\n+          compression.begin(),\n+          compression.end(),\n+          compression.begin(),\n+          ::tolower);\n+      velox::common::CompressionKind compressionKind =\n+          common::stringToCompressionKind(compression);\n+      configs[core::QueryConfig::kShuffleCompressionKind] =\n+          velox::common::compressionKindToString(compressionKind);\n     } else {\n       configs[sessionProperties_.toVeloxConfig(it.first)] = it.second;\n       sessionProperties_.updateVeloxConfig(it.first, it.second);\n\ndiff --git a/presto-native-execution/presto_cpp/main/SessionProperties.h b/presto-native-execution/presto_cpp/main/SessionProperties.h\nindex 1d9b27dae6b8e..fe1c3b4f4a65a 100644\n--- a/presto-native-execution/presto_cpp/main/SessionProperties.h\n+++ b/presto-native-execution/presto_cpp/main/SessionProperties.h\n@@ -283,9 +283,9 @@ class SessionProperties {\n   static constexpr const char* kPrefixSortMinRows =\n       \"native_prefixsort_min_rows\";\n \n-  /// If true, enable the shuffle compression.\n-  static constexpr const char* kShuffleCompressionEnabled =\n-      \"exchange_compression\";\n+  /// The compression algorithm type to compress the shuffle.\n+  static constexpr const char* kShuffleCompressionCodec =\n+      \"exchange_compression_codec\";\n \n   /// If set to true, enables scaled processing for table scans.\n   static constexpr const char* kTableScanScaledProcessingEnabled =\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/page/PagesSerde.java b/presto-spi/src/main/java/com/facebook/presto/spi/page/PagesSerde.java\nindex 139cf3ecb59ad..8541426d79ec8 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/page/PagesSerde.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/page/PagesSerde.java\n@@ -41,7 +41,7 @@\n @NotThreadSafe\n public class PagesSerde\n {\n-    private static final double MINIMUM_COMPRESSION_RATIO = 0.8;\n+    private static final double MINIMUM_COMPRESSION_RATIO = 0.9;\n \n     private final BlockEncodingSerde blockEncodingSerde;\n     private final Optional<PageCompressor> compressor;\n",
    "test_patch": "diff --git a/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java b/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\nindex 7b626d7908d2a..3548a23665462 100644\n--- a/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\n+++ b/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\n@@ -302,7 +302,7 @@ public void testSession()\n         try (Connection connection = createConnection(\"sessionProperties=query_max_run_time:2d;max_failed_task_percentage:0.6\")) {\n             assertThat(listSession(connection))\n                     .contains(\"join_distribution_type|AUTOMATIC|AUTOMATIC\")\n-                    .contains(\"exchange_compression|false|false\")\n+                    .contains(\"exchange_compression_codec|NONE|NONE\")\n                     .contains(\"query_max_run_time|2d|100.00d\")\n                     .contains(\"max_failed_task_percentage|0.6|0.3\");\n \n@@ -312,15 +312,15 @@ public void testSession()\n \n             assertThat(listSession(connection))\n                     .contains(\"join_distribution_type|BROADCAST|AUTOMATIC\")\n-                    .contains(\"exchange_compression|false|false\");\n+                    .contains(\"exchange_compression_codec|NONE|NONE\");\n \n             try (Statement statement = connection.createStatement()) {\n-                statement.execute(\"SET SESSION exchange_compression = true\");\n+                statement.execute(\"SET SESSION exchange_compression_codec = 'LZ4'\");\n             }\n \n             assertThat(listSession(connection))\n                     .contains(\"join_distribution_type|BROADCAST|AUTOMATIC\")\n-                    .contains(\"exchange_compression|true|false\");\n+                    .contains(\"exchange_compression_codec|LZ4|NONE\");\n         }\n     }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java\nindex 3df15253a5870..2758637601d44 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/TestSqlTaskExecution.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.execution;\n \n import com.facebook.airlift.stats.TestingGcMonitor;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockEncodingManager;\n import com.facebook.presto.common.type.Type;\n@@ -165,7 +166,7 @@ public void testSimple(PipelineExecutionStrategy executionStrategy)\n                     TABLE_SCAN_NODE_ID,\n                     outputBuffer,\n                     Function.identity(),\n-                    new PagesSerdeFactory(new BlockEncodingManager(), false));\n+                    new PagesSerdeFactory(new BlockEncodingManager(), CompressionCodec.NONE));\n             LocalExecutionPlan localExecutionPlan = new LocalExecutionPlan(\n                     ImmutableList.of(new DriverFactory(\n                             0,\n@@ -395,7 +396,7 @@ public void testComplex(PipelineExecutionStrategy executionStrategy)\n                     joinCNodeId,\n                     outputBuffer,\n                     Function.identity(),\n-                    new PagesSerdeFactory(new BlockEncodingManager(), false));\n+                    new PagesSerdeFactory(new BlockEncodingManager(), CompressionCodec.NONE));\n             TestingCrossJoinOperatorFactory joinOperatorFactoryA = new TestingCrossJoinOperatorFactory(2, joinANodeId, buildStatesA);\n             TestingCrossJoinOperatorFactory joinOperatorFactoryB = new TestingCrossJoinOperatorFactory(102, joinBNodeId, buildStatesB);\n             TestingCrossJoinOperatorFactory joinOperatorFactoryC = new TestingCrossJoinOperatorFactory(3, joinCNodeId, buildStatesC);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java b/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java\nindex d78a97821bb41..10bc626aeb67e 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestPagesSerde.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.execution.buffer;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.block.BlockBuilder;\n@@ -21,6 +22,7 @@\n import com.google.common.collect.ImmutableList;\n import io.airlift.slice.DynamicSliceOutput;\n import io.airlift.slice.Slice;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Test;\n \n import java.util.Iterator;\n@@ -39,10 +41,24 @@\n \n public class TestPagesSerde\n {\n-    @Test\n-    public void testRoundTrip()\n+    @DataProvider(name = \"testCompressionCodec\")\n+    public Object[][] createTestCompressionCodec()\n     {\n-        PagesSerde serde = new TestingPagesSerdeFactory().createPagesSerde();\n+        return new Object[][] {\n+                {CompressionCodec.GZIP},\n+                {CompressionCodec.LZ4},\n+                {CompressionCodec.LZO},\n+                {CompressionCodec.SNAPPY},\n+                {CompressionCodec.ZLIB},\n+                {CompressionCodec.ZSTD},\n+                {CompressionCodec.NONE}\n+        };\n+    }\n+\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testRoundTrip(CompressionCodec codec)\n+    {\n+        PagesSerde serde = new TestingPagesSerdeFactory(codec).createPagesSerde();\n         BlockBuilder expectedBlockBuilder = VARCHAR.createBlockBuilder(null, 5);\n         VARCHAR.writeString(expectedBlockBuilder, \"alice\");\n         VARCHAR.writeString(expectedBlockBuilder, \"bob\");\n@@ -63,57 +79,57 @@ public void testRoundTrip()\n         assertFalse(pageIterator.hasNext());\n     }\n \n-    @Test\n-    public void testBigintSerializedSize()\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testBigintSerializedSize(CompressionCodec codec)\n     {\n         BlockBuilder builder = BIGINT.createBlockBuilder(null, 5);\n \n         // empty page\n         Page page = new Page(builder.build());\n-        int pageSize = serializedSize(ImmutableList.of(BIGINT), page);\n+        int pageSize = serializedSize(ImmutableList.of(BIGINT), page, codec);\n         assertEquals(pageSize, 56); // page overhead ideally 35 but since a 0 sized block will be a RLEBlock we have an overhead of 13\n \n         // page with one value\n         BIGINT.writeLong(builder, 123);\n         pageSize = 35; // Now we have moved to the normal block implementation so the page size overhead is 35\n         page = new Page(builder.build());\n-        int firstValueSize = serializedSize(ImmutableList.of(BIGINT), page) - pageSize;\n+        int firstValueSize = serializedSize(ImmutableList.of(BIGINT), page, codec) - pageSize;\n         assertEquals(firstValueSize, 17); // value size + value overhead\n \n         // page with two values\n         BIGINT.writeLong(builder, 456);\n         page = new Page(builder.build());\n-        int secondValueSize = serializedSize(ImmutableList.of(BIGINT), page) - (pageSize + firstValueSize);\n+        int secondValueSize = serializedSize(ImmutableList.of(BIGINT), page, codec) - (pageSize + firstValueSize);\n         assertEquals(secondValueSize, 8); // value size (value overhead is shared with previous value)\n     }\n \n-    @Test\n-    public void testVarcharSerializedSize()\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testVarcharSerializedSize(CompressionCodec codec)\n     {\n         BlockBuilder builder = VARCHAR.createBlockBuilder(null, 5);\n \n         // empty page\n         Page page = new Page(builder.build());\n-        int pageSize = serializedSize(ImmutableList.of(VARCHAR), page);\n+        int pageSize = serializedSize(ImmutableList.of(VARCHAR), page, codec);\n         assertEquals(pageSize, 52); // page overhead\n \n         // page with one value\n         VARCHAR.writeString(builder, \"alice\");\n         page = new Page(builder.build());\n-        int firstValueSize = serializedSize(ImmutableList.of(VARCHAR), page) - pageSize;\n+        int firstValueSize = serializedSize(ImmutableList.of(VARCHAR), page, codec) - pageSize;\n         assertEquals(firstValueSize, 4 + 5); // length + \"alice\"\n \n         // page with two values\n         VARCHAR.writeString(builder, \"bob\");\n         page = new Page(builder.build());\n-        int secondValueSize = serializedSize(ImmutableList.of(VARCHAR), page) - (pageSize + firstValueSize);\n+        int secondValueSize = serializedSize(ImmutableList.of(VARCHAR), page, codec) - (pageSize + firstValueSize);\n         assertEquals(secondValueSize, 4 + 3); // length + \"bob\" (null shared with first entry)\n     }\n \n-    @Test\n-    public void testRoundTripSizeForCompactPageStaysWithinTwentyPercent()\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testRoundTripSizeForCompactPageStaysWithinTwentyPercent(CompressionCodec codec)\n     {\n-        PagesSerde serde = new TestingPagesSerdeFactory().createPagesSerde();\n+        PagesSerde serde = new TestingPagesSerdeFactory(codec).createPagesSerde();\n         BlockBuilder variableWidthBlockBuilder1 = VARCHAR.createBlockBuilder(null, 128);\n         BlockBuilder variableWidthBlockBuilder2 = VARCHAR.createBlockBuilder(null, 256);\n         BlockBuilder bigintBlockBuilder = BIGINT.createBlockBuilder(null, 128);\n@@ -138,9 +154,9 @@ public void testRoundTripSizeForCompactPageStaysWithinTwentyPercent()\n         assertTrue(actualSize < expectedMaxSize, \"Expected round trip size difference less than 20% of original page\");\n     }\n \n-    private static int serializedSize(List<? extends Type> types, Page expectedPage)\n+    private static int serializedSize(List<? extends Type> types, Page expectedPage, CompressionCodec codec)\n     {\n-        PagesSerde serde = new TestingPagesSerdeFactory().createPagesSerde();\n+        PagesSerde serde = new TestingPagesSerdeFactory(codec).createPagesSerde();\n         DynamicSliceOutput sliceOutput = new DynamicSliceOutput(1024);\n         writePages(serde, sliceOutput, expectedPage);\n         Slice slice = sliceOutput.slice();\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java b/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java\nindex 7b889d8682fb6..77cc87ec0b1bd 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/execution/buffer/TestingPagesSerdeFactory.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.execution.buffer;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockEncodingManager;\n import com.facebook.presto.common.block.BlockEncodingSerde;\n@@ -32,10 +33,10 @@\n public class TestingPagesSerdeFactory\n         extends PagesSerdeFactory\n {\n-    public TestingPagesSerdeFactory()\n+    public TestingPagesSerdeFactory(CompressionCodec compressionCodec)\n     {\n         // compression should be enabled in as many tests as possible\n-        super(new BlockEncodingManager(), true);\n+        super(new BlockEncodingManager(), compressionCodec);\n     }\n \n     public static PagesSerde testingPagesSerde()\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java b/presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java\nindex 34a3d2e339c5e..71ff26aa1a7e2 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/memory/TestMemoryPools.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.memory;\n \n import com.facebook.airlift.stats.TestingGcMonitor;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.Session;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.execution.buffer.TestingPagesSerdeFactory;\n@@ -133,7 +134,7 @@ private RevocableMemoryOperator setupConsumeRevocableMemory(DataSize reservedPer\n                     TableScanOperator.class.getSimpleName());\n \n             OutputFactory outputFactory = new PageConsumerOutputFactory(types -> (page -> {}));\n-            Operator outputOperator = outputFactory.createOutputOperator(2, new PlanNodeId(\"output\"), ImmutableList.of(), Function.identity(), Optional.empty(), new TestingPagesSerdeFactory())\n+            Operator outputOperator = outputFactory.createOutputOperator(2, new PlanNodeId(\"output\"), ImmutableList.of(), Function.identity(), Optional.empty(), new TestingPagesSerdeFactory(CompressionCodec.LZ4))\n                     .createOperator(driverContext);\n             RevocableMemoryOperator revocableMemoryOperator = new RevocableMemoryOperator(revokableOperatorContext, reservedPerPage, numberOfPages);\n             createOperator.set(revocableMemoryOperator);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java b/presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java\nindex 3ac3771c1f8cb..d93de79164d2c 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/operator/TestFileFragmentResultCacheConfig.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator;\n \n+import com.facebook.presto.CompressionCodec;\n import com.google.common.collect.ImmutableMap;\n import io.airlift.units.DataSize;\n import io.airlift.units.Duration;\n@@ -36,7 +37,7 @@ public void testDefaults()\n         assertRecordedDefaults(recordDefaults(FileFragmentResultCacheConfig.class)\n                 .setCachingEnabled(false)\n                 .setBaseDirectory(null)\n-                .setBlockEncodingCompressionEnabled(false)\n+                .setBlockEncodingCompressionCodec(CompressionCodec.NONE)\n                 .setMaxCachedEntries(10_000)\n                 .setCacheTtl(new Duration(2, DAYS))\n                 .setMaxInFlightSize(new DataSize(1, GIGABYTE))\n@@ -52,7 +53,7 @@ public void testExplicitPropertyMappings()\n         Map<String, String> properties = new ImmutableMap.Builder<String, String>()\n                 .put(\"fragment-result-cache.enabled\", \"true\")\n                 .put(\"fragment-result-cache.base-directory\", \"tcp://abc\")\n-                .put(\"fragment-result-cache.block-encoding-compression-enabled\", \"true\")\n+                .put(\"fragment-result-cache.block-encoding-compression-codec\", \"LZ4\")\n                 .put(\"fragment-result-cache.max-cached-entries\", \"100000\")\n                 .put(\"fragment-result-cache.cache-ttl\", \"1d\")\n                 .put(\"fragment-result-cache.max-in-flight-size\", \"2GB\")\n@@ -64,7 +65,7 @@ public void testExplicitPropertyMappings()\n         FileFragmentResultCacheConfig expected = new FileFragmentResultCacheConfig()\n                 .setCachingEnabled(true)\n                 .setBaseDirectory(new URI(\"tcp://abc\"))\n-                .setBlockEncodingCompressionEnabled(true)\n+                .setBlockEncodingCompressionCodec(CompressionCodec.LZ4)\n                 .setMaxCachedEntries(100000)\n                 .setCacheTtl(new Duration(1, DAYS))\n                 .setMaxInFlightSize(new DataSize(2, GIGABYTE))\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java b/presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java\nindex bb4b71ecc1789..8110b4139cb61 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/operator/TestPartitionedOutputOperator.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -182,7 +183,7 @@ private static PartitionedOutputOperator createPartitionedOutputOperator(boolean\n                     false,\n                     OptionalInt.empty());\n         }\n-        PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), false);\n+        PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), CompressionCodec.NONE);\n \n         DriverContext driverContext = TestingTaskContext.builder(EXECUTOR, SCHEDULER, TEST_SESSION)\n                 .setMemoryPoolSize(MAX_MEMORY)\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java b/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java\nindex 161824432aabf..f7a268f4a8170 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/BenchmarkPartitionedOutputOperator.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator.repartition;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.Session;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -152,8 +153,8 @@ public static class BenchmarkData\n         private static final ScheduledExecutorService SCHEDULER = newScheduledThreadPool(1, daemonThreadsNamed(\"test-%s\"));\n \n         @SuppressWarnings(\"unused\")\n-        @Param({\"true\", \"false\"})\n-        private boolean enableCompression;\n+        @Param({\"NONE\", \"LZ4\"})\n+        private String codec = \"NONE\";\n \n         @Param({\"1\", \"2\"})\n         private int channelCount = 1;\n@@ -284,6 +285,26 @@ private void createPages(String inputType)\n             types = updateBlockTypesWithHashBlockAndNullBlock(types, true, false);\n         }\n \n+        private CompressionCodec getCompressionCodec(String compressionCodec)\n+        {\n+            switch (compressionCodec) {\n+                case \"NONE\":\n+                    return CompressionCodec.NONE;\n+                case \"SNAPPY\":\n+                    return CompressionCodec.SNAPPY;\n+                case \"LZ4\":\n+                    return CompressionCodec.LZ4;\n+                case \"ZLIB\":\n+                    return CompressionCodec.ZLIB;\n+                case \"GZIP\":\n+                    return CompressionCodec.GZIP;\n+                case \"ZSTD\":\n+                    return CompressionCodec.ZSTD;\n+                default:\n+                    throw new UnsupportedOperationException(\"Unsupported compression codec: \" + compressionCodec);\n+            }\n+        }\n+\n         private PartitionedOutputBuffer createPartitionedOutputBuffer()\n         {\n             OutputBuffers buffers = createInitialEmptyOutputBuffers(PARTITIONED);\n@@ -305,7 +326,7 @@ private OptimizedPartitionedOutputOperator createOptimizedPartitionedOutputOpera\n                     IntStream.range(0, PARTITION_COUNT).toArray());\n             OutputPartitioning outputPartitioning = createOutputPartitioning(partitionFunction);\n \n-            PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), enableCompression);\n+            PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), getCompressionCodec(codec));\n             PartitionedOutputBuffer buffer = createPartitionedOutputBuffer();\n \n             OptimizedPartitionedOutputFactory operatorFactory = new OptimizedPartitionedOutputFactory(buffer, MAX_PARTITION_BUFFER_SIZE);\n@@ -320,7 +341,7 @@ private PartitionedOutputOperator createPartitionedOutputOperator()\n             PartitionFunction partitionFunction = new LocalPartitionGenerator(new PrecomputedHashGenerator(0), PARTITION_COUNT);\n             OutputPartitioning outputPartitioning = createOutputPartitioning(partitionFunction);\n \n-            PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), enableCompression);\n+            PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), getCompressionCodec(codec));\n             PartitionedOutputBuffer buffer = createPartitionedOutputBuffer();\n \n             PartitionedOutputFactory operatorFactory = new PartitionedOutputFactory(buffer, MAX_PARTITION_BUFFER_SIZE);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java b/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java\nindex f495bcea5503d..d2e576d71bdcf 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/operator/repartition/TestOptimizedPartitionedOutputOperator.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator.repartition;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.Session;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.Block;\n@@ -107,7 +108,7 @@ public class TestOptimizedPartitionedOutputOperator\n     private static final ExecutorService EXECUTOR = newCachedThreadPool(daemonThreadsNamed(\"test-EXECUTOR-%s\"));\n     private static final ScheduledExecutorService SCHEDULER = newScheduledThreadPool(1, daemonThreadsNamed(\"test-%s\"));\n     private static final DataSize MAX_MEMORY = new DataSize(1, GIGABYTE);\n-    private static final PagesSerde PAGES_SERDE = new PagesSerdeFactory(new BlockEncodingManager(), false).createPagesSerde(); //testingPagesSerde();\n+    private static final PagesSerde PAGES_SERDE = new PagesSerdeFactory(new BlockEncodingManager(), CompressionCodec.NONE).createPagesSerde(); //testingPagesSerde();\n \n     private static final int PARTITION_COUNT = 16;\n     private static final int PAGE_COUNT = 50;\n@@ -897,7 +898,7 @@ private OptimizedPartitionedOutputOperator createOptimizedPartitionedOutputOpera\n             OptionalInt nullChannel,\n             DataSize maxMemory)\n     {\n-        PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), false);\n+        PagesSerdeFactory serdeFactory = new PagesSerdeFactory(new BlockEncodingManager(), CompressionCodec.NONE);\n \n         OutputPartitioning outputPartitioning = new OutputPartitioning(\n                 partitionFunction,\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java b/presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java\nindex 48eec2458cad5..4189c28623442 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/operator/spiller/BenchmarkBinaryFileSpiller.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.PageBuilder;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -96,8 +97,8 @@ public static class BenchmarkData\n         @Param(\"10\")\n         private int pagesCount = 10;\n \n-        @Param(\"false\")\n-        private boolean compressionEnabled;\n+        @Param(\"NONE\")\n+        private CompressionCodec compressionCodec;\n \n         @Param(\"false\")\n         private boolean encryptionEnabled;\n@@ -118,7 +119,7 @@ public void setup()\n                     spillerStats,\n                     ImmutableList.of(SPILL_PATH),\n                     1.0,\n-                    compressionEnabled,\n+                    compressionCodec,\n                     encryptionEnabled);\n             spillerFactory = new GenericSpillerFactory(singleStreamSpillerFactory);\n             pages = createInputPages();\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java\nindex bd49b216ec3a5..6a7d3396e599f 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestBinaryFileSpiller.java\n@@ -70,7 +70,7 @@ public void setUp()\n         NodeSpillConfig nodeSpillConfig = new NodeSpillConfig();\n         singleStreamSpillerFactory = new FileSingleStreamSpillerFactory(blockEncodingSerde, spillerStats, featuresConfig, nodeSpillConfig);\n         factory = new GenericSpillerFactory(singleStreamSpillerFactory);\n-        PagesSerdeFactory pagesSerdeFactory = new PagesSerdeFactory(requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\"), nodeSpillConfig.isSpillCompressionEnabled());\n+        PagesSerdeFactory pagesSerdeFactory = new PagesSerdeFactory(requireNonNull(blockEncodingSerde, \"blockEncodingSerde is null\"), nodeSpillConfig.getSpillCompressionCodec());\n         pagesSerde = pagesSerdeFactory.createPagesSerde();\n         memoryContext = newSimpleAggregatedMemoryContext();\n     }\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java\nindex e37b40584a14f..554db3969943e 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpiller.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -28,6 +29,7 @@\n import com.google.common.util.concurrent.ListeningExecutorService;\n import io.airlift.slice.InputStreamSliceInput;\n import org.testng.annotations.AfterClass;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Test;\n \n import java.io.File;\n@@ -65,36 +67,35 @@ public void tearDown()\n         deleteRecursively(tempDirectory.toPath(), ALLOW_INSECURE);\n     }\n \n-    @Test\n-    public void testSpill()\n-            throws Exception\n-    {\n-        assertSpill(false, false);\n-    }\n-\n-    @Test\n-    public void testSpillCompression()\n-            throws Exception\n+    @DataProvider(name = \"testCompressionCodec\")\n+    public Object[][] createTestCompressionCodec()\n     {\n-        assertSpill(true, false);\n+        return new Object[][] {\n+                {CompressionCodec.GZIP},\n+                {CompressionCodec.LZ4},\n+                {CompressionCodec.LZO},\n+                {CompressionCodec.SNAPPY},\n+                {CompressionCodec.ZLIB},\n+                {CompressionCodec.ZSTD},\n+                {CompressionCodec.NONE}\n+        };\n     }\n \n-    @Test\n-    public void testSpillEncryption()\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testSpillCompression(CompressionCodec codec)\n             throws Exception\n     {\n-        // Both with compression enabled and disabled\n-        assertSpill(false, true);\n+        assertSpill(codec, false);\n     }\n \n-    @Test\n-    public void testSpillEncryptionWithCompression()\n+    @Test(dataProvider = \"testCompressionCodec\")\n+    public void testSpillEncryptionWithCompression(CompressionCodec codec)\n             throws Exception\n     {\n-        assertSpill(true, true);\n+        assertSpill(codec, true);\n     }\n \n-    private void assertSpill(boolean compression, boolean encryption)\n+    private void assertSpill(CompressionCodec compressionCodec, boolean encryption)\n             throws Exception\n     {\n         File spillPath = new File(tempDirectory, UUID.randomUUID().toString());\n@@ -104,7 +105,7 @@ private void assertSpill(boolean compression, boolean encryption)\n                 new SpillerStats(),\n                 ImmutableList.of(spillPath.toPath()),\n                 1.0,\n-                compression,\n+                compressionCodec,\n                 encryption);\n         LocalMemoryContext memoryContext = newSimpleAggregatedMemoryContext().newLocalMemoryContext(\"test\");\n         SingleStreamSpiller singleStreamSpiller = spillerFactory.create(TYPES, new TestingSpillContext(), memoryContext);\n@@ -124,7 +125,7 @@ private void assertSpill(boolean compression, boolean encryption)\n             Iterator<SerializedPage> serializedPages = PagesSerdeUtil.readSerializedPages(new InputStreamSliceInput(is));\n             assertTrue(serializedPages.hasNext(), \"at least one page should be successfully read back\");\n             byte markers = serializedPages.next().getPageCodecMarkers();\n-            assertEquals(PageCodecMarker.COMPRESSED.isSet(markers), compression);\n+            assertEquals(PageCodecMarker.COMPRESSED.isSet(markers), compressionCodec != CompressionCodec.NONE);\n             assertEquals(PageCodecMarker.ENCRYPTED.isSet(markers), encryption);\n         }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java\nindex d9c6d67d0d652..ce41df715065c 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestFileSingleStreamSpillerFactory.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -83,7 +84,7 @@ public void testDistributesSpillOverPaths()\n                 new SpillerStats(),\n                 spillPaths,\n                 1.0,\n-                false,\n+                CompressionCodec.NONE,\n                 false);\n \n         assertEquals(listFiles(spillPath1.toPath()).size(), 0);\n@@ -123,7 +124,7 @@ public void throwsIfNoDiskSpace()\n                 new SpillerStats(),\n                 spillPaths,\n                 0.0,\n-                false,\n+                CompressionCodec.NONE,\n                 false);\n \n         spillerFactory.create(types, new TestingSpillContext(), newSimpleAggregatedMemoryContext().newLocalMemoryContext(\"test\"));\n@@ -140,7 +141,7 @@ public void throwIfNoSpillPaths()\n                 new SpillerStats(),\n                 spillPaths,\n                 1.0,\n-                false,\n+                CompressionCodec.NONE,\n                 false);\n         spillerFactory.create(types, new TestingSpillContext(), newSimpleAggregatedMemoryContext().newLocalMemoryContext(\"test\"));\n     }\n@@ -170,7 +171,7 @@ public void testCleanupOldSpillFiles()\n                 new SpillerStats(),\n                 spillPaths,\n                 1.0,\n-                false,\n+                CompressionCodec.NONE,\n                 false);\n         spillerFactory.cleanupOldSpillFiles();\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java\nindex 1dc24631a69dd..54c3749d6cc0e 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestNodeSpillConfig.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.spiller;\n \n import com.facebook.airlift.configuration.testing.ConfigAssertions;\n+import com.facebook.presto.CompressionCodec;\n import com.google.common.collect.ImmutableMap;\n import io.airlift.units.DataSize;\n import org.testng.annotations.Test;\n@@ -35,7 +36,7 @@ public void testDefaults()\n                 .setMaxSpillPerNode(new DataSize(100, GIGABYTE))\n                 .setMaxRevocableMemoryPerNode(new DataSize(16, GIGABYTE))\n                 .setQueryMaxSpillPerNode(new DataSize(100, GIGABYTE))\n-                .setSpillCompressionEnabled(false)\n+                .setSpillCompressionCodec(CompressionCodec.NONE)\n                 .setSpillEncryptionEnabled(false)\n                 .setTempStorageBufferSize(new DataSize(4, KILOBYTE)));\n     }\n@@ -47,7 +48,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"experimental.max-spill-per-node\", \"10MB\")\n                 .put(\"experimental.max-revocable-memory-per-node\", \"24MB\")\n                 .put(\"experimental.query-max-spill-per-node\", \"15 MB\")\n-                .put(\"experimental.spill-compression-enabled\", \"true\")\n+                .put(\"experimental.spill-compression-codec\", \"LZ4\")\n                 .put(\"experimental.spill-encryption-enabled\", \"true\")\n                 .put(\"experimental.temp-storage-buffer-size\", \"24MB\")\n                 .build();\n@@ -56,7 +57,7 @@ public void testExplicitPropertyMappings()\n                 .setMaxSpillPerNode(new DataSize(10, MEGABYTE))\n                 .setMaxRevocableMemoryPerNode(new DataSize(24, MEGABYTE))\n                 .setQueryMaxSpillPerNode(new DataSize(15, MEGABYTE))\n-                .setSpillCompressionEnabled(true)\n+                .setSpillCompressionCodec(CompressionCodec.LZ4)\n                 .setSpillEncryptionEnabled(true)\n                 .setTempStorageBufferSize(new DataSize(24, MEGABYTE));\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java\nindex baa38dec89ceb..8767ef6332878 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestSpillCipherPagesSerde.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.type.Type;\n@@ -42,7 +43,7 @@ public class TestSpillCipherPagesSerde\n     public void test()\n     {\n         SpillCipher cipher = new AesSpillCipher();\n-        PagesSerde serde = new TestingPagesSerdeFactory().createPagesSerdeForSpill(Optional.of(cipher));\n+        PagesSerde serde = new TestingPagesSerdeFactory(CompressionCodec.LZ4).createPagesSerdeForSpill(Optional.of(cipher));\n         List<Type> types = ImmutableList.of(VARCHAR);\n         Page emptyPage = new Page(VARCHAR.createBlockBuilder(null, 0).build());\n         assertPageEquals(types, serde.deserialize(serde.serialize(emptyPage)), emptyPage);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java\nindex d01742221503e..9d2ca38240d0e 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/spiller/TestTempStorageSingleStreamSpiller.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spiller;\n \n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.block.BlockEncodingManager;\n@@ -80,14 +81,14 @@ public void tearDown()\n     public void testSpill()\n             throws Exception\n     {\n-        assertSpill(false, false);\n+        assertSpill(CompressionCodec.NONE, false);\n     }\n \n     @Test\n     public void testSpillCompression()\n             throws Exception\n     {\n-        assertSpill(true, false);\n+        assertSpill(CompressionCodec.LZ4, false);\n     }\n \n     @Test\n@@ -95,17 +96,17 @@ public void testSpillEncryption()\n             throws Exception\n     {\n         // Both with compression enabled and disabled\n-        assertSpill(false, true);\n+        assertSpill(CompressionCodec.NONE, true);\n     }\n \n     @Test\n     public void testSpillEncryptionWithCompression()\n             throws Exception\n     {\n-        assertSpill(true, true);\n+        assertSpill(CompressionCodec.LZ4, true);\n     }\n \n-    private void assertSpill(boolean compression, boolean encryption)\n+    private void assertSpill(CompressionCodec compressionCodec, boolean encryption)\n             throws Exception\n     {\n         File spillPath = new File(tempDirectory, UUID.randomUUID().toString());\n@@ -115,7 +116,7 @@ private void assertSpill(boolean compression, boolean encryption)\n                 executor, // executor won't be closed, because we don't call destroy() on the spiller factory\n                 new BlockEncodingManager(),\n                 new SpillerStats(),\n-                compression,\n+                compressionCodec,\n                 encryption,\n                 LocalTempStorage.NAME);\n         LocalMemoryContext memoryContext = newSimpleAggregatedMemoryContext().newLocalMemoryContext(\"test\");\n@@ -153,7 +154,7 @@ private void assertSpill(boolean compression, boolean encryption)\n             Iterator<SerializedPage> serializedPages = PagesSerdeUtil.readSerializedPages(new InputStreamSliceInput(is));\n             assertTrue(serializedPages.hasNext(), \"at least one page should be successfully read back\");\n             byte markers = serializedPages.next().getPageCodecMarkers();\n-            assertEquals(PageCodecMarker.COMPRESSED.isSet(markers), compression);\n+            assertEquals(PageCodecMarker.COMPRESSED.isSet(markers), compressionCodec != CompressionCodec.NONE);\n             assertEquals(PageCodecMarker.ENCRYPTED.isSet(markers), encryption);\n         }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java b/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\nindex 6ebe9120e8a1a..7a3fb541c3ab1 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.configuration.ConfigurationFactory;\n import com.facebook.airlift.configuration.testing.ConfigAssertions;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.sql.analyzer.FeaturesConfig.AggregationIfToFilterRewriteStrategy;\n import com.facebook.presto.sql.analyzer.FeaturesConfig.CteMaterializationStrategy;\n import com.facebook.presto.sql.analyzer.FeaturesConfig.JoinDistributionType;\n@@ -126,7 +127,7 @@ public void testDefaults()\n                 .setIgnoreStatsCalculatorFailures(true)\n                 .setPrintStatsForNonJoinQuery(false)\n                 .setDefaultFilterFactorEnabled(false)\n-                .setExchangeCompressionEnabled(false)\n+                .setExchangeCompressionCodec(CompressionCodec.NONE)\n                 .setExchangeChecksumEnabled(false)\n                 .setEnableIntermediateAggregations(false)\n                 .setPushAggregationThroughJoin(true)\n@@ -330,7 +331,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"experimental.spiller.single-stream-spiller-choice\", \"TEMP_STORAGE\")\n                 .put(\"experimental.spiller.spiller-temp-storage\", \"crail\")\n                 .put(\"experimental.spiller.max-revocable-task-memory\", \"1GB\")\n-                .put(\"exchange.compression-enabled\", \"true\")\n+                .put(\"exchange.compression-codec\", \"LZ4\")\n                 .put(\"exchange.checksum-enabled\", \"true\")\n                 .put(\"optimizer.enable-intermediate-aggregations\", \"true\")\n                 .put(\"optimizer.force-single-node-output\", \"false\")\n@@ -529,7 +530,7 @@ public void testExplicitPropertyMappings()\n                 .setSingleStreamSpillerChoice(SingleStreamSpillerChoice.TEMP_STORAGE)\n                 .setSpillerTempStorage(\"crail\")\n                 .setMaxRevocableMemoryPerTask(new DataSize(1, GIGABYTE))\n-                .setExchangeCompressionEnabled(true)\n+                .setExchangeCompressionCodec(CompressionCodec.LZ4)\n                 .setExchangeChecksumEnabled(true)\n                 .setEnableIntermediateAggregations(true)\n                 .setForceSingleNodeOutput(false)\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java b/presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java\nindex 74948580abe86..e17171967246f 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/TestExchangeOperator.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.http.client.HttpClient;\n import com.facebook.airlift.http.client.testing.TestingHttpClient;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.execution.Location;\n@@ -63,7 +64,7 @@\n public class TestExchangeOperator\n {\n     private static final List<Type> TYPES = ImmutableList.of(VARCHAR);\n-    private static final PagesSerdeFactory SERDE_FACTORY = new TestingPagesSerdeFactory();\n+    private static final PagesSerdeFactory SERDE_FACTORY = new TestingPagesSerdeFactory(CompressionCodec.LZ4);\n \n     private static final String TASK_1_ID = \"task1.0.0.0.0\";\n     private static final String TASK_2_ID = \"task2.0.0.0.0\";\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java b/presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java\nindex edf4d1ad958bb..291f4cb5af3e9 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/TestMergeOperator.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.http.client.HttpClient;\n import com.facebook.airlift.http.client.testing.TestingHttpClient;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.SortOrder;\n import com.facebook.presto.common.type.Type;\n@@ -80,7 +81,7 @@ public class TestMergeOperator\n     public void setUp()\n     {\n         executor = newSingleThreadScheduledExecutor(daemonThreadsNamed(\"test-merge-operator-%s\"));\n-        serdeFactory = new TestingPagesSerdeFactory();\n+        serdeFactory = new TestingPagesSerdeFactory(CompressionCodec.LZ4);\n \n         taskBuffers = CacheBuilder.newBuilder().build(CacheLoader.from(TestingTaskBuffer::new));\n         httpClient = new TestingHttpClient(new TestingExchangeHttpClientHandler(taskBuffers), executor);\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/server/TestServer.java b/presto-main/src/test/java/com/facebook/presto/server/TestServer.java\nindex 020c90f844663..c2ccda8fd46b5 100644\n--- a/presto-main/src/test/java/com/facebook/presto/server/TestServer.java\n+++ b/presto-main/src/test/java/com/facebook/presto/server/TestServer.java\n@@ -22,6 +22,7 @@\n import com.facebook.airlift.http.client.jetty.JettyHttpClient;\n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.airlift.testing.Closeables;\n+import com.facebook.presto.CompressionCodec;\n import com.facebook.presto.client.QueryError;\n import com.facebook.presto.client.QueryResults;\n import com.facebook.presto.common.Page;\n@@ -193,7 +194,7 @@ public void testBinaryResults()\n         byte[] decodedPage = Base64.getDecoder().decode((String) encodedPages.get(0));\n \n         BlockEncodingManager blockEncodingSerde = new BlockEncodingManager();\n-        PagesSerde pagesSerde = new PagesSerdeFactory(blockEncodingSerde, false, false).createPagesSerde();\n+        PagesSerde pagesSerde = new PagesSerdeFactory(blockEncodingSerde, CompressionCodec.NONE, false).createPagesSerde();\n         BasicSliceInput pageInput = new BasicSliceInput(Slices.wrappedBuffer(decodedPage, 0, decodedPage.length));\n         SerializedPage serializedPage = readSerializedPage(pageInput);\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24655",
    "pr_id": 24655,
    "issue_id": 24908,
    "repo": "prestodb/presto",
    "problem_statement": "[sidecar] Array_sort lambda function failure when sidecar is enabled\n`SELECT array_sort(quantities, (x, y) -> if (x < y, 1, if (x > y, -1, 0))) FROM orders_ex `\n\nfails with `line 1:31: Expected a lambda that takes 1 argument\\\\(s\\\\) but got 2`",
    "issue_word_count": 41,
    "test_files_count": 1,
    "non_test_files_count": 4,
    "pr_changed_files": [
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java",
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticException.java",
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SignatureMatchingException.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionSignatureMatcher.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java"
    ],
    "pr_changed_test_files": [
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java"
    ],
    "base_commit": "dda99ba09bfb14e16db200e0afe312e13c413b8b",
    "head_commit": "6028148848559adef9a85099680818d67fcec144",
    "repo_url": "https://github.com/prestodb/presto/pull/24655",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24655",
    "dockerfile": "",
    "pr_merged_at": "2025-05-13T19:51:38.000Z",
    "patch": "diff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\nindex d5492e6bb6932..656b6dbaab079 100644\n--- a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\n@@ -56,6 +56,7 @@ public enum SemanticErrorCode\n     FUNCTION_NOT_FOUND,\n     INVALID_FUNCTION_NAME,\n     DUPLICATE_PARAMETER_NAME,\n+    EXCEPTIONS_WHEN_RESOLVING_FUNCTIONS,\n \n     ORDER_BY_MUST_BE_IN_SELECT,\n     ORDER_BY_MUST_BE_IN_AGGREGATE,\n\ndiff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticException.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticException.java\nindex b6d0f284cd3b8..36cc475efa968 100644\n--- a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticException.java\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticException.java\n@@ -26,6 +26,7 @@ public class SemanticException\n {\n     private final SemanticErrorCode code;\n     private final Optional<NodeLocation> location;\n+    private final String formattedMessage;\n \n     public SemanticException(SemanticErrorCode code, String format, Object... args)\n     {\n@@ -44,10 +45,18 @@ public SemanticException(SemanticErrorCode code, Optional<NodeLocation> location\n \n     public SemanticException(SemanticErrorCode code, Throwable cause, Optional<NodeLocation> location, String format, Object... args)\n     {\n-        super(formatMessage(format, location, args), cause);\n+        super(cause);\n \n         this.code = requireNonNull(code, \"code is null\");\n         this.location = requireNonNull(location, \"location is null\");\n+        requireNonNull(format, \"format is null\");\n+        this.formattedMessage = formatMessage(format, location, args);\n+    }\n+\n+    @Override\n+    public String getMessage()\n+    {\n+        return formattedMessage;\n     }\n \n     // TODO: Should be replaced with analyzer agnostic location\n\ndiff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SignatureMatchingException.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SignatureMatchingException.java\nnew file mode 100644\nindex 0000000000000..7bc895548ceb2\n--- /dev/null\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SignatureMatchingException.java\n@@ -0,0 +1,39 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.analyzer;\n+\n+import java.util.List;\n+\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.EXCEPTIONS_WHEN_RESOLVING_FUNCTIONS;\n+import static java.lang.String.format;\n+\n+public class SignatureMatchingException\n+        extends SemanticException\n+{\n+    public SignatureMatchingException(\n+            String prefix,\n+            List<SemanticException> failedExceptions)\n+    {\n+        super(EXCEPTIONS_WHEN_RESOLVING_FUNCTIONS, formatMessage(prefix, failedExceptions));\n+    }\n+\n+    private static String formatMessage(String formatString, List<SemanticException> failedExceptions)\n+    {\n+        StringBuilder sb = new StringBuilder(formatString).append(\"\\n\");\n+        for (int i = 0; i < failedExceptions.size(); i++) {\n+            sb.append(format(\" Exception %d: %s%n\", i + 1, failedExceptions.get(i).getMessage()));\n+        }\n+        return sb.toString();\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionSignatureMatcher.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionSignatureMatcher.java\nindex 7c71047a3b107..2ac1ee920a13e 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionSignatureMatcher.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/FunctionSignatureMatcher.java\n@@ -19,6 +19,8 @@\n import com.facebook.presto.spi.function.FunctionKind;\n import com.facebook.presto.spi.function.Signature;\n import com.facebook.presto.spi.function.SqlFunction;\n+import com.facebook.presto.sql.analyzer.SemanticException;\n+import com.facebook.presto.sql.analyzer.SignatureMatchingException;\n import com.facebook.presto.sql.analyzer.TypeSignatureProvider;\n import com.google.common.base.Joiner;\n import com.google.common.collect.ImmutableList;\n@@ -124,15 +126,28 @@ private Optional<Signature> matchFunction(Collection<? extends SqlFunction> cand\n     private List<ApplicableFunction> identifyApplicableFunctions(Collection<? extends SqlFunction> candidates, List<TypeSignatureProvider> actualParameters, boolean allowCoercion)\n     {\n         ImmutableList.Builder<ApplicableFunction> applicableFunctions = ImmutableList.builder();\n+        ImmutableList.Builder<SemanticException> semanticExceptions = ImmutableList.builder();\n         for (SqlFunction function : candidates) {\n             Signature declaredSignature = function.getSignature();\n-            Optional<Signature> boundSignature = new SignatureBinder(functionAndTypeManager, declaredSignature, allowCoercion)\n-                    .bind(actualParameters);\n-            if (boundSignature.isPresent()) {\n-                applicableFunctions.add(new ApplicableFunction(declaredSignature, boundSignature.get(), function.isCalledOnNullInput()));\n+            try {\n+                Optional<Signature> boundSignature = new SignatureBinder(functionAndTypeManager, declaredSignature, allowCoercion)\n+                        .bind(actualParameters);\n+                boundSignature.ifPresent(signature -> applicableFunctions.add(new ApplicableFunction(declaredSignature, signature, function.isCalledOnNullInput())));\n             }\n+            catch (SemanticException e) {\n+                semanticExceptions.add(e);\n+            }\n+        }\n+\n+        List<ApplicableFunction> applicableFunctionsList = applicableFunctions.build();\n+        List<SemanticException> semanticExceptionList = semanticExceptions.build();\n+        if (applicableFunctionsList.isEmpty() && !semanticExceptionList.isEmpty()) {\n+            decideAndThrow(semanticExceptionList,\n+                    candidates.stream().findFirst()\n+                            .map(function -> function.getSignature().getName().getObjectName())\n+                            .orElse(\"\"));\n         }\n-        return applicableFunctions.build();\n+        return applicableFunctionsList;\n     }\n \n     private List<ApplicableFunction> selectMostSpecificFunctions(List<ApplicableFunction> applicableFunctions, List<TypeSignatureProvider> parameters)\n@@ -287,6 +302,22 @@ private static boolean returnsNullOnGivenInputTypes(ApplicableFunction applicabl\n         return true;\n     }\n \n+    /**\n+     * Decides which exception to throw based on the number of failed attempts.\n+     * If there's only one SemanticException, it throws that SemanticException directly.\n+     * If there are multiple SemanticExceptions, it throws the SignatureMatchingException.\n+     */\n+    private static void decideAndThrow(List<SemanticException> failedExceptions, String functionName)\n+            throws SemanticException\n+    {\n+        if (failedExceptions.size() == 1) {\n+            throw failedExceptions.get(0);\n+        }\n+        else {\n+            throw new SignatureMatchingException(format(\"Failed to find matching function signature for %s, matching failures: \", functionName), failedExceptions);\n+        }\n+    }\n+\n     static String constructFunctionNotFoundErrorMessage(QualifiedObjectName functionName, List<TypeSignatureProvider> parameterTypes, Collection<? extends SqlFunction> candidates)\n     {\n         String name = toConciseFunctionName(functionName);\n",
    "test_patch": "diff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\nindex 75e5bb13104fd..7f97726169ae4 100644\n--- a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n@@ -188,10 +188,25 @@ public void testWindowFunctions()\n     }\n \n     @Test\n-    public void testArraySort()\n+    public void testLambdaFunctions()\n     {\n-        assertQueryFails(\"SELECT array_sort(quantities, (x, y) -> if (x < y, 1, if (x > y, -1, 0))) FROM orders_ex\",\n-                \"line 1:31: Expected a lambda that takes 1 argument\\\\(s\\\\) but got 2\");\n+        // These function signatures are only supported in the native execution engine\n+        assertQuerySucceeds(\"select array_sort(array[row('apples', 23), row('bananas', 12), row('grapes', 44)], x -> x[2])\");\n+        assertQuerySucceeds(\"SELECT array_sort(quantities, x -> abs(x)) FROM orders_ex\");\n+        assertQuerySucceeds(\"SELECT array_sort(quantities, (x, y) -> if (x < y, cast(1 as bigint), if (x > y, cast(-1 as bigint), cast(0 as bigint)))) FROM orders_ex\");\n+\n+        assertQuery(\"SELECT array_sort(map_keys(map_union(quantity_by_linenumber))) FROM orders_ex\");\n+        assertQuery(\"SELECT filter(quantities, q -> q > 10) FROM orders_ex\");\n+        assertQuery(\"SELECT all_match(shuffle(quantities), x -> (x > 500.0)) FROM orders_ex\");\n+        assertQuery(\"SELECT any_match(quantities, x -> TRY(((10 / x) > 2))) FROM orders_ex\");\n+        assertQuery(\"SELECT TRY(none_match(quantities, x -> ((10 / x) > 2))) FROM orders_ex\");\n+        assertQuery(\"SELECT reduce(array[nationkey, regionkey], 103, (s, x) -> s + x, s -> s) FROM nation\");\n+        assertQuery(\"SELECT transform(array[1, 2, 3], x -> x * regionkey + nationkey) FROM nation\");\n+        assertQueryFails(\n+                \"SELECT array_sort(quantities, (x, y, z) -> if (x < y + z, cast(1 as bigint), if (x > y + z, cast(-1 as bigint), cast(0 as bigint)))) FROM orders_ex\",\n+                Pattern.quote(\"Failed to find matching function signature for array_sort, matching failures: \\n\" +\n+                        \" Exception 1: line 1:31: Expected a lambda that takes 1 argument(s) but got 3\\n\" +\n+                        \" Exception 2: line 1:31: Expected a lambda that takes 2 argument(s) but got 3\\n\"));\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24617",
    "pr_id": 24617,
    "issue_id": 24087,
    "repo": "prestodb/presto",
    "problem_statement": "Interval values can overflow\nThis produces the correct value:\r\n\r\n```\r\npresto> select interval '1' month * 2147483647;\r\n    _col0    \r\n-------------\r\n 178956970-7 \r\n(1 row)\r\n```\r\n\r\nThe next higher number will overflow:\r\n\r\n```\r\npresto> select interval '1' month * 2147483648;\r\n    _col0     \r\n--------------\r\n -178956970-8 \r\n(1 row)\r\n```\r\n\r\nNone of the [interval year to month operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java) for division, multiplication, subtraction and addition check boundaries, so this problem is not specific to multiplication.  Also, similar boundary checks should be added for multiplication by double.\r\n\r\nLikewise, the [interval day time operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java) will also overflow and need the same checks, adjusted to work for the long type that backs [IntervalDayTimeType](https://github.com/prestodb/presto/blob/150c7ebdd6a7e4b73725150587508925d58c6ad9/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeType.java#L23).\r\n\r\n## Your Environment\r\nThis is true for any functional environment of Presto.\r\n\r\n## Expected Behavior\r\nIt should be an error to exceed the boundary of the interval type.  The underlying datastructure for interval year to month is an integer, so we should raise an error for any value that exceed's Java integer range.\r\n\r\n## Current Behavior\r\nThe interval will silently overflow and return incorrect results.\r\n\r\n## Possible Solution\r\nModify the interval operators to do proper overflow checking, as is done for e.g. [`INTEGER` operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntegerOperators.java#L64-L134).\r\n\r\n## Steps to Reproduce\r\nSee the above queries for a simple reproduction.\r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nThis was discovered while converting constant folding of expressions to use Velox for their evaluation.\r\n\r\nThis was previously reported in #9342.\r\n",
    "issue_word_count": 289,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java"
    ],
    "base_commit": "cff865e1f75d42a23addc3e982490eab720ff971",
    "head_commit": "cf87eaf6ad9572807455534ae60a6039592f7637",
    "repo_url": "https://github.com/prestodb/presto/pull/24617",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24617",
    "dockerfile": "",
    "pr_merged_at": "2025-03-18T18:58:54.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java b/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java\nindex 7dbd1b197a6ff..ba03ace1645e9 100644\n--- a/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java\n+++ b/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java\n@@ -17,6 +17,7 @@\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.type.AbstractIntType;\n import com.facebook.presto.common.type.StandardTypes;\n+import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.function.BlockIndex;\n import com.facebook.presto.spi.function.BlockPosition;\n import com.facebook.presto.spi.function.IsNull;\n@@ -42,6 +43,7 @@\n import static com.facebook.presto.common.function.OperatorType.NEGATION;\n import static com.facebook.presto.common.function.OperatorType.NOT_EQUAL;\n import static com.facebook.presto.common.function.OperatorType.SUBTRACT;\n+import static com.facebook.presto.spi.StandardErrorCode.NUMERIC_VALUE_OUT_OF_RANGE;\n import static com.facebook.presto.type.IntervalYearMonthType.INTERVAL_YEAR_MONTH;\n import static io.airlift.slice.Slices.utf8Slice;\n import static java.lang.Math.toIntExact;\n@@ -56,49 +58,81 @@ private IntervalYearMonthOperators()\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n     public static long add(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long right)\n     {\n-        return left + right;\n+        try {\n+            return Math.addExact((int) left, (int) right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow adding interval year-month values: \" + left + \" + \" + right);\n+        }\n     }\n \n     @ScalarOperator(SUBTRACT)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n     public static long subtract(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long right)\n     {\n-        return left - right;\n+        try {\n+            return Math.subtractExact((int) left, (int) right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow subtracting interval year-month values: \" + left + \" - \" + right);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n-    public static long multiplyByBigint(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.BIGINT) long right)\n+    public static long multiplyByInteger(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.INTEGER) long right)\n     {\n-        return left * right;\n+        try {\n+            return Math.multiplyExact((int) left, (int) right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow multiplying interval year-month value by integer: \" + left + \" * \" + right);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n     public static long multiplyByDouble(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.DOUBLE) double right)\n     {\n-        return (long) (left * right);\n+        long result = (long) (left * right);\n+        if (result < Integer.MIN_VALUE || result > Integer.MAX_VALUE) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow multiplying interval year-month value by double: \" + left + \" * \" + right);\n+        }\n+        return result;\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n-    public static long bigintMultiply(@SqlType(StandardTypes.BIGINT) long left, @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long right)\n+    public static long integerMultiply(@SqlType(StandardTypes.INTEGER) long left, @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long right)\n     {\n-        return left * right;\n+        try {\n+            return Math.multiplyExact((int) left, (int) right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow multiplying integer by interval year-month value: \" + left + \" * \" + right);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n     public static long doubleMultiply(@SqlType(StandardTypes.DOUBLE) double left, @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long right)\n     {\n-        return (long) (left * right);\n+        long result = (long) (left * right);\n+        if (result < Integer.MIN_VALUE || result > Integer.MAX_VALUE) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow multiplying double by interval year-month value: \" + left + \" * \" + right);\n+        }\n+        return result;\n     }\n \n     @ScalarOperator(DIVIDE)\n     @SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH)\n     public static long divideByDouble(@SqlType(StandardTypes.INTERVAL_YEAR_TO_MONTH) long left, @SqlType(StandardTypes.DOUBLE) double right)\n     {\n-        return (long) (left / right);\n+        long result = (long) (left / right);\n+        if (result < Integer.MIN_VALUE || result > Integer.MAX_VALUE) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"Overflow dividing interval year-month value by double: \" + left + \" / \" + right);\n+        }\n+        return result;\n     }\n \n     @ScalarOperator(NEGATION)\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java b/presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java\nindex 66c551519dd5f..2e2f80db353de 100644\n--- a/presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java\n+++ b/presto-main/src/test/java/com/facebook/presto/type/TestIntervalYearMonth.java\n@@ -28,6 +28,7 @@ public class TestIntervalYearMonth\n         extends AbstractTestFunctions\n {\n     private static final int MAX_SHORT = Short.MAX_VALUE;\n+    private static final long MAX_INT_PLUS_1 = Integer.MAX_VALUE + 1L;\n \n     @Test\n     public void testObject()\n@@ -74,6 +75,7 @@ public void testInvalidLiteral()\n         assertInvalidFunction(\"INTERVAL '124-X' YEAR TO MONTH\", \"Invalid INTERVAL YEAR TO MONTH value: 124-X\");\n         assertInvalidFunction(\"INTERVAL '124--30' YEAR TO MONTH\", \"Invalid INTERVAL YEAR TO MONTH value: 124--30\");\n         assertInvalidFunction(\"INTERVAL '--124--30' YEAR TO MONTH\", \"Invalid INTERVAL YEAR TO MONTH value: --124--30\");\n+        assertInvalidFunction(format(\"INTERVAL '%s' MONTH\", MAX_INT_PLUS_1), \"Invalid INTERVAL MONTH value: \" + MAX_INT_PLUS_1);\n     }\n \n     @Test\n@@ -82,6 +84,7 @@ public void testAdd()\n         assertFunction(\"INTERVAL '3' MONTH + INTERVAL '3' MONTH\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(6));\n         assertFunction(\"INTERVAL '6' YEAR + INTERVAL '6' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(12 * 12));\n         assertFunction(\"INTERVAL '3' MONTH + INTERVAL '6' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth((6 * 12) + (3)));\n+        assertNumericOverflow(format(\"INTERVAL '%s' MONTH + INTERVAL '1' MONTH\", Integer.MAX_VALUE), format(\"Overflow adding interval year-month values: %s + 1\", Integer.MAX_VALUE));\n     }\n \n     @Test\n@@ -90,6 +93,7 @@ public void testSubtract()\n         assertFunction(\"INTERVAL '6' MONTH - INTERVAL '3' MONTH\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(3));\n         assertFunction(\"INTERVAL '9' YEAR - INTERVAL '6' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(3 * 12));\n         assertFunction(\"INTERVAL '3' MONTH - INTERVAL '6' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth((3) - (6 * 12)));\n+        assertNumericOverflow(format(\"-INTERVAL '%s' MONTH - INTERVAL '2' MONTH\", Integer.MAX_VALUE), format(\"Overflow subtracting interval year-month values: -%s - 2\", Integer.MAX_VALUE));\n     }\n \n     @Test\n@@ -104,6 +108,13 @@ public void testMultiply()\n         assertFunction(\"2 * INTERVAL '6' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(12 * 12));\n         assertFunction(\"INTERVAL '1' YEAR * 2.5\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth((int) (2.5 * 12)));\n         assertFunction(\"2.5 * INTERVAL '1' YEAR\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth((int) (2.5 * 12)));\n+\n+        assertNumericOverflow(format(\"INTERVAL '%s' MONTH * 2\", Integer.MAX_VALUE), format(\"Overflow multiplying interval year-month value by integer: %s * 2\", Integer.MAX_VALUE));\n+        assertNumericOverflow(format(\"2 * INTERVAL '%s' MONTH\", Integer.MAX_VALUE), format(\"Overflow multiplying integer by interval year-month value: 2 * %s\", Integer.MAX_VALUE));\n+        assertNumericOverflow(format(\"INTERVAL '%s' MONTH * 2.0\", Integer.MAX_VALUE), format(\"Overflow multiplying interval year-month value by double: %s * 2.0\", Integer.MAX_VALUE));\n+        assertNumericOverflow(format(\"DOUBLE '2' * INTERVAL '%s' MONTH\", Integer.MAX_VALUE), format(\"Overflow multiplying double by interval year-month value: 2.0 * %s\", Integer.MAX_VALUE));\n+        assertNumericOverflow(format(\"INTERVAL '2' YEAR * %s\", (long) Integer.MAX_VALUE + 1), format(\"Overflow multiplying interval year-month value by double: 24 * %s\", (double) ((long) Integer.MAX_VALUE + 1)));\n+        assertNumericOverflow(format(\"%s * INTERVAL '2' YEAR\", (long) Integer.MAX_VALUE + 1), format(\"Overflow multiplying double by interval year-month value: %s * 24\", (double) ((long) Integer.MAX_VALUE + 1)));\n     }\n \n     @Test\n@@ -114,6 +125,8 @@ public void testDivide()\n \n         assertFunction(\"INTERVAL '3' YEAR / 2\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(18));\n         assertFunction(\"INTERVAL '4' YEAR / 4.8\", INTERVAL_YEAR_MONTH, new SqlIntervalYearMonth(10));\n+\n+        assertNumericOverflow(format(\"INTERVAL '%s' MONTH / 0.5\", Integer.MAX_VALUE), format(\"Overflow dividing interval year-month value by double: %s / 0.5\", Integer.MAX_VALUE));\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24614",
    "pr_id": 24614,
    "issue_id": 24563,
    "repo": "prestodb/presto",
    "problem_statement": "Inconsistent behavior in json functions\nWhile working on implementing JSON functions in Velox for Presto-CPP, I came across some inconsistencies that I wanted to highlight and get the community's opinion on whether it makes sense for us to fix them, either both in Presto-Java and Presto-CPP or just in Presto-CPP.\n\n### 1. `json_extract` Produces Non-Canonicalized JSON as Output\n\nWe expect the JSON type to be canonicalized whenever it exists in a query. Since Presto does not store JSON as a type that can be written to files, this canonicalization is usually done when a VARCHAR column is converted to JSON using one of the JSON functions (typically `json_parse`). This is important because it can affect the equality of two JSON objects and provide inconsistent results if sometimes the JSONs are canonicalized and sometimes they are not.\n\nFor example, suppose the input string is:\n\n```json\n{ \"key_2\": 2, \"key_3\": 3, \"key_1\": 1 }\n```\n\n`json_parse` will canonicalize this to:\n\n```sql\npresto:default> select JSON_PARSE('{ \"key_2\": 2, \"key_3\": 3, \"key_1\": 1 }');\n_col0\n---------------------------------\n{\"key_1\":1,\"key_2\":2,\"key_3\":3}\n```\n\n**NOTE**: canonicalization puts the keys in ascending order and removes spaces.\n\nNow, if there are two inputs:\n\n```json\n{ \"key_2\": 2, \"key_3\": 3, \"key_1\": 1 }\n{ \"key_1\": 1, \"key_2\": 2, \"key_3\": 3 }\n```\n\nThese should be considered equal JSONs when parsed and compared; otherwise, this can result in correctness issues. If we use `json_extract`, which returns a JSON object type, it would extract and return a non-canonicalized JSON:\n\n```sql\npresto:default> select JSON_EXTRACT('{ \"key_2\": 2, \"key_3\": 3, \"key_1\": 1 }', '$');\n_col0\n---------------------------------\n{\"key_2\":2,\"key_3\":3,\"key_1\":1}\n```\n\n**NOTE**: Spaces are removed, but keys remain untouched.\n\nThe same issue occurs in `json_array_get`:\n\n```sql\npresto:default> select json_array_get('[{ \"key_2\": 2, \"key_1\": 1 }]', 0);\n_col0\n-----------------------\n{\"key_2\":2,\"key_1\":1}\n```\n\nExample of a possible correctness issue:\nA group by on the json column returning wrong results\n```\nSELECT\n    (\"json_extract\"((output_fields), '$.key')) c0\nFROM (\n    SELECT\n        *\n    FROM (\n        VALUES\n            ('{\"key\": {\"inner_1\":\"1\", \"inner_2\":\"2\"}'),\n            ('{\"key\": {\"inner_2\":\"2\", \"inner_1\":\"1\"}}')\n    ) AS t (output_fields)\n)\nGROUP BY\n    1;\n\n              c0               \n-------------------------------\n {\"inner_1\":\"1\",\"inner_2\":\"2\"} \n {\"inner_2\":\"2\",\"inner_1\":\"1\"} \n```\n\n**Recommendation**: The expectation is that once a VARCHAR is converted to JSON, it should be in a canonicalized form. Therefore, we should always canonicalize the resultant JSON in all functions that take VARCHAR as prospective JSON input and output a JSON.\n\n### 2. `json_extract` / `_scalar`, `json_array_get` Accepts Invalid JSON to Produce Valid Result\n\nIf the JSON is invalid, it will still produce valid output if and only if the target key/index specified in the path comes before the part where the JSON becomes invalid.\n\n```sql\npresto:default> select JSON_EXTRACT('{ \"key_2\": 2, \"key_1\": \"z\"a1\" }', '$.key_2');\n_col0\n-------\n2\npresto:default> select JSON_EXTRACT('{ \"key_2\": 2, \"key_1\": \"z\"a1\" }', '$.key_1');\n_col0\n-------\n\"z\"\n```\n\n**NOTE**: `\"z\"a1\"` is the invalid part. If you `json_parse` the above string, it will throw an error.\n\nThe same issue occurs with `json_array_get`:\n\n```sql\npresto:default> select json_array_get('[{ \"key_2\": 2, \"key_1\": 1 }, {\"z\"aa\"}]', 0);\n_col0\n-----------------------\n{\"key_2\":2,\"key_1\":1}\n```\n\n**Why is this a problem?**\n- This makes the behavior unpredictable in the face of invalid JSON.\n- Since `json_extract` does not canonicalize the input, this can return a valid result in one input and invalid in another, even if the two inputs are considered equal when canonicalized.\n- Moreover, `json_extract` seems to have inconsistency depending on the path. For example, Jayway treats `$.` as optional in the beginning, therefore `$.key_2` should be the same as `key_2`. However, as we'll see, it returns different values when the JSON is invalid.\n\n```sql\npresto:default> select JSON_EXTRACT('{ \"key_2\": 2, \"key_1\": \"z\"a1\" }', '$.key_2');\n_col0\n-------\n2\npresto:default> select JSON_EXTRACT('{ \"key_2\": 2, \"key_1\": \"z\"a1\" }', 'key_2');\n_col0\n-------\nNULL\n```\n\n**NOTE**: Path with `$.key_2` returns a value, but `key_2` returns NULL.\n\n**Recommendation**: If we decide to move ahead with canonicalization of all functions that produce JSON from VARCHAR (as discussed in # 1), then we can catch invalid JSONs in that step and **always** return null.\n\n### 3. `json_array_get` Returns Invalid JSON When Output is a String Scalar\n\nSince we are already on this topic and talking about `json_array_get`, I thought this would be a good time to discuss this known problem in `json_array_get` (this is already highlighted in Presto docs, see [[Presto Documentation](https://prestodb.io/docs/current/functions/json.html#json_array_get-json_array-index-json)](https://prestodb.io/docs/current/functions/json.html#json_array_get-json_array-index-json)).\n\n```sql\nSELECT json_array_get('[\"a\", [3, 9], \"c\"]', 0); -- JSON 'a' < == (invalid JSON)\n```\n\nAs we decide to add support for this in Presto-CPP (Velox), I was wondering if there are any objections to ensuring we support the correct behavior, that is, a string scalar is correctly enclosed in quotes:\n\n```sql\nSELECT json_array_get('[\"a\", [3, 9], \"c\"]', 0); -- JSON '\"a\"' < == (valid JSON)\n```\n\ncc: @amitkdutta @spershin @kgpai @kevinwilfong @Yuhta @kaikalur @feilong-liu @rschlussel @tdcmeehan @aditi-pandit",
    "issue_word_count": 793,
    "test_files_count": 6,
    "non_test_files_count": 7,
    "pr_changed_files": [
      "presto-common/src/main/java/com/facebook/presto/common/function/SqlFunctionProperties.java",
      "presto-main/src/main/java/com/facebook/presto/Session.java",
      "presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java",
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonExtract.java",
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonFunctions.java",
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonPath.java",
      "presto-main/src/main/java/com/facebook/presto/sql/analyzer/FunctionsConfig.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/BenchmarkJsonExtract.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java",
      "presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/BenchmarkJsonExtract.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java",
      "presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java"
    ],
    "base_commit": "fa9a2686c9d79b726260fa6f210a07b1237fe7ca",
    "head_commit": "e26f6f4459200463856556f64d3be5eceb9f723d",
    "repo_url": "https://github.com/prestodb/presto/pull/24614",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24614",
    "dockerfile": "",
    "pr_merged_at": "2025-03-14T15:44:25.000Z",
    "patch": "diff --git a/presto-common/src/main/java/com/facebook/presto/common/function/SqlFunctionProperties.java b/presto-common/src/main/java/com/facebook/presto/common/function/SqlFunctionProperties.java\nindex 0645910c7494b..32bbc18b8dfca 100644\n--- a/presto-common/src/main/java/com/facebook/presto/common/function/SqlFunctionProperties.java\n+++ b/presto-common/src/main/java/com/facebook/presto/common/function/SqlFunctionProperties.java\n@@ -37,6 +37,7 @@ public class SqlFunctionProperties\n     private final boolean legacyJsonCast;\n     private final Map<String, String> extraCredentials;\n     private final boolean warnOnCommonNanPatterns;\n+    private final boolean canonicalizedJsonExtract;\n \n     private SqlFunctionProperties(\n             boolean parseDecimalLiteralAsDouble,\n@@ -50,7 +51,8 @@ private SqlFunctionProperties(\n             boolean fieldNamesInJsonCastEnabled,\n             boolean legacyJsonCast,\n             Map<String, String> extraCredentials,\n-            boolean warnOnCommonNanPatterns)\n+            boolean warnOnCommonNanPatterns,\n+            boolean canonicalizedJsonExtract)\n     {\n         this.parseDecimalLiteralAsDouble = parseDecimalLiteralAsDouble;\n         this.legacyRowFieldOrdinalAccessEnabled = legacyRowFieldOrdinalAccessEnabled;\n@@ -64,6 +66,7 @@ private SqlFunctionProperties(\n         this.legacyJsonCast = legacyJsonCast;\n         this.extraCredentials = requireNonNull(extraCredentials, \"extraCredentials is null\");\n         this.warnOnCommonNanPatterns = warnOnCommonNanPatterns;\n+        this.canonicalizedJsonExtract = canonicalizedJsonExtract;\n     }\n \n     public boolean isParseDecimalLiteralAsDouble()\n@@ -127,6 +130,9 @@ public boolean shouldWarnOnCommonNanPatterns()\n         return warnOnCommonNanPatterns;\n     }\n \n+    public boolean isCanonicalizedJsonExtract()\n+    { return canonicalizedJsonExtract; }\n+\n     @Override\n     public boolean equals(Object o)\n     {\n@@ -146,7 +152,8 @@ public boolean equals(Object o)\n                 Objects.equals(sessionLocale, that.sessionLocale) &&\n                 Objects.equals(sessionUser, that.sessionUser) &&\n                 Objects.equals(extraCredentials, that.extraCredentials) &&\n-                Objects.equals(legacyJsonCast, that.legacyJsonCast);\n+                Objects.equals(legacyJsonCast, that.legacyJsonCast) &&\n+                Objects.equals(canonicalizedJsonExtract, that.legacyJsonCast);\n     }\n \n     @Override\n@@ -154,7 +161,7 @@ public int hashCode()\n     {\n         return Objects.hash(parseDecimalLiteralAsDouble, legacyRowFieldOrdinalAccessEnabled, timeZoneKey,\n                 legacyTimestamp, legacyMapSubscript, sessionStartTime, sessionLocale, sessionUser,\n-                extraCredentials, legacyJsonCast);\n+                extraCredentials, legacyJsonCast, canonicalizedJsonExtract);\n     }\n \n     public static Builder builder()\n@@ -176,6 +183,7 @@ public static class Builder\n         private boolean legacyJsonCast;\n         private Map<String, String> extraCredentials = emptyMap();\n         private boolean warnOnCommonNanPatterns;\n+        private boolean canonicalizedJsonExtract;\n \n         private Builder() {}\n \n@@ -251,6 +259,12 @@ public Builder setWarnOnCommonNanPatterns(boolean warnOnCommonNanPatterns)\n             return this;\n         }\n \n+        public Builder setCanonicalizedJsonExtract(boolean canonicalizedJsonExtract)\n+        {\n+            this.canonicalizedJsonExtract = canonicalizedJsonExtract;\n+            return this;\n+        }\n+\n         public SqlFunctionProperties build()\n         {\n             return new SqlFunctionProperties(\n@@ -265,7 +279,8 @@ public SqlFunctionProperties build()\n                     fieldNamesInJsonCastEnabled,\n                     legacyJsonCast,\n                     extraCredentials,\n-                    warnOnCommonNanPatterns);\n+                    warnOnCommonNanPatterns,\n+                    canonicalizedJsonExtract);\n         }\n     }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/Session.java b/presto-main/src/main/java/com/facebook/presto/Session.java\nindex cc302e7253bea..fd2d88c2707b9 100644\n--- a/presto-main/src/main/java/com/facebook/presto/Session.java\n+++ b/presto-main/src/main/java/com/facebook/presto/Session.java\n@@ -57,6 +57,7 @@\n import java.util.stream.Collectors;\n \n import static com.facebook.presto.SystemSessionProperties.LEGACY_JSON_CAST;\n+import static com.facebook.presto.SystemSessionProperties.isCanonicalizedJsonExtract;\n import static com.facebook.presto.SystemSessionProperties.isFieldNameInJsonCastEnabled;\n import static com.facebook.presto.SystemSessionProperties.isLegacyMapSubscript;\n import static com.facebook.presto.SystemSessionProperties.isLegacyRowFieldOrdinalAccessEnabled;\n@@ -481,6 +482,7 @@ public SqlFunctionProperties getSqlFunctionProperties()\n                 .setLegacyJsonCast(legacyJsonCast)\n                 .setExtraCredentials(identity.getExtraCredentials())\n                 .setWarnOnCommonNanPatterns(warnOnCommonNanPatterns(this))\n+                .setCanonicalizedJsonExtract(isCanonicalizedJsonExtract(this))\n                 .build();\n     }\n \n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java b/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\nindex 500796343a825..0e548c5ef5b4a 100644\n--- a/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\n+++ b/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\n@@ -305,6 +305,7 @@ public final class SystemSessionProperties\n     public static final String REWRITE_CASE_TO_MAP_ENABLED = \"rewrite_case_to_map_enabled\";\n     public static final String FIELD_NAMES_IN_JSON_CAST_ENABLED = \"field_names_in_json_cast_enabled\";\n     public static final String LEGACY_JSON_CAST = \"legacy_json_cast\";\n+    public static final String CANONICALIZED_JSON_EXTRACT = \"canonicalized_json_extract\";\n     public static final String PULL_EXPRESSION_FROM_LAMBDA_ENABLED = \"pull_expression_from_lambda_enabled\";\n     public static final String REWRITE_CONSTANT_ARRAY_CONTAINS_TO_IN_EXPRESSION = \"rewrite_constant_array_contains_to_in_expression\";\n     public static final String INFER_INEQUALITY_PREDICATES = \"infer_inequality_predicates\";\n@@ -1636,6 +1637,11 @@ public SystemSessionProperties(\n                         \"Keep the legacy json cast behavior, do not reserve the case for field names when casting to row type\",\n                         functionsConfig.isLegacyJsonCast(),\n                         true),\n+                booleanProperty(\n+                        CANONICALIZED_JSON_EXTRACT,\n+                        \"Extracts json data in a canonicalized manner, and raises a PrestoException when encountering invalid json structures within the input json path\",\n+                        functionsConfig.isCanonicalizedJsonExtract(),\n+                        true),\n                 booleanProperty(\n                         OPTIMIZE_JOIN_PROBE_FOR_EMPTY_BUILD_RUNTIME,\n                         \"Optimize join probe at runtime if build side is empty\",\n@@ -3174,4 +3180,9 @@ public static boolean isEnabledAddExchangeBelowGroupId(Session session)\n     {\n         return session.getSystemProperty(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, Boolean.class);\n     }\n+\n+    public static boolean isCanonicalizedJsonExtract(Session session)\n+    {\n+        return session.getSystemProperty(CANONICALIZED_JSON_EXTRACT, Boolean.class);\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonExtract.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonExtract.java\nindex dcd04cee7b880..d2ffb8b65df56 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonExtract.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonExtract.java\n@@ -13,6 +13,8 @@\n  */\n package com.facebook.presto.operator.scalar;\n \n+import com.facebook.airlift.json.JsonObjectMapperProvider;\n+import com.facebook.presto.common.function.SqlFunctionProperties;\n import com.facebook.presto.spi.PrestoException;\n import com.fasterxml.jackson.core.JsonFactory;\n import com.fasterxml.jackson.core.JsonGenerator;\n@@ -20,12 +22,14 @@\n import com.fasterxml.jackson.core.JsonParser;\n import com.fasterxml.jackson.core.JsonToken;\n import com.fasterxml.jackson.core.io.SerializedString;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.collect.ImmutableList;\n import io.airlift.slice.DynamicSliceOutput;\n import io.airlift.slice.Slice;\n \n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.OutputStream;\n import java.io.UncheckedIOException;\n \n import static com.facebook.presto.spi.StandardErrorCode.INVALID_FUNCTION_ARGUMENT;\n@@ -38,6 +42,7 @@\n import static com.fasterxml.jackson.core.JsonToken.START_ARRAY;\n import static com.fasterxml.jackson.core.JsonToken.START_OBJECT;\n import static com.fasterxml.jackson.core.JsonToken.VALUE_NULL;\n+import static com.fasterxml.jackson.databind.SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS;\n import static io.airlift.slice.Slices.utf8Slice;\n import static java.util.Objects.requireNonNull;\n \n@@ -121,13 +126,15 @@ public final class JsonExtract\n     private static final JsonFactory JSON_FACTORY = new JsonFactory()\n             .disable(CANONICALIZE_FIELD_NAMES);\n \n+    private static final ObjectMapper SORTED_MAPPER = new JsonObjectMapperProvider().get().configure(ORDER_MAP_ENTRIES_BY_KEYS, true);\n+\n     private JsonExtract() {}\n \n-    public static <T> T extract(Slice jsonInput, JsonExtractor<T> jsonExtractor)\n+    public static <T> T extract(Slice jsonInput, JsonExtractor<T> jsonExtractor, SqlFunctionProperties properties)\n     {\n         requireNonNull(jsonInput, \"jsonInput is null\");\n         try {\n-            return jsonExtractor.extract(jsonInput.getInput());\n+            return jsonExtractor.extract(jsonInput.getInput(), properties);\n         }\n         catch (JsonParseException e) {\n             // Return null if we failed to parse something\n@@ -156,7 +163,7 @@ public static <T> PrestoJsonExtractor<T> generateExtractor(String path, PrestoJs\n \n     public interface JsonExtractor<T>\n     {\n-        T extract(InputStream inputStream)\n+        T extract(InputStream inputStream, SqlFunctionProperties properties)\n                 throws IOException;\n     }\n \n@@ -174,11 +181,11 @@ public abstract static class PrestoJsonExtractor<T>\n          *\n          * @return the value, or null if not applicable\n          */\n-        abstract T extract(JsonParser jsonParser)\n+        abstract T extract(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException;\n \n         @Override\n-        public T extract(InputStream inputStream)\n+        public T extract(InputStream inputStream, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             try (JsonParser jsonParser = createJsonParser(JSON_FACTORY, inputStream)) {\n@@ -187,7 +194,7 @@ public T extract(InputStream inputStream)\n                     return null;\n                 }\n \n-                return extract(jsonParser);\n+                return extract(jsonParser, properties);\n             }\n         }\n     }\n@@ -214,21 +221,21 @@ public ObjectFieldJsonExtractor(String fieldName, PrestoJsonExtractor<? extends\n         }\n \n         @Override\n-        public T extract(JsonParser jsonParser)\n+        public T extract(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             if (jsonParser.getCurrentToken() == START_OBJECT) {\n-                return processJsonObject(jsonParser);\n+                return processJsonObject(jsonParser, properties);\n             }\n \n             if (jsonParser.getCurrentToken() == START_ARRAY) {\n-                return processJsonArray(jsonParser);\n+                return processJsonArray(jsonParser, properties);\n             }\n \n             throw new JsonParseException(jsonParser, \"Expected a JSON object or array\");\n         }\n \n-        public T processJsonObject(JsonParser jsonParser)\n+        public T processJsonObject(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             while (!jsonParser.nextFieldName(fieldName)) {\n@@ -244,10 +251,10 @@ public T processJsonObject(JsonParser jsonParser)\n \n             jsonParser.nextToken(); // Shift to first token of the value\n \n-            return delegate.extract(jsonParser);\n+            return delegate.extract(jsonParser, properties);\n         }\n \n-        public T processJsonArray(JsonParser jsonParser)\n+        public T processJsonArray(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             int currentIndex = 0;\n@@ -270,7 +277,7 @@ public T processJsonArray(JsonParser jsonParser)\n                 jsonParser.skipChildren(); // Skip nested structure if currently at the start of one\n             }\n \n-            return delegate.extract(jsonParser);\n+            return delegate.extract(jsonParser, properties);\n         }\n     }\n \n@@ -278,7 +285,7 @@ public static class ScalarValueJsonExtractor\n             extends PrestoJsonExtractor<Slice>\n     {\n         @Override\n-        public Slice extract(JsonParser jsonParser)\n+        public Slice extract(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             JsonToken token = jsonParser.getCurrentToken();\n@@ -296,13 +303,31 @@ public static class JsonValueJsonExtractor\n             extends PrestoJsonExtractor<Slice>\n     {\n         @Override\n-        public Slice extract(JsonParser jsonParser)\n+        public Slice extract(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             if (!jsonParser.hasCurrentToken()) {\n                 throw new JsonParseException(jsonParser, \"Unexpected end of value\");\n             }\n+            if (!properties.isCanonicalizedJsonExtract()) {\n+                return legacyExtract(jsonParser);\n+            }\n+            DynamicSliceOutput dynamicSliceOutput = new DynamicSliceOutput(ESTIMATED_JSON_OUTPUT_SIZE);\n+            // Write the JSON to output stream with sorted keys\n+            SORTED_MAPPER.writeValue((OutputStream) dynamicSliceOutput, SORTED_MAPPER.readValue(jsonParser, Object.class));\n+            // nextToken will throw an exception if there are trailing characters.\n+            try {\n+                jsonParser.nextToken();\n+            }\n+            catch (JsonParseException e) {\n+                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, e.getMessage());\n+            }\n+            return dynamicSliceOutput.slice();\n+        }\n \n+        public Slice legacyExtract(JsonParser jsonParser)\n+                throws IOException\n+        {\n             DynamicSliceOutput dynamicSliceOutput = new DynamicSliceOutput(ESTIMATED_JSON_OUTPUT_SIZE);\n             try (JsonGenerator jsonGenerator = createJsonGenerator(JSON_FACTORY, dynamicSliceOutput)) {\n                 jsonGenerator.copyCurrentStructure(jsonParser);\n@@ -315,7 +340,7 @@ public static class JsonSizeExtractor\n             extends PrestoJsonExtractor<Long>\n     {\n         @Override\n-        public Long extract(JsonParser jsonParser)\n+        public Long extract(JsonParser jsonParser, SqlFunctionProperties properties)\n                 throws IOException\n         {\n             if (!jsonParser.hasCurrentToken()) {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonFunctions.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonFunctions.java\nindex 146ac74b908fe..339ff5c10dba9 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonFunctions.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonFunctions.java\n@@ -435,51 +435,51 @@ public static Slice jsonArrayGet(@SqlType(StandardTypes.JSON) Slice json, @SqlTy\n     @SqlNullable\n     @LiteralParameters(\"x\")\n     @SqlType(\"varchar(x)\")\n-    public static Slice varcharJsonExtractScalar(@SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Slice varcharJsonExtractScalar(SqlFunctionProperties properties, @SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getScalarExtractor());\n+        return JsonExtract.extract(json, jsonPath.getScalarExtractor(), properties);\n     }\n \n     @ScalarFunction\n     @SqlNullable\n     @SqlType(StandardTypes.VARCHAR)\n-    public static Slice jsonExtractScalar(@SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Slice jsonExtractScalar(SqlFunctionProperties properties, @SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getScalarExtractor());\n+        return JsonExtract.extract(json, jsonPath.getScalarExtractor(), properties);\n     }\n \n     @ScalarFunction(\"json_extract\")\n     @LiteralParameters(\"x\")\n     @SqlNullable\n     @SqlType(StandardTypes.JSON)\n-    public static Slice varcharJsonExtract(@SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Slice varcharJsonExtract(SqlFunctionProperties properties, @SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getObjectExtractor());\n+        return JsonExtract.extract(json, jsonPath.getObjectExtractor(), properties);\n     }\n \n     @ScalarFunction\n     @SqlNullable\n     @SqlType(StandardTypes.JSON)\n-    public static Slice jsonExtract(@SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Slice jsonExtract(SqlFunctionProperties properties, @SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getObjectExtractor());\n+        return JsonExtract.extract(json, jsonPath.getObjectExtractor(), properties);\n     }\n \n     @ScalarFunction(\"json_size\")\n     @LiteralParameters(\"x\")\n     @SqlNullable\n     @SqlType(StandardTypes.BIGINT)\n-    public static Long varcharJsonSize(@SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Long varcharJsonSize(SqlFunctionProperties properties, @SqlType(\"varchar(x)\") Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getSizeExtractor());\n+        return JsonExtract.extract(json, jsonPath.getSizeExtractor(), properties);\n     }\n \n     @ScalarFunction\n     @SqlNullable\n     @SqlType(StandardTypes.BIGINT)\n-    public static Long jsonSize(@SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n+    public static Long jsonSize(SqlFunctionProperties properties, @SqlType(StandardTypes.JSON) Slice json, @SqlType(JsonPathType.NAME) JsonPath jsonPath)\n     {\n-        return JsonExtract.extract(json, jsonPath.getSizeExtractor());\n+        return JsonExtract.extract(json, jsonPath.getSizeExtractor(), properties);\n     }\n \n     public static Object getJsonObjectValue(Type valueType, SqlFunctionProperties properties, Block block, int position)\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonPath.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonPath.java\nindex 2fc980e67b9df..fedafda1eb46c 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonPath.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/JsonPath.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator.scalar;\n \n+import com.facebook.presto.common.function.SqlFunctionProperties;\n import com.facebook.presto.spi.PrestoException;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -42,7 +43,6 @@ private static JsonExtract.JsonExtractor<Slice> getScalarExtractorForJayway(com.\n     {\n         return new JsonExtract.JsonExtractor<Slice>()\n         {\n-            @Override\n             public Slice extract(InputStream inputStream)\n                     throws IOException\n             {\n@@ -52,6 +52,13 @@ public Slice extract(InputStream inputStream)\n                 }\n                 return utf8Slice(node.asText());\n             }\n+\n+            @Override\n+            public Slice extract(InputStream inputStream, SqlFunctionProperties properties)\n+                    throws IOException\n+            {\n+                return extract(inputStream);\n+            }\n         };\n     }\n \n@@ -59,7 +66,6 @@ private static JsonExtract.JsonExtractor<Slice> getObjectExtractorForJayway(com.\n     {\n         return new JsonExtract.JsonExtractor<Slice>()\n         {\n-            @Override\n             public Slice extract(InputStream inputStream)\n                     throws IOException\n             {\n@@ -69,6 +75,13 @@ public Slice extract(InputStream inputStream)\n                 }\n                 return utf8Slice(node.toString());\n             }\n+\n+            @Override\n+            public Slice extract(InputStream inputStream, SqlFunctionProperties properties)\n+                    throws IOException\n+            {\n+                return extract(inputStream);\n+            }\n         };\n     }\n \n@@ -76,7 +89,6 @@ private static JsonExtract.JsonExtractor<Long> getSizeExtractorForJayway(com.jay\n     {\n         return new JsonExtract.JsonExtractor<Long>()\n         {\n-            @Override\n             public Long extract(InputStream inputStream)\n                     throws IOException\n             {\n@@ -86,6 +98,13 @@ public Long extract(InputStream inputStream)\n                 }\n                 return (long) node.size(); // Jackson correctly returns 0 for scalar nodes\n             }\n+\n+            @Override\n+            public Long extract(InputStream inputStream, SqlFunctionProperties properties)\n+                    throws IOException\n+            {\n+                return extract(inputStream);\n+            }\n         };\n     }\n \n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FunctionsConfig.java b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FunctionsConfig.java\nindex 542d09ba00470..4e64a20d467dc 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FunctionsConfig.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FunctionsConfig.java\n@@ -18,6 +18,7 @@\n import com.facebook.presto.operator.aggregation.arrayagg.ArrayAggGroupImplementation;\n import com.facebook.presto.operator.aggregation.histogram.HistogramGroupImplementation;\n import com.facebook.presto.operator.aggregation.multimapagg.MultimapAggGroupImplementation;\n+import com.facebook.presto.spi.function.Description;\n \n import javax.validation.constraints.Min;\n \n@@ -47,6 +48,7 @@ public class FunctionsConfig\n     private boolean warnOnPossibleNans;\n     private boolean legacyCharToVarcharCoercion;\n     private boolean legacyJsonCast = true;\n+    private boolean canonicalizedJsonExtract;\n     private String defaultNamespacePrefix = JAVA_BUILTIN_NAMESPACE.toString();\n \n     @Config(\"deprecated.legacy-array-agg\")\n@@ -308,6 +310,19 @@ public boolean isLegacyJsonCast()\n         return legacyJsonCast;\n     }\n \n+    @Config(\"canonicalized-json-extract\")\n+    @Description(\"Extracts json data in a canonicalized manner, and raises a PrestoException when encountering invalid json structures within the input json path\")\n+    public FunctionsConfig setCanonicalizedJsonExtract(boolean canonicalizedJsonExtract)\n+    {\n+        this.canonicalizedJsonExtract = canonicalizedJsonExtract;\n+        return this;\n+    }\n+\n+    public boolean isCanonicalizedJsonExtract()\n+    {\n+        return canonicalizedJsonExtract;\n+    }\n+\n     @Config(\"presto.default-namespace\")\n     @ConfigDescription(\"Specifies the default function namespace prefix\")\n     public FunctionsConfig setDefaultNamespacePrefix(String defaultNamespacePrefix)\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/BenchmarkJsonExtract.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/BenchmarkJsonExtract.java\nnew file mode 100644\nindex 0000000000000..2c934134d5170\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/BenchmarkJsonExtract.java\n@@ -0,0 +1,258 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.operator.scalar;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.Page;\n+import com.facebook.presto.common.block.Block;\n+import com.facebook.presto.common.block.BlockBuilder;\n+import com.facebook.presto.common.function.SqlFunctionProperties;\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.MetadataManager;\n+import com.facebook.presto.operator.DriverYieldSignal;\n+import com.facebook.presto.operator.project.PageProcessor;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.WarningCollector;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.sql.gen.ExpressionCompiler;\n+import com.facebook.presto.sql.gen.PageFunctionCompiler;\n+import com.facebook.presto.sql.parser.SqlParser;\n+import com.facebook.presto.sql.planner.TypeProvider;\n+import com.facebook.presto.sql.relational.RowExpressionOptimizer;\n+import com.facebook.presto.sql.relational.SqlToRowExpressionTranslator;\n+import com.facebook.presto.sql.tree.Expression;\n+import com.facebook.presto.sql.tree.NodeRef;\n+import com.facebook.presto.testing.TestingConnectorSession;\n+import com.facebook.presto.testing.TestingSession;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+import org.openjdk.jmh.runner.options.VerboseMode;\n+import org.openjdk.jmh.runner.options.WarmupMode;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.TimeUnit;\n+\n+import static com.facebook.presto.common.type.JsonType.JSON;\n+import static com.facebook.presto.common.type.TimeZoneKey.UTC_KEY;\n+import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n+import static com.facebook.presto.memory.context.AggregatedMemoryContext.newSimpleAggregatedMemoryContext;\n+import static com.facebook.presto.metadata.MetadataManager.createTestMetadataManager;\n+import static com.facebook.presto.operator.scalar.FunctionAssertions.createExpression;\n+import static com.facebook.presto.spi.relation.ExpressionOptimizer.Level.OPTIMIZED;\n+import static com.facebook.presto.sql.analyzer.ExpressionAnalyzer.getExpressionTypes;\n+import static java.util.Collections.emptyMap;\n+import static java.util.Locale.ENGLISH;\n+\n+@SuppressWarnings(\"MethodMayBeStatic\")\n+@State(Scope.Thread)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@Fork(10)\n+@BenchmarkMode(Mode.AverageTime)\n+public class BenchmarkJsonExtract\n+{\n+    private static final SqlParser SQL_PARSER = new SqlParser();\n+    private static final Metadata METADATA = createTestMetadataManager();\n+    private static final Session TEST_SESSION = TestingSession.testSessionBuilder().build();\n+    public static final ConnectorSession SESSION = new TestingConnectorSession(ImmutableList.of());\n+\n+    private static final int POSITION_COUNT = 100_000;\n+    private static final int ARRAY_SIZE = 20;\n+    private static final String CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\";\n+\n+    private PageProcessor pageProcessor;\n+    private Page inputPage;\n+    private Map<String, Type> symbolTypes;\n+    private Map<VariableReferenceExpression, Integer> sourceLayout;\n+\n+    @Param({\"true\", \"false\"})\n+    boolean isCanonicalizedJsonExtract;\n+\n+    @Setup\n+    public void setup()\n+    {\n+        VariableReferenceExpression variable = new VariableReferenceExpression(Optional.empty(), VARCHAR.getDisplayName().toLowerCase(ENGLISH) + \"0\", VARCHAR);\n+        symbolTypes = ImmutableMap.of(variable.getName(), VARCHAR);\n+        sourceLayout = ImmutableMap.of(variable, 0);\n+        inputPage = new Page(createChannel());\n+        List<RowExpression> projections = ImmutableList.of(rowExpression(\"json_extract(varchar0, '$.key1')\"), rowExpression(\"json_extract(varchar0, '$.key2')\"));\n+        MetadataManager metadata = createTestMetadataManager();\n+        PageFunctionCompiler pageFunctionCompiler = new PageFunctionCompiler(metadata, 0);\n+        ExpressionCompiler expressionCompiler = new ExpressionCompiler(metadata, pageFunctionCompiler);\n+        pageProcessor = expressionCompiler.compilePageProcessor(TEST_SESSION.getSqlFunctionProperties(), Optional.empty(), projections).get();\n+    }\n+\n+    @Benchmark\n+    public List<Optional<Page>> computePage()\n+    {\n+        SqlFunctionProperties sqlFunctionProperties = SqlFunctionProperties.builder()\n+                .setTimeZoneKey(UTC_KEY)\n+                .setLegacyTimestamp(true)\n+                .setSessionStartTime(0)\n+                .setSessionLocale(ENGLISH).setSessionUser(\"user\")\n+                .setCanonicalizedJsonExtract(isCanonicalizedJsonExtract)\n+                .build();\n+\n+        return ImmutableList.copyOf(\n+                pageProcessor.process(\n+                        sqlFunctionProperties,\n+                        new DriverYieldSignal(),\n+                        newSimpleAggregatedMemoryContext().newLocalMemoryContext(PageProcessor.class.getSimpleName()),\n+                        inputPage));\n+    }\n+\n+    private RowExpression rowExpression(String value)\n+    {\n+        Expression expression = createExpression(TEST_SESSION, value, METADATA, TypeProvider.copyOf(symbolTypes));\n+        Map<NodeRef<Expression>, Type> expressionTypes = getExpressionTypes(TEST_SESSION, METADATA, SQL_PARSER, TypeProvider.copyOf(symbolTypes), expression, emptyMap(), WarningCollector.NOOP);\n+        RowExpression rowExpression = SqlToRowExpressionTranslator.translate(expression, expressionTypes, sourceLayout, METADATA.getFunctionAndTypeManager(), TEST_SESSION);\n+        RowExpressionOptimizer optimizer = new RowExpressionOptimizer(METADATA);\n+        return optimizer.optimize(rowExpression, OPTIMIZED, TEST_SESSION.toConnectorSession());\n+    }\n+\n+    private static Block createChannel()\n+    {\n+        BlockBuilder blockBuilder = JSON.createBlockBuilder(null, BenchmarkJsonExtract.POSITION_COUNT);\n+        for (int position = 0; position < BenchmarkJsonExtract.POSITION_COUNT; position++) {\n+            try (SliceOutput jsonSlice = new DynamicSliceOutput(20 * BenchmarkJsonExtract.ARRAY_SIZE)) {\n+                jsonSlice.appendByte('{');\n+                int k1Index = ThreadLocalRandom.current().nextInt(ARRAY_SIZE);\n+                int k2Index = ThreadLocalRandom.current().nextInt(ARRAY_SIZE);\n+                while (k2Index == k1Index) {\n+                    k2Index = ThreadLocalRandom.current().nextInt(ARRAY_SIZE);\n+                }\n+\n+                for (int i = 0; i < ARRAY_SIZE; i++) {\n+                    String key;\n+                    if (i == k1Index) {\n+                        key = \"key1\";\n+                    }\n+                    else if (i == k2Index) {\n+                        key = \"key2\";\n+                    }\n+                    else {\n+                        key = generateRandomKey(ThreadLocalRandom.current().nextInt(5) + 1);\n+                    }\n+                    jsonSlice.appendBytes(\"\\\"\".getBytes());\n+                    jsonSlice.appendBytes(key.getBytes());\n+                    jsonSlice.appendBytes(\"\\\"\".getBytes());\n+                    jsonSlice.appendByte(':');\n+                    String value;\n+                    if (key.equals(\"key1\") || key.equals(\"key2\") || (ThreadLocalRandom.current().nextInt(10) & 1) == 0) {\n+                        value = generateNestedJsonValue();\n+                    }\n+                    else {\n+                        value = generateRandomJsonValue();\n+                    }\n+                    jsonSlice.appendBytes(value.getBytes());\n+                    if (i < ARRAY_SIZE - 1) {\n+                        jsonSlice.appendByte(','); // Add a comma between JSON objects\n+                    }\n+                }\n+\n+                jsonSlice.appendByte('}');\n+                JSON.writeSlice(blockBuilder, jsonSlice.slice());\n+            }\n+            catch (Exception ignore) {\n+                // Ignore...\n+            }\n+        }\n+        return blockBuilder.build();\n+    }\n+\n+    private static String generateRandomJsonValue()\n+    {\n+        int length = ThreadLocalRandom.current().nextInt(10) + 1;\n+        StringBuilder builder = new StringBuilder(length + 2);\n+        builder.append('\"');\n+        for (int i = 0; i < length; i++) {\n+            char c = CHARACTERS.charAt(ThreadLocalRandom.current().nextInt(CHARACTERS.length()));\n+            if (c == '\"') {\n+                builder.append('\\\\'); // escape double quote\n+            }\n+            builder.append(c);\n+        }\n+        builder.append('\"');\n+        return builder.toString();\n+    }\n+\n+    private static String generateNestedJsonValue()\n+    {\n+        int size = ThreadLocalRandom.current().nextInt(5) + 1;\n+        StringBuilder builder = new StringBuilder(size * 10);\n+        builder.append('{');\n+        for (int i = 0; i < size; i++) {\n+            String key = generateRandomKey(ThreadLocalRandom.current().nextInt(5) + 2);\n+            builder.append(\"\\\"\").append(key).append(\"\\\":\");\n+            builder.append(generateRandomJsonValue());\n+            if (i < size - 1) {\n+                builder.append(\",\");\n+            }\n+        }\n+        builder.append('}');\n+        return builder.toString();\n+    }\n+\n+    private static String generateRandomKey(int len)\n+    {\n+        StringBuilder builder = new StringBuilder(len);\n+        for (int i = 0; i < len; i++) {\n+            builder.append(CHARACTERS.charAt(ThreadLocalRandom.current().nextInt(CHARACTERS.length())));\n+        }\n+        return builder.toString();\n+    }\n+\n+    @Test\n+    public void verify()\n+    {\n+        BenchmarkJsonToArrayCast.BenchmarkData data = new BenchmarkJsonToArrayCast.BenchmarkData();\n+        data.setup();\n+        new BenchmarkJsonToArrayCast().benchmark(data);\n+    }\n+\n+    public static void main(String[] args)\n+            throws Throwable\n+    {\n+        // assure the benchmarks are valid before running\n+        BenchmarkJsonToArrayCast.BenchmarkData data = new BenchmarkJsonToArrayCast.BenchmarkData();\n+        data.setup();\n+        new BenchmarkJsonToArrayCast().benchmark(data);\n+\n+        Options options = new OptionsBuilder()\n+                .verbosity(VerboseMode.NORMAL)\n+                .include(\".*\" + BenchmarkJsonExtract.class.getSimpleName() + \".*\")\n+                .warmupMode(WarmupMode.BULK_INDI)\n+                .build();\n+        new Runner(options).run();\n+    }\n+}\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java\nindex 07ea50939d41b..5090b7cd7a642 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtract.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.operator.scalar;\n \n+import com.facebook.presto.common.function.SqlFunctionProperties;\n import com.facebook.presto.spi.PrestoException;\n import com.google.common.collect.ImmutableList;\n import io.airlift.slice.Slice;\n@@ -23,6 +24,7 @@\n import java.io.IOException;\n import java.util.List;\n \n+import static com.facebook.presto.common.type.TimeZoneKey.UTC_KEY;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.operator.scalar.JsonExtract.JsonExtractor;\n import static com.facebook.presto.operator.scalar.JsonExtract.JsonValueJsonExtractor;\n@@ -30,6 +32,7 @@\n import static com.facebook.presto.operator.scalar.JsonExtract.ScalarValueJsonExtractor;\n import static com.facebook.presto.operator.scalar.JsonExtract.generateExtractor;\n import static com.facebook.presto.spi.StandardErrorCode.INVALID_FUNCTION_ARGUMENT;\n+import static java.util.Locale.ENGLISH;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertNull;\n import static org.testng.Assert.assertTrue;\n@@ -38,6 +41,11 @@\n public class TestJsonExtract\n         extends AbstractTestFunctions\n {\n+    public static final SqlFunctionProperties PROPERTIES_CANONICALIZED_EXTRACT_ENABLED =\n+            SqlFunctionProperties.builder().setTimeZoneKey(UTC_KEY).setLegacyTimestamp(true).setSessionStartTime(0).setSessionLocale(ENGLISH).setSessionUser(\"user\").setCanonicalizedJsonExtract(true).build();\n+\n+    public static final SqlFunctionProperties PROPERTIES_CANONICALIZED_EXTRACT_DISABLED =\n+            SqlFunctionProperties.builder().setTimeZoneKey(UTC_KEY).setLegacyTimestamp(true).setSessionStartTime(0).setSessionLocale(ENGLISH).setSessionUser(\"user\").setCanonicalizedJsonExtract(false).build();\n     @BeforeClass\n     public void setUp()\n     {\n@@ -148,20 +156,30 @@ public void testScalarValueJsonExtractor()\n         ScalarValueJsonExtractor extractor = new ScalarValueJsonExtractor();\n \n         // Check scalar values\n-        assertEquals(doExtract(extractor, \"123\"), \"123\");\n-        assertEquals(doExtract(extractor, \"-1\"), \"-1\");\n-        assertEquals(doExtract(extractor, \"0.01\"), \"0.01\");\n-        assertEquals(doExtract(extractor, \"\\\"abc\\\"\"), \"abc\");\n-        assertEquals(doExtract(extractor, \"\\\"\\\"\"), \"\");\n-        assertEquals(doExtract(extractor, \"null\"), null);\n+        assertEquals(doExtractLegacy(extractor, \"123\"), \"123\");\n+        assertEquals(doExtractLegacy(extractor, \"-1\"), \"-1\");\n+        assertEquals(doExtractLegacy(extractor, \"0.01\"), \"0.01\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"abc\\\"\"), \"abc\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"\\\"\"), \"\");\n+        assertNull(doExtractLegacy(extractor, \"null\"));\n+        assertEquals(doCanonicalizedExtract(extractor, \"123\"), \"123\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"-1\"), \"-1\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"0.01\"), \"0.01\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"abc\\\"\"), \"abc\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"\\\"\"), \"\");\n+        assertNull(doCanonicalizedExtract(extractor, \"null\"));\n \n         // Test character escaped values\n-        assertEquals(doExtract(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"ab\\001c\");\n-        assertEquals(doExtract(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"ab\\002c\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"ab\\001c\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"ab\\002c\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"ab\\001c\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"ab\\002c\");\n \n         // Complex types should return null\n-        assertNull(doExtract(extractor, \"[1, 2, 3]\"));\n-        assertNull(doExtract(extractor, \"{\\\"a\\\": 1}\"));\n+        assertNull(doExtractLegacy(extractor, \"[1, 2, 3]\"));\n+        assertNull(doExtractLegacy(extractor, \"{\\\"a\\\": 1}\"));\n+        assertNull(doCanonicalizedExtract(extractor, \"[1, 2, 3]\"));\n+        assertNull(doCanonicalizedExtract(extractor, \"{\\\"a\\\": 1}\"));\n     }\n \n     @Test\n@@ -171,20 +189,30 @@ public void testJsonValueJsonExtractor()\n         JsonValueJsonExtractor extractor = new JsonValueJsonExtractor();\n \n         // Check scalar values\n-        assertEquals(doExtract(extractor, \"123\"), \"123\");\n-        assertEquals(doExtract(extractor, \"-1\"), \"-1\");\n-        assertEquals(doExtract(extractor, \"0.01\"), \"0.01\");\n-        assertEquals(doExtract(extractor, \"\\\"abc\\\"\"), \"\\\"abc\\\"\");\n-        assertEquals(doExtract(extractor, \"\\\"\\\"\"), \"\\\"\\\"\");\n-        assertEquals(doExtract(extractor, \"null\"), \"null\");\n+        assertEquals(doExtractLegacy(extractor, \"123\"), \"123\");\n+        assertEquals(doExtractLegacy(extractor, \"-1\"), \"-1\");\n+        assertEquals(doExtractLegacy(extractor, \"0.01\"), \"0.01\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"abc\\\"\"), \"\\\"abc\\\"\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"\\\"\"), \"\\\"\\\"\");\n+        assertEquals(doExtractLegacy(extractor, \"null\"), \"null\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"123\"), \"123\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"-1\"), \"-1\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"0.01\"), \"0.01\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"abc\\\"\"), \"\\\"abc\\\"\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"\\\"\"), \"\\\"\\\"\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"null\"), \"null\");\n \n         // Test character escaped values\n-        assertEquals(doExtract(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"\\\"ab\\\\u0001c\\\"\");\n-        assertEquals(doExtract(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"\\\"ab\\\\u0002c\\\"\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"\\\"ab\\\\u0001c\\\"\");\n+        assertEquals(doExtractLegacy(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"\\\"ab\\\\u0002c\\\"\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"ab\\\\u0001c\\\"\"), \"\\\"ab\\\\u0001c\\\"\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"\\\"ab\\\\u0002c\\\"\"), \"\\\"ab\\\\u0002c\\\"\");\n \n         // Complex types should return json values\n-        assertEquals(doExtract(extractor, \"[1, 2, 3]\"), \"[1,2,3]\");\n-        assertEquals(doExtract(extractor, \"{\\\"a\\\": 1}\"), \"{\\\"a\\\":1}\");\n+        assertEquals(doExtractLegacy(extractor, \"[1, 2, 3]\"), \"[1,2,3]\");\n+        assertEquals(doExtractLegacy(extractor, \"{\\\"a\\\": 1}\"), \"{\\\"a\\\":1}\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"[1, 2, 3]\"), \"[1,2,3]\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"{\\\"a\\\": 1}\"), \"{\\\"a\\\":1}\");\n     }\n \n     @Test\n@@ -194,14 +222,20 @@ public void testArrayElementJsonExtractor()\n         ObjectFieldJsonExtractor<Slice> firstExtractor = new ObjectFieldJsonExtractor<>(\"0\", new ScalarValueJsonExtractor());\n         ObjectFieldJsonExtractor<Slice> secondExtractor = new ObjectFieldJsonExtractor<>(\"1\", new ScalarValueJsonExtractor());\n \n-        assertNull(doExtract(firstExtractor, \"[]\"));\n-        assertEquals(doExtract(firstExtractor, \"[1, 2, 3]\"), \"1\");\n-        assertEquals(doExtract(secondExtractor, \"[1, 2]\"), \"2\");\n-        assertNull(doExtract(secondExtractor, \"[1, null]\"));\n+        assertNull(doExtractLegacy(firstExtractor, \"[]\"));\n+        assertEquals(doExtractLegacy(firstExtractor, \"[1, 2, 3]\"), \"1\");\n+        assertEquals(doExtractLegacy(secondExtractor, \"[1, 2]\"), \"2\");\n+        assertNull(doExtractLegacy(secondExtractor, \"[1, null]\"));\n+        assertNull(doCanonicalizedExtract(firstExtractor, \"[]\"));\n+        assertEquals(doCanonicalizedExtract(firstExtractor, \"[1, 2, 3]\"), \"1\");\n+        assertEquals(doCanonicalizedExtract(secondExtractor, \"[1, 2]\"), \"2\");\n+        assertNull(doCanonicalizedExtract(secondExtractor, \"[1, null]\"));\n         // Out of bounds\n-        assertNull(doExtract(secondExtractor, \"[1]\"));\n+        assertNull(doExtractLegacy(secondExtractor, \"[1]\"));\n+        assertNull(doCanonicalizedExtract(secondExtractor, \"[1]\"));\n         // Check skipping complex structures\n-        assertEquals(doExtract(secondExtractor, \"[{\\\"a\\\": 1}, 2, 3]\"), \"2\");\n+        assertEquals(doExtractLegacy(secondExtractor, \"[{\\\"a\\\": 1}, 2, 3]\"), \"2\");\n+        assertEquals(doCanonicalizedExtract(secondExtractor, \"[{\\\"a\\\": 1}, 2, 3]\"), \"2\");\n     }\n \n     @Test\n@@ -210,131 +244,232 @@ public void testObjectFieldJsonExtractor()\n     {\n         ObjectFieldJsonExtractor<Slice> extractor = new ObjectFieldJsonExtractor<>(\"fuu\", new ScalarValueJsonExtractor());\n \n-        assertNull(doExtract(extractor, \"{}\"));\n-        assertNull(doExtract(extractor, \"{\\\"a\\\": 1}\"));\n-        assertEquals(doExtract(extractor, \"{\\\"fuu\\\": 1}\"), \"1\");\n-        assertEquals(doExtract(extractor, \"{\\\"a\\\": 0, \\\"fuu\\\": 1}\"), \"1\");\n+        assertNull(doExtractLegacy(extractor, \"{}\"));\n+        assertNull(doExtractLegacy(extractor, \"{\\\"a\\\": 1}\"));\n+        assertEquals(doExtractLegacy(extractor, \"{\\\"fuu\\\": 1}\"), \"1\");\n+        assertEquals(doExtractLegacy(extractor, \"{\\\"a\\\": 0, \\\"fuu\\\": 1}\"), \"1\");\n+        assertNull(doCanonicalizedExtract(extractor, \"{}\"));\n+        assertNull(doCanonicalizedExtract(extractor, \"{\\\"a\\\": 1}\"));\n+        assertEquals(doCanonicalizedExtract(extractor, \"{\\\"fuu\\\": 1}\"), \"1\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"{\\\"a\\\": 0, \\\"fuu\\\": 1}\"), \"1\");\n         // Check skipping complex structures\n-        assertEquals(doExtract(extractor, \"{\\\"a\\\": [1, 2, 3], \\\"fuu\\\": 1}\"), \"1\");\n+        assertEquals(doCanonicalizedExtract(extractor, \"{\\\"a\\\": [1, 2, 3], \\\"fuu\\\": 1}\"), \"1\");\n     }\n \n     @Test\n     public void testFullScalarExtract()\n     {\n-        assertNull(doScalarExtract(\"{}\", \"$\"));\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\"), null); // Null b/c value is complex type\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n-        assertNull(doScalarExtract(\"{\\\"fuu\\\": null}\", \"$.fuu\"));\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": 1}\", \"$.bar\"), null);\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\001\"); // Test escaped characters\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"abc\");\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\"), null); // Null b/c value is complex type\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n-        assertEquals(doScalarExtract(\"{\\\"fuu\\\": [0, {\\\"bar\\\": {\\\"key\\\" : [\\\"value\\\"]}}, 2]}\", \"$.fuu[1].bar.key[0]\"), \"value\");\n+        assertNull(doScalarExtractLegacy(\"{}\", \"$\"));\n+        assertNull(doScalarExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\")); // Null b/c value is complex type\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertNull(doScalarExtractLegacy(\"{\\\"fuu\\\": null}\", \"$.fuu\"));\n+        assertNull(doScalarExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$.bar\"));\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\001\"); // Test escaped characters\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"abc\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n+        assertNull(doScalarExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\")); // Null b/c value is complex type\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"fuu\\\": [0, {\\\"bar\\\": {\\\"key\\\" : [\\\"value\\\"]}}, 2]}\", \"$.fuu[1].bar.key[0]\"), \"value\");\n+\n+        assertNull(doScalarCanonicalizedExtract(\"{}\", \"$\"));\n+        assertNull(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\")); // Null b/c value is complex type\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertNull(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": null}\", \"$.fuu\"));\n+        assertNull(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$.bar\"));\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\001\"); // Test escaped characters\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"abc\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n+        assertNull(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\")); // Null b/c value is complex type\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"fuu\\\": [0, {\\\"bar\\\": {\\\"key\\\" : [\\\"value\\\"]}}, 2]}\", \"$.fuu[1].bar.key[0]\"), \"value\");\n \n         // Test non-object extraction\n-        assertEquals(doScalarExtract(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n-        assertEquals(doScalarExtract(\"\\\"abc\\\"\", \"$\"), \"abc\");\n-        assertEquals(doScalarExtract(\"123\", \"$\"), \"123\");\n-        assertEquals(doScalarExtract(\"null\", \"$\"), null);\n+        assertEquals(doScalarExtractLegacy(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n+        assertEquals(doScalarExtractLegacy(\"\\\"abc\\\"\", \"$\"), \"abc\");\n+        assertEquals(doScalarExtractLegacy(\"123\", \"$\"), \"123\");\n+        assertNull(doScalarExtractLegacy(\"null\", \"$\"));\n+\n+        assertEquals(doScalarCanonicalizedExtract(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n+        assertEquals(doScalarCanonicalizedExtract(\"\\\"abc\\\"\", \"$\"), \"abc\");\n+        assertEquals(doScalarCanonicalizedExtract(\"123\", \"$\"), \"123\");\n+        assertNull(doScalarCanonicalizedExtract(\"null\", \"$\"));\n \n         // Test numeric path expression matches arrays and objects\n-        assertEquals(doScalarExtract(\"[0, 1, 2]\", \"$.1\"), \"1\");\n-        assertEquals(doScalarExtract(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n-        assertEquals(doScalarExtract(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"[0, 1, 2]\", \"$.1\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n+\n+        assertEquals(doScalarCanonicalizedExtract(\"[0, 1, 2]\", \"$.1\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n \n         // Test fields starting with a digit\n-        assertEquals(doScalarExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n-        assertEquals(doScalarExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n+        assertEquals(doScalarExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n+\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n+        assertEquals(doScalarCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n     }\n \n     @Test\n     public void testFullJsonExtract()\n     {\n-        assertEquals(doJsonExtract(\"{}\", \"$\"), \"{}\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\"), \"{\\\"bar\\\":1}\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": null}\", \"$.fuu\"), \"null\");\n-        assertNull(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$.bar\"));\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"\\\"abc\\\"\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\"), \"[100,101]\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n+        assertEquals(doJsonExtractLegacy(\"{}\", \"$\"), \"{}\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\"), \"{\\\"bar\\\":1}\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": null}\", \"$.fuu\"), \"null\");\n+        assertNull(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$.bar\"));\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\"), \"[100,101]\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{}\", \"$\"), \"{}\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu\"), \"{\\\"bar\\\":1}\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$.fuu\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[fuu]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": null}\", \"$.fuu\"), \"null\");\n+        assertNull(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$.bar\"));\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$.bar\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$.fuu[0]\"), \"0.1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1]\"), \"[100,101]\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$.fuu[1][1]\"), \"101\");\n \n         // Test non-object extraction\n-        assertEquals(doJsonExtract(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n-        assertEquals(doJsonExtract(\"\\\"abc\\\"\", \"$\"), \"\\\"abc\\\"\");\n-        assertEquals(doJsonExtract(\"123\", \"$\"), \"123\");\n-        assertEquals(doJsonExtract(\"null\", \"$\"), \"null\");\n+        assertEquals(doJsonExtractLegacy(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n+        assertEquals(doJsonExtractLegacy(\"\\\"abc\\\"\", \"$\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonExtractLegacy(\"123\", \"$\"), \"123\");\n+        assertEquals(doJsonExtractLegacy(\"null\", \"$\"), \"null\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"[0, 1, 2]\", \"$[0]\"), \"0\");\n+        assertEquals(doJsonCanonicalizedExtract(\"\\\"abc\\\"\", \"$\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonCanonicalizedExtract(\"123\", \"$\"), \"123\");\n+        assertEquals(doJsonCanonicalizedExtract(\"null\", \"$\"), \"null\");\n \n         // Test extraction using bracket json path\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"]\"), \"{\\\"bar\\\":1}\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"][\\\"bar\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": null}\", \"$[\\\"fuu\\\"]\"), \"null\");\n-        assertNull(doJsonExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$[\\\"bar\\\"]\"), \"\\\"abc\\\"\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$[\\\"fuu\\\"][0]\"), \"0.1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1]\"), \"[100,101]\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1][1]\"), \"101\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"]\"), \"{\\\"bar\\\":1}\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"][\\\"bar\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": null}\", \"$[\\\"fuu\\\"]\"), \"null\");\n+        assertNull(doJsonExtractLegacy(\"{\\\"fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$[\\\"bar\\\"]\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$[\\\"fuu\\\"][0]\"), \"0.1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1]\"), \"[100,101]\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1][1]\"), \"101\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"]\"), \"{\\\"bar\\\":1}\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"][\\\"bar\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"fuu\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": null}\", \"$[\\\"fuu\\\"]\"), \"null\");\n+        assertNull(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": 1, \\\"bar\\\": \\\"abc\\\"}\", \"$[\\\"bar\\\"]\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0.1, 1, 2]}\", \"$[\\\"fuu\\\"][0]\"), \"0.1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1]\"), \"[100,101]\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu\\\"][1][1]\"), \"101\");\n \n         // Test extraction using bracket json path with special json characters in path\n-        assertEquals(doJsonExtract(\"{\\\"@$fuu\\\": {\\\".b.ar\\\": 1}}\", \"$[\\\"@$fuu\\\"]\"), \"{\\\".b.ar\\\":1}\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu..\\\": 1}\", \"$[\\\"fuu..\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fu*u\\\": null}\", \"$[\\\"fu*u\\\"]\"), \"null\");\n-        assertNull(doJsonExtract(\"{\\\",fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n-        assertEquals(doJsonExtract(\"{\\\",fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\",fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n-        assertEquals(doJsonExtract(\"{\\\":fu:u:\\\": 1, \\\":b:ar:\\\": \\\"abc\\\"}\", \"$[\\\":b:ar:\\\"]\"), \"\\\"abc\\\"\");\n-        assertEquals(doJsonExtract(\"{\\\"?()fuu\\\": [0.1, 1, 2]}\", \"$[\\\"?()fuu\\\"][0]\"), \"0.1\");\n-        assertEquals(doJsonExtract(\"{\\\"f?uu\\\": [0, [100, 101], 2]}\", \"$[\\\"f?uu\\\"][1]\"), \"[100,101]\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu()\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu()\\\"][1][1]\"), \"101\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"@$fuu\\\": {\\\".b.ar\\\": 1}}\", \"$[\\\"@$fuu\\\"]\"), \"{\\\".b.ar\\\":1}\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu..\\\": 1}\", \"$[\\\"fuu..\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fu*u\\\": null}\", \"$[\\\"fu*u\\\"]\"), \"null\");\n+        assertNull(doJsonExtractLegacy(\"{\\\",fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n+        assertEquals(doJsonExtractLegacy(\"{\\\",fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\",fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\":fu:u:\\\": 1, \\\":b:ar:\\\": \\\"abc\\\"}\", \"$[\\\":b:ar:\\\"]\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"?()fuu\\\": [0.1, 1, 2]}\", \"$[\\\"?()fuu\\\"][0]\"), \"0.1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"f?uu\\\": [0, [100, 101], 2]}\", \"$[\\\"f?uu\\\"][1]\"), \"[100,101]\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu()\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu()\\\"][1][1]\"), \"101\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"@$fuu\\\": {\\\".b.ar\\\": 1}}\", \"$[\\\"@$fuu\\\"]\"), \"{\\\".b.ar\\\":1}\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu..\\\": 1}\", \"$[\\\"fuu..\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fu*u\\\": null}\", \"$[\\\"fu*u\\\"]\"), \"null\");\n+        assertNull(doJsonCanonicalizedExtract(\"{\\\",fuu\\\": 1}\", \"$[\\\"bar\\\"]\"));\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\",fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\",fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\":fu:u:\\\": 1, \\\":b:ar:\\\": \\\"abc\\\"}\", \"$[\\\":b:ar:\\\"]\"), \"\\\"abc\\\"\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"?()fuu\\\": [0.1, 1, 2]}\", \"$[\\\"?()fuu\\\"][0]\"), \"0.1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"f?uu\\\": [0, [100, 101], 2]}\", \"$[\\\"f?uu\\\"][1]\"), \"[100,101]\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu()\\\": [0, [100, 101], 2]}\", \"$[\\\"fuu()\\\"][1][1]\"), \"101\");\n \n         // Test extraction using mix of bracket and dot notation json path\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"].bar\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu[\\\"bar\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n-        assertEquals(doJsonExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"].bar\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu[\\\"bar\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"fuu\\\"].bar\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": {\\\"bar\\\": 1}}\", \"$.fuu[\\\"bar\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$[\\\"fuu\\\"][0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"fuu\\\": [\\\"\\\\u0001\\\"]}\", \"$.fuu[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n \n         // Test extraction using  mix of bracket and dot notation json path with special json characters in path\n-        assertEquals(doJsonExtract(\"{\\\"@$fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"@$fuu\\\"].bar\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\",fuu\\\": {\\\"bar\\\": [\\\"\\\\u0001\\\"]}}\", \"$[\\\",fuu\\\"].bar[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+        assertEquals(doJsonExtractLegacy(\"{\\\"@$fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"@$fuu\\\"].bar\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\",fuu\\\": {\\\"bar\\\": [\\\"\\\\u0001\\\"]}}\", \"$[\\\",fuu\\\"].bar[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"@$fuu\\\": {\\\"bar\\\": 1}}\", \"$[\\\"@$fuu\\\"].bar\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\",fuu\\\": {\\\"bar\\\": [\\\"\\\\u0001\\\"]}}\", \"$[\\\",fuu\\\"].bar[0]\"), \"\\\"\\\\u0001\\\"\"); // Test escaped characters\n \n         // Test numeric path expression matches arrays and objects\n-        assertEquals(doJsonExtract(\"[0, 1, 2]\", \"$.1\"), \"1\");\n-        assertEquals(doJsonExtract(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n-        assertEquals(doJsonExtract(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"[0, 1, 2]\", \"$.1\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"[0, 1, 2]\", \"$.1\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"[0, 1, 2]\", \"$[1]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"[0, 1, 2]\", \"$[\\\"1\\\"]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$.1\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[1]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"0\\\" : 0, \\\"1\\\" : 1, \\\"2\\\" : 2, }\", \"$[\\\"1\\\"]\"), \"1\");\n \n         // Test fields starting with a digit\n-        assertEquals(doJsonExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n-        assertEquals(doJsonExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n+        assertEquals(doJsonExtractLegacy(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n+\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$.30day\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[30day]\"), \"1\");\n+        assertEquals(doJsonCanonicalizedExtract(\"{\\\"15day\\\" : 0, \\\"30day\\\" : 1, \\\"90day\\\" : 2, }\", \"$[\\\"30day\\\"]\"), \"1\");\n     }\n \n     @Test\n     public void testInvalidExtracts()\n     {\n-        assertInvalidExtract(\"\", \"\", \"Invalid JSON path: ''\");\n-        assertInvalidExtract(\"{}\", \"$.bar[2][-1]\", \"Invalid JSON path: '$.bar[2][-1]'\");\n-        assertInvalidExtract(\"{}\", \"$.fuu..bar\", \"Invalid JSON path: '$.fuu..bar'\");\n-        assertInvalidExtract(\"{}\", \"$.\", \"Invalid JSON path: '$.'\");\n-        assertInvalidExtract(\"\", \"$$\", \"Invalid JSON path: '$$'\");\n-        assertInvalidExtract(\"\", \" \", \"Invalid JSON path: ' '\");\n-        assertInvalidExtract(\"\", \".\", \"Invalid JSON path: '.'\");\n-        assertInvalidExtract(\"{ \\\"store\\\": { \\\"book\\\": [{ \\\"title\\\": \\\"title\\\" }] } }\", \"$.store.book[\", \"Invalid JSON path: '$.store.book['\");\n+        assertInvalidLegacyExtract(\"\", \"\", \"Invalid JSON path: ''\");\n+        assertInvalidLegacyExtract(\"{}\", \"$.bar[2][-1]\", \"Invalid JSON path: '$.bar[2][-1]'\");\n+        assertInvalidLegacyExtract(\"{}\", \"$.fuu..bar\", \"Invalid JSON path: '$.fuu..bar'\");\n+        assertInvalidLegacyExtract(\"{}\", \"$.\", \"Invalid JSON path: '$.'\");\n+        assertInvalidLegacyExtract(\"\", \"$$\", \"Invalid JSON path: '$$'\");\n+        assertInvalidLegacyExtract(\"\", \" \", \"Invalid JSON path: ' '\");\n+        assertInvalidLegacyExtract(\"\", \".\", \"Invalid JSON path: '.'\");\n+        assertInvalidLegacyExtract(\"{ \\\"store\\\": { \\\"book\\\": [{ \\\"title\\\": \\\"title\\\" }] } }\", \"$.store.book[\", \"Invalid JSON path: '$.store.book['\");\n+\n+        assertInvalidCanonicalizedExtract(\"\", \"\", \"Invalid JSON path: ''\");\n+        assertInvalidCanonicalizedExtract(\"{}\", \"$.bar[2][-1]\", \"Invalid JSON path: '$.bar[2][-1]'\");\n+        assertInvalidCanonicalizedExtract(\"{}\", \"$.fuu..bar\", \"Invalid JSON path: '$.fuu..bar'\");\n+        assertInvalidCanonicalizedExtract(\"{}\", \"$.\", \"Invalid JSON path: '$.'\");\n+        assertInvalidCanonicalizedExtract(\"\", \"$$\", \"Invalid JSON path: '$$'\");\n+        assertInvalidCanonicalizedExtract(\"\", \" \", \"Invalid JSON path: ' '\");\n+        assertInvalidCanonicalizedExtract(\"\", \".\", \"Invalid JSON path: '.'\");\n+        assertInvalidCanonicalizedExtract(\"{ \\\"store\\\": { \\\"book\\\": [{ \\\"title\\\": \\\"title\\\" }] } }\", \"$.store.book[\", \"Invalid JSON path: '$.store.book['\");\n     }\n \n     @Test\n@@ -344,22 +479,41 @@ public void testNoAutomaticEncodingDetection()\n         assertFunction(\"JSON_EXTRACT_SCALAR(UTF8(X'00 00 00 00 7b 22 72 22'), '$.x')\", VARCHAR, null);\n     }\n \n-    private static String doExtract(JsonExtractor<Slice> jsonExtractor, String json)\n+    private static String doExtractLegacy(JsonExtractor<Slice> jsonExtractor, String json)\n             throws IOException\n     {\n-        Slice extract = jsonExtractor.extract(Slices.utf8Slice(json).getInput());\n+        Slice extract = jsonExtractor.extract(Slices.utf8Slice(json).getInput(), PROPERTIES_CANONICALIZED_EXTRACT_DISABLED);\n         return (extract == null) ? null : extract.toStringUtf8();\n     }\n \n-    private static String doScalarExtract(String inputJson, String jsonPath)\n+    private static String doCanonicalizedExtract(JsonExtractor<Slice> jsonExtractor, String json)\n+            throws IOException\n     {\n-        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new ScalarValueJsonExtractor()));\n+        Slice extract = jsonExtractor.extract(Slices.utf8Slice(json).getInput(), PROPERTIES_CANONICALIZED_EXTRACT_ENABLED);\n+        return (extract == null) ? null : extract.toStringUtf8();\n+    }\n+\n+    private static String doScalarExtractLegacy(String inputJson, String jsonPath)\n+    {\n+        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new ScalarValueJsonExtractor()), PROPERTIES_CANONICALIZED_EXTRACT_DISABLED);\n         return (value == null) ? null : value.toStringUtf8();\n     }\n \n-    private static String doJsonExtract(String inputJson, String jsonPath)\n+    private static String doScalarCanonicalizedExtract(String inputJson, String jsonPath)\n     {\n-        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new JsonValueJsonExtractor()));\n+        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new ScalarValueJsonExtractor()), PROPERTIES_CANONICALIZED_EXTRACT_ENABLED);\n+        return (value == null) ? null : value.toStringUtf8();\n+    }\n+\n+    private static String doJsonExtractLegacy(String inputJson, String jsonPath)\n+    {\n+        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new JsonValueJsonExtractor()), PROPERTIES_CANONICALIZED_EXTRACT_DISABLED);\n+        return (value == null) ? null : value.toStringUtf8();\n+    }\n+\n+    private static String doJsonCanonicalizedExtract(String inputJson, String jsonPath)\n+    {\n+        Slice value = JsonExtract.extract(Slices.utf8Slice(inputJson), generateExtractor(jsonPath, new JsonValueJsonExtractor()), PROPERTIES_CANONICALIZED_EXTRACT_ENABLED);\n         return (value == null) ? null : value.toStringUtf8();\n     }\n \n@@ -368,10 +522,21 @@ private static List<String> tokenizePath(String path)\n         return ImmutableList.copyOf(new JsonPathTokenizer(path));\n     }\n \n-    private static void assertInvalidExtract(String inputJson, String jsonPath, String message)\n+    private static void assertInvalidLegacyExtract(String inputJson, String jsonPath, String message)\n+    {\n+        try {\n+            doJsonExtractLegacy(inputJson, jsonPath);\n+        }\n+        catch (PrestoException e) {\n+            assertEquals(e.getErrorCode(), INVALID_FUNCTION_ARGUMENT.toErrorCode());\n+            assertEquals(e.getMessage(), message);\n+        }\n+    }\n+\n+    private static void assertInvalidCanonicalizedExtract(String inputJson, String jsonPath, String message)\n     {\n         try {\n-            doJsonExtract(inputJson, jsonPath);\n+            doJsonCanonicalizedExtract(inputJson, jsonPath);\n         }\n         catch (PrestoException e) {\n             assertEquals(e.getErrorCode(), INVALID_FUNCTION_ARGUMENT.toErrorCode());\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java\nindex c349e2a47a8c6..b51294d9b6a2f 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestJsonExtractFunctions.java\n@@ -13,11 +13,15 @@\n  */\n package com.facebook.presto.operator.scalar;\n \n+import com.facebook.presto.sql.analyzer.FeaturesConfig;\n+import com.facebook.presto.sql.analyzer.FunctionsConfig;\n+import org.testng.annotations.BeforeClass;\n import org.testng.annotations.Test;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n import static com.facebook.presto.common.type.JsonType.JSON;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_FUNCTION_ARGUMENT;\n import static java.lang.String.format;\n \n public class TestJsonExtractFunctions\n@@ -61,6 +65,21 @@ public class TestJsonExtractFunctions\n             \"    \\\"expensive\\\": 10\\n\" +\n             \"}\";\n \n+    private static FunctionAssertions canonicalizedJsonExtractDisabled;\n+    private static FunctionAssertions canonicalizedJsonExtractEnabled;\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        registerScalar(getClass());\n+        FunctionsConfig featuresConfigWithCanonicalizedJsonExtractDisabled = new FunctionsConfig()\n+                .setCanonicalizedJsonExtract(false);\n+        canonicalizedJsonExtractDisabled = new FunctionAssertions(session, new FeaturesConfig(), featuresConfigWithCanonicalizedJsonExtractDisabled, true);\n+        FunctionsConfig featuresConfigWithCanonicalizedJsonExtractEnabled = new FunctionsConfig()\n+                .setCanonicalizedJsonExtract(true);\n+        canonicalizedJsonExtractEnabled = new FunctionAssertions(session, new FeaturesConfig(), featuresConfigWithCanonicalizedJsonExtractEnabled, true);\n+    }\n+\n     @Test\n     public void testJsonExtract()\n     {\n@@ -75,6 +94,16 @@ public void testJsonExtract()\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"INVALID_JSON\", \"$\"), JSON, null);\n         assertInvalidFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n \n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$\"), JSON, \"{\\\"x\\\":{\\\"a\\\":1,\\\"b\\\":2}}\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x\"), JSON, \"{\\\"a\\\":1,\\\"b\\\":2}\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x.a\"), JSON, \"1\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x.c\"), JSON, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : [2, 3]} }\", \"$.x.b[1]\"), JSON, \"3\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"[1,2,3]\", \"$[1]\"), JSON, \"2\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"[1,null,3]\", \"$[1]\"), JSON, \"null\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"INVALID_JSON\", \"$\"), JSON, null);\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_EXTRACT('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n+\n         // complex expressions (should run on Jayway)\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"$.store.book[*].isbn\"), JSON, \"[\\\"0-553-21311-3\\\",\\\"0-395-19395-8\\\"]\");\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"$..price\"), JSON, \"[8.95,12.99,8.99,22.99,19.95]\");\n@@ -83,10 +112,49 @@ public void testJsonExtract()\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"concat($..category)\"), JSON, \"\\\"referencefictionfictionfiction\\\"\");\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"$.store.keys()\"), JSON, \"[\\\"book\\\",\\\"bicycle\\\"]\");\n         assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"$.store.book[1].author\"), JSON, \"\\\"Evelyn Waugh\\\"\");\n-\n         assertInvalidFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, \"$...invalid\"), \"Invalid JSON path: '$...invalid'\");\n     }\n \n+    @Test\n+    public void testExtractJsonWithCanonicalOutput()\n+    {\n+        // Test with simple JSON object\n+        String json = \"{\\\"key_2\\\": 2, \\\"key_3\\\": 3, \\\"key_1\\\": 1}\";\n+        String path = \"$\";\n+        String expected = \"{\\\"key_1\\\":1,\\\"key_2\\\":2,\\\"key_3\\\":3}\";\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, expected);\n+\n+        // Test with nested JSON object\n+        json = \"{\\\"key_1\\\": {\\\"nested_key_2\\\": \\\"value_2\\\", \\\"nested_key_1\\\": \\\"value_1\\\"}, \\\"key_2\\\": 2}\";\n+        path = \"$.key_1\";\n+        expected = \"{\\\"nested_key_1\\\":\\\"value_1\\\",\\\"nested_key_2\\\":\\\"value_2\\\"}\";\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, expected);\n+\n+        // Test with Array of JSON objects\n+        json = \"[{\\\"key_b\\\":\\\"v_b\\\",\\\"key_a\\\":\\\"v_a\\\"}, {\\\"key_2\\\": \\\"value_2\\\"}]\";\n+        path = \"$[0]\";\n+        expected = \"{\\\"key_a\\\":\\\"v_a\\\",\\\"key_b\\\":\\\"v_b\\\"}\";\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, expected);\n+    }\n+\n+    @Test\n+    public void testInvalidFunctionIfJsonInvalid()\n+    {\n+        // Unbalanced quotes\n+        String json = \"{ \\\"key_2\\\": 2, \\\"key_1\\\": \\\"z\\\"a1\\\" }\";\n+        String path = \"$.key_1\";\n+        assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, \"\\\"z\\\"\");\n+        canonicalizedJsonExtractDisabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, \"\\\"z\\\"\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), INVALID_FUNCTION_ARGUMENT);\n+\n+        // Extra comma\n+        json = \"{ \\\"key_2\\\": 2, \\\"key_1\\\": \\\"value_1\\\", }\";\n+        path = \"$.key_1\";\n+        assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, \"\\\"value_1\\\"\");\n+        canonicalizedJsonExtractDisabled.assertFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), JSON, \"\\\"value_1\\\"\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_EXTRACT('%s', '%s')\", json, path), INVALID_FUNCTION_ARGUMENT);\n+    }\n+\n     @Test\n     public void testJsonSize()\n     {\n@@ -108,11 +176,33 @@ public void testJsonSize()\n         assertFunction(format(\"JSON_SIZE(null, '%s')\", \"$\"), BIGINT, null);\n         assertFunction(format(\"JSON_SIZE(JSON '%s', null)\", \"[1,2,3]\"), BIGINT, null);\n \n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$\"), BIGINT, 1L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x\"), BIGINT, 2L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : [1,2,3], \\\"c\\\" : {\\\"w\\\":9}} }\", \"$.x\"), BIGINT, 3L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x.a\"), BIGINT, 0L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"[1,2,3]\", \"$\"), BIGINT, 3L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', CHAR '%s')\", \"[1,2,3]\", \"$\"), BIGINT, 3L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(null, '%s')\", \"$\"), BIGINT, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", \"INVALID_JSON\", \"$\"), BIGINT, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', null)\", \"[1,2,3]\"), BIGINT, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$\"), BIGINT, 1L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x\"), BIGINT, 2L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : [1,2,3], \\\"c\\\" : {\\\"w\\\":9}} }\", \"$.x\"), BIGINT, 3L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x.a\"), BIGINT, 0L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', '%s')\", \"[1,2,3]\", \"$\"), BIGINT, 3L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(null, '%s')\", \"$\"), BIGINT, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE(JSON '%s', null)\", \"[1,2,3]\"), BIGINT, null);\n+\n         assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n         assertInvalidFunction(format(\"JSON_SIZE('%s', CHAR '%s')\", \"{\\\"\\\":\\\"\\\"}\", \" \"), \"Invalid JSON path: ' '\");\n         assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \".\"), \"Invalid JSON path: '.'\");\n         assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"...\"), \"Invalid JSON path: '...'\");\n \n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_SIZE('%s', CHAR '%s')\", \"{\\\"\\\":\\\"\\\"}\", \" \"), \"Invalid JSON path: ' '\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \".\"), \"Invalid JSON path: '.'\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"...\"), \"Invalid JSON path: '...'\");\n+\n         // complex expressions (should run on Jayway)\n         assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.book[*].isbn\"), BIGINT, 2L);\n         assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$..price\"), BIGINT, 5L);\n@@ -122,7 +212,16 @@ public void testJsonSize()\n         assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.keys()\"), BIGINT, 2L);\n         assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.book[1].author\"), BIGINT, 0L);\n \n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.book[*].isbn\"), BIGINT, 2L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$..price\"), BIGINT, 5L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.book[?(@.price < 10)].title\"), BIGINT, 2L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"max($..price)\"), BIGINT, 0L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"concat($..category)\"), BIGINT, 0L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.keys()\"), BIGINT, 2L);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$.store.book[1].author\"), BIGINT, 0L);\n+\n         assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$...invalid\"), \"Invalid JSON path: '$...invalid'\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_SIZE('%s', '%s')\", json, \"$...invalid\"), \"Invalid JSON path: '$...invalid'\");\n     }\n \n     @Test\n@@ -136,6 +235,13 @@ public void testJsonExtractScalar()\n         assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"[1,2,3]\", \"$[1]\"), VARCHAR, \"2\");\n         assertInvalidFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n \n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : 2} }\", \"$.x.a\"), VARCHAR, \"1\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"x\\\": {\\\"a\\\" : 1, \\\"b\\\" : [2, 3]} }\", \"$.x.b[1]\"), VARCHAR, \"3\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"[1,2,3]\", \"$[1]\"), VARCHAR, \"2\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", \"{\\\"\\\":\\\"\\\"}\", \"\"), \"Invalid JSON path: ''\");\n+\n         // complex expressions (should run on Jayway)\n         assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.book[*].isbn\"), VARCHAR, null);\n         assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$..price\"), VARCHAR, null);\n@@ -145,6 +251,15 @@ public void testJsonExtractScalar()\n         assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.keys()\"), VARCHAR, null);\n         assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.book[1].author\"), VARCHAR, \"Evelyn Waugh\");\n \n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.book[*].isbn\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$..price\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.book[?(@.price < 10)].title\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"max($..price)\"), VARCHAR, \"22.99\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"concat($..category)\"), VARCHAR, \"referencefictionfictionfiction\");\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.keys()\"), VARCHAR, null);\n+        canonicalizedJsonExtractEnabled.assertFunction(format(\"JSON_EXTRACT_SCALAR(JSON'%s', '%s')\", json, \"$.store.book[1].author\"), VARCHAR, \"Evelyn Waugh\");\n+\n         assertInvalidFunction(format(\"JSON_EXTRACT_SCALAR('%s', '%s')\", json, \"$...invalid\"), \"Invalid JSON path: '$...invalid'\");\n+        canonicalizedJsonExtractEnabled.assertInvalidFunction(format(\"JSON_EXTRACT_SCALAR('%s', '%s')\", json, \"$...invalid\"), \"Invalid JSON path: '$...invalid'\");\n     }\n }\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java\nindex e5f8cd752e2ab..ad7fc277d2658 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFunctionsConfig.java\n@@ -54,6 +54,7 @@ public void testDefaults()\n                 .setWarnOnCommonNanPatterns(false)\n                 .setLegacyCharToVarcharCoercion(false)\n                 .setLegacyJsonCast(true)\n+                .setCanonicalizedJsonExtract(false)\n                 .setDefaultNamespacePrefix(JAVA_BUILTIN_NAMESPACE.toString()));\n     }\n \n@@ -83,6 +84,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"deprecated.legacy-char-to-varchar-coercion\", \"true\")\n                 .put(\"legacy-json-cast\", \"false\")\n                 .put(\"presto.default-namespace\", \"native.default\")\n+                .put(\"canonicalized-json-extract\", \"true\")\n                 .build();\n \n         FunctionsConfig expected = new FunctionsConfig()\n@@ -107,7 +109,8 @@ public void testExplicitPropertyMappings()\n                 .setWarnOnCommonNanPatterns(true)\n                 .setLegacyCharToVarcharCoercion(true)\n                 .setLegacyJsonCast(false)\n-                .setDefaultNamespacePrefix(\"native.default\");\n+                .setDefaultNamespacePrefix(\"native.default\")\n+                .setCanonicalizedJsonExtract(true);\n         assertFullMapping(properties, expected);\n     }\n }\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java b/presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java\nindex 69b48113380d7..219b16b961383 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/gen/TestExpressionCompiler.java\n@@ -1446,17 +1446,17 @@ public void testFunctionCallJson()\n             for (String pattern : jsonPatterns) {\n                 assertExecute(generateExpression(\"json_extract(%s, %s)\", value, pattern),\n                         JSON,\n-                        value == null || pattern == null ? null : JsonFunctions.jsonExtract(utf8Slice(value), JsonPath.build(pattern)));\n+                        value == null || pattern == null ? null : JsonFunctions.jsonExtract(session.getSqlFunctionProperties(), utf8Slice(value), JsonPath.build(pattern)));\n                 assertExecute(generateExpression(\"json_extract_scalar(%s, %s)\", value, pattern),\n                         value == null ? createUnboundedVarcharType() : createVarcharType(value.length()),\n-                        value == null || pattern == null ? null : JsonFunctions.jsonExtractScalar(utf8Slice(value), JsonPath.build(pattern)));\n+                        value == null || pattern == null ? null : JsonFunctions.jsonExtractScalar(session.getSqlFunctionProperties(), utf8Slice(value), JsonPath.build(pattern)));\n \n                 assertExecute(generateExpression(\"json_extract(%s, %s || '')\", value, pattern),\n                         JSON,\n-                        value == null || pattern == null ? null : JsonFunctions.jsonExtract(utf8Slice(value), JsonPath.build(pattern)));\n+                        value == null || pattern == null ? null : JsonFunctions.jsonExtract(session.getSqlFunctionProperties(), utf8Slice(value), JsonPath.build(pattern)));\n                 assertExecute(generateExpression(\"json_extract_scalar(%s, %s || '')\", value, pattern),\n                         value == null ? createUnboundedVarcharType() : createVarcharType(value.length()),\n-                        value == null || pattern == null ? null : JsonFunctions.jsonExtractScalar(utf8Slice(value), JsonPath.build(pattern)));\n+                        value == null || pattern == null ? null : JsonFunctions.jsonExtractScalar(session.getSqlFunctionProperties(), utf8Slice(value), JsonPath.build(pattern)));\n             }\n         }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java b/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\nindex c21b4068f88ff..64f41b3a27c31 100644\n--- a/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\n+++ b/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\n@@ -78,6 +78,8 @@ public class TestRowOperators\n     private static FunctionAssertions fieldNameInJsonCastEnabled;\n     private static FunctionAssertions legacyJsonCastEnabled;\n     private static FunctionAssertions legacyJsonCastDisabled;\n+    private static FunctionAssertions canonicalizedJsonExtractDisabled;\n+    private static FunctionAssertions canonicalizedJsonExtractEnabled;\n \n     @BeforeClass\n     public void setUp()\n@@ -100,6 +102,13 @@ public void setUp()\n         FunctionsConfig featuresConfigWithLegacyJsonCastDisabled = new FunctionsConfig()\n                 .setLegacyJsonCast(false);\n         legacyJsonCastDisabled = new FunctionAssertions(session, new FeaturesConfig(), featuresConfigWithLegacyJsonCastDisabled, true);\n+\n+        FunctionsConfig featuresConfigWithCanonicalizedJsonExtractDisabled = new FunctionsConfig()\n+                .setCanonicalizedJsonExtract(false);\n+        canonicalizedJsonExtractDisabled = new FunctionAssertions(session, new FeaturesConfig(), featuresConfigWithCanonicalizedJsonExtractDisabled, true);\n+        FunctionsConfig featuresConfigWithCanonicalizedJsonExtractEnabled = new FunctionsConfig()\n+                .setCanonicalizedJsonExtract(true);\n+        canonicalizedJsonExtractEnabled = new FunctionAssertions(session, new FeaturesConfig(), featuresConfigWithCanonicalizedJsonExtractEnabled, true);\n     }\n \n     @AfterClass(alwaysRun = true)\n@@ -113,6 +122,10 @@ public final void tearDown()\n         legacyJsonCastEnabled = null;\n         legacyJsonCastDisabled.close();\n         legacyJsonCastDisabled = null;\n+        canonicalizedJsonExtractDisabled.close();\n+        canonicalizedJsonExtractDisabled = null;\n+        canonicalizedJsonExtractEnabled.close();\n+        canonicalizedJsonExtractEnabled = null;\n     }\n \n     @ScalarFunction\n@@ -539,6 +552,13 @@ public void testJsonToRow()\n         assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n                 \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: KEY1\\n\" +\n                         \"{\\\"1\\\":[{\\\"key1\\\":\\\"John\\\",\\\"KEY1\\\":\\\"Johnny\\\"}]}\");\n+        canonicalizedJsonExtractDisabled.assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n+                \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: KEY1\\n\" +\n+                        \"{\\\"1\\\":[{\\\"key1\\\":\\\"John\\\",\\\"KEY1\\\":\\\"Johnny\\\"}]}\");\n+        canonicalizedJsonExtractEnabled.assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n+                \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: key1\\n\" +\n+                        \"{\\\"1\\\":[{\\\"KEY1\\\":\\\"Johnny\\\",\\\"key1\\\":\\\"John\\\"}]}\");\n+\n         assertInvalidCast(\"CAST(unchecked_to_json('{\\\"a\\\":1,\\\"b\\\":2,\\\"a\\\":3}') AS ROW(a BIGINT, b BIGINT))\", \"Cannot cast to row(a bigint,b bigint). Duplicate field: a\\n{\\\"a\\\":1,\\\"b\\\":2,\\\"a\\\":3}\");\n         assertInvalidCast(\"CAST(unchecked_to_json('[{\\\"a\\\":1,\\\"b\\\":2,\\\"a\\\":3}]') AS ARRAY<ROW(a BIGINT, b BIGINT)>)\", \"Cannot cast to array(row(a bigint,b bigint)). Duplicate field: a\\n[{\\\"a\\\":1,\\\"b\\\":2,\\\"a\\\":3}]\");\n     }\n@@ -780,4 +800,18 @@ private void assertComparisonCombination(String base, String greater)\n             assertFunction(greater + operator + base, BOOLEAN, greaterOrInequalityOperators.contains(operator));\n         }\n     }\n+\n+    @Test\n+    public void testRowNestedJsonExtractNullVarchar()\n+    {\n+        assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n+                \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: KEY1\\n\" +\n+                        \"{\\\"1\\\":[{\\\"key1\\\":\\\"John\\\",\\\"KEY1\\\":\\\"Johnny\\\"}]}\");\n+        canonicalizedJsonExtractDisabled.assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n+                \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: KEY1\\n\" +\n+                        \"{\\\"1\\\":[{\\\"key1\\\":\\\"John\\\",\\\"KEY1\\\":\\\"Johnny\\\"}]}\");\n+        canonicalizedJsonExtractEnabled.assertInvalidCast(\"CAST(json_extract('{\\\"1\\\":[{\\\"key1\\\": \\\"John\\\", \\\"KEY1\\\":\\\"Johnny\\\"}]}', '$') AS MAP<bigint, ARRAY<ROW(key1 VARCHAR)>>)\",\n+                \"Cannot cast to map(bigint,array(row(key1 varchar))). Duplicate field: key1\\n\" +\n+                        \"{\\\"1\\\":[{\\\"KEY1\\\":\\\"Johnny\\\",\\\"key1\\\":\\\"John\\\"}]}\");\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24600",
    "pr_id": 24600,
    "issue_id": 24592,
    "repo": "prestodb/presto",
    "problem_statement": "approx_percentile: Does not enforce constant percentage between input rows within an aggregation\nIn Meta production environment, we noticed failures in the aggregate function approx_percentile with Presto-cpp and not Presto-Java. Specifically, certain queries in Presto-cpp throws an exception like `VeloxUserError: base->equalValueAt(base, baseRow, baseFirstRow) Percentile argument must be constant for all input rows`.\n\nAccording to the [Presto documentation](https://prestodb.io/docs/current/functions/aggregate.html?fbclid=IwZXh0bgNhZW0CMTEAAR237GxLTPUu_MnyCP0GpsZB32GU77AVGC7ou_VyAtZ0cUVnSx54p3TDX_4_aem_5pQeH14aH3_ScK2jV_yGSQ#approx_percentile-x-percentage-same-as-x), this is actually the desired behavior.\n\nHowever, in Presto-Java, it seems like the last percentile value amongst the rows is respected and all of the previous percentiles are ignored. This goes against the intended behavior described by the documentation. \n\n\n## Presto-Java Behavior\n```\nSELECT \n  APPROX_PERCENTILE(score, pct) \nFROM \n  (\n    SELECT \n      * \n    FROM \n      (\n        VALUES \n          (0.01, 0.1), \n          (0.02, 0.5), \n          (0.03, 0.9)\n      ) AS t(score, pct)\n  )\n\n _col0 \n-------\n  0.03 \n(1 row)\n\n\nSELECT \n  APPROX_PERCENTILE(score, pct) \nFROM \n  (\n    SELECT \n      * \n    FROM \n      (\n        VALUES \n          (0.01, 0.9), \n          (0.02, 0.5), \n          (0.03, 0.1)\n      ) AS t(score, pct)\n  )\n _col0 \n-------\n  0.01 \n(1 row)\n\nSELECT \n  APPROX_PERCENTILE(score, pct) \nFROM \n  (\n    SELECT \n      * \n    FROM \n      (\n        VALUES \n          (0.01, 0.9), \n          (0.02, 0.1), \n          (0.03, 0.5)\n      ) AS t(score, pct)\n  )\n _col0 \n-------\n0.02\n```\n\n## Presto-C++ Behavior\n```\nSELECT \n  APPROX_PERCENTILE(score, pct) \nFROM \n  (\n    SELECT \n      * \n    FROM \n      (\n        VALUES \n          (0.01, 0.1), \n          (0.02, 0.5), \n          (0.03, 0.9)\n      ) AS t(score, pct)\n  )\n\nQuery 20250218_202100_07481_tj3cr failed: base->equalValueAt(base, baseRow, baseFirstRow) Percentile argument must be constant for all input rows: 0.5 vs. 0.1\n```\nThe behavior in Java actually seems unintentional and may produce results that the user is not knowingly expecting. We are raising this issue to the community to solicit ideas for how we can move forward. \n\nThanks!\n\ncc @spershin @rschlussel @kgpai @amitkdutta @Yuhta @kagamiori \n\n",
    "issue_word_count": 297,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateLongPercentileAggregations.java",
      "presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java"
    ],
    "base_commit": "a766a3a1ecd5fa45426977bc05e707da5d1c138c",
    "head_commit": "b271822af02d97f5660f9063d86c72965e7317fd",
    "repo_url": "https://github.com/prestodb/presto/pull/24600",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24600",
    "dockerfile": "",
    "pr_merged_at": "2025-02-21T15:21:59.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateLongPercentileAggregations.java b/presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateLongPercentileAggregations.java\nindex d013387b68da2..aee65e90990c1 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateLongPercentileAggregations.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateLongPercentileAggregations.java\n@@ -92,16 +92,15 @@ private static void addInput(\n             checkAccuracy(accuracy);\n             digest = new QuantileDigest(accuracy);\n             state.setDigest(digest);\n+            state.setPercentile(percentile);\n         }\n         else {\n             state.addMemoryUsage(-digest.estimatedInMemorySizeInBytes());\n         }\n \n+        checkPercentile(percentile, state.getPercentile());\n         digest.add(value, weight);\n         state.addMemoryUsage(digest.estimatedInMemorySizeInBytes());\n-\n-        // use last percentile\n-        state.setPercentile(percentile);\n     }\n \n     @CombineFunction\n@@ -137,6 +136,11 @@ public static void output(@AggregationState DigestAndPercentileState state, Bloc\n         }\n     }\n \n+    static void checkPercentile(double percentile, double statePercentile)\n+    {\n+        checkCondition(percentile == statePercentile, INVALID_FUNCTION_ARGUMENT, \"Percentile argument must be constant for all input rows: %s vs. %s\", percentile, statePercentile);\n+    }\n+\n     static void checkAccuracy(double accuracy)\n     {\n         checkCondition(0 < accuracy && accuracy < 1, INVALID_FUNCTION_ARGUMENT, \"Percentile accuracy must be strictly between 0 and 1\");\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java b/presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java\nindex 719fcecd91bee..c54153ff35a7a 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/aggregation/TestApproximatePercentileAggregation.java\n@@ -19,6 +19,7 @@\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.metadata.FunctionAndTypeManager;\n import com.facebook.presto.metadata.MetadataManager;\n+import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.function.JavaAggregationFunctionImplementation;\n import com.google.common.collect.ImmutableList;\n import org.testng.annotations.Test;\n@@ -495,6 +496,16 @@ public void testDoublePartialStep()\n                 createRLEBlock(ImmutableList.of(0.5, 0.8), 3));\n     }\n \n+    @Test(expectedExceptions = PrestoException.class, expectedExceptionsMessageRegExp = \"Percentile argument must be constant for all input rows: 0.3 vs. 0.1\")\n+    public void testNonConstantPercentile()\n+    {\n+        assertAggregation(\n+                DOUBLE_APPROXIMATE_PERCENTILE_AGGREGATION,\n+                null,\n+                createDoublesBlock(1.0, 2.0, 3.0),\n+                createDoublesBlock(0.1, 0.3, 0.5));\n+    }\n+\n     private static JavaAggregationFunctionImplementation getAggregation(Type... arguments)\n     {\n         return FUNCTION_AND_TYPE_MANAGER.getJavaAggregateFunctionImplementation(FUNCTION_AND_TYPE_MANAGER.lookupFunction(\"approx_percentile\", fromTypes(arguments)));\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24583",
    "pr_id": 24583,
    "issue_id": 24371,
    "repo": "prestodb/presto",
    "problem_statement": "[native] Iceberg read from partitioned Date column fails\nIf an Iceberg table has been created with Presto and a paritioned date column then reading using PrestoC++ fails.\n\n```\nVeloxUserError:  Unable to parse date value: \"19091\". Valid date string pattern is (YYYY-MM-DD), and can be prefixed with [+-] \n```\n\n\n## Your Environment\n<!--- Include as many relevant details about the environment you experienced the bug in -->\n* Presto version used: Latest\n* Storage (HDFS/S3/GCS..):\n* Data source and connector used:\n* Deployment (Cloud or On-prem):\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\n\n## Expected Behavior\n<!--- Tell us what should happen -->\n\n## Current Behavior\n<!--- Tell us what happens instead of the expected behavior -->\n\n## Possible Solution\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\n\n## Steps to Reproduce\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\nIt is easy to write a E2E test using the existing Iceberg tests.\nI repro'ed the issue by modifying TestPrestoNativeIcebergGeneralQueries\n\n1. Setup:\n```\nCREATE TABLE nation_partitioned_ds(nationkey BIGINT, name VARCHAR, comment VARCHAR, regionkey VARCHAR, ds DATE) WITH (partitioning = ARRAY['ds']);\nINSERT INTO nation_partitioned_ds SELECT nationkey, name, comment, cast(regionkey as VARCHAR), date('2022-04-09') FROM tpch.tiny.nation;\nINSERT INTO nation_partitioned_ds SELECT nationkey, name, comment, cast(regionkey as VARCHAR), date('2022-03-18') FROM tpch.tiny.nation;\n```\n\n2. \n```\nSELECT * FROM nation_partitioned_ds WHERE ds >= date ('1994-01-01');\n```\n\nError:\n```\nCaused by: VeloxUserError:  Unable to parse date value: \"19091\". Valid date string pattern is (YYYY-MM-DD), and can be prefixed with [+-] Split [Hive: file:/Users/czentgr/presto/data/iceberg_data/PARQUET/HIVE/tpch/nation_partitioned_ds/data/ds=2022-04-09/43f69c27-a33c-45b7-a22a-d6bfff92ab4c.parquet 0 - 3027] Task 20250115_224840_00000_mspsp.1.0.0.0\n```\n\nGit diff:\n\n```\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java\n@@ -42,6 +42,7 @@ public class TestPrestoNativeIcebergGeneralQueries\n     protected void createTables()\n     {\n         createTableToTestHiddenColumns();\n+        createIcebergPartitionedNation();\n     }\n\n     private void createTableToTestHiddenColumns()\n@@ -53,6 +54,17 @@ public class TestPrestoNativeIcebergGeneralQueries\n         }\n     }\n\n+    private void createIcebergPartitionedNation()\n+    {\n+        QueryRunner queryRunner = ((QueryRunner) getExpectedQueryRunner());\n+        queryRunner.execute(queryRunner.getDefaultSession(), \"DROP TABLE IF EXISTS nation_partitioned_ds\");\n+        if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"nation_partitioned_ds\")) {\n+            queryRunner.execute(\"CREATE TABLE nation_partitioned_ds(nationkey BIGINT, name VARCHAR, comment VARCHAR, regionkey VARCHAR, ds DATE) WITH (partitioning = ARRAY['ds'])\");\n+            queryRunner.execute(\"INSERT INTO nation_partitioned_ds SELECT nationkey, name, comment, cast(regionkey as VARCHAR), date('2022-04-09') FROM tpch.tiny.nation\");\n+            queryRunner.execute(\"INSERT INTO nation_partitioned_ds SELECT nationkey, name, comment, cast(regionkey as VARCHAR), date('2022-03-18') FROM tpch.tiny.nation\");\n+        }\n+    }\n+\n     @Test\n     public void testPathHiddenColumn()\n     {\n@@ -94,4 +106,10 @@ public class TestPrestoNativeIcebergGeneralQueries\n                         .getOnlyValue(),\n                 0L);\n     }\n+\n+    @Test\n+    public void testPartitionedDateColumn()\n+    {\n+        assertQuery(\"SELECT * FROM nation_partitioned_ds WHERE ds >= date ('1994-01-01')\");\n+    }\n }\n```\n\n\n\n## Screenshots (if appropriate)\n\n## Context\n<!--- How has this issue affected you? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\n\n",
    "issue_word_count": 478,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java"
    ],
    "base_commit": "a766a3a1ecd5fa45426977bc05e707da5d1c138c",
    "head_commit": "6d88a9e5f3c672b2dd5a3ba4475f88b67fbceefe",
    "repo_url": "https://github.com/prestodb/presto/pull/24583",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24583",
    "dockerfile": "",
    "pr_merged_at": "2025-02-25T21:44:54.000Z",
    "patch": "diff --git a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\nindex 38c8e785663ad..e370ebdd45d5a 100644\n--- a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\n+++ b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\n@@ -1409,12 +1409,18 @@ IcebergPrestoToVeloxConnector::toVeloxColumnHandle(\n   // TODO(imjalpreet): Modify 'hiveType' argument of the 'HiveColumnHandle'\n   //  constructor similar to how Hive Connector is handling for bucketing\n   velox::type::fbhive::HiveTypeParser hiveTypeParser;\n+  auto type = stringToType(icebergColumn->type, typeParser);\n+  connector::hive::HiveColumnHandle::ColumnParseParameters columnParseParameters;\n+  if (type->isDate()) {\n+    columnParseParameters.partitionDateValueFormat = connector::hive::HiveColumnHandle::ColumnParseParameters::kDaysSinceEpoch;\n+  }\n   return std::make_unique<connector::hive::HiveColumnHandle>(\n       icebergColumn->columnIdentity.name,\n       toHiveColumnType(icebergColumn->columnType),\n-      stringToType(icebergColumn->type, typeParser),\n-      stringToType(icebergColumn->type, typeParser),\n-      toRequiredSubfields(icebergColumn->requiredSubfields));\n+      type,\n+      type,\n+      toRequiredSubfields(icebergColumn->requiredSubfields),\n+      columnParseParameters);\n }\n \n std::unique_ptr<velox::connector::ConnectorTableHandle>\n",
    "test_patch": "diff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java\nindex 45f6613f41e95..59ce312ce4584 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeIcebergGeneralQueries.java\n@@ -41,16 +41,24 @@ protected ExpectedQueryRunner createExpectedQueryRunner()\n     @Override\n     protected void createTables()\n     {\n-        createTableToTestHiddenColumns();\n+        createTestTables();\n     }\n \n-    private void createTableToTestHiddenColumns()\n+    private void createTestTables()\n     {\n         QueryRunner javaQueryRunner = ((QueryRunner) getExpectedQueryRunner());\n-        if (!javaQueryRunner.tableExists(getSession(), \"test_hidden_columns\")) {\n-            javaQueryRunner.execute(\"CREATE TABLE test_hidden_columns AS SELECT * FROM tpch.tiny.region WHERE regionkey=0\");\n-            javaQueryRunner.execute(\"INSERT INTO test_hidden_columns SELECT * FROM tpch.tiny.region WHERE regionkey=1\");\n-        }\n+\n+        javaQueryRunner.execute(\"DROP TABLE IF EXISTS test_hidden_columns\");\n+        javaQueryRunner.execute(\"CREATE TABLE test_hidden_columns AS SELECT * FROM tpch.tiny.region WHERE regionkey=0\");\n+        javaQueryRunner.execute(\"INSERT INTO test_hidden_columns SELECT * FROM tpch.tiny.region WHERE regionkey=1\");\n+\n+        javaQueryRunner.execute(\"DROP TABLE IF EXISTS ice_table_partitioned\");\n+        javaQueryRunner.execute(\"CREATE TABLE ice_table_partitioned(c1 INT, ds DATE) WITH (partitioning = ARRAY['ds'])\");\n+        javaQueryRunner.execute(\"INSERT INTO ice_table_partitioned VALUES(1, date'2022-04-09'), (2, date'2022-03-18'), (3, date'1993-01-01')\");\n+\n+        javaQueryRunner.execute(\"DROP TABLE IF EXISTS ice_table\");\n+        javaQueryRunner.execute(\"CREATE TABLE ice_table(c1 INT, ds DATE)\");\n+        javaQueryRunner.execute(\"INSERT INTO ice_table VALUES(1, date'2022-04-09'), (2, date'2022-03-18'), (3, date'1993-01-01')\");\n     }\n \n     @Test\n@@ -94,4 +102,11 @@ public void testDataSequenceNumberHiddenColumn()\n                         .getOnlyValue(),\n                 0L);\n     }\n+\n+    @Test\n+    public void testDateQueries()\n+    {\n+        assertQuery(\"SELECT * FROM ice_table_partitioned WHERE ds >= date'1994-01-01'\", \"VALUES (1, date'2022-04-09'), (2, date'2022-03-18')\");\n+        assertQuery(\"SELECT * FROM ice_table WHERE ds = date'2022-04-09'\", \"VALUES (1, date'2022-04-09')\");\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24570",
    "pr_id": 24570,
    "issue_id": 23150,
    "repo": "prestodb/presto",
    "problem_statement": "Inconsistent error throwing of array and map functions\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\nSome array and map functions throw exceptions when the input arrays contain nested NULL elements, e.g., \"arrays with NULL elements are not supported for comparison\". These exceptions are not suppressed by try() except the one from array_sort(). Below is a list of these functions and the example queries:\r\n\r\n| Functions | Example queries | Result in Presto-0.289 | Result when wrapped in try() |\r\n|-----------|-----------------|------------------------|-------------------------------|\r\n| contains | select contains(c0, c1) from (values (array[array[1], array[null]], array[null])) t(c0, c1) | throws | throws |\r\n| array_remove | select array_remove(c0, c1) from (values (array[array[1], array[null]], array[null])) t(c0, c1) | throws | throws |\r\n| array_min/array_max | select array_max(c0) from (values (array[array[1], array[null]], array[null])) t(c0, c1) | throws | throws |\r\n| array_position | select array_position(c0, c1) from (values (array[array[1], array[null]], array[null])) t(c0, c1) | throws | throws |\r\n| transform_keys | select transform_keys(c0, (k, v) -> array[null]) from (values (map(array[1, 2], array[3, 4]))) t(c0) | throws | throws |\r\n| array_distinct | select array_distinct(c0) from (values (array[array[1, null], array[1, null]])) t(c0) | [[1,null]] | [[1,null]] |\r\n| array_union | select array_union(c0, c1) from (values (array[array[1], array[null]], array[array[null]])) t(c0, c1) | [[1],[null]] | [[1],[null]] |\r\n| array_sort | select array_sort(c0) from (values (array[array[1], array[null]])) t(c0) | throws | NULL |\r\n| map_subset | select map_subset(c0, c1) from (values (map(array[array[1, 2]], array[1]), array[array[1, null], array[1, null]])) t(c0, c1) | throws | throws |\r\n\r\nI'd expect that \r\n(1) Array and map functions treat NULL elements in a consistent way, e.g., treat NULL elements as indeterminate during comparisons.\r\n(2) The exceptions are treated in a consistent way in try(). Ideally, I'd expect try() to be able to suppress all these exceptions.\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: 0.289-edge8.1\r\n* Storage (HDFS/S3/GCS..):\r\n* Data source and connector used:\r\n* Deployment (Cloud or On-prem):\r\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\n\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n",
    "issue_word_count": 457,
    "test_files_count": 3,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-common/src/main/java/com/facebook/presto/common/type/TypeUtils.java",
      "presto-main/src/main/java/com/facebook/presto/type/TypeUtils.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "base_commit": "4a0bbc9bbb22362a5c60ccf22b50c3a69058fd58",
    "head_commit": "8f539478591bdcb6c1377fbffa6d398a1c10d606",
    "repo_url": "https://github.com/prestodb/presto/pull/24570",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24570",
    "dockerfile": "",
    "pr_merged_at": "2025-02-19T17:53:04.000Z",
    "patch": "diff --git a/presto-common/src/main/java/com/facebook/presto/common/type/TypeUtils.java b/presto-common/src/main/java/com/facebook/presto/common/type/TypeUtils.java\nindex 2a0144f1bf702..6395ff46befb6 100644\n--- a/presto-common/src/main/java/com/facebook/presto/common/type/TypeUtils.java\n+++ b/presto-common/src/main/java/com/facebook/presto/common/type/TypeUtils.java\n@@ -13,7 +13,7 @@\n  */\n package com.facebook.presto.common.type;\n \n-import com.facebook.presto.common.NotSupportedException;\n+import com.facebook.presto.common.InvalidFunctionArgumentException;\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.block.BlockBuilder;\n import io.airlift.slice.Slice;\n@@ -189,7 +189,7 @@ public static boolean isFloatingPointNaN(Type type, Object value)\n     static void checkElementNotNull(boolean isNull, String errorMsg)\n     {\n         if (isNull) {\n-            throw new NotSupportedException(errorMsg);\n+            throw new InvalidFunctionArgumentException(errorMsg);\n         }\n     }\n \n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/type/TypeUtils.java b/presto-main/src/main/java/com/facebook/presto/type/TypeUtils.java\nindex 72b3c7f3c4d3c..76aff971f2362 100644\n--- a/presto-main/src/main/java/com/facebook/presto/type/TypeUtils.java\n+++ b/presto-main/src/main/java/com/facebook/presto/type/TypeUtils.java\n@@ -34,6 +34,7 @@\n import java.util.List;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_FUNCTION_ARGUMENT;\n import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Throwables.throwIfUnchecked;\n@@ -174,7 +175,7 @@ public static Page getHashPage(Page page, List<? extends Type> types, List<Integ\n     public static void checkElementNotNull(boolean isNull, String errorMsg)\n     {\n         if (isNull) {\n-            throw new PrestoException(NOT_SUPPORTED, errorMsg);\n+            throw new PrestoException(INVALID_FUNCTION_ARGUMENT, errorMsg);\n         }\n     }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java b/presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java\nindex ebd6946d51e93..64ed4a5fc4921 100644\n--- a/presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java\n+++ b/presto-main/src/test/java/com/facebook/presto/type/TestArrayOperators.java\n@@ -710,13 +710,13 @@ public void testArrayMinWithNullsInBothArraysNotComparedSecondIsMin()\n     @Test\n     public void testArrayMinWithNullInFirstArrayIsCompared()\n     {\n-        assertInvalidFunction(\"ARRAY_MIN(ARRAY [ARRAY[1, NULL], ARRAY[1, 2]])\", NOT_SUPPORTED);\n+        assertInvalidFunction(\"ARRAY_MIN(ARRAY [ARRAY[1, NULL], ARRAY[1, 2]])\", INVALID_FUNCTION_ARGUMENT);\n     }\n \n     @Test\n     public void testArrayMinWithNullInSecondArrayIsCompared()\n     {\n-        assertInvalidFunction(\"ARRAY_MIN(ARRAY [ARRAY[1, 2], ARRAY[1, NULL]])\", NOT_SUPPORTED);\n+        assertInvalidFunction(\"ARRAY_MIN(ARRAY [ARRAY[1, 2], ARRAY[1, NULL]])\", INVALID_FUNCTION_ARGUMENT);\n     }\n \n     @Test\n@@ -764,13 +764,13 @@ public void testArrayMaxWithNullsInBothArraysNotComparedFirstIsMax()\n     @Test\n     public void testArrayMaxWithNullInFirstArrayIsCompared()\n     {\n-        assertInvalidFunction(\"ARRAY_MAX(ARRAY [ARRAY[1, NULL], ARRAY[1, 2]])\", NOT_SUPPORTED);\n+        assertInvalidFunction(\"ARRAY_MAX(ARRAY [ARRAY[1, NULL], ARRAY[1, 2]])\", INVALID_FUNCTION_ARGUMENT);\n     }\n \n     @Test\n     public void testArrayMaxWithNullInSecondArrayIsCompared()\n     {\n-        assertInvalidFunction(\"ARRAY_MAX(ARRAY [ARRAY[1, 2], ARRAY[1, NULL]])\", NOT_SUPPORTED);\n+        assertInvalidFunction(\"ARRAY_MAX(ARRAY [ARRAY[1, 2], ARRAY[1, NULL]])\", INVALID_FUNCTION_ARGUMENT);\n     }\n \n     @Test\n@@ -1145,7 +1145,7 @@ public void testSort()\n         assertInvalidFunction(\n                 \"ARRAY_SORT(ARRAY[ARRAY[1], ARRAY[null]])\",\n                 INVALID_FUNCTION_ARGUMENT,\n-                \"Array contains elements not supported for comparison\");\n+                \"ARRAY comparison not supported for arrays with null elements\");\n         assertInvalidFunction(\n                 \"ARRAY_SORT(ARRAY[ROW(1), ROW(null)])\",\n                 INVALID_FUNCTION_ARGUMENT,\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java b/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\nindex 0da10ccbd90aa..c21b4068f88ff 100644\n--- a/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\n+++ b/presto-main/src/test/java/com/facebook/presto/type/TestRowOperators.java\n@@ -692,7 +692,7 @@ public void testRowComparison()\n         assertInvalidFunction(\"row(TRUE, ARRAY [1, 2], MAP(ARRAY[1, 3], ARRAY[2.0E0, 4.0E0])) > row(TRUE, ARRAY [1, 2], MAP(ARRAY[1, 3], ARRAY[2.0E0, 4.0E0]))\",\n                 SemanticErrorCode.TYPE_MISMATCH, \"line 1:64: '>' cannot be applied to row(boolean,array(integer),map(integer,double)), row(boolean,array(integer),map(integer,double))\");\n \n-        assertInvalidFunction(\"row(1, CAST(NULL AS INTEGER)) < row(1, 2)\", StandardErrorCode.NOT_SUPPORTED);\n+        assertInvalidFunction(\"row(1, CAST(NULL AS INTEGER)) < row(1, 2)\", StandardErrorCode.INVALID_FUNCTION_ARGUMENT);\n \n         assertComparisonCombination(\"row(1.0E0, ARRAY [1,2,3], row(2, 2.0E0))\", \"row(1.0E0, ARRAY [1,3,3], row(2, 2.0E0))\");\n         assertComparisonCombination(\"row(TRUE, ARRAY [1])\", \"row(TRUE, ARRAY [1, 2])\");\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\nindex 080c530875095..f303dd07651b3 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n@@ -3234,6 +3234,36 @@ public void testTry()\n \n         // test try with null\n         assertQuery(\"SELECT TRY(1 / x) FROM (SELECT NULL as x)\", \"SELECT NULL\");\n+\n+        // Test try with map method and value parameter is optional and argument is an array with null,\n+        // the error should be suppressed and just return null.\n+        assertQuery(\"SELECT\\n\" +\n+                \"    TRY(map_keys_by_top_n_values(c0, BIGINT '6455219767830808341'))\\n\" +\n+                \"FROM (\\n\" +\n+                \"    VALUES\\n\" +\n+                \"        MAP(\\n\" +\n+                \"            ARRAY[1, 2], ARRAY[\\n\" +\n+                \"                ARRAY[1, null],\\n\" +\n+                \"                ARRAY[1, null]\\n\" +\n+                \"            ]\\n\" +\n+                \"        )\\n\" +\n+                \") t(c0)\", \"SELECT NULL\");\n+\n+        assertQuery(\"SELECT\\n\" +\n+                \"    TRY(map_keys_by_top_n_values(c0, BIGINT '6455219767830808341'))\\n\" +\n+                \"FROM (\\n\" +\n+                \"    VALUES\\n\" +\n+                \"        MAP(\\n\" +\n+                \"            ARRAY[1, 2], ARRAY[\\n\" +\n+                \"                ARRAY[null, null],\\n\" +\n+                \"                ARRAY[1, 2]\\n\" +\n+                \"            ]\\n\" +\n+                \"        )\\n\" +\n+                \") t(c0)\", \"SELECT NULL\");\n+\n+        // Test try with array method with an input array containing null values.\n+        // the error should be suppressed and just return null.\n+        assertQuery(\"SELECT TRY(ARRAY_MAX(ARRAY [ARRAY[1, NULL], ARRAY[1, 2]]))\", \"SELECT NULL\");\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24555",
    "pr_id": 24555,
    "issue_id": 22907,
    "repo": "prestodb/presto",
    "problem_statement": "Flaky test TestParquetReader.testStructOfTwoNestedArrays\nError:  Tests run: 3131, Failures: 1, Errors: 0, Skipped: 90, Time elapsed: 2,626.714 s <<< FAILURE! - in TestSuite\r\nError:  com.facebook.presto.hive.parquet.TestParquetReader.testStructOfTwoNestedArrays  Time elapsed: 0.238 s  <<< FAILURE!\r\njava.lang.NullPointerException\r\n\tat com.facebook.presto.parquet.reader.ParquetColumnChunk.readDictionaryPage(ParquetColumnChunk.java:241)\r\n\tat com.facebook.presto.parquet.reader.ParquetColumnChunk.buildPageReader(ParquetColumnChunk.java:116)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.createPageReaderInternal(ParquetReader.java:494)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.createPageReader(ParquetReader.java:487)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readPrimitive(ParquetReader.java:366)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readColumnChunk(ParquetReader.java:550)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readArray(ParquetReader.java:273)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readColumnChunk(ParquetReader.java:547)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readArray(ParquetReader.java:273)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readColumnChunk(ParquetReader.java:547)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readStruct(ParquetReader.java:309)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readColumnChunk(ParquetReader.java:541)\r\n\tat com.facebook.presto.parquet.reader.ParquetReader.readBlock(ParquetReader.java:533)\r\n\tat com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:230)\r\n\tat com.facebook.presto.hive.parquet.ParquetPageSource$ParquetBlockLoader.load(ParquetPageSource.java:208)\r\n\tat com.facebook.presto.common.block.LazyBlock.assureLoaded(LazyBlock.java:313)\r\n\tat com.facebook.presto.common.block.LazyBlock.isNull(LazyBlock.java:277)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.decodeObject(ParquetTester.java:811)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.assertPageSource(ParquetTester.java:546)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.assertPageSource(ParquetTester.java:530)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.assertFileContents(ParquetTester.java:522)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.assertRoundTrip(ParquetTester.java:391)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.testRoundTripType(ParquetTester.java:321)\r\n\tat com.facebook.presto.hive.parquet.ParquetTester.testRoundTrip(ParquetTester.java:223)\r\n\tat com.facebook.presto.hive.parquet.AbstractTestParquetReader.testStructOfTwoNestedArrays(AbstractTestParquetReader.java:787)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
    "issue_word_count": 457,
    "test_files_count": 1,
    "non_test_files_count": 20,
    "pr_changed_files": [
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ArrayColumnWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ColumnWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/MapColumnWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/PrimitiveColumnWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/StructColumnWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BigintValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BooleanValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/CharValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DateValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DecimalValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DoubleValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/IntegerValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/PrimitiveValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/RealValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimeValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/UuidValuesWriter.java",
      "presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java"
    ],
    "pr_changed_test_files": [
      "presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java"
    ],
    "base_commit": "7bee8b3c539fb70e79c5a81843ff4e1636c15294",
    "head_commit": "54e2e52576e11b395d59a43e25bff7c4a8026e36",
    "repo_url": "https://github.com/prestodb/presto/pull/24555",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24555",
    "dockerfile": "",
    "pr_merged_at": "2025-03-25T01:45:38.000Z",
    "patch": "diff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ArrayColumnWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ArrayColumnWriter.java\nindex dd6141c637faf..b932fb2f566b3 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ArrayColumnWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ArrayColumnWriter.java\n@@ -85,8 +85,8 @@ public long getRetainedBytes()\n     }\n \n     @Override\n-    public void reset()\n+    public void resetChunk()\n     {\n-        elementWriter.reset();\n+        elementWriter.resetChunk();\n     }\n }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ColumnWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ColumnWriter.java\nindex e0743a2cfecae..71aa0591c9b5c 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ColumnWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ColumnWriter.java\n@@ -34,7 +34,7 @@ List<BufferData> getBuffer()\n \n     long getRetainedBytes();\n \n-    void reset();\n+    void resetChunk();\n \n     class BufferData\n     {\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/MapColumnWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/MapColumnWriter.java\nindex 4a1bc0ba6bc7c..6c49ef55a91d4 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/MapColumnWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/MapColumnWriter.java\n@@ -89,9 +89,9 @@ public long getRetainedBytes()\n     }\n \n     @Override\n-    public void reset()\n+    public void resetChunk()\n     {\n-        keyWriter.reset();\n-        valueWriter.reset();\n+        keyWriter.resetChunk();\n+        valueWriter.resetChunk();\n     }\n }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriter.java\nindex c70e312e85bab..5003212854dc8 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriter.java\n@@ -188,7 +188,7 @@ private void writeChunk(Page page)\n         if (bufferedBytes >= writerOption.getMaxRowGroupSize()) {\n             columnWriters.forEach(ColumnWriter::close);\n             flush();\n-            columnWriters.forEach(ColumnWriter::reset);\n+            columnWriters.forEach(ColumnWriter::resetChunk);\n             rows = 0;\n         }\n     }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\nindex 0204878bf0d00..aa3b60e1d5af7 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\n@@ -48,6 +48,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.function.Supplier;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n@@ -156,7 +157,7 @@ public ColumnWriter primitive(PrimitiveType primitive)\n                 case PARQUET_1_0:\n                     return new PrimitiveColumnWriterV1(prestoType,\n                             columnDescriptor,\n-                            getValueWriter(parquetProperties.newValuesWriter(columnDescriptor), prestoType, columnDescriptor.getPrimitiveType()),\n+                            getValueWriter(() -> parquetProperties.newValuesWriter(columnDescriptor), prestoType, columnDescriptor.getPrimitiveType()),\n                             parquetProperties.newDefinitionLevelWriter(columnDescriptor),\n                             parquetProperties.newRepetitionLevelWriter(columnDescriptor),\n                             compressionCodecName,\n@@ -164,7 +165,7 @@ public ColumnWriter primitive(PrimitiveType primitive)\n                 case PARQUET_2_0:\n                     return new PrimitiveColumnWriterV2(prestoType,\n                             columnDescriptor,\n-                            getValueWriter(parquetProperties.newValuesWriter(columnDescriptor), prestoType, columnDescriptor.getPrimitiveType()),\n+                            getValueWriter(() -> parquetProperties.newValuesWriter(columnDescriptor), prestoType, columnDescriptor.getPrimitiveType()),\n                             parquetProperties.newDefinitionLevelEncoder(columnDescriptor),\n                             parquetProperties.newRepetitionLevelEncoder(columnDescriptor),\n                             compressionCodecName,\n@@ -187,43 +188,43 @@ private String[] currentPath()\n         }\n     }\n \n-    private static PrimitiveValueWriter getValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    private static PrimitiveValueWriter getValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n         if (BOOLEAN.equals(type)) {\n-            return new BooleanValueWriter(valuesWriter, parquetType);\n+            return new BooleanValueWriter(valuesWriterSupplier, parquetType);\n         }\n         if (INTEGER.equals(type) || SMALLINT.equals(type) || TINYINT.equals(type)) {\n-            return new IntegerValueWriter(valuesWriter, type, parquetType);\n+            return new IntegerValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (type instanceof DecimalType) {\n-            return new DecimalValueWriter(valuesWriter, type, parquetType);\n+            return new DecimalValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (DATE.equals(type)) {\n-            return new DateValueWriter(valuesWriter, parquetType);\n+            return new DateValueWriter(valuesWriterSupplier, parquetType);\n         }\n         if (BIGINT.equals(type)) {\n-            return new BigintValueWriter(valuesWriter, type, parquetType);\n+            return new BigintValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (DOUBLE.equals(type)) {\n-            return new DoubleValueWriter(valuesWriter, parquetType);\n+            return new DoubleValueWriter(valuesWriterSupplier, parquetType);\n         }\n         if (REAL.equals(type)) {\n-            return new RealValueWriter(valuesWriter, parquetType);\n+            return new RealValueWriter(valuesWriterSupplier, parquetType);\n         }\n         if (TIMESTAMP.equals(type)) {\n-            return new TimestampValueWriter(valuesWriter, type, parquetType);\n+            return new TimestampValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (TIMESTAMP_WITH_TIME_ZONE.equals(type)) {\n-            return new TimestampWithTimezoneValueWriter(valuesWriter, type, parquetType);\n+            return new TimestampWithTimezoneValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (TIME.equals(type)) {\n-            return new TimeValueWriter(valuesWriter, type, parquetType);\n+            return new TimeValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         if (UUID.equals(type)) {\n-            return new UuidValuesWriter(valuesWriter, parquetType);\n+            return new UuidValuesWriter(valuesWriterSupplier, parquetType);\n         }\n         if (type instanceof VarcharType || type instanceof CharType || type instanceof VarbinaryType) {\n-            return new CharValueWriter(valuesWriter, type, parquetType);\n+            return new CharValueWriter(valuesWriterSupplier, type, parquetType);\n         }\n         throw new PrestoException(NOT_SUPPORTED, format(\"Unsupported type for Parquet writer: %s\", type));\n     }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/PrimitiveColumnWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/PrimitiveColumnWriter.java\nindex c979aa97b2023..6904494d98f1b 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/PrimitiveColumnWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/PrimitiveColumnWriter.java\n@@ -200,10 +200,10 @@ public long getRetainedBytes()\n     }\n \n     @Override\n-    public void reset()\n+    public void resetChunk()\n     {\n         pageBuffer.clear();\n-        primitiveValueWriter.reset();\n+        primitiveValueWriter.resetChunk();\n         closed = false;\n \n         totalCompressedSize = 0;\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/StructColumnWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/StructColumnWriter.java\nindex 493c2bdbbc2cd..b630dd4068332 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/StructColumnWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/StructColumnWriter.java\n@@ -99,8 +99,8 @@ public long getRetainedBytes()\n     }\n \n     @Override\n-    public void reset()\n+    public void resetChunk()\n     {\n-        columnWriters.forEach(ColumnWriter::reset);\n+        columnWriters.forEach(ColumnWriter::resetChunk);\n     }\n }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BigintValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BigintValueWriter.java\nindex 007cdc94cc7eb..3a309e8d1d46f 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BigintValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BigintValueWriter.java\n@@ -18,6 +18,8 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n \n public class BigintValueWriter\n@@ -25,9 +27,9 @@ public class BigintValueWriter\n {\n     private final Type type;\n \n-    public BigintValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public BigintValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n     }\n \n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BooleanValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BooleanValueWriter.java\nindex 57a8f2c392db9..b36734e0c4483 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BooleanValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/BooleanValueWriter.java\n@@ -17,18 +17,16 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n-import static java.util.Objects.requireNonNull;\n \n public class BooleanValueWriter\n         extends PrimitiveValueWriter\n {\n-    private final ValuesWriter valuesWriter;\n-\n-    public BooleanValueWriter(ValuesWriter valuesWriter, PrimitiveType parquetType)\n+    public BooleanValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        super(parquetType, valuesWriterSupplier);\n     }\n \n     @Override\n@@ -37,7 +35,7 @@ public void write(Block block)\n         for (int i = 0; i < block.getPositionCount(); i++) {\n             if (!block.isNull(i)) {\n                 boolean value = BOOLEAN.getBoolean(block, i);\n-                valuesWriter.writeBoolean(value);\n+                getValueWriter().writeBoolean(value);\n                 getStatistics().updateStats(value);\n             }\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/CharValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/CharValueWriter.java\nindex 3b156fcfda5f3..3555cde895f83 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/CharValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/CharValueWriter.java\n@@ -20,18 +20,18 @@\n import org.apache.parquet.io.api.Binary;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n \n public class CharValueWriter\n         extends PrimitiveValueWriter\n {\n-    private final ValuesWriter valuesWriter;\n     private final Type type;\n \n-    public CharValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public CharValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n     }\n \n@@ -42,7 +42,7 @@ public void write(Block block)\n             if (!block.isNull(i)) {\n                 Slice slice = type.getSlice(block, i);\n                 Binary binary = Binary.fromConstantByteBuffer(slice.toByteBuffer());\n-                valuesWriter.writeBytes(binary);\n+                getValueWriter().writeBytes(binary);\n                 getStatistics().updateStats(binary);\n             }\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DateValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DateValueWriter.java\nindex ef7b87e936594..b5d2ed0ec51ec 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DateValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DateValueWriter.java\n@@ -17,18 +17,16 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static com.facebook.presto.common.type.DateType.DATE;\n-import static java.util.Objects.requireNonNull;\n \n public class DateValueWriter\n         extends PrimitiveValueWriter\n {\n-    private final ValuesWriter valuesWriter;\n-\n-    public DateValueWriter(ValuesWriter valuesWriter, PrimitiveType parquetType)\n+    public DateValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        super(parquetType, valuesWriterSupplier);\n     }\n \n     @Override\n@@ -37,7 +35,7 @@ public void write(Block block)\n         for (int position = 0; position < block.getPositionCount(); position++) {\n             if (!block.isNull(position)) {\n                 int value = (int) DATE.getLong(block, position);\n-                valuesWriter.writeInteger(value);\n+                getValueWriter().writeInteger(value);\n                 getStatistics().updateStats(value);\n             }\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DecimalValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DecimalValueWriter.java\nindex 3fc1a3b902c3f..fb5c079c19113 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DecimalValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DecimalValueWriter.java\n@@ -24,6 +24,7 @@\n \n import java.math.BigInteger;\n import java.util.Arrays;\n+import java.util.function.Supplier;\n \n import static java.util.Objects.requireNonNull;\n \n@@ -32,9 +33,9 @@ public class DecimalValueWriter\n {\n     private final DecimalType decimalType;\n \n-    public DecimalValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public DecimalValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.decimalType = (DecimalType) requireNonNull(type, \"type is null\");\n     }\n \n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DoubleValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DoubleValueWriter.java\nindex 64e3aa2299ad6..539f941642af9 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DoubleValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/DoubleValueWriter.java\n@@ -17,18 +17,16 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static com.facebook.presto.common.type.DoubleType.DOUBLE;\n-import static java.util.Objects.requireNonNull;\n \n public class DoubleValueWriter\n         extends PrimitiveValueWriter\n {\n-    private final ValuesWriter valuesWriter;\n-\n-    public DoubleValueWriter(ValuesWriter valuesWriter, PrimitiveType parquetType)\n+    public DoubleValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        super(parquetType, valuesWriterSupplier);\n     }\n \n     @Override\n@@ -37,7 +35,7 @@ public void write(Block block)\n         for (int i = 0; i < block.getPositionCount(); ++i) {\n             if (!block.isNull(i)) {\n                 double value = DOUBLE.getDouble(block, i);\n-                valuesWriter.writeDouble(value);\n+                getValueWriter().writeDouble(value);\n                 getStatistics().updateStats(value);\n             }\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/IntegerValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/IntegerValueWriter.java\nindex 4415453224a78..31fc9dbac0031 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/IntegerValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/IntegerValueWriter.java\n@@ -18,6 +18,8 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n \n public class IntegerValueWriter\n@@ -25,9 +27,9 @@ public class IntegerValueWriter\n {\n     private final Type type;\n \n-    public IntegerValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public IntegerValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n     }\n \n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/PrimitiveValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/PrimitiveValueWriter.java\nindex e1d967ec03642..a0375329e4e88 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/PrimitiveValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/PrimitiveValueWriter.java\n@@ -21,6 +21,8 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n \n public abstract class PrimitiveValueWriter\n@@ -28,12 +30,14 @@ public abstract class PrimitiveValueWriter\n {\n     private Statistics<?> statistics;\n     private final PrimitiveType parquetType;\n-    private final ValuesWriter valuesWriter;\n+    private final Supplier<ValuesWriter> valuesWriterSupplier;\n+    private ValuesWriter valuesWriter;\n \n-    public PrimitiveValueWriter(PrimitiveType parquetType, ValuesWriter valuesWriter)\n+    public PrimitiveValueWriter(PrimitiveType parquetType, Supplier<ValuesWriter> valuesWriterSupplier)\n     {\n         this.parquetType = requireNonNull(parquetType, \"parquetType is null\");\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        this.valuesWriterSupplier = requireNonNull(valuesWriterSupplier, \"valuesWriterSupplier is null\");\n+        this.valuesWriter = this.valuesWriterSupplier.get();\n         this.statistics = Statistics.createStats(parquetType);\n     }\n \n@@ -77,6 +81,12 @@ public void reset()\n         this.statistics = Statistics.createStats(parquetType);\n     }\n \n+    public void resetChunk()\n+    {\n+        valuesWriter = valuesWriterSupplier.get();\n+        this.statistics = Statistics.createStats(parquetType);\n+    }\n+\n     @Override\n     public void close()\n     {\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/RealValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/RealValueWriter.java\nindex 3d1166f557a8f..2f4582e899f8f 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/RealValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/RealValueWriter.java\n@@ -17,20 +17,18 @@\n import org.apache.parquet.column.values.ValuesWriter;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static com.facebook.presto.common.type.RealType.REAL;\n import static java.lang.Float.intBitsToFloat;\n import static java.lang.Math.toIntExact;\n-import static java.util.Objects.requireNonNull;\n \n public class RealValueWriter\n         extends PrimitiveValueWriter\n {\n-    private final ValuesWriter valuesWriter;\n-\n-    public RealValueWriter(ValuesWriter valuesWriter, PrimitiveType parquetType)\n+    public RealValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n-        this.valuesWriter = requireNonNull(valuesWriter, \"valuesWriter is null\");\n+        super(parquetType, valuesWriterSupplier);\n     }\n \n     @Override\n@@ -39,7 +37,7 @@ public void write(Block block)\n         for (int i = 0; i < block.getPositionCount(); i++) {\n             if (!block.isNull(i)) {\n                 float value = intBitsToFloat(toIntExact(REAL.getLong(block, i)));\n-                valuesWriter.writeFloat(value);\n+                getValueWriter().writeFloat(value);\n                 getStatistics().updateStats(value);\n             }\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimeValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimeValueWriter.java\nindex 74df9e5679e3a..c54acb4178ea9 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimeValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimeValueWriter.java\n@@ -19,6 +19,8 @@\n import org.apache.parquet.schema.OriginalType;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n \n@@ -28,9 +30,9 @@ public class TimeValueWriter\n     private final Type type;\n     private final boolean writeMicroseconds;\n \n-    public TimeValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public TimeValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n         this.writeMicroseconds = parquetType.isPrimitive() && parquetType.getOriginalType() == OriginalType.TIME_MICROS;\n     }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampValueWriter.java\nindex 47217f0bfd341..eb8f64bd9e05f 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampValueWriter.java\n@@ -19,6 +19,8 @@\n import org.apache.parquet.schema.OriginalType;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n \n@@ -28,9 +30,9 @@ public class TimestampValueWriter\n     private final Type type;\n     private final boolean writeMicroseconds;\n \n-    public TimestampValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public TimestampValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n         this.writeMicroseconds = parquetType.isPrimitive() && parquetType.getOriginalType() == OriginalType.TIMESTAMP_MICROS;\n     }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java\nindex 358310a333ae9..870009b8c9c53 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java\n@@ -19,6 +19,8 @@\n import org.apache.parquet.schema.OriginalType;\n import org.apache.parquet.schema.PrimitiveType;\n \n+import java.util.function.Supplier;\n+\n import static com.facebook.presto.common.type.DateTimeEncoding.unpackMillisUtc;\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n@@ -29,9 +31,9 @@ public class TimestampWithTimezoneValueWriter\n     private final Type type;\n     private final boolean writeMicroseconds;\n \n-    public TimestampWithTimezoneValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    public TimestampWithTimezoneValueWriter(Supplier<ValuesWriter> valuesWriterSupplier, Type type, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n         this.type = requireNonNull(type, \"type is null\");\n         this.writeMicroseconds = parquetType.isPrimitive() && parquetType.getOriginalType() == OriginalType.TIMESTAMP_MICROS;\n     }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/UuidValuesWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/UuidValuesWriter.java\nindex eefe981d9281f..fb728004b245c 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/UuidValuesWriter.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/UuidValuesWriter.java\n@@ -22,6 +22,7 @@\n \n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n+import java.util.function.Supplier;\n \n import static io.airlift.slice.SizeOf.SIZE_OF_LONG;\n \n@@ -32,9 +33,9 @@ public class UuidValuesWriter\n     private final ByteBuffer writeBuffer = ByteBuffer.allocate(2 * SIZE_OF_LONG)\n             .order(ByteOrder.BIG_ENDIAN);\n \n-    public UuidValuesWriter(ValuesWriter valuesWriter, PrimitiveType parquetType)\n+    public UuidValuesWriter(Supplier<ValuesWriter> valuesWriterSupplier, PrimitiveType parquetType)\n     {\n-        super(parquetType, valuesWriter);\n+        super(parquetType, valuesWriterSupplier);\n     }\n \n     @Override\n",
    "test_patch": "diff --git a/presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java b/presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java\nindex b6c6249107d99..4e3a4583dfe12 100644\n--- a/presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java\n+++ b/presto-parquet/src/test/java/com/facebook/presto/parquet/writer/TestParquetWriter.java\n@@ -14,17 +14,22 @@\n package com.facebook.presto.parquet.writer;\n \n import com.facebook.presto.common.PageBuilder;\n+import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.type.DecimalType;\n import com.facebook.presto.common.type.MapType;\n import com.facebook.presto.common.type.RowType;\n import com.facebook.presto.common.type.TimestampType;\n import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.parquet.Field;\n import com.facebook.presto.parquet.FileParquetDataSource;\n import com.facebook.presto.parquet.cache.MetadataReader;\n+import com.facebook.presto.parquet.reader.ParquetReader;\n import com.google.common.collect.ImmutableList;\n import io.airlift.units.DataSize;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.io.ColumnIOConverter;\n+import org.apache.parquet.io.MessageColumnIO;\n import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n import org.apache.parquet.schema.PrimitiveType;\n@@ -50,10 +55,13 @@\n import static com.facebook.presto.common.type.TinyintType.TINYINT;\n import static com.facebook.presto.common.type.UuidType.UUID;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n+import static com.facebook.presto.memory.context.AggregatedMemoryContext.newSimpleAggregatedMemoryContext;\n+import static com.facebook.presto.parquet.ParquetTypeUtils.getColumnIO;\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.io.Files.createTempDir;\n import static com.google.common.io.MoreFiles.deleteRecursively;\n import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n+import static io.airlift.units.DataSize.Unit.MEGABYTE;\n import static java.util.UUID.randomUUID;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertFalse;\n@@ -78,6 +86,65 @@ public class TestParquetWriter\n             nativeValueGetter(VARCHAR));\n     private static final Type ROW = RowType.from(ImmutableList.of(RowType.field(\"varchar\", VARCHAR)));\n \n+    @Test\n+    public void testWriteAllNullDataPageAfterRowGroupFlush()\n+    {\n+        temporaryDirectory = createTempDir();\n+        parquetFile = new File(temporaryDirectory, randomUUID().toString());\n+        List<Type> types = ImmutableList.of(INTEGER, INTEGER);\n+        List<String> names = ImmutableList.of(\"col_1\", \"col_2\");\n+        ParquetWriterOptions parquetWriterOptions = ParquetWriterOptions.builder()\n+                .setMaxPageSize(DataSize.succinctBytes(64))\n+                .setMaxBlockSize(DataSize.succinctBytes(150))\n+                .setMaxDictionaryPageSize(DataSize.succinctBytes(100))\n+                .build();\n+        int rowCount = 20;\n+        int newRowCount = 10;\n+        try (ParquetWriter parquetWriter = createParquetWriter(parquetFile, types, names, parquetWriterOptions, CompressionCodecName.UNCOMPRESSED)) {\n+            for (int i = 0; i < 2; i++) {\n+                PageBuilder pageBuilder = new PageBuilder(rowCount, types);\n+                for (int rowIdx = 0; rowIdx < rowCount; rowIdx++) {\n+                    // maintain col_1's dictionary size approximately half of raw data\n+                    INTEGER.writeLong(pageBuilder.getBlockBuilder(0), rowIdx / 2);\n+                    INTEGER.writeLong(pageBuilder.getBlockBuilder(1), rowIdx);\n+                    pageBuilder.declarePosition();\n+                }\n+                parquetWriter.write(pageBuilder.build());\n+            }\n+\n+            PageBuilder pageBuilder = new PageBuilder(newRowCount, types);\n+            for (int rowIdx = 0; rowIdx < newRowCount; rowIdx++) {\n+                pageBuilder.getBlockBuilder(0).appendNull();\n+                INTEGER.writeLong(pageBuilder.getBlockBuilder(1), rowIdx);\n+                pageBuilder.declarePosition();\n+            }\n+            parquetWriter.write(pageBuilder.build());\n+            parquetWriter.close();\n+\n+            FileParquetDataSource dataSource = new FileParquetDataSource(parquetFile);\n+            ParquetMetadata parquetMetadata = MetadataReader.readFooter(dataSource, parquetFile.length(), Optional.empty(), false).getParquetMetadata();\n+            MessageType schema = parquetMetadata.getFileMetaData().getSchema();\n+            MessageColumnIO messageColumnIO = getColumnIO(schema, schema);\n+\n+            Field field = ColumnIOConverter.constructField(INTEGER, messageColumnIO.getChild(0)).get();\n+            ParquetReader parquetReader = new ParquetReader(messageColumnIO, parquetMetadata.getBlocks(), Optional.empty(), dataSource, newSimpleAggregatedMemoryContext(), new DataSize(16, MEGABYTE), false, false, null, null, false, Optional.empty());\n+\n+            int batchCount = Integer.MAX_VALUE;\n+            int totalCount = 0;\n+            while (batchCount > 0) {\n+                batchCount = parquetReader.nextBatch();\n+                if (batchCount > 0) {\n+                    Block block = parquetReader.readBlock(field);\n+                    totalCount += block.getPositionCount();\n+                }\n+            }\n+            assertEquals(totalCount, 2 * rowCount + newRowCount);\n+        }\n+        catch (Exception e) {\n+            fail(\"Should not fail, but throw an exception as follows:\", e);\n+        }\n+    }\n+\n     @Test\n     public void testRowGroupFlushInterleavedColumnWriterFallbacks()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24528",
    "pr_id": 24528,
    "issue_id": 24520,
    "repo": "prestodb/presto",
    "problem_statement": "Proposed SPI Change for ConnectorMetadata.beginDelete / finishDelete\n## Context\nFor many systems implementing row-level deletes, these operations have their own distinct requirements for worker state. For example, Iceberg and other systems implement deletes as writes of tombstone files, which are then merged with the base data files to omit deleted records.  For these systems, DELETE statements need to setup similar state for writers as INSERT or CREATE statements (location, file formats, compression options). However, the ConnectorMetadata API does not cleanly provide for this.  We should provide a distinct table handle type for delete operations in ConnectorMetadata APIs to allow the clean pass-through of the required state.\n\n## Proposal\nCurrently, the ConnectorMetadata.beginDelete() and finishDelete() methods operate on a ConnectorTableHandle type:\n```\nConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n\nvoid finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n```\n\nwhere other mutation statement hooks provide distinct types to supply required fields:\n* beginInsert() / finishInsert(): `ConnectorInsertTableHandle`\n* beginCreateTable() / finishCreateTable(): `ConnectorOutputTableHandle`\n\nThe proposal is to add a new `ConnectorDeleteTableHandle` type to supply to delete statement hooks:\n```\nConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n\nvoid finishDelete(ConnectorSession session, ConnectorDeleteTableHandle tableHandle, Collection<Slice> fragments)\n```\n\nThis will allow connector implementations to separately model the required state for delete operations without muddying up the `ConnectorTableHandle` implementations passed around in other contexts.\n\n## Impact on Existing Connectors\nWe would make a one-time breaking change to the ConnectorMetadata SPI, updating the existing beginDelete() / finishDelete() hooks with the `ConnectorDeleteTableHandle` type.  \n1. For in-repo connector implementations of these hooks, we would update the in-use types to add `implements ConnectorDeleteTableHandle`.\n2. For external connector implementations, this would be a breaking change that would require a similar update to the types in use.\n\nWe could alternately implement the new type under a new, renamed set of beginDelete() / finishDelete() methods, have `MetadataManager` try the new API hooks, and fallback to existing beginDelete() / finishDelete() in the case of NOT_SUPPORTED being thrown.  However, this would complicate the presto-spi code and make usage of a distinct type in `ExecutionWriterTarget.DeleteHandle` difficult.  And if we wanted to converge back to a single set of beginDelete() / finishDelete() hooks, this would still wind up in a breaking change.\n",
    "issue_word_count": 355,
    "test_files_count": 1,
    "non_test_files_count": 23,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/develop/delete-and-update.rst",
      "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableHandle.java",
      "presto-kudu/src/main/java/com/facebook/presto/kudu/KuduHandleResolver.java",
      "presto-kudu/src/main/java/com/facebook/presto/kudu/KuduMetadata.java",
      "presto-kudu/src/main/java/com/facebook/presto/kudu/KuduTableHandle.java",
      "presto-main/src/main/java/com/facebook/presto/execution/scheduler/ExecutionWriterTarget.java",
      "presto-main/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandle.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandleJacksonModule.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/HandleJsonModule.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/HandleResolver.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java",
      "presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/ConnectorDeleteTableHandle.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/ConnectorHandleResolver.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorPageSinkProvider.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorPageSinkProvider.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java"
    ],
    "base_commit": "95dc1555499e0a33fefb64808a0bb6754998d974",
    "head_commit": "399bba2703179a754a4684f57427fee8102aee34",
    "repo_url": "https://github.com/prestodb/presto/pull/24528",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24528",
    "dockerfile": "",
    "pr_merged_at": "2025-02-23T02:02:56.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/develop/delete-and-update.rst b/presto-docs/src/main/sphinx/develop/delete-and-update.rst\nindex 9e31630eaa5c0..094d52f7a9422 100644\n--- a/presto-docs/src/main/sphinx/develop/delete-and-update.rst\n+++ b/presto-docs/src/main/sphinx/develop/delete-and-update.rst\n@@ -106,7 +106,7 @@ A connector implementing ``DELETE`` must specify three ``ConnectorMetadata`` met\n \n * ``beginDelete()``::\n \n-    ConnectorTableHandle beginDelete(\n+    ConnectorDeleteTableHandle beginDelete(\n          ConnectorSession session,\n          ConnectorTableHandle tableHandle)\n \n@@ -116,7 +116,7 @@ A connector implementing ``DELETE`` must specify three ``ConnectorMetadata`` met\n   ``beginDelete()`` performs any orchestration needed in the connector to start processing the ``DELETE``.\n   This orchestration varies from connector to connector.\n \n-  ``beginDelete()`` returns a ``ConnectorTableHandle`` with any added information the connector needs when the handle\n+  ``beginDelete()`` returns a ``ConnectorDeleteTableHandle`` with any added information the connector needs when the handle\n   is passed back to ``finishDelete()`` and the split generation machinery.  For most connectors, the returned table\n   handle contains a flag identifying the table handle as a table handle for a ``DELETE`` operation.\n \n@@ -124,7 +124,7 @@ A connector implementing ``DELETE`` must specify three ``ConnectorMetadata`` met\n \n       void finishDelete(\n           ConnectorSession session,\n-          ConnectorTableHandle tableHandle,\n+          ConnectoDeleteTableHandle tableHandle,\n           Collection<Slice> fragments)\n \n   During ``DELETE`` processing, the Presto engine accumulates the ``Slice`` collections returned by ``UpdatablePageSource.finish()``.\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java b/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\nindex 30b6bd7577d56..d7d56ec073aaf 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\n@@ -43,6 +43,7 @@\n import com.facebook.presto.hive.statistics.HiveStatisticsProvider;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorMetadataUpdateHandle;\n import com.facebook.presto.spi.ConnectorNewTableLayout;\n@@ -2517,7 +2518,7 @@ public Optional<List<SchemaTableName>> getReferencedMaterializedViews(ConnectorS\n     }\n \n     @Override\n-    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    public ConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n     }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex 41842d2bbd26f..d46b44d9a271d 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -31,6 +31,7 @@\n import com.facebook.presto.iceberg.statistics.StatisticsFileCache;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorNewTableLayout;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n@@ -978,7 +979,7 @@ public void truncateTable(ConnectorSession session, ConnectorTableHandle tableHa\n     }\n \n     @Override\n-    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    public ConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n         Table icebergTable = getIcebergTable(session, handle.getSchemaTableName());\n@@ -991,11 +992,9 @@ public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTable\n         if (formatVersion < MIN_FORMAT_VERSION_FOR_DELETE) {\n             throw new PrestoException(NOT_SUPPORTED, format(\"This connector only supports delete where one or more partitions are deleted entirely for table versions older than %d\", MIN_FORMAT_VERSION_FOR_DELETE));\n         }\n-\n         if (getDeleteMode(icebergTable) == RowLevelOperationMode.COPY_ON_WRITE) {\n             throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely. Configure delete_mode table property to allow row level deletions.\");\n         }\n-\n         validateTableMode(session, icebergTable);\n         transaction = icebergTable.newTransaction();\n \n@@ -1003,7 +1002,7 @@ public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTable\n     }\n \n     @Override\n-    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(ConnectorSession session, ConnectorDeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n         Table icebergTable = getIcebergTable(session, handle.getSchemaTableName());\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java\nindex ca5e9fecdef3c..199939c6b7985 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.presto.hive.HiveTransactionHandle;\n import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorHandleResolver;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n@@ -62,6 +63,12 @@ public Class<? extends ConnectorInsertTableHandle> getInsertTableHandleClass()\n         return IcebergInsertTableHandle.class;\n     }\n \n+    @Override\n+    public Class<? extends ConnectorDeleteTableHandle> getDeleteTableHandleClass()\n+    {\n+        return IcebergTableHandle.class;\n+    }\n+\n     @Override\n     public Class<? extends ConnectorTransactionHandle> getTransactionHandleClass()\n     {\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableHandle.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableHandle.java\nindex 78f77e78ed599..633f80d51eb33 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableHandle.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableHandle.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.iceberg;\n \n import com.facebook.presto.hive.BaseHiveTableHandle;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.common.collect.ImmutableList;\n@@ -28,6 +29,7 @@\n \n public class IcebergTableHandle\n         extends BaseHiveTableHandle\n+            implements ConnectorDeleteTableHandle\n {\n     private final IcebergTableName icebergTableName;\n     private final boolean snapshotSpecified;\n\ndiff --git a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduHandleResolver.java b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduHandleResolver.java\nindex 5375827b9318c..a444846aff542 100755\n--- a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduHandleResolver.java\n+++ b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduHandleResolver.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.kudu;\n \n import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorHandleResolver;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n@@ -61,6 +62,12 @@ public Class<? extends ConnectorInsertTableHandle> getInsertTableHandleClass()\n         return KuduInsertTableHandle.class;\n     }\n \n+    @Override\n+    public Class<? extends ConnectorDeleteTableHandle> getDeleteTableHandleClass()\n+    {\n+        return KuduTableHandle.class;\n+    }\n+\n     @Override\n     public Class<? extends ConnectorOutputTableHandle> getOutputTableHandleClass()\n     {\n\ndiff --git a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduMetadata.java b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduMetadata.java\nindex ff6ced78b19e4..fbfcf46426398 100755\n--- a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduMetadata.java\n+++ b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduMetadata.java\n@@ -20,6 +20,7 @@\n import com.facebook.presto.kudu.properties.PartitionDesign;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorNewTableLayout;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n@@ -357,13 +358,13 @@ public ColumnHandle getDeleteRowIdColumnHandle(ConnectorSession session, Connect\n     }\n \n     @Override\n-    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    public ConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n-        return tableHandle;\n+        return (ConnectorDeleteTableHandle) tableHandle;\n     }\n \n     @Override\n-    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(ConnectorSession session, ConnectorDeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n     }\n \n\ndiff --git a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduTableHandle.java b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduTableHandle.java\nindex bf357007a9e86..7dbafa6c46c98 100755\n--- a/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduTableHandle.java\n+++ b/presto-kudu/src/main/java/com/facebook/presto/kudu/KuduTableHandle.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.kudu;\n \n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorTableHandle;\n import com.facebook.presto.spi.SchemaTableName;\n import com.fasterxml.jackson.annotation.JsonCreator;\n@@ -25,7 +26,7 @@\n import static java.util.Objects.requireNonNull;\n \n public class KuduTableHandle\n-        implements ConnectorTableHandle\n+        implements ConnectorTableHandle, ConnectorDeleteTableHandle\n {\n     private final String connectorId;\n     private final SchemaTableName schemaTableName;\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/execution/scheduler/ExecutionWriterTarget.java b/presto-main/src/main/java/com/facebook/presto/execution/scheduler/ExecutionWriterTarget.java\nindex 248bfd2def3ef..2b0c9e5167dac 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/scheduler/ExecutionWriterTarget.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/scheduler/ExecutionWriterTarget.java\n@@ -14,6 +14,7 @@\n \n package com.facebook.presto.execution.scheduler;\n \n+import com.facebook.presto.metadata.DeleteTableHandle;\n import com.facebook.presto.metadata.InsertTableHandle;\n import com.facebook.presto.metadata.OutputTableHandle;\n import com.facebook.presto.spi.SchemaTableName;\n@@ -106,12 +107,12 @@ public String toString()\n     public static class DeleteHandle\n             extends ExecutionWriterTarget\n     {\n-        private final TableHandle handle;\n+        private final DeleteTableHandle handle;\n         private final SchemaTableName schemaTableName;\n \n         @JsonCreator\n         public DeleteHandle(\n-                @JsonProperty(\"handle\") TableHandle handle,\n+                @JsonProperty(\"handle\") DeleteTableHandle handle,\n                 @JsonProperty(\"schemaTableName\") SchemaTableName schemaTableName)\n         {\n             this.handle = requireNonNull(handle, \"handle is null\");\n@@ -119,7 +120,7 @@ public DeleteHandle(\n         }\n \n         @JsonProperty\n-        public TableHandle getHandle()\n+        public DeleteTableHandle getHandle()\n         {\n             return handle;\n         }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java b/presto-main/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\nindex 6cdc1c318b08a..793b6b8f451e0 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/scheduler/TableWriteInfo.java\n@@ -145,7 +145,7 @@ private static Optional<DeleteScanInfo> createDeleteScanInfo(StreamingSubPlan pl\n     {\n         if (writerTarget.isPresent() && writerTarget.get() instanceof ExecutionWriterTarget.DeleteHandle) {\n             DeleteNode delete = getOnlyElement(findPlanNodes(plan, DeleteNode.class));\n-            return createDeleteScanInfo(delete, writerTarget, metadata, session);\n+            return createDeleteScanInfo(delete, metadata, session);\n         }\n         return Optional.empty();\n     }\n@@ -154,19 +154,18 @@ private static Optional<DeleteScanInfo> createDeleteScanInfo(PlanNode planNode,\n     {\n         if (writerTarget.isPresent() && writerTarget.get() instanceof ExecutionWriterTarget.DeleteHandle) {\n             DeleteNode delete = findSinglePlanNode(planNode, DeleteNode.class).get();\n-            return createDeleteScanInfo(delete, writerTarget, metadata, session);\n+            return createDeleteScanInfo(delete, metadata, session);\n         }\n         return Optional.empty();\n     }\n \n-    private static Optional<DeleteScanInfo> createDeleteScanInfo(DeleteNode delete, Optional<ExecutionWriterTarget> writerTarget, Metadata metadata, Session session)\n+    private static Optional<DeleteScanInfo> createDeleteScanInfo(DeleteNode delete, Metadata metadata, Session session)\n     {\n-        TableHandle tableHandle = ((ExecutionWriterTarget.DeleteHandle) writerTarget.get()).getHandle();\n         TableScanNode tableScan = getDeleteTableScan(delete);\n         TupleDomain<ColumnHandle> originalEnforcedConstraint = tableScan.getEnforcedConstraint();\n         TableLayoutResult layoutResult = metadata.getLayout(\n                 session,\n-                tableHandle,\n+                tableScan.getTable(),\n                 new Constraint<>(originalEnforcedConstraint),\n                 Optional.of(ImmutableSet.copyOf(tableScan.getAssignments().values())));\n \n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java b/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\nindex 83cd4594032f5..8679321cc952e 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\n@@ -379,13 +379,13 @@ public OptionalLong metadataDelete(Session session, TableHandle tableHandle)\n     }\n \n     @Override\n-    public TableHandle beginDelete(Session session, TableHandle tableHandle)\n+    public DeleteTableHandle beginDelete(Session session, TableHandle tableHandle)\n     {\n         return delegate.beginDelete(session, tableHandle);\n     }\n \n     @Override\n-    public void finishDelete(Session session, TableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(Session session, DeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         delegate.finishDelete(session, tableHandle, fragments);\n     }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandle.java b/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandle.java\nnew file mode 100644\nindex 0000000000000..ea05b223843d5\n--- /dev/null\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandle.java\n@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Objects;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public final class DeleteTableHandle\n+{\n+    private final ConnectorId connectorId;\n+    private final ConnectorTransactionHandle transactionHandle;\n+    private final ConnectorDeleteTableHandle connectorHandle;\n+\n+    @JsonCreator\n+    public DeleteTableHandle(\n+            @JsonProperty(\"connectorId\") ConnectorId connectorId,\n+            @JsonProperty(\"transactionHandle\") ConnectorTransactionHandle transactionHandle,\n+            @JsonProperty(\"connectorHandle\") ConnectorDeleteTableHandle connectorHandle)\n+    {\n+        this.connectorId = requireNonNull(connectorId, \"connectorId is null\");\n+        this.transactionHandle = requireNonNull(transactionHandle, \"transactionHandle is null\");\n+        this.connectorHandle = requireNonNull(connectorHandle, \"connectorHandle is null\");\n+    }\n+\n+    @JsonProperty\n+    public ConnectorId getConnectorId()\n+    {\n+        return connectorId;\n+    }\n+\n+    @JsonProperty\n+    public ConnectorTransactionHandle getTransactionHandle()\n+    {\n+        return transactionHandle;\n+    }\n+\n+    @JsonProperty\n+    public ConnectorDeleteTableHandle getConnectorHandle()\n+    {\n+        return connectorHandle;\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(connectorId, transactionHandle, connectorHandle);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if (obj == null || getClass() != obj.getClass()) {\n+            return false;\n+        }\n+        DeleteTableHandle o = (DeleteTableHandle) obj;\n+        return Objects.equals(this.connectorId, o.connectorId) &&\n+                Objects.equals(this.transactionHandle, o.transactionHandle) &&\n+                Objects.equals(this.connectorHandle, o.connectorHandle);\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return connectorId + \":\" + connectorHandle;\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandleJacksonModule.java b/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandleJacksonModule.java\nnew file mode 100644\nindex 0000000000000..513348cf0fff4\n--- /dev/null\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/DeleteTableHandleJacksonModule.java\n@@ -0,0 +1,30 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.metadata;\n+\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n+\n+import javax.inject.Inject;\n+\n+public class DeleteTableHandleJacksonModule\n+        extends AbstractTypedJacksonModule<ConnectorDeleteTableHandle>\n+{\n+    @Inject\n+    public DeleteTableHandleJacksonModule(HandleResolver handleResolver)\n+    {\n+        super(ConnectorDeleteTableHandle.class,\n+                handleResolver::getId,\n+                handleResolver::getDeleteTableHandleClass);\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/HandleJsonModule.java b/presto-main/src/main/java/com/facebook/presto/metadata/HandleJsonModule.java\nindex 0c07b99aaab4e..61eae56f41895 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/HandleJsonModule.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/HandleJsonModule.java\n@@ -32,6 +32,7 @@ public void configure(Binder binder)\n         jsonBinder(binder).addModuleBinding().to(SplitJacksonModule.class);\n         jsonBinder(binder).addModuleBinding().to(OutputTableHandleJacksonModule.class);\n         jsonBinder(binder).addModuleBinding().to(InsertTableHandleJacksonModule.class);\n+        jsonBinder(binder).addModuleBinding().to(DeleteTableHandleJacksonModule.class);\n         jsonBinder(binder).addModuleBinding().to(IndexHandleJacksonModule.class);\n         jsonBinder(binder).addModuleBinding().to(TransactionHandleJacksonModule.class);\n         jsonBinder(binder).addModuleBinding().to(PartitioningHandleJacksonModule.class);\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/HandleResolver.java b/presto-main/src/main/java/com/facebook/presto/metadata/HandleResolver.java\nindex c6293cfb74720..09992ef314575 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/HandleResolver.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/HandleResolver.java\n@@ -16,6 +16,7 @@\n import com.facebook.presto.connector.informationSchema.InformationSchemaHandleResolver;\n import com.facebook.presto.connector.system.SystemHandleResolver;\n import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorHandleResolver;\n import com.facebook.presto.spi.ConnectorIndexHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n@@ -114,6 +115,11 @@ public String getId(ConnectorInsertTableHandle insertHandle)\n         return getId(insertHandle, MaterializedHandleResolver::getInsertTableHandleClass);\n     }\n \n+    public String getId(ConnectorDeleteTableHandle deleteHandle)\n+    {\n+        return getId(deleteHandle, MaterializedHandleResolver::getDeleteTableHandleClass);\n+    }\n+\n     public String getId(ConnectorPartitioningHandle partitioningHandle)\n     {\n         return getId(partitioningHandle, MaterializedHandleResolver::getPartitioningHandleClass);\n@@ -169,6 +175,11 @@ public Class<? extends ConnectorInsertTableHandle> getInsertTableHandleClass(Str\n         return resolverFor(id).getInsertTableHandleClass().orElseThrow(() -> new IllegalArgumentException(\"No resolver for \" + id));\n     }\n \n+    public Class<? extends ConnectorDeleteTableHandle> getDeleteTableHandleClass(String id)\n+    {\n+        return resolverFor(id).getDeleteTableHandleClass().orElseThrow(() -> new IllegalArgumentException(\"No resolver for \" + id));\n+    }\n+\n     public Class<? extends ConnectorPartitioningHandle> getPartitioningHandleClass(String id)\n     {\n         return resolverFor(id).getPartitioningHandleClass().orElseThrow(() -> new IllegalArgumentException(\"No resolver for \" + id));\n@@ -214,7 +225,7 @@ private <T> String getId(T handle, Function<MaterializedHandleResolver, Optional\n             catch (UnsupportedOperationException ignored) {\n             }\n         }\n-        throw new IllegalArgumentException(\"No connector for handle: \" + handle);\n+        throw new IllegalArgumentException(\"No connector for handle: \" + handle + \" of type \" + handle.getClass());\n     }\n \n     private <T> String getFunctionNamespaceId(T handle, Function<MaterializedFunctionHandleResolver, Optional<Class<? extends T>>> getter)\n@@ -240,6 +251,7 @@ private static class MaterializedHandleResolver\n         private final Optional<Class<? extends ConnectorIndexHandle>> indexHandle;\n         private final Optional<Class<? extends ConnectorOutputTableHandle>> outputTableHandle;\n         private final Optional<Class<? extends ConnectorInsertTableHandle>> insertTableHandle;\n+        private final Optional<Class<? extends ConnectorDeleteTableHandle>> deleteTableHandle;\n         private final Optional<Class<? extends ConnectorPartitioningHandle>> partitioningHandle;\n         private final Optional<Class<? extends ConnectorTransactionHandle>> transactionHandle;\n         private final Optional<Class<? extends ConnectorMetadataUpdateHandle>> metadataUpdateHandle;\n@@ -253,6 +265,7 @@ public MaterializedHandleResolver(ConnectorHandleResolver resolver)\n             indexHandle = getHandleClass(resolver::getIndexHandleClass);\n             outputTableHandle = getHandleClass(resolver::getOutputTableHandleClass);\n             insertTableHandle = getHandleClass(resolver::getInsertTableHandleClass);\n+            deleteTableHandle = getHandleClass(resolver::getDeleteTableHandleClass);\n             partitioningHandle = getHandleClass(resolver::getPartitioningHandleClass);\n             transactionHandle = getHandleClass(resolver::getTransactionHandleClass);\n             metadataUpdateHandle = getHandleClass(resolver::getMetadataUpdateHandleClass);\n@@ -303,6 +316,11 @@ public Optional<Class<? extends ConnectorInsertTableHandle>> getInsertTableHandl\n             return insertTableHandle;\n         }\n \n+        public Optional<Class<? extends ConnectorDeleteTableHandle>> getDeleteTableHandleClass()\n+        {\n+            return deleteTableHandle;\n+        }\n+\n         public Optional<Class<? extends ConnectorPartitioningHandle>> getPartitioningHandleClass()\n         {\n             return partitioningHandle;\n@@ -335,6 +353,7 @@ public boolean equals(Object o)\n                     Objects.equals(indexHandle, that.indexHandle) &&\n                     Objects.equals(outputTableHandle, that.outputTableHandle) &&\n                     Objects.equals(insertTableHandle, that.insertTableHandle) &&\n+                    Objects.equals(deleteTableHandle, that.deleteTableHandle) &&\n                     Objects.equals(partitioningHandle, that.partitioningHandle) &&\n                     Objects.equals(transactionHandle, that.transactionHandle) &&\n                     Objects.equals(metadataUpdateHandle, that.metadataUpdateHandle);\n@@ -343,7 +362,7 @@ public boolean equals(Object o)\n         @Override\n         public int hashCode()\n         {\n-            return Objects.hash(tableHandle, layoutHandle, columnHandle, split, indexHandle, outputTableHandle, insertTableHandle, partitioningHandle, transactionHandle, metadataUpdateHandle);\n+            return Objects.hash(tableHandle, layoutHandle, columnHandle, split, indexHandle, outputTableHandle, insertTableHandle, deleteTableHandle, partitioningHandle, transactionHandle, metadataUpdateHandle);\n         }\n     }\n \n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java b/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\nindex 4f3f698be42c9..21bc189fdce47 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\n@@ -324,12 +324,12 @@ public interface Metadata\n     /**\n      * Begin delete query\n      */\n-    TableHandle beginDelete(Session session, TableHandle tableHandle);\n+    DeleteTableHandle beginDelete(Session session, TableHandle tableHandle);\n \n     /**\n      * Finish delete query\n      */\n-    void finishDelete(Session session, TableHandle tableHandle, Collection<Slice> fragments);\n+    void finishDelete(Session session, DeleteTableHandle tableHandle, Collection<Slice> fragments);\n \n     /**\n      * Begin update query\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\nindex ead35ca3cc4be..8a2b1ce55dfc8 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n@@ -29,6 +29,7 @@\n import com.facebook.presto.execution.QueryManager;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorMetadataUpdateHandle;\n@@ -895,20 +896,19 @@ public OptionalLong metadataDelete(Session session, TableHandle tableHandle)\n     }\n \n     @Override\n-    public TableHandle beginDelete(Session session, TableHandle tableHandle)\n+    public DeleteTableHandle beginDelete(Session session, TableHandle tableHandle)\n     {\n         ConnectorId connectorId = tableHandle.getConnectorId();\n         CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, connectorId);\n-        ConnectorTableHandle newHandle = catalogMetadata.getMetadata().beginDelete(session.toConnectorSession(connectorId), tableHandle.getConnectorHandle());\n-        return new TableHandle(\n+        ConnectorDeleteTableHandle newHandle = catalogMetadata.getMetadata().beginDelete(session.toConnectorSession(connectorId), tableHandle.getConnectorHandle());\n+        return new DeleteTableHandle(\n                 tableHandle.getConnectorId(),\n-                newHandle,\n                 tableHandle.getTransaction(),\n-                Optional.empty());\n+                newHandle);\n     }\n \n     @Override\n-    public void finishDelete(Session session, TableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(Session session, DeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         ConnectorId connectorId = tableHandle.getConnectorId();\n         ConnectorMetadata metadata = getMetadata(session, connectorId);\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorDeleteTableHandle.java b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorDeleteTableHandle.java\nnew file mode 100644\nindex 0000000000000..927a01c248a67\n--- /dev/null\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorDeleteTableHandle.java\n@@ -0,0 +1,19 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spi;\n+\n+@SuppressWarnings(\"MarkerInterface\")\n+public interface ConnectorDeleteTableHandle\n+{\n+}\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorHandleResolver.java b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorHandleResolver.java\nindex 1e83bb99c47c1..79f1550e7c274 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorHandleResolver.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorHandleResolver.java\n@@ -41,6 +41,11 @@ default Class<? extends ConnectorInsertTableHandle> getInsertTableHandleClass()\n         throw new UnsupportedOperationException();\n     }\n \n+    default Class<? extends ConnectorDeleteTableHandle> getDeleteTableHandleClass()\n+    {\n+        throw new UnsupportedOperationException();\n+    }\n+\n     default Class<? extends ConnectorPartitioningHandle> getPartitioningHandleClass()\n     {\n         throw new UnsupportedOperationException();\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\nindex d31c12b68e23b..73031bb83e11e 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n@@ -17,6 +17,7 @@\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorMetadataUpdateHandle;\n import com.facebook.presto.spi.ConnectorNewTableLayout;\n@@ -528,7 +529,7 @@ default ColumnHandle getUpdateRowIdColumnHandle(ConnectorSession session, Connec\n     /**\n      * Begin delete query\n      */\n-    default ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    default ConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         throw new PrestoException(NOT_SUPPORTED, \"This connector does not support deletes\");\n     }\n@@ -538,7 +539,7 @@ default ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTabl\n      *\n      * @param fragments all fragments returned by {@link com.facebook.presto.spi.UpdatablePageSource#finish()}\n      */\n-    default void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    default void finishDelete(ConnectorSession session, ConnectorDeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         throw new PrestoException(NOT_SUPPORTED, \"This connector does not support deletes\");\n     }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorPageSinkProvider.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorPageSinkProvider.java\nindex f200da4d13269..5b8665d0f14fc 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorPageSinkProvider.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorPageSinkProvider.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spi.connector;\n \n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n import com.facebook.presto.spi.ConnectorPageSink;\n@@ -24,4 +25,9 @@ public interface ConnectorPageSinkProvider\n     ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle outputTableHandle, PageSinkContext pageSinkContext);\n \n     ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle insertTableHandle, PageSinkContext pageSinkContext);\n+\n+    default ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorDeleteTableHandle deleteTableHandle, PageSinkContext pageSinkContext)\n+    {\n+        throw new UnsupportedOperationException(\"ConnectorPageSinkProvider does not support connectorDeleteTableHandle\");\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\nindex fa1aeb780f691..09f315f81535e 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n@@ -17,6 +17,7 @@\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorMetadataUpdateHandle;\n import com.facebook.presto.spi.ConnectorNewTableLayout;\n@@ -585,7 +586,7 @@ public ColumnHandle getUpdateRowIdColumnHandle(ConnectorSession session, Connect\n     }\n \n     @Override\n-    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    public ConnectorDeleteTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n             return delegate.beginDelete(session, tableHandle);\n@@ -593,7 +594,7 @@ public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTable\n     }\n \n     @Override\n-    public void finishDelete(ConnectorSession session, ConnectorTableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(ConnectorSession session, ConnectorDeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n             delegate.finishDelete(session, tableHandle, fragments);\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorPageSinkProvider.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorPageSinkProvider.java\nindex 19466998df657..761b6537e2a14 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorPageSinkProvider.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorPageSinkProvider.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.spi.connector.classloader;\n \n+import com.facebook.presto.spi.ConnectorDeleteTableHandle;\n import com.facebook.presto.spi.ConnectorInsertTableHandle;\n import com.facebook.presto.spi.ConnectorOutputTableHandle;\n import com.facebook.presto.spi.ConnectorPageSink;\n@@ -51,4 +52,12 @@ public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHa\n             return new ClassLoaderSafeConnectorPageSink(delegate.createPageSink(transactionHandle, session, insertTableHandle, pageSinkContext), classLoader);\n         }\n     }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorDeleteTableHandle deleteTableHandle, PageSinkContext pageSinkContext)\n+    {\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n+            return new ClassLoaderSafeConnectorPageSink(delegate.createPageSink(transactionHandle, session, deleteTableHandle, pageSinkContext), classLoader);\n+        }\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java b/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\nindex 49a6051c22362..1f1eb6b09bc13 100644\n--- a/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\n+++ b/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\n@@ -411,13 +411,13 @@ public OptionalLong metadataDelete(Session session, TableHandle tableHandle)\n     }\n \n     @Override\n-    public TableHandle beginDelete(Session session, TableHandle tableHandle)\n+    public DeleteTableHandle beginDelete(Session session, TableHandle tableHandle)\n     {\n         throw new UnsupportedOperationException();\n     }\n \n     @Override\n-    public void finishDelete(Session session, TableHandle tableHandle, Collection<Slice> fragments)\n+    public void finishDelete(Session session, DeleteTableHandle tableHandle, Collection<Slice> fragments)\n     {\n         throw new UnsupportedOperationException();\n     }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24525",
    "pr_id": 24525,
    "issue_id": 22953,
    "repo": "prestodb/presto",
    "problem_statement": "JDBC DatabaseMetadata.getTables to describe VIEWS as VIEWS and not tables. Duplicates responses in getTables.\n[VIEW_RETURNED_AS TABLE.txt](https://github.com/user-attachments/files/15796402/VIEW_RETURNED_AS.TABLE.txt)\r\nViews defined in Presto are described by the JDBC driver as of type TABLE.\r\n\r\nJDBC driver implementation of DatabaseMetadata.getTables returns result set from the following query, which returns type TABLE for VIEWS which is not expected. \r\n\r\nSELECT \r\n\tTABLE_CAT, TABLE_SCHEM, TABLE_NAME, TABLE_TYPE, REMARKS,\r\n\tTYPE_CAT, TYPE_SCHEM, TYPE_NAME,\r\n\tSELF_REFERENCING_COL_NAME, REF_GENERATION\r\nFROM system.jdbc.tables\r\nORDER BY TABLE_TYPE, TABLE_CAT, TABLE_SCHEM, TABLE_NAME\r\n\r\n\r\ncreate or replace view  \"hive_data\".\"dbcert_hive\".VIPADDRESS as\r\nSELECT * FROM (\r\n    VALUES\r\n        (cast(0 as integer), null),\r\n        (cast(1 as integer), IPADDRESS '10.0.0.1'),\r\n        (cast(2 as integer), IPADDRESS '2001:db8::1')\r\n) AS t (rnum, c1);\r\n\r\nIf DatabaseMetadata.getTables is called with the following vector, the views are returned twice one as type table and one as type view which is wrong.\r\n\r\nString [] tableTypes = { \r\n\t\t\t\t\"TABLE\", \"VIEW\"\r\n\t\t};\r\n\t\t\r\n\t\ttry { \r\n\t\t\t\r\n\t\tResultSet rs = dbMeta.getTables( \"hive_data\" , \"dbcert\\\\_hive\", \"%\", tableTypes);\r\n\t\t....\r\n\r\n![image](https://github.com/prestodb/presto/assets/54872285/e257933b-fe5f-463a-b956-bd32b9831af3)\r\n\r\n",
    "issue_word_count": 175,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java",
      "presto-main/src/main/java/com/facebook/presto/connector/system/jdbc/TableJdbcTable.java"
    ],
    "pr_changed_test_files": [
      "presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java"
    ],
    "base_commit": "295230a789b441b0525094b1db1544f571906507",
    "head_commit": "fa14255a92a029943d29b23c30cd1b7785a83144",
    "repo_url": "https://github.com/prestodb/presto/pull/24525",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24525",
    "dockerfile": "",
    "pr_merged_at": "2025-02-13T16:18:46.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/connector/system/jdbc/TableJdbcTable.java b/presto-main/src/main/java/com/facebook/presto/connector/system/jdbc/TableJdbcTable.java\nindex 543919acf9d11..b9d5539fcbd17 100644\n--- a/presto-main/src/main/java/com/facebook/presto/connector/system/jdbc/TableJdbcTable.java\n+++ b/presto-main/src/main/java/com/facebook/presto/connector/system/jdbc/TableJdbcTable.java\n@@ -25,10 +25,12 @@\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n import com.facebook.presto.spi.security.AccessControl;\n+import com.google.common.collect.ImmutableSet;\n \n import javax.inject.Inject;\n \n import java.util.Optional;\n+import java.util.Set;\n \n import static com.facebook.presto.common.type.VarcharType.createUnboundedVarcharType;\n import static com.facebook.presto.connector.system.SystemConnectorSessionUtil.toSession;\n@@ -88,15 +90,19 @@ public RecordCursor cursor(ConnectorTransactionHandle transactionHandle, Connect\n         for (String catalog : filter(listCatalogs(session, metadata, accessControl).keySet(), catalogFilter)) {\n             QualifiedTablePrefix prefix = tablePrefix(catalog, schemaFilter, tableFilter);\n \n-            if (FilterUtil.emptyOrEquals(typeFilter, \"TABLE\")) {\n-                for (SchemaTableName name : listTables(session, metadata, accessControl, prefix)) {\n-                    table.addRow(tableRow(catalog, name, \"TABLE\"));\n+            Set<SchemaTableName> views = ImmutableSet.of();\n+            if (FilterUtil.emptyOrEquals(typeFilter, \"VIEW\")) {\n+                views = ImmutableSet.copyOf(listViews(session, metadata, accessControl, prefix));\n+                for (SchemaTableName name : views) {\n+                    table.addRow(tableRow(catalog, name, \"VIEW\"));\n                 }\n             }\n \n-            if (FilterUtil.emptyOrEquals(typeFilter, \"VIEW\")) {\n-                for (SchemaTableName name : listViews(session, metadata, accessControl, prefix)) {\n-                    table.addRow(tableRow(catalog, name, \"VIEW\"));\n+            if (FilterUtil.emptyOrEquals(typeFilter, \"TABLE\")) {\n+                for (SchemaTableName name : listTables(session, metadata, accessControl, prefix)) {\n+                    if (!views.contains(name)) {\n+                        table.addRow(tableRow(catalog, name, \"TABLE\"));\n+                    }\n                 }\n             }\n         }\n",
    "test_patch": "diff --git a/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java b/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\nindex 7e80977af6349..7b626d7908d2a 100644\n--- a/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\n+++ b/presto-jdbc/src/test/java/com/facebook/presto/jdbc/TestJdbcConnection.java\n@@ -187,6 +187,41 @@ public void testResetAutoCommit()\n         }\n     }\n \n+    @Test\n+    public void testTableType()\n+            throws SQLException\n+    {\n+        try (Connection connection = createConnection()) {\n+            assertThat(connection.getCatalog()).isEqualTo(\"hive\");\n+            assertThat(connection.getSchema()).isEqualTo(\"default\");\n+\n+            try (Statement statement = connection.createStatement()) {\n+                statement.execute(\"CREATE TABLE test_table_type (x bigint)\");\n+                statement.execute(\"CREATE VIEW table_type_view AS SELECT * FROM test_table_type\");\n+                ResultSet rs = statement.executeQuery(\"SELECT TABLE_NAME, TABLE_TYPE FROM system.jdbc.tables WHERE TABLE_SCHEM = 'default' AND TABLE_NAME = 'table_type_view'\");\n+                int rowCount = 0;\n+                while (rs.next()) {\n+                    assertEquals(rs.getString(\"TABLE_NAME\"), \"table_type_view\");\n+                    assertEquals(rs.getString(\"TABLE_TYPE\"), \"VIEW\");\n+                    rowCount++;\n+                }\n+                assertEquals(rowCount, 1);\n+\n+                rowCount = 0;\n+                rs = statement.executeQuery(\"SELECT TABLE_NAME, TABLE_TYPE FROM system.jdbc.tables WHERE TABLE_SCHEM = 'default' AND TABLE_NAME = 'test_table_type'\");\n+                while (rs.next()) {\n+                    assertEquals(rs.getString(\"TABLE_NAME\"), \"test_table_type\");\n+                    assertEquals(rs.getString(\"TABLE_TYPE\"), \"TABLE\");\n+                    rowCount++;\n+                }\n+                assertEquals(rowCount, 1);\n+\n+                statement.execute(\"DROP TABLE test_table_type\");\n+                statement.execute(\"DROP VIEW table_type_view\");\n+            }\n+        }\n+    }\n+\n     @Test\n     public void testRollback()\n             throws SQLException\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24456",
    "pr_id": 24456,
    "issue_id": 24454,
    "repo": "prestodb/presto",
    "problem_statement": "Support for multiple query event listeners\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\nSupport multiple query event listeners that  is useful when multiple plugins need to handle events independently.\nRefer : [prestodb/presto#9100](https://github.com/prestodb/presto/issues/9100)\n\n## Expected Behavior or Use Case\n<!--- Tell us how it should work -->\n\n## Presto Component, Service, or Connector\n<!--- Tell us to which service or component this request is related to -->\npresto-main\n\n## Possible Implementation\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\n\n## Example Screenshots (if appropriate):\n\n## Context\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 150,
    "test_files_count": 7,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/develop/event-listener.rst",
      "presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerConfig.java",
      "presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerManager.java",
      "presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerModule.java",
      "presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java",
      "presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java",
      "presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerConfig.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java",
      "presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerConfig.java",
      "presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java"
    ],
    "base_commit": "8f487f6a68a8125822f944818c534c7f0d33ca9e",
    "head_commit": "a4e9bc4aed1d53f2bf725d7bf030e5ee8f2e6682",
    "repo_url": "https://github.com/prestodb/presto/pull/24456",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24456",
    "dockerfile": "",
    "pr_merged_at": "2025-03-13T22:15:38.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/develop/event-listener.rst b/presto-docs/src/main/sphinx/develop/event-listener.rst\nindex 2f9349e448f3a..61cd5254f24ff 100644\n--- a/presto-docs/src/main/sphinx/develop/event-listener.rst\n+++ b/presto-docs/src/main/sphinx/develop/event-listener.rst\n@@ -46,3 +46,14 @@ Example configuration file:\n     event-listener.name=custom-event-listener\n     custom-property1=custom-value1\n     custom-property2=custom-value2\n+\n+Multiple Event Listeners\n+------------------------\n+\n+Multiple instances of the same, or different event listeners can be\n+installed and configured by setting ``event-listener.config-files``\n+to a comma separated list of config files.\n+\n+.. code-block:: none\n+\n+    event-listener.config-files=etc/event-listener.properties,etc/event-listener-second.properties\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerConfig.java b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerConfig.java\nnew file mode 100644\nindex 0000000000000..1d248f56e59a0\n--- /dev/null\n+++ b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerConfig.java\n@@ -0,0 +1,53 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.facebook.presto.eventlistener;\n+\n+import com.facebook.airlift.configuration.Config;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import java.io.File;\n+import java.util.List;\n+\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+\n+public class EventListenerConfig\n+{\n+    private static final Splitter SPLITTER = Splitter.on(',').trimResults().omitEmptyStrings();\n+    private List<File> eventListenerFiles = ImmutableList.of();\n+\n+    @NotNull\n+    public List<File> getEventListenerFiles()\n+    {\n+        return eventListenerFiles;\n+    }\n+\n+    @Config(\"event-listener.config-files\")\n+    public EventListenerConfig setEventListenerFiles(String eventListenerFiles)\n+    {\n+        this.eventListenerFiles = SPLITTER.splitToList(eventListenerFiles).stream()\n+                .map(File::new)\n+                .collect(toImmutableList());\n+        return this;\n+    }\n+\n+    public EventListenerConfig setEventListenerFiles(List<File> eventListenerFiles)\n+    {\n+        this.eventListenerFiles = ImmutableList.copyOf(eventListenerFiles);\n+        return this;\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerManager.java b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerManager.java\nindex 24a1d04b54bb1..4118cb98ae34e 100644\n--- a/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerManager.java\n@@ -22,14 +22,18 @@\n import com.facebook.presto.spi.eventlistener.QueryProgressEvent;\n import com.facebook.presto.spi.eventlistener.QueryUpdatedEvent;\n import com.facebook.presto.spi.eventlistener.SplitCompletedEvent;\n-import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n \n import java.io.File;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n \n import static com.facebook.presto.util.PropertiesUtil.loadProperties;\n@@ -44,9 +48,17 @@ public class EventListenerManager\n     private static final Logger log = Logger.get(EventListenerManager.class);\n     private static final File EVENT_LISTENER_CONFIGURATION = new File(\"etc/event-listener.properties\");\n     private static final String EVENT_LISTENER_PROPERTY_NAME = \"event-listener.name\";\n-\n+    private final List<File> configFiles;\n     private final Map<String, EventListenerFactory> eventListenerFactories = new ConcurrentHashMap<>();\n-    private final AtomicReference<Optional<EventListener>> configuredEventListener = new AtomicReference<>(Optional.empty());\n+    private final AtomicReference<List<EventListener>> configuredEventListeners =\n+            new AtomicReference<>(ImmutableList.of());\n+    private final AtomicBoolean loading = new AtomicBoolean(false);\n+\n+    @Inject\n+    public EventListenerManager(EventListenerConfig config)\n+    {\n+        this.configFiles = ImmutableList.copyOf(config.getEventListenerFiles());\n+    }\n \n     public void addEventListenerFactory(EventListenerFactory eventListenerFactory)\n     {\n@@ -57,30 +69,41 @@ public void addEventListenerFactory(EventListenerFactory eventListenerFactory)\n         }\n     }\n \n-    public void loadConfiguredEventListener()\n-            throws Exception\n+    public void loadConfiguredEventListeners()\n     {\n-        if (EVENT_LISTENER_CONFIGURATION.exists()) {\n-            Map<String, String> properties = loadProperties(EVENT_LISTENER_CONFIGURATION);\n-            checkArgument(\n-                    !isNullOrEmpty(properties.get(EVENT_LISTENER_PROPERTY_NAME)),\n-                    \"Access control configuration %s does not contain %s\",\n-                    EVENT_LISTENER_CONFIGURATION.getAbsoluteFile(),\n-                    EVENT_LISTENER_PROPERTY_NAME);\n-            loadConfiguredEventListener(properties);\n+        checkState(loading.compareAndSet(false, true), \"Event listeners already loaded\");\n+        List<File> configFiles = this.configFiles;\n+        if (configFiles.isEmpty()) {\n+            if (!EVENT_LISTENER_CONFIGURATION.exists()) {\n+                return;\n+            }\n+            configFiles = ImmutableList.of(EVENT_LISTENER_CONFIGURATION);\n         }\n+        configFiles.forEach(this::createEventListener);\n     }\n \n-    public void loadConfiguredEventListener(Map<String, String> properties)\n+    private void createEventListener(File configFile)\n     {\n-        properties = new HashMap<>(properties);\n-        String eventListenerName = properties.remove(EVENT_LISTENER_PROPERTY_NAME);\n-        checkArgument(!isNullOrEmpty(eventListenerName), \"event-listener.name property must be present\");\n-        setConfiguredEventListener(eventListenerName, properties);\n+        log.info(\"-- Loading event listener configuration file %s --\", configFile);\n+        if (configFile.exists()) {\n+            configFile = configFile.getAbsoluteFile();\n+            log.info(\"-- Loading event listener configuration file : %s --\", configFile);\n+            try {\n+                Map<String, String> properties = new HashMap<>(loadProperties(configFile));\n+                loadConfiguredEventListener(properties);\n+                log.info(\"-- Loaded event listener configuration file %s --\", configFile);\n+            }\n+            catch (IOException e) {\n+                log.error(e, \"IOException while loading configuration file: \" + configFile);\n+                throw new UncheckedIOException(\"Failed to read configuration file: \" + configFile, e);\n+            }\n+        }\n+        else {\n+            log.info(\"Unable to locate configuration file %s --\", configFile);\n+        }\n     }\n \n-    @VisibleForTesting\n-    protected void setConfiguredEventListener(String name, Map<String, String> properties)\n+    private void setConfiguredEventListener(String name, Map<String, String> properties)\n     {\n         requireNonNull(name, \"name is null\");\n         requireNonNull(properties, \"properties is null\");\n@@ -92,39 +115,51 @@ protected void setConfiguredEventListener(String name, Map<String, String> prope\n \n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(eventListenerFactory.getClass().getClassLoader())) {\n             EventListener eventListener = eventListenerFactory.create(ImmutableMap.copyOf(properties));\n-            this.configuredEventListener.set(Optional.of(eventListener));\n+            ImmutableList<EventListener> eventListeners = ImmutableList.<EventListener>builder()\n+                    .addAll(this.configuredEventListeners.get())\n+                    .add(eventListener)\n+                    .build();\n+            this.configuredEventListeners.set(eventListeners);\n         }\n \n         log.info(\"-- Loaded event listener %s --\", name);\n     }\n \n+    public void loadConfiguredEventListener(Map<String, String> properties)\n+    {\n+        properties = new HashMap<>(properties);\n+        String eventListenerName = properties.remove(EVENT_LISTENER_PROPERTY_NAME);\n+        checkArgument(!isNullOrEmpty(eventListenerName), \"event-listener.name property must be present\");\n+        setConfiguredEventListener(eventListenerName, properties);\n+    }\n+\n     public void queryCompleted(QueryCompletedEvent queryCompletedEvent)\n     {\n-        configuredEventListener.get()\n-                .ifPresent(eventListener -> eventListener.queryCompleted(queryCompletedEvent));\n+        configuredEventListeners.get()\n+                .forEach(eventListener -> eventListener.queryCompleted(queryCompletedEvent));\n     }\n \n     public void queryCreated(QueryCreatedEvent queryCreatedEvent)\n     {\n-        configuredEventListener.get()\n-                .ifPresent(eventListener -> eventListener.queryCreated(queryCreatedEvent));\n+        configuredEventListeners.get()\n+                .forEach(eventListener -> eventListener.queryCreated(queryCreatedEvent));\n     }\n \n     public void queryUpdated(QueryUpdatedEvent queryUpdatedEvent)\n     {\n-        configuredEventListener.get()\n-                .ifPresent(eventListener -> eventListener.queryUpdated(queryUpdatedEvent));\n+        configuredEventListeners.get()\n+                .forEach(eventListener -> eventListener.queryUpdated(queryUpdatedEvent));\n     }\n \n     public void publishQueryProgress(QueryProgressEvent queryProgressEvent)\n     {\n-        configuredEventListener.get()\n-                .ifPresent(eventListener -> eventListener.publishQueryProgress(queryProgressEvent));\n+        configuredEventListeners.get()\n+                .forEach(eventListener -> eventListener.publishQueryProgress(queryProgressEvent));\n     }\n \n     public void splitCompleted(SplitCompletedEvent splitCompletedEvent)\n     {\n-        configuredEventListener.get()\n-                .ifPresent(eventListener -> eventListener.splitCompleted(splitCompletedEvent));\n+        configuredEventListeners.get()\n+                .forEach(eventListener -> eventListener.splitCompleted(splitCompletedEvent));\n     }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerModule.java b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerModule.java\nindex 5d511bca93c77..e8f579a2c0563 100644\n--- a/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerModule.java\n+++ b/presto-main/src/main/java/com/facebook/presto/eventlistener/EventListenerModule.java\n@@ -17,12 +17,17 @@\n import com.google.inject.Module;\n import com.google.inject.Scopes;\n \n+import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n+import static org.weakref.jmx.guice.ExportBinder.newExporter;\n+\n public class EventListenerModule\n         implements Module\n {\n     @Override\n     public void configure(Binder binder)\n     {\n+        configBinder(binder).bindConfig(EventListenerConfig.class);\n         binder.bind(EventListenerManager.class).in(Scopes.SINGLETON);\n+        newExporter(binder).export(EventListenerManager.class).withGeneratedName();\n     }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java b/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\nindex 19250dd963eb7..3b2e9fd11e322 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/PrestoServer.java\n@@ -181,7 +181,7 @@ public void run()\n             }\n             injector.getInstance(PasswordAuthenticatorManager.class).loadPasswordAuthenticator();\n             injector.getInstance(PrestoAuthenticatorManager.class).loadPrestoAuthenticator();\n-            injector.getInstance(EventListenerManager.class).loadConfiguredEventListener();\n+            injector.getInstance(EventListenerManager.class).loadConfiguredEventListeners();\n             injector.getInstance(TempStorageManager.class).loadTempStorages();\n             injector.getInstance(QueryPrerequisitesManager.class).loadQueryPrerequisites();\n             injector.getInstance(NodeTtlFetcherManager.class).loadNodeTtlFetcher();\n",
    "test_patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java b/presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java\nindex a34b46003ad89..3a70d8515f550 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/testing/TestingPrestoServer.java\n@@ -38,6 +38,7 @@\n import com.facebook.presto.cost.StatsCalculator;\n import com.facebook.presto.dispatcher.DispatchManager;\n import com.facebook.presto.dispatcher.QueryPrerequisitesManagerModule;\n+import com.facebook.presto.eventlistener.EventListenerConfig;\n import com.facebook.presto.eventlistener.EventListenerManager;\n import com.facebook.presto.execution.QueryInfo;\n import com.facebook.presto.execution.QueryManager;\n@@ -324,6 +325,7 @@ public TestingPrestoServer(\n                     binder.bind(TestingTempStorageManager.class).in(Scopes.SINGLETON);\n                     binder.bind(AccessControlManager.class).to(TestingAccessControlManager.class).in(Scopes.SINGLETON);\n                     binder.bind(EventListenerManager.class).to(TestingEventListenerManager.class).in(Scopes.SINGLETON);\n+                    binder.bind(EventListenerConfig.class).in(Scopes.SINGLETON);\n                     binder.bind(TempStorageManager.class).to(TestingTempStorageManager.class).in(Scopes.SINGLETON);\n                     binder.bind(AccessControl.class).to(AccessControlManager.class).in(Scopes.SINGLETON);\n                     binder.bind(ShutdownAction.class).to(TestShutdownAction.class).in(Scopes.SINGLETON);\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\nindex 814209514bf19..3ac355399945f 100644\n--- a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n+++ b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n@@ -51,6 +51,7 @@\n import com.facebook.presto.cost.TaskCountEstimator;\n import com.facebook.presto.dispatcher.NoOpQueryManager;\n import com.facebook.presto.dispatcher.QueryPrerequisitesManager;\n+import com.facebook.presto.eventlistener.EventListenerConfig;\n import com.facebook.presto.eventlistener.EventListenerManager;\n import com.facebook.presto.execution.AlterFunctionTask;\n import com.facebook.presto.execution.CommitTask;\n@@ -542,7 +543,7 @@ private LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig,\n                 accessControl,\n                 new PasswordAuthenticatorManager(),\n                 new PrestoAuthenticatorManager(new SecurityConfig()),\n-                new EventListenerManager(),\n+                new EventListenerManager(new EventListenerConfig()),\n                 blockEncodingManager,\n                 new TestingTempStorageManager(),\n                 new QueryPrerequisitesManager(),\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java b/presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java\nindex d0bc6e235917e..49571690d0fff 100644\n--- a/presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/testing/TestingEventListenerManager.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.testing;\n \n+import com.facebook.presto.eventlistener.EventListenerConfig;\n import com.facebook.presto.eventlistener.EventListenerManager;\n import com.facebook.presto.spi.eventlistener.EventListener;\n import com.facebook.presto.spi.eventlistener.EventListenerFactory;\n@@ -22,6 +23,7 @@\n import com.facebook.presto.spi.eventlistener.QueryUpdatedEvent;\n import com.facebook.presto.spi.eventlistener.SplitCompletedEvent;\n import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n \n import java.util.Optional;\n import java.util.concurrent.atomic.AtomicReference;\n@@ -31,6 +33,12 @@ public class TestingEventListenerManager\n {\n     private final AtomicReference<Optional<EventListener>> configuredEventListener = new AtomicReference<>(Optional.empty());\n \n+    @Inject\n+    public TestingEventListenerManager(EventListenerConfig config)\n+    {\n+        super(config);\n+    }\n+\n     @Override\n     public void addEventListenerFactory(EventListenerFactory eventListenerFactory)\n     {\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java b/presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java\nindex 8d850679f8e3e..1b54dc1dea4f9 100644\n--- a/presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java\n+++ b/presto-main/src/test/java/com/facebook/presto/dispatcher/TestLocalDispatchQuery.java\n@@ -20,6 +20,7 @@\n import com.facebook.presto.cost.HistoryBasedPlanStatisticsManager;\n import com.facebook.presto.event.QueryMonitor;\n import com.facebook.presto.event.QueryMonitorConfig;\n+import com.facebook.presto.eventlistener.EventListenerConfig;\n import com.facebook.presto.eventlistener.EventListenerManager;\n import com.facebook.presto.execution.ClusterSizeMonitor;\n import com.facebook.presto.execution.ExecutionFailureInfo;\n@@ -464,7 +465,7 @@ private QueryMonitor createQueryMonitor(CountingEventListener eventListener)\n \n     private EventListenerManager createEventListenerManager(CountingEventListener countingEventListener)\n     {\n-        EventListenerManager eventListenerManager = new EventListenerManager();\n+        EventListenerManager eventListenerManager = new EventListenerManager(new EventListenerConfig());\n         eventListenerManager.addEventListenerFactory(new TestEventListenerFactory(countingEventListener));\n         eventListenerManager.loadConfiguredEventListener(ImmutableMap.of(\"event-listener.name\", TestEventListenerFactory.NAME));\n         return eventListenerManager;\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerConfig.java b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerConfig.java\nnew file mode 100644\nindex 0000000000000..6127d7451c3aa\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerConfig.java\n@@ -0,0 +1,55 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.facebook.presto.eventlistener;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.util.Map;\n+\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertFullMapping;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.assertRecordedDefaults;\n+import static com.facebook.airlift.configuration.testing.ConfigAssertions.recordDefaults;\n+public class TestEventListenerConfig\n+{\n+    @Test\n+    public void testDefaults()\n+    {\n+        assertRecordedDefaults(recordDefaults(EventListenerConfig.class)\n+                .setEventListenerFiles(\"\"));\n+    }\n+\n+    @Test\n+    public void testExplicitPropertyMappings()\n+    {\n+        Map<String, String> properties = new ImmutableMap.Builder<String, String>()\n+                .put(\"event-listener.config-files\", \"a,b,c\")\n+                .build();\n+\n+        EventListenerConfig expected = new EventListenerConfig()\n+                .setEventListenerFiles(\"a,b,c\");\n+        assertFullMapping(properties, expected);\n+\n+        ImmutableList.Builder<File> filesBuilder = ImmutableList.builder();\n+        filesBuilder.add(new File(\"a\"), new File(\"b\"), new File(\"c\"));\n+        //Test List version\n+        expected = new EventListenerConfig()\n+                .setEventListenerFiles(filesBuilder.build());\n+\n+        assertFullMapping(properties, expected);\n+    }\n+}\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java\nnew file mode 100644\nindex 0000000000000..ac7179172f08c\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/eventlistener/TestEventListenerManager.java\n@@ -0,0 +1,549 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.facebook.presto.eventlistener;\n+\n+import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.common.RuntimeStats;\n+import com.facebook.presto.common.plan.PlanCanonicalizationStrategy;\n+import com.facebook.presto.common.resourceGroups.QueryType;\n+import com.facebook.presto.spi.PrestoWarning;\n+import com.facebook.presto.spi.eventlistener.CTEInformation;\n+import com.facebook.presto.spi.eventlistener.EventListener;\n+import com.facebook.presto.spi.eventlistener.EventListenerFactory;\n+import com.facebook.presto.spi.eventlistener.OperatorStatistics;\n+import com.facebook.presto.spi.eventlistener.PlanOptimizerInformation;\n+import com.facebook.presto.spi.eventlistener.QueryCompletedEvent;\n+import com.facebook.presto.spi.eventlistener.QueryContext;\n+import com.facebook.presto.spi.eventlistener.QueryCreatedEvent;\n+import com.facebook.presto.spi.eventlistener.QueryFailureInfo;\n+import com.facebook.presto.spi.eventlistener.QueryIOMetadata;\n+import com.facebook.presto.spi.eventlistener.QueryInputMetadata;\n+import com.facebook.presto.spi.eventlistener.QueryMetadata;\n+import com.facebook.presto.spi.eventlistener.QueryOutputMetadata;\n+import com.facebook.presto.spi.eventlistener.QueryStatistics;\n+import com.facebook.presto.spi.eventlistener.SplitCompletedEvent;\n+import com.facebook.presto.spi.eventlistener.SplitFailureInfo;\n+import com.facebook.presto.spi.eventlistener.SplitStatistics;\n+import com.facebook.presto.spi.eventlistener.StageStatistics;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.prestospark.PrestoSparkExecutionContext;\n+import com.facebook.presto.spi.resourceGroups.ResourceGroupId;\n+import com.facebook.presto.spi.session.ResourceEstimates;\n+import com.facebook.presto.spi.statistics.PlanStatisticsWithSourceInfo;\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.units.DataSize;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.TimeUnit;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.expectThrows;\n+\n+@Test\n+public class TestEventListenerManager\n+{\n+    private static final Logger log = Logger.get(TestEventListenerManager.class);\n+    private final EventsCapture generatedEvents = new EventsCapture();\n+\n+    @Test\n+    public void testMultipleEventListeners() throws IOException\n+    {\n+        Path tempFile1 = Files.createTempFile(\"listener1_\", \".properties\");\n+        Path tempFile2 = Files.createTempFile(\"listener2_\", \".properties\");\n+        Path tempFile3 = Files.createTempFile(\"listener3_\", \".properties\");\n+\n+        writeProperties(tempFile1, \"event-listener.name\", \"wxd-event-listener1\");\n+        writeProperties(tempFile2, \"event-listener.name\", \"wxd-event-listener2\");\n+        writeProperties(tempFile3, \"event-listener.name\", \"wxd-event-listener3\");\n+\n+        EventListenerConfig config = new EventListenerConfig()\n+                .setEventListenerFiles(tempFile1.toFile().getPath() + \",\" + tempFile2.toFile().getPath() + \",\" + tempFile3.toFile().getPath());\n+        EventListenerManager eventListenerManager = new EventListenerManager(config);\n+        TestingEventListener testingEventListener = new TestingEventListener(generatedEvents);\n+        eventListenerManager.addEventListenerFactory(new TestEventListenerFactory(testingEventListener, \"wxd-event-listener1\"));\n+        eventListenerManager.addEventListenerFactory(new TestEventListenerFactory(testingEventListener, \"wxd-event-listener2\"));\n+        eventListenerManager.addEventListenerFactory(new TestEventListenerFactory(testingEventListener, \"wxd-event-listener3\"));\n+        eventListenerManager.loadConfiguredEventListeners();\n+\n+        QueryCreatedEvent queryCreatedEvent = createDummyQueryCreatedEvent();\n+        eventListenerManager.queryCreated(queryCreatedEvent);\n+        QueryCompletedEvent queryCompletedEvent = createDummyQueryCompletedEvent();\n+        eventListenerManager.queryCompleted(queryCompletedEvent);\n+        SplitCompletedEvent splitCompletedEvent = createDummySplitCompletedEvent();\n+        eventListenerManager.splitCompleted(splitCompletedEvent);\n+\n+        assertEquals(generatedEvents.getQueryCreatedEvents().size(), 3);\n+        assertEquals(generatedEvents.getQueryCompletedEvents().size(), 3);\n+        assertEquals(generatedEvents.getSplitCompletedEvents().size(), 3);\n+        generatedEvents.getQueryCreatedEvents().forEach(event -> assertEquals(event, queryCreatedEvent));\n+        generatedEvents.getQueryCompletedEvents().forEach(event -> assertEquals(event, queryCompletedEvent));\n+        generatedEvents.getSplitCompletedEvents().forEach(event -> assertEquals(event, splitCompletedEvent));\n+\n+        tryDeleteFile(tempFile1);\n+        tryDeleteFile(tempFile2);\n+        tryDeleteFile(tempFile3);\n+    }\n+\n+    @Test\n+    public void testEventListenerNotRegistered() throws IOException\n+    {\n+        Path tempFile1 = Files.createTempFile(\"listener1_\", \".properties\");\n+        Path tempFile2 = Files.createTempFile(\"listener2_\", \".properties\");\n+\n+        writeProperties(tempFile1, \"event-listener.name\", \"wxd-event-listener1\");\n+        writeProperties(tempFile2, \"event-listener.name\", \"wxd-event-listener2\");\n+        EventListenerConfig config = new EventListenerConfig().setEventListenerFiles(tempFile1.toFile().getPath() + \",\" + tempFile2.toFile().getPath());\n+\n+        EventListenerManager eventListenerManager = new EventListenerManager(config);\n+        TestingEventListener testingEventListener = new TestingEventListener(generatedEvents);\n+        eventListenerManager.addEventListenerFactory(new TestEventListenerFactory(testingEventListener, \"wxd-event-listener1\"));\n+\n+        IllegalStateException exception = expectThrows(IllegalStateException.class, () -> {\n+            eventListenerManager.loadConfiguredEventListeners();\n+        });\n+\n+        String expectedMessage = \"Event listener wxd-event-listener2 is not registered\";\n+        assertEquals(exception.getMessage(), expectedMessage);\n+    }\n+\n+    private void writeProperties(Path filePath, String key, String value)\n+            throws IOException\n+    {\n+        Properties properties = new Properties();\n+        properties.setProperty(key, value);\n+\n+        try (FileOutputStream outputStream = new FileOutputStream(filePath.toFile())) {\n+            properties.store(outputStream, \"Test Properties\");\n+        }\n+    }\n+\n+    public static QueryCreatedEvent createDummyQueryCreatedEvent()\n+    {\n+        QueryMetadata metadata = createDummyQueryMetadata();\n+        QueryContext context = createDummyQueryContext();\n+        return new QueryCreatedEvent(Instant.now(), context, metadata);\n+    }\n+\n+    public static QueryCompletedEvent createDummyQueryCompletedEvent()\n+    {\n+        QueryMetadata metadata = createDummyQueryMetadata();\n+        QueryStatistics statistics = createDummyQueryStatistics();\n+        QueryContext context = createDummyQueryContext();\n+        QueryIOMetadata ioMetadata = createDummyQueryIoMetadata();\n+        Optional<QueryFailureInfo> failureInfo = Optional.empty();\n+        List<PrestoWarning> warnings = new ArrayList<>();\n+        Optional<QueryType> queryType = Optional.empty();\n+        List<String> failedTasks = new ArrayList<>();\n+        Instant createTime = Instant.now();\n+        Instant executionStartTime = Instant.now().minusSeconds(10);\n+        Instant endTime = Instant.now().plusSeconds(10);\n+        List<StageStatistics> stageStatistics = new ArrayList<>();\n+        List<OperatorStatistics> operatorStatistics = new ArrayList<>();\n+        List<PlanStatisticsWithSourceInfo> planStatisticsRead = new ArrayList<>();\n+        List<PlanStatisticsWithSourceInfo> planStatisticsWritten = new ArrayList<>();\n+        Map<PlanNodeId, Map<PlanCanonicalizationStrategy, String>> planNodeHash = new HashMap<>();\n+        Map<PlanCanonicalizationStrategy, String> canonicalPlan = new HashMap<>();\n+        Optional<String> statsEquivalentPlan = Optional.empty();\n+        Optional<String> expandedQuery = Optional.empty();\n+        List<PlanOptimizerInformation> optimizerInformation = new ArrayList<>();\n+        List<CTEInformation> cteInformationList = new ArrayList<>();\n+        Set<String> scalarFunctions = new HashSet<>();\n+        Set<String> aggregateFunctions = new HashSet<>();\n+        Set<String> windowFunctions = new HashSet<>();\n+        Optional<PrestoSparkExecutionContext> prestoSparkExecutionContext = Optional.empty();\n+        Map<PlanCanonicalizationStrategy, String> hboPlanHash = new HashMap<>();\n+        Optional<Map<PlanNodeId, PlanNode>> planIdNodeMap = Optional.ofNullable(new HashMap<>());\n+\n+        return new QueryCompletedEvent(\n+                metadata,\n+                statistics,\n+                context,\n+                ioMetadata,\n+                failureInfo,\n+                warnings,\n+                queryType,\n+                failedTasks,\n+                createTime,\n+                executionStartTime,\n+                endTime,\n+                stageStatistics,\n+                operatorStatistics,\n+                planStatisticsRead,\n+                planStatisticsWritten,\n+                planNodeHash,\n+                canonicalPlan,\n+                statsEquivalentPlan,\n+                expandedQuery,\n+                optimizerInformation,\n+                cteInformationList,\n+                scalarFunctions,\n+                aggregateFunctions,\n+                windowFunctions,\n+                prestoSparkExecutionContext,\n+                hboPlanHash,\n+                planIdNodeMap);\n+    }\n+\n+    public static QueryStatistics createDummyQueryStatistics()\n+    {\n+        Duration cpuTime = Duration.ofMillis(1000);\n+        Duration retriedCpuTime = Duration.ofMillis(500);\n+        Duration wallTime = Duration.ofMillis(2000);\n+        Duration waitingForPrerequisitesTime = Duration.ofMillis(300);\n+        Duration queuedTime = Duration.ofMillis(1500);\n+        Duration waitingForResourcesTime = Duration.ofMillis(600);\n+        Duration semanticAnalyzingTime = Duration.ofMillis(700);\n+        Duration columnAccessPermissionCheckingTime = Duration.ofMillis(200);\n+        Duration dispatchingTime = Duration.ofMillis(1200);\n+        Duration planningTime = Duration.ofMillis(2500);\n+        Optional<Duration> analysisTime = Optional.of(Duration.ofMillis(1800));\n+        Duration executionTime = Duration.ofMillis(3500);\n+\n+        int peakRunningTasks = 5;\n+        long peakUserMemoryBytes = 500000000L;\n+        long peakTotalNonRevocableMemoryBytes = 800000000L;\n+        long peakTaskUserMemory = 100000000L;\n+        long peakTaskTotalMemory = 200000000L;\n+        long peakNodeTotalMemory = 120000000L;\n+        long shuffledBytes = 10000000L;\n+        long shuffledRows = 200000L;\n+        long totalBytes = 30000000L;\n+        long totalRows = 400000L;\n+        long outputBytes = 5000000L;\n+        long outputRows = 60000L;\n+        long writtenOutputBytes = 7000000L;\n+        long writtenOutputRows = 80000L;\n+        long writtenIntermediateBytes = 9000000L;\n+        long spilledBytes = 1000000L;\n+        double cumulativeMemory = 150.5;\n+        double cumulativeTotalMemory = 200.5;\n+        int completedSplits = 100;\n+        boolean complete = true;\n+        RuntimeStats runtimeStats = new RuntimeStats();\n+        return new QueryStatistics(\n+                cpuTime,\n+                retriedCpuTime,\n+                wallTime,\n+                waitingForPrerequisitesTime,\n+                queuedTime,\n+                waitingForResourcesTime,\n+                semanticAnalyzingTime,\n+                columnAccessPermissionCheckingTime,\n+                dispatchingTime,\n+                planningTime,\n+                analysisTime,\n+                executionTime,\n+                peakRunningTasks,\n+                peakUserMemoryBytes,\n+                peakTotalNonRevocableMemoryBytes,\n+                peakTaskUserMemory,\n+                peakTaskTotalMemory,\n+                peakNodeTotalMemory,\n+                shuffledBytes,\n+                shuffledRows,\n+                totalBytes,\n+                totalRows,\n+                outputBytes,\n+                outputRows,\n+                writtenOutputBytes,\n+                writtenOutputRows,\n+                writtenIntermediateBytes,\n+                spilledBytes,\n+                cumulativeMemory,\n+                cumulativeTotalMemory,\n+                completedSplits,\n+                complete,\n+                runtimeStats);\n+    }\n+\n+    private static QueryMetadata createDummyQueryMetadata()\n+    {\n+        String queryId = \"20250216_173945_00000_9r4vt\";\n+        Optional<String> transactionId = Optional.of(\"dummy-transaction-id\");\n+        String query = \"SELECT * FROM dummy_table\";\n+        String queryHash = \"dummy-query-hash\";\n+        Optional<String> preparedQuery = Optional.of(\"PREPARE SELECT * FROM dummy_table\");\n+        String queryState = \"COMPLETED\";\n+        URI uri = URI.create(\"http://localhost/query/dummy-query-id\");\n+        Optional<String> plan = Optional.of(\"dummy-plan\");\n+        Optional<String> jsonPlan = Optional.of(\"{\\\"plan\\\": \\\"dummy-plan\\\"}\");\n+        Optional<String> graphvizPlan = Optional.of(\"digraph {node1 -> node2}\");\n+        Optional<String> payload = Optional.of(\"dummy-payload\");\n+        List<String> runtimeOptimizedStages = new ArrayList<>(Arrays.asList(\"stage1\", \"stage2\"));\n+        Optional<String> tracingId = Optional.of(\"dummy-tracing-id\");\n+\n+        return new QueryMetadata(\n+                queryId,\n+                transactionId,\n+                query,\n+                queryHash,\n+                preparedQuery,\n+                queryState,\n+                uri,\n+                plan,\n+                jsonPlan,\n+                graphvizPlan,\n+                payload,\n+                runtimeOptimizedStages,\n+                tracingId);\n+    }\n+\n+    private static QueryContext createDummyQueryContext()\n+    {\n+        String user = \"dummyUser\";\n+        String serverAddress = \"127.0.0.1\";\n+        String serverVersion = \"testversion\";\n+        String environment = \"testing\";\n+        String workerType = \"worker-1\";\n+\n+        Optional<String> principal = Optional.of(\"dummyPrincipal\");\n+        Optional<String> remoteClientAddress = Optional.of(\"192.168.1.100\");\n+        Optional<String> userAgent = Optional.of(\"Mozilla/5.0\");\n+        Optional<String> clientInfo = Optional.of(\"Dummy Client Info\");\n+        Optional<String> source = Optional.empty();\n+        Optional<String> catalog = Optional.of(\"dummyCatalog\");\n+        Optional<String> schema = Optional.of(\"dummySchema\");\n+        Optional<ResourceGroupId> resourceGroupId = Optional.of(new ResourceGroupId(\"dummyGroupId\"));\n+\n+        Set<String> clientTags = new HashSet<>(Arrays.asList(\"tag1\", \"tag2\", \"tag3\"));\n+\n+        Map<String, String> sessionProperties = new HashMap<>();\n+        sessionProperties.put(\"property1\", \"value1\");\n+        sessionProperties.put(\"property2\", \"value2\");\n+\n+        ResourceEstimates resourceEstimates = new ResourceEstimates(\n+                Optional.of(new io.airlift.units.Duration(1200, TimeUnit.SECONDS)),\n+                Optional.of(new io.airlift.units.Duration(1200, TimeUnit.SECONDS)),\n+                Optional.of(new io.airlift.units.DataSize(2, DataSize.Unit.GIGABYTE)),\n+                Optional.of(new io.airlift.units.DataSize(2, DataSize.Unit.GIGABYTE)));\n+        return new QueryContext(\n+                user,\n+                principal,\n+                remoteClientAddress,\n+                userAgent,\n+                clientInfo,\n+                clientTags,\n+                source,\n+                catalog,\n+                schema,\n+                resourceGroupId,\n+                sessionProperties,\n+                resourceEstimates,\n+                serverAddress,\n+                serverVersion,\n+                environment,\n+                workerType);\n+    }\n+\n+    private static QueryIOMetadata createDummyQueryIoMetadata()\n+    {\n+        List<QueryInputMetadata> inputs = new ArrayList<>();\n+        QueryInputMetadata queryInputMetadata = getQueryInputMetadata();\n+        inputs.add(queryInputMetadata);\n+        QueryOutputMetadata outputMetadata = new QueryOutputMetadata(\n+                \"dummyCatalog\",\n+                \"dummySchema\",\n+                \"dummyTable\",\n+                Optional.of(\"dummyConnectorMetadata\"),\n+                Optional.of(true),\n+                \"dummySerializedCommitOutput\");\n+        return new QueryIOMetadata(inputs, Optional.of(outputMetadata));\n+    }\n+\n+    private static QueryInputMetadata getQueryInputMetadata()\n+    {\n+        String catalogName = \"dummyCatalog\";\n+        String schema = \"dummySchema\";\n+        String table = \"dummyTable\";\n+        String serializedCommitOutput = \"commitOutputDummy\";\n+        List<String> columns = new ArrayList<>(Arrays.asList(\"column1\", \"column2\", \"column3\"));\n+        Optional<Object> connectorInfo = Optional.of(new Object());\n+        return new QueryInputMetadata(\n+                catalogName,\n+                schema,\n+                table,\n+                columns,\n+                connectorInfo,\n+                Optional.empty(),\n+                serializedCommitOutput);\n+    }\n+\n+    private static SplitCompletedEvent createDummySplitCompletedEvent()\n+    {\n+        Instant now = Instant.now();\n+        Instant startTimeDummy = now.minusSeconds(100);\n+        Instant endTimeDummy = now.minusSeconds(50);\n+        SplitStatistics stats = createDummySplitStatistics();\n+        SplitFailureInfo failureInfo = new SplitFailureInfo(\"Error\", \"Dummy failure message\");\n+        return new SplitCompletedEvent(\n+                \"query123\",\n+                \"stage456\",\n+                \"stageExec789\",\n+                \"task012\",\n+                now,\n+                Optional.of(startTimeDummy),\n+                Optional.of(endTimeDummy),\n+                stats,\n+                Optional.of(failureInfo),\n+                \"dummyPayload\");\n+    }\n+\n+    private static SplitStatistics createDummySplitStatistics()\n+    {\n+        Duration cpuTime = Duration.ofSeconds(500);\n+        Duration wallTime = Duration.ofSeconds(1000);\n+        Duration queuedTime = Duration.ofSeconds(120);\n+        Duration completedReadTime = Duration.ofSeconds(800);\n+\n+        long completedPositions = 1500;\n+        long completedDataSizeBytes = 10000000L;\n+\n+        Optional<Duration> timeToFirstByte = Optional.of(Duration.ofSeconds(10));\n+        Optional<Duration> timeToLastByte = Optional.empty();\n+\n+        return new SplitStatistics(\n+                cpuTime,\n+                wallTime,\n+                queuedTime,\n+                completedReadTime,\n+                completedPositions,\n+                completedDataSizeBytes,\n+                timeToFirstByte,\n+                timeToLastByte);\n+    }\n+\n+    private static void tryDeleteFile(Path path)\n+    {\n+        try {\n+            File file = new File(path.toUri());\n+            if (file.exists()) {\n+                Files.delete(file.toPath());\n+            }\n+        }\n+        catch (IOException e) {\n+            log.error(e, \"Could not delete file found at [%s]\", path);\n+        }\n+    }\n+\n+    private static class TestEventListenerFactory\n+            implements EventListenerFactory\n+    {\n+        public static String name;\n+        private final TestingEventListener testingEventListener;\n+\n+        public TestEventListenerFactory(TestingEventListener testingEventListener, String name)\n+        {\n+            this.testingEventListener = requireNonNull(testingEventListener, \"testingEventListener is null\");\n+            this.name = name;\n+        }\n+\n+        @Override\n+        public String getName()\n+        {\n+            return name;\n+        }\n+\n+        @Override\n+        public EventListener create(Map<String, String> config)\n+        {\n+            return testingEventListener;\n+        }\n+    }\n+\n+    private static class TestingEventListener\n+            implements EventListener\n+    {\n+        private final EventsCapture eventsCapture;\n+\n+        public TestingEventListener(EventsCapture eventsCapture)\n+        {\n+            this.eventsCapture = eventsCapture;\n+        }\n+\n+        @Override\n+        public void queryCreated(QueryCreatedEvent queryCreatedEvent)\n+        {\n+            eventsCapture.addQueryCreated(queryCreatedEvent);\n+        }\n+\n+        @Override\n+        public void queryCompleted(QueryCompletedEvent queryCompletedEvent)\n+        {\n+            eventsCapture.addQueryCompleted(queryCompletedEvent);\n+        }\n+\n+        @Override\n+        public void splitCompleted(SplitCompletedEvent splitCompletedEvent)\n+        {\n+            eventsCapture.addSplitCompleted(splitCompletedEvent);\n+        }\n+    }\n+\n+    private static class EventsCapture\n+    {\n+        private final ImmutableList.Builder<QueryCreatedEvent> queryCreatedEvents = ImmutableList.builder();\n+        private final ImmutableList.Builder<QueryCompletedEvent> queryCompletedEvents = ImmutableList.builder();\n+        private final ImmutableList.Builder<SplitCompletedEvent> splitCompletedEvents = ImmutableList.builder();\n+\n+        public synchronized void addQueryCreated(QueryCreatedEvent event)\n+        {\n+            queryCreatedEvents.add(event);\n+        }\n+\n+        public synchronized void addQueryCompleted(QueryCompletedEvent event)\n+        {\n+            queryCompletedEvents.add(event);\n+        }\n+\n+        public synchronized void addSplitCompleted(SplitCompletedEvent event)\n+        {\n+            splitCompletedEvents.add(event);\n+        }\n+\n+        public List<QueryCreatedEvent> getQueryCreatedEvents()\n+        {\n+            return queryCreatedEvents.build();\n+        }\n+\n+        public List<QueryCompletedEvent> getQueryCompletedEvents()\n+        {\n+            return queryCompletedEvents.build();\n+        }\n+\n+        public List<SplitCompletedEvent> getSplitCompletedEvents()\n+        {\n+            return splitCompletedEvents.build();\n+        }\n+    }\n+}\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java b/presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\nindex cdee04d9bdc83..92d32cd9cecf8 100644\n--- a/presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\n+++ b/presto-main/src/test/java/com/facebook/presto/execution/TaskTestUtils.java\n@@ -20,6 +20,7 @@\n import com.facebook.presto.cost.StatsAndCosts;\n import com.facebook.presto.dispatcher.NoOpQueryManager;\n import com.facebook.presto.event.SplitMonitor;\n+import com.facebook.presto.eventlistener.EventListenerConfig;\n import com.facebook.presto.eventlistener.EventListenerManager;\n import com.facebook.presto.execution.buffer.OutputBuffers;\n import com.facebook.presto.execution.scheduler.LegacyNetworkTopology;\n@@ -201,7 +202,7 @@ public static TaskInfo updateTask(SqlTask sqlTask, List<TaskSource> taskSources,\n     public static SplitMonitor createTestSplitMonitor()\n     {\n         return new SplitMonitor(\n-                new EventListenerManager(),\n+                new EventListenerManager(new EventListenerConfig()),\n                 new JsonObjectMapperProvider().get());\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24440",
    "pr_id": 24440,
    "issue_id": 23877,
    "repo": "prestodb/presto",
    "problem_statement": "Reading complex types with decimals inside with 'hive.parquet-batch-read-optimization-enabled=true' results in error\n Reading a Parquet file with complex types with decimals inside with `hive.parquet-batch-read-optimization-enabled` is `true` results in error.\r\nIt works when `hive.parquet-batch-read-optimization-enabled` is `false`.\r\n\r\nParquet definition of the complex type with decimals inside:\r\n```\r\nas_array_big_decimal: OPTIONAL F:1\r\n.list:                REPEATED F:1\r\n..element:            OPTIONAL INT32 L:DECIMAL(1,0) R:1 D:3\r\n```\r\n\r\n\r\n## Your Environment\r\n* Presto 0.289\r\n* Storage S3\r\n* Connector used: Hive connector\r\n* Trace:\r\n\r\n`java.lang.UnsupportedOperationException: com.facebook.presto.common.block.IntArrayBlock\r\n     at com.facebook.presto.common.block.Block.getLong(Block.java:81)\r\n     at com.facebook.presto.common.type.ShortDecimalType.getObjectValue(ShortDecimalType.java:77)\r\n     at com.facebook.presto.common.type.ArrayType.arrayBlockToObjectValues(ArrayType.java:160)\r\n     at com.facebook.presto.common.type.ArrayType.lambda$getObjectValue$0(ArrayType.java:147)\r\n     at com.facebook.presto.common.block.AbstractArrayBlock.apply(AbstractArrayBlock.java:298)\r\n     at com.facebook.presto.common.type.ArrayType.getObjectValue(ArrayType.java:147)\r\n     at com.facebook.presto.server.protocol.RowIterable$RowIterator.computeNext(RowIterable.java:77)\r\n     at com.facebook.presto.server.protocol.RowIterable$RowIterator.computeNext(RowIterable.java:50)\r\n     at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:145)`\r\n\r\n## Expected Behavior\r\nVectorized reading should work.\r\n\r\n## Current Behavior\r\nThe following error is obtained:\r\n```\r\n[Error: java.lang.UnsupportedOperationException: com.facebook.presto.common.block.IntArrayBlock\r\n    at com.facebook.presto.common.block.Block.getLong(Block.java:81)\r\n    at com.facebook.presto.common.type.ShortDecimalType.getObjectValue(ShortDecimalType.java:77)...]\r\n```\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n1. CREATE TABLE \"hive\".\"default\".hive_data_reader_array_primitives (\r\n    as_array_int array(integer), \r\n    as_array_long array(bigint), \r\n    as_array_byte array(tinyint), \r\n    as_array_short array(smallint), \r\n    as_array_boolean array(boolean), \r\n    as_array_float array(real), \r\n    as_array_double array(double), \r\n    as_array_string array(varchar), \r\n    as_array_binary array(varbinary), \r\n    as_array_big_decimal array(decimal(1,0)) \r\n) WITH (FORMAT = 'PARQUET', external_location = 's3a://path/to/parquet/')\r\n\r\n2. Parquet file: https://github.com/prestodb/presto/blob/master/presto-delta/src/test/resources/delta_v3/data-reader-array-primitives/part-00000-13921577-6635-457e-9e3a-f32dc7fea982.c000.snappy.parquet\r\n3. Hive catalog properties with `hive.parquet-batch-read-optimization-enabled=true`\r\n\r\n\r\n\r\n",
    "issue_word_count": 337,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java"
    ],
    "base_commit": "e223cc163bdb78cbfffaca2221e4ed05e98457e1",
    "head_commit": "7d6162529bf28e743cdd777e58c2b993c80b0bac",
    "repo_url": "https://github.com/prestodb/presto/pull/24440",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24440",
    "dockerfile": "",
    "pr_merged_at": "2025-01-29T15:23:32.000Z",
    "patch": "diff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java\nindex 485935d579fc9..e03efd94d5882 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java\n@@ -63,8 +63,8 @@ private ColumnReaderFactory()\n \n     public static ColumnReader createReader(RichColumnDescriptor descriptor, boolean batchReadEnabled)\n     {\n-        if (batchReadEnabled) {\n-            final boolean isNested = descriptor.getPath().length > 1;\n+        final boolean isNested = descriptor.getPath().length > 1;\n+        if (batchReadEnabled && (!(isNested && isDecimalType(descriptor)))) {\n             switch (descriptor.getPrimitiveType().getPrimitiveTypeName()) {\n                 case BOOLEAN:\n                     return isNested ? new BooleanNestedBatchReader(descriptor) : new BooleanFlatBatchReader(descriptor);\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java b/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\nindex c409071bd332d..8afaded8b96e3 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\n@@ -96,6 +96,7 @@\n import static com.facebook.presto.tests.StructuralTestUtil.mapType;\n import static com.google.common.base.Functions.compose;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n import static com.google.common.collect.Iterables.concat;\n import static com.google.common.collect.Iterables.cycle;\n import static com.google.common.collect.Iterables.limit;\n@@ -181,6 +182,105 @@ public void testNestedArrays()\n         tester.testRoundTrip(objectInspector, values, values, type);\n     }\n \n+    @Test\n+    public void testNestedArraysDecimalBackedByINT32()\n+            throws Exception\n+    {\n+        int precision = 1;\n+        int scale = 0;\n+        ObjectInspector objectInspector = getStandardListObjectInspector(javaIntObjectInspector);\n+        Type type = new ArrayType(createDecimalType(precision, scale));\n+        Iterable<List<Integer>> values = createTestArrays(intsBetween(1, 1_000));\n+\n+        ImmutableList.Builder<List<SqlDecimal>> expectedValues = new ImmutableList.Builder<>();\n+        for (List<Integer> value : values) {\n+            expectedValues.add(value.stream()\n+                    .map(valueInt -> SqlDecimal.of(valueInt, precision, scale))\n+                    .collect(toImmutableList()));\n+        }\n+\n+        MessageType hiveSchema = parseMessageType(format(\"message hive_list_decimal {\" +\n+                \"  optional group my_list (LIST){\" +\n+                \"    repeated group list {\" +\n+                \"        optional INT32 element (DECIMAL(%d, %d));\" +\n+                \"    }\" +\n+                \"  }\" +\n+                \"} \", precision, scale));\n+\n+        tester.testRoundTrip(objectInspector, values, expectedValues.build(), \"my_list\", type, Optional.of(hiveSchema));\n+    }\n+\n+    @Test\n+    public void testNestedArraysDecimalBackedByINT64()\n+            throws Exception\n+    {\n+        int precision = 10;\n+        int scale = 2;\n+        ObjectInspector objectInspector = getStandardListObjectInspector(javaLongObjectInspector);\n+        Type type = new ArrayType(createDecimalType(precision, scale));\n+        Iterable<List<Long>> values = createTestArrays(longsBetween(1, 1_000));\n+\n+        ImmutableList.Builder<List<SqlDecimal>> expectedValues = new ImmutableList.Builder<>();\n+        for (List<Long> value : values) {\n+            expectedValues.add(value.stream()\n+                    .map(valueLong -> SqlDecimal.of(valueLong, precision, scale))\n+                    .collect(toImmutableList()));\n+        }\n+\n+        MessageType hiveSchema = parseMessageType(format(\"message hive_list_decimal {\" +\n+                \"  optional group my_list (LIST){\" +\n+                \"    repeated group list {\" +\n+                \"        optional INT64 element (DECIMAL(%d, %d));\" +\n+                \"    }\" +\n+                \"  }\" +\n+                \"} \", precision, scale));\n+        tester.testRoundTrip(objectInspector, values, expectedValues.build(), \"my_list\", type, Optional.of(hiveSchema));\n+    }\n+\n+    @Test\n+    public void testNestedArraysShortDecimalBackedByBinary()\n+            throws Exception\n+    {\n+        int precision = 1;\n+        int scale = 0;\n+        ObjectInspector objectInspector = getStandardListObjectInspector(new JavaHiveDecimalObjectInspector(new DecimalTypeInfo(precision, scale)));\n+        Type type = new ArrayType(createDecimalType(precision, scale));\n+        Iterable<List<HiveDecimal>> values = getNestedDecimalArrayInputValues(precision, scale);\n+        List<List<SqlDecimal>> expectedValues = getNestedDecimalArrayExpectedValues(values, precision, scale);\n+\n+        MessageType hiveSchema = parseMessageType(format(\"message hive_list_decimal {\" +\n+                \"  optional group my_list (LIST){\" +\n+                \"    repeated group list {\" +\n+                \"        optional BINARY element (DECIMAL(%d, %d));\" +\n+                \"    }\" +\n+                \"  }\" +\n+                \"} \", precision, scale));\n+\n+        tester.testRoundTrip(objectInspector, values, expectedValues, \"my_list\", type, Optional.of(hiveSchema));\n+    }\n+\n+    private Iterable<List<HiveDecimal>> getNestedDecimalArrayInputValues(int precision, int scale)\n+    {\n+        ContiguousSet<BigInteger> bigIntegerValues = bigIntegersBetween(BigDecimal.valueOf(Math.pow(10, precision - 1)).toBigInteger(),\n+                BigDecimal.valueOf(Math.pow(10, precision)).toBigInteger());\n+        List<HiveDecimal> writeValues = bigIntegerValues.stream()\n+                .map(value -> HiveDecimal.create((BigInteger) value, scale))\n+                .collect(toImmutableList());\n+\n+        return createTestArrays(writeValues);\n+    }\n+\n+    private static List<List<SqlDecimal>> getNestedDecimalArrayExpectedValues(Iterable<List<HiveDecimal>> values, int precision, int scale)\n+    {\n+        ImmutableList.Builder<List<SqlDecimal>> expectedValues = new ImmutableList.Builder<>();\n+        for (List<HiveDecimal> value : values) {\n+            expectedValues.add(value.stream()\n+                    .map(valueHiveDecimal -> new SqlDecimal(valueHiveDecimal.unscaledValue(), precision, scale))\n+                    .collect(toImmutableList()));\n+        }\n+        return expectedValues.build();\n+    }\n+\n     @Test\n     public void testSingleLevelSchemaNestedArrays()\n             throws Exception\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24434",
    "pr_id": 24434,
    "issue_id": 22422,
    "repo": "prestodb/presto",
    "problem_statement": "TestIcebergParquetMetadataCaching.testParquetMetadataCaching looks flaky\nLikely flaky; need to rerun to be sure\r\n\r\n```\r\n2024-04-04T16:12:10.2637984Z [ERROR] Tests run: 2084, Failures: 1, Errors: 0, Skipped: 23, Time elapsed: 1,512.985 s <<< FAILURE! - in TestSuite\r\n2024-04-04T16:12:10.2640602Z [ERROR] com.facebook.presto.iceberg.TestIcebergParquetMetadataCaching.testParquetMetadataCaching  Time elapsed: 0.401 s  <<< FAILURE!\r\n2024-04-04T16:12:10.2642202Z java.lang.AssertionError: expected [true] but found [false]\r\n2024-04-04T16:12:10.2642946Z \tat org.testng.Assert.fail(Assert.java:110)\r\n2024-04-04T16:12:10.2643619Z \tat org.testng.Assert.failNotEquals(Assert.java:1413)\r\n2024-04-04T16:12:10.2644327Z \tat org.testng.Assert.assertTrue(Assert.java:56)\r\n2024-04-04T16:12:10.2645010Z \tat org.testng.Assert.assertTrue(Assert.java:66)\r\n2024-04-04T16:12:10.2646669Z \tat com.facebook.presto.iceberg.TestIcebergParquetMetadataCaching.testParquetMetadataCaching(TestIcebergParquetMetadataCaching.java:90)\r\n2024-04-04T16:12:10.2648290Z \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n2024-04-04T16:12:10.2649495Z \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n2024-04-04T16:12:10.2650778Z \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n2024-04-04T16:12:10.2651817Z \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n2024-04-04T16:12:10.2652984Z \tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n2024-04-04T16:12:10.2654377Z \tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n2024-04-04T16:12:10.2655619Z \tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n2024-04-04T16:12:10.2656869Z \tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n2024-04-04T16:12:10.2658170Z \tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n2024-04-04T16:12:10.2659474Z \tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n2024-04-04T16:12:10.2660890Z \tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n2024-04-04T16:12:10.2662323Z \tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n2024-04-04T16:12:10.2663650Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n2024-04-04T16:12:10.2664963Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2024-04-04T16:12:10.2665931Z \tat java.lang.Thread.run(Thread.java:750)\r\n2024-04-04T16:12:10.2666347Z \r\n2024-04-04T16:12:10.9334912Z [INFO] \r\n2024-04-04T16:12:10.9335288Z [INFO] Results:\r\n2024-04-04T16:12:10.9335694Z [INFO] \r\n2024-04-04T16:12:10.9336053Z [ERROR] Failures: \r\n2024-04-04T16:12:10.9337099Z [ERROR]   TestIcebergParquetMetadataCaching.testParquetMetadataCaching:90 expected [true] but found [false]\r\n2024-04-04T16:12:10.9338247Z [INFO] \r\n2024-04-04T16:12:10.9338794Z [ERROR] Tests run: 2084, Failures: 1, Errors: 0, Skipped: 23\r\n2024-04-04T16:12:10.9339232Z [INFO] \r\n```",
    "issue_word_count": 453,
    "test_files_count": 3,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPlugin.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/InternalIcebergConnectorFactory.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java"
    ],
    "base_commit": "943868a9183b0824d82651744b65aeef18832f44",
    "head_commit": "e7d38e15c8a4826bc1610d436e61c08fa914eb2a",
    "repo_url": "https://github.com/prestodb/presto/pull/24434",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24434",
    "dockerfile": "",
    "pr_merged_at": "2025-01-27T23:47:18.000Z",
    "patch": "diff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java\nindex edf470043fac3..a706008fcc274 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java\n@@ -18,6 +18,8 @@\n import com.facebook.presto.spi.connector.ConnectorContext;\n import com.facebook.presto.spi.connector.ConnectorFactory;\n \n+import javax.management.MBeanServer;\n+\n import java.lang.reflect.InvocationTargetException;\n import java.util.Map;\n import java.util.Optional;\n@@ -27,6 +29,13 @@\n public class IcebergConnectorFactory\n         implements ConnectorFactory\n {\n+    private final MBeanServer mBeanServer;\n+\n+    public IcebergConnectorFactory(MBeanServer mBeanServer)\n+    {\n+        this.mBeanServer = mBeanServer;\n+    }\n+\n     @Override\n     public String getName()\n     {\n@@ -45,8 +54,8 @@ public Connector create(String catalogName, Map<String, String> config, Connecto\n         ClassLoader classLoader = IcebergConnectorFactory.class.getClassLoader();\n         try {\n             return (Connector) classLoader.loadClass(InternalIcebergConnectorFactory.class.getName())\n-                    .getMethod(\"createConnector\", String.class, Map.class, ConnectorContext.class, Optional.class)\n-                    .invoke(null, catalogName, config, context, Optional.empty());\n+                    .getMethod(\"createConnector\", String.class, Map.class, ConnectorContext.class, Optional.class, MBeanServer.class)\n+                    .invoke(null, catalogName, config, context, Optional.empty(), mBeanServer);\n         }\n         catch (InvocationTargetException e) {\n             Throwable targetException = e.getTargetException();\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPlugin.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPlugin.java\nindex 4f5f6baca186f..cd9cdab76bcfb 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPlugin.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPlugin.java\n@@ -19,15 +19,30 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n \n+import javax.management.MBeanServer;\n+\n+import java.lang.management.ManagementFactory;\n import java.util.Set;\n \n public class IcebergPlugin\n         implements Plugin\n {\n+    private final MBeanServer mBeanServer;\n+\n+    public IcebergPlugin()\n+    {\n+        this(ManagementFactory.getPlatformMBeanServer());\n+    }\n+\n+    public IcebergPlugin(MBeanServer mBeanServer)\n+    {\n+        this.mBeanServer = mBeanServer;\n+    }\n+\n     @Override\n     public Iterable<ConnectorFactory> getConnectorFactories()\n     {\n-        return ImmutableList.of(new IcebergConnectorFactory());\n+        return ImmutableList.of(new IcebergConnectorFactory(mBeanServer));\n     }\n \n     @Override\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/InternalIcebergConnectorFactory.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/InternalIcebergConnectorFactory.java\nindex f459717f1c22d..9893d975db55b 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/InternalIcebergConnectorFactory.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/InternalIcebergConnectorFactory.java\n@@ -57,7 +57,6 @@\n \n import javax.management.MBeanServer;\n \n-import java.lang.management.ManagementFactory;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n@@ -72,7 +71,8 @@ public static Connector createConnector(\n             String catalogName,\n             Map<String, String> config,\n             ConnectorContext context,\n-            Optional<ExtendedHiveMetastore> metastore)\n+            Optional<ExtendedHiveMetastore> metastore,\n+            MBeanServer mBeanServer)\n     {\n         ClassLoader classLoader = InternalIcebergConnectorFactory.class.getClassLoader();\n         try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n@@ -88,8 +88,7 @@ public static Connector createConnector(\n                     new CachingModule(),\n                     new HiveCommonModule(),\n                     binder -> {\n-                        MBeanServer platformMBeanServer = ManagementFactory.getPlatformMBeanServer();\n-                        binder.bind(MBeanServer.class).toInstance(new RebindSafeMBeanServer(platformMBeanServer));\n+                        binder.bind(MBeanServer.class).toInstance(new RebindSafeMBeanServer(mBeanServer));\n                         binder.bind(NodeVersion.class).toInstance(new NodeVersion(context.getNodeManager().getCurrentNode().getVersion()));\n                         binder.bind(NodeManager.class).toInstance(context.getNodeManager());\n                         binder.bind(TypeManager.class).toInstance(context.getTypeManager());\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\nindex 6e37c8cb41bac..08bf8ae9eeb72 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n@@ -38,6 +38,9 @@\n import com.google.common.collect.ImmutableSet;\n import io.airlift.tpch.TpchTable;\n \n+import javax.management.MBeanServer;\n+import javax.management.MBeanServerFactory;\n+\n import java.io.IOException;\n import java.net.URI;\n import java.nio.file.Files;\n@@ -197,7 +200,13 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n         queryRunner.installPlugin(new TpcdsPlugin());\n         queryRunner.createCatalog(\"tpcds\", \"tpcds\");\n \n-        queryRunner.installPlugin(new IcebergPlugin());\n+        queryRunner.getServers().forEach(server -> {\n+            MBeanServer mBeanServer = MBeanServerFactory.newMBeanServer();\n+            server.installPlugin(new IcebergPlugin(mBeanServer));\n+            if (addJmxPlugin) {\n+                server.installPlugin(new JmxPlugin(mBeanServer));\n+            }\n+        });\n \n         String catalogType = extraConnectorProperties.getOrDefault(\"iceberg.catalog.type\", HIVE.name());\n         Path icebergDataDirectory = getIcebergDataDirectoryPath(queryRunner.getCoordinator().getDataDirectory(), catalogType, format, addStorageFormatToPath);\n@@ -211,7 +220,6 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n         queryRunner.createCatalog(ICEBERG_CATALOG, \"iceberg\", icebergProperties);\n \n         if (addJmxPlugin) {\n-            queryRunner.installPlugin(new JmxPlugin());\n             queryRunner.createCatalog(\"jmx\", \"jmx\");\n         }\n \n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java\nindex c6d22e05d7ea2..c9572b3499e34 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java\n@@ -18,6 +18,7 @@\n import com.google.common.collect.ImmutableMap;\n import org.testng.annotations.Test;\n \n+import java.lang.management.ManagementFactory;\n import java.util.Map;\n \n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n@@ -38,7 +39,7 @@ public void testCachingHiveMetastore()\n \n     private static void createConnector(Map<String, String> config)\n     {\n-        ConnectorFactory factory = new IcebergConnectorFactory();\n+        ConnectorFactory factory = new IcebergConnectorFactory(ManagementFactory.getPlatformMBeanServer());\n         factory.create(\"iceberg-test\", config, new TestingConnectorContext())\n                 .shutdown();\n     }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\nindex ff6e311deb697..65d4158f18dfa 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\n@@ -45,7 +45,7 @@ protected QueryRunner createQueryRunner()\n                 PARQUET,\n                 false,\n                 true,\n-                OptionalInt.of(1),\n+                OptionalInt.of(2),\n                 Optional.empty());\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24417",
    "pr_id": 24417,
    "issue_id": 24419,
    "repo": "prestodb/presto",
    "problem_statement": "Make iceberg table target split size configurable\nWe should have a way to set a table's target split size for the Iceberg connector in Presto. The current default is 128MB and in some cases generates too much overhead. We should be able to set the target split size in order to tune the performance of some or all queries.\n\n## Expected Behavior or Use Case\n\nWe should be able to set the target split size either via table properties or session properties.\n\n## Presto Component, Service, or Connector\n\nIceberg connector.\n\n## Possible Implementation\n\n- Add session property to configure split size for all queries in a session (overrides table property)\n- Add support for target split size in `ALTER TABLE ... SET PROPERTIES` statement\n\n## Context\n\nDuring table scan planning within the Iceberg connector, we generate splits based on the iceberg table's [`read.split.target-size`](https://iceberg.apache.org/docs/latest/configuration/) property. For while there was no easy way to set table properties. However #21495 introduced this feature, but still doesn't allow setting arbitrary properties unless we update the set of supported table properties in `IcebergAbstractMetadata#setTableProperties`. \n\nWe should introduce support for setting the table property size to allow tuning this parameter on Iceberg tables.",
    "issue_word_count": 203,
    "test_files_count": 7,
    "non_test_files_count": 8,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSessionProperties.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitManager.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitSource.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/changelog/ChangelogSplitSource.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java"
    ],
    "base_commit": "7d69b0a4fc71a31d8075d598fbf42cda60e93bfb",
    "head_commit": "20fcd29ed9c0b4cd8b00d297a1271b253fe81862",
    "repo_url": "https://github.com/prestodb/presto/pull/24417",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24417",
    "dockerfile": "",
    "pr_merged_at": "2025-02-18T17:25:57.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex d06ae500e6c97..87c9b1fb7d0bf 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -357,9 +357,9 @@ connector using a WITH clause:\n \n The following table properties are available, which are specific to the Presto Iceberg connector:\n \n-=======================================   ===============================================================   ============\n+=======================================   ===============================================================   =========================\n Property Name                             Description                                                       Default\n-=======================================   ===============================================================   ============\n+=======================================   ===============================================================   =========================\n ``format``                                 Optionally specifies the format of table data files,             ``PARQUET``\n                                            either ``PARQUET`` or ``ORC``.\n \n@@ -388,7 +388,11 @@ Property Name                             Description\n \n ``metrics_max_inferred_column``            Optionally specifies the maximum number of columns for which     ``100``\n                                            metrics are collected.\n-=======================================   ===============================================================   ============\n+\n+``read.split.target-size``                 The target size for an individual split when generating splits     ``134217728`` (128MB)\n+                                           for a table scan. Generated splits may still be larger or\n+                                           smaller than this value. Must be specified in bytes.\n+=======================================   ===============================================================   =========================\n \n The table definition below specifies format ``ORC``, partitioning by columns ``c1`` and ``c2``,\n and a file system location of ``s3://test_bucket/test_schema/test_table``:\n@@ -421,6 +425,9 @@ Property Name                                         Description\n ``iceberg.rows_for_metadata_optimization_threshold``  Overrides the behavior of the connector property\n                                                       ``iceberg.rows-for-metadata-optimization-threshold`` in the current\n                                                       session.\n+``iceberg.target_split_size``                         Overrides the target split size for all tables in a query in bytes.\n+                                                      Set to 0 to use the value in each Iceberg table's\n+                                                      ``read.split.target-size`` property.\n ===================================================== ======================================================================\n \n Caching Support\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex c6855be88a022..41842d2bbd26f 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -201,6 +201,7 @@\n import static org.apache.iceberg.SnapshotSummary.REMOVED_POS_DELETES_PROP;\n import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL;\n import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL_DEFAULT;\n+import static org.apache.iceberg.TableProperties.SPLIT_SIZE;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE;\n \n public abstract class IcebergAbstractMetadata\n@@ -719,6 +720,7 @@ protected ImmutableMap<String, Object> createMetadataProperties(Table icebergTab\n         properties.put(METADATA_PREVIOUS_VERSIONS_MAX, IcebergUtil.getMetadataPreviousVersionsMax(icebergTable));\n         properties.put(METADATA_DELETE_AFTER_COMMIT, IcebergUtil.isMetadataDeleteAfterCommit(icebergTable));\n         properties.put(METRICS_MAX_INFERRED_COLUMN, IcebergUtil.getMetricsMaxInferredColumn(icebergTable));\n+        properties.put(SPLIT_SIZE, IcebergUtil.getSplitSize(icebergTable));\n \n         SortOrder sortOrder = icebergTable.sortOrder();\n         // TODO: Support sort column transforms (https://github.com/prestodb/presto/issues/24250)\n@@ -1127,6 +1129,9 @@ public void setTableProperties(ConnectorSession session, ConnectorTableHandle ta\n                 case COMMIT_RETRIES:\n                     updateProperties.set(TableProperties.COMMIT_NUM_RETRIES, String.valueOf(entry.getValue()));\n                     break;\n+                case SPLIT_SIZE:\n+                    updateProperties.set(TableProperties.SPLIT_SIZE, entry.getValue().toString());\n+                    break;\n                 default:\n                     throw new PrestoException(NOT_SUPPORTED, \"Updating property \" + entry.getKey() + \" is not supported currently\");\n             }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSessionProperties.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSessionProperties.java\nindex 57f954801f2f7..e82109fc0f8df 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSessionProperties.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSessionProperties.java\n@@ -40,6 +40,7 @@\n import static com.facebook.presto.spi.session.PropertyMetadata.booleanProperty;\n import static com.facebook.presto.spi.session.PropertyMetadata.doubleProperty;\n import static com.facebook.presto.spi.session.PropertyMetadata.integerProperty;\n+import static com.facebook.presto.spi.session.PropertyMetadata.longProperty;\n import static com.facebook.presto.spi.session.PropertyMetadata.stringProperty;\n \n public final class IcebergSessionProperties\n@@ -65,6 +66,7 @@ public final class IcebergSessionProperties\n     public static final String STATISTIC_SNAPSHOT_RECORD_DIFFERENCE_WEIGHT = \"statistic_snapshot_record_difference_weight\";\n     public static final String ROWS_FOR_METADATA_OPTIMIZATION_THRESHOLD = \"rows_for_metadata_optimization_threshold\";\n     public static final String STATISTICS_KLL_SKETCH_K_PARAMETER = \"statistics_kll_sketch_k_parameter\";\n+    public static final String TARGET_SPLIT_SIZE = \"target_split_size\";\n \n     private final List<PropertyMetadata<?>> sessionProperties;\n \n@@ -189,6 +191,11 @@ public IcebergSessionProperties(\n                 .add(integerProperty(STATISTICS_KLL_SKETCH_K_PARAMETER,\n                         \"The K parameter for the Apache DataSketches KLL sketch when computing histogram statistics\",\n                         icebergConfig.getStatisticsKllSketchKParameter(),\n+                        false))\n+                .add(longProperty(\n+                        TARGET_SPLIT_SIZE,\n+                        \"The target split size. Set to 0 to use the iceberg table's read.split.target-size property\",\n+                        0L,\n                         false));\n \n         nessieConfig.ifPresent((config) -> propertiesBuilder\n@@ -323,4 +330,9 @@ public static int getStatisticsKllSketchKParameter(ConnectorSession session)\n     {\n         return session.getProperty(STATISTICS_KLL_SKETCH_K_PARAMETER, Integer.class);\n     }\n+\n+    public static Long getTargetSplitSize(ConnectorSession session)\n+    {\n+        return session.getProperty(TARGET_SPLIT_SIZE, Long.class);\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitManager.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitManager.java\nindex d824accafcab5..7b1415a4beced 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitManager.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitManager.java\n@@ -31,7 +31,6 @@\n import org.apache.iceberg.TableScan;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.util.SnapshotUtil;\n-import org.apache.iceberg.util.TableScanUtil;\n import org.weakref.jmx.Managed;\n import org.weakref.jmx.Nested;\n \n@@ -41,7 +40,6 @@\n import java.util.concurrent.ThreadPoolExecutor;\n \n import static com.facebook.presto.iceberg.ExpressionConverter.toIcebergExpression;\n-import static com.facebook.presto.iceberg.IcebergSessionProperties.getMinimumAssignedSplitWeight;\n import static com.facebook.presto.iceberg.IcebergTableType.CHANGELOG;\n import static com.facebook.presto.iceberg.IcebergTableType.EQUALITY_DELETES;\n import static com.facebook.presto.iceberg.IcebergUtil.getIcebergTable;\n@@ -95,7 +93,7 @@ public ConnectorSplitSource getSplits(\n             IncrementalChangelogScan scan = icebergTable.newIncrementalChangelogScan()\n                     .fromSnapshotExclusive(fromSnapshot)\n                     .toSnapshot(toSnapshot);\n-            return new ChangelogSplitSource(session, typeManager, icebergTable, scan, scan.targetSplitSize());\n+            return new ChangelogSplitSource(session, typeManager, icebergTable, scan);\n         }\n         else if (table.getIcebergTableName().getTableType() == EQUALITY_DELETES) {\n             CloseableIterable<DeleteFile> deleteFiles = IcebergUtil.getDeleteFiles(icebergTable,\n@@ -117,8 +115,6 @@ else if (table.getIcebergTableName().getTableType() == EQUALITY_DELETES) {\n             IcebergSplitSource splitSource = new IcebergSplitSource(\n                     session,\n                     tableScan,\n-                    TableScanUtil.splitFiles(tableScan.planFiles(), tableScan.targetSplitSize()),\n-                    getMinimumAssignedSplitWeight(session),\n                     getMetadataColumnConstraints(layoutHandle.getValidPredicate()));\n             return splitSource;\n         }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitSource.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitSource.java\nindex d6e97230ae6fd..bf71ba005fdc5 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitSource.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergSplitSource.java\n@@ -20,13 +20,13 @@\n import com.facebook.presto.spi.ConnectorSplitSource;\n import com.facebook.presto.spi.SplitWeight;\n import com.facebook.presto.spi.connector.ConnectorPartitionHandle;\n+import com.facebook.presto.spi.schedule.NodeSelectionStrategy;\n import com.google.common.collect.ImmutableList;\n import com.google.common.io.Closer;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PartitionSpecParser;\n import org.apache.iceberg.TableScan;\n-import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.CloseableIterator;\n \n import java.io.IOException;\n@@ -39,40 +39,45 @@\n \n import static com.facebook.presto.hive.HiveCommonSessionProperties.getNodeSelectionStrategy;\n import static com.facebook.presto.iceberg.FileFormat.fromIcebergFileFormat;\n+import static com.facebook.presto.iceberg.IcebergSessionProperties.getMinimumAssignedSplitWeight;\n import static com.facebook.presto.iceberg.IcebergUtil.getDataSequenceNumber;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitionKeys;\n+import static com.facebook.presto.iceberg.IcebergUtil.getTargetSplitSize;\n import static com.facebook.presto.iceberg.IcebergUtil.metadataColumnsMatchPredicates;\n import static com.facebook.presto.iceberg.IcebergUtil.partitionDataFromStructLike;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static com.google.common.collect.Iterators.limit;\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.CompletableFuture.completedFuture;\n+import static org.apache.iceberg.util.TableScanUtil.splitFiles;\n \n public class IcebergSplitSource\n         implements ConnectorSplitSource\n {\n     private CloseableIterator<FileScanTask> fileScanTaskIterator;\n \n-    private final TableScan tableScan;\n     private final Closer closer = Closer.create();\n     private final double minimumAssignedSplitWeight;\n-    private final ConnectorSession session;\n+    private final long targetSplitSize;\n+    private final NodeSelectionStrategy nodeSelectionStrategy;\n \n     private final TupleDomain<IcebergColumnHandle> metadataColumnConstraints;\n \n     public IcebergSplitSource(\n             ConnectorSession session,\n             TableScan tableScan,\n-            CloseableIterable<FileScanTask> fileScanTaskIterable,\n-            double minimumAssignedSplitWeight,\n             TupleDomain<IcebergColumnHandle> metadataColumnConstraints)\n     {\n-        this.session = requireNonNull(session, \"session is null\");\n-        this.tableScan = requireNonNull(tableScan, \"tableScan is null\");\n-        this.fileScanTaskIterator = fileScanTaskIterable.iterator();\n-        this.minimumAssignedSplitWeight = minimumAssignedSplitWeight;\n+        requireNonNull(session, \"session is null\");\n         this.metadataColumnConstraints = requireNonNull(metadataColumnConstraints, \"metadataColumnConstraints is null\");\n-        closer.register(fileScanTaskIterator);\n+        this.targetSplitSize = getTargetSplitSize(session, tableScan).toBytes();\n+        this.minimumAssignedSplitWeight = getMinimumAssignedSplitWeight(session);\n+        this.nodeSelectionStrategy = getNodeSelectionStrategy(session);\n+        this.fileScanTaskIterator = closer.register(\n+                splitFiles(\n+                        closer.register(tableScan.planFiles()),\n+                        targetSplitSize)\n+                .iterator());\n     }\n \n     @Override\n@@ -130,8 +135,8 @@ private ConnectorSplit toIcebergSplit(FileScanTask task)\n                 getPartitionKeys(task),\n                 PartitionSpecParser.toJson(spec),\n                 partitionData.map(PartitionData::toJson),\n-                getNodeSelectionStrategy(session),\n-                SplitWeight.fromProportion(Math.min(Math.max((double) task.length() / tableScan.targetSplitSize(), minimumAssignedSplitWeight), 1.0)),\n+                nodeSelectionStrategy,\n+                SplitWeight.fromProportion(Math.min(Math.max((double) task.length() / targetSplitSize, minimumAssignedSplitWeight), 1.0)),\n                 task.deletes().stream().map(DeleteFile::fromIceberg).collect(toImmutableList()),\n                 Optional.empty(),\n                 getDataSequenceNumber(task.file()));\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\nindex d7e763fe462f3..1bb8976176e86 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\n@@ -29,6 +29,7 @@\n import static com.facebook.presto.common.type.VarcharType.createUnboundedVarcharType;\n import static com.facebook.presto.spi.session.PropertyMetadata.booleanProperty;\n import static com.facebook.presto.spi.session.PropertyMetadata.integerProperty;\n+import static com.facebook.presto.spi.session.PropertyMetadata.longProperty;\n import static com.facebook.presto.spi.session.PropertyMetadata.stringProperty;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static java.util.Locale.ENGLISH;\n@@ -47,6 +48,7 @@ public class IcebergTableProperties\n     public static final String METADATA_PREVIOUS_VERSIONS_MAX = \"metadata_previous_versions_max\";\n     public static final String METADATA_DELETE_AFTER_COMMIT = \"metadata_delete_after_commit\";\n     public static final String METRICS_MAX_INFERRED_COLUMN = \"metrics_max_inferred_column\";\n+    public static final String TARGET_SPLIT_SIZE = TableProperties.SPLIT_SIZE;\n     private static final String DEFAULT_FORMAT_VERSION = \"2\";\n \n     private final List<PropertyMetadata<?>> tableProperties;\n@@ -133,6 +135,10 @@ public IcebergTableProperties(IcebergConfig icebergConfig)\n                         false,\n                         value -> RowLevelOperationMode.fromName((String) value),\n                         RowLevelOperationMode::modeName))\n+                .add(longProperty(TARGET_SPLIT_SIZE,\n+                        \"Desired size of split to generate during query scan planning\",\n+                        TableProperties.SPLIT_SIZE_DEFAULT,\n+                        false))\n                 .build();\n \n         columnProperties = ImmutableList.of(stringProperty(\n@@ -210,4 +216,9 @@ public static RowLevelOperationMode getUpdateMode(Map<String, Object> tablePrope\n     {\n         return (RowLevelOperationMode) tableProperties.get(UPDATE_MODE);\n     }\n+\n+    public static Long getTargetSplitSize(Map<String, Object> tableProperties)\n+    {\n+        return (Long) tableProperties.get(TableProperties.SPLIT_SIZE);\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\nindex 1534d77666aae..229b77d61f8f4 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n@@ -49,6 +49,7 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Sets;\n+import io.airlift.units.DataSize;\n import org.apache.iceberg.BaseTable;\n import org.apache.iceberg.ContentFile;\n import org.apache.iceberg.ContentScanTask;\n@@ -61,6 +62,7 @@\n import org.apache.iceberg.PartitionField;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.RowLevelOperationMode;\n+import org.apache.iceberg.Scan;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SortOrder;\n@@ -157,6 +159,7 @@\n import static com.google.common.collect.Streams.stream;\n import static io.airlift.slice.Slices.utf8Slice;\n import static io.airlift.slice.Slices.wrappedBuffer;\n+import static io.airlift.units.DataSize.succinctBytes;\n import static java.lang.Double.doubleToRawLongBits;\n import static java.lang.Double.longBitsToDouble;\n import static java.lang.Double.parseDouble;\n@@ -195,6 +198,8 @@\n import static org.apache.iceberg.TableProperties.METRICS_MAX_INFERRED_COLUMN_DEFAULTS_DEFAULT;\n import static org.apache.iceberg.TableProperties.ORC_COMPRESSION;\n import static org.apache.iceberg.TableProperties.PARQUET_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.SPLIT_SIZE;\n+import static org.apache.iceberg.TableProperties.SPLIT_SIZE_DEFAULT;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE_DEFAULT;\n import static org.apache.iceberg.TableProperties.WRITE_LOCATION_PROVIDER_IMPL;\n@@ -1176,6 +1181,9 @@ public static Map<String, String> populateTableProperties(ConnectorTableMetadata\n \n         Integer metricsMaxInferredColumn = IcebergTableProperties.getMetricsMaxInferredColumn(tableMetadata.getProperties());\n         propertiesBuilder.put(METRICS_MAX_INFERRED_COLUMN_DEFAULTS, String.valueOf(metricsMaxInferredColumn));\n+\n+        propertiesBuilder.put(SPLIT_SIZE, String.valueOf(IcebergTableProperties.getTargetSplitSize(tableMetadata.getProperties())));\n+\n         return propertiesBuilder.build();\n     }\n \n@@ -1286,4 +1294,23 @@ public static String dataLocation(Table icebergTable)\n         }\n         return dataLocation;\n     }\n+\n+    public static Long getSplitSize(Table table)\n+    {\n+        return Long.parseLong(table.properties()\n+                .getOrDefault(SPLIT_SIZE,\n+                        String.valueOf(SPLIT_SIZE_DEFAULT)));\n+    }\n+\n+    public static DataSize getTargetSplitSize(long sessionValueProperty, long icebergScanTargetSplitSize)\n+    {\n+        return sessionValueProperty == 0 ?\n+                succinctBytes(icebergScanTargetSplitSize) :\n+                succinctBytes(sessionValueProperty);\n+    }\n+\n+    public static DataSize getTargetSplitSize(ConnectorSession session, Scan<?, ?, ?> scan)\n+    {\n+        return getTargetSplitSize(IcebergSessionProperties.getTargetSplitSize(session), scan.targetSplitSize());\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/changelog/ChangelogSplitSource.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/changelog/ChangelogSplitSource.java\nindex ad2d8b8decade..0d585393987f6 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/changelog/ChangelogSplitSource.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/changelog/ChangelogSplitSource.java\n@@ -24,6 +24,7 @@\n import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.SplitWeight;\n import com.facebook.presto.spi.connector.ConnectorPartitionHandle;\n+import com.facebook.presto.spi.schedule.NodeSelectionStrategy;\n import com.google.common.collect.ImmutableList;\n import com.google.common.io.Closer;\n import org.apache.iceberg.AddedRowsScanTask;\n@@ -48,9 +49,11 @@\n \n import static com.facebook.presto.hive.HiveCommonSessionProperties.getNodeSelectionStrategy;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_CANNOT_OPEN_SPLIT;\n+import static com.facebook.presto.iceberg.IcebergSessionProperties.getMinimumAssignedSplitWeight;\n import static com.facebook.presto.iceberg.IcebergUtil.getColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.getDataSequenceNumber;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitionKeys;\n+import static com.facebook.presto.iceberg.IcebergUtil.getTargetSplitSize;\n import static com.facebook.presto.iceberg.IcebergUtil.partitionDataFromStructLike;\n import static com.facebook.presto.iceberg.changelog.ChangelogOperation.fromIcebergChangelogOperation;\n import static com.facebook.presto.spi.StandardErrorCode.GENERIC_INTERNAL_ERROR;\n@@ -64,23 +67,23 @@ public class ChangelogSplitSource\n     private final Closer closer = Closer.create();\n     private CloseableIterable<ChangelogScanTask> fileScanTaskIterable;\n     private CloseableIterator<ChangelogScanTask> fileScanTaskIterator;\n-    private final IncrementalChangelogScan tableScan;\n     private final double minimumAssignedSplitWeight;\n-    private final ConnectorSession session;\n+    private final long targetSplitSize;\n     private final List<IcebergColumnHandle> columnHandles;\n+    private final NodeSelectionStrategy nodeSelectionStrategy;\n \n     public ChangelogSplitSource(\n             ConnectorSession session,\n             TypeManager typeManager,\n             Table table,\n-            IncrementalChangelogScan tableScan,\n-            double minimumAssignedSplitWeight)\n+            IncrementalChangelogScan tableScan)\n     {\n-        this.session = requireNonNull(session, \"session is null\");\n+        requireNonNull(session, \"session is null\");\n         requireNonNull(typeManager, \"typeManager is null\");\n         this.columnHandles = getColumns(table.schema(), table.spec(), typeManager);\n-        this.tableScan = requireNonNull(tableScan, \"tableScan is null\");\n-        this.minimumAssignedSplitWeight = minimumAssignedSplitWeight;\n+        this.minimumAssignedSplitWeight = getMinimumAssignedSplitWeight(session);\n+        this.targetSplitSize = getTargetSplitSize(session, tableScan).toBytes();\n+        this.nodeSelectionStrategy = getNodeSelectionStrategy(session);\n         this.fileScanTaskIterable = closer.register(tableScan.planFiles());\n         this.fileScanTaskIterator = closer.register(fileScanTaskIterable.iterator());\n     }\n@@ -143,8 +146,8 @@ private IcebergSplit splitFromContentScanTask(ContentScanTask<DataFile> task, Ch\n                 getPartitionKeys(task),\n                 PartitionSpecParser.toJson(spec),\n                 partitionData.map(PartitionData::toJson),\n-                getNodeSelectionStrategy(session),\n-                SplitWeight.fromProportion(Math.min(Math.max((double) task.length() / tableScan.targetSplitSize(), minimumAssignedSplitWeight), 1.0)),\n+                nodeSelectionStrategy,\n+                SplitWeight.fromProportion(Math.min(Math.max((double) task.length() / targetSplitSize, minimumAssignedSplitWeight), 1.0)),\n                 ImmutableList.of(),\n                 Optional.of(new ChangelogSplitInfo(fromIcebergChangelogOperation(changeTask.operation()),\n                         changeTask.changeOrdinal(),\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\nindex da2c1c41a5a40..0d4c3e3c13640 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n@@ -148,6 +148,7 @@ public void testShowCreateTable()\n                         \"   metadata_delete_after_commit = false,\\n\" +\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                         \")\", schemaName, getLocation(schemaName, \"orders\")));\n     }\n@@ -430,6 +431,7 @@ protected void testCreatePartitionedTableAs(Session session, FileFormat fileForm\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n                         \"   partitioning = ARRAY['order_status','ship_priority','bucket(order_key, 9)'],\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                         \")\",\n                 getSession().getCatalog().get(),\n@@ -634,6 +636,7 @@ public void testTableComments()\n                 \"   metadata_delete_after_commit = false,\\n\" +\n                 \"   metadata_previous_versions_max = 100,\\n\" +\n                 \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                 \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                 \")\";\n         String createTableSql = format(createTableTemplate, schemaName, \"test table comment\", getLocation(schemaName, \"test_table_comments\"));\n@@ -727,6 +730,7 @@ private void testCreateTableLike()\n                 \"   metadata_previous_versions_max = 100,\\n\" +\n                 \"   metrics_max_inferred_column = 100,\\n\" +\n                 \"   partitioning = ARRAY['adate'],\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                 \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                 \")\", getLocation(schemaName, \"test_create_table_like_original\")));\n \n@@ -744,6 +748,7 @@ private void testCreateTableLike()\n                 \"   metadata_delete_after_commit = false,\\n\" +\n                 \"   metadata_previous_versions_max = 100,\\n\" +\n                 \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                 \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                 \")\", getLocation(schemaName, \"test_create_table_like_copy1\")));\n         dropTable(session, \"test_create_table_like_copy1\");\n@@ -757,6 +762,7 @@ private void testCreateTableLike()\n                 \"   metadata_delete_after_commit = false,\\n\" +\n                 \"   metadata_previous_versions_max = 100,\\n\" +\n                 \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                 \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                 \")\", getLocation(schemaName, \"test_create_table_like_copy2\")));\n         dropTable(session, \"test_create_table_like_copy2\");\n@@ -772,6 +778,7 @@ private void testCreateTableLike()\n                             \"   metadata_previous_versions_max = 100,\\n\" +\n                             \"   metrics_max_inferred_column = 100,\\n\" +\n                             \"   partitioning = ARRAY['adate'],\\n\" +\n+                            \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                             \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                             \")\",\n                     getLocation(schemaName, \"test_create_table_like_original\")));\n@@ -787,6 +794,7 @@ private void testCreateTableLike()\n                             \"   metadata_previous_versions_max = 100,\\n\" +\n                             \"   metrics_max_inferred_column = 100,\\n\" +\n                             \"   partitioning = ARRAY['adate'],\\n\" +\n+                            \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                             \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                             \")\",\n                     getLocation(schemaName, \"test_create_table_like_original\")));\n@@ -804,6 +812,7 @@ private void testCreateTableLike()\n                             \"   metadata_previous_versions_max = 100,\\n\" +\n                             \"   metrics_max_inferred_column = 100,\\n\" +\n                             \"   partitioning = ARRAY['adate'],\\n\" +\n+                            \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                             \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                             \")\",\n                     getLocation(schemaName, \"test_create_table_like_copy5\")));\n@@ -852,6 +861,7 @@ protected void testCreateTableWithFormatVersion(String formatVersion, String def\n                         \"   metadata_delete_after_commit = false,\\n\" +\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = '%s'\\n\" +\n                         \")\",\n                 getSession().getCatalog().get(),\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java\nindex af92ef89fad4e..73047fb4c9f00 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSplitManager.java\n@@ -30,15 +30,19 @@\n import com.facebook.presto.tests.AbstractTestQueryFramework;\n import com.facebook.presto.transaction.TransactionManager;\n import com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.TableProperties;\n import org.testng.annotations.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.stream.IntStream;\n \n import static com.facebook.presto.iceberg.IcebergQueryRunner.ICEBERG_CATALOG;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.createIcebergQueryRunner;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.PUSHDOWN_FILTER_ENABLED;\n+import static com.facebook.presto.iceberg.IcebergSessionProperties.TARGET_SPLIT_SIZE;\n import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertNotNull;\n@@ -166,6 +170,36 @@ private void testGetSplitsByNonIdentityPartitionColumns(String tableName, boolea\n         assertQuerySucceeds(\"DROP TABLE \" + tableName);\n     }\n \n+    @Test\n+    public void testSplitSchedulingWithTablePropertyAndSession()\n+    {\n+        Session session = Session.builder(getSession())\n+                .setCatalogSessionProperty(\"iceberg\", IcebergSessionProperties.TARGET_SPLIT_SIZE, \"0\")\n+                .build();\n+        assertQuerySucceeds(\"CREATE TABLE test_split_size as SELECT * FROM UNNEST(sequence(1, 512)) as t(i)\");\n+        // verify that the session property hasn't propagated into the table\n+        assertEquals(getQueryRunner().execute(\"SELECT value FROM \\\"test_split_size$properties\\\" WHERE key = 'read.split.target-size'\").getOnlyValue(),\n+                Long.toString(TableProperties.SPLIT_SIZE_DEFAULT));\n+        assertQuerySucceeds(\"ALTER TABLE test_split_size SET PROPERTIES (\\\"read.split.target-size\\\" = 1)\");\n+        String selectQuery = \"SELECT * FROM test_split_size\";\n+        long maxSplits = getSplitsForSql(session, selectQuery);\n+\n+        IntStream.range(1, 5)\n+                .mapToObj(i -> Math.pow(2, i))\n+                .forEach(splitSize -> {\n+                    assertQuerySucceeds(\"ALTER TABLE test_split_size SET PROPERTIES (\\\"read.split.target-size\\\" =\" + splitSize.intValue() + \")\");\n+                    assertEquals(getSplitsForSql(session, selectQuery), (double) maxSplits / splitSize, 5);\n+                });\n+        // split size should be set to 32 on the table property.\n+        // Set it to 1 with the session property to override the table value and verify we get the\n+        // same number of splits as when the table value is set to 1.\n+        Session minSplitSession = Session.builder(session)\n+                .setCatalogSessionProperty(\"iceberg\", TARGET_SPLIT_SIZE, \"1\")\n+                .build();\n+        assertEquals(getSplitsForSql(minSplitSession, selectQuery), maxSplits);\n+        assertQuerySucceeds(\"DROP TABLE test_split_size\");\n+    }\n+\n     private Session sessionWithFilterPushdown(boolean pushdown)\n     {\n         return Session.builder(getQueryRunner().getDefaultSession())\n@@ -173,6 +207,37 @@ private Session sessionWithFilterPushdown(boolean pushdown)\n                 .build();\n     }\n \n+    private long getSplitsForSql(Session session, String sql)\n+    {\n+        TransactionManager transactionManager = getQueryRunner().getTransactionManager();\n+        SplitManager splitManager = getQueryRunner().getSplitManager();\n+\n+        List<TableScanNode> tableScanNodes = getTableScanFromOptimizedPlanOfSql(sql, session);\n+        assertNotNull(tableScanNodes);\n+        assertEquals(tableScanNodes.size(), 1);\n+\n+        TransactionId transactionId = transactionManager.beginTransaction(false);\n+        session = session.beginTransactionId(transactionId, transactionManager, new AllowAllAccessControl());\n+        TableHandle tableHandle = tableScanNodes.get(0).getTable();\n+        TableHandle newTableHandle = new TableHandle(tableHandle.getConnectorId(),\n+                tableHandle.getConnectorHandle(),\n+                transactionManager.getConnectorTransaction(transactionId, tableHandle.getConnectorId()),\n+                tableHandle.getLayout(),\n+                tableHandle.getDynamicFilter());\n+\n+        try (SplitSource splitSource = splitManager.getSplits(session, newTableHandle, SplitSchedulingStrategy.UNGROUPED_SCHEDULING, WarningCollector.NOOP)) {\n+            int splits = 0;\n+            while (!splitSource.isFinished()) {\n+                splits += splitSource.getNextBatch(NOT_PARTITIONED, Lifespan.taskWide(), 1024).get().getSplits().size();\n+            }\n+            assertTrue(splitSource.isFinished());\n+            return splits;\n+        }\n+        catch (ExecutionException | InterruptedException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n     private void validateSplitsPlannedForSql(SplitManager splitManager,\n             TransactionManager transactionManager,\n             boolean filterPushdown,\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\nindex 271137a0e52f8..92016c4e263f9 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\n@@ -23,6 +23,7 @@\n import com.facebook.presto.tests.DistributedQueryRunner;\n import com.facebook.presto.transaction.TransactionManager;\n import com.google.common.collect.ImmutableMap;\n+import io.airlift.units.DataSize;\n import org.testng.annotations.AfterClass;\n import org.testng.annotations.BeforeClass;\n import org.testng.annotations.Test;\n@@ -258,11 +259,11 @@ protected void checkTableProperties(String tableName, String deleteMode)\n     {\n         assertQuery(String.format(\"SHOW COLUMNS FROM test_schema.\\\"%s$properties\\\"\", tableName),\n                 \"VALUES ('key', 'varchar', '', ''),\" + \"('value', 'varchar', '', '')\");\n-        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 8\");\n+        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 9\");\n         List<MaterializedRow> materializedRows = computeActual(getSession(),\n                 String.format(\"SELECT * FROM test_schema.\\\"%s$properties\\\"\", tableName)).getMaterializedRows();\n \n-        assertThat(materializedRows).hasSize(8);\n+        assertThat(materializedRows).hasSize(9);\n         assertThat(materializedRows)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.delete.mode\", deleteMode)))\n@@ -279,18 +280,20 @@ protected void checkTableProperties(String tableName, String deleteMode)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.delete-after-commit.enabled\", \"false\")))\n                 .anySatisfy(row -> assertThat(row)\n-                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")));\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")))\n+                .anySatisfy(row -> assertThat(row)\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, IcebergTableProperties.TARGET_SPLIT_SIZE, Long.toString(DataSize.valueOf(\"128MB\").toBytes()))));\n     }\n \n     protected void checkORCFormatTableProperties(String tableName, String deleteMode)\n     {\n         assertQuery(String.format(\"SHOW COLUMNS FROM test_schema.\\\"%s$properties\\\"\", tableName),\n                 \"VALUES ('key', 'varchar', '', ''),\" + \"('value', 'varchar', '', '')\");\n-        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 9\");\n+        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 10\");\n         List<MaterializedRow> materializedRows = computeActual(getSession(),\n                 String.format(\"SELECT * FROM test_schema.\\\"%s$properties\\\"\", tableName)).getMaterializedRows();\n \n-        assertThat(materializedRows).hasSize(9);\n+        assertThat(materializedRows).hasSize(10);\n         assertThat(materializedRows)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.delete.mode\", deleteMode)))\n@@ -309,7 +312,9 @@ protected void checkORCFormatTableProperties(String tableName, String deleteMode\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.delete-after-commit.enabled\", \"false\")))\n                 .anySatisfy(row -> assertThat(row)\n-                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")));\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")))\n+                .anySatisfy(row -> assertThat(row)\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, IcebergTableProperties.TARGET_SPLIT_SIZE, Long.toString(DataSize.valueOf(\"128MB\").toBytes()))));\n     }\n \n     @Test\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java\nindex 5c8781d3dcfe1..47c06c962a9e4 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergUtil.java\n@@ -41,9 +41,11 @@\n import static com.facebook.presto.iceberg.IcebergUtil.REAL_POSITIVE_INFINITE;\n import static com.facebook.presto.iceberg.IcebergUtil.REAL_POSITIVE_ZERO;\n import static com.facebook.presto.iceberg.IcebergUtil.getAdjacentValue;\n+import static com.facebook.presto.iceberg.IcebergUtil.getTargetSplitSize;\n import static java.lang.Double.longBitsToDouble;\n import static java.lang.Float.intBitsToFloat;\n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n \n public class TestIcebergUtil\n {\n@@ -368,4 +370,11 @@ public void testNextValueForOtherType()\n                 encodeScaledValue(new BigDecimal(111111111111111123.45)), false))\n                 .isEmpty();\n     }\n+\n+    @Test\n+    public void testGetTargetSplitSize()\n+    {\n+        assertEquals(1024, getTargetSplitSize(1024, 512).toBytes());\n+        assertEquals(512, getTargetSplitSize(0, 512).toBytes());\n+    }\n }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java\nindex 5f606777596d0..e898da70777df 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSystemTablesNessie.java\n@@ -16,6 +16,7 @@\n import com.facebook.presto.Session;\n import com.facebook.presto.iceberg.IcebergConfig;\n import com.facebook.presto.iceberg.IcebergPlugin;\n+import com.facebook.presto.iceberg.IcebergTableProperties;\n import com.facebook.presto.iceberg.TestIcebergSystemTables;\n import com.facebook.presto.testing.MaterializedResult;\n import com.facebook.presto.testing.MaterializedRow;\n@@ -23,6 +24,7 @@\n import com.facebook.presto.testing.containers.NessieContainer;\n import com.facebook.presto.tests.DistributedQueryRunner;\n import com.google.common.collect.ImmutableMap;\n+import io.airlift.units.DataSize;\n import org.testng.annotations.AfterClass;\n import org.testng.annotations.BeforeClass;\n \n@@ -89,11 +91,11 @@ protected void checkTableProperties(String tableName, String deleteMode)\n     {\n         assertQuery(String.format(\"SHOW COLUMNS FROM test_schema.\\\"%s$properties\\\"\", tableName),\n                 \"VALUES ('key', 'varchar', '', ''),\" + \"('value', 'varchar', '', '')\");\n-        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 10\");\n+        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 11\");\n         List<MaterializedRow> materializedRows = computeActual(getSession(),\n                 String.format(\"SELECT * FROM test_schema.\\\"%s$properties\\\"\", tableName)).getMaterializedRows();\n \n-        assertThat(materializedRows).hasSize(10);\n+        assertThat(materializedRows).hasSize(11);\n         assertThat(materializedRows)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.delete.mode\", deleteMode)))\n@@ -110,7 +112,9 @@ protected void checkTableProperties(String tableName, String deleteMode)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.previous-versions-max\", \"100\")))\n                 .anySatisfy(row -> assertThat(row)\n-                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")));\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")))\n+                .anySatisfy(row -> assertThat(row)\n+                        .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, IcebergTableProperties.TARGET_SPLIT_SIZE, Long.toString(DataSize.valueOf(\"128MB\").toBytes()))));\n     }\n \n     @Override\n@@ -118,11 +122,11 @@ protected void checkORCFormatTableProperties(String tableName, String deleteMode\n     {\n         assertQuery(String.format(\"SHOW COLUMNS FROM test_schema.\\\"%s$properties\\\"\", tableName),\n                 \"VALUES ('key', 'varchar', '', ''),\" + \"('value', 'varchar', '', '')\");\n-        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 11\");\n+        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 12\");\n         List<MaterializedRow> materializedRows = computeActual(getSession(),\n                 String.format(\"SELECT * FROM test_schema.\\\"%s$properties\\\"\", tableName)).getMaterializedRows();\n \n-        assertThat(materializedRows).hasSize(11);\n+        assertThat(materializedRows).hasSize(12);\n         assertThat(materializedRows)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.delete.mode\", deleteMode)))\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java\nindex ef809bbcf725d..9c0b900cc89d9 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestSetTablePropertyProcedure.java\n@@ -33,6 +33,7 @@\n import static com.facebook.presto.iceberg.IcebergQueryRunner.createIcebergQueryRunner;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.getIcebergDataDirectoryPath;\n import static java.lang.String.format;\n+import static org.apache.iceberg.TableProperties.SPLIT_SIZE_DEFAULT;\n import static org.testng.Assert.assertEquals;\n \n public class TestSetTablePropertyProcedure\n@@ -71,8 +72,8 @@ public void testSetTablePropertyProcedurePositionalArgs()\n             Table table = loadTable(tableName);\n             table.refresh();\n \n-            assertEquals(table.properties().size(), 8);\n-            assertEquals(table.properties().get(propertyKey), null);\n+            assertEquals(table.properties().size(), 9);\n+            assertEquals(Long.parseLong(table.properties().get(propertyKey)), SPLIT_SIZE_DEFAULT);\n \n             assertUpdate(format(\"CALL system.set_table_property('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, propertyKey, propertyValue));\n             table.refresh();\n@@ -99,8 +100,8 @@ public void testSetTablePropertyProcedureNamedArgs()\n             Table table = loadTable(tableName);\n             table.refresh();\n \n-            assertEquals(table.properties().size(), 8);\n-            assertEquals(table.properties().get(propertyKey), null);\n+            assertEquals(table.properties().size(), 9);\n+            assertEquals(Long.parseLong(table.properties().get(propertyKey)), SPLIT_SIZE_DEFAULT);\n \n             assertUpdate(format(\"CALL system.set_table_property(schema => '%s', key => '%s', value => '%s', table_name => '%s')\",\n                     TEST_SCHEMA, propertyKey, propertyValue, tableName));\n@@ -129,14 +130,14 @@ public void testSetTablePropertyProcedureUpdateExisting()\n             Table table = loadTable(tableName);\n             table.refresh();\n \n-            assertEquals(table.properties().size(), 8);\n+            assertEquals(table.properties().size(), 9);\n             assertEquals(table.properties().get(propertyKey), \"4\");\n \n             assertUpdate(format(\"CALL system.set_table_property('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, propertyKey, propertyValue));\n             table.refresh();\n \n             // now the table property commit.retry.num-retries should have new value\n-            assertEquals(table.properties().size(), 8);\n+            assertEquals(table.properties().size(), 9);\n             assertEquals(table.properties().get(propertyKey), propertyValue);\n         }\n         finally {\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\nindex 6131b16b751bd..b5aa83bfe05d7 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n@@ -181,6 +181,7 @@ public void testShowCreateTable()\n                         \"   metadata_delete_after_commit = false,\\n\" +\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                         \")\", schemaName, getLocation(schemaName, \"orders\")));\n     }\n@@ -217,6 +218,7 @@ public void testTableComments()\n                 \"   metadata_delete_after_commit = false,\\n\" +\n                 \"   metadata_previous_versions_max = 100,\\n\" +\n                 \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                 \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                 \")\";\n         String createTableSql = format(createTableTemplate, schemaName, \"test table comment\", getLocation(schemaName, \"test_table_comments\"));\n@@ -258,6 +260,7 @@ protected void testCreatePartitionedTableAs(Session session, FileFormat fileForm\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n                         \"   partitioning = ARRAY['order_status','ship_priority','bucket(order_key, 9)'],\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n                         \")\",\n                 getSession().getCatalog().get(),\n@@ -319,6 +322,7 @@ protected void testCreateTableWithFormatVersion(String formatVersion, String def\n                         \"   metadata_delete_after_commit = false,\\n\" +\n                         \"   metadata_previous_versions_max = 100,\\n\" +\n                         \"   metrics_max_inferred_column = 100,\\n\" +\n+                        \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n                         \"   \\\"write.update.mode\\\" = '%s'\\n\" +\n                         \")\",\n                 getSession().getCatalog().get(),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24415",
    "pr_id": 24415,
    "issue_id": 22422,
    "repo": "prestodb/presto",
    "problem_statement": "TestIcebergParquetMetadataCaching.testParquetMetadataCaching looks flaky\nLikely flaky; need to rerun to be sure\r\n\r\n```\r\n2024-04-04T16:12:10.2637984Z [ERROR] Tests run: 2084, Failures: 1, Errors: 0, Skipped: 23, Time elapsed: 1,512.985 s <<< FAILURE! - in TestSuite\r\n2024-04-04T16:12:10.2640602Z [ERROR] com.facebook.presto.iceberg.TestIcebergParquetMetadataCaching.testParquetMetadataCaching  Time elapsed: 0.401 s  <<< FAILURE!\r\n2024-04-04T16:12:10.2642202Z java.lang.AssertionError: expected [true] but found [false]\r\n2024-04-04T16:12:10.2642946Z \tat org.testng.Assert.fail(Assert.java:110)\r\n2024-04-04T16:12:10.2643619Z \tat org.testng.Assert.failNotEquals(Assert.java:1413)\r\n2024-04-04T16:12:10.2644327Z \tat org.testng.Assert.assertTrue(Assert.java:56)\r\n2024-04-04T16:12:10.2645010Z \tat org.testng.Assert.assertTrue(Assert.java:66)\r\n2024-04-04T16:12:10.2646669Z \tat com.facebook.presto.iceberg.TestIcebergParquetMetadataCaching.testParquetMetadataCaching(TestIcebergParquetMetadataCaching.java:90)\r\n2024-04-04T16:12:10.2648290Z \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n2024-04-04T16:12:10.2649495Z \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n2024-04-04T16:12:10.2650778Z \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n2024-04-04T16:12:10.2651817Z \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n2024-04-04T16:12:10.2652984Z \tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n2024-04-04T16:12:10.2654377Z \tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n2024-04-04T16:12:10.2655619Z \tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n2024-04-04T16:12:10.2656869Z \tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n2024-04-04T16:12:10.2658170Z \tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n2024-04-04T16:12:10.2659474Z \tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n2024-04-04T16:12:10.2660890Z \tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n2024-04-04T16:12:10.2662323Z \tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n2024-04-04T16:12:10.2663650Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n2024-04-04T16:12:10.2664963Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2024-04-04T16:12:10.2665931Z \tat java.lang.Thread.run(Thread.java:750)\r\n2024-04-04T16:12:10.2666347Z \r\n2024-04-04T16:12:10.9334912Z [INFO] \r\n2024-04-04T16:12:10.9335288Z [INFO] Results:\r\n2024-04-04T16:12:10.9335694Z [INFO] \r\n2024-04-04T16:12:10.9336053Z [ERROR] Failures: \r\n2024-04-04T16:12:10.9337099Z [ERROR]   TestIcebergParquetMetadataCaching.testParquetMetadataCaching:90 expected [true] but found [false]\r\n2024-04-04T16:12:10.9338247Z [INFO] \r\n2024-04-04T16:12:10.9338794Z [ERROR] Tests run: 2084, Failures: 1, Errors: 0, Skipped: 23\r\n2024-04-04T16:12:10.9339232Z [INFO] \r\n```",
    "issue_word_count": 453,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java"
    ],
    "base_commit": "84955a83df021302b179a6e798a27f3d4dec3f9b",
    "head_commit": "7fd56185b9479b8b82136ade6ee217c20f0dac47",
    "repo_url": "https://github.com/prestodb/presto/pull/24415",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24415",
    "dockerfile": "",
    "pr_merged_at": "2025-01-23T14:50:19.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\nindex 65d4158f18dfa..ff6e311deb697 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergParquetMetadataCaching.java\n@@ -45,7 +45,7 @@ protected QueryRunner createQueryRunner()\n                 PARQUET,\n                 false,\n                 true,\n-                OptionalInt.of(2),\n+                OptionalInt.of(1),\n                 Optional.empty());\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24407",
    "pr_id": 24407,
    "issue_id": 24389,
    "repo": "prestodb/presto",
    "problem_statement": "Add authentication layer to presto router\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\n\n## Expected Behavior or Use Case\nThe presto router needs an authentication layer in order to send queries through to the coordinator\n\n## Presto Component, Service, or Connector\npresto-router\n\n## Possible Implementation\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\n\n## Example Screenshots (if appropriate):\n\n## Context\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 121,
    "test_files_count": 6,
    "non_test_files_count": 16,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/router/deployment.rst",
      "presto-docs/src/main/sphinx/security/password-file.rst",
      "presto-main-base/src/main/java/com/facebook/presto/server/PluginInstaller.java",
      "presto-main-base/src/main/java/com/facebook/presto/server/PluginManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/server/PluginManagerUtil.java",
      "presto-router/etc/config.properties",
      "presto-router/pom.xml",
      "presto-router/src/main/java/com/facebook/presto/router/PrestoRouter.java",
      "presto-router/src/main/java/com/facebook/presto/router/RouterModule.java",
      "presto-router/src/main/java/com/facebook/presto/router/RouterPluginManager.java",
      "presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteClusterInfo.java",
      "presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteInfoFactory.java",
      "presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteQueryInfo.java",
      "presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java",
      "presto-router/src/main/java/com/facebook/presto/router/security/RouterSecurityModule.java",
      "presto-router/src/main/java/com/facebook/presto/router/spec/RouterSpec.java",
      "presto-router/src/main/resources/router_ui/src/components/PageTitle.jsx",
      "presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestRouterAuthentication.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java"
    ],
    "pr_changed_test_files": [
      "presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestRouterAuthentication.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java",
      "presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java"
    ],
    "base_commit": "09175f3244f038dd725eb2c4ff2c31cbe76063a5",
    "head_commit": "b718e74a21fbbfce4309fd3f4f34b274f9fcef8c",
    "repo_url": "https://github.com/prestodb/presto/pull/24407",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24407",
    "dockerfile": "",
    "pr_merged_at": "2025-05-13T14:57:32.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/router/deployment.rst b/presto-docs/src/main/sphinx/router/deployment.rst\nindex 2b8636db67a58..04f4956db8e52 100644\n--- a/presto-docs/src/main/sphinx/router/deployment.rst\n+++ b/presto-docs/src/main/sphinx/router/deployment.rst\n@@ -109,6 +109,7 @@ The following provides an example of ``etc/router-config.json``.\n       ],\n       \"scheduler\": \"RANDOM_CHOICE\",\n       \"predictor\": \"http://127.0.0.1:8000/v1\"\n+      \"user-credentials\": \"username:passwordhash\"\n     }\n \n These properties requires some explanation:\n@@ -133,6 +134,18 @@ These properties requires some explanation:\n   query resource usage information from the predictor for scheduling.\n   The default is *http://127.0.0.1:8000/v1*.\n \n+* ``user-credentials``:\n+  An optional parameter to specify to the router which credentials to use when communicating\n+  with Presto coordinator endpoints.\n+\n+.. _authentication:\n+\n+Authentication\n+------------------\n+The router supports password file based authentication. This can be configured in the same\n+way that it is with the regular Presto coordinator but within the router module (the ``etc`` folder\n+within presto-router). See :ref:`Password File Authentication <password_file_auth>`.\n+\n .. _running_router:\n \n Running Router\n\ndiff --git a/presto-docs/src/main/sphinx/security/password-file.rst b/presto-docs/src/main/sphinx/security/password-file.rst\nindex 4939262e86157..8d2754a15d3ce 100644\n--- a/presto-docs/src/main/sphinx/security/password-file.rst\n+++ b/presto-docs/src/main/sphinx/security/password-file.rst\n@@ -1,3 +1,5 @@\n+.. _password_file_auth:\n+\n ============================\n Password File Authentication\n ============================\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/server/PluginInstaller.java b/presto-main-base/src/main/java/com/facebook/presto/server/PluginInstaller.java\nnew file mode 100644\nindex 0000000000000..bebdf23b6607c\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/server/PluginInstaller.java\n@@ -0,0 +1,24 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.server;\n+\n+import com.facebook.presto.spi.CoordinatorPlugin;\n+import com.facebook.presto.spi.Plugin;\n+\n+public interface PluginInstaller\n+{\n+    void installPlugin(Plugin plugin);\n+\n+    void installCoordinatorPlugin(CoordinatorPlugin plugin);\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/server/PluginManager.java b/presto-main-base/src/main/java/com/facebook/presto/server/PluginManager.java\nindex af0f0f3bee089..0693d90bb9cc3 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/server/PluginManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/server/PluginManager.java\n@@ -34,7 +34,6 @@\n import com.facebook.presto.spi.Plugin;\n import com.facebook.presto.spi.analyzer.AnalyzerProvider;\n import com.facebook.presto.spi.analyzer.QueryPreparerProvider;\n-import com.facebook.presto.spi.classloader.ThreadContextClassLoader;\n import com.facebook.presto.spi.connector.ConnectorFactory;\n import com.facebook.presto.spi.eventlistener.EventListenerFactory;\n import com.facebook.presto.spi.function.FunctionNamespaceManagerFactory;\n@@ -64,56 +63,22 @@\n import com.facebook.presto.ttl.nodettlfetchermanagers.NodeTtlFetcherManager;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n-import com.google.common.collect.Ordering;\n import io.airlift.resolver.ArtifactResolver;\n-import io.airlift.resolver.DefaultArtifact;\n-import org.sonatype.aether.artifact.Artifact;\n \n import javax.annotation.concurrent.ThreadSafe;\n import javax.inject.Inject;\n \n import java.io.File;\n-import java.io.IOException;\n-import java.net.URL;\n-import java.net.URLClassLoader;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n import java.util.List;\n-import java.util.ServiceLoader;\n-import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import static com.facebook.presto.metadata.FunctionExtractor.extractFunctions;\n-import static com.facebook.presto.server.PluginDiscovery.discoverPlugins;\n-import static com.facebook.presto.server.PluginDiscovery.writePluginServices;\n+import static com.facebook.presto.server.PluginManagerUtil.SPI_PACKAGES;\n import static java.util.Objects.requireNonNull;\n \n @ThreadSafe\n public class PluginManager\n {\n-    // When generating code the AfterBurner module loads classes with *some* classloader.\n-    // When the AfterBurner module is configured not to use the value classloader\n-    // (e.g., AfterBurner().setUseValueClassLoader(false)) AppClassLoader is used for loading those\n-    // classes. Otherwise, the PluginClassLoader is used, which is the default behavior.\n-    // Therefore, in the former case Afterburner won't be able to load the connector classes\n-    // as AppClassLoader doesn't see them, and in the latter case the PluginClassLoader won't be\n-    // able to load the AfterBurner classes themselves. So, our solution is to use the PluginClassLoader\n-    // and whitelist the AfterBurner classes here, so that the PluginClassLoader can load the\n-    // AfterBurner classes.\n-    private static final ImmutableList<String> SPI_PACKAGES = ImmutableList.<String>builder()\n-            .add(\"com.facebook.presto.spi.\")\n-            .add(\"com.fasterxml.jackson.annotation.\")\n-            .add(\"com.fasterxml.jackson.module.afterburner.\")\n-            .add(\"io.airlift.slice.\")\n-            .add(\"io.airlift.units.\")\n-            .add(\"org.openjdk.jol.\")\n-            .add(\"com.facebook.presto.common\")\n-            .add(\"com.facebook.drift.annotations.\")\n-            .add(\"com.facebook.drift.TException\")\n-            .add(\"com.facebook.drift.TApplicationException\")\n-            .build();\n-\n     //  TODO: To make CoordinatorPlugin loading compulsory when native execution is enabled.\n     private static final String COORDINATOR_PLUGIN_SERVICES_FILE = \"META-INF/services/\" + CoordinatorPlugin.class.getName();\n     private static final String PLUGIN_SERVICES_FILE = \"META-INF/services/\" + Plugin.class.getName();\n@@ -145,6 +110,7 @@ public class PluginManager\n     private final ClientRequestFilterManager clientRequestFilterManager;\n     private final PlanCheckerProviderManager planCheckerProviderManager;\n     private final ExpressionOptimizerManager expressionOptimizerManager;\n+    private final PluginInstaller pluginInstaller;\n \n     @Inject\n     public PluginManager(\n@@ -206,63 +172,24 @@ public PluginManager(\n         this.clientRequestFilterManager = requireNonNull(clientRequestFilterManager, \"clientRequestFilterManager is null\");\n         this.planCheckerProviderManager = requireNonNull(planCheckerProviderManager, \"planCheckerProviderManager is null\");\n         this.expressionOptimizerManager = requireNonNull(expressionOptimizerManager, \"expressionManager is null\");\n+        this.pluginInstaller = new MainPluginInstaller(this);\n     }\n \n     public void loadPlugins()\n             throws Exception\n     {\n-        if (!pluginsLoading.compareAndSet(false, true)) {\n-            return;\n-        }\n-\n-        for (File file : listFiles(installedPluginsDir)) {\n-            if (file.isDirectory()) {\n-                loadPlugin(file.getAbsolutePath());\n-            }\n-        }\n-\n-        for (String plugin : plugins) {\n-            loadPlugin(plugin);\n-        }\n-\n-        metadata.verifyComparableOrderableContract();\n-\n-        pluginsLoaded.set(true);\n-    }\n-\n-    private void loadPlugin(String plugin)\n-            throws Exception\n-    {\n-        log.info(\"-- Loading plugin %s --\", plugin);\n-        URLClassLoader pluginClassLoader = buildClassLoader(plugin);\n-        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(pluginClassLoader)) {\n-            loadPlugin(pluginClassLoader, CoordinatorPlugin.class);\n-            loadPlugin(pluginClassLoader, Plugin.class);\n-        }\n-        log.info(\"-- Finished loading plugin %s --\", plugin);\n-    }\n-\n-    private void loadPlugin(URLClassLoader pluginClassLoader, Class<?> clazz)\n-    {\n-        ServiceLoader<?> serviceLoader = ServiceLoader.load(clazz, pluginClassLoader);\n-        List<?> plugins = ImmutableList.copyOf(serviceLoader);\n-\n-        if (plugins.isEmpty()) {\n-            log.warn(\"No service providers of type %s\", clazz.getName());\n-        }\n-\n-        for (Object plugin : plugins) {\n-            log.info(\"Installing %s\", plugin.getClass().getName());\n-            if (plugin instanceof Plugin) {\n-                installPlugin((Plugin) plugin);\n-            }\n-            else if (plugin instanceof CoordinatorPlugin) {\n-                installCoordinatorPlugin((CoordinatorPlugin) plugin);\n-            }\n-            else {\n-                log.warn(\"Unknown plugin type: %s\", plugin.getClass().getName());\n-            }\n-        }\n+        PluginManagerUtil.loadPlugins(\n+                pluginsLoading,\n+                pluginsLoaded,\n+                installedPluginsDir,\n+                plugins,\n+                metadata,\n+                resolver,\n+                SPI_PACKAGES,\n+                COORDINATOR_PLUGIN_SERVICES_FILE,\n+                PLUGIN_SERVICES_FILE,\n+                pluginInstaller,\n+                getClass().getClassLoader());\n     }\n \n     public void installPlugin(Plugin plugin)\n@@ -410,100 +337,26 @@ public void installCoordinatorPlugin(CoordinatorPlugin plugin)\n         }\n     }\n \n-    private URLClassLoader buildClassLoader(String plugin)\n-            throws Exception\n+    private class MainPluginInstaller\n+            implements PluginInstaller\n     {\n-        File file = new File(plugin);\n-        if (file.isFile() && (file.getName().equals(\"pom.xml\") || file.getName().endsWith(\".pom\"))) {\n-            return buildClassLoaderFromPom(file);\n-        }\n-        if (file.isDirectory()) {\n-            return buildClassLoaderFromDirectory(file);\n-        }\n-        return buildClassLoaderFromCoordinates(plugin);\n-    }\n+        private final PluginManager pluginManager;\n \n-    private URLClassLoader buildClassLoaderFromPom(File pomFile)\n-            throws Exception\n-    {\n-        List<Artifact> artifacts = resolver.resolvePom(pomFile);\n-        URLClassLoader classLoader = createClassLoader(artifacts, pomFile.getPath());\n-\n-        Artifact artifact = artifacts.get(0);\n-\n-        processPlugins(artifact, classLoader, COORDINATOR_PLUGIN_SERVICES_FILE, CoordinatorPlugin.class.getName());\n-        processPlugins(artifact, classLoader, PLUGIN_SERVICES_FILE, Plugin.class.getName());\n-\n-        return classLoader;\n-    }\n-\n-    private URLClassLoader buildClassLoaderFromDirectory(File dir)\n-            throws Exception\n-    {\n-        log.debug(\"Classpath for %s:\", dir.getName());\n-        List<URL> urls = new ArrayList<>();\n-        for (File file : listFiles(dir)) {\n-            log.debug(\"    %s\", file);\n-            urls.add(file.toURI().toURL());\n-        }\n-        return createClassLoader(urls);\n-    }\n-\n-    private URLClassLoader buildClassLoaderFromCoordinates(String coordinates)\n-            throws Exception\n-    {\n-        Artifact rootArtifact = new DefaultArtifact(coordinates);\n-        List<Artifact> artifacts = resolver.resolveArtifacts(rootArtifact);\n-        return createClassLoader(artifacts, rootArtifact.toString());\n-    }\n-\n-    private URLClassLoader createClassLoader(List<Artifact> artifacts, String name)\n-            throws IOException\n-    {\n-        log.debug(\"Classpath for %s:\", name);\n-        List<URL> urls = new ArrayList<>();\n-        for (Artifact artifact : sortedArtifacts(artifacts)) {\n-            if (artifact.getFile() == null) {\n-                throw new RuntimeException(\"Could not resolve artifact: \" + artifact);\n-            }\n-            File file = artifact.getFile().getCanonicalFile();\n-            log.debug(\"    %s\", file);\n-            urls.add(file.toURI().toURL());\n+        public MainPluginInstaller(PluginManager pluginManager)\n+        {\n+            this.pluginManager = pluginManager;\n         }\n-        return createClassLoader(urls);\n-    }\n-\n-    private URLClassLoader createClassLoader(List<URL> urls)\n-    {\n-        ClassLoader parent = getClass().getClassLoader();\n-        return new PluginClassLoader(urls, parent, SPI_PACKAGES);\n-    }\n \n-    private static List<File> listFiles(File installedPluginsDir)\n-    {\n-        if (installedPluginsDir != null && installedPluginsDir.isDirectory()) {\n-            File[] files = installedPluginsDir.listFiles();\n-            if (files != null) {\n-                Arrays.sort(files);\n-                return ImmutableList.copyOf(files);\n-            }\n+        @Override\n+        public void installPlugin(Plugin plugin)\n+        {\n+            pluginManager.installPlugin(plugin);\n         }\n-        return ImmutableList.of();\n-    }\n \n-    private static List<Artifact> sortedArtifacts(List<Artifact> artifacts)\n-    {\n-        List<Artifact> list = new ArrayList<>(artifacts);\n-        Collections.sort(list, Ordering.natural().nullsLast().onResultOf(Artifact::getFile));\n-        return list;\n-    }\n-\n-    private void processPlugins(Artifact artifact, ClassLoader classLoader, String servicesFile, String className)\n-            throws IOException\n-    {\n-        Set<String> plugins = discoverPlugins(artifact, classLoader, servicesFile, className);\n-        if (!plugins.isEmpty()) {\n-            writePluginServices(plugins, artifact.getFile(), servicesFile);\n+        @Override\n+        public void installCoordinatorPlugin(CoordinatorPlugin plugin)\n+        {\n+            pluginManager.installCoordinatorPlugin(plugin);\n         }\n     }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/server/PluginManagerUtil.java b/presto-main-base/src/main/java/com/facebook/presto/server/PluginManagerUtil.java\nnew file mode 100644\nindex 0000000000000..93ddb7d27da05\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/server/PluginManagerUtil.java\n@@ -0,0 +1,291 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.server;\n+\n+import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.spi.CoordinatorPlugin;\n+import com.facebook.presto.spi.Plugin;\n+import com.facebook.presto.spi.classloader.ThreadContextClassLoader;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Ordering;\n+import io.airlift.resolver.ArtifactResolver;\n+import io.airlift.resolver.DefaultArtifact;\n+import org.sonatype.aether.artifact.Artifact;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.net.URLClassLoader;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.ServiceLoader;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.facebook.presto.server.PluginDiscovery.discoverPlugins;\n+import static com.facebook.presto.server.PluginDiscovery.writePluginServices;\n+\n+public class PluginManagerUtil\n+{\n+    private static final Logger log = Logger.get(PluginManagerUtil.class);\n+\n+    /*\n+     When generating code the AfterBurner module loads classes with *some* classloader.\n+     When the AfterBurner module is configured not to use the value classloader\n+     (e.g., AfterBurner().setUseValueClassLoader(false)) AppClassLoader is used for loading those\n+     classes. Otherwise, the PluginClassLoader is used, which is the default behavior.\n+     Therefore, in the former case Afterburner won't be able to load the connector classes\n+     as AppClassLoader doesn't see them, and in the latter case the PluginClassLoader won't be\n+     able to load the AfterBurner classes themselves. So, our solution is to use the PluginClassLoader\n+     and whitelist the AfterBurner classes here, so that the PluginClassLoader can load the\n+     AfterBurner classes.\n+     */\n+    public static final ImmutableList<String> SPI_PACKAGES = ImmutableList.<String>builder()\n+            .add(\"com.facebook.presto.spi.\")\n+            .add(\"com.fasterxml.jackson.annotation.\")\n+            .add(\"com.fasterxml.jackson.module.afterburner.\")\n+            .add(\"io.airlift.slice.\")\n+            .add(\"io.airlift.units.\")\n+            .add(\"org.openjdk.jol.\")\n+            .add(\"com.facebook.presto.common\")\n+            .add(\"com.facebook.drift.annotations.\")\n+            .add(\"com.facebook.drift.TException\")\n+            .add(\"com.facebook.drift.TApplicationException\")\n+            .build();\n+\n+    private PluginManagerUtil()\n+    {\n+    }\n+\n+    public static void loadPlugins(\n+            AtomicBoolean pluginsLoading,\n+            AtomicBoolean pluginsLoaded,\n+            File installedPluginsDir,\n+            List<String> plugins,\n+            Metadata metadata,\n+            ArtifactResolver resolver,\n+            List<String> spiPackages,\n+            String coordinatorPluginServicesFile,\n+            String pluginServicesFile,\n+            PluginInstaller pluginInstaller,\n+            ClassLoader parent)\n+            throws Exception\n+    {\n+        if (!pluginsLoading.compareAndSet(false, true)) {\n+            return;\n+        }\n+\n+        for (File file : listFiles(installedPluginsDir)) {\n+            if (file.isDirectory()) {\n+                loadPlugin(\n+                        file.getAbsolutePath(),\n+                        resolver,\n+                        spiPackages,\n+                        coordinatorPluginServicesFile,\n+                        pluginServicesFile,\n+                        pluginInstaller,\n+                        parent);\n+            }\n+        }\n+\n+        for (String plugin : plugins) {\n+            loadPlugin(\n+                    plugin,\n+                    resolver,\n+                    spiPackages,\n+                    coordinatorPluginServicesFile,\n+                    pluginServicesFile,\n+                    pluginInstaller,\n+                    parent);\n+        }\n+\n+        if (metadata != null) {\n+            metadata.verifyComparableOrderableContract();\n+        }\n+\n+        pluginsLoaded.set(true);\n+    }\n+\n+    public static void loadPlugin(\n+            String plugin,\n+            ArtifactResolver resolver,\n+            List<String> spiPackages,\n+            String coordinatorPluginServicesFile,\n+            String pluginServicesFile,\n+            PluginInstaller pluginInstaller,\n+            ClassLoader parent)\n+            throws Exception\n+    {\n+        log.info(\"-- Loading plugin %s --\", plugin);\n+        URLClassLoader pluginClassLoader = buildClassLoader(\n+                plugin,\n+                resolver,\n+                spiPackages,\n+                coordinatorPluginServicesFile,\n+                pluginServicesFile,\n+                parent);\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(pluginClassLoader)) {\n+            loadPlugin(pluginClassLoader, CoordinatorPlugin.class, pluginInstaller);\n+            loadPlugin(pluginClassLoader, Plugin.class, pluginInstaller);\n+        }\n+        log.info(\"-- Finished loading plugin %s --\", plugin);\n+    }\n+\n+    public static void loadPlugin(\n+            URLClassLoader pluginClassLoader,\n+            Class<?> clazz,\n+            PluginInstaller pluginInstaller)\n+    {\n+        ServiceLoader<?> serviceLoader = ServiceLoader.load(clazz, pluginClassLoader);\n+        List<?> plugins = ImmutableList.copyOf(serviceLoader);\n+\n+        if (plugins.isEmpty()) {\n+            log.warn(\"No service providers of type %s\", clazz.getName());\n+        }\n+\n+        for (Object plugin : plugins) {\n+            log.info(\"Installing %s\", plugin.getClass().getName());\n+            if (plugin instanceof Plugin) {\n+                pluginInstaller.installPlugin((Plugin) plugin);\n+            }\n+            else if (plugin instanceof CoordinatorPlugin) {\n+                pluginInstaller.installCoordinatorPlugin((CoordinatorPlugin) plugin);\n+            }\n+            else {\n+                log.warn(\"Unknown plugin type: %s\", plugin.getClass().getName());\n+            }\n+        }\n+    }\n+\n+    private static URLClassLoader buildClassLoader(\n+            String plugin,\n+            ArtifactResolver resolver,\n+            List<String> spiPackages,\n+            String coordinatorPluginServicesFile,\n+            String pluginServicesFile,\n+            ClassLoader parent)\n+            throws Exception\n+    {\n+        File file = new File(plugin);\n+        if (file.isFile() && (file.getName().equals(\"pom.xml\") || file.getName().endsWith(\".pom\"))) {\n+            return buildClassLoaderFromPom(file, resolver, spiPackages, coordinatorPluginServicesFile, pluginServicesFile, parent);\n+        }\n+        if (file.isDirectory()) {\n+            return buildClassLoaderFromDirectory(file, spiPackages, parent);\n+        }\n+        return buildClassLoaderFromCoordinates(plugin, resolver, spiPackages, parent);\n+    }\n+\n+    private static URLClassLoader buildClassLoaderFromPom(\n+            File pomFile,\n+            ArtifactResolver resolver,\n+            List<String> spiPackages,\n+            String coordinatorPluginServicesFile,\n+            String pluginServicesFile,\n+            ClassLoader parent)\n+            throws Exception\n+    {\n+        List<Artifact> artifacts = resolver.resolvePom(pomFile);\n+        URLClassLoader classLoader = createClassLoader(artifacts, pomFile.getPath(), spiPackages, parent);\n+\n+        Artifact artifact = artifacts.get(0);\n+\n+        processPlugins(artifact, classLoader, pluginServicesFile, Plugin.class.getName());\n+        if (coordinatorPluginServicesFile != null) {\n+            processPlugins(artifact, classLoader, coordinatorPluginServicesFile, CoordinatorPlugin.class.getName());\n+        }\n+\n+        return classLoader;\n+    }\n+\n+    private static URLClassLoader buildClassLoaderFromDirectory(File dir, List<String> spiPackages, ClassLoader parent)\n+            throws Exception\n+    {\n+        log.debug(\"Classpath for %s:\", dir.getName());\n+        List<URL> urls = new ArrayList<>();\n+        for (File file : listFiles(dir)) {\n+            log.debug(\"    %s\", file);\n+            urls.add(file.toURI().toURL());\n+        }\n+        return createClassLoader(urls, spiPackages, parent);\n+    }\n+\n+    private static URLClassLoader buildClassLoaderFromCoordinates(\n+            String coordinates,\n+            ArtifactResolver resolver,\n+            List<String> spiPackages,\n+            ClassLoader parent)\n+            throws Exception\n+    {\n+        Artifact rootArtifact = new DefaultArtifact(coordinates);\n+        List<Artifact> artifacts = resolver.resolveArtifacts(rootArtifact);\n+        return createClassLoader(artifacts, rootArtifact.toString(), spiPackages, parent);\n+    }\n+\n+    private static URLClassLoader createClassLoader(\n+            List<Artifact> artifacts,\n+            String name,\n+            List<String> spiPackages,\n+            ClassLoader parent)\n+            throws IOException\n+    {\n+        log.debug(\"Classpath for %s:\", name);\n+        List<URL> urls = new ArrayList<>();\n+        for (Artifact artifact : sortedArtifacts(artifacts)) {\n+            if (artifact.getFile() == null) {\n+                throw new RuntimeException(\"Could not resolve artifact: \" + artifact);\n+            }\n+            File file = artifact.getFile().getCanonicalFile();\n+            log.debug(\"    %s\", file);\n+            urls.add(file.toURI().toURL());\n+        }\n+        return createClassLoader(urls, spiPackages, parent);\n+    }\n+\n+    private static URLClassLoader createClassLoader(List<URL> urls, List<String> spiPackages, ClassLoader parent)\n+    {\n+        return new PluginClassLoader(urls, parent, spiPackages);\n+    }\n+\n+    private static List<File> listFiles(File installedPluginsDir)\n+    {\n+        if (installedPluginsDir != null && installedPluginsDir.isDirectory()) {\n+            File[] files = installedPluginsDir.listFiles();\n+            if (files != null) {\n+                Arrays.sort(files);\n+                return ImmutableList.copyOf(files);\n+            }\n+        }\n+        return ImmutableList.of();\n+    }\n+\n+    private static List<Artifact> sortedArtifacts(List<Artifact> artifacts)\n+    {\n+        List<Artifact> list = new ArrayList<>(artifacts);\n+        Collections.sort(list, Ordering.natural().nullsLast().onResultOf(Artifact::getFile));\n+        return list;\n+    }\n+\n+    private static void processPlugins(Artifact artifact, ClassLoader classLoader, String servicesFile, String className)\n+            throws IOException\n+    {\n+        Set<String> plugins = discoverPlugins(artifact, classLoader, servicesFile, className);\n+        if (!plugins.isEmpty()) {\n+            writePluginServices(plugins, artifact.getFile(), servicesFile);\n+        }\n+    }\n+}\n\ndiff --git a/presto-router/etc/config.properties b/presto-router/etc/config.properties\nnew file mode 100644\nindex 0000000000000..e69de29bb2d1d\n\ndiff --git a/presto-router/pom.xml b/presto-router/pom.xml\nindex 599e5af561c18..81920417c4322 100755\n--- a/presto-router/pom.xml\n+++ b/presto-router/pom.xml\n@@ -172,13 +172,17 @@\n             <artifactId>presto-main-base</artifactId>\n         </dependency>\n \n-        <!-- test dependencies-->\n         <dependency>\n             <groupId>com.facebook.presto</groupId>\n             <artifactId>presto-main</artifactId>\n-            <scope>test</scope>\n         </dependency>\n \n+        <dependency>\n+            <groupId>io.airlift.resolver</groupId>\n+            <artifactId>resolver</artifactId>\n+        </dependency>\n+\n+        <!-- test dependencies-->\n         <dependency>\n             <groupId>com.facebook.presto</groupId>\n             <artifactId>presto-main-base</artifactId>\n@@ -187,14 +191,14 @@\n         </dependency>\n \n         <dependency>\n-            <groupId>org.testng</groupId>\n-            <artifactId>testng</artifactId>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-tpch</artifactId>\n             <scope>test</scope>\n         </dependency>\n \n         <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-tpch</artifactId>\n+            <groupId>org.testng</groupId>\n+            <artifactId>testng</artifactId>\n             <scope>test</scope>\n         </dependency>\n \n@@ -209,5 +213,11 @@\n             <artifactId>mockwebserver</artifactId>\n             <scope>test</scope>\n         </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-password-authenticators</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n </project>\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/PrestoRouter.java b/presto-router/src/main/java/com/facebook/presto/router/PrestoRouter.java\nindex 19c30ec1869a4..e03e2d8dfd073 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/PrestoRouter.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/PrestoRouter.java\n@@ -24,7 +24,13 @@\n import com.facebook.airlift.log.Logger;\n import com.facebook.airlift.node.NodeModule;\n import com.facebook.airlift.tracetoken.TraceTokenModule;\n+import com.facebook.presto.ClientRequestFilterManager;\n+import com.facebook.presto.ClientRequestFilterModule;\n+import com.facebook.presto.router.security.RouterSecurityModule;\n+import com.facebook.presto.server.security.PasswordAuthenticatorManager;\n+import com.facebook.presto.server.security.PrestoAuthenticatorManager;\n import com.google.common.collect.ImmutableList;\n+import com.google.inject.Injector;\n import com.google.inject.Module;\n import org.weakref.jmx.guice.MBeanModule;\n \n@@ -39,6 +45,7 @@ public static void start(Module... extraModules)\n         Bootstrap app = new Bootstrap(ImmutableList.<Module>builder()\n                 .add(new NodeModule())\n                 .add(new HttpServerModule())\n+                .add(new ClientRequestFilterModule())\n                 .add(new JsonModule())\n                 .add(new JaxrsModule(true))\n                 .add(new MBeanModule())\n@@ -47,13 +54,18 @@ public static void start(Module... extraModules)\n                 .add(new LogJmxModule())\n                 .add(new TraceTokenModule())\n                 .add(new EventModule())\n+                .add(new RouterSecurityModule())\n                 .add(new RouterModule())\n                 .add(extraModules)\n                 .build());\n \n         Logger log = Logger.get(RouterModule.class);\n         try {\n-            app.initialize();\n+            Injector injector = app.initialize();\n+            injector.getInstance(RouterPluginManager.class).loadPlugins();\n+            injector.getInstance(ClientRequestFilterManager.class).loadClientRequestFilters();\n+            injector.getInstance(PasswordAuthenticatorManager.class).loadPasswordAuthenticator();\n+            injector.getInstance(PrestoAuthenticatorManager.class).loadPrestoAuthenticator();\n             log.info(\"======== SERVER STARTED ========\");\n         }\n         catch (Throwable t) {\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/RouterModule.java b/presto-router/src/main/java/com/facebook/presto/router/RouterModule.java\nindex a5cb2e424eb1a..da08a408b5799 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/RouterModule.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/RouterModule.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.router;\n \n import com.facebook.airlift.configuration.AbstractConfigurationAwareModule;\n+import com.facebook.presto.client.NodeVersion;\n import com.facebook.presto.router.cluster.ClusterManager;\n import com.facebook.presto.router.cluster.ClusterStatusResource;\n import com.facebook.presto.router.cluster.ForClusterInfoTracker;\n@@ -25,6 +26,9 @@\n import com.facebook.presto.router.predictor.ForQueryMemoryPredictor;\n import com.facebook.presto.router.predictor.PredictorManager;\n import com.facebook.presto.router.predictor.RemoteQueryFactory;\n+import com.facebook.presto.server.PluginManagerConfig;\n+import com.facebook.presto.server.ServerConfig;\n+import com.facebook.presto.server.WebUiResource;\n import com.google.inject.Binder;\n import com.google.inject.Scopes;\n import io.airlift.units.Duration;\n@@ -57,23 +61,35 @@ public class RouterModule\n     @Override\n     protected void setup(Binder binder)\n     {\n+        ServerConfig serverConfig = buildConfigObject(ServerConfig.class);\n+\n         httpServerBinder(binder).bindResource(UI_PATH, ROUTER_UI).withWelcomeFile(INDEX_HTML);\n         configBinder(binder).bindConfig(RouterConfig.class);\n \n         configBinder(binder).bindConfig(RemoteStateConfig.class);\n         binder.bind(ScheduledExecutorService.class).annotatedWith(ForClusterManager.class).toInstance(newSingleThreadScheduledExecutor(threadsNamed(\"cluster-config\")));\n \n+        // resource for serving static content\n+        jaxrsBinder(binder).bind(WebUiResource.class);\n+\n         binder.bind(ClusterManager.class).in(Scopes.SINGLETON);\n         binder.bind(RemoteInfoFactory.class).in(Scopes.SINGLETON);\n \n         bindHttpClient(binder, QUERY_TRACKER, ForQueryInfoTracker.class, IDLE_TIMEOUT_SECOND, REQUEST_TIMEOUT_SECOND);\n         bindHttpClient(binder, QUERY_TRACKER, ForClusterInfoTracker.class, IDLE_TIMEOUT_SECOND, REQUEST_TIMEOUT_SECOND);\n \n+        //Determine the NodeVersion\n+        NodeVersion nodeVersion = new NodeVersion(serverConfig.getPrestoVersion());\n+        binder.bind(NodeVersion.class).toInstance(nodeVersion);\n+\n         binder.bind(ClusterStatusTracker.class).in(Scopes.SINGLETON);\n \n         binder.bind(PredictorManager.class).in(Scopes.SINGLETON);\n         binder.bind(RemoteQueryFactory.class).in(Scopes.SINGLETON);\n \n+        binder.bind(RouterPluginManager.class).in(Scopes.SINGLETON);\n+        configBinder(binder).bindConfig(PluginManagerConfig.class);\n+\n         bindHttpClient(binder, QUERY_PREDICTOR, ForQueryCpuPredictor.class, IDLE_TIMEOUT_SECOND, PREDICTOR_REQUEST_TIMEOUT_SECOND);\n         bindHttpClient(binder, QUERY_PREDICTOR, ForQueryMemoryPredictor.class, IDLE_TIMEOUT_SECOND, PREDICTOR_REQUEST_TIMEOUT_SECOND);\n \n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/RouterPluginManager.java b/presto-router/src/main/java/com/facebook/presto/router/RouterPluginManager.java\nnew file mode 100644\nindex 0000000000000..c5b62db5ef109\n--- /dev/null\n+++ b/presto-router/src/main/java/com/facebook/presto/router/RouterPluginManager.java\n@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.router;\n+\n+import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.server.PluginInstaller;\n+import com.facebook.presto.server.PluginManagerConfig;\n+import com.facebook.presto.server.PluginManagerUtil;\n+import com.facebook.presto.server.security.PasswordAuthenticatorManager;\n+import com.facebook.presto.server.security.PrestoAuthenticatorManager;\n+import com.facebook.presto.spi.CoordinatorPlugin;\n+import com.facebook.presto.spi.Plugin;\n+import com.facebook.presto.spi.security.PasswordAuthenticatorFactory;\n+import com.facebook.presto.spi.security.PrestoAuthenticatorFactory;\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.resolver.ArtifactResolver;\n+\n+import javax.annotation.concurrent.ThreadSafe;\n+import javax.inject.Inject;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.facebook.presto.server.PluginManagerUtil.SPI_PACKAGES;\n+import static java.util.Objects.requireNonNull;\n+\n+@ThreadSafe\n+public class RouterPluginManager\n+{\n+    private static final String SERVICES_FILE = \"META-INF/services/\" + Plugin.class.getName();\n+\n+    private static final Logger log = Logger.get(RouterPluginManager.class);\n+\n+    private final PasswordAuthenticatorManager passwordAuthenticatorManager;\n+    private final File installedPluginsDir;\n+    private final List<String> plugins;\n+    private final ArtifactResolver resolver;\n+    private final AtomicBoolean pluginsLoading = new AtomicBoolean();\n+    private final AtomicBoolean pluginsLoaded = new AtomicBoolean();\n+    private final PluginInstaller pluginInstaller;\n+    private final PrestoAuthenticatorManager prestoAuthenticatorManager;\n+\n+    @Inject\n+    public RouterPluginManager(\n+            PluginManagerConfig config,\n+            PasswordAuthenticatorManager passwordAuthenticatorManager,\n+            PrestoAuthenticatorManager prestoAuthenticatorManager)\n+    {\n+        requireNonNull(config, \"config is null\");\n+\n+        this.installedPluginsDir = config.getInstalledPluginsDir();\n+        if (config.getPlugins() == null) {\n+            this.plugins = ImmutableList.of();\n+        }\n+        else {\n+            this.plugins = ImmutableList.copyOf(config.getPlugins());\n+        }\n+        this.resolver = new ArtifactResolver(config.getMavenLocalRepository(), config.getMavenRemoteRepository());\n+        this.passwordAuthenticatorManager = requireNonNull(passwordAuthenticatorManager, \"passwordAuthenticatorManager is null\");\n+        this.pluginInstaller = new RouterPluginInstaller(this);\n+        this.prestoAuthenticatorManager = requireNonNull(prestoAuthenticatorManager, \"prestoAuthenticatorManager is null\");\n+    }\n+\n+    public void loadPlugins()\n+            throws Exception\n+    {\n+        PluginManagerUtil.loadPlugins(\n+                pluginsLoading,\n+                pluginsLoaded,\n+                installedPluginsDir,\n+                plugins,\n+                null,\n+                resolver,\n+                SPI_PACKAGES,\n+                null,\n+                SERVICES_FILE,\n+                pluginInstaller,\n+                getClass().getClassLoader());\n+    }\n+\n+    public void installPlugin(Plugin plugin)\n+    {\n+        for (PasswordAuthenticatorFactory authenticatorFactory : plugin.getPasswordAuthenticatorFactories()) {\n+            log.info(\"Registering password authenticator %s\", authenticatorFactory.getName());\n+            passwordAuthenticatorManager.addPasswordAuthenticatorFactory(authenticatorFactory);\n+        }\n+\n+        for (PrestoAuthenticatorFactory authenticatorFactory : plugin.getPrestoAuthenticatorFactories()) {\n+            log.info(\"Registering presto authenticator %s\", authenticatorFactory.getName());\n+            prestoAuthenticatorManager.addPrestoAuthenticatorFactory(authenticatorFactory);\n+        }\n+    }\n+\n+    private static class RouterPluginInstaller\n+            implements PluginInstaller\n+    {\n+        private final RouterPluginManager pluginManager;\n+\n+        public RouterPluginInstaller(RouterPluginManager pluginManager)\n+        {\n+            this.pluginManager = pluginManager;\n+        }\n+\n+        @Override\n+        public void installPlugin(Plugin plugin)\n+        {\n+            pluginManager.installPlugin(plugin);\n+        }\n+\n+        @Override\n+        public void installCoordinatorPlugin(CoordinatorPlugin plugin)\n+        {\n+            throw new UnsupportedOperationException(\"Cannot install coordinator plugins on router\");\n+        }\n+    }\n+}\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteClusterInfo.java b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteClusterInfo.java\nindex 7ed005cebc606..48d266254233f 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteClusterInfo.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteClusterInfo.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.http.client.HttpClient;\n import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.router.RouterConfig;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n \n@@ -39,9 +40,9 @@ public class RemoteClusterInfo\n     private final AtomicLong activeWorkers = new AtomicLong();\n     private final AtomicLong runningDrivers = new AtomicLong();\n \n-    public RemoteClusterInfo(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig)\n+    public RemoteClusterInfo(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig, RouterConfig routerConfig)\n     {\n-        super(httpClient, remoteUri, remoteStateConfig);\n+        super(httpClient, remoteUri, remoteStateConfig, routerConfig);\n     }\n \n     @Override\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteInfoFactory.java b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteInfoFactory.java\nindex d3cfd6cede058..afe375b39e3b3 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteInfoFactory.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteInfoFactory.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.router.cluster;\n \n import com.facebook.airlift.http.client.HttpClient;\n+import com.facebook.presto.router.RouterConfig;\n \n import javax.inject.Inject;\n \n@@ -29,26 +30,29 @@ public class RemoteInfoFactory\n \n     private final HttpClient clusterInfoHttpClient;\n     private final HttpClient queryInfoHttpClient;\n+    private final RouterConfig routerConfig;\n     private final RemoteStateConfig remoteStateConfig;\n \n     @Inject\n     public RemoteInfoFactory(\n             @ForClusterInfoTracker HttpClient clusterInfoHttpClient,\n             @ForQueryInfoTracker HttpClient queryInfoHttpClient,\n+            RouterConfig routerConfig,\n             RemoteStateConfig remoteStateConfig)\n     {\n         this.clusterInfoHttpClient = requireNonNull(clusterInfoHttpClient, \"Http client for cluster info is null\");\n         this.queryInfoHttpClient = requireNonNull(queryInfoHttpClient, \"Http client for cluster info is null\");\n+        this.routerConfig = requireNonNull(routerConfig, \"routerConfig is null\");\n         this.remoteStateConfig = requireNonNull(remoteStateConfig, \"remoteStateConfig is null\");\n     }\n \n     public RemoteQueryInfo createRemoteQueryInfo(URI uri)\n     {\n-        return new RemoteQueryInfo(clusterInfoHttpClient, uriBuilderFrom(uri).appendPath(QUERY_INFO).build(), remoteStateConfig);\n+        return new RemoteQueryInfo(clusterInfoHttpClient, uriBuilderFrom(uri).appendPath(QUERY_INFO).build(), remoteStateConfig, routerConfig);\n     }\n \n     public RemoteClusterInfo createRemoteClusterInfo(URI uri)\n     {\n-        return new RemoteClusterInfo(queryInfoHttpClient, uriBuilderFrom(uri).appendPath(CLUSTER_INFO).build(), remoteStateConfig);\n+        return new RemoteClusterInfo(queryInfoHttpClient, uriBuilderFrom(uri).appendPath(CLUSTER_INFO).build(), remoteStateConfig, routerConfig);\n     }\n }\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteQueryInfo.java b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteQueryInfo.java\nindex 7f6737ad7f617..813ff3805e61b 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteQueryInfo.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteQueryInfo.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.airlift.http.client.HttpClient;\n import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.router.RouterConfig;\n import com.fasterxml.jackson.core.type.TypeReference;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -37,9 +38,9 @@ public class RemoteQueryInfo\n \n     private final AtomicReference<Optional<List<JsonNode>>> queryList = new AtomicReference<>(Optional.empty());\n \n-    public RemoteQueryInfo(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig)\n+    public RemoteQueryInfo(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig, RouterConfig routerConfig)\n     {\n-        super(httpClient, remoteUri, remoteStateConfig);\n+        super(httpClient, remoteUri, remoteStateConfig, routerConfig);\n     }\n \n     public Optional<List<JsonNode>> getQueryList()\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/security/RouterSecurityModule.java b/presto-router/src/main/java/com/facebook/presto/router/security/RouterSecurityModule.java\nnew file mode 100644\nindex 0000000000000..dc1912b12bf7c\n--- /dev/null\n+++ b/presto-router/src/main/java/com/facebook/presto/router/security/RouterSecurityModule.java\n@@ -0,0 +1,30 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.router.security;\n+\n+import com.facebook.airlift.configuration.AbstractConfigurationAwareModule;\n+import com.facebook.presto.server.InternalCommunicationModule;\n+import com.facebook.presto.server.security.ServerSecurityModule;\n+import com.google.inject.Binder;\n+\n+public class RouterSecurityModule\n+        extends AbstractConfigurationAwareModule\n+{\n+    @Override\n+    protected void setup(Binder binder)\n+    {\n+        install(new InternalCommunicationModule());\n+        install(new ServerSecurityModule());\n+    }\n+}\n\ndiff --git a/presto-router/src/main/java/com/facebook/presto/router/spec/RouterSpec.java b/presto-router/src/main/java/com/facebook/presto/router/spec/RouterSpec.java\nindex 118602c3195e3..25c0fcabc9c65 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/spec/RouterSpec.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/spec/RouterSpec.java\n@@ -31,10 +31,11 @@\n \n public class RouterSpec\n {\n-    private List<GroupSpec> groups;\n-    private List<SelectorRuleSpec> selectors;\n-    private Optional<SchedulerType> schedulerType;\n-    private Optional<URI> predictorUri;\n+    private final List<GroupSpec> groups;\n+    private final List<SelectorRuleSpec> selectors;\n+    private final Optional<SchedulerType> schedulerType;\n+    private final Optional<URI> predictorUri;\n+    private final Optional<String> userCredentials;\n \n     private static final Logger log = Logger.get(RouterSpec.class);\n \n@@ -43,12 +44,14 @@ public RouterSpec(\n             @JsonProperty(\"groups\") List<GroupSpec> groups,\n             @JsonProperty(\"selectors\") List<SelectorRuleSpec> selectors,\n             @JsonProperty(\"scheduler\") Optional<SchedulerType> schedulerType,\n-            @JsonProperty(\"predictor\") Optional<URI> predictorUri)\n+            @JsonProperty(\"predictor\") Optional<URI> predictorUri,\n+            @JsonProperty(\"user-credentials\") Optional<String> userCredentials)\n     {\n         this.groups = ImmutableList.copyOf(requireNonNull(groups, \"groups is null\"));\n         this.selectors = ImmutableList.copyOf(requireNonNull(selectors, \"selectors is null\"));\n         this.schedulerType = requireNonNull(schedulerType, \"scheduleType is null\");\n         this.predictorUri = requireNonNull(predictorUri, \"predictorUri is null\");\n+        this.userCredentials = requireNonNull(userCredentials, \"userCredentials are null\");\n \n         // make sure no duplicate names in group definition\n         checkArgument(groups.stream()\n@@ -85,4 +88,10 @@ public Optional<URI> getPredictorUri()\n         }\n         return Optional.empty();\n     }\n+\n+    @JsonProperty\n+    public Optional<String> getUserCredentials()\n+    {\n+        return userCredentials;\n+    }\n }\n\ndiff --git a/presto-router/src/main/resources/router_ui/src/components/PageTitle.jsx b/presto-router/src/main/resources/router_ui/src/components/PageTitle.jsx\nindex ee7892151801d..4b17e0f3ae1d9 100755\n--- a/presto-router/src/main/resources/router_ui/src/components/PageTitle.jsx\n+++ b/presto-router/src/main/resources/router_ui/src/components/PageTitle.jsx\n@@ -129,6 +129,11 @@ export class PageTitle extends React.Component<Props, State> {\n                                         <span className=\"text\" id=\"environment\">{info.environment}</span>\n                                     </span>\n                                 </li>\n+                                <li>\n+                                    <span className=\"navbar-cluster-info logout\">\n+                                        <a className=\"btn btn-md btn-info style-check logout-btn\" href=\"/logout\">Logout</a>\n+                                    </span>\n+                                </li>\n                             </ul>\n                         </div>\n                     </div>\n",
    "test_patch": "diff --git a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java\nindex 4f2cd5ea4babf..9913d443b0297 100755\n--- a/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java\n+++ b/presto-router/src/main/java/com/facebook/presto/router/cluster/RemoteState.java\n@@ -18,16 +18,20 @@\n import com.facebook.airlift.http.client.Request;\n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.router.RouterConfig;\n+import com.facebook.presto.router.spec.RouterSpec;\n+import com.facebook.presto.spi.PrestoException;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.google.common.util.concurrent.FutureCallback;\n import com.google.common.util.concurrent.Futures;\n+import com.google.inject.Inject;\n import io.airlift.units.Duration;\n \n import javax.annotation.Nullable;\n import javax.annotation.concurrent.ThreadSafe;\n-import javax.inject.Inject;\n \n import java.net.URI;\n+import java.util.Optional;\n import java.util.concurrent.Future;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicLong;\n@@ -37,6 +41,8 @@\n import static com.facebook.airlift.http.client.HttpStatus.OK;\n import static com.facebook.airlift.http.client.Request.Builder.prepareGet;\n import static com.facebook.airlift.json.JsonCodec.jsonCodec;\n+import static com.facebook.presto.router.RouterUtil.parseRouterConfig;\n+import static com.facebook.presto.spi.StandardErrorCode.CONFIGURATION_INVALID;\n import static com.google.common.util.concurrent.MoreExecutors.directExecutor;\n import static io.airlift.units.Duration.nanosSince;\n import static java.util.Objects.requireNonNull;\n@@ -50,6 +56,7 @@ public abstract class RemoteState\n \n     private final HttpClient httpClient;\n     private final URI remoteUri;\n+    private final Optional<String> routerUserCredentials;\n     private final AtomicReference<Future<?>> future = new AtomicReference<>();\n     private final AtomicLong lastUpdateNanos = new AtomicLong();\n     private final AtomicLong lastWarningLogged = new AtomicLong();\n@@ -59,11 +66,14 @@ public abstract class RemoteState\n     private volatile long lastHealthyResponseTimeNanos = System.nanoTime();\n \n     @Inject\n-    public RemoteState(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig)\n+    public RemoteState(HttpClient httpClient, URI remoteUri, RemoteStateConfig remoteStateConfig, RouterConfig routerConfig)\n     {\n         this.isHealthy = new AtomicBoolean(true);\n         this.httpClient = requireNonNull(httpClient, \"httpClient is null\");\n         this.remoteUri = requireNonNull(remoteUri, \"remoteUri is null\");\n+        RouterSpec routerSpec = parseRouterConfig(routerConfig)\n+                .orElseThrow(() -> new PrestoException(CONFIGURATION_INVALID, \"Failed to load router config\"));\n+        this.routerUserCredentials = routerSpec.getUserCredentials();\n         this.clusterUnhealthyTimeout = remoteStateConfig.getClusterUnhealthyTimeout();\n     }\n \n@@ -88,9 +98,9 @@ public synchronized void asyncRefresh()\n         }\n \n         if (sinceUpdate.toMillis() > 1_000 && future.get() == null) {\n-            Request request = prepareGet()\n-                    .setUri(remoteUri)\n-                    .build();\n+            Request.Builder requestBuilder = prepareGet().setUri(remoteUri);\n+            routerUserCredentials.ifPresent(credentials -> requestBuilder.addHeader(\"Authorization\", \"Basic \" + credentials));\n+            Request request = requestBuilder.build();\n \n             HttpClient.HttpResponseFuture<FullJsonResponseHandler.JsonResponse<JsonNode>> responseFuture = httpClient.executeAsync(request, createFullJsonResponseHandler(JSON_CODEC));\n             future.compareAndSet(null, responseFuture);\n\ndiff --git a/presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java b/presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java\nindex 4588c6cfdb6d1..50ac1f7977ced 100755\n--- a/presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java\n+++ b/presto-router/src/test/java/com/facebook/presto/router/TestClusterManager.java\n@@ -27,10 +27,9 @@\n import com.facebook.presto.router.cluster.ClusterManager;\n import com.facebook.presto.router.cluster.ClusterManager.ClusterStatusTracker;\n import com.facebook.presto.router.cluster.RemoteInfoFactory;\n-import com.facebook.presto.router.cluster.RemoteStateConfig;\n+import com.facebook.presto.router.security.RouterSecurityModule;\n import com.facebook.presto.router.spec.RouterSpec;\n import com.facebook.presto.server.testing.TestingPrestoServer;\n-import com.facebook.presto.tpch.TpchPlugin;\n import com.google.common.collect.ImmutableList;\n import com.google.inject.Injector;\n import org.testng.annotations.AfterClass;\n@@ -39,11 +38,9 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.net.URI;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.sql.Connection;\n-import java.sql.DriverManager;\n import java.sql.ResultSet;\n import java.sql.SQLException;\n import java.sql.Statement;\n@@ -53,9 +50,10 @@\n import java.util.concurrent.CyclicBarrier;\n import java.util.concurrent.TimeoutException;\n \n+import static com.facebook.presto.router.TestingRouterUtil.createConnection;\n+import static com.facebook.presto.router.TestingRouterUtil.createPrestoServer;\n import static com.facebook.presto.router.TestingRouterUtil.getConfigFile;\n import static com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly;\n-import static java.lang.String.format;\n import static java.util.concurrent.TimeUnit.SECONDS;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertTrue;\n@@ -72,7 +70,6 @@ public class TestClusterManager\n     private ClusterStatusTracker clusterStatusTracker;\n     private File configFile;\n     private RemoteInfoFactory remoteInfoFactory;\n-    private RemoteStateConfig remoteStateConfig;\n \n     @BeforeClass\n     public void setup()\n@@ -94,19 +91,18 @@ public void setup()\n                 new TestingHttpServerModule(),\n                 new JsonModule(),\n                 new JaxrsModule(true),\n+                new RouterSecurityModule(),\n                 new RouterModule());\n \n         Injector injector = app.doNotInitializeLogging()\n                 .setRequiredConfigurationProperty(\"router.config-file\", configFile.getAbsolutePath())\n+                .setRequiredConfigurationProperty(\"presto.version\", \"test\")\n                 .quiet().initialize();\n \n         lifeCycleManager = injector.getInstance(LifeCycleManager.class);\n         httpServerInfo = injector.getInstance(HttpServerInfo.class);\n         clusterStatusTracker = injector.getInstance(ClusterStatusTracker.class);\n-\n-        // Store dependencies for later use\n         remoteInfoFactory = injector.getInstance(RemoteInfoFactory.class);\n-        remoteStateConfig = injector.getInstance(RemoteStateConfig.class);\n     }\n \n     @AfterClass(alwaysRun = true)\n@@ -165,7 +161,12 @@ public void testConfigReload()\n \n         JsonCodec<RouterSpec> jsonCodec = JsonCodec.jsonCodec(RouterSpec.class);\n         RouterSpec spec = jsonCodec.fromJson(originalConfigContent);\n-        RouterSpec newSpec = new RouterSpec(ImmutableList.of(), spec.getSelectors(), Optional.ofNullable(spec.getSchedulerType()), spec.getPredictorUri());\n+        RouterSpec newSpec = new RouterSpec(\n+                ImmutableList.of(),\n+                spec.getSelectors(),\n+                Optional.ofNullable(spec.getSchedulerType()),\n+                spec.getPredictorUri(),\n+                Optional.empty());\n \n         Files.write(newConfig.toPath(), jsonCodec.toBytes(newSpec));\n         barrier.await(10, SECONDS);\n@@ -201,22 +202,4 @@ private void assertQueryState()\n         }\n         assertEquals(total, NUM_QUERIES);\n     }\n-\n-    private static TestingPrestoServer createPrestoServer()\n-            throws Exception\n-    {\n-        TestingPrestoServer server = new TestingPrestoServer();\n-        server.installPlugin(new TpchPlugin());\n-        server.createCatalog(\"tpch\", \"tpch\");\n-        server.refreshNodes();\n-\n-        return server;\n-    }\n-\n-    private static Connection createConnection(URI uri)\n-            throws SQLException\n-    {\n-        String url = format(\"jdbc:presto://%s:%s\", uri.getHost(), uri.getPort());\n-        return DriverManager.getConnection(url, \"test\", null);\n-    }\n }\n\ndiff --git a/presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java b/presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java\nindex 4593cc0529eba..1239b64d7dbfa 100644\n--- a/presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java\n+++ b/presto-router/src/test/java/com/facebook/presto/router/TestHealthChecks.java\n@@ -16,16 +16,12 @@\n import com.facebook.airlift.bootstrap.Bootstrap;\n import com.facebook.airlift.http.server.testing.TestingHttpServerModule;\n import com.facebook.airlift.jaxrs.JaxrsModule;\n-import com.facebook.airlift.json.JsonCodec;\n import com.facebook.airlift.json.JsonModule;\n import com.facebook.airlift.log.Logging;\n import com.facebook.airlift.node.testing.TestingNodeModule;\n import com.facebook.presto.ClientRequestFilterModule;\n import com.facebook.presto.router.cluster.ClusterManager;\n import com.facebook.presto.router.cluster.RequestInfo;\n-import com.facebook.presto.router.spec.GroupSpec;\n-import com.facebook.presto.router.spec.RouterSpec;\n-import com.facebook.presto.router.spec.SelectorRuleSpec;\n import com.facebook.presto.server.MockHttpServletRequest;\n import com.facebook.presto.server.security.ServerSecurityModule;\n import com.facebook.presto.server.testing.TestingPrestoServer;\n@@ -42,13 +38,11 @@\n \n import java.io.File;\n import java.net.URI;\n-import java.nio.file.Files;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Optional;\n \n-import static com.facebook.presto.router.scheduler.SchedulerType.ROUND_ROBIN;\n-import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.facebook.presto.router.TestingRouterUtil.getConfigFile;\n import static org.testng.Assert.assertFalse;\n import static org.testng.Assert.assertTrue;\n \n@@ -62,7 +56,7 @@ public class TestHealthChecks\n     public void setup()\n             throws Exception\n     {\n-        File configFile = File.createTempFile(\"router\", \".json\");\n+        File tempFile = File.createTempFile(\"router\", \".json\");\n \n         Logging.initialize();\n \n@@ -77,16 +71,7 @@ public void setup()\n         }\n \n         prestoServers = builder.build();\n-        List<URI> serverURIs = prestoServers.stream()\n-                .map(TestingPrestoServer::getBaseUrl)\n-                .collect(toImmutableList());\n-\n-        RouterSpec spec = new RouterSpec(ImmutableList.of(new GroupSpec(\"group1\", serverURIs, Optional.empty(), Optional.empty())),\n-                  ImmutableList.of(new SelectorRuleSpec(Optional.empty(), Optional.empty(), Optional.empty(), \"group1\")),\n-                  Optional.of(ROUND_ROBIN),\n-                  Optional.empty());\n-        JsonCodec<RouterSpec> jsonCodec = JsonCodec.jsonCodec(RouterSpec.class);\n-        Files.write(configFile.toPath(), jsonCodec.toBytes(spec));\n+        getConfigFile(prestoServers, tempFile);\n \n         Bootstrap app = new Bootstrap(\n                 new TestingNodeModule(\"test\"),\n@@ -97,7 +82,8 @@ public void setup()\n                 new RouterModule());\n \n         Injector injector = app.doNotInitializeLogging()\n-                .setRequiredConfigurationProperty(\"router.config-file\", configFile.getAbsolutePath())\n+                .setRequiredConfigurationProperty(\"router.config-file\", tempFile.getAbsolutePath())\n+                .setOptionalConfigurationProperty(\"presto.version\", \"test\")\n                 .setOptionalConfigurationProperty(\"router.remote-state.cluster-unhealthy-timeout\", \"4s\")\n                 .setOptionalConfigurationProperty(\"router.remote-state.polling-interval\", \"0.5s\")\n                 .initialize();\n\ndiff --git a/presto-router/src/test/java/com/facebook/presto/router/TestRouterAuthentication.java b/presto-router/src/test/java/com/facebook/presto/router/TestRouterAuthentication.java\nnew file mode 100644\nindex 0000000000000..3a8023261de06\n--- /dev/null\n+++ b/presto-router/src/test/java/com/facebook/presto/router/TestRouterAuthentication.java\n@@ -0,0 +1,241 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.router;\n+\n+import com.facebook.airlift.bootstrap.Bootstrap;\n+import com.facebook.airlift.bootstrap.LifeCycleManager;\n+import com.facebook.airlift.http.client.HttpClient;\n+import com.facebook.airlift.http.client.HttpClientConfig;\n+import com.facebook.airlift.http.client.Request;\n+import com.facebook.airlift.http.client.Response;\n+import com.facebook.airlift.http.client.ResponseHandler;\n+import com.facebook.airlift.http.client.jetty.JettyHttpClient;\n+import com.facebook.airlift.http.server.HttpServerInfo;\n+import com.facebook.airlift.http.server.testing.TestingHttpServerModule;\n+import com.facebook.airlift.jaxrs.JaxrsModule;\n+import com.facebook.airlift.json.JsonModule;\n+import com.facebook.airlift.log.Logger;\n+import com.facebook.airlift.log.Logging;\n+import com.facebook.airlift.node.testing.TestingNodeModule;\n+import com.facebook.presto.password.file.FileAuthenticatorFactory;\n+import com.facebook.presto.router.security.RouterSecurityModule;\n+import com.facebook.presto.server.security.PasswordAuthenticatorManager;\n+import com.facebook.presto.spi.security.PasswordAuthenticatorFactory;\n+import com.google.common.base.Splitter;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.io.ByteStreams;\n+import com.google.inject.Injector;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import javax.ws.rs.core.UriBuilder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Base64;\n+\n+import static com.facebook.presto.router.TestingRouterUtil.createPrestoServer;\n+import static com.facebook.presto.router.TestingRouterUtil.getConfigFile;\n+import static java.lang.String.format;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static org.testng.Assert.assertEquals;\n+\n+@Test(singleThreaded = true)\n+public class TestRouterAuthentication\n+{\n+    private static final Logger log = Logger.get(TestRouterAuthentication.class);\n+    private static final String jksPassword = \"testPass\";\n+\n+    private File configFile;\n+    private LifeCycleManager lifeCycleManager;\n+    private Path authenticatorPropertiesFile;\n+    private HttpServerInfo httpServerInfo;\n+    private Path storePath;\n+    private Path keystorePath;\n+    private Path truststorePath;\n+    private HttpClient httpClient;\n+\n+    private void runKeytoolCommand(String command, Object... args)\n+            throws IOException, InterruptedException\n+    {\n+        String cmd = format(command, args);\n+        Process process = new ProcessBuilder()\n+                .directory(storePath.toFile())\n+                .command(Splitter.on(\" \").splitToStream(cmd).toArray(String[]::new))\n+                .start();\n+        process.waitFor();\n+        if (log.isDebugEnabled()) {\n+            log.debug(\"running command: \" + cmd + \". Output:\\n\" + new String(ByteStreams.toByteArray(process.getInputStream())));\n+        }\n+    }\n+\n+    private void setupCertStores()\n+            throws Exception\n+    {\n+        runKeytoolCommand(\"keytool -genkeypair -alias localhost -keyalg RSA -keysize 2048 -validity 365 -keystore keystore.jks -storepass %s -keypass %s -dname CN=localhost -ext SAN=ip:127.0.0.1\", jksPassword, jksPassword);\n+        runKeytoolCommand(\"keytool -exportcert -alias localhost -keystore keystore.jks -file certificate.cer -storepass %s\", jksPassword);\n+        runKeytoolCommand(\"keytool -importcert -alias localhost -file certificate.cer -keystore truststore.jks -storepass %s -noprompt\", jksPassword);\n+        keystorePath = storePath.resolve(\"keystore.jks\");\n+        truststorePath = storePath.resolve(\"truststore.jks\");\n+    }\n+\n+    @BeforeClass\n+    public void setup()\n+            throws Exception\n+    {\n+        Logging.initialize();\n+        Path tempFile = Files.createTempFile(\"temp-config\", \".json\");\n+        configFile = getConfigFile(ImmutableList.of(createPrestoServer()), tempFile.toFile());\n+\n+        authenticatorPropertiesFile = Paths.get(\"etc/password-authenticator.properties\");\n+        authenticatorPropertiesFile.getParent().toFile().mkdirs();\n+\n+        Path passwordFilePath = Files.createTempFile(\"passwords\", \".db\");\n+        // credentials come from htpasswd -n -B -C 8, input \"testpass\"\n+        Files.write(passwordFilePath, \"testuser:$2y$08$KBfSimK6KTZFyCKlJACpTu7VMBHlnFixXm8tDh9I0rDf3IIuobtHy\".getBytes(UTF_8));\n+        Files.write(\n+                authenticatorPropertiesFile,\n+                format(\"password-authenticator.name=file\\nfile.password-file=%s\", passwordFilePath).getBytes(UTF_8),\n+                StandardOpenOption.CREATE);\n+        storePath = Files.createTempDirectory(\"jks-store\");\n+        setupCertStores();\n+\n+        Bootstrap app = new Bootstrap(\n+                new TestingNodeModule(\"test\"),\n+                new TestingHttpServerModule(),\n+                new JsonModule(),\n+                new JaxrsModule(true),\n+                new RouterSecurityModule(),\n+                new RouterModule());\n+\n+        Injector injector = app.doNotInitializeLogging()\n+                .setRequiredConfigurationProperty(\"router.config-file\", configFile.getAbsolutePath())\n+                .setRequiredConfigurationProperty(\"presto.version\", \"test\")\n+                .setOptionalConfigurationProperty(\"http-server.authentication.type\", \"PASSWORD\")\n+                .setOptionalConfigurationProperty(\"http-server.http.enabled\", \"false\")\n+                .setOptionalConfigurationProperty(\"http-server.https.enabled\", \"true\")\n+                .setOptionalConfigurationProperty(\"http-server.https.keystore.path\", keystorePath.toAbsolutePath().toString())\n+                .setOptionalConfigurationProperty(\"http-server.https.keystore.key\", jksPassword)\n+                .initialize();\n+        injector.getInstance(RouterPluginManager.class).loadPlugins();\n+\n+        PasswordAuthenticatorManager passwordAuthenticatorManager = injector.getInstance(PasswordAuthenticatorManager.class);\n+        PasswordAuthenticatorFactory authFactory = new FileAuthenticatorFactory();\n+        passwordAuthenticatorManager.addPasswordAuthenticatorFactory(authFactory);\n+        passwordAuthenticatorManager.setRequired();\n+        passwordAuthenticatorManager.loadPasswordAuthenticator();\n+\n+        lifeCycleManager = injector.getInstance(LifeCycleManager.class);\n+        httpServerInfo = injector.getInstance(HttpServerInfo.class);\n+        httpClient = new JettyHttpClient(\n+                new HttpClientConfig()\n+                        .setTrustStorePath(truststorePath.toAbsolutePath().toString())\n+                        .setTrustStorePassword(jksPassword));\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDownServer()\n+    {\n+        if (authenticatorPropertiesFile != null) {\n+            try {\n+                authenticatorPropertiesFile.toFile().delete();\n+            }\n+            catch (Exception e) {\n+                // ignore\n+            }\n+        }\n+        if (lifeCycleManager != null) {\n+            lifeCycleManager.stop();\n+        }\n+    }\n+\n+    @DataProvider(name = \"routerEndpoints\")\n+    public Object[][] routerEndpoints()\n+    {\n+        return new Object[][] {\n+                // method, endpoint, expected response with auth\n+                {\"GET\", \"/\", 200},\n+                {\"GET\", \"/v1/info\", 200},\n+                {\"GET\", \"/v1/cluster\", 200},\n+                // 400 because the test doesn't actually POST a request body\n+                {\"POST\", \"/v1/statement\", 400},\n+                // TODO: The UI is intentionally left out because the airlift incorrectly configures\n+                //  some static resources\n+                // {\"GET\", \"/ui/\"}\n+        };\n+    }\n+\n+    @Test(dataProvider = \"routerEndpoints\")\n+    public void testEndpointWithoutAuthentication(String method, String endpoint, int unused)\n+            throws Exception\n+    {\n+        Request request = Request.builder()\n+                .setUri(UriBuilder.fromUri(httpServerInfo.getHttpsUri()).path(endpoint).build())\n+                .setMethod(method)\n+                .build();\n+        httpClient.execute(request, new ResponseHandler<Void, Exception>()\n+        {\n+            @Override\n+            public Void handleException(Request request, Exception exception)\n+                    throws Exception\n+            {\n+                throw exception;\n+            }\n+\n+            @Override\n+            public Void handle(Request request, Response response)\n+                    throws Exception\n+            {\n+                assertEquals(response.getStatusCode(), 401, format(\"request to %s should not have been successful. Expected 401, got: %d%n%s\",\n+                        request.getUri(), response.getStatusCode(), new String(ByteStreams.toByteArray(response.getInputStream()))));\n+                return null;\n+            }\n+        });\n+    }\n+\n+    @Test(dataProvider = \"routerEndpoints\")\n+    public void testEndpointWithAuthentication(String method, String endpoint, int expectedCode)\n+            throws Exception\n+    {\n+        Request request = Request.builder()\n+                .setUri(UriBuilder.fromUri(httpServerInfo.getHttpsUri()).path(endpoint).build())\n+                .setMethod(method)\n+                .setHeader(\"Authorization\", \"Basic \" + Base64.getEncoder().encodeToString(\"testuser:testpass\".getBytes(UTF_8)))\n+                .build();\n+        httpClient.execute(request, new ResponseHandler<Void, Exception>()\n+        {\n+            @Override\n+            public Void handleException(Request request, Exception exception)\n+                    throws Exception\n+            {\n+                throw exception;\n+            }\n+\n+            @Override\n+            public Void handle(Request request, Response response)\n+                    throws Exception\n+            {\n+                assertEquals(response.getStatusCode(), expectedCode, format(\"request to %s should have been successful. Expected %d, got: %d%n%s\",\n+                        request.getUri(), expectedCode, response.getStatusCode(), new String(ByteStreams.toByteArray(response.getInputStream()))));\n+                return null;\n+            }\n+        });\n+    }\n+}\n\ndiff --git a/presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java b/presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java\nindex 3a60ffe2ef7af..a2b0723656266 100644\n--- a/presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java\n+++ b/presto-router/src/test/java/com/facebook/presto/router/TestSelectors.java\n@@ -110,7 +110,7 @@ public void setup()\n             selectors.add(selectorRuleSpec);\n         }\n \n-        RouterSpec routerSpec = new RouterSpec(groups, selectors, schedulerType, predictorUri);\n+        RouterSpec routerSpec = new RouterSpec(groups, selectors, schedulerType, predictorUri, Optional.empty());\n         JsonCodec<RouterSpec> codec = jsonCodec(RouterSpec.class);\n         String configTemplate = codec.toJson(routerSpec);\n \n\ndiff --git a/presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java b/presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java\nindex a3f273c7f8638..ef4602fbd7b2e 100644\n--- a/presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java\n+++ b/presto-router/src/test/java/com/facebook/presto/router/TestingRouterUtil.java\n@@ -18,18 +18,23 @@\n import com.facebook.presto.router.spec.RouterSpec;\n import com.facebook.presto.router.spec.SelectorRuleSpec;\n import com.facebook.presto.server.testing.TestingPrestoServer;\n+import com.facebook.presto.tpch.TpchPlugin;\n import com.google.common.collect.ImmutableList;\n \n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n import java.nio.file.Files;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n import java.util.List;\n import java.util.Optional;\n import java.util.stream.Collectors;\n \n import static com.facebook.airlift.json.JsonCodec.jsonCodec;\n import static com.facebook.presto.router.scheduler.SchedulerType.ROUND_ROBIN;\n+import static java.lang.String.format;\n+import static java.sql.DriverManager.getConnection;\n \n public class TestingRouterUtil\n {\n@@ -46,9 +51,28 @@ public static File getConfigFile(List<TestingPrestoServer> servers, File tempFil\n         RouterSpec spec = new RouterSpec(ImmutableList.of(new GroupSpec(\"all\", serverURIs, Optional.empty(), Optional.empty())),\n                     ImmutableList.of(new SelectorRuleSpec(Optional.empty(), Optional.empty(), Optional.empty(), \"all\")),\n                     Optional.of(ROUND_ROBIN),\n+                    Optional.empty(),\n                     Optional.empty());\n         JsonCodec<RouterSpec> codec = jsonCodec(RouterSpec.class);\n         Files.write(tempFile.toPath(), codec.toBytes(spec));\n         return tempFile;\n     }\n+\n+    public static TestingPrestoServer createPrestoServer()\n+            throws Exception\n+    {\n+        TestingPrestoServer server = new TestingPrestoServer();\n+        server.installPlugin(new TpchPlugin());\n+        server.createCatalog(\"tpch\", \"tpch\");\n+        server.refreshNodes();\n+\n+        return server;\n+    }\n+\n+    public static Connection createConnection(URI uri)\n+            throws SQLException\n+    {\n+        String url = format(\"jdbc:presto://%s:%s\", uri.getHost(), uri.getPort());\n+        return getConnection(url, \"test\", null);\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24406",
    "pr_id": 24406,
    "issue_id": 24362,
    "repo": "prestodb/presto",
    "problem_statement": "Add a config flag to use varchar type in Presto - Java TPC-DS Connector\nA new TPC-DS config `tpcds.use-varchar-type` will be added to toggle the char columns to varchar, addressing the lack of support for the char data type in Presto - C++. This config allows the toggling of the char to varchar when required, ensuring consistency between Presto - Java and Presto - C++. At the schema level, all the columns which were of char type would be cast to a varchar type.\n\n## Expected Behavior or Use Case\n<!--- Tell us how it should work -->\n\n## Presto Component, Service, or Connector\nPresto - TPCDS Connector\n\n## Possible Implementation\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\n\n## Example Screenshots (if appropriate):\n\n## Context\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 159,
    "test_files_count": 15,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/tpcds.rst",
      "presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java",
      "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsConnectorFactory.java",
      "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsMetadata.java",
      "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSet.java",
      "presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSetProvider.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/AbstractTestTpcds.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsChar.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsVarchar.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/AbstractTestTpcds.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsChar.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsVarchar.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java",
      "presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java"
    ],
    "base_commit": "ea7dfa86e5e6e0c9b209d82fcaea4f1ef1a6c43a",
    "head_commit": "1fe0926c7a2c5acaf98562c6a31fb3cac5fefcb0",
    "repo_url": "https://github.com/prestodb/presto/pull/24406",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24406",
    "dockerfile": "",
    "pr_merged_at": "2025-02-19T23:16:16.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/tpcds.rst b/presto-docs/src/main/sphinx/connector/tpcds.rst\nindex bac83f73226e4..be030b25f1a36 100644\n--- a/presto-docs/src/main/sphinx/connector/tpcds.rst\n+++ b/presto-docs/src/main/sphinx/connector/tpcds.rst\n@@ -66,4 +66,6 @@ Property Name                                      Description\n ================================================== ========================================================================== ==============================\n ``tpcds.splits-per-node``                          Number of data splits generated per Presto worker node when querying       Number of available processors\n                                                    data from the TPCDS connector.\n+\n+``tpcds.use-varchar-type``                         Toggle all char columns to varchar in the TPC-DS connector.                false\n ================================================== ========================================================================== ==============================\n\ndiff --git a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsConnectorFactory.java b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsConnectorFactory.java\nindex 814e56e3136d9..092ded05a523c 100644\n--- a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsConnectorFactory.java\n+++ b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsConnectorFactory.java\n@@ -64,6 +64,13 @@ public ConnectorHandleResolver getHandleResolver()\n     public Connector create(String catalogName, Map<String, String> config, ConnectorContext context)\n     {\n         int splitsPerNode = getSplitsPerNode(config);\n+        // The Java TPC-DS connector works with either char or varchar columns.\n+        // However, native execution only supports varchar columns, hence in a native cluster `tpcds.use-varchar-type` must be true.\n+        boolean useVarcharType = useVarcharType(config);\n+        if (context.getConnectorSystemConfig().isNativeExecution() && !(useVarcharType)) {\n+            throw new IllegalArgumentException(\"`tpcds.use-varchar-type` config property is not true for a native cluster\");\n+        }\n+\n         NodeManager nodeManager = context.getNodeManager();\n         return new Connector()\n         {\n@@ -76,7 +83,7 @@ public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel\n             @Override\n             public ConnectorMetadata getMetadata(ConnectorTransactionHandle transactionHandle)\n             {\n-                return new TpcdsMetadata();\n+                return new TpcdsMetadata(useVarcharType);\n             }\n \n             @Override\n@@ -88,7 +95,7 @@ public ConnectorSplitManager getSplitManager()\n             @Override\n             public ConnectorRecordSetProvider getRecordSetProvider()\n             {\n-                return new TpcdsRecordSetProvider();\n+                return new TpcdsRecordSetProvider(useVarcharType);\n             }\n \n             @Override\n@@ -109,6 +116,16 @@ private int getSplitsPerNode(Map<String, String> properties)\n         }\n     }\n \n+    private boolean useVarcharType(Map<String, String> properties)\n+    {\n+        try {\n+            return parseBoolean(firstNonNull(properties.get(\"tpcds.use-varchar-type\"), String.valueOf(false)));\n+        }\n+        catch (NumberFormatException e) {\n+            throw new IllegalArgumentException(\"Invalid property tpcds.use-varchar-type\");\n+        }\n+    }\n+\n     private boolean isWithNoSexism(Map<String, String> properties)\n     {\n         return parseBoolean(firstNonNull(properties.get(\"tpcds.with-no-sexism\"), String.valueOf(false)));\n\ndiff --git a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsMetadata.java b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsMetadata.java\nindex d571057317e39..745191d13b4b1 100644\n--- a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsMetadata.java\n+++ b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsMetadata.java\n@@ -64,13 +64,16 @@ public class TpcdsMetadata\n     private final Set<String> tableNames;\n     private final TpcdsTableStatisticsFactory tpcdsTableStatisticsFactory = new TpcdsTableStatisticsFactory();\n \n-    public TpcdsMetadata()\n+    private final boolean useVarcharType;\n+\n+    public TpcdsMetadata(boolean useVarcharType)\n     {\n         ImmutableSet.Builder<String> tableNames = ImmutableSet.builder();\n         for (Table tpcdsTable : Table.getBaseTables()) {\n             tableNames.add(tpcdsTable.getName().toLowerCase(ENGLISH));\n         }\n         this.tableNames = tableNames.build();\n+        this.useVarcharType = useVarcharType;\n     }\n \n     @Override\n@@ -134,14 +137,14 @@ public ConnectorTableMetadata getTableMetadata(ConnectorSession session, Connect\n         Table table = Table.getTable(tpcdsTableHandle.getTableName());\n         String schemaName = scaleFactorSchemaName(tpcdsTableHandle.getScaleFactor());\n \n-        return getTableMetadata(schemaName, table);\n+        return getTableMetadata(schemaName, table, useVarcharType);\n     }\n \n-    private static ConnectorTableMetadata getTableMetadata(String schemaName, Table tpcdsTable)\n+    private static ConnectorTableMetadata getTableMetadata(String schemaName, Table tpcdsTable, boolean useVarcharType)\n     {\n         ImmutableList.Builder<ColumnMetadata> columns = ImmutableList.builder();\n         for (Column column : tpcdsTable.getColumns()) {\n-            columns.add(new ColumnMetadata(column.getName(), getPrestoType(column.getType())));\n+            columns.add(new ColumnMetadata(column.getName(), getPrestoType(column.getType(), useVarcharType)));\n         }\n         SchemaTableName tableName = new SchemaTableName(schemaName, tpcdsTable.getName());\n         return new ConnectorTableMetadata(tableName, columns.build());\n@@ -189,7 +192,7 @@ public Map<SchemaTableName, List<ColumnMetadata>> listTableColumns(ConnectorSess\n         for (String schemaName : getSchemaNames(session, Optional.ofNullable(prefix.getSchemaName()))) {\n             for (Table tpcdsTable : Table.getBaseTables()) {\n                 if (prefix.getTableName() == null || tpcdsTable.getName().equals(prefix.getTableName())) {\n-                    ConnectorTableMetadata tableMetadata = getTableMetadata(schemaName, tpcdsTable);\n+                    ConnectorTableMetadata tableMetadata = getTableMetadata(schemaName, tpcdsTable, useVarcharType);\n                     tableColumns.put(new SchemaTableName(schemaName, tpcdsTable.getName()), tableMetadata.getColumns());\n                 }\n             }\n@@ -243,7 +246,7 @@ public static double schemaNameToScaleFactor(String schemaName)\n         }\n     }\n \n-    public static Type getPrestoType(ColumnType tpcdsType)\n+    public static Type getPrestoType(ColumnType tpcdsType, boolean useVarcharType)\n     {\n         switch (tpcdsType.getBase()) {\n             case IDENTIFIER:\n@@ -255,6 +258,9 @@ public static Type getPrestoType(ColumnType tpcdsType)\n             case DECIMAL:\n                 return createDecimalType(tpcdsType.getPrecision().get(), tpcdsType.getScale().get());\n             case CHAR:\n+                if (useVarcharType) {\n+                    return createVarcharType(tpcdsType.getPrecision().get());\n+                }\n                 return createCharType(tpcdsType.getPrecision().get());\n             case VARCHAR:\n                 return createVarcharType(tpcdsType.getPrecision().get());\n\ndiff --git a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSet.java b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSet.java\nindex a3821d2cdc307..a87e5b8c1566e 100644\n--- a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSet.java\n+++ b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSet.java\n@@ -49,7 +49,9 @@ public class TpcdsRecordSet\n \n     private final List<Type> columnTypes;\n \n-    public TpcdsRecordSet(Results results, List<Column> columns)\n+    private final boolean useVarcharType;\n+\n+    public TpcdsRecordSet(Results results, List<Column> columns, boolean useVarcharType)\n     {\n         requireNonNull(results, \"results is null\");\n \n@@ -57,9 +59,10 @@ public TpcdsRecordSet(Results results, List<Column> columns)\n         this.columns = ImmutableList.copyOf(columns);\n         ImmutableList.Builder<Type> columnTypes = ImmutableList.builder();\n         for (Column column : columns) {\n-            columnTypes.add(getPrestoType(column.getType()));\n+            columnTypes.add(getPrestoType(column.getType(), useVarcharType));\n         }\n         this.columnTypes = columnTypes.build();\n+        this.useVarcharType = useVarcharType;\n     }\n \n     @Override\n@@ -103,7 +106,7 @@ public long getReadTimeNanos()\n         @Override\n         public Type getType(int field)\n         {\n-            return getPrestoType(columns.get(field).getType());\n+            return getPrestoType(columns.get(field).getType(), useVarcharType);\n         }\n \n         @Override\n\ndiff --git a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSetProvider.java b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSetProvider.java\nindex 2b85352e88a6f..21f39fa029472 100644\n--- a/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSetProvider.java\n+++ b/presto-tpcds/src/main/java/com/facebook/presto/tpcds/TpcdsRecordSetProvider.java\n@@ -33,6 +33,13 @@\n public class TpcdsRecordSetProvider\n         implements ConnectorRecordSetProvider\n {\n+    private final boolean useVarcharType;\n+\n+    public TpcdsRecordSetProvider(boolean useVarcharType)\n+    {\n+        this.useVarcharType = useVarcharType;\n+    }\n+\n     @Override\n     public RecordSet getRecordSet(ConnectorTransactionHandle transaction, ConnectorSession session, ConnectorSplit split, List<? extends ColumnHandle> columns)\n     {\n@@ -63,6 +70,6 @@ private RecordSet getRecordSet(\n                 .withTable(table)\n                 .withNoSexism(noSexism);\n         Results results = constructResults(table, session);\n-        return new TpcdsRecordSet(results, builder.build());\n+        return new TpcdsRecordSet(results, builder.build(), useVarcharType);\n     }\n }\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java b/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\nindex 8cf90df570ccb..c7b54445f7475 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\n@@ -98,6 +98,12 @@ public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> ta\n         return createQueryRunner(tables, ImmutableMap.of(), Optional.empty());\n     }\n \n+    public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tpchTables, Map<String, String> extraProperties, Map<String, String> tpcdsProperties)\n+            throws Exception\n+    {\n+        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), \"sql-standard\", ImmutableMap.of(), Optional.empty(), Optional.empty(), Optional.empty(), tpcdsProperties);\n+    }\n+\n     public static DistributedQueryRunner createQueryRunner(\n             Iterable<TpchTable<?>> tpchTables,\n             Map<String, String> extraProperties,\n@@ -105,19 +111,19 @@ public static DistributedQueryRunner createQueryRunner(\n             Optional<Path> dataDirectory)\n             throws Exception\n     {\n-        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, extraCoordinatorProperties, \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty());\n+        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, extraCoordinatorProperties, \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty(), ImmutableMap.of());\n     }\n \n     public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tpchTables, Map<String, String> extraProperties, Optional<Path> dataDirectory)\n             throws Exception\n     {\n-        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty());\n+        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty(), ImmutableMap.of());\n     }\n \n     public static DistributedQueryRunner createQueryRunner(Iterable<TpchTable<?>> tpchTables, List<String> tpcdsTableNames, Map<String, String> extraProperties, Optional<Path> dataDirectory)\n             throws Exception\n     {\n-        return createQueryRunner(tpchTables, tpcdsTableNames, extraProperties, ImmutableMap.of(), \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty());\n+        return createQueryRunner(tpchTables, tpcdsTableNames, extraProperties, ImmutableMap.of(), \"sql-standard\", ImmutableMap.of(), Optional.empty(), dataDirectory, Optional.empty(), ImmutableMap.of());\n     }\n \n     public static DistributedQueryRunner createQueryRunner(\n@@ -128,7 +134,19 @@ public static DistributedQueryRunner createQueryRunner(\n             Optional<Path> dataDirectory)\n             throws Exception\n     {\n-        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), security, extraHiveProperties, Optional.empty(), dataDirectory, Optional.empty());\n+        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), security, extraHiveProperties, Optional.empty(), dataDirectory, Optional.empty(), ImmutableMap.of());\n+    }\n+\n+    public static DistributedQueryRunner createQueryRunner(\n+            Iterable<TpchTable<?>> tpchTables,\n+            Map<String, String> extraProperties,\n+            String security,\n+            Map<String, String> extraHiveProperties,\n+            Optional<Path> dataDirectory,\n+            Map<String, String> tpcdsProperties)\n+            throws Exception\n+    {\n+        return createQueryRunner(tpchTables, ImmutableList.of(), extraProperties, ImmutableMap.of(), security, extraHiveProperties, Optional.empty(), dataDirectory, Optional.empty(), tpcdsProperties);\n     }\n \n     public static DistributedQueryRunner createQueryRunner(\n@@ -140,10 +158,11 @@ public static DistributedQueryRunner createQueryRunner(\n             Map<String, String> extraHiveProperties,\n             Optional<Integer> workerCount,\n             Optional<Path> dataDirectory,\n-            Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher)\n+            Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher,\n+            Map<String, String> tpcdsProperties)\n             throws Exception\n     {\n-        return createQueryRunner(tpchTables, tpcdsTableNames, extraProperties, extraCoordinatorProperties, security, extraHiveProperties, workerCount, dataDirectory, externalWorkerLauncher, Optional.empty());\n+        return createQueryRunner(tpchTables, tpcdsTableNames, extraProperties, extraCoordinatorProperties, security, extraHiveProperties, workerCount, dataDirectory, externalWorkerLauncher, Optional.empty(), tpcdsProperties);\n     }\n \n     public static DistributedQueryRunner createQueryRunner(\n@@ -156,7 +175,8 @@ public static DistributedQueryRunner createQueryRunner(\n             Optional<Integer> workerCount,\n             Optional<Path> dataDirectory,\n             Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher,\n-            Optional<ExtendedHiveMetastore> externalMetastore)\n+            Optional<ExtendedHiveMetastore> externalMetastore,\n+            Map<String, String> tpcdsProperties)\n             throws Exception\n     {\n         return createQueryRunner(\n@@ -170,7 +190,8 @@ public static DistributedQueryRunner createQueryRunner(\n                 dataDirectory,\n                 externalWorkerLauncher,\n                 externalMetastore,\n-                false);\n+                false,\n+                tpcdsProperties);\n     }\n \n     public static DistributedQueryRunner createQueryRunner(\n@@ -184,7 +205,8 @@ public static DistributedQueryRunner createQueryRunner(\n             Optional<Path> dataDirectory,\n             Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher,\n             Optional<ExtendedHiveMetastore> externalMetastore,\n-            boolean addJmxPlugin)\n+            boolean addJmxPlugin,\n+            Map<String, String> tpcdsProperties)\n             throws Exception\n     {\n         assertEquals(DateTimeZone.getDefault(), TIME_ZONE, \"Timezone not configured correctly. Add -Duser.timezone=America/Bahia_Banderas to your JVM arguments\");\n@@ -211,7 +233,7 @@ public static DistributedQueryRunner createQueryRunner(\n             queryRunner.installPlugin(new TpcdsPlugin());\n             queryRunner.installPlugin(new TestingHiveEventListenerPlugin());\n             queryRunner.createCatalog(\"tpch\", \"tpch\");\n-            queryRunner.createCatalog(\"tpcds\", \"tpcds\");\n+            queryRunner.createCatalog(\"tpcds\", \"tpcds\", tpcdsProperties);\n             Map<String, String> tpchProperties = ImmutableMap.<String, String>builder()\n                     .put(\"tpch.column-naming\", \"standard\")\n                     .build();\n@@ -326,7 +348,8 @@ public static DistributedQueryRunner createMaterializingQueryRunner(Iterable<Tpc\n                         \"grouped-execution-enabled\", \"true\"),\n                 \"sql-standard\",\n                 ImmutableMap.of(\"hive.create-empty-bucket-files-for-temporary-table\", \"false\"),\n-                Optional.empty());\n+                Optional.empty(),\n+                ImmutableMap.of());\n     }\n \n     public static DistributedQueryRunner createMaterializingAndSpillingQueryRunner(Iterable<TpchTable<?>> tables)\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java\nindex 3c74dca08ce57..1f083e3302d45 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveNativeLogicalPlanner.java\n@@ -25,8 +25,6 @@\n import com.google.common.collect.ImmutableMap;\n import org.testng.annotations.Test;\n \n-import java.util.Optional;\n-\n import static com.facebook.presto.hive.HiveQueryRunner.HIVE_CATALOG;\n import static com.facebook.presto.hive.HiveSessionProperties.PARTIAL_AGGREGATION_PUSHDOWN_ENABLED;\n import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.anyTree;\n@@ -47,7 +45,7 @@ protected QueryRunner createQueryRunner()\n         return HiveQueryRunner.createQueryRunner(\n                 ImmutableList.of(ORDERS),\n                 ImmutableMap.of(\"native-execution-enabled\", \"true\"),\n-                Optional.empty());\n+                ImmutableMap.of(\"tpcds.use-varchar-type\", \"true\"));\n     }\n \n     @Test\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java b/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\nindex e4bd2e092adac..a4bfe2d2bfcf4 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\n@@ -60,6 +60,7 @@ public static DistributedQueryRunner create(\n                                         hiveEndpoint.getPort()),\n                                 new MetastoreClientConfig(),\n                                 HDFS_ENVIRONMENT),\n-                        new HivePartitionMutator())));\n+                        new HivePartitionMutator())),\n+                ImmutableMap.of());\n     }\n }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\nindex b567f45566fb9..0d86fd4fdc6bb 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n@@ -149,7 +149,7 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n             Optional<Path> dataDirectory)\n             throws Exception\n     {\n-        return createIcebergQueryRunner(extraProperties, extraConnectorProperties, format, createTpchTables, addJmxPlugin, nodeCount, externalWorkerLauncher, dataDirectory, false, Optional.empty());\n+        return createIcebergQueryRunner(extraProperties, extraConnectorProperties, format, createTpchTables, addJmxPlugin, nodeCount, externalWorkerLauncher, dataDirectory, false, Optional.empty(), ImmutableMap.of());\n     }\n \n     public static DistributedQueryRunner createIcebergQueryRunner(\n@@ -161,10 +161,11 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n             OptionalInt nodeCount,\n             Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher,\n             Optional<Path> dataDirectory,\n-            boolean addStorageFormatToPath)\n+            boolean addStorageFormatToPath,\n+            Map<String, String> tpcdsProperties)\n             throws Exception\n     {\n-        return createIcebergQueryRunner(extraProperties, extraConnectorProperties, format, createTpchTables, addJmxPlugin, nodeCount, externalWorkerLauncher, dataDirectory, addStorageFormatToPath, Optional.empty());\n+        return createIcebergQueryRunner(extraProperties, extraConnectorProperties, format, createTpchTables, addJmxPlugin, nodeCount, externalWorkerLauncher, dataDirectory, addStorageFormatToPath, Optional.empty(), tpcdsProperties);\n     }\n \n     public static DistributedQueryRunner createIcebergQueryRunner(\n@@ -177,7 +178,8 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n             Optional<BiFunction<Integer, URI, Process>> externalWorkerLauncher,\n             Optional<Path> dataDirectory,\n             boolean addStorageFormatToPath,\n-            Optional<String> schemaName)\n+            Optional<String> schemaName,\n+            Map<String, String> tpcdsProperties)\n             throws Exception\n     {\n         setupLogging();\n@@ -198,7 +200,7 @@ public static DistributedQueryRunner createIcebergQueryRunner(\n         queryRunner.createCatalog(\"tpch\", \"tpch\");\n \n         queryRunner.installPlugin(new TpcdsPlugin());\n-        queryRunner.createCatalog(\"tpcds\", \"tpcds\");\n+        queryRunner.createCatalog(\"tpcds\", \"tpcds\", tpcdsProperties);\n \n         queryRunner.getServers().forEach(server -> {\n             MBeanServer mBeanServer = MBeanServerFactory.newMBeanServer();\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\nindex b5aa83bfe05d7..bf484f87452a6 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n@@ -120,7 +120,8 @@ protected QueryRunner createQueryRunner()\n                 Optional.empty(),\n                 Optional.of(warehouseLocation.toPath()),\n                 false,\n-                Optional.of(\"ns1.ns2\"));\n+                Optional.of(\"ns1.ns2\"),\n+                ImmutableMap.of());\n \n         // additional catalog for testing nested namespace disabled\n         icebergQueryRunner.createCatalog(ICEBERG_NESTED_NAMESPACE_DISABLED_CATALOG, \"iceberg\",\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java\nindex 30a6a41786d3c..debe20c99dcc5 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeTpcdsQueries.java\n@@ -91,24 +91,15 @@ private static void createTpcdsCallCenter(QueryRunner queryRunner, Session sessi\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE call_center AS \" +\n-                            \"SELECT cc_call_center_sk, cast(cc_call_center_id as varchar) as cc_call_center_id, cc_rec_start_date, cc_rec_end_date, \" +\n-                            \"   cc_closed_date_sk, cc_open_date_sk, cc_name, cc_class, cc_employees, cc_sq_ft, cast(cc_hours as varchar) as cc_hours, \" +\n-                            \"   cc_manager, cc_mkt_id, cast(cc_mkt_class as varchar) as cc_mkt_class, cc_mkt_desc, cc_market_manager,  \" +\n-                            \"   cc_division, cc_division_name, cc_company, cast(cc_company_name as varchar) as cc_company_name,\" +\n-                            \"   cast(cc_street_number as varchar ) as cc_street_number, cc_street_name, cast(cc_street_type as varchar) as cc_street_type, \" +\n-                            \"   cast(cc_suite_number as varchar) as cc_suite_number, cc_city, cc_county, cast(cc_state as varchar) as cc_state, \" +\n-                            \"   cast(cc_zip as varchar) as cc_zip, cc_country, cc_gmt_offset, cc_tax_percentage \" +\n-                            \"FROM tpcds.tiny.call_center\");\n+                            \"SELECT * FROM tpcds.tiny.call_center\");\n                     break;\n                 case \"DWRF\":\n                     queryRunner.execute(session, \"CREATE TABLE call_center AS \" +\n-                            \"SELECT cc_call_center_sk, cast(cc_call_center_id as varchar) as cc_call_center_id, cast(cc_rec_start_date as varchar) as cc_rec_start_date, \" +\n-                            \"   cast(cc_rec_end_date as varchar) as cc_rec_end_date, cc_closed_date_sk, cc_open_date_sk, cc_name, cc_class, cc_employees, cc_sq_ft,\" +\n-                            \"   cast(cc_hours as varchar) as cc_hours, cc_manager, cc_mkt_id, cast(cc_mkt_class as varchar) as cc_mkt_class, cc_mkt_desc, cc_market_manager, \" +\n-                            \"   cc_division, cc_division_name, cc_company, cast(cc_company_name as varchar) as cc_company_name,\" +\n-                            \"   cast(cc_street_number as varchar ) as cc_street_number, cc_street_name, cast(cc_street_type as varchar) as cc_street_type, \" +\n-                            \"   cast(cc_suite_number as varchar) as cc_suite_number, cc_city, cc_county, cast(cc_state as varchar) as cc_state, \" +\n-                            \"   cast(cc_zip as varchar) as cc_zip, cc_country, cc_gmt_offset, cc_tax_percentage \" +\n+                            \"SELECT cc_call_center_sk, cc_call_center_id, cast(cc_rec_start_date as varchar) as cc_rec_start_date, \" +\n+                            \"   cast(cc_rec_end_date as varchar) as cc_rec_end_date, cc_closed_date_sk, cc_open_date_sk, cc_name, cc_class, \" +\n+                            \"   cc_employees, cc_sq_ft, cc_hours, cc_manager, cc_mkt_id, cc_mkt_class, cc_mkt_desc, cc_market_manager, cc_division, \" +\n+                            \"   cc_division_name, cc_company, cc_company_name, cc_street_number, cc_street_name, cc_street_type, cc_suite_number, \" +\n+                            \"   cc_city, cc_county, cc_state, cc_zip, cc_country, cc_gmt_offset, cc_tax_percentage \" +\n                             \"FROM tpcds.tiny.call_center\");\n                     break;\n             }\n@@ -119,9 +110,7 @@ private static void createTpcdsCatalogPage(QueryRunner queryRunner, Session sess\n     {\n         if (!queryRunner.tableExists(session, \"catalog_page\")) {\n             queryRunner.execute(session, \"CREATE TABLE catalog_page AS \" +\n-                    \"SELECT cp_catalog_page_sk, cast(cp_catalog_page_id as varchar) as cp_catalog_page_id, cp_start_date_sk, cp_end_date_sk, \" +\n-                    \"   cp_department, cp_catalog_number, cp_catalog_page_number, cp_description, cp_type \" +\n-                    \"FROM tpcds.tiny.catalog_page\");\n+                    \"SELECT * FROM tpcds.tiny.catalog_page\");\n         }\n     }\n \n@@ -143,13 +132,7 @@ private static void createTpcdsCustomer(QueryRunner queryRunner, Session session\n     {\n         if (!queryRunner.tableExists(session, \"customer\")) {\n             queryRunner.execute(session, \"CREATE TABLE customer AS \" +\n-                    \"SELECT c_customer_sk, cast(c_customer_id as varchar) as c_customer_id, c_current_cdemo_sk, c_current_hdemo_sk, \" +\n-                    \"   c_current_addr_sk, c_first_shipto_date_sk, c_first_sales_date_sk, cast(c_salutation as varchar) as c_salutation, \" +\n-                    \"   cast(c_first_name as varchar) as c_first_name, cast(c_last_name as varchar) as c_last_name, \" +\n-                    \"   cast(c_preferred_cust_flag as varchar) as c_preferred_cust_flag, c_birth_day, c_birth_month, c_birth_year, \" +\n-                    \"   c_birth_country, cast(c_login as varchar) as c_login, cast(c_email_address as varchar) as c_email_address,  \" +\n-                    \"   c_last_review_date_sk \" +\n-                    \"FROM tpcds.tiny.customer\");\n+                    \"SELECT * FROM tpcds.tiny.customer\");\n         }\n     }\n \n@@ -157,11 +140,7 @@ private static void createTpcdsCustomerAddress(QueryRunner queryRunner, Session\n     {\n         if (!queryRunner.tableExists(session, \"customer_address\")) {\n             queryRunner.execute(session, \"CREATE TABLE customer_address AS \" +\n-                    \"SELECT ca_address_sk, cast(ca_address_id as varchar) as ca_address_id, cast(ca_street_number as varchar) as ca_street_number,  \" +\n-                    \"   ca_street_name, cast(ca_street_type as varchar) as ca_street_type, cast(ca_suite_number as varchar) as ca_suite_number,  \" +\n-                    \"   ca_city, ca_county, cast(ca_state as varchar) as ca_state, cast(ca_zip as varchar) as ca_zip, \" +\n-                    \"   ca_country, ca_gmt_offset, cast(ca_location_type as varchar) as ca_location_type \" +\n-                    \"FROM tpcds.tiny.customer_address\");\n+                    \"SELECT * FROM tpcds.tiny.customer_address\");\n         }\n     }\n \n@@ -169,10 +148,7 @@ private static void createTpcdsCustomerDemographics(QueryRunner queryRunner, Ses\n     {\n         if (!queryRunner.tableExists(session, \"customer_demographics\")) {\n             queryRunner.execute(session, \"CREATE TABLE customer_demographics AS \" +\n-                    \"SELECT cd_demo_sk, cast(cd_gender as varchar) as cd_gender, cast(cd_marital_status as varchar) as cd_marital_status,  \" +\n-                    \"   cast(cd_education_status as varchar) as cd_education_status, cd_purchase_estimate,  \" +\n-                    \"   cast(cd_credit_rating as varchar) as cd_credit_rating, cd_dep_count, cd_dep_employed_count, cd_dep_college_count \" +\n-                    \"FROM tpcds.tiny.customer_demographics\");\n+                    \"SELECT * FROM tpcds.tiny.customer_demographics\");\n         }\n     }\n \n@@ -183,26 +159,13 @@ private static void createTpcdsDateDim(QueryRunner queryRunner, Session session,\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE date_dim AS \" +\n-                            \"SELECT d_date_sk, cast(d_date_id as varchar) as d_date_id, d_date, \" +\n-                            \"   d_month_seq, d_week_seq, d_quarter_seq, d_year, d_dow, d_moy, d_dom, d_qoy, d_fy_year, \" +\n-                            \"   d_fy_quarter_seq, d_fy_week_seq, cast(d_day_name as varchar) as d_day_name, cast(d_quarter_name as varchar) as d_quarter_name, \" +\n-                            \"   cast(d_holiday as varchar) as d_holiday,  cast(d_weekend as varchar) as d_weekend, \" +\n-                            \"   cast(d_following_holiday as varchar) as d_following_holiday, d_first_dom, d_last_dom, d_same_day_ly, d_same_day_lq,  \" +\n-                            \"   cast(d_current_day as varchar) as d_current_day, cast(d_current_week as varchar) as d_current_week, \" +\n-                            \"   cast(d_current_month as varchar) as d_current_month,  cast(d_current_quarter as varchar) as d_current_quarter, \" +\n-                            \"   cast(d_current_year as varchar) as d_current_year \" +\n-                            \"FROM tpcds.tiny.date_dim\");\n+                            \"SELECT * FROM tpcds.tiny.date_dim\");\n                     break;\n                 case \"DWRF\":\n                     queryRunner.execute(session, \"CREATE TABLE date_dim AS \" +\n-                            \"SELECT d_date_sk, cast(d_date_id as varchar) as d_date_id, cast(d_date as varchar) as d_date, \" +\n-                            \"   d_month_seq, d_week_seq, d_quarter_seq, d_year, d_dow, d_moy, d_dom, d_qoy, d_fy_year, \" +\n-                            \"   d_fy_quarter_seq, d_fy_week_seq, cast(d_day_name as varchar) as d_day_name, cast(d_quarter_name as varchar) as d_quarter_name, \" +\n-                            \"   cast(d_holiday as varchar) as d_holiday,  cast(d_weekend as varchar) as d_weekend, \" +\n-                            \"   cast(d_following_holiday as varchar) as d_following_holiday, d_first_dom, d_last_dom, d_same_day_ly, d_same_day_lq,  \" +\n-                            \"   cast(d_current_day as varchar) as d_current_day, cast(d_current_week as varchar) as d_current_week, \" +\n-                            \"   cast(d_current_month as varchar) as d_current_month,  cast(d_current_quarter as varchar) as d_current_quarter, \" +\n-                            \"   cast(d_current_year as varchar) as d_current_year \" +\n+                            \"SELECT d_date_sk, d_date_id, cast(d_date as varchar) as d_date, d_month_seq, d_week_seq, d_quarter_seq, d_year, d_dow, d_moy, d_dom, d_qoy, d_fy_year, \" +\n+                            \"   d_fy_quarter_seq, d_fy_week_seq, d_day_name, d_quarter_name, d_holiday,  d_weekend, d_following_holiday, d_first_dom, \" +\n+                            \"   d_last_dom, d_same_day_ly, d_same_day_lq, d_current_day, d_current_week, d_current_month,  d_current_quarter, d_current_year \" +\n                             \"FROM tpcds.tiny.date_dim\");\n                     break;\n             }\n@@ -213,8 +176,7 @@ private static void createTpcdsHouseholdDemographics(QueryRunner queryRunner, Se\n     {\n         if (!queryRunner.tableExists(session, \"household_demographics\")) {\n             queryRunner.execute(session, \"CREATE TABLE household_demographics AS \" +\n-                    \"SELECT hd_demo_sk, hd_income_band_sk, cast(hd_buy_potential as varchar) as hd_buy_potential, hd_dep_count, hd_vehicle_count \" +\n-                    \"FROM tpcds.tiny.household_demographics\");\n+                    \"SELECT * FROM tpcds.tiny.household_demographics\");\n         }\n     }\n \n@@ -241,23 +203,13 @@ private static void createTpcdsItem(QueryRunner queryRunner, Session session, St\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE item AS \" +\n-                            \"SELECT i_item_sk, cast(i_item_id as varchar) as i_item_id, i_rec_start_date, i_rec_end_date, \" +\n-                            \"   i_item_desc, i_current_price, i_wholesale_cost, i_brand_id, cast(i_brand as varchar) as i_brand, \" +\n-                            \"   i_class_id,  cast(i_class as varchar) as i_class, i_category_id, cast(i_category as varchar) as i_category, i_manufact_id, \" +\n-                            \"   cast(i_manufact as varchar) as i_manufact, cast(i_size as varchar) as i_size, cast(i_formulation as varchar) as i_formulation, \" +\n-                            \"   cast(i_color as varchar) as i_color, cast(i_units as varchar) as i_units, cast(i_container as varchar) as i_container, i_manager_id, \" +\n-                            \"   cast(i_product_name as varchar) as i_product_name \" +\n-                            \"FROM tpcds.tiny.item\");\n+                            \"SELECT * FROM tpcds.tiny.item\");\n                     break;\n                 case \"DRWF\":\n                     queryRunner.execute(session, \"CREATE TABLE item AS \" +\n-                            \"SELECT i_item_sk, cast(i_item_id as varchar) as i_item_id, cast(i_rec_start_date as varchar) as i_rec_start_date, \" +\n-                            \"   cast(i_rec_end_date as varchar) as i_rec_end_date, i_item_desc, cast(i_current_price as double) as i_current_price, \" +\n-                            \"   cast(i_wholesale_cost as double) as i_wholesale_cost, i_brand_id, cast(i_brand as varchar) as i_brand, \" +\n-                            \"   i_class_id,  cast(i_class as varchar) as i_class, i_category_id, cast(i_category as varchar) as i_category, i_manufact_id, \" +\n-                            \"   cast(i_manufact as varchar) as i_manufact, cast(i_size as varchar) as i_size, cast(i_formulation as varchar) as i_formulation, \" +\n-                            \"   cast(i_color as varchar) as i_color, cast(i_units as varchar) as i_units, cast(i_container as varchar) as i_container, i_manager_id, \" +\n-                            \"   cast(i_product_name as varchar) as i_product_name \" +\n+                            \"SELECT i_item_sk, i_item_id, cast(i_rec_start_date as varchar) as i_rec_start_date, cast(i_rec_end_date as varchar) as i_rec_end_date, \" +\n+                            \"   i_item_desc, i_current_price, i_wholesale_cost, i_brand_id, i_brand, i_class_id,  i_class, i_category_id, \" +\n+                            \"   i_category, i_manufact_id, i_manufact, i_size, i_formulation, i_color, i_units, i_container, i_manager_id, i_product_name \" +\n                             \"FROM tpcds.tiny.item\");\n                     break;\n             }\n@@ -268,14 +220,7 @@ private static void createTpcdsPromotion(QueryRunner queryRunner, Session sessio\n     {\n         if (!queryRunner.tableExists(session, \"promotion\")) {\n             queryRunner.execute(session, \"CREATE TABLE promotion AS \" +\n-                    \"SELECT p_promo_sk, cast(p_promo_id as varchar) as p_promo_id, p_start_date_sk, p_end_date_sk, p_item_sk, \" +\n-                    \"   p_cost, p_response_targe, cast(p_promo_name as varchar) as p_promo_name, \" +\n-                    \"   cast(p_channel_dmail as varchar) as p_channel_dmail, cast(p_channel_email as varchar) as p_channel_email, \" +\n-                    \"   cast(p_channel_catalog as varchar) as p_channel_catalog, cast(p_channel_tv as varchar) as p_channel_tv, \" +\n-                    \"   cast(p_channel_radio as varchar) as p_channel_radio, cast(p_channel_press as varchar) as p_channel_press, \" +\n-                    \"   cast(p_channel_event as varchar) as p_channel_event, cast(p_channel_demo as varchar) as p_channel_demo, p_channel_details, \" +\n-                    \"   cast(p_purpose as varchar) as p_purpose, cast(p_discount_active as varchar) as p_discount_active \" +\n-                    \"FROM tpcds.tiny.promotion\");\n+                    \"SELECT * FROM tpcds.tiny.promotion\");\n         }\n     }\n \n@@ -283,8 +228,7 @@ private static void createTpcdsReason(QueryRunner queryRunner, Session session)\n     {\n         if (!queryRunner.tableExists(session, \"reason\")) {\n             queryRunner.execute(session, \"CREATE TABLE reason AS \" +\n-                    \"SELECT r_reason_sk, cast(r_reason_id as varchar) as r_reason_id, cast(r_reason_desc as varchar) as r_reason_desc \" +\n-                    \"FROM tpcds.tiny.reason\");\n+                    \"SELECT * FROM tpcds.tiny.reason\");\n         }\n     }\n \n@@ -292,9 +236,7 @@ private static void createTpcdsShipMode(QueryRunner queryRunner, Session session\n     {\n         if (!queryRunner.tableExists(session, \"ship_mode\")) {\n             queryRunner.execute(session, \"CREATE TABLE ship_mode AS \" +\n-                    \"SELECT sm_ship_mode_sk, cast(sm_ship_mode_id as varchar) as sm_ship_mode_id, cast(sm_type as varchar) as sm_type, \" +\n-                    \"   cast(sm_code as varchar) as sm_code, cast(sm_carrier as varchar) as sm_carrier, cast(sm_contract as varchar) as sm_contract \" +\n-                    \"FROM tpcds.tiny.ship_mode\");\n+                    \"SELECT * FROM tpcds.tiny.ship_mode\");\n         }\n     }\n \n@@ -305,24 +247,15 @@ private static void createTpcdsStore(QueryRunner queryRunner, Session session, S\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE store AS \" +\n-                            \"SELECT s_store_sk, cast(s_store_id as varchar) as s_store_id, s_rec_start_date, s_rec_end_date, \" +\n-                            \"   s_closed_date_sk, s_store_name, s_number_employees, s_floor_space, cast(s_hours as varchar) as s_hours, \" +\n-                            \"   s_manager, s_market_id, s_geography_class, s_market_desc, s_market_manager, s_division_id, s_division_name, \" +\n-                            \"   s_company_id, s_company_name, s_street_number, s_street_name, cast(s_street_type as varchar) as s_street_type, \" +\n-                            \"   cast(s_suite_number as varchar) as s_suite_number, s_city, s_county, cast(s_state as varchar ) as s_state, \" +\n-                            \"   cast(s_zip as varchar) as s_zip, s_country, s_gmt_offset, s_tax_precentage \" +\n-                            \"FROM tpcds.tiny.store\");\n+                            \"SELECT * FROM tpcds.tiny.store\");\n                     break;\n                 case \"DRWF\":\n                     queryRunner.execute(session, \"CREATE TABLE store AS \" +\n-                            \"SELECT s_store_sk, cast(s_store_id as varchar) as s_store_id, cast(s_rec_start_date as varchar) as s_rec_start_date, \" +\n-                            \"   cast(s_rec_end_date as varchar) as s_rec_end_date, s_closed_date_sk, s_store_name, s_number_employees, s_floor_space, \" +\n-                            \"   cast(s_hours as varchar) as s_hours, s_manager, s_market_id, s_geography_class, s_market_desc, s_market_manager, \" +\n-                            \"   s_division_id, s_division_name, s_company_id, s_company_name, s_street_number, s_street_name, \" +\n-                            \"   cast(s_street_type as varchar) as s_street_type, cast(s_suite_number as varchar) as s_suite_number, s_city, s_county, \" +\n-                            \"   cast(s_state as varchar ) as s_state, cast(s_zip as varchar) as s_zip, s_country, \" +\n-                            \"   cast(s_gmt_offset as double) as s_gmt_offset, \" +\n-                            \"   cast(s_tax_precentage as double) as s_tax_precentage \" +\n+                            \"SELECT s_store_sk, cast(s_rec_start_date as varchar) as s_rec_start_date, cast(s_rec_end_date as varchar) as s_rec_end_date, \" +\n+                            \"   s_closed_date_sk, s_store_name, s_number_employees, s_floor_space, s_hours, s_manager, s_market_id, s_geography_class, \" +\n+                            \"   s_market_desc, s_market_manager, s_division_id, s_division_name, s_company_id, s_company_name, s_street_number, s_street_name, \" +\n+                            \"   s_street_type, s_suite_number, s_city, s_county, s_state, s_zip, s_country, \" +\n+                            \"   cast(s_gmt_offset as double) as s_gmt_offset, cast(s_tax_precentage as double) as s_tax_precentage \" +\n                             \"FROM tpcds.tiny.store\");\n                     break;\n             }\n@@ -349,10 +282,7 @@ private static void createTpcdsTimeDim(QueryRunner queryRunner, Session session)\n     {\n         if (!queryRunner.tableExists(session, \"time_dim\")) {\n             queryRunner.execute(session, \"CREATE TABLE time_dim AS \" +\n-                    \"SELECT t_time_sk, cast(t_time_id as varchar) as t_time_id, t_time, t_hour, t_minute, t_second,  \" +\n-                    \"   cast(t_am_pm as varchar) as t_am_pm, cast(t_shift as varchar) as t_shift, \" +\n-                    \"   cast(t_sub_shift as varchar) as t_sub_shift, cast(t_meal_time as varchar) as t_meal_time \" +\n-                    \"FROM tpcds.tiny.time_dim\");\n+                    \"SELECT * FROM tpcds.tiny.time_dim\");\n         }\n     }\n \n@@ -360,11 +290,7 @@ private static void createTpcdsWarehouse(QueryRunner queryRunner, Session sessio\n     {\n         if (!queryRunner.tableExists(session, \"warehouse\")) {\n             queryRunner.execute(session, \"CREATE TABLE warehouse AS \" +\n-                    \"SELECT w_warehouse_sk, cast(w_warehouse_id as varchar) as w_warehouse_id, w_warehouse_name, w_warehouse_sq_ft, \" +\n-                    \"   cast(w_street_number as varchar) as w_street_number, w_street_name, cast(w_street_type as varchar) as w_street_type, \" +\n-                    \"   cast(w_suite_number as varchar) as w_suite_number, w_city, w_county, cast(w_state as varchar) as w_state,\" +\n-                    \"   cast(w_zip as varchar) as w_zip, w_country, w_gmt_offset \" +\n-                    \"FROM tpcds.tiny.warehouse\");\n+                    \"SELECT * FROM tpcds.tiny.warehouse\");\n         }\n     }\n \n@@ -375,17 +301,14 @@ private static void createTpcdsWebPage(QueryRunner queryRunner, Session session,\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE web_page AS \" +\n-                            \"SELECT wp_web_page_sk, cast(wp_web_page_id as varchar) as wp_web_page_id, wp_rec_start_date, wp_rec_end_date, \" +\n-                            \"   wp_creation_date_sk, wp_access_date_sk, cast(wp_autogen_flag as varchar) as wp_autogen_flag, wp_customer_sk, \" +\n-                            \"   wp_url, cast(wp_type as varchar) as wp_type, wp_char_count, wp_link_count, wp_image_count, wp_max_ad_count \" +\n-                            \"FROM tpcds.tiny.web_page\");\n+                            \"SELECT * FROM tpcds.tiny.web_page\");\n                     break;\n                 case \"DWRF\":\n                     queryRunner.execute(session, \"CREATE TABLE web_page AS \" +\n-                            \"SELECT wp_web_page_sk, cast(wp_web_page_id as varchar) as wp_web_page_id, cast(wp_rec_start_date as varchar) as wp_rec_start_date, \" +\n+                            \"SELECT wp_web_page_sk, wp_web_page_id, cast(wp_rec_start_date as varchar) as wp_rec_start_date, \" +\n                             \"   cast(wp_rec_end_date as varchar) as wp_rec_end_date, wp_creation_date_sk, wp_access_date_sk, \" +\n-                            \"   cast(wp_autogen_flag as varchar) as wp_autogen_flag, wp_customer_sk, wp_url, cast(wp_type as varchar) as wp_type, \" +\n-                            \"   wp_char_count, wp_link_count, wp_image_count, wp_max_ad_count \" +\n+                            \"   wp_autogen_flag, wp_customer_sk, wp_url, wp_type, wp_char_count, wp_link_count, wp_image_count, \" +\n+                            \"   wp_max_ad_count \" +\n                             \"FROM tpcds.tiny.web_page\");\n                     break;\n             }\n@@ -415,23 +338,15 @@ private static void createTpcdsWebSite(QueryRunner queryRunner, Session session,\n                 case \"PARQUET\":\n                 case \"ORC\":\n                     queryRunner.execute(session, \"CREATE TABLE web_site AS \" +\n-                            \"SELECT web_site_sk, cast(web_site_id as varchar) as web_site_id, web_rec_start_date, web_rec_end_date, web_name, \" +\n-                            \"   web_open_date_sk, web_close_date_sk, web_class, web_manager, web_mkt_id, web_mkt_class, web_mkt_desc, web_market_manager, \" +\n-                            \"   web_company_id, cast(web_company_name as varchar) as web_company_name, cast(web_street_number as varchar) as web_street_number, \" +\n-                            \"   web_street_name, cast(web_street_type as varchar) as web_street_type, \" +\n-                            \"   cast(web_suite_number as varchar) as web_suite_number, web_city, web_county, cast(web_state as varchar) as web_state, \" +\n-                            \"   cast(web_zip as varchar) as web_zip, web_country, web_gmt_offset, web_tax_percentage \" +\n-                            \"FROM tpcds.tiny.web_site\");\n+                            \"SELECT * FROM tpcds.tiny.web_site\");\n                     break;\n                 case \"DWRF\":\n                     queryRunner.execute(session, \"CREATE TABLE web_site AS \" +\n-                            \"SELECT web_site_sk, cast(web_site_id as varchar) as web_site_id, cast(web_rec_start_date as varchar) as web_rec_start_date, \" +\n+                            \"SELECT web_site_sk, web_site_id, cast(web_rec_start_date as varchar) as web_rec_start_date, \" +\n                             \"   cast(web_rec_end_date as varchar) as web_rec_end_date, web_name, web_open_date_sk, web_close_date_sk, web_class, \" +\n-                            \"   web_manager, web_mkt_id, web_mkt_class, web_mkt_desc, web_market_manager, web_company_id, cast(web_company_name as varchar) as web_company_name, \" +\n-                            \"   cast(web_street_number as varchar) as web_street_number, web_street_name, cast(web_street_type as varchar) as web_street_type, \" +\n-                            \"   cast(web_suite_number as varchar) as web_suite_number, web_city, web_county, cast(web_state as varchar) as web_state, \" +\n-                            \"   cast(web_zip as varchar) as web_zip, web_country, cast(web_gmt_offset as double) as web_gmt_offset, \" +\n-                            \"   cast(web_tax_percentage as double) as web_tax_percentage \" +\n+                            \"   web_manager, web_mkt_id, web_mkt_class, web_mkt_desc, web_market_manager, web_company_id, web_company_name, \" +\n+                            \"   web_street_number, web_street_name, web_street_type, web_suite_number, web_city, web_county, web_state, as web_zip, web_country, \" +\n+                            \"   cast(web_gmt_offset as double) as web_gmt_offset, cast(web_tax_percentage as double) as web_tax_percentage \" +\n                             \"FROM tpcds.tiny.web_site\");\n                     break;\n             }\n@@ -527,7 +442,7 @@ public void testTpcdsQ11()\n     public void testTpcdsQ12()\n             throws Exception\n     {\n-        assertQuery(session, getTpcdsQuery(\"12\"));\n+        assertQueryFails(session, getTpcdsQuery(\"12\"), \"[\\\\s\\\\S]*Division by zero[\\\\s\\\\S]*\");\n     }\n \n     @Test\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\nindex f515af775834d..8a0c0facf96d5 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n@@ -64,6 +64,13 @@ public static Map<String, String> getNativeSidecarProperties()\n                 .build();\n     }\n \n+    public static Map<String, String> getNativeWorkerTpcdsProperties()\n+    {\n+        return ImmutableMap.<String, String>builder()\n+                .put(\"tpcds.use-varchar-type\", \"true\")\n+                .build();\n+    }\n+\n     /**\n      * Creates all tables for local testing, except for bench tables.\n      *\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\nindex 825fbb1bc4387..75451598049a9 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n@@ -60,6 +60,7 @@\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerHiveProperties;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerIcebergProperties;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerSystemProperties;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerTpcdsProperties;\n import static com.facebook.presto.nativeworker.SymlinkManifestGeneratorUtils.createSymlinkManifest;\n import static java.lang.String.format;\n import static java.util.Collections.emptyList;\n@@ -86,6 +87,7 @@ public class PrestoNativeQueryRunnerUtils\n             ParquetHiveSerDe.class.getName(),\n             SymlinkTextInputFormat.class.getName(),\n             HiveIgnoreKeyTextOutputFormat.class.getName());\n+\n     private PrestoNativeQueryRunnerUtils() {}\n \n     public static QueryRunner createQueryRunner(boolean addStorageFormatToPath, boolean isCoordinatorSidecarEnabled)\n@@ -175,7 +177,8 @@ public static QueryRunner createJavaQueryRunner(Optional<Path> baseDataDirectory\n                                 \"offset-clause-enabled\", \"true\"),\n                         security,\n                         hivePropertiesBuilder.build(),\n-                        dataDirectory);\n+                        dataDirectory,\n+                        getNativeWorkerTpcdsProperties());\n         return queryRunner;\n     }\n \n@@ -237,7 +240,8 @@ public static QueryRunner createJavaIcebergQueryRunner(Optional<Path> baseDataDi\n                 OptionalInt.empty(),\n                 Optional.empty(),\n                 baseDataDirectory,\n-                addStorageFormatToPath);\n+                addStorageFormatToPath,\n+                getNativeWorkerTpcdsProperties());\n \n         return queryRunner;\n     }\n@@ -312,7 +316,8 @@ public static QueryRunner createNativeIcebergQueryRunner(\n                 OptionalInt.of(workerCount.orElse(4)),\n                 getExternalWorkerLauncher(\"iceberg\", prestoServerPath, cacheMaxSize, remoteFunctionServerUds, false, false),\n                 dataDirectory,\n-                addStorageFormatToPath);\n+                addStorageFormatToPath,\n+                getNativeWorkerTpcdsProperties());\n     }\n \n     public static QueryRunner createNativeQueryRunner(\n@@ -356,7 +361,8 @@ public static QueryRunner createNativeQueryRunner(\n                 hiveProperties,\n                 workerCount,\n                 Optional.of(Paths.get(addStorageFormatToPath ? dataDirectory + \"/\" + storageFormat : dataDirectory)),\n-                getExternalWorkerLauncher(\"hive\", prestoServerPath, cacheMaxSize, remoteFunctionServerUds, failOnNestedLoopJoin, isCoordinatorSidecarEnabled));\n+                getExternalWorkerLauncher(\"hive\", prestoServerPath, cacheMaxSize, remoteFunctionServerUds, failOnNestedLoopJoin, isCoordinatorSidecarEnabled),\n+                getNativeWorkerTpcdsProperties());\n     }\n \n     public static QueryRunner createNativeCteQueryRunner(boolean useThrift, String storageFormat)\n@@ -399,7 +405,8 @@ public static QueryRunner createNativeCteQueryRunner(boolean useThrift, String s\n                 hiveProperties,\n                 workerCount,\n                 Optional.of(Paths.get(addStorageFormatToPath ? dataDirectory + \"/\" + storageFormat : dataDirectory)),\n-                getExternalWorkerLauncher(\"hive\", prestoServerPath, cacheMaxSize, Optional.empty(), false, false));\n+                getExternalWorkerLauncher(\"hive\", prestoServerPath, cacheMaxSize, Optional.empty(), false, false),\n+                getNativeWorkerTpcdsProperties());\n     }\n \n     public static QueryRunner createNativeQueryRunner(String remoteFunctionServerUds)\n@@ -475,10 +482,10 @@ public static String startRemoteFunctionServer(String remoteFunctionServerBinary\n     public static NativeQueryRunnerParameters getNativeQueryRunnerParameters()\n     {\n         Path prestoServerPath = Paths.get(getProperty(\"PRESTO_SERVER\")\n-                .orElse(\"_build/debug/presto_cpp/main/presto_server\"))\n+                        .orElse(\"_build/debug/presto_cpp/main/presto_server\"))\n                 .toAbsolutePath();\n         Path dataDirectory = Paths.get(getProperty(\"DATA_DIR\")\n-                .orElse(\"target/velox_data\"))\n+                        .orElse(\"target/velox_data\"))\n                 .toAbsolutePath();\n         Optional<Integer> workerCount = getProperty(\"WORKER_COUNT\").map(Integer::parseInt);\n \n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcds.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/AbstractTestTpcds.java\nsimilarity index 93%\nrename from presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcds.java\nrename to presto-tpcds/src/test/java/com/facebook/presto/tpcds/AbstractTestTpcds.java\nindex 53355729c10f1..39c68e2296c18 100644\n--- a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcds.java\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/AbstractTestTpcds.java\n@@ -14,7 +14,6 @@\n package com.facebook.presto.tpcds;\n \n import com.facebook.presto.testing.MaterializedResult;\n-import com.facebook.presto.testing.QueryRunner;\n import com.facebook.presto.tests.AbstractTestQueryFramework;\n import org.testng.annotations.Test;\n \n@@ -25,16 +24,9 @@\n import static java.util.stream.Collectors.joining;\n import static java.util.stream.IntStream.range;\n \n-public class TestTpcds\n+public abstract class AbstractTestTpcds\n         extends AbstractTestQueryFramework\n {\n-    @Override\n-    protected QueryRunner createQueryRunner()\n-            throws Exception\n-    {\n-        return TpcdsQueryRunner.createQueryRunner();\n-    }\n-\n     @Test\n     public void testSelect()\n     {\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java\nindex 5d732fff6e1b8..432ef7c665f43 100644\n--- a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsMetadataStatistics.java\n@@ -45,7 +45,7 @@ public class TestTpcdsMetadataStatistics\n {\n     private static final EstimateAssertion estimateAssertion = new EstimateAssertion(0.01);\n     private static final ConnectorSession session = null;\n-    private final TpcdsMetadata metadata = new TpcdsMetadata();\n+    private final TpcdsMetadata metadata = new TpcdsMetadata(false);\n \n     @Test\n     public void testNoTableStatsForNotSupportedSchema()\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java\nindex f4091fb9b223b..876604765cb31 100644\n--- a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsRecordSet.java\n@@ -64,7 +64,7 @@ public void testGetColumnTypes()\n                 CC_NAME,            // Varchar\n                 CC_STATE,           // CharType\n                 CC_TAX_PERCENTAGE); // Decimal\n-        RecordSet recordSet = new TpcdsRecordSet(constructResults(table, session), columns);\n+        RecordSet recordSet = new TpcdsRecordSet(constructResults(table, session), columns, false);\n         assertEquals(recordSet.getColumnTypes(), ImmutableList.of(\n                 BIGINT,\n                 INTEGER,\n@@ -77,7 +77,7 @@ public void testGetColumnTypes()\n                 DV_CREATE_DATE, // Date\n                 DV_CREATE_TIME, // Time\n                 DV_VERSION);    // Varchar\n-        recordSet = new TpcdsRecordSet(constructResults(table, session), columns);\n+        recordSet = new TpcdsRecordSet(constructResults(table, session), columns, false);\n         assertEquals(recordSet.getColumnTypes(), ImmutableList.of(\n                 DATE,\n                 TIME,\n@@ -85,7 +85,7 @@ public void testGetColumnTypes()\n \n         table = getTable(DBGEN_VERSION.getName());\n         columns = ImmutableList.of();\n-        recordSet = new TpcdsRecordSet(constructResults(table, session), columns);\n+        recordSet = new TpcdsRecordSet(constructResults(table, session), columns, false);\n         assertEquals(recordSet.getColumnTypes(), ImmutableList.of());\n     }\n \n@@ -94,7 +94,7 @@ public void testCursor()\n     {\n         Table table = getTable(CALL_CENTER.getName());\n         Results result = constructResults(table, session);\n-        RecordSet recordSet = new TpcdsRecordSet(result, ImmutableList.copyOf(table.getColumns()));\n+        RecordSet recordSet = new TpcdsRecordSet(result, ImmutableList.copyOf(table.getColumns()), false);\n \n         try (RecordCursor cursor = recordSet.cursor()) {\n             if (cursor.advanceNextPosition()) {\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsChar.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsChar.java\nnew file mode 100644\nindex 0000000000000..b5bca766686f3\n--- /dev/null\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsChar.java\n@@ -0,0 +1,27 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.tpcds;\n+\n+import com.facebook.presto.testing.QueryRunner;\n+\n+public class TestTpcdsWithCharColumnsAsChar\n+        extends AbstractTestTpcds\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return TpcdsQueryRunner.createQueryRunner();\n+    }\n+}\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsVarchar.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsVarchar.java\nnew file mode 100644\nindex 0000000000000..05f4053027de6\n--- /dev/null\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TestTpcdsWithCharColumnsAsVarchar.java\n@@ -0,0 +1,62 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.tpcds;\n+\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.Test;\n+\n+import java.math.BigDecimal;\n+import java.util.Map;\n+\n+import static com.facebook.presto.testing.MaterializedResult.resultBuilder;\n+import static com.facebook.presto.testing.assertions.Assert.assertEquals;\n+\n+public class TestTpcdsWithCharColumnsAsVarchar\n+        extends AbstractTestTpcds\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        Map<String, String> tpcdsProperties = ImmutableMap.<String, String>builder()\n+                .put(\"tpcds.use-varchar-type\", \"true\")\n+                .build();\n+        return TpcdsQueryRunner.createQueryRunner(ImmutableMap.of(), tpcdsProperties);\n+    }\n+\n+    @Override\n+    @Test\n+    public void testSelect()\n+    {\n+        MaterializedResult actual = computeActual(\n+                \"SELECT c_first_name, c_last_name, ca_address_sk, ca_gmt_offset \" +\n+                        \"FROM customer JOIN customer_address ON c_current_addr_sk = ca_address_sk \" +\n+                        \"WHERE ca_address_sk = 4\");\n+        MaterializedResult expected = resultBuilder(getSession(), actual.getTypes())\n+                .row(\"James\", \"Brown\", 4L, new BigDecimal(\"-7.00\"))\n+                .build();\n+        assertEquals(expected, actual);\n+\n+        actual = computeActual(\n+                \"SELECT c_first_name, c_last_name \" +\n+                        \"FROM customer JOIN customer_address ON c_current_addr_sk = ca_address_sk \" +\n+                        \"WHERE ca_address_sk = 4 AND ca_gmt_offset = DECIMAL '-7.00'\");\n+        expected = resultBuilder(getSession(), actual.getTypes())\n+                .row(\"James\", \"Brown\")\n+                .build();\n+        assertEquals(expected, actual);\n+    }\n+}\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java\nindex a506e5a7c6e5c..8643bed4305d2 100644\n--- a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/TpcdsQueryRunner.java\n@@ -39,7 +39,13 @@ public static DistributedQueryRunner createQueryRunner(Map<String, String> extra\n         return createQueryRunner(extraProperties, ImmutableMap.of());\n     }\n \n-    public static DistributedQueryRunner createQueryRunner(Map<String, String> extraProperties, Map<String, String> coordinatorProperties)\n+    public static DistributedQueryRunner createQueryRunner(Map<String, String> extraProperties, Map<String, String> tpcdsProperties)\n+            throws Exception\n+    {\n+        return createQueryRunner(extraProperties, tpcdsProperties, ImmutableMap.of());\n+    }\n+\n+    public static DistributedQueryRunner createQueryRunner(Map<String, String> extraProperties, Map<String, String> tpcdsProperties, Map<String, String> coordinatorProperties)\n             throws Exception\n     {\n         Session session = testSessionBuilder()\n@@ -56,7 +62,7 @@ public static DistributedQueryRunner createQueryRunner(Map<String, String> extra\n \n         try {\n             queryRunner.installPlugin(new TpcdsPlugin());\n-            queryRunner.createCatalog(\"tpcds\", \"tpcds\");\n+            queryRunner.createCatalog(\"tpcds\", \"tpcds\", tpcdsProperties);\n             return queryRunner;\n         }\n         catch (Exception e) {\n\ndiff --git a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java\nindex f9ca1411d7ec5..ded070bf342d8 100644\n--- a/presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java\n+++ b/presto-tpcds/src/test/java/com/facebook/presto/tpcds/statistics/TableStatisticsRecorder.java\n@@ -41,7 +41,7 @@ public TableStatisticsData recordStatistics(Table table, double scaleFactor)\n                 .withNoSexism(false);\n \n         List<Column> columns = ImmutableList.copyOf(table.getColumns());\n-        RecordCursor recordCursor = new TpcdsRecordSet(Results.constructResults(table, session), columns)\n+        RecordCursor recordCursor = new TpcdsRecordSet(Results.constructResults(table, session), columns, false)\n                 .cursor();\n \n         List<ColumnStatisticsRecorder> statisticsRecorders = createStatisticsRecorders(columns);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24397",
    "pr_id": 24397,
    "issue_id": 24383,
    "repo": "prestodb/presto",
    "problem_statement": "Enabling Presto Iceberg to leverage the powerful capabilities of object storages\nCurrently, object stores become more and more common comparing with HDFS, they have higher scalability, better performance, and support for cloud services. So, enabling Presto Iceberg to leverage the powerful ability of object stores should be a good feature.\n\nAs discussed with @tdcmeehan in PR #24221, there may be issues with transaction atomicity and consistency when using HadoopCatalog to directly manage metadata on object stores. Although many people are trying to solve this problem (see: https://lists.apache.org/thread/kh4n98w4z22sc8h2vot4q8n44vdtnltg), especially after the emergence of S3 conditional writes, the current situation is that there are risks involved.\n\nHowever, referring to the Iceberg community's discussion on the capabilities and limitations of HadoopCatalog  (see: https://lists.apache.org/thread/oohcjfp1vpo005h2r0f6gfpsp6op0qps and https://lists.apache.org/thread/v7x65kxrrozwlvsgstobm7685541lf5w), we know that HadoopCatalog is capable of managing metadata on HDFS, thanks to HDFS's strictly support for atomic non-rewritable rename operations. Moreover, some Iceberg users have already used HadoopCatalog to maintain metadata files on HDFS while store data files on object stores, see: https://lists.apache.org/thread/rkg1cnmcl102o8g9ko5l0o152jzgpglm. Therefore, this is a solution that theoretically does not have transaction consistency issues and has been validated in production environments which could leverage object stores.\n\nWe can expand the capabilities of our Iceberg Hadoop catalog and achieve the followings:\n - 1. Support for setting independent data write path for Iceberg tables, which is also a native capability supported by Iceberg lib. This way, when creating a table, we can specify an independent location for its actual data, such as a path on S3.\n - 2. Add a configuration parameter for Iceberg Hadoop catalog, such as `iceberg.catalog.warehouse.datadir`, which represents the default data writing root directory for newly created tables in the entire catalog. If this value is configured, all newly created tables will default to setting their data write path based on this root dir, unless explicitly specified the table's data write path in table creation statement.\n\nAfter doing these, in production environment, we can configure \"iceberg.catalog.warehouse\" as a locally deployed HDFS path and \"iceberg.catalog.warehouse.datadir\" as an S3 path, to safely utilize the powerful storage capabilities of object stores.\n\nTest plan: Build an object storage environment base on MinIO docker, configure `iceberg.catalog.warehouse` as a local file path, and `iceberg.catalog.warehouse.datadir` as a s3 path, fully run the tests in `IcebergDistributedTestBase`, `IcebergDistributedSmokeTestBase`, and `TestIcebergDistributedQueries`.\n\nI have already completed the verification in my local workspace, and this is not a big change. Any thoughts or concerns would be greatly appreciated. @tdcmeehan @ZacBlanco @imjalpreet @agrawalreetika @kiersten-stokes ",
    "issue_word_count": 443,
    "test_files_count": 19,
    "non_test_files_count": 10,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/pom.xml",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeCatalogFactory.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTablesHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/container/IcebergMinIODataLake.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedOnS3Hadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergHadoopCatalogOnS3DistributedQueries.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeOnS3Hadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTablesHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/container/IcebergMinIODataLake.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedOnS3Hadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergHadoopCatalogOnS3DistributedQueries.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeOnS3Hadoop.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java"
    ],
    "base_commit": "0927c8ff3bacc5132835b875b4c14d56e0759828",
    "head_commit": "dc08c32338934f1c9d27c29b8e24dfaa47e3366b",
    "repo_url": "https://github.com/prestodb/presto/pull/24397",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24397",
    "dockerfile": "",
    "pr_merged_at": "2025-02-26T22:03:27.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 87c9b1fb7d0bf..dd570295342af 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -256,15 +256,52 @@ Property Name                                           Description\n ======================================================= ============================================================= ============\n ``iceberg.catalog.warehouse``                           The catalog warehouse root path for Iceberg tables.\n \n+                                                        The Hadoop catalog requires a file system that supports\n+                                                        an atomic rename operation, such as HDFS, to maintain\n+                                                        metadata files in order to implement an atomic transaction\n+                                                        commit.\n+\n                                                         Example: ``hdfs://nn:8020/warehouse/path``\n+\n+                                                        Do not set ``iceberg.catalog.warehouse`` to a path in object\n+                                                        stores or local file systems in the production environment.\n+\n                                                         This property is required if the ``iceberg.catalog.type`` is\n                                                         ``hadoop``. Otherwise, it will be ignored.\n \n+``iceberg.catalog.hadoop.warehouse.datadir``            The catalog warehouse root data path for Iceberg tables.\n+                                                        It is only supported with the Hadoop catalog.\n+\n+                                                        Example: ``s3://iceberg_bucket/warehouse``.\n+\n+                                                        This optional property can be set to a path in object\n+                                                        stores or HDFS.\n+                                                        If set, all tables in this Hadoop catalog default to saving\n+                                                        their data and delete files in the specified root\n+                                                        data directory.\n+\n ``iceberg.catalog.cached-catalog-num``                  The number of Iceberg catalogs to cache. This property is     ``10``\n                                                         required if the ``iceberg.catalog.type`` is ``hadoop``.\n                                                         Otherwise, it will be ignored.\n ======================================================= ============================================================= ============\n \n+Configure the `Amazon S3 <https://prestodb.io/docs/current/connector/hive.html#amazon-s3-configuration>`_\n+properties to specify a S3 location as the warehouse data directory for the Hadoop catalog. This way,\n+the data and delete files of Iceberg tables are stored in S3. An example configuration includes:\n+\n+.. code-block:: none\n+\n+    connector.name=iceberg\n+    iceberg.catalog.type=hadoop\n+    iceberg.catalog.warehouse=hdfs://nn:8020/warehouse/path\n+    iceberg.catalog.hadoop.warehouse.datadir=s3://iceberg_bucket/warehouse\n+\n+    hive.s3.use-instance-credentials=false\n+    hive.s3.aws-access-key=accesskey\n+    hive.s3.aws-secret-key=secretkey\n+    hive.s3.endpoint=http://192.168.0.103:9878\n+    hive.s3.path-style-access=true\n+\n Configuration Properties\n ------------------------\n \n@@ -370,6 +407,12 @@ Property Name                             Description\n ``location``                               Optionally specifies the file system location URI for\n                                            the table.\n \n+``write.data.path``                        Optionally specifies the file system location URI for\n+                                           storing the data and delete files of the table. This only\n+                                           applies to files written after this property is set. Files\n+                                           previously written aren't relocated to reflect this\n+                                           parameter.\n+\n ``format_version``                         Optionally specifies the format version of the Iceberg           ``2``\n                                            specification to use for new tables, either ``1`` or ``2``.\n \n\ndiff --git a/presto-iceberg/pom.xml b/presto-iceberg/pom.xml\nindex bbf7da4e68cf5..deaf21702d31c 100644\n--- a/presto-iceberg/pom.xml\n+++ b/presto-iceberg/pom.xml\n@@ -263,6 +263,16 @@\n             </exclusions>\n         </dependency>\n \n+        <dependency>\n+          <groupId>com.amazonaws</groupId>\n+          <artifactId>aws-java-sdk-core</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+          <groupId>com.amazonaws</groupId>\n+          <artifactId>aws-java-sdk-s3</artifactId>\n+        </dependency>\n+\n         <dependency>\n             <groupId>org.apache.iceberg</groupId>\n             <artifactId>iceberg-core</artifactId>\n@@ -598,6 +608,18 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>\n+          <groupId>org.testcontainers</groupId>\n+          <artifactId>testcontainers</artifactId>\n+          <scope>test</scope>\n+          <exclusions>\n+            <exclusion>\n+              <groupId>org.slf4j</groupId>\n+              <artifactId>slf4j-api</artifactId>\n+            </exclusion>\n+          </exclusions>\n+        </dependency>\n+\n         <dependency>\n             <groupId>org.apache.iceberg</groupId>\n             <artifactId>iceberg-core</artifactId>\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex d46b44d9a271d..9198272f5775e 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -123,6 +123,7 @@\n import static com.facebook.presto.hive.MetadataUtils.getDiscretePredicates;\n import static com.facebook.presto.hive.MetadataUtils.getPredicate;\n import static com.facebook.presto.hive.MetadataUtils.getSubfieldPredicate;\n+import static com.facebook.presto.hive.metastore.MetastoreUtil.TABLE_COMMENT;\n import static com.facebook.presto.iceberg.ExpressionConverter.toIcebergExpression;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.DATA_SEQUENCE_NUMBER_COLUMN_HANDLE;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.DATA_SEQUENCE_NUMBER_COLUMN_METADATA;\n@@ -146,6 +147,9 @@\n import static com.facebook.presto.iceberg.IcebergTableProperties.METRICS_MAX_INFERRED_COLUMN;\n import static com.facebook.presto.iceberg.IcebergTableProperties.PARTITIONING_PROPERTY;\n import static com.facebook.presto.iceberg.IcebergTableProperties.SORTED_BY_PROPERTY;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getCommitRetries;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getFormatVersion;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getWriteDataLocation;\n import static com.facebook.presto.iceberg.IcebergTableType.CHANGELOG;\n import static com.facebook.presto.iceberg.IcebergTableType.DATA;\n import static com.facebook.presto.iceberg.IcebergTableType.EQUALITY_DELETES;\n@@ -153,14 +157,20 @@\n import static com.facebook.presto.iceberg.IcebergUtil.getColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.getDeleteMode;\n import static com.facebook.presto.iceberg.IcebergUtil.getFileFormat;\n+import static com.facebook.presto.iceberg.IcebergUtil.getMetadataPreviousVersionsMax;\n+import static com.facebook.presto.iceberg.IcebergUtil.getMetricsMaxInferredColumn;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitionFields;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitionKeyColumnHandles;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitionSpecsIncludingValidData;\n import static com.facebook.presto.iceberg.IcebergUtil.getPartitions;\n import static com.facebook.presto.iceberg.IcebergUtil.getSnapshotIdTimeOperator;\n import static com.facebook.presto.iceberg.IcebergUtil.getSortFields;\n+import static com.facebook.presto.iceberg.IcebergUtil.getSplitSize;\n import static com.facebook.presto.iceberg.IcebergUtil.getTableComment;\n+import static com.facebook.presto.iceberg.IcebergUtil.getUpdateMode;\n import static com.facebook.presto.iceberg.IcebergUtil.getViewComment;\n+import static com.facebook.presto.iceberg.IcebergUtil.isMetadataDeleteAfterCommit;\n+import static com.facebook.presto.iceberg.IcebergUtil.parseFormatVersion;\n import static com.facebook.presto.iceberg.IcebergUtil.resolveSnapshotIdByName;\n import static com.facebook.presto.iceberg.IcebergUtil.toHiveColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.tryGetLocation;\n@@ -188,6 +198,7 @@\n import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n import static com.facebook.presto.spi.StandardWarningCode.SORT_COLUMN_TRANSFORM_NOT_SUPPORTED_WARNING;\n import static com.facebook.presto.spi.statistics.TableStatisticType.ROW_COUNT;\n+import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n@@ -200,10 +211,17 @@\n import static org.apache.iceberg.SnapshotSummary.DELETED_RECORDS_PROP;\n import static org.apache.iceberg.SnapshotSummary.REMOVED_EQ_DELETES_PROP;\n import static org.apache.iceberg.SnapshotSummary.REMOVED_POS_DELETES_PROP;\n+import static org.apache.iceberg.TableProperties.COMMIT_NUM_RETRIES;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL;\n import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL_DEFAULT;\n+import static org.apache.iceberg.TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED;\n+import static org.apache.iceberg.TableProperties.METRICS_MAX_INFERRED_COLUMN_DEFAULTS;\n+import static org.apache.iceberg.TableProperties.ORC_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.PARQUET_COMPRESSION;\n import static org.apache.iceberg.TableProperties.SPLIT_SIZE;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE;\n+import static org.apache.iceberg.TableProperties.WRITE_DATA_LOCATION;\n \n public abstract class IcebergAbstractMetadata\n         implements ConnectorMetadata\n@@ -716,12 +734,17 @@ protected ImmutableMap<String, Object> createMetadataProperties(Table icebergTab\n             properties.put(LOCATION_PROPERTY, icebergTable.location());\n         }\n \n-        properties.put(DELETE_MODE, IcebergUtil.getDeleteMode(icebergTable));\n-        properties.put(UPDATE_MODE, IcebergUtil.getUpdateMode(icebergTable));\n-        properties.put(METADATA_PREVIOUS_VERSIONS_MAX, IcebergUtil.getMetadataPreviousVersionsMax(icebergTable));\n-        properties.put(METADATA_DELETE_AFTER_COMMIT, IcebergUtil.isMetadataDeleteAfterCommit(icebergTable));\n-        properties.put(METRICS_MAX_INFERRED_COLUMN, IcebergUtil.getMetricsMaxInferredColumn(icebergTable));\n-        properties.put(SPLIT_SIZE, IcebergUtil.getSplitSize(icebergTable));\n+        String writeDataLocation = icebergTable.properties().get(WRITE_DATA_LOCATION);\n+        if (!isNullOrEmpty(writeDataLocation)) {\n+            properties.put(WRITE_DATA_LOCATION, writeDataLocation);\n+        }\n+\n+        properties.put(DELETE_MODE, getDeleteMode(icebergTable));\n+        properties.put(UPDATE_MODE, getUpdateMode(icebergTable));\n+        properties.put(METADATA_PREVIOUS_VERSIONS_MAX, getMetadataPreviousVersionsMax(icebergTable));\n+        properties.put(METADATA_DELETE_AFTER_COMMIT, isMetadataDeleteAfterCommit(icebergTable));\n+        properties.put(METRICS_MAX_INFERRED_COLUMN, getMetricsMaxInferredColumn(icebergTable));\n+        properties.put(SPLIT_SIZE, getSplitSize(icebergTable));\n \n         SortOrder sortOrder = icebergTable.sortOrder();\n         // TODO: Support sort column transforms (https://github.com/prestodb/presto/issues/24250)\n@@ -1140,6 +1163,62 @@ public void setTableProperties(ConnectorSession session, ConnectorTableHandle ta\n         transaction.commitTransaction();\n     }\n \n+    protected Map<String, String> populateTableProperties(ConnectorTableMetadata tableMetadata, com.facebook.presto.iceberg.FileFormat fileFormat, ConnectorSession session)\n+    {\n+        ImmutableMap.Builder<String, String> propertiesBuilder = ImmutableMap.builderWithExpectedSize(16);\n+\n+        String writeDataLocation = getWriteDataLocation(tableMetadata.getProperties());\n+        if (!isNullOrEmpty(writeDataLocation)) {\n+            propertiesBuilder.put(WRITE_DATA_LOCATION, writeDataLocation);\n+        }\n+        else {\n+            Optional<String> dataLocation = getDataLocationBasedOnWarehouseDataDir(tableMetadata.getTable());\n+            dataLocation.ifPresent(location -> propertiesBuilder.put(WRITE_DATA_LOCATION, location));\n+        }\n+\n+        Integer commitRetries = getCommitRetries(tableMetadata.getProperties());\n+        propertiesBuilder.put(DEFAULT_FILE_FORMAT, fileFormat.toString());\n+        propertiesBuilder.put(COMMIT_NUM_RETRIES, String.valueOf(commitRetries));\n+        switch (fileFormat) {\n+            case PARQUET:\n+                propertiesBuilder.put(PARQUET_COMPRESSION, getCompressionCodec(session).getParquetCompressionCodec().get().toString());\n+                break;\n+            case ORC:\n+                propertiesBuilder.put(ORC_COMPRESSION, getCompressionCodec(session).getOrcCompressionKind().name());\n+                break;\n+        }\n+        if (tableMetadata.getComment().isPresent()) {\n+            propertiesBuilder.put(TABLE_COMMENT, tableMetadata.getComment().get());\n+        }\n+\n+        String formatVersion = getFormatVersion(tableMetadata.getProperties());\n+        verify(formatVersion != null, \"Format version cannot be null\");\n+        propertiesBuilder.put(TableProperties.FORMAT_VERSION, formatVersion);\n+\n+        if (parseFormatVersion(formatVersion) < MIN_FORMAT_VERSION_FOR_DELETE) {\n+            propertiesBuilder.put(TableProperties.DELETE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n+            propertiesBuilder.put(TableProperties.UPDATE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n+        }\n+        else {\n+            RowLevelOperationMode deleteMode = IcebergTableProperties.getDeleteMode(tableMetadata.getProperties());\n+            propertiesBuilder.put(TableProperties.DELETE_MODE, deleteMode.modeName());\n+            RowLevelOperationMode updateMode = IcebergTableProperties.getUpdateMode(tableMetadata.getProperties());\n+            propertiesBuilder.put(TableProperties.UPDATE_MODE, updateMode.modeName());\n+        }\n+\n+        Integer metadataPreviousVersionsMax = IcebergTableProperties.getMetadataPreviousVersionsMax(tableMetadata.getProperties());\n+        propertiesBuilder.put(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, String.valueOf(metadataPreviousVersionsMax));\n+\n+        Boolean metadataDeleteAfterCommit = IcebergTableProperties.isMetadataDeleteAfterCommit(tableMetadata.getProperties());\n+        propertiesBuilder.put(METADATA_DELETE_AFTER_COMMIT_ENABLED, String.valueOf(metadataDeleteAfterCommit));\n+\n+        Integer metricsMaxInferredColumn = IcebergTableProperties.getMetricsMaxInferredColumn(tableMetadata.getProperties());\n+        propertiesBuilder.put(METRICS_MAX_INFERRED_COLUMN_DEFAULTS, String.valueOf(metricsMaxInferredColumn));\n+\n+        propertiesBuilder.put(SPLIT_SIZE, String.valueOf(IcebergTableProperties.getTargetSplitSize(tableMetadata.getProperties())));\n+        return propertiesBuilder.build();\n+    }\n+\n     /**\n      * Deletes all the files for a specific predicate\n      *\n@@ -1265,4 +1344,9 @@ public void finishUpdate(ConnectorSession session, ConnectorTableHandle tableHan\n                 handle.getSortOrder());\n         finishWrite(session, outputTableHandle, fragments, UPDATE_AFTER);\n     }\n+\n+    protected Optional<String> getDataLocationBasedOnWarehouseDataDir(SchemaTableName schemaTableName)\n+    {\n+        return Optional.empty();\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\nindex 6ac0fc4622b24..9f65aebba0e3a 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\n@@ -99,6 +99,8 @@\n import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n import static com.facebook.airlift.json.JsonCodecBinder.jsonCodecBinder;\n+import static com.facebook.presto.common.Utils.checkArgument;\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n import static com.facebook.presto.orc.StripeMetadataSource.CacheableRowGroupIndices;\n import static com.facebook.presto.orc.StripeMetadataSource.CacheableSlice;\n import static com.google.common.util.concurrent.MoreExecutors.newDirectExecutorService;\n@@ -142,6 +144,9 @@ protected void setup(Binder binder)\n \n         configBinder(binder).bindConfig(IcebergConfig.class);\n \n+        IcebergConfig icebergConfig = buildConfigObject(IcebergConfig.class);\n+        checkArgument(icebergConfig.getCatalogType().equals(HADOOP) || icebergConfig.getCatalogWarehouseDataDir() == null, \"'iceberg.catalog.hadoop.warehouse.datadir' can only be specified in Hadoop catalog\");\n+\n         binder.bind(IcebergSessionProperties.class).in(Scopes.SINGLETON);\n         newOptionalBinder(binder, IcebergNessieConfig.class);  // bind optional Nessie config to IcebergSessionProperties\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java\nindex 42328ddb4e7f4..d1bc37ff0e0be 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java\n@@ -50,6 +50,7 @@ public class IcebergConfig\n     private HiveCompressionCodec compressionCodec = GZIP;\n     private CatalogType catalogType = HIVE;\n     private String catalogWarehouse;\n+    private String catalogWarehouseDataDir;\n     private int catalogCacheSize = 10;\n     private int maxPartitionsPerWriter = 100;\n     private List<String> hadoopConfigResources = ImmutableList.of();\n@@ -127,6 +128,19 @@ public IcebergConfig setCatalogWarehouse(String catalogWarehouse)\n         return this;\n     }\n \n+    public String getCatalogWarehouseDataDir()\n+    {\n+        return catalogWarehouseDataDir;\n+    }\n+\n+    @Config(\"iceberg.catalog.hadoop.warehouse.datadir\")\n+    @ConfigDescription(\"Iceberg catalog default root data writing directory. This is only supported with Hadoop catalog.\")\n+    public IcebergConfig setCatalogWarehouseDataDir(String catalogWarehouseDataDir)\n+    {\n+        this.catalogWarehouseDataDir = catalogWarehouseDataDir;\n+        return this;\n+    }\n+\n     @Min(1)\n     public int getCatalogCacheSize()\n     {\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\nindex 5db5a45c24e85..e7b9f3858fc3f 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n@@ -123,7 +123,6 @@\n import static com.facebook.presto.iceberg.IcebergUtil.getColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.getHiveIcebergTable;\n import static com.facebook.presto.iceberg.IcebergUtil.isIcebergTable;\n-import static com.facebook.presto.iceberg.IcebergUtil.populateTableProperties;\n import static com.facebook.presto.iceberg.IcebergUtil.toHiveColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.tryGetProperties;\n import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeCatalogFactory.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeCatalogFactory.java\nindex 4d23a97a3ff71..5a24e94f1d8ea 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeCatalogFactory.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeCatalogFactory.java\n@@ -52,6 +52,7 @@ public class IcebergNativeCatalogFactory\n     private final String catalogName;\n     protected final CatalogType catalogType;\n     private final String catalogWarehouse;\n+    private final String catalogWarehouseDataDir;\n     protected final IcebergConfig icebergConfig;\n \n     private final List<String> hadoopConfigResources;\n@@ -69,6 +70,7 @@ public IcebergNativeCatalogFactory(\n         this.icebergConfig = requireNonNull(config, \"config is null\");\n         this.catalogType = config.getCatalogType();\n         this.catalogWarehouse = config.getCatalogWarehouse();\n+        this.catalogWarehouseDataDir = config.getCatalogWarehouseDataDir();\n         this.hadoopConfigResources = icebergConfig.getHadoopConfigResources();\n         this.s3ConfigurationUpdater = requireNonNull(s3ConfigurationUpdater, \"s3ConfigurationUpdater is null\");\n         this.gcsConfigurationInitialize = requireNonNull(gcsConfigurationInitialize, \"gcsConfigurationInitialize is null\");\n@@ -90,6 +92,11 @@ public Catalog getCatalog(ConnectorSession session)\n         }\n     }\n \n+    public String getCatalogWarehouseDataDir()\n+    {\n+        return this.catalogWarehouseDataDir;\n+    }\n+\n     public SupportsNamespaces getNamespaces(ConnectorSession session)\n     {\n         Catalog catalog = getCatalog(session);\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\nindex dd37bed769683..b79c1d29e622e 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\n@@ -58,6 +58,7 @@\n import java.util.concurrent.ConcurrentMap;\n import java.util.stream.Stream;\n \n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_COMMIT_ERROR;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.getCompressionCodec;\n import static com.facebook.presto.iceberg.IcebergTableProperties.getFileFormat;\n@@ -70,7 +71,6 @@\n import static com.facebook.presto.iceberg.IcebergUtil.getColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergTable;\n import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergView;\n-import static com.facebook.presto.iceberg.IcebergUtil.populateTableProperties;\n import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n import static com.facebook.presto.iceberg.PartitionSpecConverter.toPrestoPartitionSpec;\n import static com.facebook.presto.iceberg.SchemaConverter.toPrestoSchema;\n@@ -95,6 +95,7 @@ public class IcebergNativeMetadata\n {\n     private static final String VIEW_DIALECT = \"presto\";\n \n+    private final Optional<String> warehouseDataDir;\n     private final IcebergNativeCatalogFactory catalogFactory;\n     private final CatalogType catalogType;\n     private final ConcurrentMap<SchemaTableName, View> icebergViews = new ConcurrentHashMap<>();\n@@ -113,6 +114,7 @@ public IcebergNativeMetadata(\n         super(typeManager, functionResolution, rowExpressionService, commitTaskCodec, nodeVersion, filterStatsCalculatorService, statisticsFileCache);\n         this.catalogFactory = requireNonNull(catalogFactory, \"catalogFactory is null\");\n         this.catalogType = requireNonNull(catalogType, \"catalogType is null\");\n+        this.warehouseDataDir = Optional.ofNullable(catalogFactory.getCatalogWarehouseDataDir());\n     }\n \n     @Override\n@@ -320,20 +322,21 @@ public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, Con\n         try {\n             TableIdentifier tableIdentifier = toIcebergTableIdentifier(schemaTableName, catalogFactory.isNestedNamespaceEnabled());\n             String targetPath = getTableLocation(tableMetadata.getProperties());\n+            Map<String, String> tableProperties = populateTableProperties(tableMetadata, fileFormat, session);\n             if (!isNullOrEmpty(targetPath)) {\n                 transaction = catalogFactory.getCatalog(session).newCreateTableTransaction(\n                         tableIdentifier,\n                         schema,\n                         partitionSpec,\n                         targetPath,\n-                        populateTableProperties(tableMetadata, fileFormat, session));\n+                        tableProperties);\n             }\n             else {\n                 transaction = catalogFactory.getCatalog(session).newCreateTableTransaction(\n                         tableIdentifier,\n                         schema,\n                         partitionSpec,\n-                        populateTableProperties(tableMetadata, fileFormat, session));\n+                        tableProperties);\n             }\n         }\n         catch (AlreadyExistsException e) {\n@@ -403,4 +406,12 @@ public void unregisterTable(ConnectorSession clientSession, SchemaTableName sche\n     {\n         catalogFactory.getCatalog(clientSession).dropTable(toIcebergTableIdentifier(schemaTableName, catalogFactory.isNestedNamespaceEnabled()), false);\n     }\n+\n+    protected Optional<String> getDataLocationBasedOnWarehouseDataDir(SchemaTableName schemaTableName)\n+    {\n+        if (!catalogType.equals(HADOOP)) {\n+            return Optional.empty();\n+        }\n+        return warehouseDataDir.map(base -> base + schemaTableName.getSchemaName() + \"/\" + schemaTableName.getTableName());\n+    }\n }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\nindex 1bb8976176e86..945fd525d3d29 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergTableProperties.java\n@@ -34,6 +34,7 @@\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static java.util.Locale.ENGLISH;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE;\n+import static org.apache.iceberg.TableProperties.WRITE_DATA_LOCATION;\n \n public class IcebergTableProperties\n {\n@@ -92,6 +93,11 @@ public IcebergTableProperties(IcebergConfig icebergConfig)\n                         false,\n                         value -> (List<?>) value,\n                         value -> value))\n+                .add(stringProperty(\n+                        WRITE_DATA_LOCATION,\n+                        \"File system location URI for the table's data and delete files\",\n+                        null,\n+                        false))\n                 .add(stringProperty(\n                         FORMAT_VERSION,\n                         \"Format version for the table\",\n@@ -182,6 +188,11 @@ public static String getTableLocation(Map<String, Object> tableProperties)\n         return (String) tableProperties.get(LOCATION_PROPERTY);\n     }\n \n+    public static String getWriteDataLocation(Map<String, Object> tableProperties)\n+    {\n+        return (String) tableProperties.get(WRITE_DATA_LOCATION);\n+    }\n+\n     public static String getFormatVersion(Map<String, Object> tableProperties)\n     {\n         return (String) tableProperties.get(FORMAT_VERSION);\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\nindex 229b77d61f8f4..622aa3ff6a3a4 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n@@ -38,7 +38,6 @@\n import com.facebook.presto.spi.ColumnHandle;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.spi.ConnectorTableHandle;\n-import com.facebook.presto.spi.ConnectorTableMetadata;\n import com.facebook.presto.spi.Constraint;\n import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.SchemaTableName;\n@@ -69,7 +68,6 @@\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n-import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.TableScan;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.ViewCatalog;\n@@ -140,10 +138,7 @@\n import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_INVALID_TABLE_TIMESTAMP;\n import static com.facebook.presto.iceberg.IcebergMetadataColumn.isMetadataColumnId;\n import static com.facebook.presto.iceberg.IcebergPartitionType.IDENTITY;\n-import static com.facebook.presto.iceberg.IcebergSessionProperties.getCompressionCodec;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.isMergeOnReadModeEnabled;\n-import static com.facebook.presto.iceberg.IcebergTableProperties.getCommitRetries;\n-import static com.facebook.presto.iceberg.IcebergTableProperties.getFormatVersion;\n import static com.facebook.presto.iceberg.TypeConverter.toIcebergType;\n import static com.facebook.presto.iceberg.TypeConverter.toPrestoType;\n import static com.facebook.presto.iceberg.util.IcebergPrestoModelConverters.toIcebergTableIdentifier;\n@@ -183,12 +178,10 @@\n import static org.apache.iceberg.CatalogProperties.IO_MANIFEST_CACHE_MAX_TOTAL_BYTES;\n import static org.apache.iceberg.LocationProviders.locationsFor;\n import static org.apache.iceberg.MetadataTableUtils.createMetadataTableInstance;\n-import static org.apache.iceberg.TableProperties.COMMIT_NUM_RETRIES;\n import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n import static org.apache.iceberg.TableProperties.DELETE_MODE;\n import static org.apache.iceberg.TableProperties.DELETE_MODE_DEFAULT;\n-import static org.apache.iceberg.TableProperties.FORMAT_VERSION;\n import static org.apache.iceberg.TableProperties.MERGE_MODE;\n import static org.apache.iceberg.TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED;\n import static org.apache.iceberg.TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED_DEFAULT;\n@@ -196,13 +189,15 @@\n import static org.apache.iceberg.TableProperties.METADATA_PREVIOUS_VERSIONS_MAX_DEFAULT;\n import static org.apache.iceberg.TableProperties.METRICS_MAX_INFERRED_COLUMN_DEFAULTS;\n import static org.apache.iceberg.TableProperties.METRICS_MAX_INFERRED_COLUMN_DEFAULTS_DEFAULT;\n-import static org.apache.iceberg.TableProperties.ORC_COMPRESSION;\n-import static org.apache.iceberg.TableProperties.PARQUET_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.OBJECT_STORE_PATH;\n import static org.apache.iceberg.TableProperties.SPLIT_SIZE;\n import static org.apache.iceberg.TableProperties.SPLIT_SIZE_DEFAULT;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE;\n import static org.apache.iceberg.TableProperties.UPDATE_MODE_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_DATA_LOCATION;\n+import static org.apache.iceberg.TableProperties.WRITE_FOLDER_STORAGE_LOCATION;\n import static org.apache.iceberg.TableProperties.WRITE_LOCATION_PROVIDER_IMPL;\n+import static org.apache.iceberg.TableProperties.WRITE_METADATA_LOCATION;\n import static org.apache.iceberg.types.Type.TypeID.BINARY;\n import static org.apache.iceberg.types.Type.TypeID.FIXED;\n \n@@ -1140,53 +1135,6 @@ public void close()\n         }\n     }\n \n-    public static Map<String, String> populateTableProperties(ConnectorTableMetadata tableMetadata, FileFormat fileFormat, ConnectorSession session)\n-    {\n-        ImmutableMap.Builder<String, String> propertiesBuilder = ImmutableMap.builderWithExpectedSize(5);\n-        Integer commitRetries = getCommitRetries(tableMetadata.getProperties());\n-        propertiesBuilder.put(DEFAULT_FILE_FORMAT, fileFormat.toString());\n-        propertiesBuilder.put(COMMIT_NUM_RETRIES, String.valueOf(commitRetries));\n-        switch (fileFormat) {\n-            case PARQUET:\n-                propertiesBuilder.put(PARQUET_COMPRESSION, getCompressionCodec(session).getParquetCompressionCodec().get().toString());\n-                break;\n-            case ORC:\n-                propertiesBuilder.put(ORC_COMPRESSION, getCompressionCodec(session).getOrcCompressionKind().name());\n-                break;\n-        }\n-        if (tableMetadata.getComment().isPresent()) {\n-            propertiesBuilder.put(TABLE_COMMENT, tableMetadata.getComment().get());\n-        }\n-\n-        String formatVersion = getFormatVersion(tableMetadata.getProperties());\n-        verify(formatVersion != null, \"Format version cannot be null\");\n-        propertiesBuilder.put(FORMAT_VERSION, formatVersion);\n-\n-        if (parseFormatVersion(formatVersion) < MIN_FORMAT_VERSION_FOR_DELETE) {\n-            propertiesBuilder.put(DELETE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n-            propertiesBuilder.put(UPDATE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n-        }\n-        else {\n-            RowLevelOperationMode deleteMode = IcebergTableProperties.getDeleteMode(tableMetadata.getProperties());\n-            propertiesBuilder.put(DELETE_MODE, deleteMode.modeName());\n-            RowLevelOperationMode updateMode = IcebergTableProperties.getUpdateMode(tableMetadata.getProperties());\n-            propertiesBuilder.put(UPDATE_MODE, updateMode.modeName());\n-        }\n-\n-        Integer metadataPreviousVersionsMax = IcebergTableProperties.getMetadataPreviousVersionsMax(tableMetadata.getProperties());\n-        propertiesBuilder.put(METADATA_PREVIOUS_VERSIONS_MAX, String.valueOf(metadataPreviousVersionsMax));\n-\n-        Boolean metadataDeleteAfterCommit = IcebergTableProperties.isMetadataDeleteAfterCommit(tableMetadata.getProperties());\n-        propertiesBuilder.put(METADATA_DELETE_AFTER_COMMIT_ENABLED, String.valueOf(metadataDeleteAfterCommit));\n-\n-        Integer metricsMaxInferredColumn = IcebergTableProperties.getMetricsMaxInferredColumn(tableMetadata.getProperties());\n-        propertiesBuilder.put(METRICS_MAX_INFERRED_COLUMN_DEFAULTS, String.valueOf(metricsMaxInferredColumn));\n-\n-        propertiesBuilder.put(SPLIT_SIZE, String.valueOf(IcebergTableProperties.getTargetSplitSize(tableMetadata.getProperties())));\n-\n-        return propertiesBuilder.build();\n-    }\n-\n     public static int parseFormatVersion(String formatVersion)\n     {\n         try {\n@@ -1265,7 +1213,7 @@ public static Optional<PartitionData> partitionDataFromStructLike(PartitionSpec\n      */\n     public static String metadataLocation(Table icebergTable)\n     {\n-        String metadataLocation = icebergTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+        String metadataLocation = icebergTable.properties().get(WRITE_METADATA_LOCATION);\n \n         if (metadataLocation != null) {\n             return String.format(\"%s\", LocationUtil.stripTrailingSlash(metadataLocation));\n@@ -1282,11 +1230,11 @@ public static String metadataLocation(Table icebergTable)\n     public static String dataLocation(Table icebergTable)\n     {\n         Map<String, String> properties = icebergTable.properties();\n-        String dataLocation = properties.get(TableProperties.WRITE_DATA_LOCATION);\n+        String dataLocation = properties.get(WRITE_DATA_LOCATION);\n         if (dataLocation == null) {\n-            dataLocation = properties.get(TableProperties.OBJECT_STORE_PATH);\n+            dataLocation = properties.get(OBJECT_STORE_PATH);\n             if (dataLocation == null) {\n-                dataLocation = properties.get(TableProperties.WRITE_FOLDER_STORAGE_LOCATION);\n+                dataLocation = properties.get(WRITE_FOLDER_STORAGE_LOCATION);\n                 if (dataLocation == null) {\n                     dataLocation = String.format(\"%s/data\", icebergTable.location());\n                 }\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\nindex e950702b8fe64..9f872845c9514 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n@@ -23,13 +23,14 @@\n import com.facebook.presto.testing.QueryRunner;\n import com.facebook.presto.testing.assertions.Assert;\n import com.facebook.presto.tests.AbstractTestIntegrationSmokeTest;\n+import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.UpdateProperties;\n import org.intellij.lang.annotations.Language;\n import org.testng.annotations.DataProvider;\n import org.testng.annotations.Test;\n \n-import java.nio.file.Path;\n+import java.io.IOException;\n import java.util.function.BiConsumer;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n@@ -47,6 +48,7 @@\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.collect.Iterables.getOnlyElement;\n import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n import static java.util.Locale.ENGLISH;\n import static java.util.Objects.requireNonNull;\n import static java.util.stream.Collectors.joining;\n@@ -152,6 +154,91 @@ public void testShowCreateTable()\n                         \")\", schemaName, getLocation(schemaName, \"orders\")));\n     }\n \n+    @Test\n+    public void testTableWithSpecifiedWriteDataLocation()\n+            throws IOException\n+    {\n+        String tableName = \"test_table_with_specified_write_data_location\";\n+        String dataWriteLocation = createTempDirectory(tableName).toAbsolutePath().toString();\n+        try {\n+            assertUpdate(format(\"create table %s(a int, b varchar) with (\\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            assertUpdate(format(\"insert into %s values(1, '1001'), (2, '1002'), (3, '1003')\", tableName), 3);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002'), (3, '1003')\");\n+            assertUpdate(format(\"delete from %s where a > 2\", tableName), 1);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002')\");\n+        }\n+        finally {\n+            try {\n+                getQueryRunner().execute(\"drop table if exists \" + tableName);\n+            }\n+            catch (Exception e) {\n+                // ignored for hive catalog compatibility\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionedTableWithSpecifiedWriteDataLocation()\n+            throws IOException\n+    {\n+        String tableName = \"test_partitioned_table_with_specified_write_data_location\";\n+        String dataWriteLocation = createTempDirectory(tableName).toAbsolutePath().toString();\n+        try {\n+            assertUpdate(format(\"create table %s(a int, b varchar) with (partitioning = ARRAY['a'], \\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            assertUpdate(format(\"insert into %s values(1, '1001'), (2, '1002'), (3, '1003')\", tableName), 3);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002'), (3, '1003')\");\n+            assertUpdate(format(\"delete from %s where a > 2\", tableName), 1);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002')\");\n+        }\n+        finally {\n+            try {\n+                getQueryRunner().execute(\"drop table if exists \" + tableName);\n+            }\n+            catch (Exception e) {\n+                // ignored for hive catalog compatibility\n+            }\n+        }\n+    }\n+\n+    @Test\n+    public void testShowCreateTableWithSpecifiedWriteDataLocation()\n+            throws IOException\n+    {\n+        String tableName = \"test_show_table_with_specified_write_data_location\";\n+        String dataWriteLocation = createTempDirectory(\"test1\").toAbsolutePath().toString();\n+        try {\n+            assertUpdate(format(\"CREATE TABLE %s(a int, b varchar) with (\\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            String schemaName = getSession().getSchema().get();\n+            String location = getLocation(schemaName, tableName);\n+            String createTableSql = \"CREATE TABLE iceberg.%s.%s (\\n\" +\n+                    \"   \\\"a\\\" integer,\\n\" +\n+                    \"   \\\"b\\\" varchar\\n\" +\n+                    \")\\n\" +\n+                    \"WITH (\\n\" +\n+                    \"   delete_mode = 'merge-on-read',\\n\" +\n+                    \"   format = 'PARQUET',\\n\" +\n+                    \"   format_version = '2',\\n\" +\n+                    \"   location = '%s',\\n\" +\n+                    \"   metadata_delete_after_commit = false,\\n\" +\n+                    \"   metadata_previous_versions_max = 100,\\n\" +\n+                    \"   metrics_max_inferred_column = 100,\\n\" +\n+                    \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                    \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                    \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                    \")\";\n+            assertThat(computeActual(\"SHOW CREATE TABLE \" + tableName).getOnlyValue())\n+                    .isEqualTo(format(createTableSql, schemaName, tableName, location, dataWriteLocation));\n+        }\n+        finally {\n+            try {\n+                getQueryRunner().execute(\"DROP TABLE IF EXISTS \" + tableName);\n+            }\n+            catch (Exception e) {\n+                // ignored for hive catalog compatibility\n+            }\n+        }\n+    }\n+\n     @Test\n     public void testDecimal()\n     {\n@@ -714,7 +801,7 @@ private void testSchemaEvolution(Session session, FileFormat fileFormat)\n     }\n \n     @Test\n-    private void testCreateTableLike()\n+    protected void testCreateTableLike()\n     {\n         Session session = getSession();\n         String schemaName = session.getSchema().get();\n@@ -883,7 +970,7 @@ private void testWithAllFormatVersions(BiConsumer<String, String> test)\n         test.accept(\"2\", \"merge-on-read\");\n     }\n \n-    private String getTablePropertiesString(String tableName)\n+    protected String getTablePropertiesString(String tableName)\n     {\n         MaterializedResult showCreateTable = computeActual(\"SHOW CREATE TABLE \" + tableName);\n         String createTable = (String) getOnlyElement(showCreateTable.getOnlyColumnAsSet());\n@@ -1216,8 +1303,8 @@ protected String getLocation(String schema, String table)\n \n     protected Path getCatalogDirectory()\n     {\n-        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n-        return getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false);\n+        java.nio.file.Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        return new Path(getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false).toFile().toURI());\n     }\n \n     protected Table getIcebergTable(ConnectorSession session, String namespace, String tableName)\n@@ -1538,7 +1625,7 @@ public void testRegisterTableWithInvalidLocation()\n         assertUpdate(\"CREATE TABLE \" + tableName + \" (id integer, value integer)\");\n         assertUpdate(\"INSERT INTO \" + tableName + \" VALUES(1, 1)\", 1);\n \n-        String metadataLocation = getLocation(schemaName, tableName).replace(\"//\", \"/\") + \"_invalid\";\n+        String metadataLocation = getLocation(schemaName, tableName).replace(\"//\", \"\") + \"_invalid\";\n \n         @Language(\"RegExp\") String errorMessage = format(\"Unable to find metadata at location %s/%s\", metadataLocation, METADATA_FOLDER_NAME);\n         assertQueryFails(\"CALL system.register_table ('\" + schemaName + \"', '\" + tableName + \"', '\" + metadataLocation + \"')\", errorMessage);\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\nindex 89a4d678808d7..9de94a10f6889 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n@@ -32,6 +32,9 @@\n import com.facebook.presto.hive.HiveHdfsConfiguration;\n import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.authentication.NoHdfsAuthentication;\n+import com.facebook.presto.hive.s3.HiveS3Config;\n+import com.facebook.presto.hive.s3.PrestoS3ConfigurationUpdater;\n+import com.facebook.presto.hive.s3.S3ConfigurationUpdater;\n import com.facebook.presto.iceberg.delete.DeleteFile;\n import com.facebook.presto.metadata.CatalogMetadata;\n import com.facebook.presto.metadata.Metadata;\n@@ -63,6 +66,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.BaseTable;\n import org.apache.iceberg.CatalogUtil;\n import org.apache.iceberg.FileScanTask;\n@@ -102,7 +106,6 @@\n import java.lang.reflect.Field;\n import java.net.URI;\n import java.nio.ByteBuffer;\n-import java.nio.file.Path;\n import java.time.LocalDateTime;\n import java.time.LocalTime;\n import java.time.format.DateTimeFormatter;\n@@ -156,8 +159,8 @@\n import static com.facebook.presto.tests.sql.TestTable.randomTableSuffix;\n import static com.facebook.presto.type.DecimalParametricType.DECIMAL;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n-import static com.google.common.io.Files.createTempDir;\n import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n import static java.util.Objects.requireNonNull;\n import static java.util.UUID.randomUUID;\n import static java.util.function.Function.identity;\n@@ -613,9 +616,10 @@ private void testPartitionedByTimestampTypeForFormat(Session session, FileFormat\n \n     @Test\n     public void testCreateTableWithCustomLocation()\n+            throws IOException\n     {\n         String tableName = \"test_table_with_custom_location\";\n-        URI tableTargetURI = createTempDir().toURI();\n+        URI tableTargetURI = createTempDirectory(tableName).toUri();\n         try {\n             assertQuerySucceeds(format(\"create table %s (a int, b varchar)\" +\n                     \" with (location = '%s')\", tableName, tableTargetURI.toString()));\n@@ -1495,12 +1499,13 @@ public void testWithoutSortOrder()\n     public boolean isFileSorted(String path, String sortColumnName, String sortOrder)\n             throws IOException\n     {\n-        Configuration configuration = new Configuration();\n-        try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new org.apache.hadoop.fs.Path(path))\n+        Path filePath = new Path(path);\n+        Configuration configuration = getHdfsEnvironment().getConfiguration(new HdfsContext(SESSION), filePath);\n+        try (ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(path))\n                 .withConf(configuration)\n                 .build()) {\n             Group record;\n-            ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, new org.apache.hadoop.fs.Path(path));\n+            ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, filePath);\n             MessageType schema = readFooter.getFileMetaData().getSchema();\n             Double previousValue = null;\n             while ((record = reader.read()) != null) {\n@@ -1758,14 +1763,14 @@ public void testMetadataVersionsMaintainingProperties()\n             // Table `test_table_with_default_setting_properties`'s current metadata record all 5 previous metadata files\n             assertEquals(defaultTableMetadata.previousFiles().size(), 5);\n \n-            FileSystem fileSystem = getHdfsEnvironment().getFileSystem(new HdfsContext(SESSION), new org.apache.hadoop.fs.Path(settingTable.location()));\n+            FileSystem fileSystem = getHdfsEnvironment().getFileSystem(new HdfsContext(SESSION), new Path(settingTable.location()));\n \n             // Table `test_table_with_setting_properties`'s all existing metadata files count is 2\n-            FileStatus[] settingTableFiles = fileSystem.listStatus(new org.apache.hadoop.fs.Path(settingTable.location(), \"metadata\"), name -> name.getName().contains(METADATA_FILE_EXTENSION));\n+            FileStatus[] settingTableFiles = fileSystem.listStatus(new Path(settingTable.location(), \"metadata\"), name -> name.getName().contains(METADATA_FILE_EXTENSION));\n             assertEquals(settingTableFiles.length, 2);\n \n             // Table `test_table_with_default_setting_properties`'s all existing metadata files count is 6\n-            FileStatus[] defaultTableFiles = fileSystem.listStatus(new org.apache.hadoop.fs.Path(defaultTable.location(), \"metadata\"), name -> name.getName().contains(METADATA_FILE_EXTENSION));\n+            FileStatus[] defaultTableFiles = fileSystem.listStatus(new Path(defaultTable.location(), \"metadata\"), name -> name.getName().contains(METADATA_FILE_EXTENSION));\n             assertEquals(defaultTableFiles.length, 6);\n         }\n         finally {\n@@ -2431,12 +2436,12 @@ private void testCheckDeleteFiles(Table icebergTable, int expectedSize, List<Fil\n     private void writePositionDeleteToNationTable(Table icebergTable, String dataFilePath, long deletePos)\n             throws IOException\n     {\n-        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        java.nio.file.Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n         File metastoreDir = getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false).toFile();\n-        org.apache.hadoop.fs.Path metadataDir = new org.apache.hadoop.fs.Path(metastoreDir.toURI());\n+        Path metadataDir = new Path(metastoreDir.toURI());\n         String deleteFileName = \"delete_file_\" + randomUUID();\n         FileSystem fs = getHdfsEnvironment().getFileSystem(new HdfsContext(SESSION), metadataDir);\n-        org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(metadataDir, deleteFileName);\n+        Path path = new Path(metadataDir, deleteFileName);\n         PositionDeleteWriter<Record> writer = Parquet.writeDeletes(HadoopOutputFile.fromPath(path, fs))\n                 .createWriterFunc(GenericParquetWriter::buildWriter)\n                 .forTable(icebergTable)\n@@ -2463,13 +2468,13 @@ private void writeEqualityDeleteToNationTable(Table icebergTable, Map<String, Ob\n     private void writeEqualityDeleteToNationTable(Table icebergTable, Map<String, Object> overwriteValues, Map<String, Object> partitionValues)\n             throws Exception\n     {\n-        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        java.nio.file.Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n         File metastoreDir = getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false).toFile();\n-        org.apache.hadoop.fs.Path metadataDir = new org.apache.hadoop.fs.Path(metastoreDir.toURI());\n+        Path metadataDir = new Path(metastoreDir.toURI());\n         String deleteFileName = \"delete_file_\" + randomUUID();\n         FileSystem fs = getHdfsEnvironment().getFileSystem(new HdfsContext(SESSION), metadataDir);\n         Schema deleteRowSchema = icebergTable.schema().select(overwriteValues.keySet());\n-        Parquet.DeleteWriteBuilder writerBuilder = Parquet.writeDeletes(HadoopOutputFile.fromPath(new org.apache.hadoop.fs.Path(metadataDir, deleteFileName), fs))\n+        Parquet.DeleteWriteBuilder writerBuilder = Parquet.writeDeletes(HadoopOutputFile.fromPath(new Path(metadataDir, deleteFileName), fs))\n                 .forTable(icebergTable)\n                 .rowSchema(deleteRowSchema)\n                 .createWriterFunc(GenericParquetWriter::buildWriter)\n@@ -2490,13 +2495,19 @@ private void writeEqualityDeleteToNationTable(Table icebergTable, Map<String, Ob\n         icebergTable.newRowDelta().addDeletes(writer.toDeleteFile()).commit();\n     }\n \n-    public static HdfsEnvironment getHdfsEnvironment()\n+    protected HdfsEnvironment getHdfsEnvironment()\n     {\n         HiveClientConfig hiveClientConfig = new HiveClientConfig();\n         MetastoreClientConfig metastoreClientConfig = new MetastoreClientConfig();\n-        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hiveClientConfig, metastoreClientConfig),\n-                ImmutableSet.of(),\n-                hiveClientConfig);\n+        HiveS3Config hiveS3Config = new HiveS3Config();\n+        return getHdfsEnvironment(hiveClientConfig, metastoreClientConfig, hiveS3Config);\n+    }\n+\n+    public static HdfsEnvironment getHdfsEnvironment(HiveClientConfig hiveClientConfig, MetastoreClientConfig metastoreClientConfig, HiveS3Config hiveS3Config)\n+    {\n+        S3ConfigurationUpdater s3ConfigurationUpdater = new PrestoS3ConfigurationUpdater(hiveS3Config);\n+        HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hiveClientConfig, metastoreClientConfig, s3ConfigurationUpdater, ignored -> {}),\n+                ImmutableSet.of(), hiveClientConfig);\n         return new HdfsEnvironment(hdfsConfiguration, metastoreClientConfig, new NoHdfsAuthentication());\n     }\n \n@@ -2518,18 +2529,18 @@ protected Table loadTable(String tableName)\n \n     protected Map<String, String> getProperties()\n     {\n-        File metastoreDir = getCatalogDirectory();\n+        Path metastoreDir = getCatalogDirectory();\n         return ImmutableMap.of(\"warehouse\", metastoreDir.toString());\n     }\n \n-    protected File getCatalogDirectory()\n+    protected Path getCatalogDirectory()\n     {\n-        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        java.nio.file.Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n         switch (catalogType) {\n             case HIVE:\n             case HADOOP:\n             case NESSIE:\n-                return getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false).toFile();\n+                return new Path(getIcebergDataDirectoryPath(dataDirectory, catalogType.name(), new IcebergConfig().getFileFormat(), false).toFile().toURI());\n         }\n \n         throw new PrestoException(NOT_SUPPORTED, \"Unsupported Presto Iceberg catalog type \" + catalogType);\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\nindex 6d71f2dcc8d07..217522ea721a6 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergQueryRunner.java\n@@ -232,15 +232,14 @@ public IcebergQueryRunner build()\n \n             Path icebergDataDirectory = getIcebergDataDirectoryPath(queryRunner.getCoordinator().getDataDirectory(), catalogType.name(), format, addStorageFormatToPath);\n \n-            Map<String, String> icebergProperties = ImmutableMap.<String, String>builder()\n-                    .put(\"iceberg.file-format\", format.name())\n-                    .put(\"iceberg.catalog.type\", catalogType.name())\n-                    .putAll(getConnectorProperties(catalogType, icebergDataDirectory))\n-                    .putAll(extraConnectorProperties)\n-                    .build();\n-\n-            queryRunner.createCatalog(ICEBERG_CATALOG, \"iceberg\", icebergProperties);\n-            icebergCatalogs.put(ICEBERG_CATALOG, icebergProperties);\n+            Map<String, String> icebergProperties = new HashMap<>();\n+            icebergProperties.put(\"iceberg.file-format\", format.name());\n+            icebergProperties.put(\"iceberg.catalog.type\", catalogType.name());\n+            icebergProperties.putAll(getConnectorProperties(catalogType, icebergDataDirectory));\n+            icebergProperties.putAll(extraConnectorProperties);\n+\n+            queryRunner.createCatalog(ICEBERG_CATALOG, \"iceberg\", ImmutableMap.copyOf(icebergProperties));\n+            icebergCatalogs.put(ICEBERG_CATALOG, ImmutableMap.copyOf(icebergProperties));\n \n             if (addJmxPlugin) {\n                 queryRunner.createCatalog(\"jmx\", \"jmx\");\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java\nindex 588b7273d44c5..bc66bae273792 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConfig.java\n@@ -49,6 +49,7 @@ public void testDefaults()\n                 .setCompressionCodec(GZIP)\n                 .setCatalogType(HIVE)\n                 .setCatalogWarehouse(null)\n+                .setCatalogWarehouseDataDir(null)\n                 .setCatalogCacheSize(10)\n                 .setHadoopConfigResources(null)\n                 .setHiveStatisticsMergeFlags(\"\")\n@@ -81,6 +82,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"iceberg.compression-codec\", \"NONE\")\n                 .put(\"iceberg.catalog.type\", \"HADOOP\")\n                 .put(\"iceberg.catalog.warehouse\", \"path\")\n+                .put(\"iceberg.catalog.hadoop.warehouse.datadir\", \"path_data_dir\")\n                 .put(\"iceberg.catalog.cached-catalog-num\", \"6\")\n                 .put(\"iceberg.hadoop.config.resources\", \"/etc/hadoop/conf/core-site.xml\")\n                 .put(\"iceberg.max-partitions-per-writer\", \"222\")\n@@ -110,6 +112,7 @@ public void testExplicitPropertyMappings()\n                 .setCompressionCodec(NONE)\n                 .setCatalogType(HADOOP)\n                 .setCatalogWarehouse(\"path\")\n+                .setCatalogWarehouseDataDir(\"path_data_dir\")\n                 .setCatalogCacheSize(6)\n                 .setHadoopConfigResources(\"/etc/hadoop/conf/core-site.xml\")\n                 .setMaxPartitionsPerWriter(222)\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java\nindex c9b84e4ee2904..014cb8a20b2c6 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergFileWriter.java\n@@ -25,10 +25,13 @@\n import com.facebook.presto.hive.FileFormatDataSourceStats;\n import com.facebook.presto.hive.HdfsContext;\n import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n import com.facebook.presto.hive.HiveCompressionCodec;\n import com.facebook.presto.hive.HiveDwrfEncryptionProvider;\n+import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.NodeVersion;\n import com.facebook.presto.hive.OrcFileWriterConfig;\n+import com.facebook.presto.hive.s3.HiveS3Config;\n import com.facebook.presto.metadata.SessionPropertyManager;\n import com.facebook.presto.parquet.FileParquetDataSource;\n import com.facebook.presto.parquet.cache.MetadataReader;\n@@ -61,6 +64,7 @@\n import static com.facebook.presto.common.type.VarbinaryType.VARBINARY;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.iceberg.IcebergAbstractMetadata.toIcebergSchema;\n+import static com.facebook.presto.iceberg.IcebergDistributedTestBase.getHdfsEnvironment;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.ICEBERG_CATALOG;\n import static com.facebook.presto.iceberg.IcebergSessionProperties.dataSizeSessionProperty;\n import static com.facebook.presto.metadata.SessionPropertyManager.createTestingSessionPropertyManager;\n@@ -114,7 +118,7 @@ public void setup()\n         this.connectorSession = session.toConnectorSession(connectorId);\n         TypeManager typeManager = new TestingTypeManager();\n         this.hdfsContext = new HdfsContext(connectorSession);\n-        HdfsEnvironment hdfsEnvironment = IcebergDistributedTestBase.getHdfsEnvironment();\n+        HdfsEnvironment hdfsEnvironment = getHdfsEnvironment(new HiveClientConfig(), new MetastoreClientConfig(), new HiveS3Config());\n         this.icebergFileWriterFactory = new IcebergFileWriterFactory(hdfsEnvironment, typeManager,\n                 new FileFormatDataSourceStats(), new NodeVersion(\"test\"), new OrcFileWriterConfig(), HiveDwrfEncryptionProvider.NO_ENCRYPTION);\n     }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\nindex 92016c4e263f9..56195902ca8bf 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTables.java\n@@ -255,15 +255,25 @@ public void testSessionPropertiesInManuallyStartedTransaction()\n         }\n     }\n \n+    protected void checkTableProperties(String schemaName, String tableName, String deleteMode, String dataWriteLocation)\n+    {\n+        checkTableProperties(schemaName, tableName, deleteMode, 10, ImmutableMap.of(\"write.data.path\", dataWriteLocation));\n+    }\n+\n     protected void checkTableProperties(String tableName, String deleteMode)\n     {\n-        assertQuery(String.format(\"SHOW COLUMNS FROM test_schema.\\\"%s$properties\\\"\", tableName),\n+        checkTableProperties(\"test_schema\", tableName, deleteMode, 9, ImmutableMap.of());\n+    }\n+\n+    protected void checkTableProperties(String schemaName, String tableName, String deleteMode, int propertiesCount, Map<String, String> additionalValidateProperties)\n+    {\n+        assertQuery(String.format(\"SHOW COLUMNS FROM %s.\\\"%s$properties\\\"\", schemaName, tableName),\n                 \"VALUES ('key', 'varchar', '', ''),\" + \"('value', 'varchar', '', '')\");\n-        assertQuery(String.format(\"SELECT COUNT(*) FROM test_schema.\\\"%s$properties\\\"\", tableName), \"VALUES 9\");\n+        assertQuery(String.format(\"SELECT COUNT(*) FROM %s.\\\"%s$properties\\\"\", schemaName, tableName), \"VALUES \" + propertiesCount);\n         List<MaterializedRow> materializedRows = computeActual(getSession(),\n-                String.format(\"SELECT * FROM test_schema.\\\"%s$properties\\\"\", tableName)).getMaterializedRows();\n+                String.format(\"SELECT * FROM %s.\\\"%s$properties\\\"\", schemaName, tableName)).getMaterializedRows();\n \n-        assertThat(materializedRows).hasSize(9);\n+        assertThat(materializedRows).hasSize(propertiesCount);\n         assertThat(materializedRows)\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.delete.mode\", deleteMode)))\n@@ -283,6 +293,11 @@ protected void checkTableProperties(String tableName, String deleteMode)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, \"write.metadata.metrics.max-inferred-column-defaults\", \"100\")))\n                 .anySatisfy(row -> assertThat(row)\n                         .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, IcebergTableProperties.TARGET_SPLIT_SIZE, Long.toString(DataSize.valueOf(\"128MB\").toBytes()))));\n+\n+        additionalValidateProperties.entrySet().stream()\n+                .forEach(entry -> assertThat(materializedRows)\n+                        .anySatisfy(row -> assertThat(row)\n+                                .isEqualTo(new MaterializedRow(MaterializedResult.DEFAULT_PRECISION, entry.getKey(), entry.getValue()))));\n     }\n \n     protected void checkORCFormatTableProperties(String tableName, String deleteMode)\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTablesHadoop.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTablesHadoop.java\nnew file mode 100644\nindex 0000000000000..3e252994399cc\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergSystemTablesHadoop.java\n@@ -0,0 +1,51 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.testing.QueryRunner;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n+\n+public class TestIcebergSystemTablesHadoop\n+        extends TestIcebergSystemTables\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return IcebergQueryRunner.builder()\n+                .setCatalogType(HADOOP)\n+                .build().getQueryRunner();\n+    }\n+\n+    @Test\n+    public void testPropertiesTableWithSpecifiedDataWriteLocation()\n+            throws IOException\n+    {\n+        String dataLocation = Files.createTempDirectory(\"test_table_with_write_data_location\").toAbsolutePath().toString();\n+        assertUpdate(\"CREATE SCHEMA test_schema_temp\");\n+        try {\n+            assertUpdate(String.format(\"CREATE TABLE test_schema_temp.test_table_with_write_data_location (_bigint BIGINT, _date DATE) WITH (partitioning = ARRAY['_date'], \\\"write.data.path\\\" = '%s')\", dataLocation));\n+            checkTableProperties(\"test_schema_temp\", \"test_table_with_write_data_location\", \"merge-on-read\", dataLocation);\n+        }\n+        finally {\n+            assertUpdate(\"DROP TABLE test_schema_temp.test_table_with_write_data_location\");\n+            assertUpdate(\"DROP SCHEMA test_schema_temp\");\n+        }\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/container/IcebergMinIODataLake.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/container/IcebergMinIODataLake.java\nnew file mode 100644\nindex 0000000000000..ff626fb4e1ea9\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/container/IcebergMinIODataLake.java\n@@ -0,0 +1,115 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.container;\n+\n+import com.amazonaws.auth.AWSStaticCredentialsProvider;\n+import com.amazonaws.auth.BasicAWSCredentials;\n+import com.amazonaws.client.builder.AwsClientBuilder;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n+import com.facebook.presto.testing.containers.MinIOContainer;\n+import com.facebook.presto.util.AutoCloseableCloser;\n+import com.google.common.collect.ImmutableMap;\n+import org.testcontainers.containers.Network;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.util.Objects.requireNonNull;\n+import static org.testcontainers.containers.Network.newNetwork;\n+\n+public class IcebergMinIODataLake\n+        implements Closeable\n+{\n+    public static final String ACCESS_KEY = \"minioadmin\";\n+    public static final String SECRET_KEY = \"minioadmin\";\n+\n+    private final String bucketName;\n+    private final String warehouseDir;\n+    private final MinIOContainer minIOContainer;\n+\n+    private final AtomicBoolean isStarted = new AtomicBoolean(false);\n+    private final AutoCloseableCloser closer = AutoCloseableCloser.create();\n+\n+    public IcebergMinIODataLake(String bucketName, String warehouseDir)\n+    {\n+        this.bucketName = requireNonNull(bucketName, \"bucketName is null\");\n+        this.warehouseDir = requireNonNull(warehouseDir, \"warehouseDir is null\");\n+        Network network = closer.register(newNetwork());\n+        this.minIOContainer = closer.register(\n+                MinIOContainer.builder()\n+                        .withNetwork(network)\n+                        .withEnvVars(ImmutableMap.<String, String>builder()\n+                                .put(\"MINIO_ACCESS_KEY\", ACCESS_KEY)\n+                                .put(\"MINIO_SECRET_KEY\", SECRET_KEY)\n+                                .build())\n+                        .build());\n+    }\n+\n+    public void start()\n+    {\n+        if (isStarted()) {\n+            return;\n+        }\n+        try {\n+            this.minIOContainer.start();\n+            AmazonS3 s3Client = AmazonS3ClientBuilder\n+                    .standard()\n+                    .withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(\n+                            \"http://localhost:\" + minIOContainer.getMinioApiEndpoint().getPort(),\n+                            \"us-east-1\"))\n+                    .withPathStyleAccessEnabled(true)\n+                    .withCredentials(new AWSStaticCredentialsProvider(\n+                            new BasicAWSCredentials(ACCESS_KEY, SECRET_KEY)))\n+                    .build();\n+            s3Client.createBucket(this.bucketName);\n+            s3Client.putObject(this.bucketName, this.warehouseDir, \"\");\n+        }\n+        finally {\n+            isStarted.set(true);\n+        }\n+    }\n+\n+    public boolean isStarted()\n+    {\n+        return isStarted.get();\n+    }\n+\n+    public void stop()\n+    {\n+        if (!isStarted()) {\n+            return;\n+        }\n+        try {\n+            closer.close();\n+            isStarted.set(false);\n+        }\n+        catch (Exception e) {\n+            throw new RuntimeException(\"Failed to stop IcebergMinioDataLake\", e);\n+        }\n+    }\n+\n+    public MinIOContainer getMinio()\n+    {\n+        return minIOContainer;\n+    }\n+\n+    @Override\n+    public void close()\n+            throws IOException\n+    {\n+        stop();\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java\nindex c08a2799514de..a5efac992adae 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedHadoop.java\n@@ -16,11 +16,12 @@\n import com.facebook.presto.iceberg.IcebergDistributedTestBase;\n import org.testng.annotations.Test;\n \n+import java.io.IOException;\n import java.net.URI;\n \n import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n-import static com.google.common.io.Files.createTempDir;\n import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n \n @Test\n public class TestIcebergDistributedHadoop\n@@ -33,9 +34,10 @@ public TestIcebergDistributedHadoop()\n \n     @Override\n     public void testCreateTableWithCustomLocation()\n+            throws IOException\n     {\n         String tableName = \"test_hadoop_table_with_custom_location\";\n-        URI tableTargetURI = createTempDir().toURI();\n+        URI tableTargetURI = createTempDirectory(tableName).toUri();\n         assertQueryFails(format(\"create table %s (a int, b varchar)\" + \" with (location = '%s')\", tableName, tableTargetURI.toString()),\n                 \"Cannot set a custom location for a path-based table.*\");\n     }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedOnS3Hadoop.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedOnS3Hadoop.java\nnew file mode 100644\nindex 0000000000000..5e1eda7e5afd3\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergDistributedOnS3Hadoop.java\n@@ -0,0 +1,136 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.hadoop;\n+\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.MetastoreClientConfig;\n+import com.facebook.presto.hive.s3.HiveS3Config;\n+import com.facebook.presto.iceberg.IcebergDistributedTestBase;\n+import com.facebook.presto.iceberg.IcebergQueryRunner;\n+import com.facebook.presto.iceberg.container.IcebergMinIODataLake;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.net.HostAndPort;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.ACCESS_KEY;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.SECRET_KEY;\n+import static com.facebook.presto.testing.TestingConnectorSession.SESSION;\n+import static com.facebook.presto.tests.sql.TestTable.randomTableSuffix;\n+import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n+\n+public class TestIcebergDistributedOnS3Hadoop\n+        extends IcebergDistributedTestBase\n+{\n+    static final String WAREHOUSE_DATA_DIR = \"warehouse_data/\";\n+    final String bucketName;\n+    final String catalogWarehouseDir;\n+    private IcebergMinIODataLake dockerizedS3DataLake;\n+    HostAndPort hostAndPort;\n+\n+    public TestIcebergDistributedOnS3Hadoop()\n+            throws IOException\n+    {\n+        super(HADOOP);\n+        bucketName = \"forhadoop-\" + randomTableSuffix();\n+        catalogWarehouseDir = createTempDirectory(bucketName).toUri().toString();\n+    }\n+\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return IcebergQueryRunner.builder()\n+                .setCatalogType(HADOOP)\n+                .setExtraConnectorProperties(ImmutableMap.of(\n+                        \"iceberg.catalog.warehouse\", catalogWarehouseDir,\n+                        \"iceberg.catalog.hadoop.warehouse.datadir\", getCatalogDataDirectory().toString(),\n+                        \"hive.s3.aws-access-key\", ACCESS_KEY,\n+                        \"hive.s3.aws-secret-key\", SECRET_KEY,\n+                        \"hive.s3.endpoint\", format(\"http://%s:%s\", hostAndPort.getHost(), hostAndPort.getPort()),\n+                        \"hive.s3.path-style-access\", \"true\"))\n+                .build().getQueryRunner();\n+    }\n+\n+    @BeforeClass\n+    @Override\n+    public void init()\n+            throws Exception\n+    {\n+        this.dockerizedS3DataLake = new IcebergMinIODataLake(bucketName, WAREHOUSE_DATA_DIR);\n+        this.dockerizedS3DataLake.start();\n+        hostAndPort = this.dockerizedS3DataLake.getMinio().getMinioApiEndpoint();\n+        super.init();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        if (dockerizedS3DataLake != null) {\n+            dockerizedS3DataLake.stop();\n+        }\n+    }\n+\n+    @Override\n+    public void testCreateTableWithCustomLocation()\n+            throws IOException\n+    {\n+        String tableName = \"test_hadoop_table_with_custom_location\";\n+        URI tableTargetURI = createTempDirectory(tableName).toUri();\n+        assertQueryFails(format(\"create table %s (a int, b varchar)\" + \" with (location = '%s')\", tableName, tableTargetURI.toString()),\n+                \"Cannot set a custom location for a path-based table.*\");\n+    }\n+\n+    protected Path getCatalogDataDirectory()\n+    {\n+        return new Path(URI.create(format(\"s3://%s/%s\", bucketName, WAREHOUSE_DATA_DIR)));\n+    }\n+\n+    protected Path getCatalogDirectory()\n+    {\n+        return new Path(catalogWarehouseDir);\n+    }\n+\n+    protected HdfsEnvironment getHdfsEnvironment()\n+    {\n+        HiveClientConfig hiveClientConfig = new HiveClientConfig();\n+        MetastoreClientConfig metastoreClientConfig = new MetastoreClientConfig();\n+        HiveS3Config hiveS3Config = new HiveS3Config()\n+                .setS3AwsAccessKey(ACCESS_KEY)\n+                .setS3AwsSecretKey(SECRET_KEY)\n+                .setS3PathStyleAccess(true)\n+                .setS3Endpoint(format(\"http://%s:%s\", hostAndPort.getHost(), hostAndPort.getPort()));\n+        return getHdfsEnvironment(hiveClientConfig, metastoreClientConfig, hiveS3Config);\n+    }\n+\n+    protected Table loadTable(String tableName)\n+    {\n+        Configuration configuration = getHdfsEnvironment().getConfiguration(new HdfsContext(SESSION), getCatalogDataDirectory());\n+        Catalog catalog = CatalogUtil.loadCatalog(HADOOP.getCatalogImpl(), \"test-hive\", getProperties(), configuration);\n+        return catalog.loadTable(TableIdentifier.of(\"tpch\", tableName));\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergHadoopCatalogOnS3DistributedQueries.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergHadoopCatalogOnS3DistributedQueries.java\nnew file mode 100644\nindex 0000000000000..17f4d9529ad46\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergHadoopCatalogOnS3DistributedQueries.java\n@@ -0,0 +1,95 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.hadoop;\n+\n+import com.facebook.presto.iceberg.IcebergQueryRunner;\n+import com.facebook.presto.iceberg.TestIcebergDistributedQueries;\n+import com.facebook.presto.iceberg.container.IcebergMinIODataLake;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.net.HostAndPort;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+\n+import java.io.IOException;\n+\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.ACCESS_KEY;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.SECRET_KEY;\n+import static com.facebook.presto.tests.sql.TestTable.randomTableSuffix;\n+import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n+\n+public class TestIcebergHadoopCatalogOnS3DistributedQueries\n+        extends TestIcebergDistributedQueries\n+{\n+    static final String WAREHOUSE_DATA_DIR = \"warehouse_data/\";\n+    final String bucketName;\n+    final String catalogWarehouseDir;\n+    private IcebergMinIODataLake dockerizedS3DataLake;\n+    HostAndPort hostAndPort;\n+\n+    public TestIcebergHadoopCatalogOnS3DistributedQueries()\n+            throws IOException\n+    {\n+        super(HADOOP);\n+        bucketName = \"forhadoop-\" + randomTableSuffix();\n+        catalogWarehouseDir = createTempDirectory(bucketName).toUri().toString();\n+    }\n+\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return IcebergQueryRunner.builder()\n+                .setCatalogType(HADOOP)\n+                .setExtraConnectorProperties(ImmutableMap.of(\n+                        \"iceberg.catalog.warehouse\", catalogWarehouseDir,\n+                        \"iceberg.catalog.hadoop.warehouse.datadir\", format(\"s3://%s/%s\", bucketName, WAREHOUSE_DATA_DIR),\n+                        \"hive.s3.aws-access-key\", ACCESS_KEY,\n+                        \"hive.s3.aws-secret-key\", SECRET_KEY,\n+                        \"hive.s3.endpoint\", format(\"http://%s:%s\", hostAndPort.getHost(), hostAndPort.getPort()),\n+                        \"hive.s3.path-style-access\", \"true\"))\n+                .build().getQueryRunner();\n+    }\n+\n+    @BeforeClass\n+    @Override\n+    public void init()\n+            throws Exception\n+    {\n+        this.dockerizedS3DataLake = new IcebergMinIODataLake(bucketName, WAREHOUSE_DATA_DIR);\n+        this.dockerizedS3DataLake.start();\n+        hostAndPort = this.dockerizedS3DataLake.getMinio().getMinioApiEndpoint();\n+        super.init();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        if (dockerizedS3DataLake != null) {\n+            dockerizedS3DataLake.stop();\n+        }\n+    }\n+\n+    protected boolean supportsViews()\n+    {\n+        return false;\n+    }\n+\n+    @Override\n+    public void testRenameTable()\n+    {\n+        // Rename table are not supported by hadoop catalog\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java\nindex 630d78efc3a50..e7429bb8adce5 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeHadoop.java\n@@ -24,12 +24,10 @@\n import com.facebook.presto.iceberg.IcebergUtil;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.spi.SchemaTableName;\n+import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.Table;\n import org.testng.annotations.Test;\n \n-import java.io.File;\n-import java.nio.file.Path;\n-\n import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.ICEBERG_CATALOG;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.getIcebergDataDirectoryPath;\n@@ -47,15 +45,15 @@ public TestIcebergSmokeHadoop()\n     @Override\n     protected String getLocation(String schema, String table)\n     {\n-        File tempLocation = getCatalogDirectory().toFile();\n-        return format(\"%s%s/%s\", tempLocation.toURI(), schema, table);\n+        Path tempLocation = getCatalogDirectory();\n+        return format(\"%s%s/%s\", tempLocation.toUri(), schema, table);\n     }\n \n     @Override\n     protected Path getCatalogDirectory()\n     {\n-        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n-        Path catalogDirectory = getIcebergDataDirectoryPath(dataDirectory, HADOOP.name(), new IcebergConfig().getFileFormat(), false);\n+        java.nio.file.Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        Path catalogDirectory = new Path(getIcebergDataDirectoryPath(dataDirectory, HADOOP.name(), new IcebergConfig().getFileFormat(), false).toFile().toURI());\n         return catalogDirectory;\n     }\n \n@@ -64,7 +62,7 @@ protected Table getIcebergTable(ConnectorSession session, String schema, String\n     {\n         IcebergConfig icebergConfig = new IcebergConfig();\n         icebergConfig.setCatalogType(HADOOP);\n-        icebergConfig.setCatalogWarehouse(getCatalogDirectory().toFile().getPath());\n+        icebergConfig.setCatalogWarehouse(getCatalogDirectory().toString());\n \n         IcebergNativeCatalogFactory catalogFactory = new IcebergNativeCatalogFactory(icebergConfig,\n                 new IcebergCatalogName(ICEBERG_CATALOG),\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeOnS3Hadoop.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeOnS3Hadoop.java\nnew file mode 100644\nindex 0000000000000..fc85c0af1187c\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hadoop/TestIcebergSmokeOnS3Hadoop.java\n@@ -0,0 +1,496 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.hadoop;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.hive.gcs.HiveGcsConfig;\n+import com.facebook.presto.hive.gcs.HiveGcsConfigurationInitializer;\n+import com.facebook.presto.hive.s3.HiveS3Config;\n+import com.facebook.presto.hive.s3.PrestoS3ConfigurationUpdater;\n+import com.facebook.presto.iceberg.FileFormat;\n+import com.facebook.presto.iceberg.IcebergCatalogName;\n+import com.facebook.presto.iceberg.IcebergConfig;\n+import com.facebook.presto.iceberg.IcebergDistributedSmokeTestBase;\n+import com.facebook.presto.iceberg.IcebergNativeCatalogFactory;\n+import com.facebook.presto.iceberg.IcebergQueryRunner;\n+import com.facebook.presto.iceberg.container.IcebergMinIODataLake;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.net.HostAndPort;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.Table;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n+import static com.facebook.presto.iceberg.IcebergQueryRunner.ICEBERG_CATALOG;\n+import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergTable;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.ACCESS_KEY;\n+import static com.facebook.presto.iceberg.container.IcebergMinIODataLake.SECRET_KEY;\n+import static com.facebook.presto.tests.sql.TestTable.randomTableSuffix;\n+import static com.google.common.collect.Iterables.getOnlyElement;\n+import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n+import static java.util.Locale.ENGLISH;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestIcebergSmokeOnS3Hadoop\n+        extends IcebergDistributedSmokeTestBase\n+{\n+    static final String WAREHOUSE_DATA_DIR = \"warehouse_data/\";\n+    final String bucketName;\n+    final String catalogWarehouseDir;\n+\n+    private IcebergMinIODataLake dockerizedS3DataLake;\n+    HostAndPort hostAndPort;\n+\n+    public TestIcebergSmokeOnS3Hadoop()\n+            throws IOException\n+    {\n+        super(HADOOP);\n+        bucketName = \"forhadoop-\" + randomTableSuffix();\n+        catalogWarehouseDir = createTempDirectory(bucketName).toUri().toString();\n+    }\n+\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return IcebergQueryRunner.builder()\n+                .setCatalogType(HADOOP)\n+                .setExtraConnectorProperties(ImmutableMap.of(\n+                        \"iceberg.catalog.warehouse\", catalogWarehouseDir,\n+                        \"iceberg.catalog.hadoop.warehouse.datadir\", getCatalogDataDirectory().toString(),\n+                        \"hive.s3.aws-access-key\", ACCESS_KEY,\n+                        \"hive.s3.aws-secret-key\", SECRET_KEY,\n+                        \"hive.s3.endpoint\", format(\"http://%s:%s\", hostAndPort.getHost(), hostAndPort.getPort()),\n+                        \"hive.s3.path-style-access\", \"true\"))\n+                .build().getQueryRunner();\n+    }\n+\n+    @BeforeClass\n+    @Override\n+    public void init()\n+            throws Exception\n+    {\n+        this.dockerizedS3DataLake = new IcebergMinIODataLake(bucketName, WAREHOUSE_DATA_DIR);\n+        this.dockerizedS3DataLake.start();\n+        hostAndPort = this.dockerizedS3DataLake.getMinio().getMinioApiEndpoint();\n+        super.init();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        if (dockerizedS3DataLake != null) {\n+            dockerizedS3DataLake.stop();\n+        }\n+    }\n+\n+    @Test\n+    public void testShowCreateTableWithSpecifiedWriteDataLocation()\n+    {\n+        String tableName = \"test_table_with_specified_write_data_location\";\n+        String dataWriteLocation = getPathBasedOnDataDirectory(\"test-\" + randomTableSuffix());\n+        try {\n+            assertUpdate(format(\"CREATE TABLE %s(a int, b varchar) with (\\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            String schemaName = getSession().getSchema().get();\n+            String location = getLocation(schemaName, tableName);\n+            String createTableSql = \"CREATE TABLE iceberg.%s.%s (\\n\" +\n+                    \"   \\\"a\\\" integer,\\n\" +\n+                    \"   \\\"b\\\" varchar\\n\" +\n+                    \")\\n\" +\n+                    \"WITH (\\n\" +\n+                    \"   delete_mode = 'merge-on-read',\\n\" +\n+                    \"   format = 'PARQUET',\\n\" +\n+                    \"   format_version = '2',\\n\" +\n+                    \"   location = '%s',\\n\" +\n+                    \"   metadata_delete_after_commit = false,\\n\" +\n+                    \"   metadata_previous_versions_max = 100,\\n\" +\n+                    \"   metrics_max_inferred_column = 100,\\n\" +\n+                    \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                    \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                    \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                    \")\";\n+            assertThat(computeActual(\"SHOW CREATE TABLE \" + tableName).getOnlyValue())\n+                    .isEqualTo(format(createTableSql, schemaName, tableName, location, dataWriteLocation));\n+        }\n+        finally {\n+            assertUpdate(\"DROP TABLE IF EXISTS \" + tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testTableWithSpecifiedWriteDataLocation()\n+    {\n+        String tableName = \"test_table_with_specified_write_data_location2\";\n+        String dataWriteLocation = getPathBasedOnDataDirectory(\"test-\" + randomTableSuffix());\n+        try {\n+            assertUpdate(format(\"create table %s(a int, b varchar) with (\\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            assertUpdate(format(\"insert into %s values(1, '1001'), (2, '1002'), (3, '1003')\", tableName), 3);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002'), (3, '1003')\");\n+            assertUpdate(format(\"delete from %s where a > 2\", tableName), 1);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002')\");\n+        }\n+        finally {\n+            assertUpdate(\"drop table if exists \" + tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionedTableWithSpecifiedWriteDataLocation()\n+    {\n+        String tableName = \"test_table_with_specified_write_data_location3\";\n+        String dataWriteLocation = getPathBasedOnDataDirectory(\"test-\" + randomTableSuffix());\n+        try {\n+            assertUpdate(format(\"create table %s(a int, b varchar) with (partitioning = ARRAY['a'], \\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            assertUpdate(format(\"insert into %s values(1, '1001'), (2, '1002'), (3, '1003')\", tableName), 3);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002'), (3, '1003')\");\n+            assertUpdate(format(\"delete from %s where a > 2\", tableName), 1);\n+            assertQuery(\"select * from \" + tableName, \"values(1, '1001'), (2, '1002')\");\n+        }\n+        finally {\n+            assertUpdate(\"drop table if exists \" + tableName);\n+        }\n+    }\n+\n+    @Override\n+    protected void testCreatePartitionedTableAs(Session session, FileFormat fileFormat)\n+    {\n+        String tableName = \"test_create_partitioned_table_as_\" + fileFormat.toString().toLowerCase(ENGLISH);\n+        @Language(\"SQL\") String createTable = \"\" +\n+                \"CREATE TABLE \" + tableName + \" \" +\n+                \"WITH (\" +\n+                \"format = '\" + fileFormat + \"', \" +\n+                \"partitioning = ARRAY['ORDER_STATUS', 'Ship_Priority', 'Bucket(order_key,9)']\" +\n+                \") \" +\n+                \"AS \" +\n+                \"SELECT orderkey AS order_key, shippriority AS ship_priority, orderstatus AS order_status \" +\n+                \"FROM tpch.tiny.orders\";\n+\n+        assertUpdate(session, createTable, \"SELECT count(*) from orders\");\n+\n+        String createTableSql = \"\" +\n+                \"CREATE TABLE %s.%s.%s (\\n\" +\n+                \"   \\\"order_key\\\" bigint,\\n\" +\n+                \"   \\\"ship_priority\\\" integer,\\n\" +\n+                \"   \\\"order_status\\\" varchar\\n\" +\n+                \")\\n\" +\n+                \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = '\" + fileFormat + \"',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   partitioning = ARRAY['order_status','ship_priority','bucket(order_key, 9)'],\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+\n+        MaterializedResult actualResult = computeActual(\"SHOW CREATE TABLE \" + tableName);\n+        assertEquals(getOnlyElement(actualResult.getOnlyColumnAsSet()),\n+                format(createTableSql,\n+                        getSession().getCatalog().get(),\n+                        getSession().getSchema().get(),\n+                        tableName,\n+                        getLocation(getSession().getSchema().get(), tableName),\n+                        getPathBasedOnDataDirectory(getSession().getSchema().get() + \"/\" + tableName)));\n+\n+        assertQuery(session, \"SELECT * from \" + tableName, \"SELECT orderkey, shippriority, orderstatus FROM orders\");\n+\n+        dropTable(session, tableName);\n+    }\n+\n+    @Override\n+    protected void testCreateTableLike()\n+    {\n+        Session session = getSession();\n+        String schemaName = session.getSchema().get();\n+\n+        String tablePropertiesString = \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'PARQUET',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   partitioning = ARRAY['adate'],\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+        assertUpdate(session, \"CREATE TABLE test_create_table_like_original (col1 INTEGER, aDate DATE) WITH(format = 'PARQUET', partitioning = ARRAY['aDate'])\");\n+        assertEquals(getTablePropertiesString(\"test_create_table_like_original\"),\n+                format(tablePropertiesString,\n+                        getLocation(schemaName, \"test_create_table_like_original\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/test_create_table_like_original\")));\n+\n+        assertUpdate(session, \"CREATE TABLE test_create_table_like_copy0 (LIKE test_create_table_like_original, col2 INTEGER)\");\n+        assertUpdate(session, \"INSERT INTO test_create_table_like_copy0 (col1, aDate, col2) VALUES (1, CAST('1950-06-28' AS DATE), 3)\", 1);\n+        assertQuery(session, \"SELECT * from test_create_table_like_copy0\", \"VALUES(1, CAST('1950-06-28' AS DATE), 3)\");\n+        dropTable(session, \"test_create_table_like_copy0\");\n+\n+        assertUpdate(session, \"CREATE TABLE test_create_table_like_copy1 (LIKE test_create_table_like_original)\");\n+        tablePropertiesString = \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'PARQUET',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+        assertEquals(getTablePropertiesString(\"test_create_table_like_copy1\"),\n+                format(tablePropertiesString,\n+                        getLocation(schemaName, \"test_create_table_like_copy1\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/test_create_table_like_copy1\")));\n+        dropTable(session, \"test_create_table_like_copy1\");\n+\n+        assertUpdate(session, \"CREATE TABLE test_create_table_like_copy2 (LIKE test_create_table_like_original EXCLUDING PROPERTIES)\");\n+        tablePropertiesString = \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'PARQUET',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+        assertEquals(getTablePropertiesString(\"test_create_table_like_copy2\"),\n+                format(tablePropertiesString,\n+                        getLocation(schemaName, \"test_create_table_like_copy2\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/test_create_table_like_copy2\")));\n+        dropTable(session, \"test_create_table_like_copy2\");\n+\n+        assertUpdate(session, \"CREATE TABLE test_create_table_like_copy5 (LIKE test_create_table_like_original INCLUDING PROPERTIES)\" +\n+                \" WITH (location = '', \\\"write.data.path\\\" = '', format = 'ORC')\");\n+        tablePropertiesString = \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'ORC',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   partitioning = ARRAY['adate'],\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+        assertEquals(getTablePropertiesString(\"test_create_table_like_copy5\"),\n+                format(tablePropertiesString,\n+                        getLocation(schemaName, \"test_create_table_like_copy5\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/test_create_table_like_copy5\")));\n+        dropTable(session, \"test_create_table_like_copy5\");\n+\n+        assertQueryFails(session, \"CREATE TABLE test_create_table_like_copy6 (LIKE test_create_table_like_original INCLUDING PROPERTIES)\",\n+                \"Cannot set a custom location for a path-based table.*\");\n+\n+        dropTable(session, \"test_create_table_like_original\");\n+    }\n+\n+    @Override\n+    protected void testCreateTableWithFormatVersion(String formatVersion, String defaultDeleteMode)\n+    {\n+        String tableName = \"test_create_table_with_format_version_\" + formatVersion;\n+        @Language(\"SQL\") String createTable = \"\" +\n+                \"CREATE TABLE \" + tableName + \" \" +\n+                \"WITH (\" +\n+                \"format = 'PARQUET', \" +\n+                \"format_version = '\" + formatVersion + \"'\" +\n+                \") \" +\n+                \"AS \" +\n+                \"SELECT orderkey AS order_key, shippriority AS ship_priority, orderstatus AS order_status \" +\n+                \"FROM tpch.tiny.orders\";\n+\n+        Session session = getSession();\n+\n+        assertUpdate(session, createTable, \"SELECT count(*) from orders\");\n+\n+        String createTableSql = \"\" +\n+                \"CREATE TABLE %s.%s.%s (\\n\" +\n+                \"   \\\"order_key\\\" bigint,\\n\" +\n+                \"   \\\"ship_priority\\\" integer,\\n\" +\n+                \"   \\\"order_status\\\" varchar\\n\" +\n+                \")\\n\" +\n+                \"WITH (\\n\" +\n+                \"   delete_mode = '%s',\\n\" +\n+                \"   format = 'PARQUET',\\n\" +\n+                \"   format_version = '%s',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = '%s'\\n\" +\n+                \")\";\n+\n+        MaterializedResult actualResult = computeActual(\"SHOW CREATE TABLE \" + tableName);\n+        assertEquals(getOnlyElement(actualResult.getOnlyColumnAsSet()),\n+                format(createTableSql,\n+                        getSession().getCatalog().get(),\n+                        getSession().getSchema().get(),\n+                        tableName,\n+                        defaultDeleteMode,\n+                        formatVersion,\n+                        getLocation(getSession().getSchema().get(), tableName),\n+                        getPathBasedOnDataDirectory(getSession().getSchema().get() + \"/\" + tableName),\n+                        defaultDeleteMode));\n+\n+        dropTable(session, tableName);\n+    }\n+\n+    @Override\n+    public void testShowCreateTable()\n+    {\n+        String schemaName = getSession().getSchema().get();\n+        String createTableSql = \"CREATE TABLE iceberg.%s.orders (\\n\" +\n+                \"   \\\"orderkey\\\" bigint,\\n\" +\n+                \"   \\\"custkey\\\" bigint,\\n\" +\n+                \"   \\\"orderstatus\\\" varchar,\\n\" +\n+                \"   \\\"totalprice\\\" double,\\n\" +\n+                \"   \\\"orderdate\\\" date,\\n\" +\n+                \"   \\\"orderpriority\\\" varchar,\\n\" +\n+                \"   \\\"clerk\\\" varchar,\\n\" +\n+                \"   \\\"shippriority\\\" integer,\\n\" +\n+                \"   \\\"comment\\\" varchar\\n\" +\n+                \")\\n\" +\n+                \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'PARQUET',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+        assertThat(computeActual(\"SHOW CREATE TABLE orders\").getOnlyValue())\n+                .isEqualTo(format(createTableSql,\n+                        schemaName,\n+                        getLocation(schemaName, \"orders\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/orders\")));\n+    }\n+\n+    @Test\n+    public void testTableComments()\n+    {\n+        Session session = getSession();\n+        String schemaName = session.getSchema().get();\n+\n+        @Language(\"SQL\") String createTable = \"\" +\n+                \"CREATE TABLE iceberg.%s.test_table_comments (\\n\" +\n+                \"   \\\"_x\\\" bigint\\n\" +\n+                \")\\n\" +\n+                \"COMMENT '%s'\\n\" +\n+                \"WITH (\\n\" +\n+                \"   format = 'ORC',\\n\" +\n+                \"   format_version = '2'\\n\" +\n+                \")\";\n+\n+        assertUpdate(format(createTable, schemaName, \"test table comment\"));\n+\n+        String createTableSql = \"\" +\n+                \"CREATE TABLE iceberg.%s.test_table_comments (\\n\" +\n+                \"   \\\"_x\\\" bigint\\n\" +\n+                \")\\n\" +\n+                \"COMMENT '%s'\\n\" +\n+                \"WITH (\\n\" +\n+                \"   delete_mode = 'merge-on-read',\\n\" +\n+                \"   format = 'ORC',\\n\" +\n+                \"   format_version = '2',\\n\" +\n+                \"   location = '%s',\\n\" +\n+                \"   metadata_delete_after_commit = false,\\n\" +\n+                \"   metadata_previous_versions_max = 100,\\n\" +\n+                \"   metrics_max_inferred_column = 100,\\n\" +\n+                \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                \")\";\n+\n+        MaterializedResult resultOfCreate = computeActual(\"SHOW CREATE TABLE test_table_comments\");\n+        assertEquals(getOnlyElement(resultOfCreate.getOnlyColumnAsSet()),\n+                format(createTableSql, schemaName, \"test table comment\",\n+                        getLocation(schemaName, \"test_table_comments\"),\n+                        getPathBasedOnDataDirectory(schemaName + \"/test_table_comments\")));\n+\n+        dropTable(session, \"test_table_comments\");\n+    }\n+\n+    @Override\n+    protected String getLocation(String schema, String table)\n+    {\n+        Path tempLocation = getCatalogDirectory();\n+        return format(\"%s/%s/%s\", tempLocation.toUri(), schema, table);\n+    }\n+\n+    @Override\n+    protected Table getIcebergTable(ConnectorSession session, String schema, String tableName)\n+    {\n+        IcebergConfig icebergConfig = new IcebergConfig();\n+        icebergConfig.setCatalogType(HADOOP);\n+        icebergConfig.setCatalogWarehouse(getCatalogDirectory().toString());\n+\n+        HiveS3Config hiveS3Config = new HiveS3Config()\n+                .setS3AwsAccessKey(ACCESS_KEY)\n+                .setS3AwsSecretKey(SECRET_KEY)\n+                .setS3PathStyleAccess(true)\n+                .setS3Endpoint(format(\"http://%s:%s\", hostAndPort.getHost(), hostAndPort.getPort()));\n+\n+        IcebergNativeCatalogFactory catalogFactory = new IcebergNativeCatalogFactory(icebergConfig,\n+                new IcebergCatalogName(ICEBERG_CATALOG),\n+                new PrestoS3ConfigurationUpdater(hiveS3Config),\n+                new HiveGcsConfigurationInitializer(new HiveGcsConfig()));\n+\n+        return getNativeIcebergTable(catalogFactory,\n+                session,\n+                SchemaTableName.valueOf(schema + \".\" + tableName));\n+    }\n+\n+    protected Path getCatalogDirectory()\n+    {\n+        return new Path(catalogWarehouseDir);\n+    }\n+\n+    private Path getCatalogDataDirectory()\n+    {\n+        return new Path(URI.create(format(\"s3://%s/%s\", bucketName, WAREHOUSE_DATA_DIR)));\n+    }\n+\n+    private String getPathBasedOnDataDirectory(String name)\n+    {\n+        return new Path(getCatalogDataDirectory(), name).toString();\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java\nindex f0f8627813bf7..68fba51a4526a 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergDistributedHive.java\n@@ -82,7 +82,7 @@ protected Table loadTable(String tableName)\n     protected ExtendedHiveMetastore getFileHiveMetastore()\n     {\n         IcebergFileHiveMetastore fileHiveMetastore = new IcebergFileHiveMetastore(getHdfsEnvironment(),\n-                getCatalogDirectory().getPath(),\n+                getCatalogDirectory().toString(),\n                 \"test\");\n         return memoizeMetastore(fileHiveMetastore, false, 1000, 0);\n     }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java\nindex 406d522ecfc20..e1334bcaad855 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/hive/TestIcebergSmokeHive.java\n@@ -68,7 +68,7 @@ protected static HdfsEnvironment getHdfsEnvironment()\n     protected ExtendedHiveMetastore getFileHiveMetastore()\n     {\n         IcebergFileHiveMetastore fileHiveMetastore = new IcebergFileHiveMetastore(getHdfsEnvironment(),\n-                getCatalogDirectory().toFile().getPath(),\n+                getCatalogDirectory().toString(),\n                 \"test\");\n         return memoizeMetastore(fileHiveMetastore, false, 1000, 0);\n     }\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java\nindex 3aacc290d2055..095e25e74a0d9 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergDistributedNessie.java\n@@ -18,11 +18,11 @@\n import com.facebook.presto.testing.QueryRunner;\n import com.facebook.presto.testing.containers.NessieContainer;\n import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.fs.Path;\n import org.testng.annotations.AfterClass;\n import org.testng.annotations.BeforeClass;\n import org.testng.annotations.Test;\n \n-import java.io.File;\n import java.util.Map;\n \n import static com.facebook.presto.iceberg.CatalogType.NESSIE;\n@@ -43,7 +43,7 @@ protected TestIcebergDistributedNessie()\n     @Override\n     protected Map<String, String> getProperties()\n     {\n-        File metastoreDir = getCatalogDirectory();\n+        Path metastoreDir = getCatalogDirectory();\n         return ImmutableMap.of(\"warehouse\", metastoreDir.toString(), \"uri\", nessieContainer.getRestApiUri());\n     }\n \n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\nindex 1e9e882eb0a36..064679730c9e2 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/nessie/TestIcebergSmokeNessie.java\n@@ -105,7 +105,7 @@ protected Table getIcebergTable(ConnectorSession session, String schema, String\n     {\n         IcebergConfig icebergConfig = new IcebergConfig();\n         icebergConfig.setCatalogType(NESSIE);\n-        icebergConfig.setCatalogWarehouse(getCatalogDirectory().toFile().getPath());\n+        icebergConfig.setCatalogWarehouse(getCatalogDirectory().toString());\n \n         IcebergNessieConfig nessieConfig = new IcebergNessieConfig().setServerUri(nessieContainer.getRestApiUri());\n \n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java\nindex 1a6f71068f0ba..a874a42168c2b 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/IcebergRestTestUtil.java\n@@ -20,6 +20,9 @@\n import com.facebook.airlift.node.NodeInfo;\n import com.facebook.presto.hive.HdfsContext;\n import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.MetastoreClientConfig;\n+import com.facebook.presto.hive.s3.HiveS3Config;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.testing.TestingConnectorSession;\n import com.google.common.collect.ImmutableList;\n@@ -61,7 +64,7 @@ public static Map<String, String> restConnectorProperties(String serverUri)\n     public static TestingHttpServer getRestServer(String location)\n     {\n         JdbcCatalog backingCatalog = new JdbcCatalog();\n-        HdfsEnvironment hdfsEnvironment = getHdfsEnvironment();\n+        HdfsEnvironment hdfsEnvironment = getHdfsEnvironment(new HiveClientConfig(), new MetastoreClientConfig(), new HiveS3Config());\n         backingCatalog.setConf(hdfsEnvironment.getConfiguration(new HdfsContext(SESSION), new Path(location)));\n \n         Map<String, String> properties = ImmutableMap.<String, String>builder()\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\nindex 113ee320fdd27..62627eb352d04 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRestNestedNamespace.java\n@@ -38,6 +38,7 @@\n import org.testng.annotations.Test;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.Map;\n import java.util.Optional;\n \n@@ -50,6 +51,7 @@\n import static com.google.common.io.MoreFiles.deleteRecursively;\n import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n import static java.lang.String.format;\n+import static java.nio.file.Files.createTempDirectory;\n import static java.util.Locale.ENGLISH;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n@@ -182,6 +184,40 @@ public void testShowCreateTable()\n                         \")\", schemaName, getLocation(schemaName, \"orders\")));\n     }\n \n+    @Test\n+    public void testShowCreateTableWithSpecifiedWriteDataLocation()\n+            throws IOException\n+    {\n+        String tableName = \"test_table_with_specified_write_data_location\";\n+        String dataWriteLocation = createTempDirectory(\"test1\").toAbsolutePath().toString();\n+        try {\n+            assertUpdate(format(\"CREATE TABLE %s(a int, b varchar) with (\\\"write.data.path\\\" = '%s')\", tableName, dataWriteLocation));\n+            String schemaName = getSession().getSchema().get();\n+            String location = getLocation(schemaName, tableName);\n+            String createTableSql = \"CREATE TABLE iceberg.\\\"%s\\\".%s (\\n\" +\n+                    \"   \\\"a\\\" integer,\\n\" +\n+                    \"   \\\"b\\\" varchar\\n\" +\n+                    \")\\n\" +\n+                    \"WITH (\\n\" +\n+                    \"   delete_mode = 'merge-on-read',\\n\" +\n+                    \"   format = 'PARQUET',\\n\" +\n+                    \"   format_version = '2',\\n\" +\n+                    \"   location = '%s',\\n\" +\n+                    \"   metadata_delete_after_commit = false,\\n\" +\n+                    \"   metadata_previous_versions_max = 100,\\n\" +\n+                    \"   metrics_max_inferred_column = 100,\\n\" +\n+                    \"   \\\"read.split.target-size\\\" = 134217728,\\n\" +\n+                    \"   \\\"write.data.path\\\" = '%s',\\n\" +\n+                    \"   \\\"write.update.mode\\\" = 'merge-on-read'\\n\" +\n+                    \")\";\n+            assertThat(computeActual(\"SHOW CREATE TABLE \" + tableName).getOnlyValue())\n+                    .isEqualTo(format(createTableSql, schemaName, tableName, location, dataWriteLocation));\n+        }\n+        finally {\n+            assertUpdate((\"DROP TABLE IF EXISTS \" + tableName));\n+        }\n+    }\n+\n     @Test\n     @Override // override due to double quotes around nested namespace\n     public void testTableComments()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24353",
    "pr_id": 24353,
    "issue_id": 24087,
    "repo": "prestodb/presto",
    "problem_statement": "Interval values can overflow\nThis produces the correct value:\r\n\r\n```\r\npresto> select interval '1' month * 2147483647;\r\n    _col0    \r\n-------------\r\n 178956970-7 \r\n(1 row)\r\n```\r\n\r\nThe next higher number will overflow:\r\n\r\n```\r\npresto> select interval '1' month * 2147483648;\r\n    _col0     \r\n--------------\r\n -178956970-8 \r\n(1 row)\r\n```\r\n\r\nNone of the [interval year to month operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntervalYearMonthOperators.java) for division, multiplication, subtraction and addition check boundaries, so this problem is not specific to multiplication.  Also, similar boundary checks should be added for multiplication by double.\r\n\r\nLikewise, the [interval day time operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java) will also overflow and need the same checks, adjusted to work for the long type that backs [IntervalDayTimeType](https://github.com/prestodb/presto/blob/150c7ebdd6a7e4b73725150587508925d58c6ad9/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeType.java#L23).\r\n\r\n## Your Environment\r\nThis is true for any functional environment of Presto.\r\n\r\n## Expected Behavior\r\nIt should be an error to exceed the boundary of the interval type.  The underlying datastructure for interval year to month is an integer, so we should raise an error for any value that exceed's Java integer range.\r\n\r\n## Current Behavior\r\nThe interval will silently overflow and return incorrect results.\r\n\r\n## Possible Solution\r\nModify the interval operators to do proper overflow checking, as is done for e.g. [`INTEGER` operators](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/type/IntegerOperators.java#L64-L134).\r\n\r\n## Steps to Reproduce\r\nSee the above queries for a simple reproduction.\r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nThis was discovered while converting constant folding of expressions to use Velox for their evaluation.\r\n\r\nThis was previously reported in #9342.\r\n",
    "issue_word_count": 289,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java",
      "presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java"
    ],
    "base_commit": "8549ed7c909c76fa64fb005df41fd0eb10f724be",
    "head_commit": "8559ab465ef6cc9e4a6591da9e9214e35652d3f2",
    "repo_url": "https://github.com/prestodb/presto/pull/24353",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24353",
    "dockerfile": "",
    "pr_merged_at": "2025-02-10T02:13:18.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java b/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java\nindex 15c92d2be7867..c49edc9d2a99c 100644\n--- a/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java\n+++ b/presto-main/src/main/java/com/facebook/presto/type/IntervalDayTimeOperators.java\n@@ -16,6 +16,7 @@\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.type.AbstractLongType;\n import com.facebook.presto.common.type.StandardTypes;\n+import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.function.BlockIndex;\n import com.facebook.presto.spi.function.BlockPosition;\n import com.facebook.presto.spi.function.IsNull;\n@@ -42,8 +43,11 @@\n import static com.facebook.presto.common.function.OperatorType.NEGATION;\n import static com.facebook.presto.common.function.OperatorType.NOT_EQUAL;\n import static com.facebook.presto.common.function.OperatorType.SUBTRACT;\n+import static com.facebook.presto.spi.StandardErrorCode.DIVISION_BY_ZERO;\n+import static com.facebook.presto.spi.StandardErrorCode.NUMERIC_VALUE_OUT_OF_RANGE;\n import static com.facebook.presto.type.IntervalDayTimeType.INTERVAL_DAY_TIME;\n import static io.airlift.slice.Slices.utf8Slice;\n+import static java.lang.String.format;\n \n public final class IntervalDayTimeOperators\n {\n@@ -55,56 +59,104 @@ private IntervalDayTimeOperators()\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long add(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long left, @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long right)\n     {\n-        return left + right;\n+        try {\n+            return Math.addExact(left, right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second addition overflow: %s ms + %s ms\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(SUBTRACT)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long subtract(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long left, @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long right)\n     {\n-        return left - right;\n+        try {\n+            return Math.subtractExact(left, right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second subtraction overflow: %s ms - %s ms\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long multiplyByBigint(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long left, @SqlType(StandardTypes.BIGINT) long right)\n     {\n-        return left * right;\n+        try {\n+            return Math.multiplyExact(left, right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second multiply overflow: %s ms * %s\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long multiplyByDouble(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long left, @SqlType(StandardTypes.DOUBLE) double right)\n     {\n-        return (long) (left * right);\n+        try {\n+            return Math.addExact(\n+                    (long) (left * (right - (long) right)),\n+                    Math.multiplyExact(left, (long) right));\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second multiply overflow: %s ms * %s\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long bigintMultiply(@SqlType(StandardTypes.BIGINT) long left, @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long right)\n     {\n-        return left * right;\n+        try {\n+            return Math.multiplyExact(left, right);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second multiply overflow: %s * %s ms\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(MULTIPLY)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long doubleMultiply(@SqlType(StandardTypes.DOUBLE) double left, @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long right)\n     {\n-        return (long) (left * right);\n+        try {\n+            return Math.addExact(\n+                Math.multiplyExact((long) left, right),\n+                (long) ((left - (long) left) * right));\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second multiply overflow: %s * %s ms\", left, right), e);\n+        }\n     }\n \n     @ScalarOperator(DIVIDE)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long divideByDouble(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long left, @SqlType(StandardTypes.DOUBLE) double right)\n     {\n-        return (long) (left / right);\n+        if (right == 0) {\n+            throw new PrestoException(DIVISION_BY_ZERO, format(\"interval_day_to_second division by zero: %s ms / %s\", left, right));\n+        }\n+\n+        try {\n+            return multiplyByDouble(left, 1.0 / right);\n+        }\n+        catch (PrestoException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, format(\"interval_day_to_second division overflow: %s ms / %s\", left, right));\n+        }\n     }\n \n     @ScalarOperator(NEGATION)\n     @SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND)\n     public static long negate(@SqlType(StandardTypes.INTERVAL_DAY_TO_SECOND) long value)\n     {\n-        return -value;\n+        try {\n+            return Math.negateExact(value);\n+        }\n+        catch (ArithmeticException e) {\n+            throw new PrestoException(NUMERIC_VALUE_OUT_OF_RANGE, \"interval_day_to_second negation overflow: \" + value, e);\n+        }\n     }\n \n     @ScalarOperator(EQUAL)\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java b/presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java\nindex 36dddb114c1aa..4e09408e80546 100644\n--- a/presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java\n+++ b/presto-main/src/test/java/com/facebook/presto/type/TestIntervalDayTime.java\n@@ -21,6 +21,7 @@\n import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.type.IntervalDayTimeType.INTERVAL_DAY_TIME;\n+import static java.lang.String.format;\n import static java.util.concurrent.TimeUnit.DAYS;\n import static org.testng.Assert.assertEquals;\n \n@@ -131,6 +132,9 @@ public void testMultiply()\n         assertFunction(\"2 * INTERVAL '6' DAY\", INTERVAL_DAY_TIME, new SqlIntervalDayTime(12 * 24 * 60 * 60 * 1000));\n         assertFunction(\"INTERVAL '1' DAY * 2.5\", INTERVAL_DAY_TIME, new SqlIntervalDayTime((long) (2.5 * 24 * 60 * 60 * 1000)));\n         assertFunction(\"2.5 * INTERVAL '1' DAY\", INTERVAL_DAY_TIME, new SqlIntervalDayTime((long) (2.5 * 24 * 60 * 60 * 1000)));\n+        assertNumericOverflow(\n+                format(\"%s * INTERVAL '%s' DAY\", Integer.MAX_VALUE, 64),\n+                format(\"interval_day_to_second multiply overflow: %s * %s ms\", Integer.MAX_VALUE, (64 * 24 * 60 * 60 * 1000L)));\n     }\n \n     @Test\n@@ -141,6 +145,9 @@ public void testDivide()\n \n         assertFunction(\"INTERVAL '3' DAY / 2\", INTERVAL_DAY_TIME, new SqlIntervalDayTime((long) (1.5 * 24 * 60 * 60 * 1000)));\n         assertFunction(\"INTERVAL '4' DAY / 2.5\", INTERVAL_DAY_TIME, new SqlIntervalDayTime((long) (1.6 * 24 * 60 * 60 * 1000)));\n+        assertNumericOverflow(\n+                format(\"INTERVAL '%s' DAY / %s\", 64, 1 / (double) Integer.MAX_VALUE),\n+                format(\"interval_day_to_second division overflow: %s ms / %s\", (64 * 24 * 60 * 60 * 1000L), 1 / (double) Integer.MAX_VALUE));\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24344",
    "pr_id": 24344,
    "issue_id": 20958,
    "repo": "prestodb/presto",
    "problem_statement": "Clickhouse connector incorrectly transforms Timestamp constants to DateTime literals\nPresto-clickhouse connector transforms a `TIMESTAMP WITH TIME ZONE` constant like `from_iso8601_timestamp('2023-09-20T00:00:00+09:00')` to `TIMESTAMP '2023-09-19 15:00:00.000'` literal in native Clickhouse SQL. `TIMESTAMP` in Clickhouse is an alias to the `DateTime` type, which doesn't offer sub-second precision. This leads to a parsing error in Clickhouse.\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: 0.283\r\n* Data source and connector used: presto-clickhouse\r\n* Deployment (Cloud or On-prem): on-perm\r\n* Clickhouse server version:  23.6.2\r\n\r\nA Clickhouse table with the following schema (native Clickhouse DDL syntax):\r\n```sql\r\nCREATE TABLE default.t\r\n(\r\n    `ts` DateTime,\r\n    `value` Float32\r\n)\r\nENGINE = MergeTree\r\nORDER BY ts\r\n```\r\n\r\nPresto-clickhouse  connector is configured like this:\r\n```\r\nconnector.name=clickhouse\r\nclickhouse.connection-url=jdbc:clickhouse://localhost:8123\r\nclickhouse.connection-user=default\r\n```\r\n\r\n\r\n## Expected behavior\r\nI expect the following query to complete without errors (Presto SQL):\r\n\r\n```sql\r\nselect count(*) \r\nFROM clickhouse.default.t \r\nwhere ts >= from_iso8601_timestamp('2023-09-20T00:00:00+09:00');\r\n```\r\n\r\n## Current Behavior\r\nQuery returns an error:\r\n```\r\nQuery 20230925_102132_00013_842zf failed: ClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 6. DB::Exception: Cannot parse string '2023-09-19 15:00:00.000' as DateTime: syntax error at position 19 (parsed just '2023-09-19 15:00:00'): While processing SELECT count() FROM default.t WHERE ts >= toDateTime('2023-09-19 15:00:00.000'). (CANNOT_PARSE_TEXT) (version 23.6.2.18 (official build))\r\n```\r\n\r\n<details>\r\n<summary> Stacktrace </summary>\r\n\r\n```\r\n2023-09-25T19:21:33.041+0900    ERROR   SplitRunner-15-131      com.facebook.presto.execution.executor.TaskExecutor     Error processing Split 20230925_102132_00013_842zf.1.0.0.0-0 com.facebook.presto.plugin.clickhouse.ClickHouseSplit@140367c (start = 3.9098605869769E7, wall = 21 ms, cpu = 0 ms, wait = 0 ms, calls = 1): JDBC_ERROR: ClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 6. DB::Exception: Cannot parse string '2023-09-19 15:00:00.000' as DateTime: syntax error at position 19 (parsed just '2023-09-19 15:00:00'): While processing SELECT count() FROM default.t WHERE ts >= toDateTime('2023-09-19 15:00:00.000'). (CANNOT_PARSE_TEXT) (version 23.6.2.18 (official build))\r\n\r\n2023-09-25T19:21:33.059+0900    ERROR   remote-task-callback-20 com.facebook.presto.execution.StageExecutionStateMachine        Stage execution 20230925_102132_00013_842zf.1.0 failed\r\ncom.facebook.presto.spi.PrestoException: ClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 6. DB::Exception: Cannot parse string '2023-09-19 15:00:00.000' as DateTime: syntax error at position 19 (parsed just '2023-09-19 15:00:00'): While processing SELECT count() FROM default.t WHERE ts >= toDateTime('2023-09-19 15:00:00.000'). (CANNOT_PARSE_TEXT) (version 23.6.2.18 (official build))\r\n\r\n        at com.facebook.presto.plugin.clickhouse.ClickHouseRecordCursor.handleSqlException(ClickHouseRecordCursor.java:236)\r\n        at com.facebook.presto.plugin.clickhouse.ClickHouseRecordCursor.<init>(ClickHouseRecordCursor.java:95)\r\n        at com.facebook.presto.plugin.clickhouse.ClickHouseRecordSet.cursor(ClickHouseRecordSet.java:59)\r\n        at com.facebook.presto.spi.RecordPageSource.<init>(RecordPageSource.java:40)\r\n        at com.facebook.presto.split.RecordPageSourceProvider.createPageSource(RecordPageSourceProvider.java:48)\r\n        at com.facebook.presto.spi.connector.ConnectorPageSourceProvider.createPageSource(ConnectorPageSourceProvider.java:52)\r\n        at com.facebook.presto.split.PageSourceManager.createPageSource(PageSourceManager.java:80)\r\n        at com.facebook.presto.operator.TableScanOperator.getOutput(TableScanOperator.java:263)\r\n        at com.facebook.presto.operator.Driver.processInternal(Driver.java:428)\r\n        at com.facebook.presto.operator.Driver.lambda$processFor$9(Driver.java:311)\r\n        at com.facebook.presto.operator.Driver.tryWithLock(Driver.java:732)\r\n        at com.facebook.presto.operator.Driver.processFor(Driver.java:304)\r\n        at com.facebook.presto.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1079)\r\n        at com.facebook.presto.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:165)\r\n        at com.facebook.presto.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:603)\r\n        at com.facebook.presto.$gen.Presto_0_283_1fa586a____20230925_100436_1.run(Unknown Source)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: ru.yandex.clickhouse.except.ClickHouseUnknownException: ClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 6. DB::Exception: Cannot parse string '2023-09-19 15:00:00.000' as DateTime: syntax error at position 19 (parsed just '2023-09-19 15:00:00'): While processing SELECT count() FROM default.t WHERE ts >= toDateTime('2023-09-19 15:00:00.000'). (CANNOT_PARSE_TEXT) (version 23.6.2.18 (official build))\r\n\r\n        at ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.getException(ClickHouseExceptionSpecifier.java:91)\r\n        at ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:55)\r\n        at ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:28)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.checkForErrorAndThrow(ClickHouseStatementImpl.java:875)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.getInputStream(ClickHouseStatementImpl.java:616)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:117)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:100)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:95)\r\n        at ru.yandex.clickhouse.ClickHouseStatementImpl.executeQuery(ClickHouseStatementImpl.java:90)\r\n        at ru.yandex.clickhouse.ClickHousePreparedStatementImpl.executeQuery(ClickHousePreparedStatementImpl.java:110)\r\n        at com.facebook.presto.plugin.clickhouse.ClickHouseRecordCursor.<init>(ClickHouseRecordCursor.java:92)\r\n        ... 17 more\r\nCaused by: java.lang.Throwable: Code: 6. DB::Exception: Cannot parse string '2023-09-19 15:00:00.000' as DateTime: syntax error at position 19 (parsed just '2023-09-19 15:00:00'): While processing SELECT count() FROM default.t WHERE ts >= toDateTime('2023-09-19 15:00:00.000'). (CANNOT_PARSE_TEXT) (version 23.6.2.18 (official build))\r\n\r\n        at ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:53)\r\n        ... 26 more\r\n```\r\n</details>\r\n\r\n## Possible Solution\r\n\r\nI believe that this happens because of the incorrect handling of `TimestampWithTimeZoneType` /`Timestamp` constants in `ClickHousePushdownUtils.java`: https://github.com/prestodb/presto/blob/0dde0a27677c106d1469b57f1211b03191ec2181/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/optimization/ClickHousePushdownUtils.java#L130-L135\r\n\r\n`getTimestampLiteralAsString` produces literals like `TIMESTAMP '2023-09-25 00:00:00.000'`:\r\nhttps://github.com/prestodb/presto/blob/0dde0a27677c106d1469b57f1211b03191ec2181/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/optimization/ClickHousePushdownUtils.java#L139-L143\r\n\r\nClickHouse transforms `TIMESTAMP '2023-09-25 00:00:00.000'` to `toDateTime('2023-09-25 00:00:00.000')`, which triggers a parsing error.\r\n\r\nSwitching from type casting (`TIMESTAMP '...'`) to `parseDateTimeBestEffort` function works for me so far:\r\n\r\n```java\r\n   private static String getTimestampLiteralAsString(ConnectorSession session, long millisUtc)\r\n    {\r\n        SqlTimestamp sqlTimestamp = new SqlTimestamp(millisUtc, MILLISECONDS);\r\n        return \"parseDateTimeBestEffort('\" + sqlTimestamp.toString() + \"')\";\r\n    }\r\n```\r\n\r\n",
    "issue_word_count": 1038,
    "test_files_count": 2,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHouseClient.java",
      "presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHousePageSink.java",
      "presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/StandardReadMappings.java",
      "presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/TimestampUtil.java",
      "presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java",
      "presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java",
      "presto-docs/src/main/sphinx/connector/clickhouse.rst"
    ],
    "pr_changed_test_files": [
      "presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java",
      "presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java"
    ],
    "base_commit": "fed9c5de8d33eeb4eb8e0175336254b47c62e92d",
    "head_commit": "7a3feeaba0521b524f6887ae1e4ef4c33d86958c",
    "repo_url": "https://github.com/prestodb/presto/pull/24344",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24344",
    "dockerfile": "",
    "pr_merged_at": "2025-01-16T15:29:39.000Z",
    "patch": "diff --git a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHouseClient.java b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHouseClient.java\nindex 87eb7606cd841..8359b748f5ef1 100755\n--- a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHouseClient.java\n+++ b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHouseClient.java\n@@ -17,6 +17,7 @@\n import com.facebook.presto.common.predicate.TupleDomain;\n import com.facebook.presto.common.type.CharType;\n import com.facebook.presto.common.type.DecimalType;\n+import com.facebook.presto.common.type.TimestampType;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.common.type.VarbinaryType;\n import com.facebook.presto.common.type.VarcharType;\n@@ -875,6 +876,9 @@ private String toWriteMapping(Type type)\n         if (type == DATE) {\n             return \"Date\";\n         }\n+        if (type instanceof TimestampType) {\n+            return \"DateTime64(3)\";\n+        }\n         throw new PrestoException(NOT_SUPPORTED, \"Unsupported column type: \" + type);\n     }\n \n\ndiff --git a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHousePageSink.java b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHousePageSink.java\nindex 6c2964eab8ec6..1125832780530 100755\n--- a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHousePageSink.java\n+++ b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/ClickHousePageSink.java\n@@ -17,6 +17,7 @@\n import com.facebook.presto.common.Page;\n import com.facebook.presto.common.block.Block;\n import com.facebook.presto.common.type.DecimalType;\n+import com.facebook.presto.common.type.TimestampType;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.spi.ConnectorPageSink;\n import com.facebook.presto.spi.ConnectorSession;\n@@ -163,6 +164,10 @@ else if (DATE.equals(type)) {\n             // convert to midnight in default time zone\n             statement.setDate(parameter, convertZonedDaysToDate(type.getLong(block, position)));\n         }\n+        else if (type instanceof TimestampType) {\n+            // setTimestamp doesn't work, so we use setLong as described at https://github.com/ClickHouse/clickhouse-java/issues/608\n+            statement.setLong(parameter, type.getLong(block, position));\n+        }\n         else {\n             throw new PrestoException(NOT_SUPPORTED, \"Unsupported column type: \" + type.getDisplayName());\n         }\n\ndiff --git a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/StandardReadMappings.java b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/StandardReadMappings.java\nindex 73f3f094be171..e7a57abeb483e 100755\n--- a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/StandardReadMappings.java\n+++ b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/StandardReadMappings.java\n@@ -48,6 +48,7 @@\n import static com.facebook.presto.plugin.clickhouse.DateTimeUtil.getMillisOfDay;\n import static com.facebook.presto.plugin.clickhouse.ReadMapping.longReadMapping;\n import static com.facebook.presto.plugin.clickhouse.ReadMapping.sliceReadMapping;\n+import static com.facebook.presto.plugin.clickhouse.TimestampUtil.getMillisecondsFromTimestampString;\n import static io.airlift.slice.Slices.utf8Slice;\n import static io.airlift.slice.Slices.wrappedBuffer;\n import static java.lang.Float.floatToRawIntBits;\n@@ -140,7 +141,8 @@ public static ReadMapping timestampReadMapping()\n     {\n         return longReadMapping(TIMESTAMP, (resultSet, columnIndex) -> {\n             Timestamp timestamp = resultSet.getTimestamp(columnIndex);\n-            return timestamp.getTime();\n+            // getTimestamp loses the milliseconds, but we can get them from the getString\n+            return timestamp.getTime() + getMillisecondsFromTimestampString(resultSet.getString(columnIndex));\n         });\n     }\n \n@@ -163,6 +165,8 @@ public static Optional<ReadMapping> jdbcTypeToPrestoType(ClickHouseTypeHandle ty\n                     return Optional.of(varcharReadMapping(createUnboundedVarcharType()));\n                 }\n                 return Optional.of(varbinaryReadMapping());\n+            case \"DateTime64\": // DateTime64(n)\n+                return Optional.of(timestampReadMapping());\n             case \"block\":\n                 return Optional.of(doubleReadMapping());\n         }\n\ndiff --git a/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/TimestampUtil.java b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/TimestampUtil.java\nnew file mode 100644\nindex 0000000000000..9ad771335c274\n--- /dev/null\n+++ b/presto-clickhouse/src/main/java/com/facebook/presto/plugin/clickhouse/TimestampUtil.java\n@@ -0,0 +1,35 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.plugin.clickhouse;\n+\n+public class TimestampUtil\n+{\n+    private TimestampUtil() {}\n+\n+    public static int getMillisecondsFromTimestampString(String timestampString)\n+    {\n+        int dotIndex = timestampString.indexOf('.');\n+        if (dotIndex == -1) {\n+            return 0;\n+        }\n+\n+        String fraction = timestampString.substring(dotIndex + 1);\n+        int nonNormalized = Integer.parseInt(fraction);\n+        if (nonNormalized == 0 || fraction.length() == 3) {\n+            return nonNormalized;\n+        }\n+        // this will make sure it's always 3 digits. e.g., 7 -> 700; 71 -> 710; 7591 -> 759\n+        return (int) Math.round(nonNormalized * Math.pow(10, -(Math.floor(Math.log10(nonNormalized)) - 2)));\n+    }\n+}\n\ndiff --git a/presto-docs/src/main/sphinx/connector/clickhouse.rst b/presto-docs/src/main/sphinx/connector/clickhouse.rst\nindex 338d98836727b..f8510cc8424a4 100644\n--- a/presto-docs/src/main/sphinx/connector/clickhouse.rst\n+++ b/presto-docs/src/main/sphinx/connector/clickhouse.rst\n@@ -89,6 +89,26 @@ Run ``SELECT`` to access the ``cks`` table in the ``tpch`` database::\n     If you used a different name for your catalog properties file, use\n     that catalog name instead of ``clickhouse`` in the above examples.\n \n+PrestoDB to ClickHouse Type Mapping\n+-----------------------------------\n+\n+========================================== ========================= =================================================================================\n+**PrestoDB Type**                          **ClickHouse Type**       **Notes**\n+========================================== ========================= =================================================================================\n+BOOLEAN                                    UInt8                     ClickHouse uses UInt8 as boolean, restricted values to 0 and 1.\n+TINYINT                                    Int8\n+SMALLINT                                   Int16\n+INTEGER                                    Int32\n+BIGINT                                     Int64\n+REAL                                       Float32\n+DOUBLE                                     Float64\n+DECIMAL                                    Decimal(precision, scale) The precision and scale are dynamic based on the PrestoDB type.\n+CHAR / VARCHAR                             String                    The String type replaces VARCHAR, BLOB, CLOB, and related types from other DBMSs.\n+VARBINARY                                  String\n+DATE                                       Date\n+TIMESTAMP                                  DateTime64(3)             Timestamp with 3 digits of millisecond precision.\n+========================================== ========================= =================================================================================\n+\n Table properties\n ----------------\n \n",
    "test_patch": "diff --git a/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java b/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java\nindex e9c6e1d915435..56e6d924e5fb6 100755\n--- a/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java\n+++ b/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestClickHouseDistributedQueries.java\n@@ -25,18 +25,23 @@\n import org.testng.annotations.Test;\n \n import java.security.SecureRandom;\n+import java.time.ZoneId;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n \n import static com.facebook.presto.common.type.BigintType.BIGINT;\n import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.plugin.clickhouse.ClickHouseQueryRunner.createClickHouseQueryRunner;\n import static com.facebook.presto.testing.MaterializedResult.resultBuilder;\n+import static com.facebook.presto.testing.TestingSession.DEFAULT_TIME_ZONE_KEY;\n import static com.facebook.presto.testing.assertions.Assert.assertEquals;\n import static com.facebook.presto.tests.QueryAssertions.assertEqualsIgnoreOrder;\n import static java.lang.Character.MAX_RADIX;\n import static java.lang.Math.abs;\n import static java.lang.Math.min;\n import static java.lang.String.format;\n+import static java.time.format.DateTimeFormatter.ISO_ZONED_DATE_TIME;\n import static java.util.Objects.requireNonNull;\n import static java.util.stream.Collectors.joining;\n import static java.util.stream.IntStream.range;\n@@ -224,6 +229,75 @@ public void testInsertIntoNotNullColumn()\n         assertUpdate(\"DROP TABLE test_not_null_with_insert\");\n     }\n \n+    @Test\n+    public void testInsertAndSelectFromDateTimeTables()\n+    {\n+        // ----- Table T - No milliseconds -----\n+        ZonedDateTime originalTimestamp = ZonedDateTime.parse(\"2025-01-08T12:34:56Z\", ISO_ZONED_DATE_TIME);\n+        // the test session is Pacific/Apia\n+        ZonedDateTime adjustedTimestamp = originalTimestamp.withZoneSameInstant(\n+                ZoneId.of(DEFAULT_TIME_ZONE_KEY.getId()));\n+\n+        // Pacific/Apia becomes 2025-01-09 01:34:56\n+        String adjustedTimestampString = adjustedTimestamp.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"));\n+\n+        assertUpdate(\"CREATE TABLE t (ts timestamp not null)\");\n+        assertUpdate(\"INSERT INTO t (ts) VALUES (timestamp '\" + adjustedTimestampString + \"')\", 1);\n+        assertQuery(\n+                \"SELECT * FROM t LIMIT 100\",\n+                \"VALUES (timestamp  '\" + adjustedTimestampString + \"')\");\n+        assertUpdate(\"DROP TABLE IF EXISTS t\");\n+        // ----- End of Table T - No milliseconds -----\n+\n+        // ----- Table T1 - 1 digit of milliseconds -----\n+        originalTimestamp = ZonedDateTime.parse(\"2025-01-08T12:34:56.7Z\", ISO_ZONED_DATE_TIME);\n+        // the test session is Pacific/Apia\n+        adjustedTimestamp = originalTimestamp.withZoneSameInstant(ZoneId.of(DEFAULT_TIME_ZONE_KEY.getId()));\n+\n+        // Pacific/Apia becomes 2025-01-09 01:34:56.7\n+        adjustedTimestampString = adjustedTimestamp.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.S\"));\n+\n+        assertUpdate(\"CREATE TABLE t1 (ts timestamp not null)\");\n+        assertUpdate(\"INSERT INTO t1 (ts) VALUES (timestamp '\" + adjustedTimestampString + \"')\", 1);\n+        assertQuery(\n+                \"SELECT * FROM t1 LIMIT 100\",\n+                \"VALUES (timestamp  '\" + adjustedTimestampString + \"')\");\n+        assertUpdate(\"DROP TABLE IF EXISTS t1\");\n+        // ----- End of Table T1 - 1 digit of milliseconds -----\n+\n+        // ----- Table T2 - 2 digits of milliseconds -----\n+        originalTimestamp = ZonedDateTime.parse(\"2025-01-08T12:34:56.75Z\", ISO_ZONED_DATE_TIME);\n+        // the test session is Pacific/Apia\n+        adjustedTimestamp = originalTimestamp.withZoneSameInstant(ZoneId.of(DEFAULT_TIME_ZONE_KEY.getId()));\n+\n+        // Pacific/Apia becomes 2025-01-09 01:34:56.75\n+        adjustedTimestampString = adjustedTimestamp.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SS\"));\n+\n+        assertUpdate(\"CREATE TABLE t2 (ts timestamp not null)\");\n+        assertUpdate(\"INSERT INTO t2 (ts) VALUES (timestamp '\" + adjustedTimestampString + \"')\", 1);\n+        assertQuery(\n+                \"SELECT * FROM t2 LIMIT 100\",\n+                \"VALUES (timestamp  '\" + adjustedTimestampString + \"')\");\n+        assertUpdate(\"DROP TABLE IF EXISTS t2\");\n+        // ----- End of Table T2 - 2 digits of milliseconds -----\n+\n+        // ----- Table T3 - 3 digits of milliseconds -----\n+        originalTimestamp = ZonedDateTime.parse(\"2025-01-08T12:34:56.759Z\", ISO_ZONED_DATE_TIME);\n+        // the test session is Pacific/Apia\n+        adjustedTimestamp = originalTimestamp.withZoneSameInstant(ZoneId.of(DEFAULT_TIME_ZONE_KEY.getId()));\n+\n+        // Pacific/Apia becomes 2025-01-09 01:34:56.759\n+        adjustedTimestampString = adjustedTimestamp.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss.SSS\"));\n+\n+        assertUpdate(\"CREATE TABLE t3 (ts timestamp not null)\");\n+        assertUpdate(\"INSERT INTO t3 (ts) VALUES (timestamp '\" + adjustedTimestampString + \"')\", 1);\n+        assertQuery(\n+                \"SELECT * FROM t3 LIMIT 100\",\n+                \"VALUES (timestamp  '\" + adjustedTimestampString + \"')\");\n+        assertUpdate(\"DROP TABLE IF EXISTS t3\");\n+        // ----- End of Table T3 - 3 digits of milliseconds -----\n+    }\n+\n     @Override\n     public void testDropColumn()\n     {\n\ndiff --git a/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java b/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java\nindex 659c42cc40204..87dbb916f1179 100755\n--- a/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java\n+++ b/presto-clickhouse/src/test/java/com/facebook/presto/plugin/clickhouse/TestingClickHouseServer.java\n@@ -26,7 +26,7 @@\n public class TestingClickHouseServer\n         implements Closeable\n {\n-    private static final String CLICKHOUSE_IMAGE = \"yandex/clickhouse-server:20.8\";\n+    private static final String CLICKHOUSE_IMAGE = \"clickhouse/clickhouse-server:23.12.2.59\";\n     private final ClickHouseContainer dockerContainer;\n \n     public TestingClickHouseServer()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24333",
    "pr_id": 24333,
    "issue_id": 24327,
    "repo": "prestodb/presto",
    "problem_statement": "Modules presto-function-namespace-managers and presto-mysql failing in CI\nhttps://github.com/prestodb/presto/actions/runs/12646508402/job/35238800130?pr=24326\r\n\r\n```\r\n2025-01-07T07:32:04.7779783Z [ERROR] Tests run: 91, Failures: 1, Errors: 0, Skipped: 75, Time elapsed: 1.312 s <<< FAILURE! - in TestSuite\r\n2025-01-07T07:32:04.7795120Z [ERROR] com.facebook.presto.functionNamespace.mysql.TestMySqlFunctionNamespaceManager.setup  Time elapsed: 0.29 s  <<< FAILURE!\r\n2025-01-07T07:32:04.7796290Z java.lang.RuntimeException: \r\n2025-01-07T07:32:04.7798483Z io.airlift.command.CommandFailedException: [/tmp/testing-mysql-server3402480835468141148/bin/mysqld, --no-defaults, --initialize-insecure, --skip-sync-frm, --innodb-flush-method=nosync, --datadir, /tmp/testing-mysql-server3402480835468141148/data] exited with 127\r\n2025-01-07T07:32:04.7801460Z /tmp/testing-mysql-server3402480835468141148/bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory\r\n2025-01-07T07:32:04.7802478Z \r\n2025-01-07T07:32:04.7803029Z \tat com.facebook.presto.testing.mysql.AbstractEmbeddedMySql.system(AbstractEmbeddedMySql.java:282)\r\n2025-01-07T07:32:04.7804318Z \tat com.facebook.presto.testing.mysql.AbstractEmbeddedMySql.initialize(AbstractEmbeddedMySql.java:186)\r\n2025-01-07T07:32:04.7805576Z \tat com.facebook.presto.testing.mysql.AbstractEmbeddedMySql.<init>(AbstractEmbeddedMySql.java:84)\r\n2025-01-07T07:32:04.7806734Z \tat com.facebook.presto.testing.mysql.EmbeddedMySql5.<init>(EmbeddedMySql5.java:27)\r\n2025-01-07T07:32:04.7807765Z \tat com.facebook.presto.testing.mysql.TestingMySqlServer.<init>(TestingMySqlServer.java:38)\r\n2025-01-07T07:32:04.7808900Z \tat com.facebook.presto.testing.mysql.TestingMySqlServer.<init>(TestingMySqlServer.java:32)\r\n2025-01-07T07:32:04.7810052Z \tat com.facebook.presto.testing.mysql.TestingMySqlServer.<init>(TestingMySqlServer.java:26)\r\n2025-01-07T07:32:04.7811818Z \tat com.facebook.presto.functionNamespace.mysql.TestMySqlFunctionNamespaceManager.setup(TestMySqlFunctionNamespaceManager.java:98)\r\n2025-01-07T07:32:04.7813149Z \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n2025-01-07T07:32:04.7814053Z \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n2025-01-07T07:32:04.7815159Z \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n2025-01-07T07:32:04.7816101Z \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n2025-01-07T07:32:04.7817051Z \tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n2025-01-07T07:32:04.7818520Z \tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethodConsideringTimeout(MethodInvocationHelper.java:65)\r\n2025-01-07T07:32:04.7820030Z \tat org.testng.internal.invokers.ConfigInvoker.invokeConfigurationMethod(ConfigInvoker.java:381)\r\n2025-01-07T07:32:04.7821760Z \tat org.testng.internal.invokers.ConfigInvoker.invokeConfigurations(ConfigInvoker.java:319)\r\n2025-01-07T07:32:04.7823271Z \tat org.testng.internal.invokers.TestMethodWorker.invokeBeforeClassMethods(TestMethodWorker.java:178)\r\n2025-01-07T07:32:04.7824523Z \tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:122)\r\n2025-01-07T07:32:04.7825738Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n2025-01-07T07:32:04.7826913Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2025-01-07T07:32:04.7827620Z \tat java.lang.Thread.run(Thread.java:750)\r\n2025-01-07T07:32:04.7828862Z Caused by: io.airlift.command.CommandFailedException: [/tmp/testing-mysql-server3402480835468141148/bin/mysqld, --no-defaults, --initialize-insecure, --skip-sync-frm, --innodb-flush-method=nosync, --datadir, /tmp/testing-mysql-server3402480835468141148/data] exited with 127\r\n2025-01-07T07:32:04.7830423Z /tmp/testing-mysql-server3402480835468141148/bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory\r\n2025-01-07T07:32:04.7831052Z \r\n2025-01-07T07:32:04.7831518Z \tat io.airlift.command.ProcessCallable.call(ProcessCallable.java:64)\r\n2025-01-07T07:32:04.7844411Z \tat io.airlift.command.ProcessCallable.call(ProcessCallable.java:22)\r\n2025-01-07T07:32:04.7844858Z \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n2025-01-07T07:32:04.7845176Z \t... 3 more\r\n2025-01-07T07:32:04.7845280Z \r\n2025-01-07T07:32:04.8017084Z [INFO] \r\n2025-01-07T07:32:04.8017393Z [INFO] Results:\r\n2025-01-07T07:32:04.8017686Z [INFO] \r\n2025-01-07T07:32:04.8017948Z [ERROR] Failures: \r\n2025-01-07T07:32:04.8022550Z [ERROR]   TestMySqlFunctionNamespaceManager.setup:98 » Runtime io.airlift.command.CommandFailedException: [/tmp/testing-mysql-server3402480835468141148/bin/mysqld, --no-defaults, --initialize-insecure, --skip-sync-frm, --innodb-flush-method=nosync, --datadir, /tmp/testing-mysql-server3402480835468141148/data] exited with 127\r\n2025-01-07T07:32:04.8024680Z /tmp/testing-mysql-server3402480835468141148/bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory\r\n```",
    "issue_word_count": 715,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      ".github/workflows/test-other-modules.yml"
    ],
    "pr_changed_test_files": [
      ".github/workflows/test-other-modules.yml"
    ],
    "base_commit": "b230be13ab42ca44233b3197f2d3c85cb852c6d0",
    "head_commit": "3016827cdd8a81074359aff0aa2881c4e861b272",
    "repo_url": "https://github.com/prestodb/presto/pull/24333",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24333",
    "dockerfile": "",
    "pr_merged_at": "2025-01-08T17:30:26.000Z",
    "patch": "",
    "test_patch": "diff --git a/.github/workflows/test-other-modules.yml b/.github/workflows/test-other-modules.yml\nindex 3c3f84817a0d1..7b5ba858bdc4d 100644\n--- a/.github/workflows/test-other-modules.yml\n+++ b/.github/workflows/test-other-modules.yml\n@@ -53,6 +53,9 @@ jobs:\n           key: ${{ runner.os }}-maven-2-${{ hashFiles('**/pom.xml') }}\n           restore-keys: |\n             ${{ runner.os }}-maven-2-\n+      # Workaround for Ubuntu 24 and mysql to find the dependent library (https://github.com/prestodb/presto/issues/24327)\n+      - name: Create symlink for libaio.so.1\n+        run: sudo ln -s /usr/lib/x86_64-linux-gnu/libaio.so.1t64 /usr/lib/x86_64-linux-gnu/libaio.so.1\n       - name: Populate maven cache\n         if: steps.cache-maven.outputs.cache-hit != 'true'\n         run: ./mvnw de.qaware.maven:go-offline-maven-plugin:resolve-dependencies --no-transfer-progress && .github/bin/download_nodejs\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24326",
    "pr_id": 24326,
    "issue_id": 24325,
    "repo": "prestodb/presto",
    "problem_statement": "Metastore caching hides Iceberg table updates when they are performed by another engine\nThe Iceberg connector should not utilize the Hive metastore cache for any table metadata operations. Currently, the metastore cache can be enabled even when using the Iceberg connector, which violates the ACID contract.\r\n\r\n**Steps to reproduce the issue:**\r\n1. Create an Iceberg table in Presto.\r\n2. Insert rows into the table.\r\n3. Insert additional rows using another engine.\r\n\r\nAt this point, Presto will fail to read the newly inserted rows until the metastore cache is manually refreshed or expires, thereby breaching the ACID contract.\r\n\r\nThe ideal long-term solution would involve allowing caching of specific metadata, such as table statistics or the list of table names, which do not violate the ACID contract. However, as an immediate fix, we should disallow enabling the metastore cache when using the Iceberg connector.",
    "issue_word_count": 143,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveModule.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java"
    ],
    "base_commit": "e2bb5c207aa08aef0938e240dc05e3dbfc6fe732",
    "head_commit": "77c5c4971fdf46621357927a09ab5f6eaaa75987",
    "repo_url": "https://github.com/prestodb/presto/pull/24326",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24326",
    "dockerfile": "",
    "pr_merged_at": "2025-01-09T16:58:30.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 919c75d995e25..b35326cb27e2b 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -515,19 +515,7 @@ JMX queries to get the metrics and verify the cache usage::\n Metastore Cache\n ^^^^^^^^^^^^^^^\n \n-Metastore Cache only caches the schema, table, and table statistics. The table object cached in the `tableCache`\n-is only used for reading the table metadata location and table properties and, the rest of the table metadata\n-is fetched from the filesystem/object storage metadata location.\n-\n-.. note::\n-\n-    Metastore Cache would be applicable only for Hive Catalog in the Presto Iceberg connector.\n-\n-.. code-block:: none\n-\n-    hive.metastore-cache-ttl=2d\n-    hive.metastore-refresh-interval=3d\n-    hive.metastore-cache-maximum-size=10000000\n+Iceberg Connector does not support Metastore Caching.\n \n Extra Hidden Metadata Columns\n -----------------------------\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveModule.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveModule.java\nindex 522c313e88451..3663d1b9086db 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveModule.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveModule.java\n@@ -29,6 +29,7 @@\n import java.util.Optional;\n \n import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n+import static com.google.common.base.Preconditions.checkArgument;\n import static org.weakref.jmx.ObjectNames.generatedNameOf;\n import static org.weakref.jmx.guice.ExportBinder.newExporter;\n \n@@ -52,6 +53,10 @@ public void setup(Binder binder)\n         configBinder(binder).bindConfig(IcebergHiveTableOperationsConfig.class);\n \n         configBinder(binder).bindConfig(MetastoreClientConfig.class);\n+\n+        long metastoreCacheTtl = buildConfigObject(MetastoreClientConfig.class).getMetastoreCacheTtl().toMillis();\n+        checkArgument(metastoreCacheTtl == 0, \"In-memory hive metastore caching must not be enabled for Iceberg\");\n+\n         binder.bind(PartitionMutator.class).to(HivePartitionMutator.class).in(Scopes.SINGLETON);\n \n         binder.bind(MetastoreCacheStats.class).to(HiveMetastoreCacheStats.class).in(Scopes.SINGLETON);\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java\nnew file mode 100644\nindex 0000000000000..c6d22e05d7ea2\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergConnectorFactory.java\n@@ -0,0 +1,45 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.spi.connector.ConnectorFactory;\n+import com.facebook.presto.testing.TestingConnectorContext;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.Test;\n+\n+import java.util.Map;\n+\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+public class TestIcebergConnectorFactory\n+{\n+    @Test\n+    public void testCachingHiveMetastore()\n+    {\n+        Map<String, String> config = ImmutableMap.<String, String>builder()\n+                .put(\"hive.metastore.uri\", \"thrift://localhost:9083\")\n+                .put(\"hive.metastore-cache-ttl\", \"10m\")\n+                .buildOrThrow();\n+\n+        assertThatThrownBy(() -> createConnector(config))\n+                .hasMessageContaining(\"In-memory hive metastore caching must not be enabled for Iceberg\");\n+    }\n+\n+    private static void createConnector(Map<String, String> config)\n+    {\n+        ConnectorFactory factory = new IcebergConnectorFactory();\n+        factory.create(\"iceberg-test\", config, new TestingConnectorContext())\n+                .shutdown();\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24292",
    "pr_id": 24292,
    "issue_id": 24290,
    "repo": "prestodb/presto",
    "problem_statement": "Flaky test: TestIcebergMetadataListing.testTableColumnListing\nSee: https://github.com/prestodb/presto/actions/runs/12403254688/job/34626370903?pr=24091\r\n\r\nError stack:\r\n\r\n```\r\n[ERROR] com.facebook.presto.iceberg.TestIcebergMetadataListing.testTableColumnListing  Time elapsed: 1.146 s  <<< FAILURE!\r\njava.lang.AssertionError: \r\nFor query: \r\n SELECT table_name, column_name FROM iceberg.information_schema.columns WHERE table_schema = 'test_schema'\r\n actual column types:\r\n [varchar, varchar]\r\nexpected column types:\r\n [varchar, varchar]\r\n\r\nnot equal\r\nActual rows (6 of 6 extra rows shown, 11 rows in total):\r\n    [test_view2_renamed, _string]\r\n    [test_view2_renamed, _integer]\r\n    [iceberg_test_table, _string]\r\n    [iceberg_test_table, _integer]\r\n    [test_view_renamed, _string]\r\n    [test_view_renamed, _integer]\r\n\r\n\tat org.testng.Assert.fail(Assert.java:110)\r\n\tat com.facebook.presto.tests.QueryAssertions.assertEqualsIgnoreOrder(QueryAssertions.java:280)\r\n\tat com.facebook.presto.tests.QueryAssertions.assertQuery(QueryAssertions.java:233)\r\n\tat com.facebook.presto.tests.QueryAssertions.assertQuery(QueryAssertions.java:106)\r\n\tat com.facebook.presto.tests.AbstractTestQueryFramework.assertQuery(AbstractTestQueryFramework.java:174)\r\n\tat com.facebook.presto.iceberg.TestIcebergMetadataListing.testTableColumnListing(TestIcebergMetadataListing.java:113)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```",
    "issue_word_count": 281,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java"
    ],
    "base_commit": "737f5e17681a8cac07702901171e44406ccfc123",
    "head_commit": "565034dcb64c9b677ab52dd7f5f6970332110044",
    "repo_url": "https://github.com/prestodb/presto/pull/24292",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24292",
    "dockerfile": "",
    "pr_merged_at": "2024-12-21T03:47:54.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\nindex 4b18df2cacce7..0c7cb8e8b324c 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\n@@ -157,16 +157,18 @@ public void testTableValidation()\n     @Test\n     public void testRenameView()\n     {\n-        assertQuerySucceeds(\"CREATE TABLE iceberg.test_schema.iceberg_test_table (_string VARCHAR, _integer INTEGER)\");\n-        assertUpdate(\"CREATE VIEW iceberg.test_schema.test_view_to_be_renamed AS SELECT * FROM iceberg.test_schema.iceberg_test_table\");\n-        assertUpdate(\"ALTER VIEW IF EXISTS iceberg.test_schema.test_view_to_be_renamed RENAME TO iceberg.test_schema.test_view_renamed\");\n-        assertUpdate(\"CREATE VIEW iceberg.test_schema.test_view2_to_be_renamed AS SELECT * FROM iceberg.test_schema.iceberg_test_table\");\n-        assertUpdate(\"ALTER VIEW iceberg.test_schema.test_view2_to_be_renamed RENAME TO iceberg.test_schema.test_view2_renamed\");\n-        assertQuerySucceeds(\"SELECT * FROM iceberg.test_schema.test_view_renamed\");\n-        assertQuerySucceeds(\"SELECT * FROM iceberg.test_schema.test_view2_renamed\");\n-        assertUpdate(\"DROP VIEW iceberg.test_schema.test_view_renamed\");\n-        assertUpdate(\"DROP VIEW iceberg.test_schema.test_view2_renamed\");\n-        assertUpdate(\"DROP TABLE iceberg.test_schema.iceberg_test_table\");\n+        assertQuerySucceeds(\"CREATE SCHEMA iceberg.test_rename_view_schema\");\n+        assertQuerySucceeds(\"CREATE TABLE iceberg.test_rename_view_schema.iceberg_test_table (_string VARCHAR, _integer INTEGER)\");\n+        assertUpdate(\"CREATE VIEW iceberg.test_rename_view_schema.test_view_to_be_renamed AS SELECT * FROM iceberg.test_rename_view_schema.iceberg_test_table\");\n+        assertUpdate(\"ALTER VIEW IF EXISTS iceberg.test_rename_view_schema.test_view_to_be_renamed RENAME TO iceberg.test_rename_view_schema.test_view_renamed\");\n+        assertUpdate(\"CREATE VIEW iceberg.test_rename_view_schema.test_view2_to_be_renamed AS SELECT * FROM iceberg.test_rename_view_schema.iceberg_test_table\");\n+        assertUpdate(\"ALTER VIEW iceberg.test_rename_view_schema.test_view2_to_be_renamed RENAME TO iceberg.test_rename_view_schema.test_view2_renamed\");\n+        assertQuerySucceeds(\"SELECT * FROM iceberg.test_rename_view_schema.test_view_renamed\");\n+        assertQuerySucceeds(\"SELECT * FROM iceberg.test_rename_view_schema.test_view2_renamed\");\n+        assertUpdate(\"DROP VIEW iceberg.test_rename_view_schema.test_view_renamed\");\n+        assertUpdate(\"DROP VIEW iceberg.test_rename_view_schema.test_view2_renamed\");\n+        assertUpdate(\"DROP TABLE iceberg.test_rename_view_schema.iceberg_test_table\");\n+        assertQuerySucceeds(\"DROP SCHEMA IF EXISTS iceberg.test_rename_view_schema\");\n     }\n     @Test\n     public void testRenameViewIfNotExists()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24277",
    "pr_id": 24277,
    "issue_id": 24278,
    "repo": "prestodb/presto",
    "problem_statement": "Add support for row filtering and column masking access control\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\r\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\r\n\r\nI would like to discuss adding row filtering and column masking to access control as part of governance requirements. This has been discussed several times before, but hasn't reached on consensus on implementation, see https://github.com/prestodb/presto/issues/20572,  https://github.com/prestodb/presto/pull/21913 and https://github.com/prestodb/presto/pull/18119.\r\n\r\nI propose using the following commits cherry-picked from TrinoDB as a basis:\r\n\r\n* Adding support for row filters https://github.com/trinodb/trino/commit/fae3147212fe58dac5ad782f0f13a17d9e962dd6\r\n* Adding support for column masking https://github.com/trinodb/trino/commit/7e0d88e6906cb91387d707f8daf9dc46095ad0bf\r\n\r\nAdditional followup commits will be added to retrieve a list of row filters and get column masks in bulk.\r\n\r\nThis implementation has existed for a while, it is straight-forward, has been in use and compatible with current production systems. There are existing extensions with Ranger https://github.com/trinodb/trino/blob/9499dc82f2d23314dbc76b0443bedd121e6400eb/plugin/trino-ranger/src/main/java/io/trino/plugin/ranger/RangerSystemAccessControl.java#L822, Opa https://github.com/trinodb/trino/blob/9499dc82f2d23314dbc76b0443bedd121e6400eb/plugin/trino-opa/src/main/java/io/trino/plugin/opa/OpaAccessControl.java#L732, etc. These could also be ported to Presto. \r\n\r\n## Expected Behavior or Use Case\r\n<!--- Tell us how it should work -->\r\n\r\n## Presto Component, Service, or Connector\r\n<!--- Tell us to which service or component this request is related to -->\r\n\r\nAccess control SPIs to add interfaces to get row filters and column masks. Changes required in Presto main to apply filters/masks to queries.\r\n\r\n## Possible Implementation\r\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\r\n\r\nBelow are the major changes to Presto from this implementation:\r\n\r\n## Changes to SPI\r\n\r\nThe major changes to the SPI are for access control to add interfaces for getting row filters and column masks\r\n\r\n```java\r\n    /**\r\n     * Get row filters associated with the given table and identity.\r\n     * <p>\r\n     * Each filter must be a scalar SQL expression of boolean type over the columns in the table.\r\n     *\r\n     * @return the list of filters, or empty list if not applicable\r\n     */\r\n    default List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\r\n    {\r\n        return Collections.emptyList();\r\n    }\r\n\r\n    /**\r\n     * Bulk method for getting column masks for a subset of columns in a table.\r\n     * <p>\r\n     * Each mask must be a scalar SQL expression of a type coercible to the type of the column being masked. The expression\r\n     * must be written in terms of columns in the table.\r\n     *\r\n     * @return a mapping from columns to masks, or an empty map if not applicable. The keys of the return Map are a subset of {@code columns}.\r\n     */\r\n    default Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\r\n    {\r\n        return Collections.emptyMap();\r\n    }\r\n```\r\n\r\nClass `ViewExpression` to hold the filter/mask expression\r\n\r\n```java\r\npublic ViewExpression(String identity, Optional<String> catalog, Optional<String> schema, String expression)\r\n```\r\n\r\n## Changes to Presto main\r\n\r\nThe major changes to presto-main are done in `StatementAnalyzer.java` to retrieve filter/masks from access control during the analysis phase and translate the filter or mask into an `Expression`.\r\n\r\nThen in `RelationPlanner.java` the filter/mask `Expression`s are applied during a rewrite of the plan.\r\n\r\n\r\n## Draft PR with cherry-picked commits\r\nhttps://github.com/prestodb/presto/pull/24277\r\n\r\n\r\n## Alternate design considered\r\n\r\nAn alternative way to rewrite the query to apply column masks and row filters is to use the existing SPI for connector plan optimization. This allows the query plan to be rewritten during the optimization phase. This approach has been discussed before and the major downside is that each connector would need to add the feature to enable row filtering and column masking, making it not as centralized as the design above.\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\nGovernance is required in most production systems that include the need to filter and mask sensitive data. Presto should have  this functionality built-in.",
    "issue_word_count": 699,
    "test_files_count": 6,
    "non_test_files_count": 18,
    "pr_changed_files": [
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/Analysis.java",
      "presto-docs/src/main/sphinx/develop/system-access-control.rst",
      "presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java",
      "presto-main-base/src/main/java/com/facebook/presto/security/AccessControlManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java",
      "presto-main-base/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/RelationPlanner.java",
      "presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestAccessControlFiltersMasks.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/TestColumnMask.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/TestRowFilter.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/ViewExpression.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestAccessControlFiltersMasks.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/TestColumnMask.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/query/TestRowFilter.java"
    ],
    "base_commit": "e5998e84053f7ba9cb81ae2be7efd91339d0f646",
    "head_commit": "5af314f4dc3e490e5b4cf839963d7ac6eded8d0d",
    "repo_url": "https://github.com/prestodb/presto/pull/24277",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24277",
    "dockerfile": "",
    "pr_merged_at": "2025-04-28T20:29:31.000Z",
    "patch": "diff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/Analysis.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/Analysis.java\nindex ade11574c3974..99fc10ed0a4c8 100644\n--- a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/Analysis.java\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/Analysis.java\n@@ -52,16 +52,19 @@\n import com.facebook.presto.sql.tree.SubqueryExpression;\n import com.facebook.presto.sql.tree.Table;\n import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.HashMultiset;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.ListMultimap;\n import com.google.common.collect.Multimap;\n+import com.google.common.collect.Multiset;\n \n import javax.annotation.Nullable;\n import javax.annotation.concurrent.Immutable;\n \n import java.util.ArrayDeque;\n+import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Deque;\n import java.util.HashMap;\n@@ -70,6 +73,7 @@\n import java.util.LinkedHashSet;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n \n@@ -157,6 +161,12 @@ public class Analysis\n \n     private final Map<NodeRef<QuerySpecification>, List<GroupingOperation>> groupingOperations = new LinkedHashMap<>();\n \n+    private final Multiset<RowFilterScopeEntry> rowFilterScopes = HashMultiset.create();\n+    private final Map<NodeRef<Table>, List<Expression>> rowFilters = new LinkedHashMap<>();\n+\n+    private final Multiset<ColumnMaskScopeEntry> columnMaskScopes = HashMultiset.create();\n+    private final Map<NodeRef<Table>, Map<String, Expression>> columnMasks = new LinkedHashMap<>();\n+\n     // for create table\n     private Optional<QualifiedObjectName> createTableDestination = Optional.empty();\n     private Map<String, Expression> createTableProperties = ImmutableMap.of();\n@@ -994,6 +1004,59 @@ public Map<FunctionKind, Set<String>> getInvokedFunctions()\n         return functionMap.entrySet().stream().collect(toImmutableMap(Map.Entry::getKey, entry -> ImmutableSet.copyOf(entry.getValue())));\n     }\n \n+    public boolean hasRowFilter(QualifiedObjectName table, String identity)\n+    {\n+        return rowFilterScopes.contains(new RowFilterScopeEntry(table, identity));\n+    }\n+\n+    public void registerTableForRowFiltering(QualifiedObjectName table, String identity)\n+    {\n+        rowFilterScopes.add(new RowFilterScopeEntry(table, identity));\n+    }\n+\n+    public void unregisterTableForRowFiltering(QualifiedObjectName table, String identity)\n+    {\n+        rowFilterScopes.remove(new RowFilterScopeEntry(table, identity));\n+    }\n+\n+    public void addRowFilter(Table table, Expression filter)\n+    {\n+        rowFilters.computeIfAbsent(NodeRef.of(table), node -> new ArrayList<>())\n+                .add(filter);\n+    }\n+\n+    public List<Expression> getRowFilters(Table node)\n+    {\n+        return rowFilters.getOrDefault(NodeRef.of(node), ImmutableList.of());\n+    }\n+\n+    public boolean hasColumnMask(QualifiedObjectName table, String column, String identity)\n+    {\n+        return columnMaskScopes.contains(new ColumnMaskScopeEntry(table, column, identity));\n+    }\n+\n+    public void registerTableForColumnMasking(QualifiedObjectName table, String column, String identity)\n+    {\n+        columnMaskScopes.add(new ColumnMaskScopeEntry(table, column, identity));\n+    }\n+\n+    public void unregisterTableForColumnMasking(QualifiedObjectName table, String column, String identity)\n+    {\n+        columnMaskScopes.remove(new ColumnMaskScopeEntry(table, column, identity));\n+    }\n+\n+    public void addColumnMask(Table table, String column, Expression mask)\n+    {\n+        Map<String, Expression> masks = columnMasks.computeIfAbsent(NodeRef.of(table), node -> new LinkedHashMap<>());\n+        checkArgument(!masks.containsKey(column), \"Mask already exists for column %s\", column);\n+        masks.put(column, mask);\n+    }\n+\n+    public Map<String, Expression> getColumnMasks(Table table)\n+    {\n+        return columnMasks.getOrDefault(NodeRef.of(table), ImmutableMap.of());\n+    }\n+\n     @Immutable\n     public static final class Insert\n     {\n@@ -1177,4 +1240,71 @@ public boolean isFromView()\n             return isFromView;\n         }\n     }\n+\n+    private static class RowFilterScopeEntry\n+    {\n+        private final QualifiedObjectName table;\n+        private final String identity;\n+\n+        public RowFilterScopeEntry(QualifiedObjectName table, String identity)\n+        {\n+            this.table = requireNonNull(table, \"table is null\");\n+            this.identity = requireNonNull(identity, \"identity is null\");\n+        }\n+\n+        @Override\n+        public boolean equals(Object o)\n+        {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            RowFilterScopeEntry that = (RowFilterScopeEntry) o;\n+            return table.equals(that.table) &&\n+                    identity.equals(that.identity);\n+        }\n+\n+        @Override\n+        public int hashCode()\n+        {\n+            return Objects.hash(table, identity);\n+        }\n+    }\n+\n+    private static class ColumnMaskScopeEntry\n+    {\n+        private final QualifiedObjectName table;\n+        private final String column;\n+        private final String identity;\n+\n+        public ColumnMaskScopeEntry(QualifiedObjectName table, String column, String identity)\n+        {\n+            this.table = requireNonNull(table, \"table is null\");\n+            this.column = requireNonNull(column, \"column is null\");\n+            this.identity = requireNonNull(identity, \"identity is null\");\n+        }\n+\n+        @Override\n+        public boolean equals(Object o)\n+        {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            ColumnMaskScopeEntry that = (ColumnMaskScopeEntry) o;\n+            return table.equals(that.table) &&\n+                    column.equals(that.column) &&\n+                    identity.equals(that.identity);\n+        }\n+\n+        @Override\n+        public int hashCode()\n+        {\n+            return Objects.hash(table, column, identity);\n+        }\n+    }\n }\n\ndiff --git a/presto-docs/src/main/sphinx/develop/system-access-control.rst b/presto-docs/src/main/sphinx/develop/system-access-control.rst\nindex 767cfa8e6c636..774cb782b966f 100644\n--- a/presto-docs/src/main/sphinx/develop/system-access-control.rst\n+++ b/presto-docs/src/main/sphinx/develop/system-access-control.rst\n@@ -29,6 +29,16 @@ name which is used by the administrator in a Presto configuration.\n The implementation of ``SystemAccessControl`` and ``SystemAccessControlFactory``\n must be wrapped as a plugin and installed on the Presto cluster.\n \n+Row Filters and Column Masks\n+----------------------------\n+\n+The access control implementation can optionally provide row filters and column masks to\n+control viewing of specific rows or mask sensitive values in columns. The filters\n+and masks are retrieved per table from the given ``Identity``, schema, table, and\n+column names. The returned filters and masks will be in the form of a ``ViewExpression``\n+that is then applied to the query plan before execution. Filters and masks can also be\n+supplied at the connector level from a ``ConnectorAccessControl`` implementation.\n+\n Configuration\n -------------\n \n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java b/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\nindex d1b77eb6d685d..59c2fe40a9a3a 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\n@@ -19,6 +19,7 @@\n import com.facebook.presto.hive.TransactionalMetadata;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n import com.facebook.presto.hive.metastore.Table;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorAccessControl;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n@@ -26,9 +27,13 @@\n import com.facebook.presto.spi.security.ConnectorIdentity;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n \n import javax.inject.Inject;\n \n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n@@ -286,4 +291,16 @@ public void checkCanAddConstraint(ConnectorTransactionHandle transaction, Connec\n             denyAddConstraint(tableName.toString());\n         }\n     }\n+\n+    @Override\n+    public List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n }\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java b/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\nindex 682fa482c1ae6..5f225ba0a094c 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\n@@ -21,6 +21,7 @@\n import com.facebook.presto.hive.metastore.Database;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorAccessControl;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n@@ -30,9 +31,13 @@\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.RoleGrant;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n \n import javax.inject.Inject;\n \n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n@@ -702,6 +707,18 @@ public void checkCanShowRoleGrants(ConnectorTransactionHandle transactionHandle,\n     {\n     }\n \n+    @Override\n+    public List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n+\n     private boolean isAdmin(ConnectorTransactionHandle transaction, ConnectorIdentity identity, MetastoreContext metastoreContext)\n     {\n         return getMetastore(transaction)\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/security/AccessControlManager.java b/presto-main-base/src/main/java/com/facebook/presto/security/AccessControlManager.java\nindex 468d7d0ed2f3b..f618ff1fbabe7 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/security/AccessControlManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/security/AccessControlManager.java\n@@ -20,6 +20,7 @@\n import com.facebook.presto.common.Subfield;\n import com.facebook.presto.common.transaction.TransactionId;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.SchemaTableName;\n@@ -33,8 +34,10 @@\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.SystemAccessControl;\n import com.facebook.presto.spi.security.SystemAccessControlFactory;\n+import com.facebook.presto.spi.security.ViewExpression;\n import com.facebook.presto.transaction.TransactionManager;\n import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import org.weakref.jmx.Managed;\n@@ -55,6 +58,7 @@\n import java.util.concurrent.atomic.AtomicReference;\n \n import static com.facebook.presto.metadata.MetadataUtil.toSchemaTableName;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_COLUMN_MASK;\n import static com.facebook.presto.spi.StandardErrorCode.SERVER_STARTING_UP;\n import static com.facebook.presto.util.PropertiesUtil.loadProperties;\n import static com.google.common.base.Preconditions.checkArgument;\n@@ -798,6 +802,54 @@ public void checkCanAddConstraints(TransactionId transactionId, Identity identit\n         }\n     }\n \n+    @Override\n+    public List<ViewExpression> getRowFilters(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName)\n+    {\n+        requireNonNull(transactionId, \"transactionId is null\");\n+        requireNonNull(identity, \"identity is null\");\n+        requireNonNull(tableName, \"catalogName is null\");\n+\n+        ImmutableList.Builder<ViewExpression> filters = ImmutableList.builder();\n+        CatalogAccessControlEntry entry = getConnectorAccessControl(transactionId, tableName.getCatalogName());\n+        if (entry != null) {\n+            entry.getAccessControl().getRowFilters(entry.getTransactionHandle(transactionId), identity.toConnectorIdentity(tableName.getCatalogName()), context, toSchemaTableName(tableName))\n+                    .forEach(filters::add);\n+        }\n+\n+        systemAccessControl.get().getRowFilters(identity, context, toCatalogSchemaTableName(tableName))\n+                .forEach(filters::add);\n+\n+        return filters.build();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName, List<ColumnMetadata> columns)\n+    {\n+        requireNonNull(transactionId, \"transactionId is null\");\n+        requireNonNull(identity, \"identity is null\");\n+        requireNonNull(tableName, \"catalogName is null\");\n+        requireNonNull(columns, \"columns is null\");\n+\n+        ImmutableMap.Builder<ColumnMetadata, ViewExpression> columnMasksBuilder = ImmutableMap.builder();\n+\n+        // connector-provided masks take precedence over global masks\n+        CatalogAccessControlEntry entry = getConnectorAccessControl(transactionId, tableName.getCatalogName());\n+        if (entry != null) {\n+            Map<ColumnMetadata, ViewExpression> connectorMasks = entry.getAccessControl().getColumnMasks(entry.getTransactionHandle(transactionId), identity.toConnectorIdentity(tableName.getCatalogName()), context, toSchemaTableName(tableName), columns);\n+            columnMasksBuilder.putAll(connectorMasks);\n+        }\n+\n+        Map<ColumnMetadata, ViewExpression> systemMasks = systemAccessControl.get().getColumnMasks(identity, context, toCatalogSchemaTableName(tableName), columns);\n+        columnMasksBuilder.putAll(systemMasks);\n+\n+        try {\n+            return columnMasksBuilder.buildOrThrow();\n+        }\n+        catch (IllegalArgumentException exception) {\n+            throw new PrestoException(INVALID_COLUMN_MASK, \"Multiple masks for the same column found\", exception);\n+        }\n+    }\n+\n     private CatalogAccessControlEntry getConnectorAccessControl(TransactionId transactionId, String catalogName)\n     {\n         return transactionManager.getOptionalCatalogMetadata(transactionId, catalogName)\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java b/presto-main-base/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\nindex a0e1ea6c315f2..65ae6cda52e7f 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.presto.common.CatalogSchemaName;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.security.AccessControlContext;\n import com.facebook.presto.spi.security.AuthorizedIdentity;\n@@ -23,6 +24,9 @@\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.SystemAccessControl;\n import com.facebook.presto.spi.security.SystemAccessControlFactory;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n \n import java.security.Principal;\n import java.security.cert.X509Certificate;\n@@ -231,4 +235,16 @@ public void checkCanDropConstraint(Identity identity, AccessControlContext conte\n     public void checkCanAddConstraint(Identity identity, AccessControlContext context, CatalogSchemaTableName table)\n     {\n     }\n+\n+    @Override\n+    public List<ViewExpression> getRowFilters(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java b/presto-main-base/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\nindex 199490f096f29..3e18f55fc289b 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\n@@ -19,6 +19,7 @@\n import com.facebook.presto.plugin.base.security.SchemaAccessControlRule;\n import com.facebook.presto.security.CatalogAccessControlRule.AccessMode;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.security.AccessControlContext;\n@@ -28,7 +29,9 @@\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.SystemAccessControl;\n import com.facebook.presto.spi.security.SystemAccessControlFactory;\n+import com.facebook.presto.spi.security.ViewExpression;\n import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import io.airlift.units.Duration;\n \n@@ -450,6 +453,18 @@ public void checkCanAddConstraint(Identity identity, AccessControlContext contex\n         }\n     }\n \n+    @Override\n+    public List<ViewExpression> getRowFilters(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n+\n     private boolean isSchemaOwner(Identity identity, CatalogSchemaName schema)\n     {\n         if (!canAccessCatalog(identity, schema.getCatalogName(), ALL)) {\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\nindex d52540a1660f0..7199ae33a3581 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n@@ -55,6 +55,7 @@\n import com.facebook.presto.spi.security.AllowAllAccessControl;\n import com.facebook.presto.spi.security.Identity;\n import com.facebook.presto.spi.security.ViewAccessControl;\n+import com.facebook.presto.spi.security.ViewExpression;\n import com.facebook.presto.sql.ExpressionUtils;\n import com.facebook.presto.sql.MaterializedViewUtils;\n import com.facebook.presto.sql.SqlFormatterUtil;\n@@ -205,8 +206,11 @@\n import static com.facebook.presto.metadata.MetadataUtil.createQualifiedObjectName;\n import static com.facebook.presto.metadata.MetadataUtil.getConnectorIdOrThrow;\n import static com.facebook.presto.metadata.MetadataUtil.toSchemaTableName;\n+import static com.facebook.presto.spi.StandardErrorCode.DATATYPE_MISMATCH;\n import static com.facebook.presto.spi.StandardErrorCode.INVALID_ARGUMENTS;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_COLUMN_MASK;\n import static com.facebook.presto.spi.StandardErrorCode.INVALID_FUNCTION_ARGUMENT;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_ROW_FILTER;\n import static com.facebook.presto.spi.StandardWarningCode.PERFORMANCE_WARNING;\n import static com.facebook.presto.spi.StandardWarningCode.REDUNDANT_ORDER_BY;\n import static com.facebook.presto.spi.analyzer.AccessControlRole.TABLE_CREATE;\n@@ -412,7 +416,16 @@ protected Scope visitInsert(Insert insert, Optional<Scope> scope)\n \n             analysis.addAccessControlCheckForTable(TABLE_INSERT, new AccessControlInfoForTable(accessControl, session.getIdentity(), session.getTransactionId(), session.getAccessControlContext(), targetTable));\n \n+            if (!accessControl.getRowFilters(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), targetTable).isEmpty()) {\n+                throw new SemanticException(NOT_SUPPORTED, insert, \"Insert into table with row filter is not supported\");\n+            }\n+\n             List<ColumnMetadata> columnsMetadata = tableColumnsMetadata.getColumnsMetadata();\n+\n+            if (!accessControl.getColumnMasks(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), targetTable, columnsMetadata).isEmpty()) {\n+                throw new SemanticException(NOT_SUPPORTED, insert, \"Insert into table with column masks is not supported\");\n+            }\n+\n             List<String> tableColumns = columnsMetadata.stream()\n                     .filter(column -> !column.isHidden())\n                     .map(ColumnMetadata::getName)\n@@ -600,6 +613,16 @@ protected Scope visitDelete(Delete node, Optional<Scope> scope)\n \n             analysis.addAccessControlCheckForTable(TABLE_DELETE, new AccessControlInfoForTable(accessControl, session.getIdentity(), session.getTransactionId(), session.getAccessControlContext(), tableName));\n \n+            if (!accessControl.getRowFilters(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), tableName).isEmpty()) {\n+                throw new SemanticException(NOT_SUPPORTED, node, \"Delete from table with row filter is not supported\");\n+            }\n+\n+            TableColumnMetadata tableColumnsMetadata = getTableColumnsMetadata(session, metadataResolver, analysis.getMetadataHandle(), tableName);\n+            List<ColumnMetadata> columnsMetadata = tableColumnsMetadata.getColumnsMetadata();\n+            if (!accessControl.getColumnMasks(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), tableName, columnsMetadata).isEmpty()) {\n+                throw new SemanticException(NOT_SUPPORTED, node, \"Delete from table with column mask is not supported\");\n+            }\n+\n             return createAndAssignScope(node, scope, Field.newUnqualified(node.getLocation(), \"rows\", BIGINT));\n         }\n \n@@ -1341,6 +1364,7 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n             }\n \n             TableColumnMetadata tableColumnsMetadata = getTableColumnsMetadata(session, metadataResolver, analysis.getMetadataHandle(), name);\n+            List<ColumnMetadata> columnsMetadata = tableColumnsMetadata.getColumnsMetadata();\n             Optional<TableHandle> tableHandle = getTableHandle(tableColumnsMetadata, table, name, scope);\n \n             Map<String, ColumnHandle> columnHandles = tableColumnsMetadata.getColumnHandles();\n@@ -1348,7 +1372,7 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n             // TODO: discover columns lazily based on where they are needed (to support connectors that can't enumerate all tables)\n             ImmutableList.Builder<Field> fields = ImmutableList.builder();\n \n-            for (ColumnMetadata column : tableColumnsMetadata.getColumnsMetadata()) {\n+            for (ColumnMetadata column : columnsMetadata) {\n                 Field field = Field.newQualified(\n                         Optional.empty(),\n                         table.getName(),\n@@ -1366,6 +1390,23 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n \n             analysis.registerTable(table, tableHandle.get());\n \n+            List<Field> outputFields = fields.build();\n+\n+            Scope accessControlScope = Scope.builder()\n+                    .withRelationType(RelationId.anonymous(), new RelationType(outputFields))\n+                    .build();\n+\n+            Map<ColumnMetadata, ViewExpression> masks = accessControl.getColumnMasks(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), name, columnsMetadata);\n+\n+            for (Map.Entry<ColumnMetadata, ViewExpression> maskEntry : masks.entrySet()) {\n+                analyzeColumnMask(session.getIdentity().getUser(), table, name, maskEntry.getKey(), accessControlScope, maskEntry.getValue());\n+            }\n+\n+            accessControl.getRowFilters(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), name)\n+                    .forEach(filter -> analyzeRowFilter(session.getIdentity().getUser(), table, name, accessControlScope, filter));\n+\n+            analysis.registerTable(table, tableHandle.get());\n+\n             if (statement instanceof RefreshMaterializedView) {\n                 Table view = ((RefreshMaterializedView) statement).getTarget();\n                 if (!table.equals(view) && !analysis.hasTableRegisteredForMaterializedView(view, table)) {\n@@ -1379,7 +1420,7 @@ protected Scope visitTable(Table table, Optional<Scope> scope)\n                 }\n             }\n \n-            return createAndAssignScope(table, scope, fields.build());\n+            return createAndAssignScope(table, scope, outputFields);\n         }\n \n         private Optional<TableHandle> getTableHandle(TableColumnMetadata tableColumnsMetadata, Table table, QualifiedObjectName name, Optional<Scope> scope)\n@@ -2874,21 +2915,8 @@ private RelationType analyzeView(Query query, QualifiedObjectName name, Optional\n                     viewAccessControl = accessControl;\n                 }\n \n-                Session.SessionBuilder viewSessionBuilder = Session.builder(metadata.getSessionPropertyManager())\n-                        .setQueryId(session.getQueryId())\n-                        .setTransactionId(session.getTransactionId().orElse(null))\n-                        .setIdentity(identity)\n-                        .setSource(session.getSource().orElse(null))\n-                        .setCatalog(catalog.orElse(null))\n-                        .setSchema(schema.orElse(null))\n-                        .setTimeZoneKey(session.getTimeZoneKey())\n-                        .setLocale(session.getLocale())\n-                        .setRemoteUserAddress(session.getRemoteUserAddress().orElse(null))\n-                        .setUserAgent(session.getUserAgent().orElse(null))\n-                        .setClientInfo(session.getClientInfo().orElse(null))\n-                        .setStartTime(session.getStartTime());\n-                session.getConnectorProperties().forEach((connectorId, properties) -> properties.forEach((k, v) -> viewSessionBuilder.setConnectionProperty(connectorId, k, v)));\n-                Session viewSession = viewSessionBuilder.build();\n+                Session viewSession = createViewSession(catalog, schema, identity);\n+\n                 StatementAnalyzer analyzer = new StatementAnalyzer(analysis, metadata, sqlParser, viewAccessControl, viewSession, warningCollector);\n                 Scope queryScope = analyzer.analyze(query, Scope.create());\n                 return queryScope.getRelationType().withAlias(name.getObjectName(), null);\n@@ -2899,6 +2927,25 @@ private RelationType analyzeView(Query query, QualifiedObjectName name, Optional\n             }\n         }\n \n+        private Session createViewSession(Optional<String> catalog, Optional<String> schema, Identity identity)\n+        {\n+            Session.SessionBuilder viewSessionBuilder = Session.builder(metadata.getSessionPropertyManager())\n+                    .setQueryId(session.getQueryId())\n+                    .setTransactionId(session.getTransactionId().orElse(null))\n+                    .setIdentity(identity)\n+                    .setSource(session.getSource().orElse(null))\n+                    .setCatalog(catalog.orElse(null))\n+                    .setSchema(schema.orElse(null))\n+                    .setTimeZoneKey(session.getTimeZoneKey())\n+                    .setLocale(session.getLocale())\n+                    .setRemoteUserAddress(session.getRemoteUserAddress().orElse(null))\n+                    .setUserAgent(session.getUserAgent().orElse(null))\n+                    .setClientInfo(session.getClientInfo().orElse(null))\n+                    .setStartTime(session.getStartTime());\n+            session.getConnectorProperties().forEach((connectorId, properties) -> properties.forEach((k, v) -> viewSessionBuilder.setConnectionProperty(connectorId, k, v)));\n+            return viewSessionBuilder.build();\n+        }\n+\n         private Query parseView(String view, QualifiedObjectName name, Node node)\n         {\n             try {\n@@ -2941,6 +2988,112 @@ private ExpressionAnalysis analyzeExpression(Expression expression, Scope scope)\n                     warningCollector);\n         }\n \n+        private void analyzeRowFilter(String currentIdentity, Table table, QualifiedObjectName name, Scope scope, ViewExpression filter)\n+        {\n+            if (analysis.hasRowFilter(name, currentIdentity)) {\n+                throw new PrestoException(INVALID_ROW_FILTER, format(\"Row filter for '%s' is recursive\", name), null);\n+            }\n+\n+            Expression expression;\n+            try {\n+                expression = sqlParser.createExpression(filter.getExpression(), createParsingOptions(session));\n+            }\n+            catch (ParsingException e) {\n+                throw new PrestoException(INVALID_ROW_FILTER, format(\"Invalid row filter for '%s': %s\", name, e.getErrorMessage()), e);\n+            }\n+\n+            analysis.registerTableForRowFiltering(name, currentIdentity);\n+            ExpressionAnalysis expressionAnalysis;\n+            try {\n+                expressionAnalysis = ExpressionAnalyzer.analyzeExpression(\n+                        createViewSession(filter.getCatalog(), filter.getSchema(), new Identity(filter.getIdentity(), Optional.empty())), // TODO: path should be included in row filter\n+                        metadata,\n+                        accessControl,\n+                        sqlParser,\n+                        scope,\n+                        analysis,\n+                        expression,\n+                        warningCollector);\n+            }\n+            catch (PrestoException e) {\n+                throw new PrestoException(e::getErrorCode, format(\"Invalid row filter for '%s: %s'\", name, e.getMessage()), e);\n+            }\n+            finally {\n+                analysis.unregisterTableForRowFiltering(name, currentIdentity);\n+            }\n+\n+            verifyNoAggregateWindowOrGroupingFunctions(analysis.getFunctionHandles(), functionAndTypeResolver, expression, format(\"Row filter for '%s'\", name));\n+\n+            analysis.recordSubqueries(expression, expressionAnalysis);\n+\n+            Type actualType = expressionAnalysis.getType(expression);\n+            if (!actualType.equals(BOOLEAN)) {\n+                if (!metadata.getFunctionAndTypeManager().canCoerce(actualType, BOOLEAN)) {\n+                    throw new PrestoException(DATATYPE_MISMATCH, format(\"Expected row filter for '%s' to be of type BOOLEAN, but was %s\", name, actualType), null);\n+                }\n+\n+                analysis.addCoercion(expression, BOOLEAN, false);\n+            }\n+\n+            analysis.addRowFilter(table, expression);\n+        }\n+\n+        private void analyzeColumnMask(String currentIdentity, Table table, QualifiedObjectName tableName, ColumnMetadata columnMetadata, Scope scope, ViewExpression mask)\n+        {\n+            String column = columnMetadata.getName();\n+            if (analysis.hasColumnMask(tableName, column, currentIdentity)) {\n+                throw new PrestoException(INVALID_COLUMN_MASK, format(\"Column mask for '%s.%s' is recursive\", tableName, column), null);\n+            }\n+\n+            Expression expression;\n+            try {\n+                expression = sqlParser.createExpression(mask.getExpression(), createParsingOptions(session));\n+            }\n+            catch (ParsingException e) {\n+                throw new PrestoException(INVALID_COLUMN_MASK, format(\"Invalid column mask for '%s.%s': %s\", tableName, column, e.getErrorMessage()), e);\n+            }\n+\n+            ExpressionAnalysis expressionAnalysis;\n+            analysis.registerTableForColumnMasking(tableName, column, currentIdentity);\n+            try {\n+                expressionAnalysis = ExpressionAnalyzer.analyzeExpression(\n+                        createViewSession(mask.getCatalog(), mask.getSchema(), new Identity(mask.getIdentity(), Optional.empty())), // TODO: path should be included in row filter\n+                        metadata,\n+                        accessControl,\n+                        sqlParser,\n+                        scope,\n+                        analysis,\n+                        expression,\n+                        warningCollector);\n+            }\n+            catch (PrestoException e) {\n+                throw new PrestoException(e::getErrorCode, format(\"Invalid column mask for '%s.%s: %s'\", tableName, column, e.getMessage()), e);\n+            }\n+            finally {\n+                analysis.unregisterTableForColumnMasking(tableName, column, currentIdentity);\n+            }\n+\n+            verifyNoAggregateWindowOrGroupingFunctions(analysis.getFunctionHandles(), functionAndTypeResolver, expression, format(\"Column mask for '%s.%s'\", table.getName(), column));\n+\n+            analysis.recordSubqueries(expression, expressionAnalysis);\n+\n+            Type expectedType = columnMetadata.getType();\n+            Type actualType = expressionAnalysis.getType(expression);\n+            if (!actualType.equals(expectedType)) {\n+                if (!metadata.getFunctionAndTypeManager().canCoerce(actualType, columnMetadata.getType())) {\n+                    throw new PrestoException(DATATYPE_MISMATCH, format(\"Expected column mask for '%s.%s' to be of type %s, but was %s\", tableName, column, columnMetadata.getType(), actualType), null);\n+                }\n+\n+                // TODO: this should be \"coercion.isTypeOnlyCoercion(actualType, expectedType)\", but type-only coercions are broken\n+                // due to the line \"changeType(value, returnType)\" in SqlToRowExpressionTranslator.visitCast. If there's an expression\n+                // like CAST(CAST(x AS VARCHAR(1)) AS VARCHAR(2)), it determines that the outer cast is type-only and converts the expression\n+                // to CAST(x AS VARCHAR(2)) by changing the type of the inner cast.\n+                analysis.addCoercion(expression, expectedType, false);\n+            }\n+\n+            analysis.addColumnMask(table, column, expression);\n+        }\n+\n         private List<Expression> descriptorToFields(Scope scope)\n         {\n             ImmutableList.Builder<Expression> builder = ImmutableList.builder();\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/RelationPlanner.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/RelationPlanner.java\nindex e260ea6b98d82..7e22aeb2ad5ff 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/RelationPlanner.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/RelationPlanner.java\n@@ -222,7 +222,68 @@ protected RelationPlan visitTable(Table node, SqlPlannerContext context)\n         PlanNode root = new TableScanNode(getSourceLocation(node.getLocation()), idAllocator.getNextId(), handle, outputVariables, columns.build(),\n                 tableConstraints, TupleDomain.all(), TupleDomain.all(), Optional.empty());\n \n-        return new RelationPlan(root, scope, outputVariables);\n+        RelationPlan tableScan = new RelationPlan(root, scope, outputVariables);\n+        tableScan = addRowFilters(node, tableScan, context);\n+        tableScan = addColumnMasks(node, tableScan, context);\n+        return tableScan;\n+    }\n+\n+    private RelationPlan addRowFilters(Table node, RelationPlan plan, SqlPlannerContext context)\n+    {\n+        PlanBuilder planBuilder = initializePlanBuilder(plan);\n+\n+        for (Expression filter : analysis.getRowFilters(node)) {\n+            planBuilder = subqueryPlanner.handleSubqueries(planBuilder, filter, filter, context);\n+\n+            planBuilder = planBuilder.withNewRoot(new FilterNode(\n+                    getSourceLocation(node.getLocation()),\n+                    idAllocator.getNextId(),\n+                    planBuilder.getRoot(),\n+                    rowExpression(planBuilder.rewrite(filter), context)));\n+        }\n+\n+        return new RelationPlan(planBuilder.getRoot(), plan.getScope(), plan.getFieldMappings());\n+    }\n+\n+    private RelationPlan addColumnMasks(Table table, RelationPlan plan, SqlPlannerContext context)\n+    {\n+        Map<String, Expression> columnMasks = analysis.getColumnMasks(table);\n+\n+        PlanBuilder planBuilder = initializePlanBuilder(plan);\n+        List<VariableReferenceExpression> mappings = plan.getFieldMappings();\n+        ImmutableList.Builder<VariableReferenceExpression> newMappings = ImmutableList.builder();\n+\n+        Assignments.Builder assignments = new Assignments.Builder();\n+        for (VariableReferenceExpression variableReferenceExpression : planBuilder.getRoot().getOutputVariables()) {\n+            assignments.put(variableReferenceExpression, rowExpression(new SymbolReference(variableReferenceExpression.getName()), context));\n+        }\n+\n+        for (int i = 0; i < plan.getDescriptor().getAllFieldCount(); i++) {\n+            Field field = plan.getDescriptor().getFieldByIndex(i);\n+\n+            VariableReferenceExpression fieldMapping;\n+            RowExpression rowExpression;\n+            if (field.getName().isPresent() && columnMasks.containsKey(field.getName().get())) {\n+                Expression mask = columnMasks.get(field.getName().get());\n+                planBuilder = subqueryPlanner.handleSubqueries(planBuilder, mask, mask, context);\n+                fieldMapping = newVariable(variableAllocator, field);\n+                rowExpression = rowExpression(planBuilder.rewrite(mask), context);\n+            }\n+            else {\n+                fieldMapping = mappings.get(i);\n+                rowExpression = rowExpression(createSymbolReference(fieldMapping), context);\n+            }\n+\n+            assignments.put(fieldMapping, rowExpression);\n+            newMappings.add(fieldMapping);\n+        }\n+\n+        planBuilder = planBuilder.withNewRoot(new ProjectNode(\n+                idAllocator.getNextId(),\n+                planBuilder.getRoot(),\n+                assignments.build()));\n+\n+        return new RelationPlan(planBuilder.getRoot(), plan.getScope(), newMappings.build());\n     }\n \n     @Override\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\nindex 30e99555fe432..8b9fccdaf5940 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.plugin.base.security;\n \n import com.facebook.presto.common.Subfield;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorAccessControl;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n@@ -21,7 +22,11 @@\n import com.facebook.presto.spi.security.ConnectorIdentity;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n \n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n@@ -210,4 +215,16 @@ public void checkCanDropConstraint(ConnectorTransactionHandle transactionHandle,\n     public void checkCanAddConstraint(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n     {\n     }\n+\n+    @Override\n+    public List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n }\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\nindex e1ee1b874815c..fc1f66c700c6f 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.presto.common.Subfield;\n import com.facebook.presto.plugin.base.security.TableAccessControlRule.TablePrivilege;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorAccessControl;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n@@ -23,6 +24,9 @@\n import com.facebook.presto.spi.security.ConnectorIdentity;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n \n import javax.inject.Inject;\n@@ -336,6 +340,18 @@ public void checkCanAddConstraint(ConnectorTransactionHandle transactionHandle,\n         }\n     }\n \n+    @Override\n+    public List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return ImmutableMap.of();\n+    }\n+\n     private boolean canSetSessionProperty(ConnectorIdentity identity, String property)\n     {\n         for (SessionPropertyAccessControlRule rule : sessionPropertyRules) {\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\nindex dd28ee9183d56..0987eaf8640bd 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.plugin.base.security;\n \n import com.facebook.presto.common.Subfield;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.connector.ConnectorAccessControl;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n@@ -21,7 +22,9 @@\n import com.facebook.presto.spi.security.ConnectorIdentity;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n+import com.facebook.presto.spi.security.ViewExpression;\n \n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n@@ -262,4 +265,16 @@ public void checkCanAddConstraint(ConnectorTransactionHandle transactionHandle,\n     {\n         delegate().checkCanAddConstraint(transactionHandle, identity, context, tableName);\n     }\n+\n+    @Override\n+    public List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return delegate().getRowFilters(transactionHandle, identity, context, tableName);\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return delegate().getColumnMasks(transactionHandle, identity, context, tableName, columns);\n+    }\n }\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\nindex 6e9cd6a91c715..6f616ad88bd67 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.presto.common.CatalogSchemaName;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.security.AccessControlContext;\n import com.facebook.presto.spi.security.AuthorizedIdentity;\n@@ -22,10 +23,12 @@\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.SystemAccessControl;\n+import com.facebook.presto.spi.security.ViewExpression;\n \n import java.security.Principal;\n import java.security.cert.X509Certificate;\n import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n import java.util.function.Supplier;\n@@ -253,4 +256,16 @@ public void checkCanAddConstraint(Identity identity, AccessControlContext contex\n     {\n         delegate().checkCanAddConstraint(identity, context, table);\n     }\n+\n+    @Override\n+    public List<ViewExpression> getRowFilters(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName)\n+    {\n+        return delegate().getRowFilters(identity, context, tableName);\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return delegate().getColumnMasks(identity, context, tableName, columns);\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java b/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\nindex 4de55e4aa38d6..df884221f3af0 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/StandardErrorCode.java\n@@ -75,6 +75,9 @@ public enum StandardErrorCode\n     INVALID_LIMIT_CLAUSE(0x0000_0031, USER_ERROR),\n     COLUMN_NOT_FOUND(0x0000_0032, USER_ERROR),\n     UNKNOWN_TYPE(0x0000_0033, USER_ERROR),\n+    INVALID_ROW_FILTER(0x0000_0034, USER_ERROR),\n+    INVALID_COLUMN_MASK(0x0000_0035, USER_ERROR),\n+    DATATYPE_MISMATCH(0x0000_0036, USER_ERROR),\n \n     GENERIC_INTERNAL_ERROR(0x0001_0000, INTERNAL_ERROR),\n     TOO_MANY_REQUESTS_FAILED(0x0001_0001, INTERNAL_ERROR, true),\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\nindex e5621d458c0b0..a4d6ac064b408 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\n@@ -14,12 +14,16 @@\n package com.facebook.presto.spi.connector;\n \n import com.facebook.presto.common.Subfield;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.security.AccessControlContext;\n import com.facebook.presto.spi.security.ConnectorIdentity;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n+import com.facebook.presto.spi.security.ViewExpression;\n \n+import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n@@ -405,4 +409,29 @@ default void checkCanAddConstraint(ConnectorTransactionHandle transactionHandle,\n     {\n         denyAddConstraint(tableName.toString());\n     }\n+\n+    /**\n+     * Get row filters associated with the given table and identity.\n+     * <p>\n+     * Each filter must be a scalar SQL expression of boolean type over the columns in the table.\n+     *\n+     * @return the list of filters, or empty list if not applicable\n+     */\n+    default List<ViewExpression> getRowFilters(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName)\n+    {\n+        return Collections.emptyList();\n+    }\n+\n+    /**\n+     * Bulk method for getting column masks for a subset of columns in a table.\n+     * <p>\n+     * Each mask must be a scalar SQL expression of a type coercible to the type of the column being masked. The expression\n+     * must be written in terms of columns in the table.\n+     *\n+     * @return a mapping from columns to masks, or an empty map if not applicable. The keys of the return Map are a subset of {@code columns}.\n+     */\n+    default Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return Collections.emptyMap();\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\nindex 131c0af1577d2..8041a8ed82397 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\n@@ -17,10 +17,12 @@\n import com.facebook.presto.common.QualifiedObjectName;\n import com.facebook.presto.common.Subfield;\n import com.facebook.presto.common.transaction.TransactionId;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n \n import java.security.Principal;\n import java.security.cert.X509Certificate;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n@@ -322,4 +324,29 @@ default AuthorizedIdentity selectAuthorizedIdentity(Identity identity, AccessCon\n      * @throws com.facebook.presto.spi.security.AccessDeniedException if not allowed\n      */\n     void checkCanAddConstraints(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName constraintName);\n+\n+    /**\n+     * Get row filters associated with the given table and identity.\n+     * <p>\n+     * Each filter must be a scalar SQL expression of boolean type over the columns in the table.\n+     *\n+     * @return the list of filters, or empty list if not applicable\n+     */\n+    default List<ViewExpression> getRowFilters(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName)\n+    {\n+        return Collections.emptyList();\n+    }\n+\n+    /**\n+     * Bulk method for getting column masks for a subset of columns in a table.\n+     * <p>\n+     * Each mask must be a scalar SQL expression of a type coercible to the type of the column being masked. The expression\n+     * must be written in terms of columns in the table.\n+     *\n+     * @return a mapping from columns to masks, or an empty map if not applicable. The keys of the return Map are a subset of {@code columns}.\n+     */\n+    default Map<ColumnMetadata, ViewExpression> getColumnMasks(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName, List<ColumnMetadata> columns)\n+    {\n+        return Collections.emptyMap();\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\nindex fc5e8cbf6805f..f0601674c7257 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\n@@ -15,12 +15,14 @@\n \n import com.facebook.presto.common.CatalogSchemaName;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.SchemaTableName;\n \n import java.security.Principal;\n import java.security.cert.X509Certificate;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n \n@@ -380,4 +382,29 @@ default void checkCanAddConstraint(Identity identity, AccessControlContext conte\n     {\n         denyAddConstraint(table.toString());\n     }\n+\n+    /**\n+     * Get row filters associated with the given table and identity.\n+     * <p>\n+     * Each filter must be a scalar SQL expression of boolean type over the columns in the table.\n+     *\n+     * @return a list of filters, or empty list if not applicable\n+     */\n+    default List<ViewExpression> getRowFilters(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName)\n+    {\n+        return Collections.emptyList();\n+    }\n+\n+    /**\n+     * Bulk method for getting column masks for a subset of columns in a table.\n+     * <p>\n+     * Each mask must be a scalar SQL expression of a type coercible to the type of the column being masked. The expression\n+     * must be written in terms of columns in the table.\n+     *\n+     * @return a mapping from columns to masks, or an empty map if not applicable. The keys of the return Map are a subset of {@code columns}.\n+     */\n+    default Map<ColumnMetadata, ViewExpression> getColumnMasks(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName, List<ColumnMetadata> columns)\n+    {\n+        return Collections.emptyMap();\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/ViewExpression.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/ViewExpression.java\nnew file mode 100644\nindex 0000000000000..53a253ebcf138\n--- /dev/null\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/ViewExpression.java\n@@ -0,0 +1,58 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spi.security;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class ViewExpression\n+{\n+    private final String identity;\n+    private final Optional<String> catalog;\n+    private final Optional<String> schema;\n+    private final String expression;\n+\n+    public ViewExpression(String identity, Optional<String> catalog, Optional<String> schema, String expression)\n+    {\n+        this.identity = requireNonNull(identity, \"identity is null\");\n+        this.catalog = requireNonNull(catalog, \"catalog is null\");\n+        this.schema = requireNonNull(schema, \"schema is null\");\n+        this.expression = requireNonNull(expression, \"expression is null\");\n+\n+        if (!catalog.isPresent() && schema.isPresent()) {\n+            throw new IllegalArgumentException(\"catalog must be present if schema is present\");\n+        }\n+    }\n+\n+    public String getIdentity()\n+    {\n+        return identity;\n+    }\n+\n+    public Optional<String> getCatalog()\n+    {\n+        return catalog;\n+    }\n+\n+    public Optional<String> getSchema()\n+    {\n+        return schema;\n+    }\n+\n+    public String getExpression()\n+    {\n+        return expression;\n+    }\n+}\n",
    "test_patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java b/presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\nindex fa2e681c1878d..9a7be061f767b 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\n@@ -19,16 +19,22 @@\n import com.facebook.presto.common.transaction.TransactionId;\n import com.facebook.presto.security.AccessControlManager;\n import com.facebook.presto.security.AllowAllSystemAccessControl;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.security.AccessControlContext;\n import com.facebook.presto.spi.security.Identity;\n+import com.facebook.presto.spi.security.ViewExpression;\n import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n \n import javax.inject.Inject;\n \n import java.security.Principal;\n+import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n import java.util.Optional;\n@@ -89,6 +95,8 @@ public class TestingAccessControlManager\n         extends AccessControlManager\n {\n     private final Set<TestingPrivilege> denyPrivileges = new HashSet<>();\n+    private final Map<RowFilterKey, List<ViewExpression>> rowFilters = new HashMap<>();\n+    private final Map<ColumnMaskKey, ViewExpression> columnMasks = new HashMap<>();\n \n     @Inject\n     public TestingAccessControlManager(TransactionManager transactionManager)\n@@ -115,6 +123,19 @@ public void deny(TestingPrivilege... deniedPrivileges)\n     public void reset()\n     {\n         denyPrivileges.clear();\n+        rowFilters.clear();\n+        columnMasks.clear();\n+    }\n+\n+    public void rowFilter(QualifiedObjectName table, String identity, ViewExpression filter)\n+    {\n+        rowFilters.computeIfAbsent(new RowFilterKey(identity, table), key -> new ArrayList<>())\n+                .add(filter);\n+    }\n+\n+    public void columnMask(QualifiedObjectName table, String column, String identity, ViewExpression mask)\n+    {\n+        columnMasks.put(new ColumnMaskKey(identity, table, column), mask);\n     }\n \n     @Override\n@@ -378,6 +399,29 @@ public void checkCanAddConstraints(TransactionId transactionId, Identity identit\n         super.checkCanAddConstraints(transactionId, identity, context, tableName);\n     }\n \n+    @Override\n+    public List<ViewExpression> getRowFilters(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName)\n+    {\n+        return rowFilters.getOrDefault(new RowFilterKey(identity.getUser(), tableName), ImmutableList.of());\n+    }\n+\n+    @Override\n+    public Map<ColumnMetadata, ViewExpression> getColumnMasks(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName tableName, List<ColumnMetadata> columns)\n+    {\n+        Map<ColumnMetadata, ViewExpression> superResult = super.getColumnMasks(transactionId, identity, context, tableName, columns);\n+        ImmutableMap.Builder<ColumnMetadata, ViewExpression> columnMaskBuilder = ImmutableMap.builder();\n+        for (ColumnMetadata column : columns) {\n+            ColumnMaskKey columnMaskKey = new ColumnMaskKey(identity.getUser(), tableName, column.getName());\n+            if (columnMasks.containsKey(columnMaskKey)) {\n+                columnMaskBuilder.put(column, columnMasks.get(columnMaskKey));\n+            }\n+            else if (superResult.containsKey(column)) {\n+                columnMaskBuilder.put(column, superResult.get(column));\n+            }\n+        }\n+        return columnMaskBuilder.buildOrThrow();\n+    }\n+\n     private boolean shouldDenyPrivilege(String userName, String entityName, TestingPrivilegeType type)\n     {\n         TestingPrivilege testPrivilege = privilege(userName, entityName, type);\n@@ -450,4 +494,71 @@ public String toString()\n                     .toString();\n         }\n     }\n+\n+    private static class RowFilterKey\n+    {\n+        private final String identity;\n+        private final QualifiedObjectName table;\n+\n+        public RowFilterKey(String identity, QualifiedObjectName table)\n+        {\n+            this.identity = requireNonNull(identity, \"identity is null\");\n+            this.table = requireNonNull(table, \"table is null\");\n+        }\n+\n+        @Override\n+        public boolean equals(Object o)\n+        {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            RowFilterKey that = (RowFilterKey) o;\n+            return identity.equals(that.identity) &&\n+                    table.equals(that.table);\n+        }\n+\n+        @Override\n+        public int hashCode()\n+        {\n+            return Objects.hash(identity, table);\n+        }\n+    }\n+\n+    private static class ColumnMaskKey\n+    {\n+        private final String identity;\n+        private final QualifiedObjectName table;\n+        private final String column;\n+\n+        public ColumnMaskKey(String identity, QualifiedObjectName table, String column)\n+        {\n+            this.identity = identity;\n+            this.table = table;\n+            this.column = column;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o)\n+        {\n+            if (this == o) {\n+                return true;\n+            }\n+            if (o == null || getClass() != o.getClass()) {\n+                return false;\n+            }\n+            ColumnMaskKey that = (ColumnMaskKey) o;\n+            return identity.equals(that.identity) &&\n+                    table.equals(that.table) &&\n+                    column.equals(that.column);\n+        }\n+\n+        @Override\n+        public int hashCode()\n+        {\n+            return Objects.hash(identity, table, column);\n+        }\n+    }\n }\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java b/presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\nindex 42853893f4c6f..6a1785eaf41ed 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\n@@ -24,6 +24,7 @@\n import com.facebook.presto.metadata.InMemoryNodeManager;\n import com.facebook.presto.metadata.MetadataManager;\n import com.facebook.presto.spi.CatalogSchemaTableName;\n+import com.facebook.presto.spi.ColumnMetadata;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.spi.QueryId;\n@@ -41,6 +42,7 @@\n import com.facebook.presto.spi.security.Privilege;\n import com.facebook.presto.spi.security.SystemAccessControl;\n import com.facebook.presto.spi.security.SystemAccessControlFactory;\n+import com.facebook.presto.spi.security.ViewExpression;\n import com.facebook.presto.testing.TestingConnectorContext;\n import com.facebook.presto.tpch.TpchConnectorFactory;\n import com.facebook.presto.transaction.TransactionManager;\n@@ -51,12 +53,15 @@\n \n import java.security.Principal;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n \n+import static com.facebook.presto.common.type.BigintType.BIGINT;\n import static com.facebook.presto.spi.ConnectorId.createInformationSchemaConnectorId;\n import static com.facebook.presto.spi.ConnectorId.createSystemTablesConnectorId;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_COLUMN_MASK;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyQueryIntegrityCheck;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectColumns;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectTable;\n@@ -66,6 +71,7 @@\n import static java.util.Objects.requireNonNull;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertThrows;\n+import static org.testng.Assert.expectThrows;\n import static org.testng.Assert.fail;\n \n public class TestAccessControlManager\n@@ -258,6 +264,78 @@ public void testDenySystemAccessControl()\n                 });\n     }\n \n+    @Test\n+    public void testColumnMaskOrdering()\n+    {\n+        CatalogManager catalogManager = new CatalogManager();\n+        TransactionManager transactionManager = createTestTransactionManager(catalogManager);\n+        AccessControlManager accessControlManager = new AccessControlManager(transactionManager);\n+\n+        accessControlManager.addSystemAccessControlFactory(new SystemAccessControlFactory() {\n+            @Override\n+            public String getName()\n+            {\n+                return \"test\";\n+            }\n+\n+            @Override\n+            public SystemAccessControl create(Map<String, String> config)\n+            {\n+                return new SystemAccessControl() {\n+                    @Override\n+                    public Map<ColumnMetadata, ViewExpression> getColumnMasks(Identity identity, AccessControlContext context, CatalogSchemaTableName tableName, List<ColumnMetadata> columns)\n+                    {\n+                        ImmutableMap.Builder<ColumnMetadata, ViewExpression> columnMaskBuilder = ImmutableMap.builder();\n+                        for (ColumnMetadata column : columns) {\n+                            columnMaskBuilder.put(column, new ViewExpression(\"user\", Optional.empty(), Optional.empty(), \"system mask\"));\n+                        }\n+                        return columnMaskBuilder.buildOrThrow();\n+                    }\n+\n+                    @Override\n+                    public void checkCanSetUser(Identity identity, AccessControlContext context, Optional<Principal> principal, String userName)\n+                    {\n+                    }\n+\n+                    @Override\n+                    public void checkQueryIntegrity(Identity identity, AccessControlContext context, String query)\n+                    {\n+                    }\n+\n+                    @Override\n+                    public void checkCanSetSystemSessionProperty(Identity identity, AccessControlContext context, String propertyName)\n+                    {\n+                    }\n+                };\n+            }\n+        });\n+        accessControlManager.setSystemAccessControl(\"test\", ImmutableMap.of());\n+\n+        ConnectorId connectorId = registerBogusConnector(catalogManager, transactionManager, accessControlManager, \"catalog\");\n+        accessControlManager.addCatalogAccessControl(connectorId, new ConnectorAccessControl() {\n+            @Override\n+            public Map<ColumnMetadata, ViewExpression> getColumnMasks(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName tableName, List<ColumnMetadata> columns)\n+            {\n+                ImmutableMap.Builder<ColumnMetadata, ViewExpression> columnMaskBuilder = ImmutableMap.builder();\n+                for (ColumnMetadata column : columns) {\n+                    columnMaskBuilder.put(column, new ViewExpression(\"user\", Optional.empty(), Optional.empty(), \"connector mask\"));\n+                }\n+                return columnMaskBuilder.buildOrThrow();\n+            }\n+        });\n+\n+        PrestoException exception = expectThrows(\n+                PrestoException.class,\n+                () -> transaction(transactionManager, accessControlManager)\n+                            .execute(transactionId -> {\n+                                accessControlManager.getColumnMasks(transactionId, new Identity(USER_NAME, Optional.of(PRINCIPAL)),\n+                                        new AccessControlContext(new QueryId(QUERY_ID), Optional.empty(), Collections.emptySet(), Optional.empty(), WarningCollector.NOOP, new RuntimeStats(), Optional.empty(), Optional.empty(), Optional.empty()), new QualifiedObjectName(\"catalog\", \"schema\", \"table\"),\n+                                        ImmutableList.of(ColumnMetadata.builder().setName(\"column\").setType(BIGINT).build()));\n+                            }));\n+        assertEquals(exception.getErrorCode(), INVALID_COLUMN_MASK.toErrorCode());\n+        assertEquals(exception.getMessage(), \"Multiple masks for the same column found\");\n+    }\n+\n     private static ConnectorId registerBogusConnector(CatalogManager catalogManager, TransactionManager transactionManager, AccessControl accessControl, String catalogName)\n     {\n         ConnectorId connectorId = new ConnectorId(catalogName);\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestAccessControlFiltersMasks.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestAccessControlFiltersMasks.java\nnew file mode 100644\nindex 0000000000000..ac5dfb3f453f6\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/TestAccessControlFiltersMasks.java\n@@ -0,0 +1,118 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner;\n+\n+import com.facebook.presto.common.QualifiedObjectName;\n+import com.facebook.presto.spi.plan.SemiJoinNode;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.facebook.presto.sql.planner.assertions.BasePlanTest;\n+import com.facebook.presto.testing.LocalQueryRunner;\n+import com.facebook.presto.testing.TestingAccessControlManager;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.anyTree;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.expression;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.filter;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.node;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.project;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.tableScan;\n+\n+public class TestAccessControlFiltersMasks\n+        extends BasePlanTest\n+{\n+    private static final String CATALOG = \"local\";\n+    private static final String USER = \"user\";\n+    private static final String RUN_AS_USER = \"run-as-user\";\n+\n+    private LocalQueryRunner runner;\n+    private TestingAccessControlManager accessControl;\n+\n+    @BeforeClass\n+    public void init()\n+    {\n+        runner = getQueryRunner();\n+        accessControl = getQueryRunner().getAccessControl();\n+    }\n+\n+    @Test\n+    public void testBasicRowFilter()\n+    {\n+        executeExclusively(() -> {\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"orderkey < 10\"));\n+            assertPlan(\"SELECT * FROM orders\",\n+                    anyTree(\n+                            filter(\"ORDERKEY < 10\",\n+                                    tableScan(\"orders\", ImmutableMap.of(\"ORDERKEY\", \"orderkey\")))));\n+            accessControl.reset();\n+        });\n+    }\n+\n+    @Test\n+    public void testMultipleIdentityFilters()\n+    {\n+        executeExclusively(() -> {\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    RUN_AS_USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey = 1\"));\n+\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT orderkey FROM orders)\"));\n+            assertPlan(\"SELECT count(*) FROM orders\",\n+                    anyTree(\n+                            node(SemiJoinNode.class,\n+                                anyTree(filter(\"O_ORDERKEY = 1\", tableScan(\"orders\", ImmutableMap.of(\"O_ORDERKEY\", \"orderkey\")))),\n+                                anyTree(filter(\"S_ORDERKEY = 1\", tableScan(\"orders\", ImmutableMap.of(\"S_ORDERKEY\", \"orderkey\")))))));\n+            accessControl.reset();\n+        });\n+    }\n+\n+    @Test\n+    public void testBasicColumnMask()\n+    {\n+        executeExclusively(() -> {\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"NULL\"));\n+            assertPlan(\"SELECT custkey FROM orders WHERE orderkey = 1\",\n+                    anyTree(\n+                            project(ImmutableMap.of(\"custkey_0\", expression(\"NULL\")),\n+                                    anyTree(node(TableScanNode.class)))));\n+            accessControl.reset();\n+        });\n+    }\n+\n+    protected void executeExclusively(Runnable executionBlock)\n+    {\n+        runner.getExclusiveLock().lock();\n+        try {\n+            executionBlock.run();\n+        }\n+        finally {\n+            runner.getExclusiveLock().unlock();\n+        }\n+    }\n+}\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java b/presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java\nindex 8f765ed6010bf..acf3d1d70482b 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/query/QueryAssertions.java\n@@ -142,4 +142,15 @@ public void close()\n     {\n         runner.close();\n     }\n+\n+    protected void executeExclusively(Runnable executionBlock)\n+    {\n+        runner.getExclusiveLock().lock();\n+        try {\n+            executionBlock.run();\n+        }\n+        finally {\n+            runner.getExclusiveLock().unlock();\n+        }\n+    }\n }\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestColumnMask.java b/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestColumnMask.java\nnew file mode 100644\nindex 0000000000000..f04214fe9584c\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestColumnMask.java\n@@ -0,0 +1,382 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.query;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.QualifiedObjectName;\n+import com.facebook.presto.spi.security.Identity;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.facebook.presto.testing.LocalQueryRunner;\n+import com.facebook.presto.testing.TestingAccessControlManager;\n+import com.facebook.presto.tpch.TpchConnectorFactory;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static com.facebook.presto.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+\n+public class TestColumnMask\n+{\n+    private static final String CATALOG = \"local\";\n+    private static final String USER = \"user\";\n+    private static final String RUN_AS_USER = \"run-as-user\";\n+\n+    private QueryAssertions assertions;\n+    private TestingAccessControlManager accessControl;\n+\n+    @BeforeClass\n+    public void init()\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(CATALOG)\n+                .setSchema(TINY_SCHEMA_NAME)\n+                .setIdentity(new Identity(USER, Optional.empty())).build();\n+\n+        LocalQueryRunner runner = new LocalQueryRunner(session);\n+\n+        runner.createCatalog(CATALOG, new TpchConnectorFactory(1), ImmutableMap.of());\n+\n+        assertions = new QueryAssertions(runner);\n+        accessControl = assertions.getQueryRunner().getAccessControl();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void teardown()\n+    {\n+        assertions.close();\n+        assertions = null;\n+    }\n+\n+    @Test\n+    public void testSimpleMask()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"-custkey\"));\n+            assertions.assertQuery(\"SELECT custkey FROM orders WHERE orderkey = 1\", \"VALUES BIGINT '-370'\");\n+        });\n+\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"NULL\"));\n+            assertions.assertQuery(\"SELECT custkey FROM orders WHERE orderkey = 1\", \"VALUES CAST(NULL AS BIGINT)\");\n+        });\n+    }\n+\n+    @Test\n+    public void testMultipleMasksOnDifferentColumns()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"-custkey\"));\n+\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderstatus\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"'X'\"));\n+\n+            assertions.assertQuery(\"SELECT custkey, orderstatus FROM orders WHERE orderkey = 1\", \"VALUES (BIGINT '-370', 'X')\");\n+        });\n+    }\n+\n+    @Test\n+    public void testCoercibleType()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"clerk\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"CAST(clerk AS VARCHAR(5))\"));\n+            assertions.assertQuery(\"SELECT clerk FROM orders WHERE orderkey = 1\", \"VALUES CAST('Clerk' AS VARCHAR(15))\");\n+        });\n+    }\n+\n+    @Test\n+    public void testSubquery()\n+    {\n+        // uncorrelated\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"clerk\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT cast(max(name) AS VARCHAR(15)) FROM nation)\"));\n+            assertions.assertQuery(\"SELECT clerk FROM orders WHERE orderkey = 1\", \"VALUES CAST('VIETNAM' AS VARCHAR(15))\");\n+        });\n+\n+        // correlated\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"clerk\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT cast(max(name) AS VARCHAR(15)) FROM nation WHERE nationkey = orderkey)\"));\n+            assertions.assertQuery(\"SELECT clerk FROM orders WHERE orderkey = 1\", \"VALUES CAST('ARGENTINA' AS VARCHAR(15))\");\n+        });\n+    }\n+\n+    @Test\n+    public void testTableReferenceInWithClause()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"-custkey\"));\n+            assertions.assertQuery(\"WITH t AS (SELECT custkey FROM orders WHERE orderkey = 1) SELECT * FROM t\", \"VALUES BIGINT '-370'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testOtherSchema()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"sf1\"), \"(SELECT count(*) FROM customer)\")); // count is 15000 only when evaluating against sf1\n+            assertions.assertQuery(\"SELECT max(orderkey) FROM orders\", \"VALUES BIGINT '150000'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testDifferentIdentity()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    RUN_AS_USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"100\"));\n+\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT sum(orderkey) FROM orders)\"));\n+\n+            assertions.assertQuery(\"SELECT max(orderkey) FROM orders\", \"VALUES BIGINT '1500000'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testRecursion()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT orderkey FROM orders)\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \".*\\\\QColumn mask for 'local.tiny.orders.orderkey' is recursive\\\\E.*\");\n+        });\n+\n+        // different reference style to same table\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT orderkey FROM local.tiny.orders)\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \".*\\\\QColumn mask for 'local.tiny.orders.orderkey' is recursive\\\\E.*\");\n+        });\n+\n+        // mutual recursion\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    RUN_AS_USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT orderkey FROM orders)\"));\n+\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT orderkey FROM orders)\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \".*\\\\QColumn mask for 'local.tiny.orders.orderkey' is recursive\\\\E.*\");\n+        });\n+    }\n+\n+    @Test\n+    public void testLimitedScope()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"customer\"),\n+                    \"custkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey\"));\n+            assertions.assertFails(\n+                    \"SELECT (SELECT min(custkey) FROM customer WHERE customer.custkey = orders.custkey) FROM orders\",\n+                    \"\\\\Qline 1:1: Column 'orderkey' cannot be resolved\\\\E\");\n+        });\n+    }\n+\n+    @Test\n+    public void testSqlInjection()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"nation\"),\n+                    \"name\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"(SELECT name FROM region WHERE regionkey = 0)\"));\n+            assertions.assertQuery(\n+                    \"WITH region(regionkey, name) AS (VALUES (0, 'ASIA'))\" +\n+                            \"SELECT name FROM nation ORDER BY name LIMIT 1\",\n+                    \"VALUES CAST('AFRICA' AS VARCHAR(25))\"); // if sql-injection would work then query would return ASIA\n+        });\n+    }\n+\n+    @Test\n+    public void testInvalidMasks()\n+    {\n+        // parse error\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"$$$\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\QInvalid column mask for 'local.tiny.orders.orderkey': mismatched input '$'. Expecting: <expression>\\\\E\");\n+        });\n+\n+        // unknown column\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"unknown_column\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\Qline 1:1: Column 'unknown_column' cannot be resolved\\\\E\");\n+        });\n+\n+        // invalid type\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"'foo'\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\QExpected column mask for 'local.tiny.orders.orderkey' to be of type bigint, but was varchar(3)\\\\E\");\n+        });\n+\n+        // aggregation\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"count(*) > 0\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\Qline 1:10: Column mask for 'orders.orderkey' cannot contain aggregations, window functions or grouping operations: [\\\"count\\\"(*)]\\\\E\");\n+        });\n+\n+        // window function\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"row_number() OVER () > 0\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\Qline 1:22: Column mask for 'orders.orderkey' cannot contain aggregations, window functions or grouping operations: [\\\"row_number\\\"() OVER ()]\\\\E\");\n+        });\n+\n+        // grouping function\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"orderkey\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"grouping(orderkey) = 0\"));\n+\n+            assertions.assertFails(\"SELECT orderkey FROM orders\", \"\\\\Qline 1:20: Column mask for 'orders.orderkey' cannot contain aggregations, window functions or grouping operations: [GROUPING (orderkey)]\\\\E\");\n+        });\n+    }\n+\n+    @Test\n+    public void testInsertWithColumnMasking()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"clerk\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"clerk\"));\n+\n+            assertions.assertFails(\"INSERT INTO orders SELECT * FROM orders\", \"Insert into table with column masks is not supported\");\n+        });\n+    }\n+\n+    @Test\n+    public void testDeleteWithColumnMasking()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.columnMask(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    \"clerk\",\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"clerk\"));\n+\n+            assertions.assertFails(\"DELETE FROM orders\", \"\\\\Qline 1:1: Delete from table with column mask is not supported\\\\E\");\n+        });\n+    }\n+}\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestRowFilter.java b/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestRowFilter.java\nnew file mode 100644\nindex 0000000000000..075db8d3d71a0\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/query/TestRowFilter.java\n@@ -0,0 +1,334 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.query;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.QualifiedObjectName;\n+import com.facebook.presto.spi.security.Identity;\n+import com.facebook.presto.spi.security.ViewExpression;\n+import com.facebook.presto.testing.LocalQueryRunner;\n+import com.facebook.presto.testing.TestingAccessControlManager;\n+import com.facebook.presto.tpch.TpchConnectorFactory;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static com.facebook.presto.tpch.TpchMetadata.TINY_SCHEMA_NAME;\n+\n+public class TestRowFilter\n+{\n+    private static final String CATALOG = \"local\";\n+    private static final String USER = \"user\";\n+    private static final String RUN_AS_USER = \"run-as-user\";\n+\n+    private QueryAssertions assertions;\n+    private TestingAccessControlManager accessControl;\n+\n+    @BeforeClass\n+    public void init()\n+    {\n+        Session session = testSessionBuilder()\n+                .setCatalog(CATALOG)\n+                .setSchema(TINY_SCHEMA_NAME)\n+                .setIdentity(new Identity(USER, Optional.empty())).build();\n+\n+        LocalQueryRunner runner = new LocalQueryRunner(session);\n+\n+        runner.createCatalog(CATALOG, new TpchConnectorFactory(1), ImmutableMap.of());\n+\n+        assertions = new QueryAssertions(runner);\n+        accessControl = assertions.getQueryRunner().getAccessControl();\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void teardown()\n+    {\n+        assertions.close();\n+        assertions = null;\n+    }\n+\n+    @Test\n+    public void testSimpleFilter()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"orderkey < 10\"));\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '7'\");\n+        });\n+\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"NULL\"));\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '0'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testMultipleFilters()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"orderkey < 10\"));\n+\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"orderkey > 5\"));\n+\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '2'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testCorrelatedSubquery()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"EXISTS (SELECT 1 FROM nation WHERE nationkey = orderkey)\"));\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '7'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testTableReferenceInWithClause()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.empty(), Optional.empty(), \"orderkey = 1\"));\n+            assertions.assertQuery(\"WITH t AS (SELECT count(*) FROM orders) SELECT * FROM t\", \"VALUES BIGINT '1'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testOtherSchema()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"sf1\"), \"(SELECT count(*) FROM customer) = 150000\")); // Filter is TRUE only if evaluating against sf1.customer\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '15000'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testDifferentIdentity()\n+    {\n+        // This does not fail the recursive check because the initial filter is added to the subquery with RUN_AS_USER identity,\n+        // then the second filter is added with USER identity, allowing both filters to produce 1 row in the result.\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    RUN_AS_USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey = 1\"));\n+\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT orderkey FROM orders)\"));\n+\n+            assertions.assertQuery(\"SELECT count(*) FROM orders\", \"VALUES BIGINT '1'\");\n+        });\n+    }\n+\n+    @Test\n+    public void testRecursion()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT orderkey FROM orders)\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \".*\\\\QRow filter for 'local.tiny.orders' is recursive\\\\E.*\");\n+        });\n+\n+        // different reference style to same table\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT local.tiny.orderkey FROM orders)\"));\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \".*\\\\QRow filter for 'local.tiny.orders' is recursive\\\\E.*\");\n+        });\n+\n+        // mutual recursion\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    RUN_AS_USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT orderkey FROM orders)\"));\n+\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey IN (SELECT orderkey FROM orders)\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \".*\\\\QRow filter for 'local.tiny.orders' is recursive\\\\E.*\");\n+        });\n+    }\n+\n+    @Test\n+    public void testLimitedScope()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"customer\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey = 1\"));\n+            assertions.assertFails(\n+                    \"SELECT (SELECT min(name) FROM customer WHERE customer.custkey = orders.custkey) FROM orders\",\n+                    \"\\\\Qline 1:1: Column 'orderkey' cannot be resolved\\\\E\");\n+        });\n+    }\n+\n+    @Test\n+    public void testSqlInjection()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"nation\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"regionkey IN (SELECT regionkey FROM region WHERE name = 'ASIA')\"));\n+            assertions.assertQuery(\n+                    \"WITH region(regionkey, name) AS (VALUES (0, 'ASIA'), (1, 'ASIA'), (2, 'ASIA'), (3, 'ASIA'), (4, 'ASIA'))\" +\n+                            \"SELECT name FROM nation ORDER BY name LIMIT 1\",\n+                    \"VALUES CAST('CHINA' AS VARCHAR(25))\"); // if sql-injection would work then query would return ALGERIA\n+        });\n+    }\n+\n+    @Test\n+    public void testInvalidFilter()\n+    {\n+        // parse error\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"$$$\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"\\\\QInvalid row filter for 'local.tiny.orders': mismatched input '$'. Expecting: <expression>\\\\E\");\n+        });\n+\n+        // unknown column\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"unknown_column\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"line 1:1: Column 'unknown_column' cannot be resolved\");\n+        });\n+\n+        // invalid type\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"1\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"\\\\QExpected row filter for 'local.tiny.orders' to be of type BOOLEAN, but was integer\\\\E\");\n+        });\n+\n+        // aggregation\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"count(*) > 0\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"\\\\Qline 1:10: Row filter for 'local.tiny.orders' cannot contain aggregations, window functions or grouping operations: [\\\"count\\\"(*)]\\\\E\");\n+        });\n+\n+        // window function\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"row_number() OVER () > 0\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"\\\\Qline 1:22: Row filter for 'local.tiny.orders' cannot contain aggregations, window functions or grouping operations: [\\\"row_number\\\"() OVER ()]\\\\E\");\n+        });\n+\n+        // window function\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(RUN_AS_USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"grouping(orderkey) = 0\"));\n+\n+            assertions.assertFails(\"SELECT count(*) FROM orders\", \"\\\\Qline 1:20: Row filter for 'local.tiny.orders' cannot contain aggregations, window functions or grouping operations: [GROUPING (orderkey)]\\\\E\");\n+        });\n+    }\n+\n+    @Test\n+    public void testInsertWithRowFiltering()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey < 10\"));\n+\n+            assertions.assertFails(\"INSERT INTO orders SELECT * FROM orders\", \"Insert into table with row filter is not supported\");\n+        });\n+    }\n+\n+    @Test\n+    public void testDeleteWithRowFiltering()\n+    {\n+        assertions.executeExclusively(() -> {\n+            accessControl.reset();\n+            accessControl.rowFilter(\n+                    new QualifiedObjectName(CATALOG, \"tiny\", \"orders\"),\n+                    USER,\n+                    new ViewExpression(USER, Optional.of(CATALOG), Optional.of(\"tiny\"), \"orderkey < 10\"));\n+\n+            assertions.assertFails(\"DELETE FROM orders\", \"\\\\Qline 1:1: Delete from table with row filter is not supported\\\\E\");\n+        });\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24236",
    "pr_id": 24236,
    "issue_id": 24023,
    "repo": "prestodb/presto",
    "problem_statement": "Migrate Presto C++ CI jobs off of CircleCI\nThe jobs [found here](https://github.com/prestodb/presto/tree/master/.circleci) all need to be migrated to Github Actions, as our CircleCI account will be suspended at the end of this year.\r\n\r\n## Expected Behavior or Use Case\r\nWe remove all critical CircleCI jobs\r\n\r\n## Presto Component, Service, or Connector\r\nCI\r\n\r\n## Possible Implementation\r\nSee previous attempts:\r\n\r\n* https://github.com/prestodb/presto/pull/23938\r\n* https://github.com/prestodb/presto/pull/21545\r\n\r\nWe are obtaining funding for larger runners to accomadate the CI needs of these jobs.  We expect to have sufficient funding by end of year.\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\nOur CircleCI account will be defunct by EOY and we need to migrate to a different CI runner.",
    "issue_word_count": 125,
    "test_files_count": 1,
    "non_test_files_count": 4,
    "pr_changed_files": [
      ".github/workflows/prestocpp-format-and-header-check.yml",
      ".github/workflows/prestocpp-linux-adapters-build.yml",
      ".github/workflows/prestocpp-linux-build-and-unit-test.yml",
      ".github/workflows/prestocpp-linux-build.yml",
      ".github/workflows/prestocpp-macos-build.yml"
    ],
    "pr_changed_test_files": [
      ".github/workflows/prestocpp-linux-build-and-unit-test.yml"
    ],
    "base_commit": "901da6de96495e2871b111cbbcb9417a566a50fe",
    "head_commit": "35816fddc80ac10398dd18a58d1e6c21fb157b18",
    "repo_url": "https://github.com/prestodb/presto/pull/24236",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24236",
    "dockerfile": "",
    "pr_merged_at": "2024-12-14T00:06:50.000Z",
    "patch": "diff --git a/.github/workflows/prestocpp-format-and-header-check.yml b/.github/workflows/prestocpp-format-and-header-check.yml\nnew file mode 100644\nindex 0000000000000..42845f670dc6f\n--- /dev/null\n+++ b/.github/workflows/prestocpp-format-and-header-check.yml\n@@ -0,0 +1,33 @@\n+name: prestocpp-format-and-header-check\n+\n+on:\n+  workflow_dispatch:\n+  pull_request:\n+    paths:\n+      - 'presto-native-execution/**'\n+      - '.github/workflows/prestocpp-format-and-header-check.yml'\n+\n+jobs:\n+  prestocpp-format-and-header-check:\n+    runs-on: ubuntu-latest\n+    container:\n+      image: public.ecr.aws/oss-presto/velox-dev:check\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Check formatting\n+        run: |\n+          git fetch origin master\n+          cd presto-native-execution\n+          make format-check\n+\n+      - name: Check license headers\n+        run: |\n+          git fetch origin master\n+          cd presto-native-execution\n+          make header-check\n\ndiff --git a/.github/workflows/prestocpp-linux-adapters-build.yml b/.github/workflows/prestocpp-linux-adapters-build.yml\nnew file mode 100644\nindex 0000000000000..34f6e6af96b38\n--- /dev/null\n+++ b/.github/workflows/prestocpp-linux-adapters-build.yml\n@@ -0,0 +1,91 @@\n+name: prestocpp-linux-adapters-build\n+\n+on:\n+  workflow_dispatch:\n+  # Disable the automatic execution on PR because it currently will run out of disk space and\n+  # we will address this subsequently.\n+  # - use smaller image - in the works\n+  # - remove the adapters downloaded files after install - JWT needs fixing because of the cmake files end up\n+  #pull_request:\n+  #  paths:\n+  #    - 'presto-native-execution/scripts/**'\n+  #    - '.github/workflows/prestocpp-linux-adapters-build.yml'\n+\n+jobs:\n+  prestocpp-linux-adapters-build:\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      CCACHE_DIR: \"${{ github.workspace }}/ccache\"\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Update submodules\n+        run: |\n+          cd presto-native-execution\n+          make submodules\n+\n+      - name: Build all adapter dependencies\n+        run: |\n+          mkdir -p ${GITHUB_WORKSPACE}/adapter-deps/install\n+          mkdir -p ${GITHUB_WORKSPACE}/adapter-deps/download\n+          source /opt/rh/gcc-toolset-12/enable\n+          set -xu\n+          cd presto-native-execution\n+          export DEPENDENCY_DIR=${GITHUB_WORKSPACE}/adapter-deps/download\n+          export INSTALL_PREFIX=${GITHUB_WORKSPACE}/adapter-deps/install\n+          PROMPT_ALWAYS_RESPOND=n ./velox/scripts/setup-adapters.sh\n+          PROMPT_ALWAYS_RESPOND=n ./scripts/setup-adapters.sh\n+\n+      - name: Install Github CLI for using apache/infrastructure-actions/stash\n+        run: |\n+          curl -L https://github.com/cli/cli/releases/download/v2.63.2/gh_2.63.2_linux_amd64.rpm > gh_2.63.2_linux_amd64.rpm\n+          rpm -iv gh_2.63.2_linux_amd64.rpm\n+\n+      - uses: apache/infrastructure-actions/stash/restore@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-adapters-build\n+\n+      - name: Zero ccache statistics\n+        run: ccache -sz\n+\n+      - name: Build engine\n+        run: |\n+          source /opt/rh/gcc-toolset-12/enable\n+          cd presto-native-execution\n+          cmake \\\n+            -B _build/release \\\n+            -GNinja \\\n+            -DTREAT_WARNINGS_AS_ERRORS=1 \\\n+            -DENABLE_ALL_WARNINGS=1 \\\n+            -DCMAKE_BUILD_TYPE=Release \\\n+            -DPRESTO_ENABLE_PARQUET=ON \\\n+            -DPRESTO_ENABLE_S3=ON \\\n+            -DPRESTO_ENABLE_GCS=ON \\\n+            -DPRESTO_ENABLE_ABFS=OFF \\\n+            -DPRESTO_ENABLE_HDFS=ON \\\n+            -DPRESTO_ENABLE_REMOTE_FUNCTIONS=ON \\\n+            -DPRESTO_ENABLE_JWT=ON \\\n+            -DPRESTO_STATS_REPORTER_TYPE=PROMETHEUS \\\n+            -DPRESTO_MEMORY_CHECKER_TYPE=LINUX_MEMORY_CHECKER \\\n+            -DPRESTO_ENABLE_TESTING=OFF \\\n+            -DCMAKE_PREFIX_PATH=/usr/local \\\n+            -DThrift_ROOT=/usr/local \\\n+            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n+            -DMAX_LINK_JOBS=4\n+          ninja -C _build/release -j 4\n+\n+      - name: Ccache after\n+        run: ccache -s\n+\n+      - uses: apache/infrastructure-actions/stash/save@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-adapters-build\n\ndiff --git a/.github/workflows/prestocpp-linux-build.yml b/.github/workflows/prestocpp-linux-build.yml\nnew file mode 100644\nindex 0000000000000..372be3b56fdfd\n--- /dev/null\n+++ b/.github/workflows/prestocpp-linux-build.yml\n@@ -0,0 +1,75 @@\n+name: prestocpp-linux-build\n+\n+on:\n+  workflow_dispatch:\n+  pull_request:\n+    paths:\n+      - 'presto-native-execution/**'\n+      - '.github/workflows/prestocpp-linux-build.yml'\n+\n+jobs:\n+  prestocpp-linux-build-engine:\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      CCACHE_DIR: \"${{ github.workspace }}/ccache\"\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Update velox\n+        run: |\n+          cd presto-native-execution\n+          make velox-submodule\n+\n+      - name: Install Github CLI for using apache/infrastructure-actions/stash\n+        run: |\n+          curl -L https://github.com/cli/cli/releases/download/v2.63.2/gh_2.63.2_linux_amd64.rpm > gh_2.63.2_linux_amd64.rpm\n+          rpm -iv gh_2.63.2_linux_amd64.rpm\n+\n+      - uses: apache/infrastructure-actions/stash/restore@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-build-engine\n+\n+      - name: Zero ccache statistics\n+        run: ccache -sz\n+\n+      - name: Build engine\n+        run: |\n+          source /opt/rh/gcc-toolset-12/enable\n+          cd presto-native-execution\n+          cmake \\\n+            -B _build/debug \\\n+            -GNinja \\\n+            -DTREAT_WARNINGS_AS_ERRORS=1 \\\n+            -DENABLE_ALL_WARNINGS=1 \\\n+            -DCMAKE_BUILD_TYPE=Debug \\\n+            -DPRESTO_ENABLE_PARQUET=ON \\\n+            -DPRESTO_ENABLE_S3=ON \\\n+            -DPRESTO_ENABLE_GCS=ON \\\n+            -DPRESTO_ENABLE_ABFS=OFF \\\n+            -DPRESTO_ENABLE_HDFS=ON \\\n+            -DPRESTO_ENABLE_REMOTE_FUNCTIONS=ON \\\n+            -DPRESTO_ENABLE_JWT=ON \\\n+            -DPRESTO_STATS_REPORTER_TYPE=PROMETHEUS \\\n+            -DPRESTO_MEMORY_CHECKER_TYPE=LINUX_MEMORY_CHECKER \\\n+            -DPRESTO_ENABLE_TESTING=OFF \\\n+            -DCMAKE_PREFIX_PATH=/usr/local \\\n+            -DThrift_ROOT=/usr/local \\\n+            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n+            -DMAX_LINK_JOBS=4\n+          ninja -C _build/debug -j 4\n+\n+      - name: Ccache after\n+        run: ccache -s\n+\n+      - uses: apache/infrastructure-actions/stash/save@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-build-engine\n\ndiff --git a/.github/workflows/prestocpp-macos-build.yml b/.github/workflows/prestocpp-macos-build.yml\nnew file mode 100644\nindex 0000000000000..05a656e87247f\n--- /dev/null\n+++ b/.github/workflows/prestocpp-macos-build.yml\n@@ -0,0 +1,73 @@\n+name: prestocpp-macos-build\n+\n+on:\n+  workflow_dispatch:\n+  pull_request:\n+    paths:\n+      - 'presto-native-execution/**'\n+      - '.github/workflows/prestocpp-macos-build.yml'\n+\n+jobs:\n+  prestocpp-macos-build-engine:\n+    runs-on: macos-13\n+    env:\n+      CCACHE_DIR: \"${{ github.workspace }}/ccache\"\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Update submodules\n+        run: |\n+          cd presto-native-execution\n+          make submodules\n+\n+      # The current Github runners for newer versions of macOS (14, 15) only provide 7GB memory while macOS 13 runners provide 14GB.\n+      # Thus, we are installing XCode 15 on macOS 13 to build with Clang 15.\n+      # https://docs.github.com/en/actions/using-github-hosted-runners/using-github-hosted-runners/about-github-hosted-runners#standard-github-hosted-runners-for-public-repositories\n+      - uses: maxim-lobanov/setup-xcode@v1.6.0\n+        with:\n+          xcode-version: 15\n+\n+      - name: \"Setup MacOS\"\n+        run: |\n+          set -xu\n+          mkdir ~/deps ~/deps-src\n+          git clone --depth 1 https://github.com/Homebrew/brew ~/deps\n+          PATH=~/deps/bin:${PATH} DEPENDENCY_DIR=~/deps-src INSTALL_PREFIX=~/deps PROMPT_ALWAYS_RESPOND=n ./presto-native-execution/scripts/setup-macos.sh\n+          # Calculate the prefix path before we delete brew's repos and taps.\n+          rm -rf ~/deps/.git ~/deps/Library/Taps/  # Reduce cache size by 70%.\n+          rm -rf ~/deps-src\n+\n+      - name: Install Github CLI for using apache/infrastructure-actions/stash\n+        run: |\n+          PATH=~/deps/bin:${PATH}\n+          brew install gh\n+\n+      - uses: apache/infrastructure-actions/stash/restore@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-macos-build-engine\n+\n+      - name: Zero ccache statistics\n+        run: ccache -sz\n+\n+      - name: \"Build presto_cpp on MacOS\"\n+        run: |\n+          clang --version\n+          export INSTALL_PREFIX=~/deps\n+          export PATH=~/deps/bin:$(brew --prefix m4)/bin:$(brew --prefix bison)/bin:${PATH}\n+          cd presto-native-execution\n+          cmake -B _build/debug -GNinja -DTREAT_WARNINGS_AS_ERRORS=1 -DENABLE_ALL_WARNINGS=1 -DCMAKE_BUILD_TYPE=Debug -DCMAKE_CXX_COMPILER_LAUNCHER=ccache\n+          ninja -C _build/debug\n+\n+      - name: Ccache after\n+        run: ccache -s\n+\n+      - uses: apache/infrastructure-actions/stash/save@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-macos-prestocpp\n",
    "test_patch": "diff --git a/.github/workflows/prestocpp-linux-build-and-unit-test.yml b/.github/workflows/prestocpp-linux-build-and-unit-test.yml\nnew file mode 100644\nindex 0000000000000..a3770fb1a8df5\n--- /dev/null\n+++ b/.github/workflows/prestocpp-linux-build-and-unit-test.yml\n@@ -0,0 +1,325 @@\n+name: prestocpp-linux-build-and-unit-test\n+\n+on:\n+  workflow_dispatch:\n+  pull_request:\n+    paths:\n+      - 'presto-native-execution/**'\n+      - '.github/workflows/prestocpp-linux-build-and-unit-test.yml'\n+\n+jobs:\n+  prestocpp-linux-build-for-test:\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      CCACHE_DIR: \"${{ github.workspace }}/ccache\"\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Update velox\n+        run: |\n+          cd presto-native-execution\n+          make velox-submodule\n+\n+      - name: Install Github CLI for using apache/infrastructure-actions/stash\n+        run: |\n+          curl -L https://github.com/cli/cli/releases/download/v2.63.2/gh_2.63.2_linux_amd64.rpm > gh_2.63.2_linux_amd64.rpm\n+          rpm -iv gh_2.63.2_linux_amd64.rpm\n+\n+      - uses: apache/infrastructure-actions/stash/restore@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-build-for-test\n+\n+      - name: Zero ccache statistics\n+        run: ccache -sz\n+\n+      - name: Build engine\n+        run: |\n+          source /opt/rh/gcc-toolset-12/enable\n+          cd presto-native-execution\n+          cmake \\\n+            -B _build/release \\\n+            -GNinja \\\n+            -DTREAT_WARNINGS_AS_ERRORS=1 \\\n+            -DENABLE_ALL_WARNINGS=1 \\\n+            -DCMAKE_BUILD_TYPE=Release \\\n+            -DPRESTO_ENABLE_PARQUET=ON \\\n+            -DPRESTO_ENABLE_REMOTE_FUNCTIONS=ON \\\n+            -DPRESTO_ENABLE_JWT=ON \\\n+            -DPRESTO_STATS_REPORTER_TYPE=PROMETHEUS \\\n+            -DPRESTO_MEMORY_CHECKER_TYPE=LINUX_MEMORY_CHECKER \\\n+            -DCMAKE_PREFIX_PATH=/usr/local \\\n+            -DThrift_ROOT=/usr/local \\\n+            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n+            -DMAX_LINK_JOBS=4\n+          ninja -C _build/release -j 4\n+\n+      - name: Ccache after\n+        run: ccache -s\n+\n+      - uses: apache/infrastructure-actions/stash/save@4ab8682fbd4623d2b4fc1c98db38aba5091924c3\n+        with:\n+          path: '${{ env.CCACHE_DIR }}'\n+          key: ccache-prestocpp-linux-build-for-test\n+\n+      - name: Run Unit Tests\n+        run: |\n+          # Ensure transitive dependency libboost-iostreams is found.\n+          ldconfig /usr/local/lib\n+          cd presto-native-execution/_build/release\n+          ctest -j 4 -VV --output-on-failure --exclude-regex velox.*\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: presto-native-build\n+          path: |\n+            presto-native-execution/_build/release/presto_cpp/main/presto_server\n+            presto-native-execution/_build/release/velox/velox/functions/remote/server/velox_functions_remote_server_main\n+\n+  prestocpp-linux-presto-e2e-tests:\n+    needs: prestocpp-linux-build-for-test\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      MAVEN_OPTS: \"-Xmx4G -XX:+ExitOnOutOfMemoryError\"\n+      MAVEN_FAST_INSTALL: \"-B -V --quiet -T 1C -DskipTests -Dair.check.skip-all -Dmaven.javadoc.skip=true\"\n+      MAVEN_TEST: \"-B -Dair.check.skip-all -Dmaven.javadoc.skip=true -DLogTestDurationListener.enabled=true --fail-at-end\"\n+    steps:\n+      - uses: actions/checkout@v4\n+\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Download artifacts\n+        uses: actions/download-artifact@v4\n+        with:\n+          name: presto-native-build\n+          path: presto-native-execution/_build/release\n+\n+      # Permissions are lost when uploading. Details here: https://github.com/actions/upload-artifact/issues/38\n+      - name: Restore execute permissions and library path\n+        run: |\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/velox/velox/functions/remote/server/velox_functions_remote_server_main\n+          # Ensure transitive dependency libboost-iostreams is found.\n+          ldconfig /usr/local/lib\n+\n+      - name: Install OpenJDK8\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: 'temurin'\n+          java-version: '8'\n+\n+      - name: Cache local Maven repository\n+        id: cache-maven\n+        uses: actions/cache@v4\n+        with:\n+          path: ~/.m2/repository\n+          key: ${{ runner.os }}-maven-2-${{ hashFiles('**/pom.xml') }}\n+          restore-keys: |\n+            ${{ runner.os }}-maven-2-\n+\n+      - name: Populate maven cache\n+        if: steps.cache-maven.outputs.cache-hit != 'true'\n+        run: ./mvnw de.qaware.maven:go-offline-maven-plugin:resolve-dependencies\n+\n+      - name: Maven install\n+        env:\n+          # Use different Maven options to install.\n+          MAVEN_OPTS: \"-Xmx2G -XX:+ExitOnOutOfMemoryError\"\n+        run: |\n+          for i in $(seq 1 3); do ./mvnw clean install $MAVEN_FAST_INSTALL -pl 'presto-native-execution' -am && s=0 && break || s=$? && sleep 10; done; (exit $s)\n+\n+      - name: Run presto-native e2e tests\n+        run: |\n+          export PRESTO_SERVER_PATH=\"${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\"\n+          export TESTFILES=`find ./presto-native-execution/src/test -type f -name 'TestPrestoNative*.java'`\n+          # Convert file paths to comma separated class names\n+          export TESTCLASSES=\n+          for test_file in $TESTFILES\n+          do\n+            tmp=${test_file##*/}\n+            test_class=${tmp%%\\.*}\n+            export TESTCLASSES=\"${TESTCLASSES},$test_class\"\n+          done\n+          export TESTCLASSES=${TESTCLASSES#,}\n+          echo \"TESTCLASSES = $TESTCLASSES\"\n+          # TODO: neeed to enable remote function tests with\n+          # \"-Ppresto-native-execution-remote-functions\" once\n+          # > https://github.com/facebookincubator/velox/discussions/6163\n+          # is fixed.\n+\n+          mvn test \\\n+            ${MAVEN_TEST} \\\n+            -pl 'presto-native-execution' \\\n+            -Dtest=\"${TESTCLASSES}\" \\\n+            -DPRESTO_SERVER=${PRESTO_SERVER_PATH} \\\n+            -DDATA_DIR=${RUNNER_TEMP} \\\n+            -Duser.timezone=America/Bahia_Banderas \\\n+            -T1C\n+\n+  prestocpp-linux-spark-e2e-tests:\n+    needs: prestocpp-linux-build-for-test\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      MAVEN_OPTS: \"-Xmx4G -XX:+ExitOnOutOfMemoryError\"\n+      MAVEN_FAST_INSTALL: \"-B -V --quiet -T 1C -DskipTests -Dair.check.skip-all -Dmaven.javadoc.skip=true\"\n+      MAVEN_TEST: \"-B -Dair.check.skip-all -Dmaven.javadoc.skip=true -DLogTestDurationListener.enabled=true --fail-at-end\"\n+    steps:\n+      - uses: actions/checkout@v4\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Download artifacts\n+        uses: actions/download-artifact@v4\n+        with:\n+          name: presto-native-build\n+          path: presto-native-execution/_build/release\n+\n+      # Permissions are lost when uploading. Details here: https://github.com/actions/upload-artifact/issues/38\n+      - name: Restore execute permissions and library path\n+        run: |\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/velox/velox/functions/remote/server/velox_functions_remote_server_main\n+          # Ensure transitive dependency libboost-iostreams is found.\n+          ldconfig /usr/local/lib\n+\n+      - name: Check Java\n+        run: java --version\n+\n+      - name: Cache local Maven repository\n+        id: cache-maven\n+        uses: actions/cache@v4\n+        with:\n+          path: ~/.m2/repository\n+          key: ${{ runner.os }}-maven-2-${{ hashFiles('**/pom.xml') }}\n+          restore-keys: |\n+            ${{ runner.os }}-maven-2-\n+\n+      - name: Populate maven cache\n+        if: steps.cache-maven.outputs.cache-hit != 'true'\n+        run: ./mvnw de.qaware.maven:go-offline-maven-plugin:resolve-dependencies\n+\n+      - name: Maven install\n+        env:\n+          # Use different Maven options to install.\n+          MAVEN_OPTS: \"-Xmx2G -XX:+ExitOnOutOfMemoryError\"\n+        run: |\n+          for i in $(seq 1 3); do ./mvnw clean install $MAVEN_FAST_INSTALL -pl 'presto-native-execution' -am && s=0 && break || s=$? && sleep 10; done; (exit $s)\n+\n+      - name: Run spark e2e tests\n+        run: |\n+          export PRESTO_SERVER_PATH=\"${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\"\n+          export TESTFILES=`find ./presto-native-execution/src/test -type f -name 'TestPrestoSpark*.java'`\n+          # Convert file paths to comma separated class names\n+          export TESTCLASSES=\n+          for test_file in $TESTFILES\n+          do\n+            tmp=${test_file##*/}\n+            test_class=${tmp%%\\.*}\n+            export TESTCLASSES=\"${TESTCLASSES},$test_class\"\n+          done\n+          export TESTCLASSES=${TESTCLASSES#,}\n+          echo \"TESTCLASSES = $TESTCLASSES\"\n+          mvn test \\\n+            ${MAVEN_TEST} \\\n+            -pl 'presto-native-execution' \\\n+            -Dtest=\"${TESTCLASSES}\" \\\n+            -DPRESTO_SERVER=${PRESTO_SERVER_PATH} \\\n+            -DDATA_DIR=${RUNNER_TEMP} \\\n+            -Duser.timezone=America/Bahia_Banderas \\\n+            -T1C\n+\n+  prestocpp-linux-presto-sidecar-tests:\n+    needs: prestocpp-linux-build-for-test\n+    runs-on: ubuntu-22.04\n+    container:\n+      image: prestodb/presto-native-dependency:0.290-20241014120930-e1fc090\n+    env:\n+      MAVEN_OPTS: \"-Xmx4G -XX:+ExitOnOutOfMemoryError\"\n+      MAVEN_FAST_INSTALL: \"-B -V --quiet -T 1C -DskipTests -Dair.check.skip-all -Dmaven.javadoc.skip=true\"\n+      MAVEN_TEST: \"-B -Dair.check.skip-all -Dmaven.javadoc.skip=true -DLogTestDurationListener.enabled=true --fail-at-end\"\n+    steps:\n+      - uses: actions/checkout@v4\n+      - name: Fix git permissions\n+        # Usually actions/checkout does this but as we run in a container\n+        # it doesn't work\n+        run: git config --global --add safe.directory ${GITHUB_WORKSPACE}\n+\n+      - name: Download artifacts\n+        uses: actions/download-artifact@v4\n+        with:\n+          name: presto-native-build\n+          path: presto-native-execution/_build/release\n+\n+      # Permissions are lost when uploading. Details here: https://github.com/actions/upload-artifact/issues/38\n+      - name: Restore execute permissions and library path\n+        run: |\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\n+          chmod +x ${GITHUB_WORKSPACE}/presto-native-execution/_build/release/velox/velox/functions/remote/server/velox_functions_remote_server_main\n+          # Ensure transitive dependency libboost-iostreams is found.\n+          ldconfig /usr/local/lib\n+\n+      - name: Install OpenJDK8\n+        uses: actions/setup-java@v4\n+        with:\n+          distribution: 'temurin'\n+          java-version: '8'\n+\n+      - name: Cache local Maven repository\n+        id: cache-maven\n+        uses: actions/cache@v4\n+        with:\n+          path: ~/.m2/repository\n+          key: ${{ runner.os }}-maven-2-${{ hashFiles('**/pom.xml') }}\n+          restore-keys: |\n+            ${{ runner.os }}-maven-2-\n+\n+      - name: Populate maven cache\n+        if: steps.cache-maven.outputs.cache-hit != 'true'\n+        run: ./mvnw de.qaware.maven:go-offline-maven-plugin:resolve-dependencies\n+\n+      - name: Maven install\n+        env:\n+          # Use different Maven options to install.\n+          MAVEN_OPTS: \"-Xmx2G -XX:+ExitOnOutOfMemoryError\"\n+        run: |\n+          for i in $(seq 1 3); do ./mvnw clean install $MAVEN_FAST_INSTALL -pl 'presto-native-execution' -am && s=0 && break || s=$? && sleep 10; done; (exit $s)\n+\n+      - name: Run presto-native sidecar tests\n+        run: |\n+          export PRESTO_SERVER_PATH=\"${GITHUB_WORKSPACE}/presto-native-execution/_build/release/presto_cpp/main/presto_server\"\n+          export TESTFILES=`find ./presto-native-sidecar-plugin/src/test -type f -name 'Test*.java'`\n+          # Convert file paths to comma separated class names\n+          export TESTCLASSES=\n+          for test_file in $TESTFILES\n+          do\n+            tmp=${test_file##*/}\n+            test_class=${tmp%%\\.*}\n+            export TESTCLASSES=\"${TESTCLASSES},$test_class\"\n+          done\n+          export TESTCLASSES=${TESTCLASSES#,}\n+          echo \"TESTCLASSES = $TESTCLASSES\"\n+          mvn test \\\n+            ${MAVEN_TEST} \\\n+            -pl 'presto-native-sidecar-plugin' \\\n+            -Dtest=\"${TESTCLASSES}\" \\\n+            -DPRESTO_SERVER=${PRESTO_SERVER_PATH} \\\n+            -DDATA_DIR=${RUNNER_TEMP} \\\n+            -Duser.timezone=America/Bahia_Banderas \\\n+            -T1C\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24235",
    "pr_id": 24235,
    "issue_id": 22895,
    "repo": "prestodb/presto",
    "problem_statement": "Feature Enhancement: Enable Presto Server to Transmit Catalog Name for Enhanced Functionality in the Metastore Layer\n## Expected Behavior or Use Case\r\nWhen creating a schema in Presto, the catalog name should be passed to the metastore layer.\r\n\r\n## Presto Component, Service, or Connector\r\nThis request is related to the Presto service and its interaction with the metastore layer.\r\n\r\n## Possible Implementation\r\nModify Presto to include the catalog name information when creating a schema, allowing the metastore layer to utilize it internally.\r\n\r\n## Example Screenshots (if appropriate):\r\nN/A\r\n\r\n## Context\r\nCurrently, Presto does not share the catalog name with the metastore layer when creating a schema. This change will enhance the metastore's ability to effectively manage schemas and metadata. \r\n\r\nPassing the catalog name to the metastore layer from Presto would unlock several benefits:\r\n- **Enhanced Flexibility**: The metastore layer could use the catalog name information for various internal purposes, such as performing additional validation checks or organizing schemas into logical groups.\r\n- **Improved Efficiency**: By persisting the catalog name, the metastore could streamline operations like setting default locations for schemas based on the catalog's location.\r\n- **Better Integration**: Providing this information lays the groundwork for seamless integration with downstream components that rely on the metastore for data access and management.\r\n\r\nOverall, this enhancement would not only optimize the functionality of Presto but also enhance the capabilities of the metastore layer, leading to a more robust and efficient data processing system.",
    "issue_word_count": 235,
    "test_files_count": 30,
    "non_test_files_count": 24,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/deltalake.rst",
      "presto-docs/src/main/sphinx/connector/hive.rst",
      "presto-docs/src/main/sphinx/connector/hudi.rst",
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-hive-common/src/main/java/com/facebook/presto/hive/HiveCommonClientConfig.java",
      "presto-hive-common/src/main/java/com/facebook/presto/hive/MetadataUtils.java",
      "presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java",
      "presto-hive-common/src/test/java/com/facebook/presto/hive/TestMetadataUtils.java",
      "presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Database.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Partition.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Table.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileMetastoreModule.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/TableMetadata.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/GlueMetastoreModule.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/converter/GlueToPrestoConverter.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastore.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClient.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClientFactory.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastore.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreClient.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreStats.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/HivePartitionObjectBuilder.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/SyncPartitionMetadataProcedure.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveQueriesWithCatalogName.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java",
      "presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "pr_changed_test_files": [
      "presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java",
      "presto-hive-common/src/test/java/com/facebook/presto/hive/TestMetadataUtils.java",
      "presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java",
      "presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveQueriesWithCatalogName.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java",
      "presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "base_commit": "2f29ba927fdc965414039c73a8c304904849b060",
    "head_commit": "23ee6bc9b8553688a8efa23839f0f33b81042d39",
    "repo_url": "https://github.com/prestodb/presto/pull/24235",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24235",
    "dockerfile": "",
    "pr_merged_at": "2025-04-23T17:41:58.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/deltalake.rst b/presto-docs/src/main/sphinx/connector/deltalake.rst\nindex 7630cb80074d5..96c7899e56972 100644\n--- a/presto-docs/src/main/sphinx/connector/deltalake.rst\n+++ b/presto-docs/src/main/sphinx/connector/deltalake.rst\n@@ -41,6 +41,9 @@ Property Name                                   Description\n                                                 metastore to find the location of Delta Lake tables.\n                                                 From the Delta Log at given location, schema and data\n                                                 file list of the table is found.\n+\n+``hive.metastore.catalog.name``                 Specifies the catalog name to be passed to the metastore.\n+\n ``delta.parquet-dereference-pushdown-enabled``  Enable pushing nested column dereferences into            ``true``\n                                                 table scan so that only the required fields\n                                                 selected in a ``struct`` data type column are selected.\n\ndiff --git a/presto-docs/src/main/sphinx/connector/hive.rst b/presto-docs/src/main/sphinx/connector/hive.rst\nindex 440aadf69c312..113c5520dd12c 100644\n--- a/presto-docs/src/main/sphinx/connector/hive.rst\n+++ b/presto-docs/src/main/sphinx/connector/hive.rst\n@@ -206,6 +206,8 @@ Property Name                                      Description\n                                                    error iterating through empty files.\n \n  ``hive.file-status-cache.max-retained-size``      Maximum size in bytes of the directory listing cache          ``0KB``\n+\n+ ``hive.metastore.catalog.name``                   Specifies the catalog name to be passed to the metastore.\n ================================================== ============================================================ ============\n \n Metastore Configuration Properties\n\ndiff --git a/presto-docs/src/main/sphinx/connector/hudi.rst b/presto-docs/src/main/sphinx/connector/hudi.rst\nindex 2581948de6bf4..6d6ef20b17187 100644\n--- a/presto-docs/src/main/sphinx/connector/hudi.rst\n+++ b/presto-docs/src/main/sphinx/connector/hudi.rst\n@@ -38,6 +38,8 @@ Property Name                           Description\n ======================================= ============================================= ===========\n ``hudi.metadata-table-enabled``         Fetch the list of file names and sizes from   false\n                                         Hudi's metadata table rather than storage.\n+``hive.metastore.catalog.name``         Specifies the catalog name to be passed to\n+                                        the metastore.\n ======================================= ============================================= ===========\n \n File-Based Metastore\n\ndiff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 74154c079f763..dbc1c7a730b6d 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -69,6 +69,8 @@ Property Name                                            Description\n                                                          ``iceberg.catalog.type`` is ``hive`` and ``hive.metastore``\n                                                          is ``thrift``.\n \n+``hive.metastore.catalog.name``                          Specifies the catalog name to be passed to the metastore.\n+\n ``iceberg.hive-statistics-merge-strategy``               Comma separated list of statistics to use from the\n                                                          Hive Metastore to override Iceberg table statistics.\n                                                          The available values are ``NUMBER_OF_DISTINCT_VALUES``\n\ndiff --git a/presto-hive-common/src/main/java/com/facebook/presto/hive/HiveCommonClientConfig.java b/presto-hive-common/src/main/java/com/facebook/presto/hive/HiveCommonClientConfig.java\nindex f86e252f9880d..9a2ec46dd00f9 100644\n--- a/presto-hive-common/src/main/java/com/facebook/presto/hive/HiveCommonClientConfig.java\n+++ b/presto-hive-common/src/main/java/com/facebook/presto/hive/HiveCommonClientConfig.java\n@@ -46,6 +46,7 @@ public class HiveCommonClientConfig\n     private boolean readNullMaskedParquetEncryptedValueEnabled;\n     private boolean useParquetColumnNames;\n     private boolean zstdJniDecompressionEnabled;\n+    private String catalogName;\n     private DataSize affinitySchedulingFileSectionSize = new DataSize(256, MEGABYTE);\n \n     public NodeSelectionStrategy getNodeSelectionStrategy()\n@@ -286,6 +287,19 @@ public HiveCommonClientConfig setZstdJniDecompressionEnabled(boolean zstdJniDeco\n         return this;\n     }\n \n+    public String getCatalogName()\n+    {\n+        return catalogName;\n+    }\n+\n+    @Config(\"hive.metastore.catalog.name\")\n+    @ConfigDescription(\"Specified property to store the metastore catalog name.\")\n+    public HiveCommonClientConfig setCatalogName(String catalogName)\n+    {\n+        this.catalogName = catalogName;\n+        return this;\n+    }\n+\n     @NotNull\n     public DataSize getAffinitySchedulingFileSectionSize()\n     {\n\ndiff --git a/presto-hive-common/src/main/java/com/facebook/presto/hive/MetadataUtils.java b/presto-hive-common/src/main/java/com/facebook/presto/hive/MetadataUtils.java\nindex b1428356b3644..67e4c4201968c 100644\n--- a/presto-hive-common/src/main/java/com/facebook/presto/hive/MetadataUtils.java\n+++ b/presto-hive-common/src/main/java/com/facebook/presto/hive/MetadataUtils.java\n@@ -30,6 +30,8 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Iterables;\n \n+import javax.annotation.Nullable;\n+\n import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n@@ -49,6 +51,10 @@\n \n public final class MetadataUtils\n {\n+    private static final String CATALOG_DB_SEPARATOR = \"#\";\n+    private static final String CATALOG_DB_THRIFT_NAME_MARKER = \"@\";\n+    private static final String DB_EMPTY_MARKER = \"!\";\n+    private static final String DEFAULT_DATABASE = \"default\";\n     private MetadataUtils() {}\n \n     public static Optional<DiscretePredicates> getDiscretePredicates(List<ColumnHandle> partitionColumns, List<HivePartition> partitions)\n@@ -160,4 +166,26 @@ private static Domain buildColumnDomain(ColumnHandle column, List<HivePartition>\n \n         return Domain.onlyNull(type);\n     }\n+\n+    /**\n+     * Constructs the schema name, including catalog name if applicable.\n+     *\n+     * @param schemaName the original schema name\n+     * @return the formatted schema name (Example - @catalog_name#schema_name)\n+     */\n+    public static String constructSchemaName(Optional<String> catalogName, @Nullable String schemaName)\n+    {\n+        if (!catalogName.isPresent() || DEFAULT_DATABASE.equals(schemaName) ||\n+                (schemaName != null && schemaName.contains(CATALOG_DB_SEPARATOR))) {\n+            return schemaName;\n+        }\n+\n+        StringBuilder catalogDatabaseName = new StringBuilder()\n+                .append(CATALOG_DB_THRIFT_NAME_MARKER)\n+                .append(catalogName.get()) // Safe since we checked isPresent()\n+                .append(CATALOG_DB_SEPARATOR)\n+                .append(schemaName == null ? \"\" : schemaName.isEmpty() ? DB_EMPTY_MARKER : schemaName);\n+\n+        return catalogDatabaseName.toString();\n+    }\n }\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Database.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Database.java\nindex bbc6ac56bc0bb..2d751e714c694 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Database.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Database.java\n@@ -39,6 +39,7 @@ public class Database\n     private final PrincipalType ownerType;\n     private final Optional<String> comment;\n     private final Map<String, String> parameters;\n+    private final Optional<String> catalogName;\n \n     @JsonCreator\n     public Database(\n@@ -47,7 +48,8 @@ public Database(\n             @JsonProperty(\"ownerName\") String ownerName,\n             @JsonProperty(\"ownerType\") PrincipalType ownerType,\n             @JsonProperty(\"comment\") Optional<String> comment,\n-            @JsonProperty(\"parameters\") Map<String, String> parameters)\n+            @JsonProperty(\"parameters\") Map<String, String> parameters,\n+            @JsonProperty(\"catalogName\") Optional<String> catalogName)\n     {\n         this.databaseName = requireNonNull(databaseName, \"databaseName is null\");\n         this.location = requireNonNull(location, \"location is null\");\n@@ -55,6 +57,7 @@ public Database(\n         this.ownerType = requireNonNull(ownerType, \"ownerType is null\");\n         this.comment = requireNonNull(comment, \"comment is null\");\n         this.parameters = ImmutableMap.copyOf(requireNonNull(parameters, \"parameters is null\"));\n+        this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n     }\n \n     @JsonProperty\n@@ -103,6 +106,12 @@ public static Builder builder(Database database)\n         return new Builder(database);\n     }\n \n+    @JsonProperty\n+    public Optional<String> getCatalogName()\n+    {\n+        return catalogName;\n+    }\n+\n     public static class Builder\n     {\n         private String databaseName;\n@@ -111,6 +120,7 @@ public static class Builder\n         private PrincipalType ownerType;\n         private Optional<String> comment = Optional.empty();\n         private Map<String, String> parameters = new LinkedHashMap<>();\n+        private Optional<String> catalogName = Optional.empty();\n \n         public Builder() {}\n \n@@ -122,6 +132,7 @@ public Builder(Database database)\n             this.ownerType = database.ownerType;\n             this.comment = database.comment;\n             this.parameters = database.parameters;\n+            this.catalogName = database.catalogName;\n         }\n \n         public Builder setDatabaseName(String databaseName)\n@@ -166,6 +177,12 @@ public Builder setParameters(Map<String, String> parameters)\n             return this;\n         }\n \n+        public Builder setCatalogName(Optional<String> catalogName)\n+        {\n+            this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n+            return this;\n+        }\n+\n         public Database build()\n         {\n             return new Database(\n@@ -174,7 +191,8 @@ public Database build()\n                     ownerName,\n                     ownerType,\n                     comment,\n-                    parameters);\n+                    parameters,\n+                    catalogName);\n         }\n     }\n \n@@ -188,6 +206,7 @@ public String toString()\n                 .add(\"ownerType\", ownerType)\n                 .add(\"comment\", comment)\n                 .add(\"parameters\", parameters)\n+                .add(\"catalogName\", catalogName)\n                 .toString();\n     }\n \n@@ -207,12 +226,13 @@ public boolean equals(Object o)\n                 Objects.equals(ownerName, database.ownerName) &&\n                 ownerType == database.ownerType &&\n                 Objects.equals(comment, database.comment) &&\n-                Objects.equals(parameters, database.parameters);\n+                Objects.equals(parameters, database.parameters) &&\n+                Objects.equals(catalogName, database.catalogName);\n     }\n \n     @Override\n     public int hashCode()\n     {\n-        return Objects.hash(databaseName, location, ownerName, ownerType, comment, parameters);\n+        return Objects.hash(databaseName, location, ownerName, ownerType, comment, parameters, catalogName);\n     }\n }\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Partition.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Partition.java\nindex fdf8e87a3153e..40163d4bc302e 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Partition.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Partition.java\n@@ -35,6 +35,7 @@\n @Immutable\n public class Partition\n {\n+    private final Optional<String> catalogName;\n     private final String databaseName;\n     private final String tableName;\n     private final List<String> values;\n@@ -63,6 +64,37 @@ public Partition(\n             @JsonProperty(\"lastDataCommitTime\") long lastDataCommitTime,\n             @JsonProperty(\"rowIdPartitionComponent\") Optional<byte[]> rowIdPartitionComponent)\n     {\n+        this(\n+                Optional.empty(),\n+                databaseName,\n+                tableName,\n+                values,\n+                storage,\n+                columns,\n+                parameters,\n+                partitionVersion,\n+                eligibleToIgnore,\n+                sealedPartition,\n+                createTime,\n+                lastDataCommitTime,\n+                rowIdPartitionComponent);\n+    }\n+    public Partition(\n+            Optional<String> catalogName,\n+            String databaseName,\n+            String tableName,\n+            List<String> values,\n+            Storage storage,\n+            List<Column> columns,\n+            Map<String, String> parameters,\n+            Optional<Long> partitionVersion,\n+            boolean eligibleToIgnore,\n+            boolean sealedPartition,\n+            int createTime,\n+            long lastDataCommitTime,\n+            Optional<byte[]> rowIdPartitionComponent)\n+    {\n+        this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n         this.databaseName = requireNonNull(databaseName, \"databaseName is null\");\n         this.tableName = requireNonNull(tableName, \"tableName is null\");\n         this.values = ImmutableList.copyOf(requireNonNull(values, \"values is null\"));\n@@ -77,6 +109,11 @@ public Partition(\n         this.rowIdPartitionComponent = requireNonNull(rowIdPartitionComponent);\n     }\n \n+    @JsonProperty\n+    public Optional<String> getCatalogName()\n+    {\n+        return catalogName;\n+    }\n     @JsonProperty\n     public String getDatabaseName()\n     {\n@@ -184,7 +221,8 @@ public boolean equals(Object o)\n         }\n \n         Partition partition = (Partition) o;\n-        return Objects.equals(databaseName, partition.databaseName) &&\n+        return Objects.equals(catalogName, partition.catalogName) &&\n+                Objects.equals(databaseName, partition.databaseName) &&\n                 Objects.equals(tableName, partition.tableName) &&\n                 Objects.equals(values, partition.values) &&\n                 Objects.equals(storage, partition.storage) &&\n@@ -200,7 +238,7 @@ public boolean equals(Object o)\n     @Override\n     public int hashCode()\n     {\n-        return Objects.hash(databaseName, tableName, values, storage, columns, parameters, partitionVersion, eligibleToIgnore, sealedPartition, createTime, lastDataCommitTime);\n+        return Objects.hash(catalogName, databaseName, tableName, values, storage, columns, parameters, partitionVersion, eligibleToIgnore, sealedPartition, createTime, lastDataCommitTime);\n     }\n \n     public static Builder builder()\n@@ -216,6 +254,7 @@ public static Builder builder(Partition partition)\n     public static class Builder\n     {\n         private final Storage.Builder storageBuilder;\n+        private Optional<String> catalogName;\n         private String databaseName;\n         private String tableName;\n         private List<String> values;\n@@ -236,6 +275,7 @@ private Builder()\n         private Builder(Partition partition)\n         {\n             this.storageBuilder = Storage.builder(partition.getStorage());\n+            this.catalogName = partition.getCatalogName();\n             this.databaseName = partition.getDatabaseName();\n             this.tableName = partition.getTableName();\n             this.values = partition.getValues();\n@@ -248,6 +288,11 @@ private Builder(Partition partition)\n             this.rowIdPartitionComponent = partition.getRowIdPartitionComponent();\n         }\n \n+        public Builder setCatalogName(Optional<String> catalogName)\n+        {\n+            this.catalogName = catalogName;\n+            return this;\n+        }\n         public Builder setDatabaseName(String databaseName)\n         {\n             this.databaseName = databaseName;\n@@ -332,7 +377,7 @@ public Builder setRowIdPartitionComponent(Optional<byte[]> rowIdPartitionCompone\n \n         public Partition build()\n         {\n-            return new Partition(databaseName, tableName, values, storageBuilder.build(), columns, parameters, partitionVersion, isEligibleToIgnore, isSealedPartition, createTime, lastDataCommitTime, rowIdPartitionComponent);\n+            return new Partition(catalogName, databaseName, tableName, values, storageBuilder.build(), columns, parameters, partitionVersion, isEligibleToIgnore, isSealedPartition, createTime, lastDataCommitTime, rowIdPartitionComponent);\n         }\n     }\n }\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Table.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Table.java\nindex 654e2fd4b475a..9f8ee10dba1dc 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Table.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/Table.java\n@@ -37,6 +37,7 @@\n @Immutable\n public class Table\n {\n+    private final Optional<String> catalogName;\n     private final String databaseName;\n     private final String tableName;\n     private final String owner;\n@@ -50,6 +51,7 @@ public class Table\n \n     @JsonCreator\n     public Table(\n+            @JsonProperty(\"catalogName\") Optional<String> catalogName,\n             @JsonProperty(\"databaseName\") String databaseName,\n             @JsonProperty(\"tableName\") String tableName,\n             @JsonProperty(\"owner\") String owner,\n@@ -61,6 +63,7 @@ public Table(\n             @JsonProperty(\"viewOriginalText\") Optional<String> viewOriginalText,\n             @JsonProperty(\"viewExpandedText\") Optional<String> viewExpandedText)\n     {\n+        this.catalogName = requireNonNull(catalogName, \"catalogName is null\");\n         this.databaseName = requireNonNull(databaseName, \"databaseName is null\");\n         this.tableName = requireNonNull(tableName, \"tableName is null\");\n         this.owner = requireNonNull(owner, \"owner is null\");\n@@ -73,6 +76,11 @@ public Table(\n         this.viewExpandedText = requireNonNull(viewExpandedText, \"viewExpandedText is null\");\n     }\n \n+    @JsonProperty\n+    public Optional<String> getCatalogName()\n+    {\n+        return catalogName;\n+    }\n     @JsonProperty\n     public String getDatabaseName()\n     {\n@@ -160,6 +168,7 @@ public static Builder builder(Table table)\n     public String toString()\n     {\n         return toStringHelper(this)\n+                .add(\"catalogName\", catalogName)\n                 .add(\"databaseName\", databaseName)\n                 .add(\"tableName\", tableName)\n                 .add(\"owner\", owner)\n@@ -184,7 +193,8 @@ public boolean equals(Object o)\n         }\n \n         Table table = (Table) o;\n-        return Objects.equals(databaseName, table.databaseName) &&\n+        return Objects.equals(catalogName, table.catalogName) &&\n+                Objects.equals(databaseName, table.databaseName) &&\n                 Objects.equals(tableName, table.tableName) &&\n                 Objects.equals(owner, table.owner) &&\n                 Objects.equals(tableType, table.tableType) &&\n@@ -200,6 +210,7 @@ public boolean equals(Object o)\n     public int hashCode()\n     {\n         return Objects.hash(\n+                catalogName,\n                 databaseName,\n                 tableName,\n                 owner,\n@@ -215,6 +226,7 @@ public int hashCode()\n     public static class Builder\n     {\n         private final Storage.Builder storageBuilder;\n+        private Optional<String> catalogName = Optional.empty();\n         private String databaseName;\n         private String tableName;\n         private String owner;\n@@ -232,6 +244,7 @@ private Builder()\n \n         private Builder(Table table)\n         {\n+            catalogName = table.catalogName;\n             databaseName = table.databaseName;\n             tableName = table.tableName;\n             owner = table.owner;\n@@ -244,6 +257,11 @@ private Builder(Table table)\n             viewExpandedText = table.viewExpandedText;\n         }\n \n+        public Builder setCatalogName(Optional<String> catalogName)\n+        {\n+            this.catalogName = catalogName;\n+            return this;\n+        }\n         public Builder setDatabaseName(String databaseName)\n         {\n             this.databaseName = databaseName;\n@@ -324,6 +342,7 @@ public Builder withStorage(Consumer<Storage.Builder> consumer)\n         public Table build()\n         {\n             return new Table(\n+                    catalogName,\n                     databaseName,\n                     tableName,\n                     owner,\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileMetastoreModule.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileMetastoreModule.java\nindex 212e0579433f6..c37a22af47b7f 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileMetastoreModule.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileMetastoreModule.java\n@@ -13,20 +13,22 @@\n  */\n package com.facebook.presto.hive.metastore.file;\n \n+import com.facebook.airlift.configuration.AbstractConfigurationAwareModule;\n import com.facebook.presto.hive.ForCachingHiveMetastore;\n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.InMemoryCachingHiveMetastore;\n import com.google.inject.Binder;\n-import com.google.inject.Module;\n import com.google.inject.Scopes;\n \n import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n+import static com.google.common.base.Preconditions.checkArgument;\n import static java.util.Objects.requireNonNull;\n import static org.weakref.jmx.ObjectNames.generatedNameOf;\n import static org.weakref.jmx.guice.ExportBinder.newExporter;\n \n public class FileMetastoreModule\n-        implements Module\n+        extends AbstractConfigurationAwareModule\n {\n     private final String connectorId;\n \n@@ -36,8 +38,9 @@ public FileMetastoreModule(String connectorId)\n     }\n \n     @Override\n-    public void configure(Binder binder)\n+    public void setup(Binder binder)\n     {\n+        checkArgument(buildConfigObject(HiveCommonClientConfig.class).getCatalogName() == null, \"'hive.metastore.catalog.name' should not be set for file metastore\");\n         configBinder(binder).bindConfig(FileHiveMetastoreConfig.class);\n         binder.bind(ExtendedHiveMetastore.class).annotatedWith(ForCachingHiveMetastore.class).to(FileHiveMetastore.class).in(Scopes.SINGLETON);\n         binder.bind(ExtendedHiveMetastore.class).to(InMemoryCachingHiveMetastore.class).in(Scopes.SINGLETON);\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/TableMetadata.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/TableMetadata.java\nindex f9520ea40d33e..705796b940e3b 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/TableMetadata.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/TableMetadata.java\n@@ -318,6 +318,7 @@ public TableMetadata withColumnStatistics(Map<String, HiveColumnStatistics> colu\n     public Table toTable(String databaseName, String tableName, String location)\n     {\n         return new Table(\n+                Optional.empty(),\n                 databaseName,\n                 tableName,\n                 owner,\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/GlueMetastoreModule.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/GlueMetastoreModule.java\nindex 98d7c3aa1e3dc..0bc9bee6b81fc 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/GlueMetastoreModule.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/GlueMetastoreModule.java\n@@ -14,11 +14,12 @@\n package com.facebook.presto.hive.metastore.glue;\n \n import com.facebook.airlift.concurrent.BoundedExecutor;\n+import com.facebook.airlift.configuration.AbstractConfigurationAwareModule;\n import com.facebook.presto.hive.ForCachingHiveMetastore;\n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.InMemoryCachingHiveMetastore;\n import com.google.inject.Binder;\n-import com.google.inject.Module;\n import com.google.inject.Provides;\n import com.google.inject.Scopes;\n import com.google.inject.Singleton;\n@@ -27,6 +28,7 @@\n \n import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n+import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.util.concurrent.MoreExecutors.directExecutor;\n import static java.util.Objects.requireNonNull;\n import static java.util.concurrent.Executors.newCachedThreadPool;\n@@ -34,7 +36,7 @@\n import static org.weakref.jmx.guice.ExportBinder.newExporter;\n \n public class GlueMetastoreModule\n-        implements Module\n+        extends AbstractConfigurationAwareModule\n {\n     private final String connectorId;\n \n@@ -44,8 +46,9 @@ public GlueMetastoreModule(String connectorId)\n     }\n \n     @Override\n-    public void configure(Binder binder)\n+    public void setup(Binder binder)\n     {\n+        checkArgument(buildConfigObject(HiveCommonClientConfig.class).getCatalogName() == null, \"'hive.metastore.catalog.name' should not be set for glue metastore\");\n         configBinder(binder).bindConfig(GlueHiveMetastoreConfig.class);\n         binder.bind(GlueHiveMetastore.class).in(Scopes.SINGLETON);\n         binder.bind(ExtendedHiveMetastore.class).annotatedWith(ForCachingHiveMetastore.class).to(GlueHiveMetastore.class).in(Scopes.SINGLETON);\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/converter/GlueToPrestoConverter.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/converter/GlueToPrestoConverter.java\nindex 542cb8b74f609..83de7d0150fc7 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/converter/GlueToPrestoConverter.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/glue/converter/GlueToPrestoConverter.java\n@@ -166,6 +166,7 @@ public Partition apply(com.amazonaws.services.glue.model.Partition gluePartition\n             }\n \n             Partition.Builder partitionBuilder = Partition.builder()\n+                    .setCatalogName(Optional.empty())\n                     .setDatabaseName(databaseName)\n                     .setTableName(tableName)\n                     .setValues(gluePartition.getValues()) // No memoization benefit\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastore.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastore.java\nindex d5172c78ceefc..cf23a1e85aadf 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastore.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastore.java\n@@ -62,6 +62,11 @@ public interface HiveMetastore\n \n     MetastoreOperationResult alterTable(MetastoreContext metastoreContext, String databaseName, String tableName, Table table);\n \n+    default List<String> getDatabases(MetastoreContext metastoreContext, String pattern)\n+    {\n+        return getAllDatabases(metastoreContext);\n+    }\n+\n     List<String> getAllDatabases(MetastoreContext metastoreContext);\n \n     Optional<List<String>> getAllTables(MetastoreContext metastoreContext, String databaseName);\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClient.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClient.java\nindex 7f240bdbd4404..4880eedcdd6ee 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClient.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClient.java\n@@ -50,6 +50,9 @@ public interface HiveMetastoreClient\n     String getDelegationToken(String owner, String renewer)\n             throws TException;\n \n+    List<String> getDatabases(String pattern)\n+            throws TException;\n+\n     List<String> getAllDatabases()\n             throws TException;\n \n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClientFactory.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClientFactory.java\nindex ff58232057acc..19787c95fbad1 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClientFactory.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/HiveMetastoreClientFactory.java\n@@ -14,6 +14,7 @@\n package com.facebook.presto.hive.metastore.thrift;\n \n import com.facebook.airlift.security.pem.PemReader;\n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.authentication.HiveMetastoreAuthentication;\n import com.facebook.presto.spi.PrestoException;\n@@ -55,22 +56,25 @@ public class HiveMetastoreClientFactory\n     private final Optional<HostAndPort> socksProxy;\n     private final int timeoutMillis;\n     private final HiveMetastoreAuthentication metastoreAuthentication;\n+    private final String catalogName;\n     public static final String PROTOCOL = \"TLS\";\n \n     public HiveMetastoreClientFactory(\n             Optional<SSLContext> sslContext,\n             Optional<HostAndPort> socksProxy,\n             Duration timeout,\n-            HiveMetastoreAuthentication metastoreAuthentication)\n+            HiveMetastoreAuthentication metastoreAuthentication,\n+            String catalogName)\n     {\n         this.sslContext = requireNonNull(sslContext, \"sslContext is null\");\n         this.socksProxy = requireNonNull(socksProxy, \"socksProxy is null\");\n         this.timeoutMillis = toIntExact(timeout.toMillis());\n         this.metastoreAuthentication = requireNonNull(metastoreAuthentication, \"metastoreAuthentication is null\");\n+        this.catalogName = catalogName;\n     }\n \n     @Inject\n-    public HiveMetastoreClientFactory(MetastoreClientConfig metastoreClientConfig, ThriftHiveMetastoreConfig thriftHiveMetastoreConfig, HiveMetastoreAuthentication metastoreAuthentication)\n+    public HiveMetastoreClientFactory(MetastoreClientConfig metastoreClientConfig, ThriftHiveMetastoreConfig thriftHiveMetastoreConfig, HiveMetastoreAuthentication metastoreAuthentication, HiveCommonClientConfig hiveCommonClientConfig)\n     {\n         this(buildSslContext(thriftHiveMetastoreConfig.isTlsEnabled(),\n                 Optional.ofNullable(thriftHiveMetastoreConfig.getKeystorePath()),\n@@ -78,13 +82,13 @@ public HiveMetastoreClientFactory(MetastoreClientConfig metastoreClientConfig, T\n                 Optional.ofNullable(thriftHiveMetastoreConfig.getTruststorePath()),\n                 Optional.ofNullable(thriftHiveMetastoreConfig.getTrustStorePassword())),\n                 Optional.ofNullable(metastoreClientConfig.getMetastoreSocksProxy()),\n-                metastoreClientConfig.getMetastoreTimeout(), metastoreAuthentication);\n+                metastoreClientConfig.getMetastoreTimeout(), metastoreAuthentication, hiveCommonClientConfig.getCatalogName());\n     }\n \n     public HiveMetastoreClient create(HostAndPort address, Optional<String> token)\n             throws TTransportException\n     {\n-        return new ThriftHiveMetastoreClient(Transport.create(address, sslContext, socksProxy, timeoutMillis, metastoreAuthentication, token));\n+        return new ThriftHiveMetastoreClient(Transport.create(address, sslContext, socksProxy, timeoutMillis, metastoreAuthentication, token), catalogName);\n     }\n \n     /**\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastore.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastore.java\nindex 1171f9a85458d..24056038648c9 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastore.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastore.java\n@@ -321,6 +321,23 @@ public List<NotNullConstraint<String>> getNotNullConstraints(MetastoreContext me\n         }\n     }\n \n+    @Override\n+    public List<String> getDatabases(MetastoreContext context, String pattern)\n+    {\n+        try {\n+            return retry()\n+                    .stopOnIllegalExceptions()\n+                    .run(\"getDatabases\", stats.getGetDatabases().wrap(() ->\n+                            getMetastoreClientThenCall(context, client -> client.getDatabases(pattern))));\n+        }\n+        catch (TException e) {\n+            throw new PrestoException(HIVE_METASTORE_ERROR, e);\n+        }\n+        catch (Exception e) {\n+            throw propagate(e);\n+        }\n+    }\n+\n     @Override\n     public List<String> getAllDatabases(MetastoreContext context)\n     {\n@@ -1646,15 +1663,16 @@ public MetastoreOperationResult addConstraint(MetastoreContext metastoreContext,\n \n         if (tableConstraint instanceof PrimaryKeyConstraint) {\n             for (String column : constraintColumns) {\n-                primaryKeyConstraint.add(\n-                        new SQLPrimaryKey(table.getDbName(),\n-                                table.getTableName(),\n-                                column,\n-                                keySequence++,\n-                                tableConstraint.getName().orElse(null),\n-                                tableConstraint.isEnabled(),\n-                                tableConstraint.isEnforced(),\n-                                tableConstraint.isRely()));\n+                SQLPrimaryKey sqlPrimaryKey = new SQLPrimaryKey(table.getDbName(),\n+                        table.getTableName(),\n+                        column,\n+                        keySequence++,\n+                        tableConstraint.getName().orElse(null),\n+                        tableConstraint.isEnabled(),\n+                        tableConstraint.isEnforced(),\n+                        tableConstraint.isRely());\n+                sqlPrimaryKey.setCatName(table.getCatName());\n+                primaryKeyConstraint.add(sqlPrimaryKey);\n             }\n             callableName = \"addPrimaryKeyConstraint\";\n             apiStats = stats.getAddPrimaryKeyConstraint();\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreClient.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreClient.java\nindex 4b6ef497b17e1..f34955d09432e 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreClient.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreClient.java\n@@ -64,6 +64,8 @@\n import java.util.Map;\n import java.util.Optional;\n \n+import static com.facebook.presto.hive.MetadataUtils.constructSchemaName;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n import static java.util.Collections.emptyList;\n import static java.util.Objects.requireNonNull;\n import static org.apache.thrift.TApplicationException.UNKNOWN_METHOD;\n@@ -73,17 +75,20 @@ public class ThriftHiveMetastoreClient\n {\n     private final TTransport transport;\n     private final ThriftHiveMetastore.Client client;\n+    private final Optional<String> catalogName;\n \n-    public ThriftHiveMetastoreClient(TTransport transport)\n+    public ThriftHiveMetastoreClient(TTransport transport, String catalogName)\n     {\n         this.transport = requireNonNull(transport, \"transport is null\");\n         this.client = new ThriftHiveMetastore.Client(new TBinaryProtocol(transport));\n+        this.catalogName = Optional.ofNullable(catalogName);\n     }\n \n-    public ThriftHiveMetastoreClient(TProtocol protocol)\n+    public ThriftHiveMetastoreClient(TProtocol protocol, String catalogName)\n     {\n         this.transport = protocol.getTransport();\n         this.client = new ThriftHiveMetastore.Client(protocol);\n+        this.catalogName = Optional.ofNullable(catalogName);\n     }\n \n     @Override\n@@ -98,11 +103,20 @@ public String getDelegationToken(String owner, String renewer)\n     {\n         return client.get_delegation_token(owner, renewer);\n     }\n+    @Override\n+    public List<String> getDatabases(String pattern)\n+            throws TException\n+    {\n+        return client.get_databases(constructSchemaName(catalogName, pattern));\n+    }\n \n     @Override\n     public List<String> getAllDatabases()\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            return getDatabases(constructSchemaName(catalogName, null));\n+        }\n         return client.get_all_databases();\n     }\n \n@@ -110,27 +124,30 @@ public List<String> getAllDatabases()\n     public Database getDatabase(String dbName)\n             throws TException\n     {\n-        return client.get_database(dbName);\n+        return client.get_database(constructSchemaName(catalogName, dbName));\n     }\n \n     @Override\n     public List<String> getAllTables(String databaseName)\n             throws TException\n     {\n-        return client.get_all_tables(databaseName);\n+        return client.get_all_tables(constructSchemaName(catalogName, databaseName));\n     }\n \n     @Override\n     public List<String> getTableNamesByFilter(String databaseName, String filter)\n             throws TException\n     {\n-        return client.get_table_names_by_filter(databaseName, filter, (short) -1);\n+        return client.get_table_names_by_filter(constructSchemaName(catalogName, databaseName), filter, (short) -1);\n     }\n \n     @Override\n     public void createDatabase(Database database)\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            database.setCatalogName(catalogName.get());\n+        }\n         client.create_database(database);\n     }\n \n@@ -138,20 +155,23 @@ public void createDatabase(Database database)\n     public void dropDatabase(String databaseName, boolean deleteData, boolean cascade)\n             throws TException\n     {\n-        client.drop_database(databaseName, deleteData, cascade);\n+        client.drop_database(constructSchemaName(catalogName, databaseName), deleteData, cascade);\n     }\n \n     @Override\n     public void alterDatabase(String databaseName, Database database)\n             throws TException\n     {\n-        client.alter_database(databaseName, database);\n+        client.alter_database(constructSchemaName(catalogName, databaseName), database);\n     }\n \n     @Override\n     public void createTable(Table table)\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            table.setCatName(catalogName.get());\n+        }\n         client.create_table(table);\n     }\n \n@@ -159,6 +179,9 @@ public void createTable(Table table)\n     public void createTableWithConstraints(Table table, List<SQLPrimaryKey> primaryKeys, List<SQLUniqueConstraint> uniqueConstraints, List<SQLNotNullConstraint> notNullConstraints)\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            table.setCatName(catalogName.get());\n+        }\n         client.create_table_with_constraints(table, primaryKeys, emptyList(), uniqueConstraints, notNullConstraints, emptyList(), emptyList());\n     }\n \n@@ -166,28 +189,28 @@ public void createTableWithConstraints(Table table, List<SQLPrimaryKey> primaryK\n     public void dropTable(String databaseName, String name, boolean deleteData)\n             throws TException\n     {\n-        client.drop_table(databaseName, name, deleteData);\n+        client.drop_table(constructSchemaName(catalogName, databaseName), name, deleteData);\n     }\n \n     @Override\n     public void alterTable(String databaseName, String tableName, Table newTable)\n             throws TException\n     {\n-        client.alter_table(databaseName, tableName, newTable);\n+        client.alter_table(constructSchemaName(catalogName, databaseName), tableName, newTable);\n     }\n \n     @Override\n     public Table getTable(String databaseName, String tableName)\n             throws TException\n     {\n-        return client.get_table(databaseName, tableName);\n+        return client.get_table(constructSchemaName(catalogName, databaseName), tableName);\n     }\n \n     @Override\n     public List<FieldSchema> getFields(String databaseName, String tableName)\n             throws TException\n     {\n-        return client.get_fields(databaseName, tableName);\n+        return client.get_fields(constructSchemaName(catalogName, databaseName), tableName);\n     }\n \n     @Override\n@@ -195,6 +218,9 @@ public List<ColumnStatisticsObj> getTableColumnStatistics(String databaseName, S\n             throws TException\n     {\n         TableStatsRequest tableStatsRequest = new TableStatsRequest(databaseName, tableName, columnNames);\n+        if (catalogName.isPresent()) {\n+            tableStatsRequest.setCatName(catalogName.get());\n+        }\n         return client.get_table_statistics_req(tableStatsRequest).getTableStats();\n     }\n \n@@ -203,6 +229,9 @@ public void setTableColumnStatistics(String databaseName, String tableName, List\n             throws TException\n     {\n         ColumnStatisticsDesc statisticsDescription = new ColumnStatisticsDesc(true, databaseName, tableName);\n+        if (catalogName.isPresent()) {\n+            statisticsDescription.setCatName(catalogName.get());\n+        }\n         ColumnStatistics request = new ColumnStatistics(statisticsDescription, statistics);\n         client.update_table_column_statistics(request);\n     }\n@@ -211,7 +240,7 @@ public void setTableColumnStatistics(String databaseName, String tableName, List\n     public void deleteTableColumnStatistics(String databaseName, String tableName, String columnName)\n             throws TException\n     {\n-        client.delete_table_column_statistics(databaseName, tableName, columnName);\n+        client.delete_table_column_statistics(constructSchemaName(catalogName, databaseName), tableName, columnName);\n     }\n \n     @Override\n@@ -219,6 +248,9 @@ public Map<String, List<ColumnStatisticsObj>> getPartitionColumnStatistics(Strin\n             throws TException\n     {\n         PartitionsStatsRequest partitionsStatsRequest = new PartitionsStatsRequest(databaseName, tableName, columnNames, partitionNames);\n+        if (catalogName.isPresent()) {\n+            partitionsStatsRequest.setCatName(catalogName.get());\n+        }\n         return client.get_partitions_statistics_req(partitionsStatsRequest).getPartStats();\n     }\n \n@@ -228,6 +260,9 @@ public void setPartitionColumnStatistics(String databaseName, String tableName,\n     {\n         ColumnStatisticsDesc statisticsDescription = new ColumnStatisticsDesc(false, databaseName, tableName);\n         statisticsDescription.setPartName(partitionName);\n+        if (catalogName.isPresent()) {\n+            statisticsDescription.setCatName(catalogName.get());\n+        }\n         ColumnStatistics request = new ColumnStatistics(statisticsDescription, statistics);\n         client.update_partition_column_statistics(request);\n     }\n@@ -236,27 +271,34 @@ public void setPartitionColumnStatistics(String databaseName, String tableName,\n     public void deletePartitionColumnStatistics(String databaseName, String tableName, String partitionName, String columnName)\n             throws TException\n     {\n-        client.delete_partition_column_statistics(databaseName, tableName, partitionName, columnName);\n+        client.delete_partition_column_statistics(constructSchemaName(catalogName, databaseName), tableName, partitionName, columnName);\n     }\n \n     @Override\n     public List<String> getPartitionNames(String databaseName, String tableName)\n             throws TException\n     {\n-        return client.get_partition_names(databaseName, tableName, (short) -1);\n+        return client.get_partition_names(constructSchemaName(catalogName, databaseName), tableName, (short) -1);\n     }\n \n     @Override\n     public List<String> getPartitionNamesFiltered(String databaseName, String tableName, List<String> partitionValues)\n             throws TException\n     {\n-        return client.get_partition_names_ps(databaseName, tableName, partitionValues, (short) -1);\n+        return client.get_partition_names_ps(constructSchemaName(catalogName, databaseName), tableName, partitionValues, (short) -1);\n     }\n \n     @Override\n     public int addPartitions(List<Partition> newPartitions)\n             throws TException\n     {\n+        // Check if catalog name is present, update each partition in the newPartitions list by setting its catalog name.\n+        if (catalogName.isPresent()) {\n+            String catalog = catalogName.get();\n+            for (Partition partition : newPartitions) {\n+                partition.setCatName(catalog);\n+            }\n+        }\n         return client.add_partitions(newPartitions);\n     }\n \n@@ -264,28 +306,28 @@ public int addPartitions(List<Partition> newPartitions)\n     public boolean dropPartition(String databaseName, String tableName, List<String> partitionValues, boolean deleteData)\n             throws TException\n     {\n-        return client.drop_partition(databaseName, tableName, partitionValues, deleteData);\n+        return client.drop_partition(constructSchemaName(catalogName, databaseName), tableName, partitionValues, deleteData);\n     }\n \n     @Override\n     public void alterPartition(String databaseName, String tableName, Partition partition)\n             throws TException\n     {\n-        client.alter_partition(databaseName, tableName, partition);\n+        client.alter_partition(constructSchemaName(catalogName, databaseName), tableName, partition);\n     }\n \n     @Override\n     public Partition getPartition(String databaseName, String tableName, List<String> partitionValues)\n             throws TException\n     {\n-        return client.get_partition(databaseName, tableName, partitionValues);\n+        return client.get_partition(constructSchemaName(catalogName, databaseName), tableName, partitionValues);\n     }\n \n     @Override\n     public List<Partition> getPartitionsByNames(String databaseName, String tableName, List<String> partitionNames)\n             throws TException\n     {\n-        return client.get_partitions_by_names(databaseName, tableName, partitionNames);\n+        return client.get_partitions_by_names(constructSchemaName(catalogName, databaseName), tableName, partitionNames);\n     }\n \n     @Override\n@@ -454,7 +496,9 @@ public Optional<PrimaryKeysResponse> getPrimaryKey(String dbName, String tableNa\n             throws TException\n     {\n         PrimaryKeysRequest pkRequest = new PrimaryKeysRequest(dbName, tableName);\n-        PrimaryKeysResponse pkResponse;\n+        if (catalogName.isPresent()) {\n+            pkRequest.setCatName(catalogName.get());\n+        }\n \n         try {\n             return Optional.of(client.get_primary_keys(pkRequest));\n@@ -473,8 +517,10 @@ public Optional<PrimaryKeysResponse> getPrimaryKey(String dbName, String tableNa\n     public Optional<UniqueConstraintsResponse> getUniqueConstraints(String catName, String dbName, String tableName)\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            catName = catalogName.get();\n+        }\n         UniqueConstraintsRequest uniqueConstraintsRequest = new UniqueConstraintsRequest(catName, dbName, tableName);\n-        UniqueConstraintsResponse uniqueConstraintsResponse;\n \n         try {\n             return Optional.of(client.get_unique_constraints(uniqueConstraintsRequest));\n@@ -491,8 +537,10 @@ public Optional<UniqueConstraintsResponse> getUniqueConstraints(String catName,\n     public Optional<NotNullConstraintsResponse> getNotNullConstraints(String catName, String dbName, String tableName)\n             throws TException\n     {\n+        if (catalogName.isPresent()) {\n+            catName = catalogName.get();\n+        }\n         NotNullConstraintsRequest notNullConstraintsRequest = new NotNullConstraintsRequest(catName, dbName, tableName);\n-        NotNullConstraintsResponse notNullConstraintsResponse;\n \n         try {\n             return Optional.of(client.get_not_null_constraints(notNullConstraintsRequest));\n@@ -510,6 +558,9 @@ public void dropConstraint(String dbName, String tableName, String constraintNam\n             throws TException\n     {\n         DropConstraintRequest dropConstraintRequest = new DropConstraintRequest(dbName, tableName, constraintName);\n+        if (catalogName.isPresent()) {\n+            dropConstraintRequest.setCatName(catalogName.get());\n+        }\n         client.drop_constraint(dropConstraintRequest);\n     }\n \n@@ -517,7 +568,16 @@ public void dropConstraint(String dbName, String tableName, String constraintNam\n     public void addUniqueConstraint(List<SQLUniqueConstraint> constraint)\n             throws TException\n     {\n-        AddUniqueConstraintRequest addUniqueConstraintRequest = new AddUniqueConstraintRequest(constraint);\n+        List<SQLUniqueConstraint> updatedConstraints = constraint;\n+        if (catalogName.isPresent()) {\n+            updatedConstraints = constraint.stream().map(uniqueConstraint -> {\n+                uniqueConstraint = uniqueConstraint.deepCopy();\n+                uniqueConstraint.setCatName(catalogName.get());\n+                return uniqueConstraint;\n+            })\n+            .collect(toImmutableList());\n+        }\n+        AddUniqueConstraintRequest addUniqueConstraintRequest = new AddUniqueConstraintRequest(updatedConstraints);\n         client.add_unique_constraint(addUniqueConstraintRequest);\n     }\n \n@@ -525,7 +585,16 @@ public void addUniqueConstraint(List<SQLUniqueConstraint> constraint)\n     public void addPrimaryKeyConstraint(List<SQLPrimaryKey> constraint)\n             throws TException\n     {\n-        AddPrimaryKeyRequest addPrimaryKeyRequest = new AddPrimaryKeyRequest(constraint);\n+        List<SQLPrimaryKey> updatedConstraints = constraint;\n+        if (catalogName.isPresent()) {\n+            updatedConstraints = constraint.stream().map(primaryKeyConstraint -> {\n+                primaryKeyConstraint = primaryKeyConstraint.deepCopy();\n+                primaryKeyConstraint.setCatName(catalogName.get());\n+                return primaryKeyConstraint;\n+            })\n+            .collect(toImmutableList());\n+        }\n+        AddPrimaryKeyRequest addPrimaryKeyRequest = new AddPrimaryKeyRequest(updatedConstraints);\n         client.add_primary_key(addPrimaryKeyRequest);\n     }\n \n@@ -533,7 +602,16 @@ public void addPrimaryKeyConstraint(List<SQLPrimaryKey> constraint)\n     public void addNotNullConstraint(List<SQLNotNullConstraint> constraint)\n             throws TException\n     {\n-        AddNotNullConstraintRequest addNotNullConstraintRequest = new AddNotNullConstraintRequest(constraint);\n+        List<SQLNotNullConstraint> updatedConstraints = constraint;\n+        if (catalogName.isPresent()) {\n+            updatedConstraints = constraint.stream().map(notNullConstraint -> {\n+                notNullConstraint = notNullConstraint.deepCopy();\n+                notNullConstraint.setCatName(catalogName.get());\n+                return notNullConstraint;\n+            })\n+            .collect(toImmutableList());\n+        }\n+        AddNotNullConstraintRequest addNotNullConstraintRequest = new AddNotNullConstraintRequest(updatedConstraints);\n         client.add_not_null_constraint(addNotNullConstraintRequest);\n     }\n }\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreStats.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreStats.java\nindex 9b152bf2d6596..543c1a2ba27da 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreStats.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftHiveMetastoreStats.java\n@@ -19,6 +19,7 @@\n public class ThriftHiveMetastoreStats\n {\n     private final HiveMetastoreApiStats getAllDatabases = new HiveMetastoreApiStats();\n+    private final HiveMetastoreApiStats getDatabases = new HiveMetastoreApiStats();\n     private final HiveMetastoreApiStats getDatabase = new HiveMetastoreApiStats();\n     private final HiveMetastoreApiStats getAllTables = new HiveMetastoreApiStats();\n     private final HiveMetastoreApiStats getAllViews = new HiveMetastoreApiStats();\n@@ -61,6 +62,13 @@ public class ThriftHiveMetastoreStats\n     private final HiveMetastoreApiStats addPrimaryKeyConstraint = new HiveMetastoreApiStats();\n     private final HiveMetastoreApiStats addNotNullConstraint = new HiveMetastoreApiStats();\n \n+    @Managed\n+    @Nested\n+    public HiveMetastoreApiStats getGetDatabases()\n+    {\n+        return getDatabases;\n+    }\n+\n     @Managed\n     @Nested\n     public HiveMetastoreApiStats getGetAllDatabases()\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java\nindex 40a1afcc6398c..74858baba829c 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/thrift/ThriftMetastoreUtil.java\n@@ -150,6 +150,7 @@ public static org.apache.hadoop.hive.metastore.api.Database toMetastoreApiDataba\n         result.setOwnerType(toMetastoreApiPrincipalType(database.getOwnerType()));\n         database.getComment().ifPresent(result::setDescription);\n         result.setParameters(database.getParameters());\n+        database.getCatalogName().ifPresent(result::setCatalogName);\n         return result;\n     }\n \n@@ -157,6 +158,7 @@ public static org.apache.hadoop.hive.metastore.api.Table toMetastoreApiTable(Tab\n     {\n         org.apache.hadoop.hive.metastore.api.Table result = new org.apache.hadoop.hive.metastore.api.Table();\n \n+        table.getCatalogName().ifPresent(result::setCatName);\n         result.setDbName(table.getDatabaseName());\n         result.setTableName(table.getTableName());\n         result.setOwner(table.getOwner());\n@@ -371,6 +373,7 @@ public static org.apache.hadoop.hive.metastore.api.Partition toMetastoreApiParti\n     public static org.apache.hadoop.hive.metastore.api.Partition toMetastoreApiPartition(Partition partition, ColumnConverter columnConverter)\n     {\n         org.apache.hadoop.hive.metastore.api.Partition result = new org.apache.hadoop.hive.metastore.api.Partition();\n+        partition.getCatalogName().ifPresent(result::setCatName);\n         result.setDbName(partition.getDatabaseName());\n         result.setTableName(partition.getTableName());\n         result.setValues(partition.getValues());\n@@ -506,6 +509,7 @@ public static Partition fromMetastoreApiPartition(org.apache.hadoop.hive.metasto\n         }\n \n         Partition.Builder partitionBuilder = Partition.builder()\n+                .setCatalogName(Optional.ofNullable(partition.getCatName()))\n                 .setDatabaseName(partition.getDbName())\n                 .setTableName(partition.getTableName())\n                 .setValues(partition.getValues())\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java b/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\nindex 2c96999e3242d..7ab586065f8c0 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java\n@@ -492,6 +492,13 @@ public SemiTransactionalHiveMetastore getMetastore()\n         return metastore;\n     }\n \n+    @Override\n+    public boolean schemaExists(ConnectorSession session, String schemaName)\n+    {\n+        Optional<Database> database = metastore.getDatabase(getMetastoreContext(session), schemaName);\n+        return database.isPresent();\n+    }\n+\n     @Override\n     public HiveStatisticsProvider getHiveStatisticsProvider()\n     {\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HivePartitionObjectBuilder.java b/presto-hive/src/main/java/com/facebook/presto/hive/HivePartitionObjectBuilder.java\nindex c6ef5d983a65d..786bedc460fa0 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HivePartitionObjectBuilder.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HivePartitionObjectBuilder.java\n@@ -47,6 +47,7 @@ public Partition buildPartitionObject(\n                         param -> extraParametersBuilder.put(\"user_supplied\", param));\n \n         return Partition.builder()\n+                .setCatalogName(table.getCatalogName())\n                 .setDatabaseName(table.getDatabaseName())\n                 .setTableName(table.getTableName())\n                 .setColumns(table.getDataColumns())\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/SyncPartitionMetadataProcedure.java b/presto-hive/src/main/java/com/facebook/presto/hive/SyncPartitionMetadataProcedure.java\nindex 67f6b59d0f924..3bdbb0f0f7b8a 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/SyncPartitionMetadataProcedure.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/SyncPartitionMetadataProcedure.java\n@@ -260,6 +260,7 @@ private static void dropPartitions(\n     private static Partition buildPartitionObject(ConnectorSession session, Table table, String partitionName)\n     {\n         return Partition.builder()\n+                .setCatalogName(table.getCatalogName())\n                 .setDatabaseName(table.getDatabaseName())\n                 .setTableName(table.getTableName())\n                 .setColumns(table.getDataColumns())\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\nindex 1dcbc670b8957..9b237a0ee6440 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n@@ -194,6 +194,13 @@ public ManifestFileCache getManifestFileCache()\n         return manifestFileCache;\n     }\n \n+    @Override\n+    public boolean schemaExists(ConnectorSession session, String schemaName)\n+    {\n+        Optional<Database> database = metastore.getDatabase(getMetastoreContext(session), schemaName);\n+        return database.isPresent();\n+    }\n+\n     @Override\n     protected org.apache.iceberg.Table getRawIcebergTable(ConnectorSession session, SchemaTableName schemaTableName)\n     {\n",
    "test_patch": "diff --git a/presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java b/presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java\nindex 5db4217f6f17e..b543c456cd308 100644\n--- a/presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java\n+++ b/presto-hive-common/src/test/java/com/facebook/presto/hive/TestHiveCommonClientConfig.java\n@@ -49,6 +49,7 @@ public void testDefaults()\n                 .setParquetBatchReaderVerificationEnabled(false)\n                 .setParquetBatchReadOptimizationEnabled(false)\n                 .setReadNullMaskedParquetEncryptedValue(false)\n+                .setCatalogName(null)\n                 .setAffinitySchedulingFileSectionSize(new DataSize(256, MEGABYTE)));\n     }\n \n@@ -74,6 +75,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"hive.enable-parquet-batch-reader-verification\", \"true\")\n                 .put(\"hive.parquet-batch-read-optimization-enabled\", \"true\")\n                 .put(\"hive.read-null-masked-parquet-encrypted-value-enabled\", \"true\")\n+                .put(\"hive.metastore.catalog.name\", \"catalogName\")\n                 .put(\"hive.affinity-scheduling-file-section-size\", \"512MB\")\n                 .build();\n \n@@ -96,6 +98,7 @@ public void testExplicitPropertyMappings()\n                 .setParquetBatchReaderVerificationEnabled(true)\n                 .setParquetBatchReadOptimizationEnabled(true)\n                 .setReadNullMaskedParquetEncryptedValue(true)\n+                .setCatalogName(\"catalogName\")\n                 .setAffinitySchedulingFileSectionSize(new DataSize(512, MEGABYTE));\n \n         ConfigAssertions.assertFullMapping(properties, expected);\n\ndiff --git a/presto-hive-common/src/test/java/com/facebook/presto/hive/TestMetadataUtils.java b/presto-hive-common/src/test/java/com/facebook/presto/hive/TestMetadataUtils.java\nnew file mode 100644\nindex 0000000000000..d8bea3a4396f5\n--- /dev/null\n+++ b/presto-hive-common/src/test/java/com/facebook/presto/hive/TestMetadataUtils.java\n@@ -0,0 +1,74 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive;\n+\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static com.facebook.presto.hive.MetadataUtils.constructSchemaName;\n+import static org.testng.Assert.assertNull;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestMetadataUtils\n+{\n+    @Test\n+    public void testWithCatalogAndValidSchema()\n+    {\n+        String result = constructSchemaName(Optional.of(\"testCatalog\"), \"testSchema\");\n+        assertTrue(result.equals(\"@testCatalog#testSchema\"));\n+    }\n+\n+    @Test\n+    public void testWithCatalogAndDefaultSchema()\n+    {\n+        String result = constructSchemaName(Optional.of(\"testCatalog\"), \"default\");\n+        assertTrue(result.equals(\"default\"));\n+    }\n+\n+    @Test\n+    public void testWithCatalogAndSchemaContainingSeparator()\n+    {\n+        String result = constructSchemaName(Optional.of(\"testCatalog\"), \"schema#with#dot\");\n+        assertTrue(result.equals(\"schema#with#dot\"));\n+    }\n+\n+    @Test\n+    public void testWithoutCatalog()\n+    {\n+        String result = constructSchemaName(Optional.empty(), \"testSchema\");\n+        assertTrue(result.equals(\"testSchema\"));\n+    }\n+\n+    @Test\n+    public void testWithNullSchema()\n+    {\n+        String result = constructSchemaName(Optional.empty(), null);\n+        assertNull(result);\n+    }\n+\n+    @Test\n+    public void testWithoutCatalogNameAndEmptySchemaName()\n+    {\n+        String result = constructSchemaName(Optional.empty(), \"\");\n+        assertTrue(result.isEmpty());\n+    }\n+\n+    @Test\n+    public void testWithCatalogNameAndEmptySchemaName()\n+    {\n+        String result = constructSchemaName(Optional.of(\"testCatalog\"), \"\");\n+        assertTrue(result.equals(\"@testCatalog#!\"));\n+    }\n+}\n\ndiff --git a/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java b/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\nindex e81470dda7e9a..e28d632f64354 100644\n--- a/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\n+++ b/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\n@@ -28,6 +28,7 @@\n import com.facebook.presto.hive.HiveCoercionPolicy;\n import com.facebook.presto.hive.HiveColumnConverterProvider;\n import com.facebook.presto.hive.HiveColumnHandle;\n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.HiveEncryptionInformationProvider;\n import com.facebook.presto.hive.HiveFileRenamer;\n import com.facebook.presto.hive.HiveHdfsConfiguration;\n@@ -137,7 +138,7 @@ public S3SelectTestHelper(String host,\n             metastoreClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));\n         }\n \n-        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port);\n+        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port, new HiveCommonClientConfig());\n         executor = newCachedThreadPool(daemonThreadsNamed(\"hive-%s\"));\n         HivePartitionManager hivePartitionManager = new HivePartitionManager(FUNCTION_AND_TYPE_MANAGER, config);\n \n\ndiff --git a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java\nindex e267316145fa3..d98968c73af62 100644\n--- a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java\n+++ b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/TestRecordingHiveMetastore.java\n@@ -67,7 +67,8 @@ public class TestRecordingHiveMetastore\n             \"owner\",\n             USER,\n             Optional.of(\"comment\"),\n-            ImmutableMap.of(\"param\", \"value\"));\n+            ImmutableMap.of(\"param\", \"value\"),\n+            Optional.of(\"catalogName\"));\n     private static final Column TABLE_COLUMN = new Column(\n             \"column\",\n             HiveType.HIVE_INT,\n@@ -96,6 +97,7 @@ public class TestRecordingHiveMetastore\n             ImmutableMap.of(\"param\", \"value2\"),\n             ImmutableMap.of());\n     private static final Table TABLE = new Table(\n+            Optional.of(\"catalogName\"),\n             \"database\",\n             \"table\",\n             \"owner\",\n\ndiff --git a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java\nindex d76118d55e7b9..4ee92ab0e2b47 100644\n--- a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java\n+++ b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClient.java\n@@ -135,6 +135,16 @@ public String getDelegationToken(String owner, String renewer)\n         return TEST_TOKEN;\n     }\n \n+    @Override\n+    public List<String> getDatabases(String pattern)\n+    {\n+        accessCount.incrementAndGet();\n+        if (throwException) {\n+            throw new IllegalStateException();\n+        }\n+        return ImmutableList.of(TEST_DATABASE);\n+    }\n+\n     @Override\n     public List<String> getAllTables(String dbName)\n     {\n\ndiff --git a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java\nindex 5ff6e10b0b7c0..196a72018ca72 100644\n--- a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java\n+++ b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/MockHiveMetastoreClientFactory.java\n@@ -32,7 +32,7 @@ public class MockHiveMetastoreClientFactory\n \n     public MockHiveMetastoreClientFactory(Optional<HostAndPort> socksProxy, Duration timeout, List<HiveMetastoreClient> clients)\n     {\n-        super(Optional.empty(), socksProxy, timeout, new NoHiveMetastoreAuthentication());\n+        super(Optional.empty(), socksProxy, timeout, new NoHiveMetastoreAuthentication(), null);\n         this.clients = new ArrayList<>(requireNonNull(clients, \"clients is null\"));\n     }\n \n\ndiff --git a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java\nindex 74e8e3f2ec0ad..0e755b81b41ff 100644\n--- a/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java\n+++ b/presto-hive-metastore/src/test/java/com/facebook/presto/hive/metastore/thrift/TestingHiveCluster.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.hive.metastore.thrift;\n \n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.authentication.NoHiveMetastoreAuthentication;\n import com.google.common.net.HostAndPort;\n@@ -29,19 +30,21 @@ public class TestingHiveCluster\n     private final MetastoreClientConfig metastoreClientConfig;\n     private final ThriftHiveMetastoreConfig thriftHiveMetastoreConfig;\n     private final HostAndPort address;\n+    private final HiveCommonClientConfig hiveCommonClientConfig;\n \n-    public TestingHiveCluster(MetastoreClientConfig metastoreClientConfig, ThriftHiveMetastoreConfig thriftHiveMetastoreConfig, String host, int port)\n+    public TestingHiveCluster(MetastoreClientConfig metastoreClientConfig, ThriftHiveMetastoreConfig thriftHiveMetastoreConfig, String host, int port, HiveCommonClientConfig hiveCommonClientConfig)\n     {\n         this.metastoreClientConfig = requireNonNull(metastoreClientConfig, \"metastore config is null\");\n         this.thriftHiveMetastoreConfig = requireNonNull(thriftHiveMetastoreConfig, \"thrift metastore config is null\");\n         this.address = HostAndPort.fromParts(requireNonNull(host, \"host is null\"), port);\n+        this.hiveCommonClientConfig = hiveCommonClientConfig;\n     }\n \n     @Override\n     public HiveMetastoreClient createMetastoreClient(Optional<String> token)\n             throws TException\n     {\n-        return new HiveMetastoreClientFactory(metastoreClientConfig, thriftHiveMetastoreConfig, new NoHiveMetastoreAuthentication()).create(address, token);\n+        return new HiveMetastoreClientFactory(metastoreClientConfig, thriftHiveMetastoreConfig, new NoHiveMetastoreAuthentication(), hiveCommonClientConfig).create(address, token);\n     }\n \n     @Override\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\nindex db870cc59d846..9035d5b5de72d 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\n@@ -984,8 +984,7 @@ protected final void setup(String host, int port, String databaseName, String ti\n         if (proxy != null) {\n             metastoreClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));\n         }\n-\n-        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port);\n+        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port, new HiveCommonClientConfig());\n         HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hiveClientConfig, metastoreClientConfig), ImmutableSet.of(), hiveClientConfig);\n         hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, metastoreClientConfig, new NoHdfsAuthentication());\n         ExtendedHiveMetastore metastore = new InMemoryCachingHiveMetastore(\n@@ -3916,6 +3915,7 @@ protected Partition createDummyPartition(Table table, String partitionName, Opti\n     {\n         byte[] rowIdPartitionComponent = {98, 45};\n         return Partition.builder()\n+                .setCatalogName(table.getCatalogName())\n                 .setDatabaseName(table.getDatabaseName())\n                 .setTableName(table.getTableName())\n                 .setColumns(table.getDataColumns())\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\nindex 9f3c1eec79977..e27b042b04a66 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\n@@ -188,7 +188,7 @@ protected void setup(String host, int port, String databaseName, BiFunction<Hive\n             metastoreClientConfig.setMetastoreSocksProxy(HostAndPort.fromString(proxy));\n         }\n \n-        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port);\n+        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, host, port, new HiveCommonClientConfig());\n         ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(\"hive-%s\"));\n         HivePartitionManager hivePartitionManager = new HivePartitionManager(FUNCTION_AND_TYPE_MANAGER, config);\n \n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java\nindex 782faa069c9d5..1b6c013be6cf6 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestAbstractDwrfEncryptionInformationSource.java\n@@ -50,6 +50,7 @@ public class TestAbstractDwrfEncryptionInformationSource\n     private static Table createTable(HiveStorageFormat storageFormat, Optional<DwrfTableEncryptionProperties> tableEncryptionProperties, boolean isPartitioned)\n     {\n         return new Table(\n+                Optional.of(\"catalogName\"),\n                 \"dbName\",\n                 \"tableName\",\n                 \"owner\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java\nindex ec67d51d49b27..3af29f6740729 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveClientFileMetastore.java\n@@ -220,6 +220,7 @@ private Partition createDummyPartition(Table table, String partitionName, Option\n                 .putAll(dynamicPartitionParameters)\n                 .build();\n         return Partition.builder()\n+                .setCatalogName(table.getCatalogName())\n                 .setDatabaseName(table.getDatabaseName())\n                 .setTableName(table.getTableName())\n                 .setColumns(table.getDataColumns())\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\nindex f8d4c592a124a..ead56db6ac6c1 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\n@@ -267,6 +267,7 @@ private HiveMetadata getHiveMetadata(TestingExtendedHiveMetastore metastore, Hiv\n     private Partition createPartition(String partitionName, String partitionLocation)\n     {\n         Partition.Builder partitionBuilder = Partition.builder()\n+                .setCatalogName(Optional.of(\"hive\"))\n                 .setDatabaseName(TEST_SCHEMA)\n                 .setTableName(TEST_TABLE)\n                 .setColumns(ImmutableList.of())\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java\nindex 6547e2f658c0b..083ea6d5b6794 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveEncryptionInformationProvider.java\n@@ -37,6 +37,7 @@\n public class TestHiveEncryptionInformationProvider\n {\n     private static final Table TEST_TABLE = new Table(\n+            Optional.of(\"catalogName\"),\n             \"test_db\",\n             \"test_table\",\n             \"test_owner\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\nindex 3761e8098049c..c1d9c11660993 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveLogicalPlanner.java\n@@ -930,6 +930,7 @@ public void testMetadataAggregationFoldingWithFilters(boolean pushdownSubfieldsE\n     private static Partition createDummyPartition(Table table, String partitionName)\n     {\n         return Partition.builder()\n+                .setCatalogName(Optional.of(\"hive\"))\n                 .setDatabaseName(table.getDatabaseName())\n                 .setTableName(table.getTableName())\n                 .setColumns(table.getDataColumns())\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java\nindex 37a9a0455f94b..85a6c471bb373 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMaterializedViewUtils.java\n@@ -623,6 +623,7 @@ private static Table getTable(List<Column> partitionColumns)\n     private static Table getTable(String tableName, List<Column> partitionColumns)\n     {\n         return new Table(\n+                Optional.of(\"catalogName\"),\n                 SCHEMA_NAME,\n                 tableName,\n                 USER_NAME,\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java\nindex bf486a436dcbf..046d925b39f6c 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadata.java\n@@ -84,6 +84,7 @@ public void testColumnMetadataGetter()\n                 Optional.empty());\n         Column partitionColumn = new Column(\"ds\", HIVE_STRING, Optional.empty(), Optional.empty());\n         Table mockTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"schema\",\n                 \"table\",\n                 \"user\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java\nindex db0986ab5531f..c63f5e9d36edb 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHivePartitionManager.java\n@@ -59,6 +59,7 @@ public class TestHivePartitionManager\n     private static final Column PARTITION_COLUMN = new Column(\"ds\", HIVE_STRING, Optional.empty(), Optional.empty());\n     private static final Column BUCKET_COLUMN = new Column(\"c1\", HIVE_INT, Optional.empty(), Optional.empty());\n     private static final Table TABLE = new Table(\n+            Optional.of(\"catalogName\"),\n             SCHEMA_NAME,\n             TABLE_NAME,\n             USER_NAME,\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveQueriesWithCatalogName.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveQueriesWithCatalogName.java\nnew file mode 100644\nindex 0000000000000..fd5170abd6e4a\n--- /dev/null\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveQueriesWithCatalogName.java\n@@ -0,0 +1,244 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive;\n+\n+import com.facebook.presto.hive.containers.HiveMinIODataLake;\n+import com.facebook.presto.hive.s3.S3HiveQueryRunner;\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.AbstractTestQueryFramework;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static com.facebook.airlift.testing.Closeables.closeAllRuntimeException;\n+import static com.facebook.presto.tests.sql.TestTable.randomTableSuffix;\n+import static java.lang.String.format;\n+import static java.util.stream.Collectors.joining;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestHiveQueriesWithCatalogName\n+        extends AbstractTestQueryFramework\n+{\n+    private static final String HIVE_TEST_SCHEMA_1 = \"hive_test_schema_1\";\n+    private static final String HIVE_CATALOG = \"hive\";\n+    private String bucketName;\n+    private HiveMinIODataLake dockerizedS3DataLake;\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        this.bucketName = \"test-schema-with-hive-catalog-name-\" + randomTableSuffix();\n+        this.dockerizedS3DataLake = new HiveMinIODataLake(bucketName, ImmutableMap.of());\n+        this.dockerizedS3DataLake.start();\n+        return S3HiveQueryRunner.create(\n+                this.dockerizedS3DataLake.getHiveHadoop().getHiveMetastoreEndpoint(),\n+                this.dockerizedS3DataLake.getMinio().getMinioApiEndpoint(),\n+                HiveMinIODataLake.ACCESS_KEY,\n+                HiveMinIODataLake.SECRET_KEY,\n+                ImmutableMap.<String, String>builder()\n+                        // This is required when using MinIO which requires path style access\n+                        .put(\"hive.s3.path-style-access\", \"true\")\n+                        .put(\"hive.insert-existing-partitions-behavior\", \"OVERWRITE\")\n+                        .put(\"hive.non-managed-table-writes-enabled\", \"true\")\n+                        // This new conf is added to pass the catalog information to metastore\n+                        .put(\"hive.metastore.catalog.name\", HIVE_CATALOG)\n+                        .build(), new HashMap<>());\n+    }\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        computeActual(format(\n+                \"CREATE SCHEMA hive.%1$s WITH (location='s3a://%2$s/%1$s')\",\n+                HIVE_TEST_SCHEMA_1,\n+                bucketName));\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void close()\n+            throws Exception\n+    {\n+        closeAllRuntimeException(dockerizedS3DataLake);\n+    }\n+\n+    @Test\n+    public void testInsertOverwritePartitionedTable()\n+    {\n+        String testTable = getTestTableName();\n+        computeActual(getCreateTableStatement(\n+                testTable,\n+                \"partitioned_by=ARRAY['regionkey']\"));\n+        copyTpchNationToTable(testTable);\n+        assertOverwritePartition(testTable);\n+    }\n+\n+    @Test\n+    public void testInsertOverwritePartitionedAndBucketedTable()\n+    {\n+        String testTable = getTestTableName();\n+        computeActual(getCreateTableStatement(\n+                testTable,\n+                \"partitioned_by=ARRAY['regionkey']\",\n+                \"bucketed_by = ARRAY['nationkey']\",\n+                \"bucket_count = 3\"));\n+        copyTpchNationToTable(testTable);\n+        assertOverwritePartition(testTable);\n+    }\n+\n+    @Test\n+    public void testInsertOverwritePartitionedAndBucketedExternalTable()\n+    {\n+        String testTable = getTestTableName();\n+        // Store table data in data lake bucket\n+        computeActual(getCreateTableStatement(\n+                testTable,\n+                \"partitioned_by=ARRAY['regionkey']\",\n+                \"bucketed_by = ARRAY['nationkey']\",\n+                \"bucket_count = 3\"));\n+        copyTpchNationToTable(testTable);\n+\n+        String tableName = testTable.substring(testTable.lastIndexOf('.') + 1);\n+        // Map this table as external table\n+        String externalTableName = testTable + \"_ext\";\n+        computeActual(getCreateTableStatement(\n+                externalTableName,\n+                \"partitioned_by=ARRAY['regionkey']\",\n+                \"bucketed_by = ARRAY['nationkey']\",\n+                \"bucket_count = 3\",\n+                format(\"external_location = 's3a://%s/%s/%s/'\", this.bucketName, HIVE_TEST_SCHEMA_1, tableName)));\n+        copyTpchNationToTable(testTable);\n+        assertOverwritePartition(externalTableName);\n+    }\n+\n+    protected void assertOverwritePartition(String testTable)\n+    {\n+        computeActual(format(\n+                \"INSERT INTO %s VALUES \" +\n+                        \"('POLAND', 'Test Data', 25, 5), \" +\n+                        \"('CZECH', 'Test Data', 26, 5)\",\n+                testTable));\n+\n+        String oldPartitionPath = getPartitionPath(testTable);\n+        assertQuery(format(\"SELECT count(*) FROM %s WHERE regionkey = 5\", testTable), \"SELECT 2\");\n+\n+        computeActual(format(\"INSERT INTO %s values('POLAND', 'Overwrite', 25, 5)\", testTable));\n+\n+        String newPartitionPath = getPartitionPath(testTable);\n+        assertQuery(format(\"SELECT count(*) FROM %s WHERE regionkey = 5\", testTable), \"SELECT 1\");\n+\n+        assertEquals(oldPartitionPath, newPartitionPath);\n+        computeActual(format(\"DROP TABLE %s\", testTable));\n+    }\n+\n+    private String getPartitionPath(String testTable)\n+    {\n+        MaterializedResult result = computeActual(format(\"SELECT \\\"$PATH\\\" from %s where regionkey = 5\", testTable));\n+        assertTrue(result.getMaterializedRows().size() > 0);\n+        String path = result.getMaterializedRows().get(0).getField(0).toString();\n+        return path.substring(0, path.lastIndexOf(\"/\"));\n+    }\n+\n+    protected String getTestTableName()\n+    {\n+        return format(\"hive.%s.%s\", HIVE_TEST_SCHEMA_1, \"nation_\" + randomTableSuffix());\n+    }\n+\n+    protected String getCreateTableStatement(String tableName, String... propertiesEntries)\n+    {\n+        return getCreateTableStatement(tableName, Arrays.asList(propertiesEntries));\n+    }\n+\n+    protected String getCreateTableStatement(String tableName, List<String> propertiesEntries)\n+    {\n+        return format(\n+                \"CREATE TABLE %s (\" +\n+                        \"    name varchar(25), \" +\n+                        \"    comment varchar(152),  \" +\n+                        \"    nationkey bigint, \" +\n+                        \"    regionkey bigint) \" +\n+                        (propertiesEntries.isEmpty() ? \"\" : propertiesEntries\n+                                .stream()\n+                                .collect(joining(\",\", \"WITH (\", \")\"))),\n+                tableName);\n+    }\n+\n+    protected void copyTpchNationToTable(String testTable)\n+    {\n+        computeActual(format(\"INSERT INTO \" + testTable + \" SELECT name, comment, nationkey, regionkey FROM tpch.tiny.nation\"));\n+    }\n+\n+    @Test\n+    public void testListSchemasAndListTablesAcrossCatalogs()\n+    {\n+        String hiveTestSchema01 = \"hive_test_schema_01\";\n+        String hiveTestSchema02 = \"hive_test_schema_02\";\n+        // Create two schemas in different locations\n+        computeActual(format(\n+                \"CREATE SCHEMA hive.%1$s WITH (location='s3a://%2$s/%1$s')\",\n+                hiveTestSchema01,\n+                bucketName));\n+\n+        computeActual(format(\n+                \"CREATE SCHEMA hive.%1$s WITH (location='s3a://%2$s/%1$s')\",\n+                hiveTestSchema02,\n+                bucketName));\n+\n+        // Create tables in both schemas\n+        String testTableSchema1 = format(\"hive.%s.%s\", hiveTestSchema01, \"nation_\" + randomTableSuffix());\n+        String testTableSchema2 = format(\"hive.%s.%s\", hiveTestSchema02, \"region_\" + randomTableSuffix());\n+\n+        computeActual(getCreateTableStatement(testTableSchema1));\n+        computeActual(getCreateTableStatement(testTableSchema2));\n+\n+        // Verify that listSchemas contains both schemas\n+        MaterializedResult schemas = computeActual(\"SHOW SCHEMAS FROM hive\");\n+        List<String> schemaNames = schemas.getMaterializedRows().stream()\n+                .map(row -> row.getField(0).toString())\n+                .collect(Collectors.toList());\n+\n+        assertTrue(schemaNames.contains(hiveTestSchema01), \"Schema 1 is missing\");\n+        assertTrue(schemaNames.contains(hiveTestSchema02), \"Schema 2 is missing\");\n+\n+        // Verify that each table is listed under its own schema\n+        MaterializedResult tablesSchema1 = computeActual(format(\"SHOW TABLES FROM hive.%s\", hiveTestSchema01));\n+        MaterializedResult tablesSchema2 = computeActual(format(\"SHOW TABLES FROM hive.%s\", hiveTestSchema02));\n+\n+        List<String> tableNamesSchema1 = tablesSchema1.getMaterializedRows().stream()\n+                .map(row -> row.getField(0).toString())\n+                .collect(Collectors.toList());\n+\n+        List<String> tableNamesSchema2 = tablesSchema2.getMaterializedRows().stream()\n+                .map(row -> row.getField(0).toString())\n+                .collect(Collectors.toList());\n+\n+        assertTrue(tableNamesSchema1.contains(testTableSchema1.substring(testTableSchema1.lastIndexOf('.') + 1)),\n+                \"Table in Schema 1 is missing\");\n+\n+        assertTrue(tableNamesSchema2.contains(testTableSchema2.substring(testTableSchema2.lastIndexOf('.') + 1)),\n+                \"Table in Schema 2 is missing\");\n+\n+        // Cleanup tables\n+        computeActual(format(\"DROP TABLE %s\", testTableSchema1));\n+        computeActual(format(\"DROP TABLE %s\", testTableSchema2));\n+    }\n+}\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\nindex 9be421b8d3632..9d1ca4454779f 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\n@@ -147,6 +147,7 @@ public class TestHiveSplitManager\n     private static final Table TEST_TABLE = createTestTable(VIEW_STORAGE_FORMAT, ImmutableMap.of());\n \n     private ListeningExecutorService executor;\n+    private static final String TEST_CATALOG_NAME = \"catalogName\";\n \n     @BeforeClass\n     public void setUp()\n@@ -162,7 +163,8 @@ public void shutdown()\n \n     private static Table createTestTable(StorageFormat storageFormat, Map<String, String> parameters)\n     {\n-        return new Table(\"test_db\",\n+        return new Table(Optional.of(TEST_CATALOG_NAME),\n+                \"test_db\",\n                 \"test_table\",\n                 \"test_owner\",\n                 MANAGED_TABLE,\n@@ -472,8 +474,7 @@ private void assertRedundantColumnDomains(Range predicateRange, PartitionStatist\n \n         // Prepare partition with stats\n         PartitionWithStatistics partitionWithStatistics = new PartitionWithStatistics(\n-                new Partition(\n-                        \"test_db\",\n+                new Partition(\"test_db\",\n                         \"test_table\",\n                         ImmutableList.of(PARTITION_VALUE),\n                         new Storage(\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java\nindex 3426f5bfb23a6..b3cd8801f5c85 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHudiDirectoryLister.java\n@@ -91,6 +91,7 @@ private Configuration getHadoopConfWithCopyOnFirstWriteEnabled()\n     private Table getMockTable()\n     {\n         return new Table(\n+                Optional.of(\"catalogName\"),\n                 \"schema\",\n                 \"hudi_non_part_cow\",\n                 \"user\",\n@@ -116,6 +117,7 @@ private Table getMockTable()\n     private Table getMockMORTableWithPartition()\n     {\n         return new Table(\n+                Optional.empty(),\n                 \"schema\",\n                 \"hudi_mor_part_update\",\n                 \"user\",\n@@ -288,6 +290,7 @@ public void testDirectoryListerForHudiTableWithCopyOnFirstWriteEnabled()\n     public void testDirectoryListerForNonHudiTable()\n     {\n         Table mockTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"schema\",\n                 \"non_hudi_table\",\n                 \"user\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java\nindex ad0c3e6848608..c962bd843e474 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestingExtendedHiveMetastore.java\n@@ -52,7 +52,7 @@ public List<String> getAllDatabases(MetastoreContext metastoreContext)\n     @Override\n     public Optional<Database> getDatabase(MetastoreContext metastoreContext, String databaseName)\n     {\n-        return Optional.of(new Database(databaseName, Optional.of(\"/\"), \"test_owner\", PrincipalType.USER, Optional.empty(), ImmutableMap.of()));\n+        return Optional.of(new Database(databaseName, Optional.of(\"/\"), \"test_owner\", PrincipalType.USER, Optional.empty(), ImmutableMap.of(), Optional.of(\"testcatalog\")));\n     }\n \n     @Override\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java\nindex 81d93de579b8e..e60cb67dfacec 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestingSemiTransactionalHiveMetastore.java\n@@ -81,7 +81,7 @@ public static TestingSemiTransactionalHiveMetastore create()\n         ThriftHiveMetastoreConfig thriftHiveMetastoreConfig = new ThriftHiveMetastoreConfig();\n         HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(config, metastoreClientConfig), ImmutableSet.of(), config);\n         HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, metastoreClientConfig, new NoHdfsAuthentication());\n-        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, HOST, PORT);\n+        HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, thriftHiveMetastoreConfig, HOST, PORT, new HiveCommonClientConfig());\n         ColumnConverterProvider columnConverterProvider = HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER;\n         ExtendedHiveMetastore delegate = new BridgingHiveMetastore(new ThriftHiveMetastore(hiveCluster, metastoreClientConfig, hdfsEnvironment), new HivePartitionMutator());\n         ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(\"hive-%s\"));\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java b/presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java\nindex 512125b8e2ea6..02f702c14cafd 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/hudi/HudiTestingDataGenerator.java\n@@ -173,6 +173,7 @@ private void addPartition(HoodieTableType type, String tableName, List<String> p\n         List<PartitionWithStatistics> partitions = new ArrayList<>();\n         for (String partitionName : partitionNames) {\n             Partition partition = Partition.builder()\n+                    .setCatalogName(Optional.of(\"hive\"))\n                     .setDatabaseName(schemaName)\n                     .setTableName(tableName)\n                     .setValues(extractPartitionValues(partitionName))\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java b/presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java\nindex c9ce1106caa82..be540ec76279a 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/metastore/glue/TestingMetastoreObjects.java\n@@ -126,6 +126,7 @@ public static com.facebook.presto.hive.metastore.Table getPrestoTestTable(String\n     public static com.facebook.presto.hive.metastore.Partition getPrestoTestPartition(String dbName, String tblName, List<String> values)\n     {\n         return com.facebook.presto.hive.metastore.Partition.builder()\n+                .setCatalogName(Optional.of(\"hive\"))\n                 .setDatabaseName(dbName)\n                 .setTableName(tblName)\n                 .setValues(values)\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java b/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\nindex 568aece12130c..9f275f58bf9e4 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/s3/S3HiveQueryRunner.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.hive.s3;\n \n+import com.facebook.presto.hive.HiveCommonClientConfig;\n import com.facebook.presto.hive.HiveQueryRunner;\n import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.metastore.HivePartitionMutator;\n@@ -73,7 +74,7 @@ public static DistributedQueryRunner create(\n                                 new TestingHiveCluster(metastoreClientConfig,\n                                         thriftHiveMetastoreConfig,\n                                         hiveEndpoint.getHost(),\n-                                        hiveEndpoint.getPort()), metastoreClientConfig,\n+                                        hiveEndpoint.getPort(), new HiveCommonClientConfig()), metastoreClientConfig,\n                                 HDFS_ENVIRONMENT),\n                         new HivePartitionMutator())),\n                 ImmutableMap.of());\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java b/presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java\nindex 7d2570f1a7817..721ed29fc21e4 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/s3select/TestS3SelectPushdown.java\n@@ -77,6 +77,7 @@ public void setUp()\n         partition = new Partition(\"db\", \"table\", emptyList(), storage, singletonList(column), emptyMap(), Optional.empty(), false, false, 1234, 4567L, Optional.empty());\n \n         table = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"db\",\n                 \"table\",\n                 \"owner\",\n@@ -132,6 +133,7 @@ public void testShouldNotEnableSelectPushdownWhenIsNotSupportedSerde()\n                 .setLocation(\"location\")\n                 .build();\n         Table newTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"db\",\n                 \"table\",\n                 \"owner\",\n@@ -157,6 +159,7 @@ public void testShouldNotEnableSelectPushdownWhenInputFormatIsNotSupported()\n                 .setLocation(\"location\")\n                 .build();\n         Table newTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"db\",\n                 \"table\",\n                 \"owner\",\n@@ -175,6 +178,7 @@ public void testShouldNotEnableSelectPushdownWhenColumnTypesAreNotSupported()\n     {\n         Column newColumn = new Column(\"column\", HIVE_BINARY, Optional.empty(), Optional.empty());\n         Table newTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 \"db\",\n                 \"table\",\n                 \"owner\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\nindex 2d88a15e49bc7..c8cb678cfbcd1 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\n@@ -191,6 +191,7 @@ private ImmutableList<HiveFileInfo> buildHiveFileInfos(String basePath, String p\n     private void setUp()\n     {\n         Table table = new Table(\n+                Optional.of(\"catalogName\"),\n                 TEST_SCHEMA,\n                 TEST_TABLE,\n                 \"owner\",\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\nindex 31166b5142ce4..b22d980cb042f 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\n@@ -148,6 +148,7 @@ public void setUp()\n                 ImmutableMap.of(),\n                 ImmutableMap.of());\n         Partition mockPartition = new Partition(\n+                Optional.of(\"catalogName\"),\n                 TEST_SCHEMA,\n                 TEST_TABLE,\n                 ImmutableList.of(),\n@@ -161,6 +162,7 @@ public void setUp()\n                 0,\n                 Optional.empty());\n         Table mockTable = new Table(\n+                Optional.of(\"catalogName\"),\n                 TEST_SCHEMA,\n                 TEST_TABLE,\n                 \"owner\",\n\ndiff --git a/presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java b/presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java\nindex afe713bc21dba..ba445bc1d2054 100644\n--- a/presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java\n+++ b/presto-hudi/src/test/java/com/facebook/presto/hudi/TestHudiPartitionManager.java\n@@ -56,6 +56,7 @@ public class TestHudiPartitionManager\n     private static final Column PARTITION_COLUMN = new Column(\"ds\", HIVE_STRING, Optional.empty(), Optional.empty());\n     private static final Column BUCKET_COLUMN = new Column(\"c1\", HIVE_INT, Optional.empty(), Optional.empty());\n     private static final Table TABLE = new Table(\n+            Optional.of(\"catalogName\"),\n             SCHEMA_NAME,\n             TABLE_NAME,\n             USER_NAME,\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\nindex 8e72dc28e4f3c..acd0244e054f5 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n@@ -667,6 +667,7 @@ public static void setupJsonFunctionNamespaceManager(QueryRunner queryRunner, St\n     private static Table createHiveSymlinkTable(String databaseName, String tableName, List<Column> columns, String location)\n     {\n         return new Table(\n+                Optional.of(\"catalogName\"),\n                 databaseName,\n                 tableName,\n                 \"hive\",\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24182",
    "pr_id": 24182,
    "issue_id": 23881,
    "repo": "prestodb/presto",
    "problem_statement": "Unauthorized User Can Execute USE Command on Schema or Catalog\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nA user with any role who has not been granted access to a specific catalog or schema is still able to execute the USE <schema> command. The expected behavior is that such users should receive an \"Access Denied\" error when attempting to access a catalog or schema for which they lack permission.\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: Latest\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nUsers should receive an \"Access Denied\" error when accessing a catalog or schema without permission.\r\n\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\nThe USE command succeeds without an error.\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Add a user with the User role\r\n2. Do not grant user1 access to the catalog.\r\n3. Connect to the catalog using user1 in Presto:\r\n./presto-cli-0.289-SNAPSHOT-executable.jar --user user1 --password\r\n4. Execute the USE command successfully.\r\n\r\n",
    "issue_word_count": 216,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/execution/UseTask.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java"
    ],
    "base_commit": "1a5f46278a07891ec20b62948bb3551edc23c0d3",
    "head_commit": "e46b50ef6b8d36360fe4dfbe885c3661c55f7350",
    "repo_url": "https://github.com/prestodb/presto/pull/24182",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24182",
    "dockerfile": "",
    "pr_merged_at": "2025-01-13T20:40:29.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\nindex 03a582bcec592..bbb914d9e332d 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n@@ -15,18 +15,24 @@\n \n import com.facebook.presto.Session;\n import com.facebook.presto.common.CatalogSchemaName;\n+import com.facebook.presto.common.transaction.TransactionId;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.spi.security.AccessControl;\n+import com.facebook.presto.spi.security.AccessControlContext;\n+import com.facebook.presto.spi.security.AccessDeniedException;\n+import com.facebook.presto.spi.security.Identity;\n import com.facebook.presto.sql.analyzer.SemanticException;\n import com.facebook.presto.sql.tree.Expression;\n import com.facebook.presto.sql.tree.Identifier;\n import com.facebook.presto.sql.tree.Use;\n import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableSet;\n import com.google.common.util.concurrent.ListenableFuture;\n \n import java.util.List;\n \n import static com.facebook.presto.metadata.MetadataUtil.getConnectorIdOrThrow;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyCatalogAccess;\n import static com.facebook.presto.sql.analyzer.SemanticErrorCode.CATALOG_NOT_SPECIFIED;\n import static com.facebook.presto.sql.analyzer.SemanticErrorCode.MISSING_SCHEMA;\n import static com.google.common.util.concurrent.Futures.immediateFuture;\n@@ -54,9 +60,9 @@ public ListenableFuture<?> execute(\n \n         checkCatalogAndSessionPresent(statement, session);\n \n-        checkAndSetCatalog(statement, metadata, stateMachine, session);\n+        checkAndSetCatalog(statement, metadata, stateMachine, session, accessControl);\n \n-        checkAndSetSchema(statement, metadata, stateMachine, session);\n+        checkAndSetSchema(statement, metadata, stateMachine, session, accessControl);\n \n         return immediateFuture(null);\n     }\n@@ -68,16 +74,19 @@ private void checkCatalogAndSessionPresent(Use statement, Session session)\n         }\n     }\n \n-    private void checkAndSetCatalog(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session)\n+    private void checkAndSetCatalog(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session, AccessControl accessControl)\n     {\n-        if (statement.getCatalog().isPresent()) {\n-            String catalog = statement.getCatalog().get().getValueLowerCase();\n-            getConnectorIdOrThrow(session, metadata, catalog);\n-            stateMachine.setSetCatalog(catalog);\n+        String catalog = statement.getCatalog()\n+                .map(Identifier::getValueLowerCase)\n+                .orElseGet(() -> session.getCatalog().map(String::toLowerCase).get());\n+        getConnectorIdOrThrow(session, metadata, catalog);\n+        if (!hasCatalogAccess(session.getIdentity(), session.getAccessControlContext(), catalog, accessControl)) {\n+            denyCatalogAccess(catalog);\n         }\n+        stateMachine.setSetCatalog(catalog);\n     }\n \n-    private void checkAndSetSchema(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session)\n+    private void checkAndSetSchema(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session, AccessControl accessControl)\n     {\n         String catalog = statement.getCatalog()\n                 .map(Identifier::getValueLowerCase)\n@@ -86,6 +95,19 @@ private void checkAndSetSchema(Use statement, Metadata metadata, QueryStateMachi\n         if (!metadata.getMetadataResolver(session).schemaExists(new CatalogSchemaName(catalog, schema))) {\n             throw new SemanticException(MISSING_SCHEMA, format(\"Schema does not exist: %s.%s\", catalog, schema));\n         }\n+        if (!hasSchemaAccess(session.getTransactionId().get(), session.getIdentity(), session.getAccessControlContext(), catalog, schema, accessControl)) {\n+            throw new AccessDeniedException(\"Cannot access schema: \" + new CatalogSchemaName(catalog, schema));\n+        }\n         stateMachine.setSetSchema(schema);\n     }\n+\n+    private boolean hasCatalogAccess(Identity identity, AccessControlContext context, String catalog, AccessControl accessControl)\n+    {\n+        return !accessControl.filterCatalogs(identity, context, ImmutableSet.of(catalog)).isEmpty();\n+    }\n+\n+    private boolean hasSchemaAccess(TransactionId transactionId, Identity identity, AccessControlContext context, String catalog, String schema, AccessControl accessControl)\n+    {\n+        return !accessControl.filterSchemas(transactionId, identity, context, catalog, ImmutableSet.of(schema)).isEmpty();\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java b/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java\nnew file mode 100644\nindex 0000000000000..eb6887a3b63d8\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java\n@@ -0,0 +1,164 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.connector.MockConnectorFactory;\n+import com.facebook.presto.metadata.Catalog;\n+import com.facebook.presto.metadata.CatalogManager;\n+import com.facebook.presto.metadata.MetadataManager;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.security.AccessControl;\n+import com.facebook.presto.spi.security.AccessDeniedException;\n+import com.facebook.presto.spi.security.AllowAllAccessControl;\n+import com.facebook.presto.spi.security.DenyAllAccessControl;\n+import com.facebook.presto.spi.security.Identity;\n+import com.facebook.presto.sql.analyzer.SemanticException;\n+import com.facebook.presto.sql.tree.Identifier;\n+import com.facebook.presto.sql.tree.Use;\n+import com.facebook.presto.testing.TestingConnectorContext;\n+import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+\n+import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static com.facebook.presto.SessionTestUtils.TEST_SESSION;\n+import static com.facebook.presto.connector.MockConnectorFactory.builder;\n+import static com.facebook.presto.execution.TaskTestUtils.createQueryStateMachine;\n+import static com.facebook.presto.metadata.MetadataManager.createTestMetadataManager;\n+import static com.facebook.presto.spi.ConnectorId.createInformationSchemaConnectorId;\n+import static com.facebook.presto.spi.ConnectorId.createSystemTablesConnectorId;\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static com.facebook.presto.transaction.InMemoryTransactionManager.createTestTransactionManager;\n+import static java.util.Collections.emptyList;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+\n+@Test(singleThreaded = true)\n+public class TestUseTask\n+{\n+    private final ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(\"test-%s\"));\n+    private CatalogManager catalogManager;\n+    private TransactionManager transactionManager;\n+    private MetadataManager metadata;\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        catalogManager = new CatalogManager();\n+        transactionManager = createTestTransactionManager(catalogManager);\n+        metadata = createTestMetadataManager(transactionManager);\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        executor.shutdownNow();\n+    }\n+\n+    @Test\n+    public void testUse()\n+    {\n+        Use use = new Use(Optional.of(new Identifier(\"test_catalog\")), new Identifier(\"test_schema\"));\n+        String sqlString = \"USE test_catalog.test_schema\";\n+        AccessControl accessControl = new AllowAllAccessControl();\n+        executeUse(use, sqlString, TEST_SESSION, accessControl);\n+    }\n+\n+    @Test(\n+            expectedExceptions = SemanticException.class,\n+            expectedExceptionsMessageRegExp = \"Catalog must be specified when session catalog is not set\")\n+    public void testUseNoCatalog()\n+    {\n+        Use use = new Use(Optional.empty(), new Identifier(\"test_schema\"));\n+        String sqlString = \"USE test_schema\";\n+        Session session = testSessionBuilder()\n+                .setCatalog(null)\n+                .setSchema(null)\n+                .build();\n+        AccessControl accessControl = new AllowAllAccessControl();\n+        executeUse(use, sqlString, session, accessControl);\n+    }\n+\n+    @Test(\n+            expectedExceptions = SemanticException.class,\n+            expectedExceptionsMessageRegExp = \"Catalog does not exist: invalid_catalog\")\n+    public void testUseInvalidCatalog()\n+    {\n+        Use use = new Use(Optional.of(new Identifier(\"invalid_catalog\")), new Identifier(\"test_schema\"));\n+        String sqlString = \"USE invalid_catalog.test_schema\";\n+        AccessControl accessControl = new AllowAllAccessControl();\n+        executeUse(use, sqlString, TEST_SESSION, accessControl);\n+    }\n+\n+    @Test(\n+            expectedExceptions = SemanticException.class,\n+            expectedExceptionsMessageRegExp = \"Schema does not exist: test_catalog.invalid_schema\")\n+    public void testUseInvalidSchema()\n+    {\n+        Use use = new Use(Optional.of(new Identifier(\"test_catalog\")), new Identifier(\"invalid_schema\"));\n+        String sqlString = \"USE test_catalog.invalid_schema\";\n+        Session session = testSessionBuilder()\n+                .setSchema(\"invalid_schema\")\n+                .build();\n+        AccessControl accessControl = new AllowAllAccessControl();\n+        executeUse(use, sqlString, session, accessControl);\n+    }\n+\n+    @Test(\n+            expectedExceptions = AccessDeniedException.class,\n+            expectedExceptionsMessageRegExp = \"Access Denied: Cannot access catalog test_catalog\")\n+    public void testUseAccessDenied()\n+    {\n+        Use use = new Use(Optional.of(new Identifier(\"test_catalog\")), new Identifier(\"test_schema\"));\n+        String sqlString = \"USE test_catalog.test_schema\";\n+        Session session = testSessionBuilder()\n+                .setIdentity(new Identity(\"user\", Optional.empty()))\n+                .build();\n+        AccessControl accessControl = new DenyAllAccessControl();\n+        executeUse(use, sqlString, session, accessControl);\n+    }\n+\n+    private void executeUse(Use use, String sqlString, Session session, AccessControl accessControl)\n+    {\n+        MockConnectorFactory mockConnectorFactory = builder()\n+                .withListSchemaNames(connectorSession -> ImmutableList.of(\"test_schema\"))\n+                .build();\n+        catalogManager = new CatalogManager();\n+        transactionManager = createTestTransactionManager(catalogManager);\n+        metadata = createTestMetadataManager(transactionManager);\n+\n+        Connector testConnector = mockConnectorFactory.create(\"test\", ImmutableMap.of(), new TestingConnectorContext());\n+        String catalogName = \"test_catalog\";\n+        ConnectorId connectorId = new ConnectorId(catalogName);\n+        catalogManager.registerCatalog(new Catalog(\n+                catalogName,\n+                connectorId,\n+                testConnector,\n+                createInformationSchemaConnectorId(connectorId),\n+                testConnector,\n+                createSystemTablesConnectorId(connectorId),\n+                testConnector));\n+\n+        QueryStateMachine stateMachine = createQueryStateMachine(sqlString, session, false, transactionManager, executor, metadata);\n+        UseTask useTask = new UseTask();\n+        useTask.execute(use, transactionManager, metadata, accessControl, stateMachine, emptyList());\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24151",
    "pr_id": 24151,
    "issue_id": 24130,
    "repo": "prestodb/presto",
    "problem_statement": "Result mismatch between native and java engine for ipaddress cast\nNative engine:\r\n\r\n```\r\npresto:de> SELECT checksum(cast(v as ipaddress)) FROM (VALUES '192.168.1.1', NULL ) as t (v);\r\n          _col0          \r\n-------------------------\r\n 63 5c d2 cd 6c 38 90 4b\r\n```\r\n\r\nJava engine:\r\n\r\n```\r\npresto:de> SELECT checksum(cast(v as ipaddress)) FROM (VALUES '192.168.1.1', NULL ) as t (v);\r\n          _col0          \r\n-------------------------\r\n 66 d1 3b fe d3 8e c3 e5 \r\n```",
    "issue_word_count": 68,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java"
    ],
    "base_commit": "7cc07c8bce3a9f81f7c0cfa46523f586cfbab44b",
    "head_commit": "5ce8cb2243bd1cfc6db7240b15c765275c904503",
    "repo_url": "https://github.com/prestodb/presto/pull/24151",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24151",
    "dockerfile": "",
    "pr_merged_at": "2024-11-26T21:01:25.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java\nindex eeee56edbb02f..18df97fc24eb6 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeAggregations.java\n@@ -327,9 +327,7 @@ public void testChecksum()\n         assertQuery(\"SELECT checksum(quantity_by_linenumber) FROM orders_ex\");\n         assertQuery(\"SELECT shipmode, checksum(extendedprice) FROM lineitem GROUP BY shipmode\");\n         assertQuery(\"SELECT checksum(from_unixtime(orderkey, '+01:00')) FROM lineitem WHERE orderkey < 20\");\n-\n-        // https://github.com/prestodb/presto/issues/24130\n-        //assertQuery(\"SELECT checksum(cast(v as ipaddress)) FROM (VALUES '192.168.1.1', NULL ) as t (v)\");\n+        assertQuery(\"SELECT checksum(cast(v as ipaddress)) FROM (VALUES '192.168.1.1', NULL ) as t (v)\");\n \n         // test DECIMAL data\n         assertQuery(\"SELECT checksum(a), checksum(b) FROM (VALUES (DECIMAL '1.234', DECIMAL '611180549424.4633133')) AS t(a, b)\");\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24141",
    "pr_id": 24141,
    "issue_id": 24140,
    "repo": "prestodb/presto",
    "problem_statement": "The behavior of executing `USE` statement doesn't seem very reasonable when specified schema does not exist \nCurrently, when we execute statements like `USE catalog.non_exist_schema` to set the default schema to a non-existent one, we will succeed without any abnormalities. However, after that when we execute statments as follows:\r\n\r\n```\r\nshow tables;\r\n\r\ncreate table my_table(a int, b varchar);\r\n\r\nselect * from test_table;\r\n\r\n......\r\n```\r\n\r\nWe will get exceptions like `Schema 'non_exist_schema' does not exist`. It seems a little weird. We should fail directly when executing `USE catalog.non_exist_schema`. As a reference, when we execute `USE non_exist_catalog.whatever_schema`, we will fail with an exception `Catalog does not exist: non_exist_catalog` directly.\r\n\r\n## Expected Behavior\r\n\r\nFail directly when executing `USE catalog.non_exist_schema`.\r\n\r\n## Current Behavior\r\n\r\nThe statement will succeed without any abnormalities, but some kinds of subsequent statements will fail with `Schema 'non_exist_schema' does not exist`.\r\n\r\n",
    "issue_word_count": 140,
    "test_files_count": 5,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/execution/UseTask.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/MetadataUtil.java",
      "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java"
    ],
    "pr_changed_test_files": [
      "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java"
    ],
    "base_commit": "4912ca77847f3f95e2b7b0ce3a093737dc020ce3",
    "head_commit": "0d24d36e08aa169f52f405b86ce74e908ed8fc00",
    "repo_url": "https://github.com/prestodb/presto/pull/24141",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24141",
    "dockerfile": "",
    "pr_merged_at": "2024-11-27T02:22:02.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\nindex f677165a97f64..03a582bcec592 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n@@ -14,10 +14,12 @@\n package com.facebook.presto.execution;\n \n import com.facebook.presto.Session;\n+import com.facebook.presto.common.CatalogSchemaName;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.spi.security.AccessControl;\n import com.facebook.presto.sql.analyzer.SemanticException;\n import com.facebook.presto.sql.tree.Expression;\n+import com.facebook.presto.sql.tree.Identifier;\n import com.facebook.presto.sql.tree.Use;\n import com.facebook.presto.transaction.TransactionManager;\n import com.google.common.util.concurrent.ListenableFuture;\n@@ -26,7 +28,9 @@\n \n import static com.facebook.presto.metadata.MetadataUtil.getConnectorIdOrThrow;\n import static com.facebook.presto.sql.analyzer.SemanticErrorCode.CATALOG_NOT_SPECIFIED;\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.MISSING_SCHEMA;\n import static com.google.common.util.concurrent.Futures.immediateFuture;\n+import static java.lang.String.format;\n \n public class UseTask\n         implements SessionTransactionControlTask<Use>\n@@ -52,7 +56,7 @@ public ListenableFuture<?> execute(\n \n         checkAndSetCatalog(statement, metadata, stateMachine, session);\n \n-        stateMachine.setSetSchema(statement.getSchema().getValueLowerCase());\n+        checkAndSetSchema(statement, metadata, stateMachine, session);\n \n         return immediateFuture(null);\n     }\n@@ -72,4 +76,16 @@ private void checkAndSetCatalog(Use statement, Metadata metadata, QueryStateMach\n             stateMachine.setSetCatalog(catalog);\n         }\n     }\n+\n+    private void checkAndSetSchema(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session)\n+    {\n+        String catalog = statement.getCatalog()\n+                .map(Identifier::getValueLowerCase)\n+                .orElseGet(() -> session.getCatalog().map(String::toLowerCase).get());\n+        String schema = statement.getSchema().getValueLowerCase();\n+        if (!metadata.getMetadataResolver(session).schemaExists(new CatalogSchemaName(catalog, schema))) {\n+            throw new SemanticException(MISSING_SCHEMA, format(\"Schema does not exist: %s.%s\", catalog, schema));\n+        }\n+        stateMachine.setSetSchema(schema);\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataUtil.java b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataUtil.java\nindex 6e13a37eccbf5..7433057b3a30a 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataUtil.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataUtil.java\n@@ -41,7 +41,6 @@\n import java.util.List;\n import java.util.Optional;\n \n-import static com.facebook.presto.spi.StandardErrorCode.NOT_FOUND;\n import static com.facebook.presto.spi.StandardErrorCode.SYNTAX_ERROR;\n import static com.facebook.presto.spi.security.PrincipalType.ROLE;\n import static com.facebook.presto.spi.security.PrincipalType.USER;\n@@ -94,7 +93,7 @@ public static SchemaTableName toSchemaTableName(QualifiedObjectName qualifiedObj\n     public static ConnectorId getConnectorIdOrThrow(Session session, Metadata metadata, String catalogName)\n     {\n         return metadata.getCatalogHandle(session, catalogName)\n-                .orElseThrow(() -> new PrestoException(NOT_FOUND, \"Catalog does not exist: \" + catalogName));\n+                .orElseThrow(() -> new SemanticException(MISSING_CATALOG, \"Catalog does not exist: \" + catalogName));\n     }\n \n     public static ConnectorId getConnectorIdOrThrow(Session session, Metadata metadata, String catalogName, Statement statement, String errorMsg)\n",
    "test_patch": "diff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java\nindex 4da201f77a9dc..17c9c957cca20 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoQueries.java\n@@ -15,6 +15,7 @@\n \n import com.facebook.presto.testing.QueryRunner;\n import com.facebook.presto.tests.AbstractTestQueries;\n+import org.testng.annotations.Test;\n \n public class TestPrestoQueries\n         extends AbstractTestQueries\n@@ -193,4 +194,10 @@ public void testSetSessionNativeWorkerSessionProperty()\n     {\n         // prepared statement is not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testUse()\n+    {\n+        // USE statement is not supported\n+    }\n }\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\nindex 787d43f37279f..080c530875095 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n@@ -2788,6 +2788,28 @@ public void testShowSchemasFrom()\n         assertTrue(result.getOnlyColumnAsSet().containsAll(ImmutableSet.of(getSession().getSchema().get(), INFORMATION_SCHEMA)));\n     }\n \n+    @Test\n+    public void testUse()\n+    {\n+        Session sessionWithDefaultCatalogAndSchema = getSession();\n+        String catalog = sessionWithDefaultCatalogAndSchema.getCatalog().get();\n+        String schema = sessionWithDefaultCatalogAndSchema.getSchema().get();\n+\n+        assertQueryFails(sessionWithDefaultCatalogAndSchema, \"USE non_exist_schema\", format(\"Schema does not exist: %s.non_exist_schema\", catalog));\n+        assertQueryFails(sessionWithDefaultCatalogAndSchema, \"USE non_exist_catalog.any_schema\", \"Catalog does not exist: non_exist_catalog\");\n+        assertQueryFails(sessionWithDefaultCatalogAndSchema, format(\"USE %s.non_exist_schema\", catalog), format(\"Schema does not exist: %s.non_exist_schema\", catalog));\n+        assertUpdate(sessionWithDefaultCatalogAndSchema, format(\"USE %s.%s\", catalog, schema));\n+\n+        Session sessionWithoutDefaultCatalogAndSchema = Session.builder(getSession())\n+                .setCatalog(null)\n+                .setSchema(null)\n+                .build();\n+        assertQueryFails(sessionWithoutDefaultCatalogAndSchema, \"USE any_schema\", \".* Catalog must be specified when session catalog is not set\");\n+        assertQueryFails(sessionWithoutDefaultCatalogAndSchema, \"USE non_exist_catalog.any_schema\", \"Catalog does not exist: non_exist_catalog\");\n+        assertQueryFails(sessionWithoutDefaultCatalogAndSchema, format(\"USE %s.non_exist_schema\", catalog), format(\"Schema does not exist: %s.non_exist_schema\", catalog));\n+        assertUpdate(sessionWithoutDefaultCatalogAndSchema, format(\"USE %s.%s\", catalog, schema));\n+    }\n+\n     @Test\n     public void testShowSchemasLike()\n     {\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java\nindex 63d4fc2e5838a..b381b01f5ada3 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestLocalQueries.java\n@@ -117,6 +117,12 @@ public void testDecimal()\n         assertQuery(\"SELECT 0.1\", \"SELECT CAST('0.1' AS DECIMAL)\");\n     }\n \n+    @Test\n+    public void testUse()\n+    {\n+        // USE statement is not supported\n+    }\n+\n     @Test\n     public void testIOExplain()\n     {\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java\nindex 9f51fc2130404..98fdee5fe7ebf 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestQueryPlanDeterminism.java\n@@ -216,6 +216,12 @@ protected void assertTableColumnNames(String tableName, String... columnNames)\n     {\n     }\n \n+    @Test\n+    public void testUse()\n+    {\n+        // USE statement is not supported\n+    }\n+\n     @Override\n     protected MaterializedResult computeExpected(@Language(\"SQL\") String sql, List<? extends Type> resultTypes)\n     {\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java\nindex 2cbcaf93babc8..3cb9f26f1b8fd 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestVerboseOptimizerInfo.java\n@@ -157,6 +157,12 @@ public void testSetSessionNativeWorkerSessionProperty()\n     {\n     }\n \n+    @Test\n+    public void testUse()\n+    {\n+        // USE statement is not supported\n+    }\n+\n     private void checkOptimizerInfo(String explain, String optimizerType, List<String> optimizers)\n     {\n         checkOptimizerInfo(explain, optimizerType, optimizers, new ArrayList<>());\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24115",
    "pr_id": 24115,
    "issue_id": 23152,
    "repo": "prestodb/presto",
    "problem_statement": "Implement Jdbc join pushdown capabilities in presto\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\r\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\r\n\r\n## Expected Behavior or Use Case\r\n\r\nWhen presto identifies a join query which is specific to a Jdbc remote datasource, it split the join query into multiple select query based on the number of tables involved in the join query and select all records from each tables using the jdbc connector without passing the join condition. \r\n\r\nIf we  \"Push down\" or send these joins which involves same catalog/remote datasource as part of the  SQL  to the remote data source it increase the performance 3x to 10x.\r\n\r\n## Presto Component, Service, or Connector\r\n<!--- Tell us to which service or component this request is related to -->\r\nJdbc Connector and spi module which create presto PlanNode\r\n\r\n## Possible Implementation\r\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\r\nBegin to look at Presto analyze component that breaks down a multi join query into multiple subqueries and do the following actions\r\n\r\n- Retain the join for the same catalog (Jdbc connector) tables instead of breaks to multiple select query\r\n-  Pass predicates that are related to that query before you pass it to JDBC.\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\nThis git issue is to address a performance limitation of Presto federation of SQLs of JDBC connector to remote data sources such as DB2, Postgres, Oracle etc.\r\n\r\nWhat is the limitation:\r\nFor a given query that might involve different remote data sources, the query is divided into sub-queries that can be sent to the respective data source, results of these sub-queries are fetched into the presto workers, additional operations such as filters, joins, sorts are applied before sending back to the user.\r\n\r\nOperations such as filters, joins involve tables from the same remote data source, it is best to \"Push down\" or send these joins as part of the SQL query to the remote data source and it can be 10x to 100x faster than fetch all data of the table into presto workers and then apply these operators.\r\n\r\n",
    "issue_word_count": 364,
    "test_files_count": 8,
    "non_test_files_count": 15,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/admin/properties-session.rst",
      "presto-docs/src/main/sphinx/admin/properties.rst",
      "presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/Metadata.java",
      "presto-main-base/src/main/java/com/facebook/presto/metadata/MetadataManager.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/ReorderJoins.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/GroupInnerJoinsByConnectorRuleSet.java",
      "presto-main-base/src/main/java/com/facebook/presto/sql/planner/plan/MultiJoinNode.java",
      "presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestGroupInnerJoinsByConnectorRuleSet.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/JoinTableInfo.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/JoinTableSet.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorCapabilities.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestGroupInnerJoinsByConnectorRuleSet.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java",
      "presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java"
    ],
    "base_commit": "b2919faba781a0aeea2eb52d44bd807579fdcd41",
    "head_commit": "97d1bce4e3a91af8837681f42623bc167029c1e4",
    "repo_url": "https://github.com/prestodb/presto/pull/24115",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24115",
    "dockerfile": "",
    "pr_merged_at": "2025-04-22T04:35:14.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/admin/properties-session.rst b/presto-docs/src/main/sphinx/admin/properties-session.rst\nindex fe1bb5e246db7..1f55bd6939b8e 100644\n--- a/presto-docs/src/main/sphinx/admin/properties-session.rst\n+++ b/presto-docs/src/main/sphinx/admin/properties-session.rst\n@@ -336,7 +336,27 @@ The corresponding configuration property is :ref:`admin/properties:\\`\\`optimizer\n \n Enable retry for failed queries who can potentially be helped by HBO. \n \n-The corresponding configuration property is :ref:`admin/properties:\\`\\`optimizer.retry-query-with-history-based-optimization\\`\\``. \n+The corresponding configuration property is :ref:`admin/properties:\\`\\`optimizer.retry-query-with-history-based-optimization\\`\\``.\n+\n+``optimizer_inner_join_pushdown_enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* **Type:** ``boolean``\n+* **Default value:** ``false``\n+\n+Enable push down inner join predicates to database. Only allows equality joins to be pushed down.\n+Use :ref:`admin/properties-session:\\`\\`optimizer_inequality_join_pushdown_enabled\\`\\`` along with this configuration to push down inequality join predicates.\n+\n+The corresponding configuration property is :ref:`admin/properties:\\`\\`optimizer.inner-join-pushdown-enabled\\`\\``.\n+\n+``optimizer_inequality_join_pushdown_enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* **Type:** ``boolean``\n+* **Default value:** ``false``\n+\n+Enable push down inner join inequality predicates to database. For this configuration to be enabled, :ref:`admin/properties-session:\\`\\`optimizer_inner_join_pushdown_enabled\\`\\`` should be set to ``true``.\n+The corresponding configuration property is :ref:`admin/properties:\\`\\`optimizer.inequality-join-pushdown-enabled\\`\\``.\n \n JDBC Properties\n ---------------\n\ndiff --git a/presto-docs/src/main/sphinx/admin/properties.rst b/presto-docs/src/main/sphinx/admin/properties.rst\nindex 36b411c6e3a46..d6f85f194a38f 100644\n--- a/presto-docs/src/main/sphinx/admin/properties.rst\n+++ b/presto-docs/src/main/sphinx/admin/properties.rst\n@@ -912,7 +912,27 @@ The corresponding session property is :ref:`admin/properties-session:\\`\\`treat-l\n \n Enable retry for failed queries who can potentially be helped by HBO. \n \n-The corresponding session property is :ref:`admin/properties-session:\\`\\`retry-query-with-history-based-optimization\\`\\``. \n+The corresponding session property is :ref:`admin/properties-session:\\`\\`retry-query-with-history-based-optimization\\`\\``.\n+\n+``optimizer.inner-join-pushdown-enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* **Type:** ``boolean``\n+* **Default value:** ``false``\n+\n+Enable push down inner join predicates to database. Only allows equality joins to be pushed down.\n+Use :ref:`admin/properties:\\`\\`optimizer.inequality-join-pushdown-enabled\\`\\`` along with this configuration to push down inequality join predicates.\n+\n+The corresponding session property is :ref:`admin/properties-session:\\`\\`optimizer_inner_join_pushdown_enabled\\`\\``.\n+\n+``optimizer.inequality-join-pushdown-enabled``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* **Type:** ``boolean``\n+* **Default value:** ``false``\n+\n+Enable push down inner join inequality predicates to database. For this configuration to be enabled, :ref:`admin/properties:\\`\\`optimizer.inner-join-pushdown-enabled\\`\\`` should be set to ``true``.\n+The corresponding session property is :ref:`admin/properties-session:\\`\\`optimizer_inequality_join_pushdown_enabled\\`\\``.\n \n ``optimizer.use-histograms``\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java b/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\nindex fe1b35cc3c368..91faf426781f5 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/SystemSessionProperties.java\n@@ -337,6 +337,8 @@ public final class SystemSessionProperties\n     private static final String NATIVE_EXECUTION_EXECUTABLE_PATH = \"native_execution_executable_path\";\n     private static final String NATIVE_EXECUTION_PROGRAM_ARGUMENTS = \"native_execution_program_arguments\";\n     public static final String NATIVE_EXECUTION_PROCESS_REUSE_ENABLED = \"native_execution_process_reuse_enabled\";\n+    public static final String INNER_JOIN_PUSHDOWN_ENABLED = \"optimizer_inner_join_pushdown_enabled\";\n+    public static final String INEQUALITY_JOIN_PUSHDOWN_ENABLED = \"optimizer_inequality_join_pushdown_enabled\";\n     public static final String NATIVE_MIN_COLUMNAR_ENCODING_CHANNELS_TO_PREFER_ROW_WISE_ENCODING = \"native_min_columnar_encoding_channels_to_prefer_row_wise_encoding\";\n     public static final String NATIVE_ENFORCE_JOIN_BUILD_INPUT_PARTITION = \"native_enforce_join_build_input_partition\";\n     public static final String NATIVE_EXECUTION_SCALE_WRITER_THREADS_ENABLED = \"native_execution_scale_writer_threads_enabled\";\n@@ -1851,6 +1853,16 @@ public SystemSessionProperties(\n                         \"Include values node for connector optimizer\",\n                         featuresConfig.isIncludeValuesNodeInConnectorOptimizer(),\n                         false),\n+                booleanProperty(\n+                        INNER_JOIN_PUSHDOWN_ENABLED,\n+                        \"Enable Join Predicate Pushdown\",\n+                        featuresConfig.isInnerJoinPushdownEnabled(),\n+                        false),\n+                booleanProperty(\n+                        INEQUALITY_JOIN_PUSHDOWN_ENABLED,\n+                        \"Enable Join Pushdown for Inequality Predicates\",\n+                        featuresConfig.isInEqualityJoinPushdownEnabled(),\n+                    false),\n                 integerProperty(\n                         NATIVE_MIN_COLUMNAR_ENCODING_CHANNELS_TO_PREFER_ROW_WISE_ENCODING,\n                         \"Minimum number of columnar encoding channels to consider row wise encoding for partitioned exchange. Native execution only\",\n@@ -3165,6 +3177,16 @@ public static boolean isIncludeValuesNodeInConnectorOptimizer(Session session)\n         return session.getSystemProperty(INCLUDE_VALUES_NODE_IN_CONNECTOR_OPTIMIZER, Boolean.class);\n     }\n \n+    public static Boolean isInnerJoinPushdownEnabled(Session session)\n+    {\n+        return session.getSystemProperty(INNER_JOIN_PUSHDOWN_ENABLED, Boolean.class);\n+    }\n+\n+    public static Boolean isInEqualityPushdownEnabled(Session session)\n+    {\n+        return session.getSystemProperty(INEQUALITY_JOIN_PUSHDOWN_ENABLED, Boolean.class);\n+    }\n+\n     public static int getMinColumnarEncodingChannelsToPreferRowWiseEncoding(Session session)\n     {\n         return session.getSystemProperty(NATIVE_MIN_COLUMNAR_ENCODING_CHANNELS_TO_PREFER_ROW_WISE_ENCODING, Integer.class);\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/Metadata.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/Metadata.java\nindex 96ac818709096..002b3437c30ff 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/metadata/Metadata.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/Metadata.java\n@@ -45,6 +45,8 @@\n import com.facebook.presto.spi.constraints.TableConstraint;\n import com.facebook.presto.spi.function.SqlFunction;\n import com.facebook.presto.spi.plan.PartitioningHandle;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.spi.security.GrantInfo;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n@@ -523,4 +525,14 @@ default TableLayoutFilterCoverage getTableLayoutFilterCoverage(Session session,\n     void dropConstraint(Session session, TableHandle tableHandle, Optional<String> constraintName, Optional<String> columnName);\n \n     void addConstraint(Session session, TableHandle tableHandle, TableConstraint<String> tableConstraint);\n+\n+    /**\n+     * Check if the join predicate can be translated and pushed down to the underlying datasource\n+     *\n+     * @return TRUE if there is connector is able to translate and push down join predicate to the underlying datasource, FALSE otherwise\n+     */\n+    default boolean isPushdownSupportedForFilter(Session session, TableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+    {\n+        return false;\n+    }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/metadata/MetadataManager.java b/presto-main-base/src/main/java/com/facebook/presto/metadata/MetadataManager.java\nindex 87a83d7060ff0..0c023556410ae 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n@@ -65,6 +65,8 @@\n import com.facebook.presto.spi.constraints.TableConstraint;\n import com.facebook.presto.spi.function.SqlFunction;\n import com.facebook.presto.spi.plan.PartitioningHandle;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.spi.security.GrantInfo;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n@@ -1549,4 +1551,15 @@ public static Function<SchemaTableName, QualifiedObjectName> convertFromSchemaTa\n     {\n         return input -> new QualifiedObjectName(catalogName, input.getSchemaName(), input.getTableName());\n     }\n+\n+    @Override\n+    public boolean isPushdownSupportedForFilter(Session session, TableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+    {\n+        ConnectorId connectorId = tableHandle.getConnectorId();\n+        CatalogMetadata catalogMetadata = getCatalogMetadata(session, connectorId);\n+        ConnectorSession connectorSession = session.toConnectorSession(catalogMetadata.getConnectorId());\n+        ConnectorMetadata metadata = catalogMetadata.getMetadata();\n+\n+        return metadata.isPushdownSupportedForFilter(connectorSession, tableHandle.getConnectorHandle(), filter, symbolToColumnHandleMap);\n+    }\n }\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\nindex a962fb66004e7..cd5dad391ddf6 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n@@ -293,6 +293,8 @@ public class FeaturesConfig\n \n     private boolean setExcludeInvalidWorkerSessionProperties;\n     private int eagerPlanValidationThreadPoolSize = 20;\n+    private boolean innerJoinPushdownEnabled;\n+    private boolean inEqualityJoinPushdownEnabled;\n \n     private boolean prestoSparkExecutionEnvironment;\n     private boolean singleNodeExecutionEnabled;\n@@ -2885,6 +2887,31 @@ public int getEagerPlanValidationThreadPoolSize()\n         return this.eagerPlanValidationThreadPoolSize;\n     }\n \n+    @Config(\"optimizer.inner-join-pushdown-enabled\")\n+    @ConfigDescription(\"Push down inner join predicates to database\")\n+    public FeaturesConfig setInnerJoinPushdownEnabled(boolean innerJoinPushdownEnabled)\n+    {\n+        this.innerJoinPushdownEnabled = innerJoinPushdownEnabled;\n+        return this;\n+    }\n+\n+    public boolean isInnerJoinPushdownEnabled()\n+    {\n+        return innerJoinPushdownEnabled;\n+    }\n+\n+    @Config(\"optimizer.inequality-join-pushdown-enabled\")\n+    @ConfigDescription(\"Push down inner join inequality predicates to database\")\n+    public FeaturesConfig setInEqualityJoinPushdownEnabled(boolean inEqualityJoinPushdownEnabled)\n+    {\n+        this.inEqualityJoinPushdownEnabled = inEqualityJoinPushdownEnabled;\n+        return this;\n+    }\n+\n+    public boolean isInEqualityJoinPushdownEnabled()\n+    {\n+        return inEqualityJoinPushdownEnabled;\n+    }\n     public boolean isPrestoSparkExecutionEnvironment()\n     {\n         return prestoSparkExecutionEnvironment;\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\nindex 8e0f97689bcef..d00dcd18d2c5b 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\n@@ -154,6 +154,7 @@\n import com.facebook.presto.sql.planner.optimizations.ApplyConnectorOptimization;\n import com.facebook.presto.sql.planner.optimizations.CheckSubqueryNodesAreRewritten;\n import com.facebook.presto.sql.planner.optimizations.CteProjectionAndPredicatePushDown;\n+import com.facebook.presto.sql.planner.optimizations.GroupInnerJoinsByConnectorRuleSet;\n import com.facebook.presto.sql.planner.optimizations.HashGenerationOptimizer;\n import com.facebook.presto.sql.planner.optimizations.HistoricalStatisticsEquivalentPlanMarkingOptimizer;\n import com.facebook.presto.sql.planner.optimizations.ImplementIntersectAndExceptAsUnion;\n@@ -770,6 +771,13 @@ public PlanOptimizers(\n         // After this step, nodes with same `statsEquivalentPlanNode` will share same history based statistics.\n         builder.add(new StatsRecordingPlanOptimizer(optimizerStats, new HistoricalStatisticsEquivalentPlanMarkingOptimizer(statsCalculator)));\n \n+        builder.add(new IterativeOptimizer(\n+                metadata,\n+                ruleStats,\n+                statsCalculator,\n+                estimatedExchangesCostCalculator,\n+                new GroupInnerJoinsByConnectorRuleSet(metadata, predicatePushDown).rules()));\n+\n         builder.add(new IterativeOptimizer(\n                 metadata,\n                 // Because ReorderJoins runs only once,\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/ReorderJoins.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/ReorderJoins.java\nindex f8dd1f9c6d85b..818450a21ca2c 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/ReorderJoins.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/iterative/rule/ReorderJoins.java\n@@ -29,7 +29,6 @@\n import com.facebook.presto.spi.plan.FilterNode;\n import com.facebook.presto.spi.plan.JoinNode;\n import com.facebook.presto.spi.plan.PlanNode;\n-import com.facebook.presto.spi.plan.PlanNodeId;\n import com.facebook.presto.spi.plan.PlanNodeIdAllocator;\n import com.facebook.presto.spi.plan.ProjectNode;\n import com.facebook.presto.spi.relation.CallExpression;\n@@ -37,10 +36,10 @@\n import com.facebook.presto.spi.relation.RowExpression;\n import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.sql.analyzer.FeaturesConfig.JoinDistributionType;\n-import com.facebook.presto.sql.planner.CanonicalJoinNode;\n import com.facebook.presto.sql.planner.EqualityInference;\n import com.facebook.presto.sql.planner.iterative.Lookup;\n import com.facebook.presto.sql.planner.iterative.Rule;\n+import com.facebook.presto.sql.planner.plan.MultiJoinNode;\n import com.facebook.presto.sql.relational.FunctionResolution;\n import com.facebook.presto.sql.relational.RowExpressionDeterminismEvaluator;\n import com.google.common.annotations.VisibleForTesting;\n@@ -72,7 +71,6 @@\n import static com.facebook.presto.SystemSessionProperties.shouldHandleComplexEquiJoins;\n import static com.facebook.presto.expressions.LogicalRowExpressions.TRUE_CONSTANT;\n import static com.facebook.presto.expressions.LogicalRowExpressions.and;\n-import static com.facebook.presto.expressions.LogicalRowExpressions.extractConjuncts;\n import static com.facebook.presto.expressions.RowExpressionNodeInliner.replaceExpression;\n import static com.facebook.presto.spi.plan.JoinDistributionType.PARTITIONED;\n import static com.facebook.presto.spi.plan.JoinDistributionType.REPLICATED;\n@@ -87,7 +85,6 @@\n import static com.facebook.presto.sql.planner.iterative.rule.DetermineJoinDistributionType.mustPartition;\n import static com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.JoinEnumerationResult.INFINITE_COST_RESULT;\n import static com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.JoinEnumerationResult.UNKNOWN_COST_RESULT;\n-import static com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.MultiJoinNode.toMultiJoinNode;\n import static com.facebook.presto.sql.planner.optimizations.JoinNodeUtils.toRowExpression;\n import static com.facebook.presto.sql.planner.optimizations.QueryCardinalityUtil.isAtMostScalar;\n import static com.facebook.presto.sql.planner.plan.AssignmentUtils.getNonIdentityAssignments;\n@@ -598,270 +595,148 @@ private JoinEnumerationResult createJoinEnumerationResult(PlanNode planNode)\n         }\n     }\n \n-    /**\n-     * This class represents a set of inner joins that can be executed in any order.\n-     */\n-    @VisibleForTesting\n-    static class MultiJoinNode\n+    public static MultiJoinNode toMultiJoinNode(JoinNode joinNode, Lookup lookup, int joinLimit, boolean handleComplexEquiJoins, FunctionResolution functionResolution, DeterminismEvaluator determinismEvaluator)\n     {\n-        // Use a linked hash set to ensure optimizer is deterministic\n-        private final CanonicalJoinNode node;\n-        private final Assignments assignments;\n-\n-        public MultiJoinNode(LinkedHashSet<PlanNode> sources, RowExpression filter, List<VariableReferenceExpression> outputVariables,\n-                Assignments assignments)\n-        {\n-            checkArgument(sources.size() > 1, \"sources size is <= 1\");\n-\n-            requireNonNull(sources, \"sources is null\");\n-            requireNonNull(filter, \"filter is null\");\n-            requireNonNull(outputVariables, \"outputVariables is null\");\n-            requireNonNull(assignments, \"assignments is null\");\n-\n-            this.assignments = assignments;\n-            // Plan node id doesn't matter here as we don't use this in planner\n-            this.node = new CanonicalJoinNode(\n-                    new PlanNodeId(\"\"),\n-                    sources.stream().collect(toImmutableList()),\n-                    INNER,\n-                    ImmutableSet.of(),\n-                    ImmutableSet.of(filter),\n-                    outputVariables);\n-        }\n+        // the number of sources is the number of joins + 1\n+        return new JoinNodeFlattener(joinNode, lookup, joinLimit + 1, handleComplexEquiJoins, functionResolution, determinismEvaluator).toMultiJoinNode();\n+    }\n \n-        public RowExpression getFilter()\n-        {\n-            return node.getFilters().stream().findAny().get();\n-        }\n+    @VisibleForTesting\n+    private static class JoinNodeFlattener\n+    {\n+        private final LinkedHashSet<PlanNode> sources = new LinkedHashSet<>();\n+        private final Assignments intermediateAssignments;\n+        private final boolean handleComplexEquiJoins;\n+        private List<RowExpression> filters = new ArrayList<>();\n+        private final List<VariableReferenceExpression> outputVariables;\n+        private final FunctionResolution functionResolution;\n+        private final DeterminismEvaluator determinismEvaluator;\n+        private final Lookup lookup;\n \n-        public LinkedHashSet<PlanNode> getSources()\n+        JoinNodeFlattener(JoinNode node, Lookup lookup, int sourceLimit, boolean handleComplexEquiJoins, FunctionResolution functionResolution,\n+                DeterminismEvaluator determinismEvaluator)\n         {\n-            return new LinkedHashSet<>(node.getSources());\n-        }\n+            requireNonNull(node, \"node is null\");\n+            checkState(node.getType() == INNER, \"join type must be INNER\");\n+            this.outputVariables = node.getOutputVariables();\n+            this.lookup = requireNonNull(lookup, \"lookup is null\");\n+            this.functionResolution = requireNonNull(functionResolution, \"functionResolution is null\");\n+            this.determinismEvaluator = requireNonNull(determinismEvaluator, \"determinismEvaluator is null\");\n+            this.handleComplexEquiJoins = handleComplexEquiJoins;\n \n-        public List<VariableReferenceExpression> getOutputVariables()\n-        {\n-            return node.getOutputVariables();\n-        }\n+            Map<VariableReferenceExpression, RowExpression> intermediateAssignments = new HashMap<>();\n+            flattenNode(node, sourceLimit, intermediateAssignments);\n \n-        public Assignments getAssignments()\n-        {\n-            return assignments;\n+            // We resolve the intermediate assignments to only inputs of the flattened join node\n+            ImmutableSet<VariableReferenceExpression> inputVariables = sources.stream().flatMap(s -> s.getOutputVariables().stream()).collect(toImmutableSet());\n+            this.intermediateAssignments = resolveAssignments(intermediateAssignments, inputVariables);\n+            rewriteFilterWithInlinedAssignments(this.intermediateAssignments);\n         }\n \n-        public static Builder builder()\n+        private Assignments resolveAssignments(Map<VariableReferenceExpression, RowExpression> assignments, Set<VariableReferenceExpression> availableVariables)\n         {\n-            return new Builder();\n-        }\n+            HashSet<VariableReferenceExpression> resolvedVariables = new HashSet<>();\n+            ImmutableList.copyOf(assignments.keySet()).forEach(variable -> resolveVariable(variable, resolvedVariables, assignments, availableVariables));\n \n-        @Override\n-        public int hashCode()\n-        {\n-            return Objects.hash(getSources(), ImmutableSet.copyOf(extractConjuncts(getFilter())), getOutputVariables());\n+            return Assignments.builder().putAll(assignments).build();\n         }\n \n-        @Override\n-        public boolean equals(Object obj)\n+        private void resolveVariable(VariableReferenceExpression variable, HashSet<VariableReferenceExpression> resolvedVariables, Map<VariableReferenceExpression,\n+                RowExpression> assignments, Set<VariableReferenceExpression> availableVariables)\n         {\n-            if (!(obj instanceof MultiJoinNode)) {\n-                return false;\n-            }\n+            RowExpression expression = assignments.get(variable);\n+            Sets.SetView<VariableReferenceExpression> variablesToResolve = Sets.difference(Sets.difference(extractUnique(expression), availableVariables), resolvedVariables);\n \n-            MultiJoinNode other = (MultiJoinNode) obj;\n-            return getSources().equals(other.getSources())\n-                    && ImmutableSet.copyOf(extractConjuncts(getFilter())).equals(ImmutableSet.copyOf(extractConjuncts(other.getFilter())))\n-                    && getOutputVariables().equals(other.getOutputVariables())\n-                    && getAssignments().equals(other.getAssignments());\n-        }\n+            // Recursively resolve any unresolved variables\n+            variablesToResolve.forEach(variableToResolve -> resolveVariable(variableToResolve, resolvedVariables, assignments, availableVariables));\n \n-        @Override\n-        public String toString()\n-        {\n-            return \"MultiJoinNode{\" +\n-                    \"node=\" + node +\n-                    \", assignments=\" + assignments +\n-                    '}';\n+            // Modify the assignment for the variable : Replace it with the now resolved constituent variables\n+            assignments.put(variable, replaceExpression(expression, assignments));\n+            // Mark this variable as resolved\n+            resolvedVariables.add(variable);\n         }\n \n-        static MultiJoinNode toMultiJoinNode(JoinNode joinNode, Lookup lookup, int joinLimit, boolean handleComplexEquiJoins, FunctionResolution functionResolution, DeterminismEvaluator determinismEvaluator)\n+        private void rewriteFilterWithInlinedAssignments(Assignments assignments)\n         {\n-            // the number of sources is the number of joins + 1\n-            return new JoinNodeFlattener(joinNode, lookup, joinLimit + 1, handleComplexEquiJoins, functionResolution, determinismEvaluator).toMultiJoinNode();\n+            ImmutableList.Builder<RowExpression> modifiedFilters = ImmutableList.builder();\n+            filters.forEach(filter -> modifiedFilters.add(replaceExpression(filter, assignments.getMap())));\n+            filters = modifiedFilters.build();\n         }\n \n-        private static class JoinNodeFlattener\n+        private void flattenNode(PlanNode node, int limit, Map<VariableReferenceExpression, RowExpression> assignmentsBuilder)\n         {\n-            private final LinkedHashSet<PlanNode> sources = new LinkedHashSet<>();\n-            private final Assignments intermediateAssignments;\n-            private final boolean handleComplexEquiJoins;\n-            private List<RowExpression> filters = new ArrayList<>();\n-            private final List<VariableReferenceExpression> outputVariables;\n-            private final FunctionResolution functionResolution;\n-            private final DeterminismEvaluator determinismEvaluator;\n-            private final Lookup lookup;\n-\n-            JoinNodeFlattener(JoinNode node, Lookup lookup, int sourceLimit, boolean handleComplexEquiJoins, FunctionResolution functionResolution,\n-                    DeterminismEvaluator determinismEvaluator)\n-            {\n-                requireNonNull(node, \"node is null\");\n-                checkState(node.getType() == INNER, \"join type must be INNER\");\n-                this.outputVariables = node.getOutputVariables();\n-                this.lookup = requireNonNull(lookup, \"lookup is null\");\n-                this.functionResolution = requireNonNull(functionResolution, \"functionResolution is null\");\n-                this.determinismEvaluator = requireNonNull(determinismEvaluator, \"determinismEvaluator is null\");\n-                this.handleComplexEquiJoins = handleComplexEquiJoins;\n-\n-                Map<VariableReferenceExpression, RowExpression> intermediateAssignments = new HashMap<>();\n-                flattenNode(node, sourceLimit, intermediateAssignments);\n-\n-                // We resolve the intermediate assignments to only inputs of the flattened join node\n-                ImmutableSet<VariableReferenceExpression> inputVariables = sources.stream().flatMap(s -> s.getOutputVariables().stream()).collect(toImmutableSet());\n-                this.intermediateAssignments = resolveAssignments(intermediateAssignments, inputVariables);\n-                rewriteFilterWithInlinedAssignments(this.intermediateAssignments);\n-            }\n-\n-            private Assignments resolveAssignments(Map<VariableReferenceExpression, RowExpression> assignments, Set<VariableReferenceExpression> availableVariables)\n-            {\n-                HashSet<VariableReferenceExpression> resolvedVariables = new HashSet<>();\n-                ImmutableList.copyOf(assignments.keySet()).forEach(variable -> resolveVariable(variable, resolvedVariables, assignments, availableVariables));\n-\n-                return Assignments.builder().putAll(assignments).build();\n-            }\n-\n-            private void resolveVariable(VariableReferenceExpression variable, HashSet<VariableReferenceExpression> resolvedVariables, Map<VariableReferenceExpression,\n-                    RowExpression> assignments, Set<VariableReferenceExpression> availableVariables)\n-            {\n-                RowExpression expression = assignments.get(variable);\n-                Sets.SetView<VariableReferenceExpression> variablesToResolve = Sets.difference(Sets.difference(extractUnique(expression), availableVariables), resolvedVariables);\n-\n-                // Recursively resolve any unresolved variables\n-                variablesToResolve.forEach(variableToResolve -> resolveVariable(variableToResolve, resolvedVariables, assignments, availableVariables));\n-\n-                // Modify the assignment for the variable : Replace it with the now resolved constituent variables\n-                assignments.put(variable, replaceExpression(expression, assignments));\n-                // Mark this variable as resolved\n-                resolvedVariables.add(variable);\n-            }\n-\n-            private void rewriteFilterWithInlinedAssignments(Assignments assignments)\n-            {\n-                ImmutableList.Builder<RowExpression> modifiedFilters = ImmutableList.builder();\n-                filters.forEach(filter -> modifiedFilters.add(replaceExpression(filter, assignments.getMap())));\n-                filters = modifiedFilters.build();\n-            }\n-\n-            private void flattenNode(PlanNode node, int limit, Map<VariableReferenceExpression, RowExpression> assignmentsBuilder)\n-            {\n-                PlanNode resolved = lookup.resolve(node);\n-\n-                if (resolved instanceof ProjectNode) {\n-                    ProjectNode projectNode = (ProjectNode) resolved;\n-                    // A ProjectNode could be 'hiding' a join source by building an assignment of a complex equi-join criteria like `left.key = right1.key1 + right1.key2`\n-                    // We open up the join space by tracking the assignments from this Project node; these will be inlined into the overall filters once we finish\n-                    // traversing the join graph\n-                    // We only do this if the ProjectNode assignments are deterministic\n-                    if (handleComplexEquiJoins && lookup.resolve(projectNode.getSource()) instanceof JoinNode &&\n-                            projectNode.getAssignments().getExpressions().stream().allMatch(determinismEvaluator::isDeterministic)) {\n-                        //We keep track of only the non-identity assignments since these are the ones that will be inlined into the overall filters\n-                        assignmentsBuilder.putAll(getNonIdentityAssignments(projectNode.getAssignments()));\n-                        flattenNode(projectNode.getSource(), limit, assignmentsBuilder);\n-                    }\n-                    else {\n-                        sources.add(node);\n-                    }\n-                    return;\n+            PlanNode resolved = lookup.resolve(node);\n+\n+            if (resolved instanceof ProjectNode) {\n+                ProjectNode projectNode = (ProjectNode) resolved;\n+                // A ProjectNode could be 'hiding' a join source by building an assignment of a complex equi-join criteria like `left.key = right1.key1 + right1.key2`\n+                // We open up the join space by tracking the assignments from this Project node; these will be inlined into the overall filters once we finish\n+                // traversing the join graph\n+                // We only do this if the ProjectNode assignments are deterministic\n+                if (handleComplexEquiJoins && lookup.resolve(projectNode.getSource()) instanceof JoinNode &&\n+                        projectNode.getAssignments().getExpressions().stream().allMatch(determinismEvaluator::isDeterministic)) {\n+                    //We keep track of only the non-identity assignments since these are the ones that will be inlined into the overall filters\n+                    assignmentsBuilder.putAll(getNonIdentityAssignments(projectNode.getAssignments()));\n+                    flattenNode(projectNode.getSource(), limit, assignmentsBuilder);\n                 }\n-\n-                // (limit - 2) because you need to account for adding left and right side\n-                if (!(resolved instanceof JoinNode) || (sources.size() > (limit - 2))) {\n-                    sources.add(node);\n-                    return;\n-                }\n-\n-                JoinNode joinNode = (JoinNode) resolved;\n-                if (joinNode.getType() != INNER || !determinismEvaluator.isDeterministic(joinNode.getFilter().orElse(TRUE_CONSTANT)) || joinNode.getDistributionType().isPresent()) {\n+                else {\n                     sources.add(node);\n-                    return;\n                 }\n-\n-                // we set the left limit to limit - 1 to account for the node on the right\n-                flattenNode(joinNode.getLeft(), limit - 1, assignmentsBuilder);\n-                flattenNode(joinNode.getRight(), limit, assignmentsBuilder);\n-                joinNode.getCriteria().stream()\n-                        .map(criteria -> toRowExpression(criteria, functionResolution))\n-                        .forEach(filters::add);\n-                joinNode.getFilter().ifPresent(filters::add);\n+                return;\n             }\n \n-            MultiJoinNode toMultiJoinNode()\n-            {\n-                ImmutableSet<VariableReferenceExpression> inputVariables = sources.stream().flatMap(source -> source.getOutputVariables().stream()).collect(toImmutableSet());\n-\n-                // We could have some output variables that were possibly generated from intermediate assignments\n-                // For each of these variables, use the intermediate assignments to replace this variable with the set of input variables it uses\n-\n-                // Additionally, we build an overall set of assignments for the reordered Join node - this is used to add a wrapper Project over the updated output variables\n-                // We do this to satisfy the invariant that the rewritten Join node must produce the same output variables as the input Join node\n-                ImmutableSet.Builder<VariableReferenceExpression> updatedOutputVariables = ImmutableSet.builder();\n-                Assignments.Builder overallAssignments = Assignments.builder();\n-                boolean nonIdentityAssignmentsFound = false;\n-\n-                for (VariableReferenceExpression outputVariable : outputVariables) {\n-                    if (inputVariables.contains(outputVariable)) {\n-                        overallAssignments.put(outputVariable, outputVariable);\n-                        updatedOutputVariables.add(outputVariable);\n-                        continue;\n-                    }\n-\n-                    checkState(intermediateAssignments.getMap().containsKey(outputVariable),\n-                            \"Output variable [%s] not found in input variables or in intermediate assignments\", outputVariable);\n-                    nonIdentityAssignmentsFound = true;\n-                    overallAssignments.put(outputVariable, intermediateAssignments.get(outputVariable));\n-                    updatedOutputVariables.addAll(extractUnique(intermediateAssignments.get(outputVariable)));\n-                }\n-\n-                return new MultiJoinNode(sources,\n-                        and(filters),\n-                        updatedOutputVariables.build().asList(),\n-                        nonIdentityAssignmentsFound ? overallAssignments.build() : Assignments.of());\n+            // (limit - 2) because you need to account for adding left and right side\n+            if (!(resolved instanceof JoinNode) || (sources.size() > (limit - 2))) {\n+                sources.add(node);\n+                return;\n             }\n-        }\n \n-        static class Builder\n-        {\n-            private List<PlanNode> sources;\n-            private RowExpression filter;\n-            private List<VariableReferenceExpression> outputVariables;\n-            private Assignments assignments = Assignments.of();\n-\n-            public Builder setSources(PlanNode... sources)\n-            {\n-                this.sources = ImmutableList.copyOf(sources);\n-                return this;\n+            JoinNode joinNode = (JoinNode) resolved;\n+            if (joinNode.getType() != INNER || !determinismEvaluator.isDeterministic(joinNode.getFilter().orElse(TRUE_CONSTANT)) || joinNode.getDistributionType().isPresent()) {\n+                sources.add(node);\n+                return;\n             }\n \n-            public Builder setFilter(RowExpression filter)\n-            {\n-                this.filter = filter;\n-                return this;\n-            }\n+            // we set the left limit to limit - 1 to account for the node on the right\n+            flattenNode(joinNode.getLeft(), limit - 1, assignmentsBuilder);\n+            flattenNode(joinNode.getRight(), limit, assignmentsBuilder);\n+            joinNode.getCriteria().stream()\n+                    .map(criteria -> toRowExpression(criteria, functionResolution))\n+                    .forEach(filters::add);\n+            joinNode.getFilter().ifPresent(filters::add);\n+        }\n \n-            public Builder setAssignments(Assignments assignments)\n-            {\n-                this.assignments = assignments;\n-                return this;\n-            }\n+        MultiJoinNode toMultiJoinNode()\n+        {\n+            ImmutableSet<VariableReferenceExpression> inputVariables = sources.stream().flatMap(source -> source.getOutputVariables().stream()).collect(toImmutableSet());\n+\n+            // We could have some output variables that were possibly generated from intermediate assignments\n+            // For each of these variables, use the intermediate assignments to replace this variable with the set of input variables it uses\n+\n+            // Additionally, we build an overall set of assignments for the reordered Join node - this is used to add a wrapper Project over the updated output variables\n+            // We do this to satisfy the invariant that the rewritten Join node must produce the same output variables as the input Join node\n+            ImmutableSet.Builder<VariableReferenceExpression> updatedOutputVariables = ImmutableSet.builder();\n+            Assignments.Builder overallAssignments = Assignments.builder();\n+            boolean nonIdentityAssignmentsFound = false;\n+\n+            for (VariableReferenceExpression outputVariable : outputVariables) {\n+                if (inputVariables.contains(outputVariable)) {\n+                    overallAssignments.put(outputVariable, outputVariable);\n+                    updatedOutputVariables.add(outputVariable);\n+                    continue;\n+                }\n \n-            public Builder setOutputVariables(VariableReferenceExpression... outputVariables)\n-            {\n-                this.outputVariables = ImmutableList.copyOf(outputVariables);\n-                return this;\n+                checkState(intermediateAssignments.getMap().containsKey(outputVariable),\n+                        \"Output variable [%s] not found in input variables or in intermediate assignments\", outputVariable);\n+                nonIdentityAssignmentsFound = true;\n+                overallAssignments.put(outputVariable, intermediateAssignments.get(outputVariable));\n+                updatedOutputVariables.addAll(extractUnique(intermediateAssignments.get(outputVariable)));\n             }\n \n-            public MultiJoinNode build()\n-            {\n-                return new MultiJoinNode(new LinkedHashSet<>(sources), filter, outputVariables, assignments);\n-            }\n+            return new MultiJoinNode(sources,\n+                    and(filters),\n+                    updatedOutputVariables.build().asList(),\n+                    nonIdentityAssignmentsFound ? overallAssignments.build() : Assignments.of(), false, Optional.empty());\n         }\n     }\n \n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/GroupInnerJoinsByConnectorRuleSet.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/GroupInnerJoinsByConnectorRuleSet.java\nnew file mode 100644\nindex 0000000000000..ee9b57357a40c\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/optimizations/GroupInnerJoinsByConnectorRuleSet.java\n@@ -0,0 +1,746 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner.optimizations;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.function.OperatorType;\n+import com.facebook.presto.expressions.DefaultRowExpressionTraversalVisitor;\n+import com.facebook.presto.expressions.LogicalRowExpressions;\n+import com.facebook.presto.matching.Capture;\n+import com.facebook.presto.matching.Captures;\n+import com.facebook.presto.matching.Pattern;\n+import com.facebook.presto.metadata.FunctionAndTypeManager;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.ConnectorTableHandle;\n+import com.facebook.presto.spi.JoinTableInfo;\n+import com.facebook.presto.spi.JoinTableSet;\n+import com.facebook.presto.spi.TableHandle;\n+import com.facebook.presto.spi.VariableAllocator;\n+import com.facebook.presto.spi.plan.Assignments;\n+import com.facebook.presto.spi.plan.FilterNode;\n+import com.facebook.presto.spi.plan.JoinNode;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.PlanNodeIdAllocator;\n+import com.facebook.presto.spi.plan.ProjectNode;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.spi.relation.CallExpression;\n+import com.facebook.presto.spi.relation.DeterminismEvaluator;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.sql.planner.EqualityInference;\n+import com.facebook.presto.sql.planner.NullabilityAnalyzer;\n+import com.facebook.presto.sql.planner.TypeProvider;\n+import com.facebook.presto.sql.planner.iterative.GroupReference;\n+import com.facebook.presto.sql.planner.iterative.Lookup;\n+import com.facebook.presto.sql.planner.iterative.Rule;\n+import com.facebook.presto.sql.planner.plan.AssignmentUtils;\n+import com.facebook.presto.sql.planner.plan.MultiJoinNode;\n+import com.facebook.presto.sql.planner.plan.Patterns;\n+import com.facebook.presto.sql.relational.FunctionResolution;\n+import com.facebook.presto.sql.relational.RowExpressionDeterminismEvaluator;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Predicate;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.Sets;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.facebook.presto.SystemSessionProperties.INEQUALITY_JOIN_PUSHDOWN_ENABLED;\n+import static com.facebook.presto.SystemSessionProperties.isInnerJoinPushdownEnabled;\n+import static com.facebook.presto.common.function.OperatorType.GREATER_THAN;\n+import static com.facebook.presto.common.function.OperatorType.GREATER_THAN_OR_EQUAL;\n+import static com.facebook.presto.common.function.OperatorType.LESS_THAN;\n+import static com.facebook.presto.common.function.OperatorType.LESS_THAN_OR_EQUAL;\n+import static com.facebook.presto.common.function.OperatorType.NOT_EQUAL;\n+import static com.facebook.presto.expressions.LogicalRowExpressions.TRUE_CONSTANT;\n+import static com.facebook.presto.expressions.LogicalRowExpressions.and;\n+import static com.facebook.presto.expressions.LogicalRowExpressions.extractConjuncts;\n+import static com.facebook.presto.matching.Capture.newCapture;\n+import static com.facebook.presto.spi.connector.ConnectorCapabilities.SUPPORTS_JOIN_PUSHDOWN;\n+import static com.facebook.presto.spi.plan.JoinType.INNER;\n+import static com.facebook.presto.sql.planner.PlannerUtils.restrictOutput;\n+import static com.facebook.presto.sql.planner.VariablesExtractor.extractUnique;\n+import static com.facebook.presto.sql.planner.optimizations.JoinNodeUtils.toRowExpression;\n+import static com.facebook.presto.sql.planner.plan.Patterns.join;\n+import static com.facebook.presto.sql.planner.plan.Patterns.source;\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static com.google.common.collect.Iterables.filter;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * This optimizer attempts to group TableScanNode's of an inner-join graph that belong to the same connector\n+ * This allows those connectors that can participate in join pushdown, to rewrite these sources to a new TableScanNode that represents the result of the pushed down join\n+ * This re-written join graph has filter's pulled up, so filters need to be re pushed down again with the PredicatePushdown rule\n+ * This optimizer checks filter determinism before applying query optimizations, as non-deterministic filters can hinder filter pushdown leading to inconsistent results.\n+ * <p>\n+ * Example 1:\n+ * Before Transformation:\n+ * --OutputNode\n+ * `-- InnerJoin1\n+ * |-- InnerJoin2\n+ * |   |-- TableScanNode1\n+ * |   |   `-- TableHandle1\n+ * |   `-- TableScanNode2\n+ * |       `-- TableHandle2\n+ * `-- TableScanNode3\n+ * `-- TableHandle3\n+ * <p>\n+ * Suppose that TableScanNode1, TableScanNode2 and TableScanNode3 are from the same catalog.\n+ * <p>\n+ * After Transformation:\n+ * --OutputNode\n+ * `--FilterNode(pulled up equi-join clause + join filters)\n+ *  `-- TableScanNode (with all the details of the three TableScanNodes)\n+ *  `-- Set<ConnectorTableHandle> (ConnectorHandleSet)\n+ *     `-- TableHandle1\n+ *     `-- TableHandle2\n+ *     `-- TableHandle3\n+ * <p>\n+ * Example 2:\n+ * Before Transformation:\n+ * -- OutputNode\n+ * `-- InnerJoin1\n+ * |-- InnerJoin2\n+ * |   |-- TableScanNode1\n+ * |   |   `-- TableHandle1\n+ * |   `-- InnerJoin3\n+ * |       |-- TableScanNode2\n+ * |       |   `-- TableHandle2\n+ * |       `-- TableScanNode4\n+ * |           `-- TableHandle4\n+ * `-- TableScanNode3\n+ *     `-- TableHandle3\n+ * <p>\n+ * Suppose that TableScanNode1, TableScanNode2 and TableScanNode3 are from the same catalog, but TableScanNode4 is from another catalog.\n+ * <p>\n+ * After Transformation:\n+ * --OutputNode\n+ * `--FilterNode(pulled up equi-join clause + join filters)\n+ *  `-- CrossJoin\n+ *      |-- TableScanNode (with all the details of the three TableScanNodes)\n+ *      |   `-- Set<ConnectorTableHandle> (ConnectorHandleSet)\n+ *      |       |-- TableHandle1\n+ *      |       `-- TableHandle2\n+ *      |       `-- TableHandle3\n+ *      `-- TableScanNode4\n+ *          `-- TableHandle4\n+ */\n+\n+public class GroupInnerJoinsByConnectorRuleSet\n+{\n+    private final Metadata metadata;\n+    private final PlanOptimizer predicatePushdownOptimizer;\n+\n+    public GroupInnerJoinsByConnectorRuleSet(Metadata metadata, PlanOptimizer predicatePushdown)\n+    {\n+        this.metadata = metadata;\n+        this.predicatePushdownOptimizer = predicatePushdown;\n+    }\n+\n+    public Set<Rule<?>> rules()\n+    {\n+        return ImmutableSet.of(\n+                new OnlyJoinRule(metadata, predicatePushdownOptimizer),\n+                new FilterOnJoinRule(metadata, predicatePushdownOptimizer));\n+    }\n+\n+    public static class OnlyJoinRule\n+            extends BaseGroupInnerJoinsByConnector<JoinNode>\n+    {\n+        public OnlyJoinRule(Metadata metadata, PlanOptimizer predicatePushdownOptimizer)\n+        {\n+            super(metadata, predicatePushdownOptimizer);\n+        }\n+\n+        @Override\n+        public Pattern<JoinNode> getPattern()\n+        {\n+            return join().matching(\n+                    joinNode -> joinNode.getType() == INNER\n+                            && determinismEvaluator.isDeterministic(joinNode.getFilter().orElse(TRUE_CONSTANT)));\n+        }\n+\n+        @Override\n+        public Result apply(JoinNode node, Captures captures, Context context)\n+        {\n+            PlanNode rewrittenPlan = getCombinedJoin(node, functionResolution, determinismEvaluator, metadata, context);\n+\n+            if (rewrittenPlan.equals(node)) {\n+                return Result.empty();\n+            }\n+            else {\n+                return Result.ofPlanNode(rewrittenPlan);\n+            }\n+        }\n+    }\n+\n+    public static class FilterOnJoinRule\n+            extends BaseGroupInnerJoinsByConnector<FilterNode>\n+    {\n+        private static final Capture<JoinNode> JOIN = newCapture();\n+\n+        public FilterOnJoinRule(Metadata metadata, PlanOptimizer predicatePushdownOptimizer)\n+        {\n+            super(metadata, predicatePushdownOptimizer);\n+        }\n+\n+        @Override\n+        public Pattern<FilterNode> getPattern()\n+        {\n+            return Patterns.filter().with(source().matching(join().matching(\n+                    joinNode -> joinNode.getType() == INNER\n+                            && determinismEvaluator.isDeterministic(joinNode.getFilter().orElse(TRUE_CONSTANT))).capturedAs(JOIN)));\n+        }\n+\n+        @Override\n+        public Result apply(FilterNode filterNode, Captures captures, Context context)\n+        {\n+            JoinNode capturedJoinNode = captures.get(JOIN);\n+\n+            ImmutableList.Builder<RowExpression> predicates = ImmutableList.builder();\n+            predicates.add(filterNode.getPredicate()); // Add FilterNode's filter\n+            capturedJoinNode.getFilter().ifPresent(predicates::add);  // Combine with JoinNode's filter\n+\n+            JoinNode joinNode = new JoinNode(capturedJoinNode.getSourceLocation(),\n+                    context.getIdAllocator().getNextId(),\n+                    capturedJoinNode.getStatsEquivalentPlanNode(),\n+                    capturedJoinNode.getType(),\n+                    capturedJoinNode.getLeft(),\n+                    capturedJoinNode.getRight(),\n+                    capturedJoinNode.getCriteria(),\n+                    capturedJoinNode.getOutputVariables(),\n+                    Optional.of(LogicalRowExpressions.and(predicates.build())),\n+                    capturedJoinNode.getLeftHashVariable(),\n+                    capturedJoinNode.getRightHashVariable(),\n+                    capturedJoinNode.getDistributionType(),\n+                    capturedJoinNode.getDynamicFilters());\n+\n+            PlanNode rewrittenPlan = getCombinedJoin(joinNode, functionResolution, determinismEvaluator, metadata, context);\n+\n+            if (rewrittenPlan.equals(filterNode)) {\n+                return Result.empty();\n+            }\n+            else {\n+                return Result.ofPlanNode(rewrittenPlan);\n+            }\n+        }\n+    }\n+\n+    public abstract static class BaseGroupInnerJoinsByConnector<T>\n+            implements Rule<T>\n+    {\n+        final FunctionResolution functionResolution;\n+        final DeterminismEvaluator determinismEvaluator;\n+        final Metadata metadata;\n+        final NullabilityAnalyzer nullabilityAnalyzer;\n+        boolean isEnabledForTesting;\n+\n+        final FunctionAndTypeManager functionAndTypeManager;\n+        private final PlanOptimizer predicatePushdownOptimizer;\n+\n+        public BaseGroupInnerJoinsByConnector(Metadata metadata, PlanOptimizer predicatePushdownOptimizer)\n+        {\n+            this.functionResolution = new FunctionResolution(metadata.getFunctionAndTypeManager().getFunctionAndTypeResolver());\n+            this.determinismEvaluator = new RowExpressionDeterminismEvaluator(metadata.getFunctionAndTypeManager());\n+            this.metadata = metadata;\n+            this.functionAndTypeManager = metadata.getFunctionAndTypeManager();\n+            this.nullabilityAnalyzer = new NullabilityAnalyzer(functionAndTypeManager);\n+            this.predicatePushdownOptimizer = predicatePushdownOptimizer;\n+        }\n+\n+        @Override\n+        public boolean isEnabled(Session session)\n+        {\n+            return isEnabledForTesting || isInnerJoinPushdownEnabled(session);\n+        }\n+\n+        public void setEnabledForTesting(boolean isSet)\n+        {\n+            isEnabledForTesting = isSet;\n+        }\n+\n+        private static List<RowExpression> getExpressionsWithinVariableScope(Set<RowExpression> rowExpressions, Set<VariableReferenceExpression> variableScope)\n+        {\n+            return rowExpressions.stream()\n+                    .filter(rowExpression -> Sets.difference(extractUnique(rowExpression), variableScope).isEmpty())\n+                    .collect(toImmutableList());\n+        }\n+\n+        private static boolean isOperation(RowExpression expression, OperatorType type, FunctionAndTypeManager functionAndTypeManager)\n+        {\n+            if (expression instanceof CallExpression) {\n+                CallExpression call = (CallExpression) expression;\n+                Optional<OperatorType> expressionOperatorType = functionAndTypeManager.getFunctionMetadata(call.getFunctionHandle()).getOperatorType();\n+                if (expressionOperatorType.isPresent()) {\n+                    return expressionOperatorType.get() == type;\n+                }\n+            }\n+            return false;\n+        }\n+\n+        private static RowExpression getLeft(RowExpression expression)\n+        {\n+            checkArgument(expression instanceof CallExpression && ((CallExpression) expression).getArguments().size() == 2, \"must be binary call expression\");\n+            return ((CallExpression) expression).getArguments().get(0);\n+        }\n+\n+        private static RowExpression getRight(RowExpression expression)\n+        {\n+            checkArgument(expression instanceof CallExpression && ((CallExpression) expression).getArguments().size() == 2, \"must be binary call expression\");\n+            return ((CallExpression) expression).getArguments().get(1);\n+        }\n+\n+        private static Set<VariableReferenceExpression> extractVariableExpressions(RowExpression expression)\n+        {\n+            ImmutableSet.Builder<VariableReferenceExpression> builder = ImmutableSet.builder();\n+            expression.accept(new VariableReferenceBuilderVisitor(), builder);\n+            return builder.build();\n+        }\n+\n+        private static class VariableReferenceBuilderVisitor\n+                extends DefaultRowExpressionTraversalVisitor<ImmutableSet.Builder<VariableReferenceExpression>>\n+        {\n+            @Override\n+            public Void visitVariableReference(\n+                    VariableReferenceExpression variable,\n+                    ImmutableSet.Builder<VariableReferenceExpression> builder)\n+            {\n+                builder.add(variable);\n+                return null;\n+            }\n+        }\n+\n+        protected PlanNode getCombinedJoin(JoinNode node, FunctionResolution functionResolution, DeterminismEvaluator determinismEvaluator, Metadata metadata, Context context)\n+        {\n+            Lookup lookup = context.getLookup();\n+            Session session = context.getSession();\n+            PlanNodeIdAllocator idAllocator = context.getIdAllocator();\n+            VariableAllocator variableAllocator = context.getVariableAllocator();\n+\n+            MultiJoinNode groupInnerJoinsMultiJoinNode = new JoinNodeFlattener(node, functionResolution, determinismEvaluator, lookup).toMultiJoinNode();\n+            MultiJoinNode rewrittenMultiJoinNode = joinPushdownCombineSources(groupInnerJoinsMultiJoinNode, idAllocator, metadata, session, lookup);\n+            if (rewrittenMultiJoinNode.getContainsCombinedSources()) {\n+                // Create a left deep join tree\n+                PlanNode leftDeepJoinTree = createLeftDeepJoinTree(rewrittenMultiJoinNode, idAllocator);\n+                // Push pulled up predicates to re-form the Join conditions and remove CrossJoins\n+                return predicatePushdownOptimizer.optimize(\n+                        leftDeepJoinTree,\n+                        session,\n+                        TypeProvider.viewOf(variableAllocator.getVariables()),\n+                        variableAllocator,\n+                        idAllocator,\n+                        context.getWarningCollector()).getPlanNode();\n+            }\n+            return node;\n+        }\n+\n+        /**\n+         * Creates a left deep Join tree of CrossJoins, with a FilterNode at the top\n+         * The final result then needs a predicate pushdown / EliminateCrossJoins pass for the equality criteria to be set\n+         *\n+         * @param multiJoinNode\n+         * @param idAllocator\n+         * @return\n+         */\n+        private PlanNode createLeftDeepJoinTree(MultiJoinNode multiJoinNode, PlanNodeIdAllocator idAllocator)\n+        {\n+            PlanNode joinNode = createJoin(0, ImmutableList.copyOf(multiJoinNode.getSources()), idAllocator);\n+            RowExpression combinedFilters = and(multiJoinNode.getJoinFilter().get(), multiJoinNode.getFilter());\n+            FilterNode withFilters = new FilterNode(Optional.empty(), idAllocator.getNextId(), joinNode, combinedFilters);\n+            return restrictOutput(withFilters, idAllocator, multiJoinNode.getOutputVariables());\n+        }\n+\n+        private PlanNode createJoin(int index, List<PlanNode> sources, PlanNodeIdAllocator idAllocator)\n+        {\n+            if (index == sources.size() - 1) {\n+                return sources.get(index);\n+            }\n+\n+            PlanNode leftNode = createJoin(index + 1, sources, idAllocator);\n+            PlanNode rightNode = sources.get(index);\n+            return new JoinNode(\n+                    Optional.empty(),\n+                    idAllocator.getNextId(),\n+                    INNER,\n+                    leftNode,\n+                    rightNode,\n+                    ImmutableList.of(),\n+                    ImmutableList.<VariableReferenceExpression>builder()\n+                            .addAll(leftNode.getOutputVariables())\n+                            .addAll(rightNode.getOutputVariables())\n+                            .build(),\n+                    Optional.empty(),\n+                    Optional.empty(),\n+                    Optional.empty(),\n+                    Optional.empty(),\n+                    ImmutableMap.of());\n+        }\n+\n+        private MultiJoinNode joinPushdownCombineSources(MultiJoinNode multiJoinNode, PlanNodeIdAllocator idAllocator,\n+                                                         Metadata metadata, Session session, Lookup lookup)\n+        {\n+            LinkedHashSet<PlanNode> rewrittenSources = new LinkedHashSet<>();\n+            List<RowExpression> overallTableFilter = extractConjuncts(multiJoinNode.getFilter());\n+            Map<String, List<PlanNode>> sourcesByConnector = new HashMap<>();\n+            final boolean isInEqualityPushDownEnabled = session.getSystemProperty(INEQUALITY_JOIN_PUSHDOWN_ENABLED, Boolean.class);\n+\n+//          Here the join push down is happening based on  multiJoinNode.getJoinFilter() criteria.\n+//          JoinQueries that Inference presto to remove join criteria are not able to  push down.\n+//          Join push down should happen only for the tables which have valid join criteria in Presto flow [presto PlanNode]\n+//          For all join where no join criteria is treated as cross join, detailed discussion is available here https://github.ibm.com/lakehouse/tracker/issues/16482\n+\n+            EqualityInference filterEqualityInference = new EqualityInference.Builder(metadata)\n+                    .addEqualityInference(multiJoinNode.getJoinFilter().get())\n+                    .build();\n+            Iterable<RowExpression> inequalityPredicates = isInEqualityPushDownEnabled ? filter(extractConjuncts(multiJoinNode.getJoinFilter().get()), isInequalityInferenceCandidate()) : ImmutableSet.of();\n+            AtomicReference<Boolean> wereSourcesRewritten = new AtomicReference<>(false);\n+            Set<PlanNode> sources = multiJoinNode.getSources()\n+                    .stream().flatMap(planNode -> {\n+                        if (planNode instanceof GroupReference) {\n+                            return lookup.resolveGroup(planNode);\n+                        }\n+                        return Stream.of(planNode);\n+                    }).collect(Collectors.toCollection(LinkedHashSet::new));\n+            for (PlanNode source : sources) {\n+                Optional<String> connectorId = getConnectorIdFromSource(source, session, lookup);\n+                if (connectorId.isPresent()) {\n+                    // This source can be combined with other 'sources' of the same connector to produce a single TableScanNode\n+                    sourcesByConnector.computeIfAbsent(connectorId.get(), k -> new ArrayList<>());\n+                    sourcesByConnector.get(connectorId.get()).add(source);\n+                }\n+                else {\n+                    rewrittenSources.add(source);\n+                }\n+            }\n+\n+            sourcesByConnector.forEach(((connectorId, planNodes) -> {\n+                PlanNode newSource = getNewTableScanNode(planNodes, filterEqualityInference, inequalityPredicates, rewrittenSources, idAllocator, session);\n+                if (null != newSource) {\n+                    wereSourcesRewritten.set(true);\n+                    rewrittenSources.add(newSource);\n+                }\n+            }));\n+\n+            return new MultiJoinNode(\n+                    rewrittenSources,\n+                    and(overallTableFilter),\n+                    multiJoinNode.getOutputVariables(),\n+                    multiJoinNode.getAssignments(), wereSourcesRewritten.get(),\n+                    multiJoinNode.getJoinFilter());\n+        }\n+\n+        private Predicate<RowExpression> isInequalityInferenceCandidate()\n+        {\n+            return expression -> (isOperation(expression, GREATER_THAN_OR_EQUAL, functionAndTypeManager) ||\n+                    isOperation(expression, GREATER_THAN, functionAndTypeManager) ||\n+                    isOperation(expression, LESS_THAN_OR_EQUAL, functionAndTypeManager) ||\n+                    isOperation(expression, LESS_THAN, functionAndTypeManager) ||\n+                    isOperation(expression, NOT_EQUAL, functionAndTypeManager)) &&\n+                    determinismEvaluator.isDeterministic(expression) &&\n+                    !nullabilityAnalyzer.mayReturnNullOnNonNullInput(expression) &&\n+                    !getLeft(expression).equals(getRight(expression));\n+        }\n+\n+        private PlanNode getNewTableScanNode(List<PlanNode> groupedSources,\n+                                             EqualityInference filterEqualityInference,\n+                                             Iterable<RowExpression> inequalityPredicates, LinkedHashSet<PlanNode> rewrittenSources,\n+                                             PlanNodeIdAllocator idAllocator, Session session)\n+        {\n+            /*\n+             At present, we are not pushing down the ProjectNode if it is not an identity projection.\n+             All FilterNode is already handled and pushed into overall predicate and no FilterNode will reach here.\n+             All nodes except TableScanNode that resolved here (FilterNode, ProjectNode, JoinNode, etc.) need not for grouping and join push down\n+            */\n+            ImmutableList.Builder<PlanNode> nodesToCombineBuilder = ImmutableList.builder();\n+            ImmutableList.Builder<PlanNode> joinPushdownSourcesBuilder = ImmutableList.builder();\n+\n+            groupedSources.forEach(planNode -> {\n+                if (planNode instanceof TableScanNode) {\n+                    nodesToCombineBuilder.add(planNode);\n+                }\n+                else {\n+                    rewrittenSources.add(planNode);\n+                }\n+            });\n+            List<PlanNode> nodesToCombine = nodesToCombineBuilder.build();\n+            if (nodesToCombine.isEmpty()) {\n+                return null;\n+            }\n+            // Build combined output variables\n+            Set<VariableReferenceExpression> combinedOutputVariables = nodesToCombine.stream()\n+                    .flatMap(o -> o.getOutputVariables().stream())\n+                    .collect(Collectors.toSet());\n+\n+            List<RowExpression> equiJoinFilters = filterEqualityInference.generateEqualitiesPartitionedBy(combinedOutputVariables::contains)\n+                    .getScopeEqualities();\n+            Set<RowExpression> inequalityPredicateSet = StreamSupport.stream(inequalityPredicates.spliterator(), false)\n+                    .collect(Collectors.toSet());\n+            Map<VariableReferenceExpression, ColumnHandle> groupAssignments = nodesToCombine.stream().map(this::getTableScanNode).map(TableScanNode::getAssignments)\n+                    .flatMap(map -> map.entrySet().stream()) // Flatten the maps into a stream of entries\n+                    .collect(Collectors.toMap(\n+                            entry -> entry.getKey(),\n+                            entry -> entry.getValue()));\n+\n+            Set<ConnectorTableHandle> nodeHandles = new HashSet<>();\n+            TableHandle firstResolvedTableHandle = null;\n+            Set<JoinTableInfo> joinTableInfos = new HashSet<>();\n+            for (PlanNode planNode : nodesToCombine) {\n+                TableHandle resolvedTableHandle = getTableScanNode(planNode).getTable();\n+                if (firstResolvedTableHandle == null) {\n+                    firstResolvedTableHandle = resolvedTableHandle;\n+                }\n+                ConnectorTableHandle connectorHandle = resolvedTableHandle.getConnectorHandle();\n+                nodeHandles.add(connectorHandle);\n+                joinTableInfos.add(new JoinTableInfo(connectorHandle, getTableScanNode(planNode).getAssignments(), planNode.getOutputVariables()));\n+            }\n+            JoinTableSet combinedTableHandles = new JoinTableSet(joinTableInfos);\n+            TableHandle combinedTableHandle = new TableHandle(firstResolvedTableHandle.getConnectorId(),\n+                    combinedTableHandles,\n+                    firstResolvedTableHandle.getTransaction(),\n+                    firstResolvedTableHandle.getLayout());\n+\n+            List<RowExpression> translatableJoinFilters = new ArrayList<>();\n+            for (RowExpression filter : equiJoinFilters) {\n+                if (metadata.isPushdownSupportedForFilter(session, combinedTableHandle, filter, groupAssignments)) {\n+                    translatableJoinFilters.add(filter);\n+                }\n+            }\n+\n+            List<RowExpression> scopedInequalities = getExpressionsWithinVariableScope(inequalityPredicateSet, combinedOutputVariables);\n+            for (RowExpression nonEquiFilter : scopedInequalities) {\n+                if (metadata.isPushdownSupportedForFilter(session, combinedTableHandle, nonEquiFilter, groupAssignments)) {\n+                    translatableJoinFilters.add(nonEquiFilter);\n+                }\n+            }\n+\n+            RowExpression joinFilters = and(translatableJoinFilters);\n+            Set<VariableReferenceExpression> referredVariables = extractVariableExpressions(joinFilters);\n+\n+            if (referredVariables.isEmpty()) {\n+                rewrittenSources.addAll(nodesToCombine);\n+            }\n+\n+            nodesToCombine.forEach(node -> {\n+                if (node.getOutputVariables().stream().anyMatch(referredVariables::contains)) {\n+                    // At least one of the output variables of this node was involved in join with another source\n+                    // So there is a valid JOIN with one of the other sources\n+                    joinPushdownSourcesBuilder.add(node);\n+                }\n+                else {\n+                    rewrittenSources.add(node);\n+                }\n+            });\n+            List<PlanNode> joinPushdownSources = joinPushdownSourcesBuilder.build();\n+\n+            // At least two source required for Join\n+            if (joinPushdownSources.isEmpty()) {\n+                return null;\n+            }\n+            else if (joinPushdownSources.size() == 1) {\n+                rewrittenSources.add(joinPushdownSources.get(0));\n+                return null;\n+            }\n+\n+            /*\n+             At this point we should have\n+             1. All the table references that belong to the same connector that need to be combined\n+             2. All the predicates that refer to these tables\n+             3. A list of overall output variables\n+             We can now build our new TableScanNode which represents the join pushed down tables and the final TableScanNode could create at connector level\n+            */\n+            return buildSingleTableScan(joinPushdownSources, idAllocator);\n+        }\n+\n+        private PlanNode buildSingleTableScan(List<PlanNode> groupNodes, PlanNodeIdAllocator idAllocator)\n+        {\n+            // Build a set of individual TableHandles that need to be combined\n+            TableHandle firstResolvedTableHandle = null;\n+            TableScanNode firstResolvedTableScanNode = null;\n+            ImmutableSet.Builder<JoinTableInfo> builder = ImmutableSet.builder();\n+            // Get over all output variables and assignments from grouped TableScanNode\n+            List<VariableReferenceExpression> outputVariables = new ArrayList<>();\n+            Map<VariableReferenceExpression, ColumnHandle> assignments = new HashMap<>();\n+            for (PlanNode groupNode : groupNodes) {\n+                TableScanNode tableScanNode = getTableScanNode(groupNode);\n+                TableHandle tableHandle = tableScanNode.getTable();\n+                if (firstResolvedTableHandle == null) {\n+                    firstResolvedTableHandle = tableHandle;\n+                    firstResolvedTableScanNode = tableScanNode;\n+                }\n+\n+                outputVariables.addAll(tableScanNode.getOutputVariables());\n+                assignments.putAll(tableScanNode.getAssignments());\n+                builder.add(new JoinTableInfo(tableHandle.getConnectorHandle(), tableScanNode.getAssignments(), tableScanNode.getOutputVariables()));\n+            }\n+\n+            // Build a new TableHandle that represents the combined set of TableHandles\n+            TableHandle updatedTableHandle = new TableHandle(firstResolvedTableHandle.getConnectorId(),\n+                    new JoinTableSet(builder.build()),\n+                    firstResolvedTableHandle.getTransaction(),\n+                    firstResolvedTableHandle.getLayout(),\n+                    firstResolvedTableHandle.getDynamicFilter());\n+\n+            return new TableScanNode(Optional.empty(),\n+                    idAllocator.getNextId(),\n+                    updatedTableHandle,\n+                    outputVariables,\n+                    assignments,\n+                    firstResolvedTableScanNode.getCurrentConstraint(),\n+                    firstResolvedTableScanNode.getEnforcedConstraint(),\n+                    firstResolvedTableScanNode.getCteMaterializationInfo());\n+        }\n+\n+        private TableScanNode getTableScanNode(PlanNode planNode)\n+        {\n+            while (!(planNode instanceof TableScanNode)) {\n+                planNode = planNode.getSources().get(0);\n+            }\n+            return (TableScanNode) planNode;\n+        }\n+\n+        /**\n+         * For a join source, see if we can resolve it to TableScanNode and if it resolves to TableScanNode\n+         * then get it connector and check connector capabilities for join push down.\n+         * This will only happen iff the parent hierarchy only contains {Project, Filter, TableScanNode}'s as the parent's\n+         *\n+         * @param resolved\n+         * @param session\n+         * @param lookup\n+         * @return Optional<String>\n+         */\n+        private Optional<String> getConnectorIdFromSource(PlanNode resolved, Session session, Lookup lookup)\n+        {\n+            if (resolved instanceof GroupReference) {\n+                return getConnectorIdFromSource(lookup.resolve(resolved), session, lookup);\n+            }\n+            if (resolved instanceof ProjectNode) {\n+                return getConnectorIdFromSource(((ProjectNode) resolved).getSource(), session, lookup);\n+            }\n+            if (resolved instanceof FilterNode) {\n+                return getConnectorIdFromSource(((FilterNode) resolved).getSource(), session, lookup);\n+            }\n+            if (resolved instanceof TableScanNode) {\n+                TableScanNode ts = (TableScanNode) resolved;\n+                ConnectorId connectorId = ts.getTable().getConnectorId();\n+                boolean supportsJoinPushDown = metadata.getConnectorCapabilities(session, connectorId).contains(SUPPORTS_JOIN_PUSHDOWN);\n+                if (supportsJoinPushDown) {\n+                    return Optional.of(connectorId.toString());\n+                }\n+            }\n+            return Optional.empty();\n+        }\n+\n+        @VisibleForTesting\n+        private static class JoinNodeFlattener\n+        {\n+            private final LinkedHashSet<PlanNode> sources = new LinkedHashSet<>();\n+            private List<RowExpression> joinCriteriaFilters = new ArrayList<>();\n+            private List<RowExpression> filters = new ArrayList<>();\n+            private final List<VariableReferenceExpression> outputVariables;\n+            private final FunctionResolution functionResolution;\n+            private final DeterminismEvaluator determinismEvaluator;\n+            private final boolean connectorJoinNode = false;\n+\n+            JoinNodeFlattener(PlanNode node, FunctionResolution functionResolution, DeterminismEvaluator determinismEvaluator, Lookup lookup)\n+            {\n+                requireNonNull(node, \"node is null\");\n+                this.outputVariables = node.getOutputVariables();\n+                this.functionResolution = requireNonNull(functionResolution, \"functionResolution is null\");\n+                this.determinismEvaluator = requireNonNull(determinismEvaluator, \"determinismEvaluator is null\");\n+\n+                flattenNode(node, lookup);\n+            }\n+\n+            private void flattenNode(PlanNode resolved, Lookup lookup)\n+            {\n+                PlanNode resolvedNode = lookup.resolve(resolved);\n+                if (resolvedNode instanceof FilterNode) {\n+                /*\n+                    We pull up all Filters to the top of the join graph, these will be pushed down again by predicate pushdown\n+                    We do this in hope of surfacing any TableScan nodes that can be combined\n+                */\n+                    FilterNode filterNode = (FilterNode) resolvedNode;\n+                    filters.add(filterNode.getPredicate());\n+                    flattenNode(filterNode.getSource(), lookup);\n+                    return;\n+                }\n+\n+                if (!(resolvedNode instanceof JoinNode)) {\n+                    if (resolvedNode instanceof ProjectNode) {\n+                        /*\n+                            Certain ProjectNodes can be 'inlined' into the parent TableScan, e.g a CAST expression\n+                            We will do this here while flattening the JoinNode if possible\n+                            For now, we log the fact that we saw a ProjectNode and if identity projection, will continue\n+                        */\n+                        // Only identity projections can be handled.\n+                        if (AssignmentUtils.isIdentity(((ProjectNode) resolvedNode).getAssignments())) {\n+                            flattenNode(((ProjectNode) resolvedNode).getSource(), lookup);\n+                            return;\n+                        }\n+                    }\n+                    sources.add(resolvedNode);\n+                    return;\n+                }\n+                JoinNode joinNode = (JoinNode) resolvedNode;\n+                if (joinNode.getType() != INNER || !determinismEvaluator.isDeterministic(joinNode.getFilter().orElse(TRUE_CONSTANT))) {\n+                    sources.add(resolvedNode);\n+                    return;\n+                }\n+\n+                flattenNode(joinNode.getLeft(), lookup);\n+                flattenNode(joinNode.getRight(), lookup);\n+                joinNode.getCriteria().stream()\n+                        .map(criteria -> toRowExpression(criteria, functionResolution))\n+                        .forEach(joinCriteriaFilters::add);\n+                joinNode.getFilter().ifPresent(joinCriteriaFilters::add);\n+            }\n+\n+            MultiJoinNode toMultiJoinNode()\n+            {\n+                ImmutableSet<VariableReferenceExpression> inputVariables = sources.stream().flatMap(source -> source.getOutputVariables().stream()).collect(toImmutableSet());\n+                /*\n+                    We do this to satisfy the invariant that the rewritten Join node must produce the same output variables as the input Join node\n+                */\n+                ImmutableSet.Builder<VariableReferenceExpression> updatedOutputVariables = ImmutableSet.builder();\n+\n+                for (VariableReferenceExpression outputVariable : outputVariables) {\n+                    if (inputVariables.contains(outputVariable)) {\n+                        updatedOutputVariables.add(outputVariable);\n+                    }\n+                }\n+\n+                return new MultiJoinNode(sources,\n+                        and(filters),\n+                        updatedOutputVariables.build().asList(), Assignments.of(), connectorJoinNode, Optional.of(and(joinCriteriaFilters)));\n+            }\n+        }\n+    }\n+}\n\ndiff --git a/presto-main-base/src/main/java/com/facebook/presto/sql/planner/plan/MultiJoinNode.java b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/plan/MultiJoinNode.java\nnew file mode 100644\nindex 0000000000000..22fdc4f8af564\n--- /dev/null\n+++ b/presto-main-base/src/main/java/com/facebook/presto/sql/planner/plan/MultiJoinNode.java\n@@ -0,0 +1,180 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner.plan;\n+\n+import com.facebook.presto.spi.plan.Assignments;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.sql.planner.CanonicalJoinNode;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+import static com.facebook.presto.expressions.LogicalRowExpressions.extractConjuncts;\n+import static com.facebook.presto.spi.plan.JoinType.INNER;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * This class represents a set of inner joins that can be executed in any order.\n+ */\n+public class MultiJoinNode\n+{\n+    // Use a linked hash set to ensure optimizer is deterministic\n+    protected CanonicalJoinNode node;\n+    protected Assignments assignments;\n+    private final boolean containsCombinedSources;\n+    private final Optional<RowExpression> joinFilter;\n+\n+    public MultiJoinNode(LinkedHashSet<PlanNode> sources, RowExpression filter, List<VariableReferenceExpression> outputVariables,\n+            Assignments assignments, boolean containsCombinedSources, Optional<RowExpression> joinFilter)\n+    {\n+        requireNonNull(sources, \"sources is null\");\n+        requireNonNull(filter, \"filter is null\");\n+        requireNonNull(outputVariables, \"outputVariables is null\");\n+        requireNonNull(assignments, \"assignments is null\");\n+\n+        this.assignments = assignments;\n+        // Plan node id doesn't matter here as we don't use this in planner\n+        this.node = new CanonicalJoinNode(\n+                new PlanNodeId(\"\"),\n+                sources.stream().collect(toImmutableList()),\n+                INNER,\n+                ImmutableSet.of(),\n+                ImmutableSet.of(filter),\n+                outputVariables);\n+        this.containsCombinedSources = containsCombinedSources;\n+        this.joinFilter = joinFilter;\n+    }\n+\n+    public RowExpression getFilter()\n+    {\n+        return node.getFilters().stream().findAny().get();\n+    }\n+\n+    public LinkedHashSet<PlanNode> getSources()\n+    {\n+        return new LinkedHashSet<>(node.getSources());\n+    }\n+\n+    public List<VariableReferenceExpression> getOutputVariables()\n+    {\n+        return node.getOutputVariables();\n+    }\n+\n+    public Assignments getAssignments()\n+    {\n+        return assignments;\n+    }\n+\n+    public Optional<RowExpression> getJoinFilter()\n+    {\n+        return joinFilter;\n+    }\n+\n+    public boolean getContainsCombinedSources()\n+    {\n+        return containsCombinedSources;\n+    }\n+\n+    public static Builder builder()\n+    {\n+        return new Builder();\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(getSources(), ImmutableSet.copyOf(extractConjuncts(getFilter())), getOutputVariables());\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (!(obj instanceof MultiJoinNode)) {\n+            return false;\n+        }\n+\n+        MultiJoinNode other = (MultiJoinNode) obj;\n+        return getSources().equals(other.getSources())\n+                && ImmutableSet.copyOf(extractConjuncts(getFilter())).equals(ImmutableSet.copyOf(extractConjuncts(other.getFilter())))\n+                && getOutputVariables().equals(other.getOutputVariables())\n+                && getAssignments().equals(other.getAssignments());\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return \"MultiJoinNode{\" +\n+                \"node=\" + node +\n+                \", assignments=\" + assignments +\n+                '}';\n+    }\n+\n+    public static class Builder\n+    {\n+        private List<PlanNode> sources;\n+        private RowExpression filter;\n+        private List<VariableReferenceExpression> outputVariables;\n+        private Assignments assignments = Assignments.of();\n+        private boolean containsCombinedSources;\n+        private Optional<RowExpression> joinFilter;\n+\n+        public MultiJoinNode.Builder setSources(PlanNode... sources)\n+        {\n+            this.sources = ImmutableList.copyOf(sources);\n+            return this;\n+        }\n+\n+        public MultiJoinNode.Builder setFilter(RowExpression filter)\n+        {\n+            this.filter = filter;\n+            return this;\n+        }\n+\n+        public MultiJoinNode.Builder setAssignments(Assignments assignments)\n+        {\n+            this.assignments = assignments;\n+            return this;\n+        }\n+\n+        public MultiJoinNode.Builder setOutputVariables(VariableReferenceExpression... outputVariables)\n+        {\n+            this.outputVariables = ImmutableList.copyOf(outputVariables);\n+            return this;\n+        }\n+\n+        public MultiJoinNode.Builder setContainsCombinedSources(boolean containsCombinedSources)\n+        {\n+            this.containsCombinedSources = containsCombinedSources;\n+            return this;\n+        }\n+\n+        public MultiJoinNode.Builder setJoinFilter(Optional<RowExpression> joinFilter)\n+        {\n+            this.joinFilter = joinFilter;\n+            return this;\n+        }\n+        public MultiJoinNode build()\n+        {\n+            return new MultiJoinNode(new LinkedHashSet<>(sources), filter, outputVariables, assignments, containsCombinedSources, joinFilter);\n+        }\n+    }\n+}\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableInfo.java b/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableInfo.java\nnew file mode 100644\nindex 0000000000000..9ad08b01d4e60\n--- /dev/null\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableInfo.java\n@@ -0,0 +1,71 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spi;\n+\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+public class JoinTableInfo\n+{\n+    private final ConnectorTableHandle tableHandle;\n+    private final Map<VariableReferenceExpression, ColumnHandle> assignments;\n+    private final List<VariableReferenceExpression> outputVariables;\n+\n+    public JoinTableInfo(ConnectorTableHandle tableHandle, Map<VariableReferenceExpression, ColumnHandle> assignments, List<VariableReferenceExpression> outputVariables)\n+    {\n+        this.tableHandle = tableHandle;\n+        this.assignments = Collections.unmodifiableMap(assignments);\n+        this.outputVariables = Collections.unmodifiableList(outputVariables);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if ((obj == null) || (getClass() != obj.getClass())) {\n+            return false;\n+        }\n+        JoinTableInfo o = (JoinTableInfo) obj;\n+        return Objects.equals(this.tableHandle, o.tableHandle)\n+                && Objects.equals(this.assignments, o.assignments)\n+                && Objects.equals(this.outputVariables, o.outputVariables);\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(tableHandle, assignments, outputVariables);\n+    }\n+\n+    public ConnectorTableHandle getTableHandle()\n+    {\n+        return tableHandle;\n+    }\n+\n+    public Map<VariableReferenceExpression, ColumnHandle> getAssignments()\n+    {\n+        return assignments;\n+    }\n+\n+    public List<VariableReferenceExpression> getOutputVariables()\n+    {\n+        return outputVariables;\n+    }\n+}\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableSet.java b/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableSet.java\nnew file mode 100644\nindex 0000000000000..a1ed24c2ac69f\n--- /dev/null\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/JoinTableSet.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spi;\n+\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class JoinTableSet\n+        implements ConnectorTableHandle\n+{\n+    private final Set<JoinTableInfo> innerJoinTableInfos;\n+\n+    public JoinTableSet(Set<JoinTableInfo> innerJoinTableInfos)\n+    {\n+        this.innerJoinTableInfos = innerJoinTableInfos;\n+    }\n+\n+    public Set<JoinTableInfo> getInnerJoinTableInfos()\n+    {\n+        return innerJoinTableInfos;\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if ((obj == null) || (getClass() != obj.getClass())) {\n+            return false;\n+        }\n+        JoinTableSet o = (JoinTableSet) obj;\n+        return Objects.equals(this.innerJoinTableInfos, o.innerJoinTableInfos);\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(innerJoinTableInfos);\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return innerJoinTableInfos.stream()\n+            .map(info -> info != null ? info.toString() : \"null\")\n+            .collect(Collectors.joining(\":\"));\n+    }\n+}\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorCapabilities.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorCapabilities.java\nindex cec4890310175..a4739cb002533 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorCapabilities.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorCapabilities.java\n@@ -21,5 +21,6 @@ public enum ConnectorCapabilities\n     PRIMARY_KEY_CONSTRAINT,\n     UNIQUE_CONSTRAINT,\n     ENFORCE_CONSTRAINTS,\n-    ALTER_COLUMN\n+    ALTER_COLUMN,\n+    SUPPORTS_JOIN_PUSHDOWN\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\nindex 4f2aef245af2e..29cead181b037 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n@@ -41,6 +41,8 @@\n import com.facebook.presto.spi.TableLayoutFilterCoverage;\n import com.facebook.presto.spi.api.Experimental;\n import com.facebook.presto.spi.constraints.TableConstraint;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.spi.security.GrantInfo;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n@@ -851,4 +853,18 @@ default void addConstraint(ConnectorSession session, ConnectorTableHandle tableH\n     {\n         throw new PrestoException(NOT_SUPPORTED, \"This connector does not support adding table constraints\");\n     }\n+\n+    /**\n+     * Check if pushdown is supported for the given filter against columns of the table handle\n+     *\n+     * @param session\n+     * @param tableHandle\n+     * @param filter\n+     * @param symbolToColumnHandleMap\n+     * @return\n+     */\n+    default boolean isPushdownSupportedForFilter(ConnectorSession session, ConnectorTableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+    {\n+        return false;\n+    }\n }\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\nindex e33b7d708a79c..89798fb5406b3 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n@@ -45,6 +45,8 @@\n import com.facebook.presto.spi.connector.ConnectorPartitioningMetadata;\n import com.facebook.presto.spi.connector.ConnectorTableVersion;\n import com.facebook.presto.spi.constraints.TableConstraint;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.spi.security.GrantInfo;\n import com.facebook.presto.spi.security.PrestoPrincipal;\n import com.facebook.presto.spi.security.Privilege;\n@@ -792,4 +794,12 @@ public void addConstraint(ConnectorSession session, ConnectorTableHandle tableHa\n             delegate.addConstraint(session, tableHandle, tableConstraint);\n         }\n     }\n+\n+    @Override\n+    public boolean isPushdownSupportedForFilter(ConnectorSession session, ConnectorTableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+    {\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n+            return delegate.isPushdownSupportedForFilter(session, tableHandle, filter, symbolToColumnHandleMap);\n+        }\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java b/presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java\nindex f369996db2c3c..43c26a2447636 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/connector/MockConnectorFactory.java\n@@ -27,12 +27,15 @@\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.SchemaTablePrefix;\n import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.connector.ConnectorCapabilities;\n import com.facebook.presto.spi.connector.ConnectorContext;\n import com.facebook.presto.spi.connector.ConnectorFactory;\n import com.facebook.presto.spi.connector.ConnectorMetadata;\n import com.facebook.presto.spi.connector.ConnectorRecordSetProvider;\n import com.facebook.presto.spi.connector.ConnectorSplitManager;\n import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.spi.statistics.TableStatistics;\n import com.facebook.presto.spi.transaction.IsolationLevel;\n import com.facebook.presto.tpch.TpchColumnHandle;\n@@ -41,6 +44,7 @@\n import com.facebook.presto.tpch.TpchSplitManager;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n \n import java.util.List;\n import java.util.Map;\n@@ -52,6 +56,7 @@\n import java.util.stream.IntStream;\n \n import static com.facebook.presto.common.type.VarcharType.createUnboundedVarcharType;\n+import static com.facebook.presto.spi.connector.ConnectorCapabilities.SUPPORTS_JOIN_PUSHDOWN;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n import static java.util.Objects.requireNonNull;\n@@ -64,19 +69,22 @@ public class MockConnectorFactory\n     private final BiFunction<ConnectorSession, SchemaTablePrefix, Map<SchemaTableName, ConnectorViewDefinition>> getViews;\n     private final BiFunction<ConnectorSession, ConnectorTableHandle, Map<String, TpchColumnHandle>> getColumnHandles;\n     private final Supplier<TableStatistics> getTableStatistics;\n+    private final Set<ConnectorCapabilities> connectorCapabilities;\n \n     private MockConnectorFactory(\n             Function<ConnectorSession, List<String>> listSchemaNames,\n             BiFunction<ConnectorSession, String, List<SchemaTableName>> listTables,\n             BiFunction<ConnectorSession, SchemaTablePrefix, Map<SchemaTableName, ConnectorViewDefinition>> getViews,\n             BiFunction<ConnectorSession, ConnectorTableHandle, Map<String, TpchColumnHandle>> getColumnHandles,\n-            Supplier<TableStatistics> getTableStatistics)\n+            Supplier<TableStatistics> getTableStatistics,\n+            Set<ConnectorCapabilities> connectorCapabilities)\n     {\n         this.listSchemaNames = requireNonNull(listSchemaNames, \"listSchemaNames is null\");\n         this.listTables = requireNonNull(listTables, \"listTables is null\");\n         this.getViews = requireNonNull(getViews, \"getViews is null\");\n         this.getColumnHandles = requireNonNull(getColumnHandles, \"getColumnHandles is null\");\n         this.getTableStatistics = requireNonNull(getTableStatistics, \"getTableStatistics is null\");\n+        this.connectorCapabilities = requireNonNull(connectorCapabilities, \"connectorCapabilities is null\");\n     }\n \n     @Override\n@@ -94,7 +102,7 @@ public ConnectorHandleResolver getHandleResolver()\n     @Override\n     public Connector create(String catalogName, Map<String, String> config, ConnectorContext context)\n     {\n-        return new MockConnector(context, listSchemaNames, listTables, getViews, getColumnHandles, getTableStatistics);\n+        return new MockConnector(context, listSchemaNames, listTables, getViews, getColumnHandles, getTableStatistics, connectorCapabilities);\n     }\n \n     public static Builder builder()\n@@ -111,6 +119,7 @@ private static class MockConnector\n         private final BiFunction<ConnectorSession, SchemaTablePrefix, Map<SchemaTableName, ConnectorViewDefinition>> getViews;\n         private final BiFunction<ConnectorSession, ConnectorTableHandle, Map<String, TpchColumnHandle>> getColumnHandles;\n         private final Supplier<TableStatistics> getTableStatistics;\n+        private final Set<ConnectorCapabilities> connectorCapabilities;\n \n         public MockConnector(\n                 ConnectorContext context,\n@@ -118,7 +127,8 @@ public MockConnector(\n                 BiFunction<ConnectorSession, String, List<SchemaTableName>> listTables,\n                 BiFunction<ConnectorSession, SchemaTablePrefix, Map<SchemaTableName, ConnectorViewDefinition>> getViews,\n                 BiFunction<ConnectorSession, ConnectorTableHandle, Map<String, TpchColumnHandle>> getColumnHandles,\n-                Supplier<TableStatistics> getTableStatistics)\n+                Supplier<TableStatistics> getTableStatistics,\n+                Set<ConnectorCapabilities> connectorCapabilities)\n         {\n             this.context = requireNonNull(context, \"context is null\");\n             this.listSchemaNames = requireNonNull(listSchemaNames, \"listSchemaNames is null\");\n@@ -126,6 +136,7 @@ public MockConnector(\n             this.getViews = requireNonNull(getViews, \"getViews is null\");\n             this.getColumnHandles = requireNonNull(getColumnHandles, \"getColumnHandles is null\");\n             this.getTableStatistics = requireNonNull(getTableStatistics, \"getTableStatistics is null\");\n+            this.connectorCapabilities = requireNonNull(connectorCapabilities, \"connectorCapabilities is null\");\n         }\n \n         @Override\n@@ -152,6 +163,12 @@ public ConnectorRecordSetProvider getRecordSetProvider()\n             return new TpchRecordSetProvider();\n         }\n \n+        @Override\n+        public Set<ConnectorCapabilities> getCapabilities()\n+        {\n+            return this.connectorCapabilities;\n+        }\n+\n         private class MockConnectorMetadata\n                 implements ConnectorMetadata\n         {\n@@ -236,6 +253,12 @@ public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTab\n             {\n                 return getTableStatistics.get();\n             }\n+\n+            @Override\n+            public boolean isPushdownSupportedForFilter(ConnectorSession session, ConnectorTableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+            {\n+                return connectorCapabilities.contains(SUPPORTS_JOIN_PUSHDOWN);\n+            }\n         }\n     }\n \n@@ -246,6 +269,7 @@ public static final class Builder\n         private BiFunction<ConnectorSession, SchemaTablePrefix, Map<SchemaTableName, ConnectorViewDefinition>> getViews = (session, schemaTablePrefix) -> ImmutableMap.of();\n         private BiFunction<ConnectorSession, ConnectorTableHandle, Map<String, TpchColumnHandle>> getColumnHandles = (session, tableHandle) -> notSupported();\n         private Supplier<TableStatistics> getTableStatistics = TableStatistics::empty;\n+        private Set<ConnectorCapabilities> connectorCapabilities = ImmutableSet.of();\n \n         public Builder withListSchemaNames(Function<ConnectorSession, List<String>> listSchemaNames)\n         {\n@@ -253,6 +277,12 @@ public Builder withListSchemaNames(Function<ConnectorSession, List<String>> list\n             return this;\n         }\n \n+        public Builder withConnectorCapabilities(Set<ConnectorCapabilities> connectorCapabilities)\n+        {\n+            this.connectorCapabilities = connectorCapabilities;\n+            return this;\n+        }\n+\n         public Builder withListTables(BiFunction<ConnectorSession, String, List<SchemaTableName>> listTables)\n         {\n             this.listTables = requireNonNull(listTables, \"listTables is null\");\n@@ -279,7 +309,7 @@ public Builder withGetTableStatistics(Supplier<TableStatistics> getTableStatisti\n \n         public MockConnectorFactory build()\n         {\n-            return new MockConnectorFactory(listSchemaNames, listTables, getViews, getColumnHandles, getTableStatistics);\n+            return new MockConnectorFactory(listSchemaNames, listTables, getViews, getColumnHandles, getTableStatistics, connectorCapabilities);\n         }\n \n         private static <T> T notSupported()\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java b/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\nindex 7a3fb541c3ab1..da186146eb2d3 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n@@ -255,7 +255,10 @@ public void testDefaults()\n                 .setEnhancedCTESchedulingEnabled(true)\n                 .setExpressionOptimizerName(\"default\")\n                 .setExcludeInvalidWorkerSessionProperties(false)\n-                .setAddExchangeBelowPartialAggregationOverGroupId(false));\n+                .setAddExchangeBelowPartialAggregationOverGroupId(false)\n+                .setInnerJoinPushdownEnabled(false)\n+                .setInEqualityJoinPushdownEnabled(false)\n+                .setPrestoSparkExecutionEnvironment(false));\n     }\n \n     @Test\n@@ -452,6 +455,8 @@ public void testExplicitPropertyMappings()\n                 .put(\"optimizer.include-values-node-in-connector-optimizer\", \"false\")\n                 .put(\"eager-plan-validation-enabled\", \"true\")\n                 .put(\"eager-plan-validation-thread-pool-size\", \"2\")\n+                .put(\"optimizer.inner-join-pushdown-enabled\", \"true\")\n+                .put(\"optimizer.inequality-join-pushdown-enabled\", \"true\")\n                 .put(\"presto-spark-execution-environment\", \"true\")\n                 .put(\"single-node-execution-enabled\", \"true\")\n                 .put(\"native-execution-scale-writer-threads-enabled\", \"true\")\n@@ -659,7 +664,10 @@ public void testExplicitPropertyMappings()\n                 .setEnhancedCTESchedulingEnabled(false)\n                 .setExpressionOptimizerName(\"custom\")\n                 .setExcludeInvalidWorkerSessionProperties(true)\n-                .setAddExchangeBelowPartialAggregationOverGroupId(true);\n+                .setAddExchangeBelowPartialAggregationOverGroupId(true)\n+                .setInEqualityJoinPushdownEnabled(true)\n+                .setInnerJoinPushdownEnabled(true)\n+                .setPrestoSparkExecutionEnvironment(true);\n         assertFullMapping(properties, expected);\n     }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestGroupInnerJoinsByConnectorRuleSet.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestGroupInnerJoinsByConnectorRuleSet.java\nnew file mode 100644\nindex 0000000000000..3e3fbf6089d8d\n--- /dev/null\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestGroupInnerJoinsByConnectorRuleSet.java\n@@ -0,0 +1,493 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner.iterative.rule;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.cost.StatsProvider;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.ConnectorTableHandle;\n+import com.facebook.presto.spi.JoinTableInfo;\n+import com.facebook.presto.spi.JoinTableSet;\n+import com.facebook.presto.spi.NodeManager;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.spi.TableHandle;\n+import com.facebook.presto.spi.TestingColumnHandle;\n+import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.connector.ConnectorCapabilities;\n+import com.facebook.presto.spi.connector.ConnectorContext;\n+import com.facebook.presto.spi.connector.ConnectorFactory;\n+import com.facebook.presto.spi.connector.ConnectorMetadata;\n+import com.facebook.presto.spi.connector.ConnectorNodePartitioningProvider;\n+import com.facebook.presto.spi.connector.ConnectorRecordSetProvider;\n+import com.facebook.presto.spi.connector.ConnectorSplitManager;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import com.facebook.presto.spi.plan.EquiJoinClause;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.PlanNodeIdAllocator;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.spi.transaction.IsolationLevel;\n+import com.facebook.presto.sql.TestingRowExpressionTranslator;\n+import com.facebook.presto.sql.planner.TypeProvider;\n+import com.facebook.presto.sql.planner.assertions.MatchResult;\n+import com.facebook.presto.sql.planner.assertions.Matcher;\n+import com.facebook.presto.sql.planner.assertions.PlanMatchPattern;\n+import com.facebook.presto.sql.planner.assertions.SymbolAliases;\n+import com.facebook.presto.sql.planner.iterative.rule.test.PlanBuilder;\n+import com.facebook.presto.sql.planner.iterative.rule.test.RuleAssert;\n+import com.facebook.presto.sql.planner.iterative.rule.test.RuleTester;\n+import com.facebook.presto.sql.planner.optimizations.GroupInnerJoinsByConnectorRuleSet;\n+import com.facebook.presto.sql.planner.optimizations.PlanOptimizerResult;\n+import com.facebook.presto.sql.tree.SymbolReference;\n+import com.facebook.presto.testing.LocalQueryRunner;\n+import com.facebook.presto.testing.TestingMetadata;\n+import com.facebook.presto.testing.TestingTransactionHandle;\n+import com.facebook.presto.tpch.ColumnNaming;\n+import com.facebook.presto.tpch.TpchConnectorFactory;\n+import com.facebook.presto.tpch.TpchMetadata;\n+import com.facebook.presto.tpch.TpchNodePartitioningProvider;\n+import com.facebook.presto.tpch.TpchRecordSetProvider;\n+import com.facebook.presto.tpch.TpchSplitManager;\n+import com.facebook.presto.tpch.TpchTransactionHandle;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static com.facebook.airlift.testing.Closeables.closeAllRuntimeException;\n+import static com.facebook.presto.SessionTestUtils.TEST_SESSION;\n+import static com.facebook.presto.SystemSessionProperties.INEQUALITY_JOIN_PUSHDOWN_ENABLED;\n+import static com.facebook.presto.SystemSessionProperties.INNER_JOIN_PUSHDOWN_ENABLED;\n+import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.metadata.SessionPropertyManager.createTestingSessionPropertyManager;\n+import static com.facebook.presto.spi.connector.ConnectorCapabilities.SUPPORTS_JOIN_PUSHDOWN;\n+import static com.facebook.presto.spi.plan.JoinType.FULL;\n+import static com.facebook.presto.spi.plan.JoinType.INNER;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.filter;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.join;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.node;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.project;\n+import static com.facebook.presto.sql.planner.iterative.rule.test.PlanBuilder.expression;\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toMap;\n+\n+public class TestGroupInnerJoinsByConnectorRuleSet\n+{\n+    public static final String CATALOG_SUPPORTING_JOIN_PUSHDOWN = \"catalog_join_pushdown_supported\";\n+    public static final String LOCAL = \"local\";\n+    public static final String TEST_SCHEMA = \"test-schema\";\n+    public static final String TEST_TABLE = \"test-table\";\n+    public static final String OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN = \"other_catalog_join_pushdown_supported\";\n+    private PlanBuilder planBuilder;\n+    private RuleTester tester;\n+\n+    @BeforeClass\n+    public void setUp()\n+    {\n+        LocalQueryRunner runner = new LocalQueryRunner(TEST_SESSION);\n+        ConnectorFactory pushdownConnectorFactory = new TestingJoinPushdownConnectorFactory()\n+        {\n+            @Override\n+            public String getName()\n+            {\n+                return \"tpch_with_join_pushdown\";\n+            }\n+        };\n+        ConnectorFactory pushdownConnectorFactory1 = new TestingJoinPushdownConnectorFactory()\n+        {\n+            @Override\n+            public String getName()\n+            {\n+                return \"tpch_with_join_pushdown1\";\n+            }\n+        };\n+        runner.createCatalog(CATALOG_SUPPORTING_JOIN_PUSHDOWN, pushdownConnectorFactory, ImmutableMap.of());\n+        runner.createCatalog(OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN, pushdownConnectorFactory1, ImmutableMap.of());\n+\n+        tester = new RuleTester(\n+                ImmutableList.of(),\n+                RuleTester.getSession(ImmutableMap.of(INNER_JOIN_PUSHDOWN_ENABLED, \"true\", INEQUALITY_JOIN_PUSHDOWN_ENABLED, \"true\"), createTestingSessionPropertyManager()),\n+                runner,\n+                new TpchConnectorFactory(1));\n+\n+        planBuilder = new PlanBuilder(TEST_SESSION, new PlanNodeIdAllocator(), runner.getMetadata());\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        closeAllRuntimeException(tester);\n+        tester = null;\n+    }\n+\n+    @Test\n+    public void testDoesNotPushDownOuterJoin()\n+    {\n+        String connectorName = \"test_catalog\";\n+\n+        assertGroupInnerJoinsByConnectorRuleSet()\n+                .on(p ->\n+                        p.join(\n+                                FULL,\n+                                tableScan(connectorName, \"a1\", \"b1\"),\n+                                tableScan(connectorName, \"a2\", \"b2\"),\n+                                new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"a2\"))))\n+                .doesNotFire();\n+    }\n+\n+    @Test\n+    public void testDPartialPushDownTwoDifferentConnectors()\n+    {\n+        Set<JoinTableInfo> joinTableInfos = new HashSet<>();\n+\n+        JoinTableInfo joinTableInfo1 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"a1\"), new TestingColumnHandle(\"a1\"), newBigintVariable(\"a2\"),\n+                        new TestingColumnHandle(\"a2\")), ImmutableList.of(newBigintVariable(\"a1\"), newBigintVariable(\"a2\")));\n+        JoinTableInfo joinTableInfo2 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"c1\"), new TestingColumnHandle(\"c1\"), newBigintVariable(\"c2\"),\n+                        new TestingColumnHandle(\"c2\")), ImmutableList.of(newBigintVariable(\"c1\"), newBigintVariable(\"c2\")));\n+        joinTableInfos.add(joinTableInfo1);\n+        joinTableInfos.add(joinTableInfo2);\n+        JoinTableSet tableHandleSet = new JoinTableSet(joinTableInfos);\n+        TableHandle tableHandle1 = new TableHandle(\n+                new ConnectorId(CATALOG_SUPPORTING_JOIN_PUSHDOWN),\n+                tableHandleSet,\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+        TableHandle tableHandle2 = new TableHandle(\n+                new ConnectorId(LOCAL),\n+                new TestingMetadata.TestingTableHandle(),\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+        assertGroupInnerJoinsByConnectorRuleSet()\n+                .on(p ->\n+                        p.join(\n+                                INNER,\n+                                p.join(\n+                                        INNER,\n+                                        tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a1\", \"a2\"),\n+                                        tableScan(LOCAL, \"b1\", \"b2\"),\n+                                        new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"b1\"))),\n+                                tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"c1\", \"c2\"),\n+                                new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"c1\"))))\n+                .matches(\n+                        project(\n+                                filter(\n+                                \"a1 = b1 and a1 = c1 and true\",\n+                                        join(\n+                                                JoinTableScanMatcher.tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, tableHandle1, \"a1\", \"a2\", \"c1\", \"c2\"),\n+                                                JoinTableScanMatcher.tableScan(LOCAL, tableHandle2, \"b1\", \"b2\")))));\n+    }\n+\n+    @Test\n+    public void testValidPushdownForSameConnector()\n+    {\n+        Set<JoinTableInfo> joinTableInfos = new HashSet<>();\n+\n+        JoinTableInfo joinTableInfo1 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"b1\"), new TestingColumnHandle(\"b1\"), newBigintVariable(\"a1\"),\n+                        new TestingColumnHandle(\"a1\")), ImmutableList.of(newBigintVariable(\"a1\"), newBigintVariable(\"b1\")));\n+        JoinTableInfo joinTableInfo2 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"b2\"), new TestingColumnHandle(\"b2\"), newBigintVariable(\"a2\"),\n+                        new TestingColumnHandle(\"a2\")), ImmutableList.of(newBigintVariable(\"a2\"), newBigintVariable(\"b2\")));\n+        joinTableInfos.add(joinTableInfo1);\n+        joinTableInfos.add(joinTableInfo2);\n+        JoinTableSet tableHandleSet = new JoinTableSet(joinTableInfos);\n+        TableHandle tableHandle = new TableHandle(\n+                new ConnectorId(CATALOG_SUPPORTING_JOIN_PUSHDOWN),\n+                tableHandleSet,\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+\n+        assertGroupInnerJoinsByConnectorRuleSet()\n+                .on(p ->\n+                        p.join(\n+                                INNER,\n+                                tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a1\", \"b1\"),\n+                                tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a2\", \"b2\"),\n+                                new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"a2\")))\n+                ).matches(\n+                        project(\n+                                filter(\n+                                        \"a1 = a2 and true\",\n+                                        JoinTableScanMatcher.tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, tableHandle, \"a1\", \"a2\"))));\n+    }\n+\n+    @Test\n+    public void testJoinPushDownHappenedWithFilters()\n+    {\n+        Set<JoinTableInfo> joinTableInfos = new HashSet<>();\n+\n+        JoinTableInfo joinTableInfo1 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"a1\"), new TestingColumnHandle(\"a1\"), newBigintVariable(\"b1\"),\n+                        new TestingColumnHandle(\"b1\")), ImmutableList.of(newBigintVariable(\"a1\"), newBigintVariable(\"b1\")));\n+        JoinTableInfo joinTableInfo2 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(newBigintVariable(\"a2\"), new TestingColumnHandle(\"a2\"), newBigintVariable(\"b2\"),\n+                        new TestingColumnHandle(\"b2\")), ImmutableList.of(newBigintVariable(\"a2\"), newBigintVariable(\"b2\")));\n+        joinTableInfos.add(joinTableInfo1);\n+        joinTableInfos.add(joinTableInfo2);\n+        JoinTableSet tableHandleSet = new JoinTableSet(joinTableInfos);\n+        TableHandle tableHandle = new TableHandle(\n+                new ConnectorId(CATALOG_SUPPORTING_JOIN_PUSHDOWN),\n+                tableHandleSet,\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+\n+        String expression = \"a1 > b1\";\n+        TypeProvider typeProvider = TypeProvider.copyOf(ImmutableMap.of(\"a1\", BIGINT, \"b1\", BIGINT));\n+        TestingRowExpressionTranslator sqlToRowExpressionTranslator = new TestingRowExpressionTranslator(tester.getMetadata());\n+        RowExpression rowExpression = sqlToRowExpressionTranslator.translateAndOptimize(expression(expression), typeProvider);\n+\n+        assertGroupInnerJoinsByConnectorRuleSet()\n+                .on(p ->\n+                        p.join(\n+                                INNER,\n+                                tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a1\", \"b1\"),\n+                                tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a2\", \"b2\"),\n+                                rowExpression,\n+                                new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"a2\"))))\n+                .matches(\n+                        project(\n+                                filter(\n+                        \"a1 = a2 and a1 > b1 and true\",\n+                                        JoinTableScanMatcher.tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, tableHandle, \"a1\", \"a2\", \"b1\"))));\n+    }\n+\n+    @Test\n+    public void testPushDownWithTwoDifferentConnectors()\n+    {\n+        Set<JoinTableInfo> joinTableSet1 = new HashSet<>();\n+\n+        JoinTableInfo tableInfo1 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(\n+                        newBigintVariable(\"a1\"), new TestingColumnHandle(\"a1\"),\n+                        newBigintVariable(\"a2\"), new TestingColumnHandle(\"a2\")), ImmutableList.of(newBigintVariable(\"a1\"), newBigintVariable(\"a2\")));\n+        JoinTableInfo tableInfo2 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(\n+                        newBigintVariable(\"b1\"), new TestingColumnHandle(\"b1\"),\n+                        newBigintVariable(\"b2\"), new TestingColumnHandle(\"b2\")), ImmutableList.of(newBigintVariable(\"b1\"), newBigintVariable(\"b2\")));\n+\n+        joinTableSet1.add(tableInfo1);\n+        joinTableSet1.add(tableInfo2);\n+        JoinTableSet tableHandleSet1 = new JoinTableSet(joinTableSet1);\n+        TableHandle tableHandle1 = new TableHandle(\n+                new ConnectorId(CATALOG_SUPPORTING_JOIN_PUSHDOWN),\n+                tableHandleSet1,\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+\n+        Set<JoinTableInfo> joinTableSet2 = new HashSet<>();\n+\n+        JoinTableInfo table2Info1 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(\n+                        newBigintVariable(\"c1\"), new TestingColumnHandle(\"c1\"),\n+                        newBigintVariable(\"c2\"), new TestingColumnHandle(\"c2\")), ImmutableList.of(newBigintVariable(\"c1\"), newBigintVariable(\"c2\")));\n+        JoinTableInfo table2Info2 = new JoinTableInfo(new TestingMetadata.TestingTableHandle(new SchemaTableName(TEST_SCHEMA, TEST_TABLE)),\n+                ImmutableMap.of(\n+                        newBigintVariable(\"d1\"), new TestingColumnHandle(\"d1\"),\n+                        newBigintVariable(\"d2\"), new TestingColumnHandle(\"d2\")), ImmutableList.of(newBigintVariable(\"d1\"), newBigintVariable(\"d2\")));\n+\n+        joinTableSet2.add(table2Info1);\n+        joinTableSet2.add(table2Info2);\n+        JoinTableSet tableHandleSet2 = new JoinTableSet(joinTableSet2);\n+        TableHandle tableHandle2 = new TableHandle(\n+                new ConnectorId(OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN),\n+                tableHandleSet2,\n+                TestingTransactionHandle.create(),\n+                Optional.empty());\n+\n+        assertGroupInnerJoinsByConnectorRuleSet()\n+                .on(p ->\n+                        p.join(\n+                                INNER,\n+                                p.join(\n+                                        INNER,\n+                                        tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"a1\", \"a2\"),\n+                                        tableScan(OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"d1\", \"d2\"),\n+                                        new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"b1\")),\n+                                        new EquiJoinClause(newBigintVariable(\"a1\"), newBigintVariable(\"d1\"))),\n+                                p.join(\n+                                        INNER,\n+                                        tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"b1\", \"b2\"),\n+                                        tableScan(OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN, \"c1\", \"c2\"),\n+                                        new EquiJoinClause(newBigintVariable(\"b1\"), newBigintVariable(\"c1\"))),\n+                                        new EquiJoinClause(newBigintVariable(\"c1\"), newBigintVariable(\"d1\"))))\n+                .matches(\n+                        project(\n+                                filter(\n+                        \"((a1 = b1 and a1 = d1) and (b1 = c1 and c1 = d1)) and true\",\n+                                        join(\n+                                                JoinTableScanMatcher.tableScan(CATALOG_SUPPORTING_JOIN_PUSHDOWN, tableHandle1, \"a1\", \"b1\"),\n+                                                JoinTableScanMatcher.tableScan(OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN, tableHandle2, \"c1\", \"d1\")))));\n+    }\n+\n+    private RuleAssert assertGroupInnerJoinsByConnectorRuleSet()\n+    {\n+        // For testing, we do not wish to push down pulled up predicates\n+        return tester.assertThat(new GroupInnerJoinsByConnectorRuleSet.OnlyJoinRule(tester.getMetadata(),\n+                (plan, session, types, variableAllocator, idAllocator, warningCollector) ->\n+                        PlanOptimizerResult.optimizerResult(plan, false)),\n+                ImmutableList.of(CATALOG_SUPPORTING_JOIN_PUSHDOWN, OTHER_CATALOG_SUPPORTING_JOIN_PUSHDOWN));\n+    }\n+\n+    private TableScanNode tableScan(String connectorName, String... columnNames)\n+    {\n+        return planBuilder.tableScan(\n+                connectorName,\n+                Arrays.stream(columnNames).map(TestGroupInnerJoinsByConnectorRuleSet::newBigintVariable).collect(toImmutableList()),\n+                Arrays.stream(columnNames)\n+                        .map(TestGroupInnerJoinsByConnectorRuleSet::newBigintVariable)\n+                        .collect(Collectors.toMap(identity(),\n+                                variable -> new TestingColumnHandle(variable.getName()))));\n+    }\n+\n+    private static VariableReferenceExpression newBigintVariable(String name)\n+    {\n+        return new VariableReferenceExpression(Optional.empty(), name, BIGINT);\n+    }\n+\n+    private static class TestingJoinPushdownConnectorFactory\n+            extends TpchConnectorFactory\n+    {\n+        @Override\n+        public Connector create(String catalogName, Map<String, String> properties, ConnectorContext context)\n+        {\n+            int splitsPerNode = super.getSplitsPerNode(properties);\n+            ColumnNaming columnNaming = ColumnNaming.valueOf(properties.getOrDefault(TPCH_COLUMN_NAMING_PROPERTY, ColumnNaming.SIMPLIFIED.name()).toUpperCase());\n+            NodeManager nodeManager = context.getNodeManager();\n+\n+            return new Connector()\n+            {\n+                @Override\n+                public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel, boolean readOnly)\n+                {\n+                    return TpchTransactionHandle.INSTANCE;\n+                }\n+\n+                @Override\n+                public ConnectorMetadata getMetadata(ConnectorTransactionHandle transaction)\n+                {\n+                    return new TpchMetadata(catalogName, columnNaming, isPredicatePushdownEnabled(), isPartitioningEnabled(properties))\n+                    {\n+                        @Override\n+                        public boolean isPushdownSupportedForFilter(ConnectorSession session, ConnectorTableHandle tableHandle, RowExpression filter, Map<VariableReferenceExpression, ColumnHandle> symbolToColumnHandleMap)\n+                        {\n+                            return true;\n+                        }\n+                    };\n+                }\n+\n+                @Override\n+                public ConnectorSplitManager getSplitManager()\n+                {\n+                    return new TpchSplitManager(nodeManager, splitsPerNode);\n+                }\n+\n+                @Override\n+                public ConnectorRecordSetProvider getRecordSetProvider()\n+                {\n+                    return new TpchRecordSetProvider();\n+                }\n+\n+                @Override\n+                public ConnectorNodePartitioningProvider getNodePartitioningProvider()\n+                {\n+                    return new TpchNodePartitioningProvider(nodeManager, splitsPerNode);\n+                }\n+\n+                @Override\n+                public Set<ConnectorCapabilities> getCapabilities()\n+                {\n+                    return ImmutableSet.of(SUPPORTS_JOIN_PUSHDOWN);\n+                }\n+            };\n+        }\n+    }\n+\n+    private static final class JoinTableScanMatcher\n+            implements Matcher\n+    {\n+        private final ConnectorId connectorId;\n+        private final TableHandle tableHandle;\n+        private final String[] columns;\n+\n+        public static PlanMatchPattern tableScan(String connectorName, TableHandle tableHandle, String... columnNames)\n+        {\n+            return node(TableScanNode.class)\n+                    .with(new JoinTableScanMatcher(\n+                            new ConnectorId(connectorName),\n+                            tableHandle,\n+                            columnNames));\n+        }\n+\n+        private JoinTableScanMatcher(\n+                ConnectorId connectorId,\n+                TableHandle tableHandle,\n+                String... columns)\n+        {\n+            this.connectorId = connectorId;\n+            this.tableHandle = tableHandle;\n+            this.columns = columns;\n+        }\n+\n+        @Override\n+        public boolean shapeMatches(PlanNode node)\n+        {\n+            return node instanceof TableScanNode;\n+        }\n+\n+        @Override\n+        public MatchResult detailMatches(PlanNode node, StatsProvider stats, Session session, Metadata metadata, SymbolAliases symbolAliases)\n+        {\n+            checkState(shapeMatches(node), \"Plan testing framework error: shapeMatches returned false in detailMatches in %s\", this.getClass().getName());\n+            TableScanNode tableScanNode = (TableScanNode) node;\n+            TableHandle otherTable = tableScanNode.getTable();\n+            ConnectorTableHandle connectorHandle = otherTable.getConnectorHandle();\n+\n+            if (connectorId.equals(otherTable.getConnectorId()) && Objects.equals(otherTable.getConnectorId(), this.tableHandle.getConnectorId()) &&\n+                            Objects.equals(otherTable.getConnectorHandle(), this.tableHandle.getConnectorHandle()) &&\n+                            Objects.equals(otherTable.getLayout().isPresent(), this.tableHandle.getLayout().isPresent())) {\n+                return MatchResult.match(SymbolAliases.builder().putAll(Arrays.stream(columns).collect(toMap(identity(), SymbolReference::new))).build());\n+            }\n+\n+            return MatchResult.NO_MATCH;\n+        }\n+\n+        @Override\n+        public String toString()\n+        {\n+            return toStringHelper(this)\n+                    .omitNullValues()\n+                    .add(\"connectorId\", connectorId)\n+                    .toString();\n+        }\n+    }\n+}\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java\nindex c6f67a02c2a05..bfb271cc1fcdc 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinEnumerator.java\n@@ -41,8 +41,8 @@\n import com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.JoinEnumerationResult;\n import com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.JoinEnumerator;\n import com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.JoinEnumerator.JoinCondition;\n-import com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.MultiJoinNode;\n import com.facebook.presto.sql.planner.iterative.rule.test.PlanBuilder;\n+import com.facebook.presto.sql.planner.plan.MultiJoinNode;\n import com.facebook.presto.sql.relational.FunctionResolution;\n import com.facebook.presto.sql.relational.RowExpressionDeterminismEvaluator;\n import com.facebook.presto.sql.tree.SymbolReference;\n@@ -139,7 +139,9 @@ public void testDoesNotCreateJoinWhenPartitionedOnCrossJoin()\n                 new LinkedHashSet<>(ImmutableList.of(p.values(a1), p.values(b1))),\n                 TRUE_CONSTANT,\n                 ImmutableList.of(a1, b1),\n-                Assignments.of());\n+                Assignments.of(),\n+                false,\n+                Optional.empty());\n         JoinEnumerator joinEnumerator = new JoinEnumerator(\n                 new CostComparator(1, 1, 1),\n                 multiJoinNode.getFilter(),\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java\nindex f693b69e90667..80b4619304fb9 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestJoinNodeFlattener.java\n@@ -26,8 +26,8 @@\n import com.facebook.presto.spi.relation.RowExpression;\n import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.sql.analyzer.FunctionAndTypeResolver;\n-import com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.MultiJoinNode;\n import com.facebook.presto.sql.planner.iterative.rule.test.PlanBuilder;\n+import com.facebook.presto.sql.planner.plan.MultiJoinNode;\n import com.facebook.presto.sql.relational.FunctionResolution;\n import com.facebook.presto.sql.relational.RowExpressionDeterminismEvaluator;\n import com.facebook.presto.testing.LocalQueryRunner;\n@@ -50,7 +50,7 @@\n import static com.facebook.presto.spi.plan.JoinType.LEFT;\n import static com.facebook.presto.sql.analyzer.TypeSignatureProvider.fromTypes;\n import static com.facebook.presto.sql.planner.iterative.Lookup.noLookup;\n-import static com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.MultiJoinNode.toMultiJoinNode;\n+import static com.facebook.presto.sql.planner.iterative.rule.ReorderJoins.toMultiJoinNode;\n import static com.facebook.presto.sql.relational.Expressions.call;\n import static com.facebook.presto.sql.relational.Expressions.constant;\n import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n@@ -215,7 +215,9 @@ public void testCombinesCriteriaAndFilters()\n                 new LinkedHashSet<>(ImmutableList.of(valuesA, valuesB, valuesC)),\n                 and(createEqualsExpression(b1, c1), createEqualsExpression(a1, b1), bcFilter, abcFilter),\n                 ImmutableList.of(a1, b1, b2, c1, c2),\n-                Assignments.builder().build());\n+                Assignments.builder().build(),\n+                false,\n+                Optional.empty());\n         assertEquals(toMultiJoinNode(joinNode, noLookup(), DEFAULT_JOIN_LIMIT, false, functionResolution, determinismEvaluator), expected);\n     }\n \n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java\nindex 8d8f9e28fb5f8..8bfca678a940d 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleAssert.java\n@@ -43,9 +43,11 @@\n import com.facebook.presto.sql.planner.iterative.properties.LogicalPropertiesProviderImpl;\n import com.facebook.presto.sql.relational.FunctionResolution;\n import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n \n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.function.Function;\n@@ -68,6 +70,7 @@ public class RuleAssert\n     private final PlanNodeIdAllocator idAllocator = new PlanNodeIdAllocator();\n     private final TransactionManager transactionManager;\n     private final AccessControl accessControl;\n+    private final List<String> extraCatalogs;\n \n     private Session session;\n     private TypeProvider types;\n@@ -76,11 +79,11 @@ public class RuleAssert\n \n     public RuleAssert(Metadata metadata, StatsCalculator statsCalculator, CostCalculator costCalculator, Session session, Rule rule, TransactionManager transactionManager, AccessControl accessControl)\n     {\n-        this(metadata, statsCalculator, costCalculator, session, rule, transactionManager, accessControl, Optional.empty());\n+        this(metadata, statsCalculator, costCalculator, session, rule, transactionManager, accessControl, Optional.empty(), ImmutableList.of());\n     }\n \n     public RuleAssert(Metadata metadata, StatsCalculator statsCalculator, CostCalculator costCalculator, Session session, Rule rule,\n-            TransactionManager transactionManager, AccessControl accessControl, Optional<LogicalPropertiesProvider> logicalPropertiesProvider)\n+                      TransactionManager transactionManager, AccessControl accessControl, Optional<LogicalPropertiesProvider> logicalPropertiesProvider, List<String> extraCatalogs)\n     {\n         this.metadata = requireNonNull(metadata, \"metadata is null\");\n         this.statsCalculator = new TestingStatsCalculator(requireNonNull(statsCalculator, \"statsCalculator is null\"));\n@@ -90,6 +93,7 @@ public RuleAssert(Metadata metadata, StatsCalculator statsCalculator, CostCalcul\n         this.transactionManager = requireNonNull(transactionManager, \"transactionManager is null\");\n         this.accessControl = requireNonNull(accessControl, \"accessControl is null\");\n         this.logicalPropertiesProvider = requireNonNull(logicalPropertiesProvider, \"logicalPropertiesProvider is null\");\n+        this.extraCatalogs = requireNonNull(extraCatalogs, \"extraCatalogs is null\");\n     }\n \n     public RuleAssert setSystemProperty(String key, String value)\n@@ -250,6 +254,7 @@ private <T> T inTransaction(Function<Session, T> transactionSessionConsumer)\n                 .execute(session, session -> {\n                     // metadata.getCatalogHandle() registers the catalog for the transaction\n                     session.getCatalog().ifPresent(catalog -> metadata.getCatalogHandle(session, catalog));\n+                    extraCatalogs.forEach(catalog -> metadata.getCatalogHandle(session, catalog));\n                     return transactionSessionConsumer.apply(session);\n                 });\n     }\n\ndiff --git a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java\nindex 33d46ff6b885e..4ed53b74cbc05 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/RuleTester.java\n@@ -36,6 +36,7 @@\n import com.facebook.presto.testing.LocalQueryRunner;\n import com.facebook.presto.tpch.TpchConnectorFactory;\n import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n \n import java.io.Closeable;\n@@ -91,20 +92,23 @@ public RuleTester(List<Plugin> plugins, Map<String, String> sessionProperties, O\n \n     public RuleTester(List<Plugin> plugins, Map<String, String> sessionProperties, SessionPropertyManager sessionPropertyManager, Optional<Integer> nodeCountForStats, ConnectorFactory connectorFactory)\n     {\n-        Session.SessionBuilder sessionBuilder = testSessionBuilder(sessionPropertyManager)\n-                .setCatalog(CATALOG_ID)\n-                .setSchema(\"tiny\")\n-                .setSystemProperty(\"task_concurrency\", \"1\"); // these tests don't handle exchanges from local parallel\n+        this(plugins, getSession(sessionProperties, sessionPropertyManager), nodeCountForStats, connectorFactory);\n+    }\n \n-        for (Map.Entry<String, String> entry : sessionProperties.entrySet()) {\n-            sessionBuilder.setSystemProperty(entry.getKey(), entry.getValue());\n-        }\n+    public RuleTester(List<Plugin> plugins, Session session, Optional<Integer> nodeCountForStats, ConnectorFactory connectorFactory)\n+    {\n+        this(plugins,\n+                session,\n+                nodeCountForStats.map(nodeCount -> LocalQueryRunner.queryRunnerWithFakeNodeCountForStats(session, nodeCount))\n+                        .orElseGet(() -> new LocalQueryRunner(session)),\n+                connectorFactory);\n+    }\n \n-        session = sessionBuilder.build();\n+    public RuleTester(List<Plugin> plugins, Session session, LocalQueryRunner preCreatedRunner, ConnectorFactory connectorFactory)\n+    {\n+        this.session = session;\n+        queryRunner = preCreatedRunner;\n \n-        queryRunner = nodeCountForStats\n-                .map(nodeCount -> LocalQueryRunner.queryRunnerWithFakeNodeCountForStats(session, nodeCount))\n-                .orElseGet(() -> new LocalQueryRunner(session));\n         queryRunner.createCatalog(session.getCatalog().get(),\n                 connectorFactory,\n                 ImmutableMap.of());\n@@ -119,6 +123,20 @@ public RuleTester(List<Plugin> plugins, Map<String, String> sessionProperties, S\n         this.sqlParser = queryRunner.getSqlParser();\n     }\n \n+    public static Session getSession(Map<String, String> sessionProperties, SessionPropertyManager sessionPropertyManager)\n+    {\n+        Session.SessionBuilder sessionBuilder = testSessionBuilder(sessionPropertyManager)\n+                .setCatalog(CATALOG_ID)\n+                .setSchema(\"tiny\")\n+                .setSystemProperty(\"task_concurrency\", \"1\"); // these tests don't handle exchanges from local parallel\n+\n+        for (Map.Entry<String, String> entry : sessionProperties.entrySet()) {\n+            sessionBuilder.setSystemProperty(entry.getKey(), entry.getValue());\n+        }\n+\n+        return sessionBuilder.build();\n+    }\n+\n     public RuleAssert assertThat(Rule rule)\n     {\n         return new RuleAssert(metadata, queryRunner.getStatsCalculator(), queryRunner.getEstimatedExchangesCostCalculator(), session, rule, transactionManager, accessControl);\n@@ -126,7 +144,12 @@ public RuleAssert assertThat(Rule rule)\n \n     public RuleAssert assertThat(Rule rule, LogicalPropertiesProvider logicalPropertiesProvider)\n     {\n-        return new RuleAssert(metadata, queryRunner.getStatsCalculator(), queryRunner.getEstimatedExchangesCostCalculator(), session, rule, transactionManager, accessControl, Optional.of(logicalPropertiesProvider));\n+        return new RuleAssert(metadata, queryRunner.getStatsCalculator(), queryRunner.getEstimatedExchangesCostCalculator(), session, rule, transactionManager, accessControl, Optional.of(logicalPropertiesProvider), ImmutableList.of());\n+    }\n+\n+    public RuleAssert assertThat(Rule rule, List<String> extraCatalogs)\n+    {\n+        return new RuleAssert(metadata, queryRunner.getStatsCalculator(), queryRunner.getEstimatedExchangesCostCalculator(), session, rule, transactionManager, accessControl, Optional.empty(), extraCatalogs);\n     }\n \n     public OptimizerAssert assertThat(Set<Rule<?>> rules)\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java\nindex 98739db46c920..e9f361f019dd3 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestMetadataManager.java\n@@ -31,6 +31,7 @@\n import com.facebook.presto.transaction.TransactionBuilder;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n import org.intellij.lang.annotations.Language;\n import org.testng.annotations.AfterClass;\n import org.testng.annotations.BeforeClass;\n@@ -82,6 +83,7 @@ public Iterable<ConnectorFactory> getConnectorFactories()\n                         .withGetTableStatistics(() -> {\n                             throw new UnsupportedOperationException();\n                         })\n+                        .withConnectorCapabilities(ImmutableSet.of())\n                         .build();\n                 return ImmutableList.of(connectorFactory);\n             }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24103",
    "pr_id": 24103,
    "issue_id": 24055,
    "repo": "prestodb/presto",
    "problem_statement": "width_bucket() does not seem to process NULL in the array elements properly.\nThese return 0:\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[NULL]) t(c0);\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[NULL, 1]) t(c0);\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[1, NULL, 4]) t(c0);\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[NULL, 1, NULL, 4]) t(c0);\r\n\r\nThese return 1:\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[NULL]) t(c0);\r\n\r\nThese return 2:\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[NULL, 1]) t(c0);\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[1, NULL, 4]) t(c0);\r\n\r\nThese return 3:\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[NULL, 1, NULL, 4]) t(c0);\r\n\r\nThese fail with \"Bin values are not sorted in ascending order\":\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[1, NULL]) t(c0);\r\nSELECT width_bucket(-1, c0) from (VALUES ARRAY[1, NULL, 4, NULL]) t(c0);\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[1, NULL]) t(c0);\r\nSELECT width_bucket(1, c0) from (VALUES ARRAY[1, NULL, 4, NULL]) t(c0);\r\n\r\nAnd so on.\r\nThere is no check for null elements in the code and unclear how the function should behave if it encounters one.\r\nFrom strict perspective it seems like the function should fail whenever it finds a null element because \"Bin values are not sorted in ascending order\".\r\nWhat is interesting is depending on the 1st argument and the array, it is not guaranteed that we stumble on a particular NULL due to the binary search nature of the algorithm.\r\n\r\nTrying to understand if we should change the function behavior.\r\n\r\n## Expected Behavior\r\nUnclear.\r\n\r\n## Current Behavior\r\nSee description.\r\n\r\n## Possible Solution\r\n1. If NULL element encountered - fail the query.\r\n2. Scan for any null elements beforehand and fail if found - cold be too expensive.\r\n3. If NULL element encountered - return NULL.\r\n4. Leave as is, update documentation.\r\n\r\n## Steps to Reproduce\r\nIn any environment run the example queries.",
    "issue_word_count": 321,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/functions/math.rst",
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java"
    ],
    "base_commit": "0d60c68ed38d524c07210426aec955c173fa3727",
    "head_commit": "4c77c0c9edc26179df1ffa811a19ece3363d7d92",
    "repo_url": "https://github.com/prestodb/presto/pull/24103",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24103",
    "dockerfile": "",
    "pr_merged_at": "2024-11-22T17:42:35.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/functions/math.rst b/presto-docs/src/main/sphinx/functions/math.rst\nindex ceab1579c5ead..1d1f18ff15cf6 100644\n--- a/presto-docs/src/main/sphinx/functions/math.rst\n+++ b/presto-docs/src/main/sphinx/functions/math.rst\n@@ -168,8 +168,12 @@ Mathematical Functions\n .. function:: width_bucket(x, bins) -> bigint\n \n     Returns the bin number of ``x`` according to the bins specified by the\n-    array ``bins``. The ``bins`` parameter must be an array of doubles and is\n-    assumed to be in sorted ascending order.\n+    array ``bins``. The ``bins`` parameter must be an array of doubles, should not\n+    contain ``null`` or non-finite elements, and is assumed to be in sorted ascending order.\n+\n+    Note: The function returns an error if it encounters a ``null`` or non-finite\n+    element in ``bins``, but due to the binary search algorithm some such elements\n+    might go unnoticed and the function will return a result.\n \n Probability Functions: cdf\n --------------------------\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\nindex 231e8db1b0aa8..707de7d28cd0f 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\n@@ -1572,17 +1572,23 @@ public static long widthBucket(@SqlType(StandardTypes.DOUBLE) double operand, @S\n \n         int index;\n         double bin;\n+        double lowerBin;\n+        double upperBin;\n \n         while (lower < upper) {\n-            if (DOUBLE.getDouble(bins, lower) > DOUBLE.getDouble(bins, upper - 1)) {\n-                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Bin values are not sorted in ascending order\");\n+            index = (lower + upper) / 2;\n+            if (bins.isNull(lower) || bins.isNull(index) || bins.isNull(upper - 1)) {\n+                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Bin values cannot be NULL\");\n             }\n \n-            index = (lower + upper) / 2;\n             bin = DOUBLE.getDouble(bins, index);\n-\n-            if (!isFinite(bin)) {\n-                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Bin value must be finite, got \" + bin);\n+            lowerBin = DOUBLE.getDouble(bins, lower);\n+            upperBin = DOUBLE.getDouble(bins, upper - 1);\n+            if (lowerBin > upperBin || lowerBin > bin || bin > upperBin) {\n+                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Bin values are not sorted in ascending order\");\n+            }\n+            if (!isFinite(bin) || !isFinite(lowerBin) || !isFinite(upperBin)) {\n+                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Bin values must be finite\");\n             }\n \n             if (operand < bin) {\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\nindex a5ab98de85fa3..9601f388f81e9 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\n@@ -1315,15 +1315,26 @@ public void testWidthBucketArray()\n         // failure modes\n         assertInvalidFunction(\"width_bucket(3.14E0, array[])\", \"Bins cannot be an empty array\");\n         assertInvalidFunction(\"width_bucket(nan(), array[1.0E0, 2.0E0, 3.0E0])\", \"Operand cannot be NaN\");\n-        assertInvalidFunction(\"width_bucket(3.14E0, array[0.0E0, infinity()])\", \"Bin value must be finite, got Infinity\");\n+        assertInvalidFunction(\"width_bucket(3.14E0, array[0.0E0, infinity()])\", \"Bin values must be finite\");\n \n         // fail if we aren't sorted\n+        assertInvalidFunction(\"width_bucket(3.14E0, array[0.0E0, infinity(), 10.0E0])\", \"Bin values are not sorted in ascending order\");\n         assertInvalidFunction(\"width_bucket(3.145E0, array[1.0E0, 0.0E0])\", \"Bin values are not sorted in ascending order\");\n         assertInvalidFunction(\"width_bucket(3.145E0, array[1.0E0, 0.0E0, -1.0E0])\", \"Bin values are not sorted in ascending order\");\n         assertInvalidFunction(\"width_bucket(3.145E0, array[1.0E0, 0.3E0, 0.0E0, -1.0E0])\", \"Bin values are not sorted in ascending order\");\n-\n-        // this is a case that we can't catch because we are using binary search to bisect the bins array\n-        assertFunction(\"width_bucket(1.5E0, array[1.0E0, 2.3E0, 2.0E0])\", BIGINT, 1L);\n+        assertInvalidFunction(\"width_bucket(1.5E0, array[1.0E0, 2.3E0, 2.0E0])\", \"Bin values are not sorted in ascending order\");\n+\n+        // Cases with nulls. When we hit a null element we throw.\n+        assertInvalidFunction(\"width_bucket(3.14E0, array[cast(null as double)])\", \"Bin values cannot be NULL\");\n+        assertInvalidFunction(\"width_bucket(3.14E0, array[0.0E0, null, 4.0E0])\", \"Bin values cannot be NULL\");\n+        assertInvalidFunction(\"width_bucket(3.14E0, array[0.0E0, 2.0E0, 4.0E0, null])\", \"Bin values cannot be NULL\");\n+\n+        // Cases we cannot catch due to the binary search algorithm.\n+        // Has null elements.\n+        assertFunction(\"width_bucket(3.14E0, array[0.0E0, null, 2.0E0, 4.0E0])\", BIGINT, 3L);\n+        assertFunction(\"width_bucket(3.14E0, array[0.0E0, null, 1.0E0, 2.0E0, 4.0E0])\", BIGINT, 4L);\n+        // Not properly sorted and has infinity.\n+        assertFunction(\"width_bucket(3.14E0, array[0.0E0, infinity(), 1.0E0, 2.0E0, 4.0E0])\", BIGINT, 4L);\n     }\n \n     @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24065",
    "pr_id": 24065,
    "issue_id": 23754,
    "repo": "prestodb/presto",
    "problem_statement": "ClassCastException in testArrayCumSum: Double cannot be cast to Float\n## Bug Description\r\nThe testArrayCumSum test in the presto-native-tests module fails with a ClassCastException due to type mismatch between Double and Float. The test expects Float values (corresponding to the SQL REAL type), but the array_cum_sum function returns Double values (corresponding to the SQL DOUBLE type) instead, causing the exception.\r\n\r\n### Test Case Location:\r\n\r\n**File:** presto-native-execution/presto-native-tests/src/test/java/com/facebook/presto/nativetests/AbstractTestQueriesNative.java\r\n\r\n**Test Method:** testArrayCumSum\r\n\r\nCode Snippet (from AbstractTestQueriesNative.java):\r\n\r\n```\r\nsql = \"select array_cum_sum(k) from (values (array[cast(5.1 as real), 6.1, 0.5]), (ARRAY[]), (CAST(NULL AS array(real))), (ARRAY [cast(null as real), 6.1, 0.5]), (ARRAY [cast(2.5 as real), 6.1, null, 3.2])) t(k)\";\r\nMaterializedResult raw = computeActual(sql);\r\nList<MaterializedRow> rowList = raw.getMaterializedRows();\r\nList<Float> actualFloat = (List<Float>) rowList.get(0).getField(0);  // Fails here due to Double-to-Float cast\r\n```\r\nError Message:\r\n\r\n`java.lang.ClassCastException: class java.lang.Double cannot be cast to class java.lang.Float (java.lang.Double and java.lang.Float are in module java.base of loader 'bootstrap')`\r\n\r\n## Expected Behavior\r\nThe array_cum_sum function should return cumulative sums for REAL values as Float. The test should pass without encountering any ClassCastException.\r\n\r\n## Current Behavior\r\nThe array_cum_sum function promotes REAL (floating-point Float in Java) values to DOUBLE during execution, leading to a ClassCastException when the test expects Float results.\r\nError Message:\r\n`java.lang.ClassCastException: class java.lang.Double cannot be cast to class java.lang.Float\r\n`\r\n\r\n## Possible Solution\r\nEnsure that array_cum_sum maintains the type consistency and does not promote REAL values to DOUBLE unless explicitly requested.\r\n\r\n\r\n\r\n",
    "issue_word_count": 276,
    "test_files_count": 3,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "base_commit": "0edeb7e2b0a1d174df0640482161b0246edc2dad",
    "head_commit": "dd3e9246d25b61c2fc99f68c818e7af099700be3",
    "repo_url": "https://github.com/prestodb/presto/pull/24065",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24065",
    "dockerfile": "",
    "pr_merged_at": "2024-11-19T02:22:47.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java\nindex 6c2ed80ac9fc4..c9a80ab9ac44f 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeGeneralQueries.java\n@@ -274,10 +274,10 @@ public void testAnalyzeStats()\n         // column_name | data_size | distinct_values_count | nulls_fraction | row_count | low_value | high_value\n         assertQuery(\"SHOW STATS FOR region\",\n                 \"SELECT * FROM (VALUES\" +\n-                        \"('regionkey', NULL, 5.0, 0.0, NULL, '0', '4', NULL),\" +\n-                        \"('name', 54.0, 5.0, 0.0, NULL, NULL, NULL, NULL),\" +\n-                        \"('comment', 350.0, 5.0, 0.0, NULL, NULL, NULL, NULL),\" +\n-                        \"(NULL, NULL, NULL, NULL, 5.0, NULL, NULL, NULL))\");\n+                        \"('regionkey', NULL, 5e0, 0e0, NULL, '0', '4', NULL),\" +\n+                        \"('name', 5.4e1, 5e0, 0e0, NULL, NULL, NULL, NULL),\" +\n+                        \"('comment', 3.5e2, 5e0, 0e0, NULL, NULL, NULL, NULL),\" +\n+                        \"(NULL, NULL, NULL, NULL, 5e0, NULL, NULL, NULL))\");\n \n         // Create a partitioned table and run analyze on it.\n         String tmpTableName = generateRandomTableName();\n@@ -291,17 +291,17 @@ public void testAnalyzeStats()\n             assertUpdate(String.format(\"ANALYZE %s\", tmpTableName), 25);\n             assertQuery(String.format(\"SHOW STATS for %s\", tmpTableName),\n                     \"SELECT * FROM (VALUES\" +\n-                            \"('name', 277.0, 1.0, 0.0, NULL, NULL, NULL, NULL),\" +\n-                            \"('regionkey', NULL, 5.0, 0.0, NULL, '0', '4', NULL),\" +\n-                            \"('nationkey', NULL, 25.0, 0.0, NULL, '0', '24', NULL),\" +\n-                            \"(NULL, NULL, NULL, NULL, 25.0, NULL, NULL, NULL))\");\n+                            \"('name', 2.77e2, 1e0, 0e0, NULL, NULL, NULL, NULL),\" +\n+                            \"('regionkey', NULL, 5e0, 0e0, NULL, '0', '4', NULL),\" +\n+                            \"('nationkey', NULL, 2.5e1, 0e0, NULL, '0', '24', NULL),\" +\n+                            \"(NULL, NULL, NULL, NULL, 2.5e1, NULL, NULL, NULL))\");\n             assertUpdate(String.format(\"ANALYZE %s WITH (partitions = ARRAY[ARRAY['0','0'],ARRAY['4', '11']])\", tmpTableName), 2);\n             assertQuery(String.format(\"SHOW STATS for (SELECT * FROM %s where regionkey=4 and nationkey=11)\", tmpTableName),\n                     \"SELECT * FROM (VALUES\" +\n-                            \"('name', 8.0, 1.0, 0.0, NULL, NULL, NULL, NULL),\" +\n-                            \"('regionkey', NULL, 1.0, 0.0, NULL, '4', '4', NULL),\" +\n-                            \"('nationkey', NULL, 1.0, 0.0, NULL, '11', '11', NULL),\" +\n-                            \"(NULL, NULL, NULL, NULL, 1.0, NULL, NULL, NULL))\");\n+                            \"('name', 8e0, 1e0, 0e0, NULL, NULL, NULL, NULL),\" +\n+                            \"('regionkey', NULL, 1e0, 0e0, NULL, '4', '4', NULL),\" +\n+                            \"('nationkey', NULL, 1e0, 0e0, NULL, '11', '11', NULL),\" +\n+                            \"(NULL, NULL, NULL, NULL, 1e0, NULL, NULL, NULL))\");\n         }\n         finally {\n             dropTableIfExists(tmpTableName);\n@@ -321,9 +321,9 @@ public void testAnalyzeStatsOnDecimals()\n             assertUpdate(String.format(\"ANALYZE %s\", tmpTableName), 7);\n             assertQuery(String.format(\"SHOW STATS for %s\", tmpTableName),\n                     \"SELECT * FROM (VALUES\" +\n-                            \"('c0', NULL,4.0 , 0.2857142857142857, NULL, '-542392.89', '1000000.12', NULL),\" +\n-                            \"('c1', NULL,4.0 , 0.2857142857142857, NULL,  '-6.72398239210929E12', '2.823982323232357E13', NULL),\" +\n-                            \"(NULL, NULL, NULL, NULL, 7.0, NULL, NULL, NULL))\");\n+                            \"('c0', NULL, 4e0, 2.857142857142857e-1, NULL, '-542392.89', '1000000.12', NULL),\" +\n+                            \"('c1', NULL, 4e0, 2.857142857142857e-1, NULL, '-6.72398239210929E12', '2.823982323232357E13', NULL),\" +\n+                            \"(NULL, NULL, NULL, NULL, 7e0, NULL, NULL, NULL))\");\n         }\n         finally {\n             dropTableIfExists(tmpTableName);\n@@ -503,13 +503,13 @@ public void testCast()\n         // Cast to Json type.\n         assertQuery(\"SELECT cast(regionkey = 2 as JSON) FROM nation\");\n         assertQuery(\"SELECT cast(size as JSON), cast(partkey as JSON), cast(brand as JSON), cast(name as JSON) FROM part\");\n-        assertQuery(\"SELECT cast(nationkey + 0.01 as JSON), cast(array[suppkey, nationkey] as JSON), cast(map(array[name, address, phone], array[1.1, 2.2, 3.3]) as JSON), cast(row(name, suppkey) as JSON), cast(array[map(array[name, address], array[1, 2]), map(array[name, phone], array[3, 4])] as JSON), cast(map(array[name, address, phone], array[array[1, 2], array[3, 4], array[5, 6]]) as JSON), cast(map(array[suppkey], array[name]) as JSON), cast(row(array[name, address], array[], array[null], map(array[phone], array[null])) as JSON) from supplier\");\n+        assertQuery(\"SELECT cast(nationkey + 1e-2 as JSON), cast(array[suppkey, nationkey] as JSON), cast(map(array[name, address, phone], array[1.1, 2.2, 3.3]) as JSON), cast(row(name, suppkey) as JSON), cast(array[map(array[name, address], array[1, 2]), map(array[name, phone], array[3, 4])] as JSON), cast(map(array[name, address, phone], array[array[1e0, 2], array[3, 4], array[5, 6]]) as JSON), cast(map(array[suppkey], array[name]) as JSON), cast(row(array[name, address], array[], array[null], map(array[phone], array[null])) as JSON) from supplier\");\n         assertQuery(\"SELECT cast(orderdate as JSON) FROM orders\");\n         assertQueryFails(\"SELECT cast(map(array[from_unixtime(suppkey)], array[1]) as JSON) from supplier\", \"Cannot cast .* to JSON\");\n \n         assertQuery(\"SELECT try_cast(regionkey = 2 as JSON) FROM nation\");\n         assertQuery(\"SELECT try_cast(size as JSON), try_cast(partkey as JSON), try_cast(brand as JSON), try_cast(name as JSON) FROM part\");\n-        assertQuery(\"SELECT try_cast(nationkey + 0.01 as JSON), try_cast(array[suppkey, nationkey] as JSON), try_cast(map(array[name, address, phone], array[1.1, 2.2, 3.3]) as JSON), try_cast(row(name, suppkey) as JSON), try_cast(array[map(array[name, address], array[1, 2]), map(array[name, phone], array[3, 4])] as JSON), try_cast(map(array[name, address, phone], array[array[1, 2], array[3, 4], array[5, 6]]) as JSON), try_cast(map(array[suppkey], array[name]) as JSON), try_cast(row(array[name, address], array[], array[null], map(array[phone], array[null])) as JSON) from supplier\");\n+        assertQuery(\"SELECT try_cast(nationkey + 1e-2 as JSON), try_cast(array[suppkey, nationkey] as JSON), try_cast(map(array[name, address, phone], array[1.1, 2.2, 3.3]) as JSON), try_cast(row(name, suppkey) as JSON), try_cast(array[map(array[name, address], array[1, 2]), map(array[name, phone], array[3, 4])] as JSON), try_cast(map(array[name, address, phone], array[array[1e0, 2], array[3, 4], array[5, 6]]) as JSON), try_cast(map(array[suppkey], array[name]) as JSON), try_cast(row(array[name, address], array[], array[null], map(array[phone], array[null])) as JSON) from supplier\");\n         assertQuery(\"SELECT try_cast(orderdate as JSON) FROM orders\");\n         assertQueryFails(\"SELECT try_cast(map(array[from_unixtime(suppkey)], array[1]) as JSON) from supplier\", \"Cannot cast .* to JSON\");\n \n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\nindex e63b9436b77df..de7d0f33b1012 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n@@ -45,7 +45,6 @@ public static Map<String, String> getNativeWorkerSystemProperties()\n         return ImmutableMap.<String, String>builder()\n                 .put(\"native-execution-enabled\", \"true\")\n                 .put(\"optimizer.optimize-hash-generation\", \"false\")\n-                .put(\"parse-decimal-literals-as-double\", \"true\")\n                 .put(\"regex-library\", \"RE2J\")\n                 .put(\"offset-clause-enabled\", \"true\")\n                 // By default, Presto will expand some functions into its SQL equivalent (e.g. array_duplicates()).\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\nindex 94f72266d4cc2..596b49015c0a8 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n@@ -171,7 +171,6 @@ public static QueryRunner createJavaQueryRunner(Optional<Path> baseDataDirectory\n                 HiveQueryRunner.createQueryRunner(\n                         ImmutableList.of(),\n                         ImmutableMap.of(\n-                                \"parse-decimal-literals-as-double\", \"true\",\n                                 \"regex-library\", \"RE2J\",\n                                 \"offset-clause-enabled\", \"true\"),\n                         security,\n@@ -228,7 +227,6 @@ public static QueryRunner createJavaIcebergQueryRunner(Optional<Path> baseDataDi\n \n         DistributedQueryRunner queryRunner = createIcebergQueryRunner(\n                 ImmutableMap.of(\n-                        \"parse-decimal-literals-as-double\", \"true\",\n                         \"regex-library\", \"RE2J\",\n                         \"offset-clause-enabled\", \"true\",\n                         \"query.max-stage-count\", \"110\"),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24059",
    "pr_id": 24059,
    "issue_id": 24077,
    "repo": "prestodb/presto",
    "problem_statement": "Failure to parse SqlFunctionHandle\nUnable to parse SqlFunctionHandle when the functions passed down have zero params or greater than one param.\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: Latest master\r\n* Storage (HDFS/S3/GCS..):\r\n* Data source and connector used:\r\n* Deployment (Cloud or On-prem):\r\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\r\n\r\n## Expected Behavior\r\nThe params in the SqlFunctionHandle should be parsed correctly.\r\n\r\n## Current Behavior\r\nWhile parsing the SqlFunctionHandle, the exit condition never gets hit leading to a never-ending loop.\r\n\r\n## Possible Solution\r\nA fix: https://github.com/prestodb/presto/pull/24059\r\n\r\n## Steps to Reproduce\r\nUnit tests included in PR\r\n\r\n\r\n",
    "issue_word_count": 113,
    "test_files_count": 2,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.cpp",
      "presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.h",
      "presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt",
      "presto-native-execution/presto_cpp/main/types/tests/PrestoToVeloxQueryPlanTest.cpp"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt",
      "presto-native-execution/presto_cpp/main/types/tests/PrestoToVeloxQueryPlanTest.cpp"
    ],
    "base_commit": "e416ff11482b92f812c2c47ef5ea0f0b19d85393",
    "head_commit": "3386da83949f1bac5c09bfcad6081383ca97b204",
    "repo_url": "https://github.com/prestodb/presto/pull/24059",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24059",
    "dockerfile": "",
    "pr_merged_at": "2024-12-05T09:22:38.000Z",
    "patch": "diff --git a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.cpp b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.cpp\nindex f0929edb16fac..eeab749ff2997 100644\n--- a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.cpp\n+++ b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.cpp\n@@ -842,27 +842,7 @@ void VeloxQueryPlanConverterBase::toAggregations(\n         auto sqlFunction =\n             std::dynamic_pointer_cast<protocol::SqlFunctionHandle>(\n                 prestoAggregation.functionHandle)) {\n-      const auto& functionId = sqlFunction->functionId;\n-\n-      // functionId format is function-name;arg-type1;arg-type2;...\n-      // For example: foo;INTEGER;VARCHAR.\n-      auto start = functionId.find(\";\");\n-      if (start != std::string::npos) {\n-        for (;;) {\n-          auto pos = functionId.find(\";\", start + 1);\n-          if (pos == std::string::npos) {\n-            auto argumentType = functionId.substr(start + 1);\n-            aggregate.rawInputTypes.push_back(\n-                stringToType(argumentType, typeParser_));\n-            break;\n-          }\n-\n-          auto argumentType = functionId.substr(start + 1, pos - start - 1);\n-          aggregate.rawInputTypes.push_back(\n-              stringToType(argumentType, typeParser_));\n-          pos = start + 1;\n-        }\n-      }\n+      parseSqlFunctionHandle(sqlFunction, aggregate.rawInputTypes, typeParser_);\n     } else {\n       VELOX_USER_FAIL(\n           \"Unsupported aggregate function handle: {}\",\n@@ -2110,4 +2090,30 @@ void registerPrestoPlanNodeSerDe() {\n   registry.Register(\n       \"BroadcastWriteNode\", presto::operators::BroadcastWriteNode::create);\n }\n+\n+void parseSqlFunctionHandle(\n+    const std::shared_ptr<protocol::SqlFunctionHandle>& sqlFunction,\n+    std::vector<velox::TypePtr>& rawInputTypes,\n+    TypeParser& typeParser) {\n+  const auto& functionId = sqlFunction->functionId;\n+  // functionId format is function-name;arg-type1;arg-type2;...\n+  // For example: foo;INTEGER;VARCHAR.\n+  auto start = functionId.find(\";\");\n+  if (start != std::string::npos) {\n+    for (;;) {\n+      auto pos = functionId.find(\";\", start + 1);\n+      if (pos == std::string::npos) {\n+        auto argumentType = functionId.substr(start + 1);\n+        if (!argumentType.empty()) {\n+          rawInputTypes.push_back(stringToType(argumentType, typeParser));\n+        }\n+        break;\n+      }\n+      auto argumentType = functionId.substr(start + 1, pos - start - 1);\n+      VELOX_CHECK(!argumentType.empty());\n+      rawInputTypes.push_back(stringToType(argumentType, typeParser));\n+      start = pos;\n+    }\n+  }\n+}\n } // namespace facebook::presto\n\ndiff --git a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.h b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.h\nindex c469891a96e33..4e0168dbafe0f 100644\n--- a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.h\n+++ b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxQueryPlan.h\n@@ -271,4 +271,9 @@ class VeloxBatchQueryPlanConverter : public VeloxQueryPlanConverterBase {\n };\n \n void registerPrestoPlanNodeSerDe();\n+\n+void parseSqlFunctionHandle(\n+    const std::shared_ptr<protocol::SqlFunctionHandle>& sqlFunction,\n+    std::vector<velox::TypePtr>& rawInputTypes,\n+    TypeParser& typeParser);\n } // namespace facebook::presto\n",
    "test_patch": "diff --git a/presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt b/presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt\nindex 7a735501dc725..caa5943ec3d79 100644\n--- a/presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt\n+++ b/presto-native-execution/presto_cpp/main/types/tests/CMakeLists.txt\n@@ -110,3 +110,20 @@ target_link_libraries(\n   velox_window\n   GTest::gtest\n   GTest::gtest_main)\n+\n+add_executable(presto_to_velox_query_plan_test PrestoToVeloxQueryPlanTest.cpp)\n+\n+add_test(\n+  NAME presto_to_velox_query_plan_test\n+  COMMAND presto_to_velox_query_plan_test\n+  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR})\n+\n+target_link_libraries(\n+  presto_to_velox_query_plan_test\n+  presto_operators\n+  presto_protocol\n+  presto_type_converter\n+  presto_types\n+  velox_exec_test_lib\n+  GTest::gtest\n+  GTest::gtest_main)\n\ndiff --git a/presto-native-execution/presto_cpp/main/types/tests/PrestoToVeloxQueryPlanTest.cpp b/presto-native-execution/presto_cpp/main/types/tests/PrestoToVeloxQueryPlanTest.cpp\nnew file mode 100644\nindex 0000000000000..1457805745ea5\n--- /dev/null\n+++ b/presto-native-execution/presto_cpp/main/types/tests/PrestoToVeloxQueryPlanTest.cpp\n@@ -0,0 +1,138 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+#include <gtest/gtest.h>\n+\n+#include \"presto_cpp/main/types/PrestoToVeloxQueryPlan.h\"\n+#include \"velox/common/base/tests/GTestUtils.h\"\n+#include \"velox/functions/prestosql/types/HyperLogLogType.h\"\n+#include \"velox/functions/prestosql/types/IPAddressType.h\"\n+#include \"velox/functions/prestosql/types/IPPrefixType.h\"\n+#include \"velox/functions/prestosql/types/JsonType.h\"\n+#include \"velox/functions/prestosql/types/TimestampWithTimeZoneType.h\"\n+#include \"velox/functions/prestosql/types/UuidType.h\"\n+\n+using namespace facebook::presto;\n+using namespace facebook::velox;\n+\n+namespace {\n+inline void validateSqlFunctionHandleParsing(\n+    const std::shared_ptr<facebook::presto::protocol::FunctionHandle>&\n+        functionHandle,\n+    std::vector<TypePtr> expectedRawInputTypes) {\n+  std::vector<TypePtr> actualRawInputTypes;\n+  TypeParser typeParser;\n+  auto sqlFunctionHandle =\n+      std::static_pointer_cast<protocol::SqlFunctionHandle>(functionHandle);\n+  facebook::presto::parseSqlFunctionHandle(\n+      sqlFunctionHandle, actualRawInputTypes, typeParser);\n+  EXPECT_EQ(expectedRawInputTypes.size(), actualRawInputTypes.size());\n+  for (int i = 0; i < expectedRawInputTypes.size(); i++) {\n+    EXPECT_EQ(*expectedRawInputTypes[i], *actualRawInputTypes[i]);\n+  }\n+}\n+} // namespace\n+\n+class PrestoToVeloxQueryPlanTest : public ::testing::Test {\n+ public:\n+  PrestoToVeloxQueryPlanTest() {\n+    registerHyperLogLogType();\n+    registerIPAddressType();\n+    registerIPPrefixType();\n+    registerJsonType();\n+    registerTimestampWithTimeZoneType();\n+    registerUuidType();\n+  }\n+};\n+\n+TEST_F(PrestoToVeloxQueryPlanTest, parseSqlFunctionHandleWithZeroParam) {\n+  std::string str = R\"(\n+      {\n+        \"@type\": \"json_file\",\n+        \"functionId\": \"json_file.test.count;\",\n+        \"version\": \"1\"\n+      }\n+    )\";\n+\n+  json j = json::parse(str);\n+  std::shared_ptr<facebook::presto::protocol::FunctionHandle> functionHandle =\n+      j;\n+  ASSERT_NE(functionHandle, nullptr);\n+  validateSqlFunctionHandleParsing(functionHandle, {});\n+}\n+\n+TEST_F(PrestoToVeloxQueryPlanTest, parseSqlFunctionHandleWithOneParam) {\n+  std::string str = R\"(\n+          {\n+            \"@type\": \"json_file\",\n+            \"functionId\": \"json_file.test.sum;tinyint\",\n+            \"version\": \"1\"\n+          }\n+    )\";\n+\n+  json j = json::parse(str);\n+  std::shared_ptr<facebook::presto::protocol::FunctionHandle> functionHandle =\n+      j;\n+  ASSERT_NE(functionHandle, nullptr);\n+\n+  std::vector<TypePtr> expectedRawInputTypes{TINYINT()};\n+  validateSqlFunctionHandleParsing(functionHandle, expectedRawInputTypes);\n+}\n+\n+TEST_F(PrestoToVeloxQueryPlanTest, parseSqlFunctionHandleWithMultipleParam) {\n+  std::string str = R\"(\n+        {\n+          \"@type\": \"json_file\",\n+          \"functionId\": \"json_file.test.avg;array(decimal(15, 2));varchar\",\n+          \"version\": \"1\"\n+        }\n+    )\";\n+\n+  json j = json::parse(str);\n+  std::shared_ptr<facebook::presto::protocol::FunctionHandle> functionHandle =\n+      j;\n+  ASSERT_NE(functionHandle, nullptr);\n+\n+  std::vector<TypePtr> expectedRawInputTypes{ARRAY(DECIMAL(15, 2)), VARCHAR()};\n+  validateSqlFunctionHandleParsing(functionHandle, expectedRawInputTypes);\n+}\n+\n+TEST_F(PrestoToVeloxQueryPlanTest, parseSqlFunctionHandleAllComplexTypes) {\n+  std::string str = R\"(\n+        {\n+          \"@type\": \"json_file\",\n+          \"functionId\": \"json_file.test.all_complex_types;row(map(hugeint, ipaddress), ipprefix);row(array(varbinary), timestamp, date, json, hyperloglog, timestamp with time zone, interval year to month, interval day to second);function(double, boolean);uuid\",\n+          \"version\": \"1\"\n+        }\n+    )\";\n+\n+  json j = json::parse(str);\n+  std::shared_ptr<facebook::presto::protocol::FunctionHandle> functionHandle =\n+      j;\n+  ASSERT_NE(functionHandle, nullptr);\n+\n+  std::vector<TypePtr> expectedRawInputTypes{\n+      ROW({MAP(HUGEINT(), IPADDRESS()), IPPREFIX()}),\n+      ROW(\n+          {ARRAY(VARBINARY()),\n+           TIMESTAMP(),\n+           DATE(),\n+           JSON(),\n+           HYPERLOGLOG(),\n+           TIMESTAMP_WITH_TIME_ZONE(),\n+           INTERVAL_YEAR_MONTH(),\n+           INTERVAL_DAY_TIME()}),\n+      FUNCTION({DOUBLE()}, BOOLEAN()),\n+      UUID()};\n+  validateSqlFunctionHandleParsing(functionHandle, expectedRawInputTypes);\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-24047",
    "pr_id": 24047,
    "issue_id": 23475,
    "repo": "prestodb/presto",
    "problem_statement": "Add Exchange before GroupId to improve Partial Aggregation\nhttps://github.com/prestodb/presto/pull/11741 showed good improvements for TPCDS Q22 & Q67\r\n\r\nI could verify that this does indeed improve Q22 performance in Trino (where it was added via https://github.com/trinodb/trino/pull/105)\r\n\r\nWe should re-open this PR",
    "issue_word_count": 52,
    "test_files_count": 10,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java",
      "presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java",
      "presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java",
      "presto-main/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java",
      "presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/AddExchangesBelowPartialAggregationOverGroupIdRuleSet.java",
      "presto-main/src/main/java/com/facebook/presto/sql/planner/plan/GroupIdNode.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java"
    ],
    "base_commit": "4615d4314634609dcf15cb9d5f729009602dbdac",
    "head_commit": "84cafecb80f2300795264de603a4e97e6e3b4c84",
    "repo_url": "https://github.com/prestodb/presto/pull/24047",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/24047",
    "dockerfile": "",
    "pr_merged_at": "2025-02-11T17:24:37.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java b/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\nindex a86d4ae6cbe18..ae8698136bad7 100644\n--- a/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\n+++ b/presto-main/src/main/java/com/facebook/presto/SystemSessionProperties.java\n@@ -328,6 +328,7 @@ public final class SystemSessionProperties\n     public static final String INCLUDE_VALUES_NODE_IN_CONNECTOR_OPTIMIZER = \"include_values_node_in_connector_optimizer\";\n     public static final String SINGLE_NODE_EXECUTION_ENABLED = \"single_node_execution_enabled\";\n     public static final String EXPRESSION_OPTIMIZER_NAME = \"expression_optimizer_name\";\n+    public static final String ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID = \"add_exchange_below_partial_aggregation_over_group_id\";\n \n     // TODO: Native execution related session properties that are temporarily put here. They will be relocated in the future.\n     public static final String NATIVE_AGGREGATION_SPILL_ALL = \"native_aggregation_spill_all\";\n@@ -1858,6 +1859,10 @@ public SystemSessionProperties(\n                         EXPRESSION_OPTIMIZER_NAME,\n                         \"Configure which expression optimizer to use\",\n                         featuresConfig.getExpressionOptimizerName(),\n+                        false),\n+                booleanProperty(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID,\n+                        \"Enable adding an exchange below partial aggregation over a GroupId node to improve partial aggregation performance\",\n+                        featuresConfig.getAddExchangeBelowPartialAggregationOverGroupId(),\n                         false));\n     }\n \n@@ -3164,4 +3169,9 @@ public static String getExpressionOptimizerName(Session session)\n     {\n         return session.getSystemProperty(EXPRESSION_OPTIMIZER_NAME, String.class);\n     }\n+\n+    public static boolean isEnabledAddExchangeBelowGroupId(Session session)\n+    {\n+        return session.getSystemProperty(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, Boolean.class);\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\nindex 033c6b26f489d..8d24e8673e35e 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n@@ -297,6 +297,7 @@ public class FeaturesConfig\n     private boolean singleNodeExecutionEnabled;\n     private boolean nativeExecutionScaleWritersThreadsEnabled;\n     private String expressionOptimizerName = DEFAULT_EXPRESSION_OPTIMIZER_NAME;\n+    private boolean addExchangeBelowPartialAggregationOverGroupId;\n \n     public enum PartitioningPrecisionStrategy\n     {\n@@ -2945,4 +2946,17 @@ public boolean isExcludeInvalidWorkerSessionProperties()\n     {\n         return this.setExcludeInvalidWorkerSessionProperties;\n     }\n+\n+    @Config(\"optimizer.add-exchange-below-partial-aggregation-over-group-id\")\n+    @ConfigDescription(\"Enable adding an exchange below partial aggregation over a GroupId node to improve partial aggregation performance\")\n+    public FeaturesConfig setAddExchangeBelowPartialAggregationOverGroupId(boolean addExchangeBelowPartialAggregationOverGroupId)\n+    {\n+        this.addExchangeBelowPartialAggregationOverGroupId = addExchangeBelowPartialAggregationOverGroupId;\n+        return this;\n+    }\n+\n+    public boolean getAddExchangeBelowPartialAggregationOverGroupId()\n+    {\n+        return addExchangeBelowPartialAggregationOverGroupId;\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java b/presto-main/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\nindex 25607c03c9c86..a7ec8cb353dfd 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/planner/PlanOptimizers.java\n@@ -18,6 +18,7 @@\n import com.facebook.presto.cost.CostComparator;\n import com.facebook.presto.cost.StatsCalculator;\n import com.facebook.presto.cost.TaskCountEstimator;\n+import com.facebook.presto.execution.TaskManagerConfig;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.split.PageSourceManager;\n import com.facebook.presto.split.SplitManager;\n@@ -27,6 +28,7 @@\n import com.facebook.presto.sql.planner.iterative.IterativeOptimizer;\n import com.facebook.presto.sql.planner.iterative.Rule;\n import com.facebook.presto.sql.planner.iterative.properties.LogicalPropertiesProviderImpl;\n+import com.facebook.presto.sql.planner.iterative.rule.AddExchangesBelowPartialAggregationOverGroupIdRuleSet;\n import com.facebook.presto.sql.planner.iterative.rule.AddIntermediateAggregations;\n import com.facebook.presto.sql.planner.iterative.rule.AddNotNullFiltersToJoinNode;\n import com.facebook.presto.sql.planner.iterative.rule.CombineApproxPercentileFunctions;\n@@ -222,7 +224,8 @@ public PlanOptimizers(\n             TaskCountEstimator taskCountEstimator,\n             PartitioningProviderManager partitioningProviderManager,\n             FeaturesConfig featuresConfig,\n-            ExpressionOptimizerManager expressionOptimizerManager)\n+            ExpressionOptimizerManager expressionOptimizerManager,\n+            TaskManagerConfig taskManagerConfig)\n     {\n         this(metadata,\n                 sqlParser,\n@@ -238,7 +241,8 @@ public PlanOptimizers(\n                 taskCountEstimator,\n                 partitioningProviderManager,\n                 featuresConfig,\n-                expressionOptimizerManager);\n+                expressionOptimizerManager,\n+                taskManagerConfig);\n     }\n \n     @PostConstruct\n@@ -270,7 +274,8 @@ public PlanOptimizers(\n             TaskCountEstimator taskCountEstimator,\n             PartitioningProviderManager partitioningProviderManager,\n             FeaturesConfig featuresConfig,\n-            ExpressionOptimizerManager expressionOptimizerManager)\n+            ExpressionOptimizerManager expressionOptimizerManager,\n+            TaskManagerConfig taskManagerConfig)\n     {\n         this.exporter = exporter;\n         ImmutableList.Builder<PlanOptimizer> builder = ImmutableList.builder();\n@@ -820,6 +825,7 @@ public PlanOptimizers(\n \n         if (!noExchange) {\n             builder.add(new ReplicateSemiJoinInDelete()); // Must run before AddExchanges\n+\n             builder.add(new IterativeOptimizer(\n                     metadata,\n                     ruleStats,\n@@ -830,6 +836,7 @@ public PlanOptimizers(\n                             // Must run before AddExchanges and after ReplicateSemiJoinInDelete\n                             // to avoid temporarily having an invalid plan\n                             new DetermineSemiJoinDistributionType(costComparator, taskCountEstimator))));\n+\n             builder.add(new RandomizeNullKeyInOuterJoin(metadata.getFunctionAndTypeManager(), statsCalculator),\n                     new PruneUnreferencedOutputs(),\n                     new IterativeOptimizer(\n@@ -841,6 +848,7 @@ public PlanOptimizers(\n                                     new PruneRedundantProjectionAssignments(),\n                                     new InlineProjections(metadata.getFunctionAndTypeManager()),\n                                     new RemoveRedundantIdentityProjections())));\n+\n             builder.add(new ShardJoins(metadata, metadata.getFunctionAndTypeManager(), statsCalculator),\n                     new PruneUnreferencedOutputs());\n             builder.add(\n@@ -914,6 +922,13 @@ public PlanOptimizers(\n                         ImmutableSet.of(\n                                 new PruneJoinColumns())));\n \n+        builder.add(new IterativeOptimizer(\n+                metadata,\n+                ruleStats,\n+                statsCalculator,\n+                costCalculator,\n+                new AddExchangesBelowPartialAggregationOverGroupIdRuleSet(taskCountEstimator, taskManagerConfig, metadata).rules()));\n+\n         builder.add(new IterativeOptimizer(\n                 metadata,\n                 ruleStats,\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/AddExchangesBelowPartialAggregationOverGroupIdRuleSet.java b/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/AddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\nnew file mode 100644\nindex 0000000000000..1d45d2e715025\n--- /dev/null\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/AddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\n@@ -0,0 +1,361 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner.iterative.rule;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.cost.PlanNodeStatsEstimate;\n+import com.facebook.presto.cost.TaskCountEstimator;\n+import com.facebook.presto.cost.VariableStatsEstimate;\n+import com.facebook.presto.execution.TaskManagerConfig;\n+import com.facebook.presto.matching.Capture;\n+import com.facebook.presto.matching.Captures;\n+import com.facebook.presto.matching.Pattern;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.spi.plan.AggregationNode;\n+import com.facebook.presto.spi.plan.Partitioning;\n+import com.facebook.presto.spi.plan.PartitioningScheme;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.ProjectNode;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.sql.planner.iterative.Rule;\n+import com.facebook.presto.sql.planner.optimizations.StreamPreferredProperties;\n+import com.facebook.presto.sql.planner.optimizations.StreamPropertyDerivations.StreamProperties;\n+import com.facebook.presto.sql.planner.plan.ExchangeNode;\n+import com.facebook.presto.sql.planner.plan.GroupIdNode;\n+import com.facebook.presto.sql.relational.ProjectNodeUtils;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.Multiset;\n+import io.airlift.units.DataSize;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import static com.facebook.presto.SystemSessionProperties.getTaskConcurrency;\n+import static com.facebook.presto.SystemSessionProperties.isEnabledAddExchangeBelowGroupId;\n+import static com.facebook.presto.matching.Capture.newCapture;\n+import static com.facebook.presto.matching.Pattern.nonEmpty;\n+import static com.facebook.presto.matching.Pattern.typeOf;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.StreamPreferredProperties.fixedParallelism;\n+import static com.facebook.presto.sql.planner.optimizations.StreamPropertyDerivations.deriveProperties;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.LOCAL;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.REMOTE_STREAMING;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.partitionedExchange;\n+import static com.facebook.presto.sql.planner.plan.Patterns.Aggregation.groupingColumns;\n+import static com.facebook.presto.sql.planner.plan.Patterns.Aggregation.step;\n+import static com.facebook.presto.sql.planner.plan.Patterns.source;\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMultiset.toImmutableMultiset;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n+import static java.lang.Double.isNaN;\n+import static java.lang.Math.min;\n+import static java.util.Comparator.comparing;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Transforms\n+ * <pre>\n+ *   - Exchange\n+ *     - [ Projection ]\n+ *       - Partial Aggregation\n+ *         - GroupId\n+ * </pre>\n+ * to\n+ * <pre>\n+ *   - Exchange\n+ *     - [ Projection ]\n+ *       - Partial Aggregation\n+ *         - GroupId\n+ *           - LocalExchange\n+ *             - RemoteExchange\n+ * </pre>\n+ * <p>\n+ * Rationale: GroupId increases number of rows (number of times equal to number of grouping sets) and then\n+ * partial aggregation reduces number of rows. However, under certain conditions, exchanging the rows before\n+ * GroupId (before multiplication) makes partial aggregation more effective, resulting in less data being\n+ * exchanged afterwards.\n+ */\n+public class AddExchangesBelowPartialAggregationOverGroupIdRuleSet\n+{\n+    private static final Capture<ProjectNode> PROJECTION = newCapture();\n+    private static final Capture<AggregationNode> AGGREGATION = newCapture();\n+    private static final Capture<GroupIdNode> GROUP_ID = newCapture();\n+    private static final Capture<ExchangeNode> REMOTE_EXCHANGE = newCapture();\n+\n+    private static final Pattern<ExchangeNode> WITH_PROJECTION =\n+            // If there was no exchange here, adding new exchanges could break property derivations logic of AddExchanges, AddLocalExchanges\n+            typeOf(ExchangeNode.class)\n+                    .matching(e -> e.getScope().isRemote()).capturedAs(REMOTE_EXCHANGE)\n+                    .with(source().matching(\n+                            // PushPartialAggregationThroughExchange adds a projection. However, it can be removed if RemoveRedundantIdentityProjections is run in the mean-time.\n+                            typeOf(ProjectNode.class).matching(ProjectNodeUtils::isIdentity).capturedAs(PROJECTION)\n+                                    .with(source().matching(\n+                                            typeOf(AggregationNode.class).capturedAs(AGGREGATION)\n+                                                    .with(step().equalTo(AggregationNode.Step.PARTIAL))\n+                                                    .with(nonEmpty(groupingColumns()))\n+                                                    .with(source().matching(\n+                                                            typeOf(GroupIdNode.class).capturedAs(GROUP_ID)))))));\n+\n+    private static final Pattern<ExchangeNode> WITHOUT_PROJECTION =\n+            // If there was no exchange here, adding new exchanges could break property derivations logic of AddExchanges, AddLocalExchanges\n+            typeOf(ExchangeNode.class)\n+                    .matching(e -> e.getScope().isRemote()).capturedAs(REMOTE_EXCHANGE)\n+                    .with(source().matching(\n+                            typeOf(AggregationNode.class).capturedAs(AGGREGATION)\n+                                    .with(step().equalTo(AggregationNode.Step.PARTIAL))\n+                                    .with(nonEmpty(groupingColumns()))\n+                                    .with(source().matching(\n+                                            typeOf(GroupIdNode.class).capturedAs(GROUP_ID)))));\n+\n+    private static final double GROUPING_SETS_SYMBOL_REQUIRED_FREQUENCY = 0.5;\n+    private static final double ANTI_SKEWNESS_MARGIN = 3;\n+    private final TaskCountEstimator taskCountEstimator;\n+    private final DataSize maxPartialAggregationMemoryUsage;\n+    private final Metadata metadata;\n+\n+    public AddExchangesBelowPartialAggregationOverGroupIdRuleSet(\n+            TaskCountEstimator taskCountEstimator,\n+            TaskManagerConfig taskManagerConfig,\n+            Metadata metadata)\n+    {\n+        this.taskCountEstimator = requireNonNull(taskCountEstimator, \"taskCountEstimator is null\");\n+        this.maxPartialAggregationMemoryUsage = taskManagerConfig.getMaxPartialAggregationMemoryUsage();\n+        this.metadata = metadata;\n+    }\n+\n+    public Set<Rule<?>> rules()\n+    {\n+        return ImmutableSet.of(\n+                belowProjectionRule(),\n+                belowExchangeRule());\n+    }\n+\n+    @VisibleForTesting\n+    AddExchangesBelowExchangePartialAggregationGroupId belowExchangeRule()\n+    {\n+        return new AddExchangesBelowExchangePartialAggregationGroupId();\n+    }\n+\n+    @VisibleForTesting\n+    AddExchangesBelowProjectionPartialAggregationGroupId belowProjectionRule()\n+    {\n+        return new AddExchangesBelowProjectionPartialAggregationGroupId();\n+    }\n+\n+    @VisibleForTesting\n+    class AddExchangesBelowProjectionPartialAggregationGroupId\n+            extends BaseAddExchangesBelowExchangePartialAggregationGroupId\n+    {\n+        @Override\n+        public Pattern<ExchangeNode> getPattern()\n+        {\n+            return WITH_PROJECTION;\n+        }\n+\n+        @Override\n+        public Result apply(ExchangeNode exchange, Captures captures, Context context)\n+        {\n+            ProjectNode project = captures.get(PROJECTION);\n+            AggregationNode aggregation = captures.get(AGGREGATION);\n+            GroupIdNode groupId = captures.get(GROUP_ID);\n+            return transform(aggregation, groupId, context)\n+                    .map(newAggregation -> Result.ofPlanNode(\n+                            exchange.replaceChildren(ImmutableList.of(\n+                                    project.replaceChildren(ImmutableList.of(\n+                                            newAggregation))))))\n+                    .orElseGet(Result::empty);\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    class AddExchangesBelowExchangePartialAggregationGroupId\n+            extends BaseAddExchangesBelowExchangePartialAggregationGroupId\n+    {\n+        @Override\n+        public Pattern<ExchangeNode> getPattern()\n+        {\n+            return WITHOUT_PROJECTION;\n+        }\n+\n+        @Override\n+        public Result apply(ExchangeNode exchange, Captures captures, Context context)\n+        {\n+            AggregationNode aggregation = captures.get(AGGREGATION);\n+            GroupIdNode groupId = captures.get(GROUP_ID);\n+            return transform(aggregation, groupId, context)\n+                    .map(newAggregation -> {\n+                        PlanNode newExchange = exchange.replaceChildren(ImmutableList.of(newAggregation));\n+                        return Result.ofPlanNode(newExchange);\n+                    })\n+                    .orElseGet(Result::empty);\n+        }\n+    }\n+\n+    private abstract class BaseAddExchangesBelowExchangePartialAggregationGroupId\n+            implements Rule<ExchangeNode>\n+    {\n+        @Override\n+        public boolean isEnabled(Session session)\n+        {\n+            return isEnabledAddExchangeBelowGroupId(session);\n+        }\n+\n+        protected Optional<PlanNode> transform(AggregationNode aggregation, GroupIdNode groupId, Context context)\n+        {\n+            Set<VariableReferenceExpression> groupingKeys = aggregation.getGroupingKeys().stream()\n+                    .filter(symbol -> !groupId.getGroupIdVariable().equals(symbol))\n+                    .collect(toImmutableSet());\n+\n+            Multiset<VariableReferenceExpression> groupingSetHistogram = groupId.getGroupingSets().stream()\n+                    .flatMap(Collection::stream)\n+                    .collect(toImmutableMultiset());\n+\n+            if (!Objects.equals(groupingSetHistogram.elementSet(), groupingKeys)) {\n+                // TODO handle the case when some aggregation keys are pass-through in GroupId (e.g. common in all grouping sets)\n+                // TODO handle the case when some grouping set symbols are not used in aggregation (possible?)\n+                return Optional.empty();\n+            }\n+\n+            double aggregationMemoryRequirements = estimateAggregationMemoryRequirements(groupingKeys, groupId, groupingSetHistogram, context);\n+            if (isNaN(aggregationMemoryRequirements) || aggregationMemoryRequirements < maxPartialAggregationMemoryUsage.toBytes()) {\n+                // Aggregation will be effective even without exchanges (or we have insufficient information).\n+                return Optional.empty();\n+            }\n+\n+            List<VariableReferenceExpression> desiredHashVariables = groupingSetHistogram.entrySet().stream()\n+                    // Take only frequently used symbols\n+                    .filter(entry -> entry.getCount() >= groupId.getGroupingSets().size() * GROUPING_SETS_SYMBOL_REQUIRED_FREQUENCY)\n+                    .map(Multiset.Entry::getElement)\n+                    // And only the symbols used in the aggregation (these are usually all symbols)\n+                    .peek(symbol -> verify(groupingKeys.contains(symbol), \"%s not found in the grouping keys [%s]\", symbol, groupingKeys))\n+                    // Transform to symbols before GroupId\n+                    .map(groupId.getGroupingColumns()::get)\n+                    .collect(toImmutableList());\n+\n+            // Use only the symbol with the highest cardinality (if we have statistics). This makes partial aggregation more efficient in case of\n+            // low correlation between symbol that are in every grouping set vs additional symbols.\n+            PlanNodeStatsEstimate sourceStats = context.getStatsProvider().getStats(groupId.getSource());\n+            desiredHashVariables = desiredHashVariables.stream()\n+                    .filter(symbol -> !isNaN(sourceStats.getVariableStatistics(symbol).getDistinctValuesCount()))\n+                    .max(comparing(symbol -> sourceStats.getVariableStatistics(symbol).getDistinctValuesCount()))\n+                    .map(symbol -> (List<VariableReferenceExpression>) ImmutableList.of(symbol)).orElse(desiredHashVariables);\n+\n+            StreamPreferredProperties requiredProperties = fixedParallelism().withPartitioning(desiredHashVariables);\n+            StreamProperties sourceProperties = derivePropertiesRecursively(groupId.getSource(), context);\n+            if (requiredProperties.isSatisfiedBy(sourceProperties)) {\n+                // Stream is already (locally) partitioned just as we want.\n+                // In fact, there might be just a LocalExchange below and no Remote. For now, we give up in this situation anyway. To properly support such situation:\n+                //  1. aggregation effectiveness estimation below need to consider the (helpful) fact that stream is already partitioned, so each operator will need less memory\n+                //  2. if the local exchange becomes unnecessary (after we add a remove on top of it), it should be removed. What if the local exchange is somewhere further\n+                //     down the tree?\n+                return Optional.empty();\n+            }\n+\n+            double estimatedGroups = estimateGroupCount(desiredHashVariables, context.getStatsProvider().getStats(groupId.getSource()));\n+            if (isNaN(estimatedGroups) || estimatedGroups * ANTI_SKEWNESS_MARGIN < maximalConcurrencyAfterRepartition(context)) {\n+                // Desired hash symbols form too few groups. Hashing over them would harm concurrency.\n+                // TODO instead of taking symbols with >GROUPING_SETS_SYMBOL_REQUIRED_FREQUENCY presence, we could take symbols from high freq to low until there are enough groups\n+                return Optional.empty();\n+            }\n+\n+            PlanNode source = groupId.getSource();\n+\n+            // Above we only checked the data is not yet locally partitioned and it could be already globally partitioned (but not locally). TODO avoid remote exchange in this case\n+            // TODO If the aggregation memory requirements are only slightly above `maxPartialAggregationMemoryUsage`, adding only LocalExchange could be enough\n+            source = partitionedExchange(\n+                    context.getIdAllocator().getNextId(),\n+                    REMOTE_STREAMING,\n+                    source,\n+                    new PartitioningScheme(\n+                            Partitioning.create(FIXED_HASH_DISTRIBUTION, desiredHashVariables),\n+                            source.getOutputVariables()));\n+\n+            source = partitionedExchange(\n+                    context.getIdAllocator().getNextId(),\n+                    LOCAL,\n+                    source,\n+                    new PartitioningScheme(\n+                            Partitioning.create(FIXED_HASH_DISTRIBUTION, desiredHashVariables),\n+                            source.getOutputVariables()));\n+\n+            PlanNode newGroupId = groupId.replaceChildren(ImmutableList.of(source));\n+            PlanNode newAggregation = aggregation.replaceChildren(ImmutableList.of(newGroupId));\n+\n+            return Optional.of(newAggregation);\n+        }\n+\n+        private int maximalConcurrencyAfterRepartition(Context context)\n+        {\n+            return getTaskConcurrency(context.getSession()) * taskCountEstimator.estimateHashedTaskCount(context.getSession());\n+        }\n+\n+        private double estimateAggregationMemoryRequirements(Set<VariableReferenceExpression> groupingKeys,\n+                GroupIdNode groupId,\n+                Multiset<VariableReferenceExpression> groupingSetHistogram,\n+                Context context)\n+        {\n+            checkArgument(Objects.equals(groupingSetHistogram.elementSet(), groupingKeys)); // Otherwise math below would be off-topic\n+\n+            PlanNodeStatsEstimate sourceStats = context.getStatsProvider().getStats(groupId.getSource());\n+            double keysMemoryRequirements = 0;\n+\n+            for (List<VariableReferenceExpression> groupingSet : groupId.getGroupingSets()) {\n+                List<VariableReferenceExpression> sourceVariables = groupingSet.stream()\n+                        .map(groupId.getGroupingColumns()::get)\n+                        .collect(toImmutableList());\n+\n+                double keyWidth = sourceStats.getOutputSizeForVariables(sourceVariables) / sourceStats.getOutputRowCount();\n+                double keyNdv = min(estimateGroupCount(sourceVariables, sourceStats), sourceStats.getOutputRowCount());\n+\n+                keysMemoryRequirements += keyWidth * keyNdv;\n+            }\n+\n+            // TODO consider also memory requirements for aggregation values\n+            return keysMemoryRequirements;\n+        }\n+\n+        private double estimateGroupCount(List<VariableReferenceExpression> variables, PlanNodeStatsEstimate statsEstimate)\n+        {\n+            return variables.stream()\n+                    .map(statsEstimate::getVariableStatistics)\n+                    .mapToDouble(this::ndvIncludingNull)\n+                    // This assumes no correlation, maximum number of aggregation keys\n+                    .reduce(1, (a, b) -> a * b);\n+        }\n+\n+        private double ndvIncludingNull(VariableStatsEstimate variableStatsEstimate)\n+        {\n+            if (variableStatsEstimate.getNullsFraction() == 0.) {\n+                return variableStatsEstimate.getDistinctValuesCount();\n+            }\n+            return variableStatsEstimate.getDistinctValuesCount() + 1;\n+        }\n+\n+        private StreamProperties derivePropertiesRecursively(PlanNode node, Context context)\n+        {\n+            PlanNode resolvedPlanNode = context.getLookup().resolve(node);\n+            List<StreamProperties> inputProperties = resolvedPlanNode.getSources().stream()\n+                    .map(source -> derivePropertiesRecursively(source, context))\n+                    .collect(toImmutableList());\n+            return deriveProperties(resolvedPlanNode, inputProperties, metadata, context.getSession());\n+        }\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/planner/plan/GroupIdNode.java b/presto-main/src/main/java/com/facebook/presto/sql/planner/plan/GroupIdNode.java\nindex 1b91e75e7dea5..b77a1fd2e7194 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/planner/plan/GroupIdNode.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/planner/plan/GroupIdNode.java\n@@ -81,7 +81,9 @@ public GroupIdNode(\n     {\n         super(sourceLocation, id, statsEquivalentPlanNode);\n         this.source = requireNonNull(source);\n-        this.groupingSets = listOfListsCopy(requireNonNull(groupingSets, \"groupingSets is null\"));\n+        checkArgument(requireNonNull(groupingSets, \"groupingSets is null\").size() > 1,\n+                \"groupingSets must have more than one grouping set, passed set was [%s]\", groupingSets);\n+        this.groupingSets = listOfListsCopy(groupingSets);\n         this.groupingColumns = ImmutableMap.copyOf(requireNonNull(groupingColumns));\n         this.aggregationArguments = ImmutableList.copyOf(aggregationArguments);\n         this.groupIdVariable = requireNonNull(groupIdVariable);\n",
    "test_patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java b/presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java\nindex 3f192bea26eb4..8e6228078068a 100644\n--- a/presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java\n+++ b/presto-main/src/main/java/com/facebook/presto/cost/TaskCountEstimator.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.cost;\n \n+import com.facebook.presto.Session;\n import com.facebook.presto.execution.scheduler.NodeSchedulerConfig;\n import com.facebook.presto.metadata.InternalNode;\n import com.facebook.presto.metadata.InternalNodeManager;\n@@ -22,6 +23,8 @@\n import java.util.Set;\n import java.util.function.IntSupplier;\n \n+import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n+import static java.lang.Math.min;\n import static java.lang.Math.toIntExact;\n import static java.util.Objects.requireNonNull;\n \n@@ -54,4 +57,9 @@ public int estimateSourceDistributedTaskCount()\n     {\n         return numberOfNodes.getAsInt();\n     }\n+\n+    public int estimateHashedTaskCount(Session session)\n+    {\n+        return min(numberOfNodes.getAsInt(), getHashPartitionCount(session));\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\nindex 8d2a9ac22e5bb..60626a7dbdc14 100644\n--- a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n+++ b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n@@ -353,6 +353,7 @@ public class LocalQueryRunner\n     private final NodeSpillConfig nodeSpillConfig;\n     private final NodeSchedulerConfig nodeSchedulerConfig;\n     private final FragmentStatsProvider fragmentStatsProvider;\n+    private final TaskManagerConfig taskManagerConfig;\n     private boolean printPlan;\n \n     private final PlanChecker distributedPlanChecker;\n@@ -378,19 +379,25 @@ public LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, F\n \n     public LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, FunctionsConfig functionsConfig, NodeSpillConfig nodeSpillConfig, boolean withInitialTransaction, boolean alwaysRevokeMemory)\n     {\n-        this(defaultSession, featuresConfig, functionsConfig, nodeSpillConfig, withInitialTransaction, alwaysRevokeMemory, 1, new ObjectMapper());\n+        this(defaultSession, featuresConfig, functionsConfig, nodeSpillConfig, withInitialTransaction, alwaysRevokeMemory, new ObjectMapper());\n     }\n \n     public LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, FunctionsConfig functionsConfig, NodeSpillConfig nodeSpillConfig, boolean withInitialTransaction, boolean alwaysRevokeMemory, ObjectMapper objectMapper)\n     {\n-        this(defaultSession, featuresConfig, functionsConfig, nodeSpillConfig, withInitialTransaction, alwaysRevokeMemory, 1, objectMapper);\n+        this(defaultSession, featuresConfig, functionsConfig, nodeSpillConfig, withInitialTransaction, alwaysRevokeMemory, 1, objectMapper, new TaskManagerConfig().setTaskConcurrency(4));\n     }\n \n-    private LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, FunctionsConfig functionsConfig, NodeSpillConfig nodeSpillConfig, boolean withInitialTransaction, boolean alwaysRevokeMemory, int nodeCountForStats, ObjectMapper objectMapper)\n+    public LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, FunctionsConfig functionsConfig, NodeSpillConfig nodeSpillConfig, boolean withInitialTransaction, boolean alwaysRevokeMemory, ObjectMapper objectMapper, TaskManagerConfig taskManagerConfig)\n+    {\n+        this(defaultSession, featuresConfig, functionsConfig, nodeSpillConfig, withInitialTransaction, alwaysRevokeMemory, 1, objectMapper, taskManagerConfig);\n+    }\n+\n+    private LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig, FunctionsConfig functionsConfig, NodeSpillConfig nodeSpillConfig, boolean withInitialTransaction, boolean alwaysRevokeMemory, int nodeCountForStats, ObjectMapper objectMapper, TaskManagerConfig taskManagerConfig)\n     {\n         requireNonNull(defaultSession, \"defaultSession is null\");\n         checkArgument(!defaultSession.getTransactionId().isPresent() || !withInitialTransaction, \"Already in transaction\");\n \n+        this.taskManagerConfig = taskManagerConfig;\n         this.nodeSpillConfig = requireNonNull(nodeSpillConfig, \"nodeSpillConfig is null\");\n         this.alwaysRevokeMemory = alwaysRevokeMemory;\n         this.notificationExecutor = newCachedThreadPool(daemonThreadsNamed(\"local-query-runner-executor-%s\"));\n@@ -624,7 +631,7 @@ public static LocalQueryRunner queryRunnerWithInitialTransaction(Session default\n \n     public static LocalQueryRunner queryRunnerWithFakeNodeCountForStats(Session defaultSession, int nodeCount)\n     {\n-        return new LocalQueryRunner(defaultSession, new FeaturesConfig(), new FunctionsConfig(), new NodeSpillConfig(), false, false, nodeCount, new ObjectMapper());\n+        return new LocalQueryRunner(defaultSession, new FeaturesConfig(), new FunctionsConfig(), new NodeSpillConfig(), false, false, nodeCount, new ObjectMapper(), new TaskManagerConfig().setTaskConcurrency(4));\n     }\n \n     @Override\n@@ -987,7 +994,7 @@ private List<Driver> createDrivers(Session session, Plan plan, OutputFactory out\n                 pageFunctionCompiler,\n                 joinFilterFunctionCompiler,\n                 new IndexJoinLookupStats(),\n-                new TaskManagerConfig().setTaskConcurrency(4),\n+                taskManagerConfig,\n                 new MemoryManagerConfig(),\n                 new FunctionsConfig(),\n                 spillerFactory,\n@@ -1153,7 +1160,8 @@ public List<PlanOptimizer> getPlanOptimizers(boolean noExchange)\n                 taskCountEstimator,\n                 partitioningProviderManager,\n                 featuresConfig,\n-                expressionOptimizerManager).getPlanningTimeOptimizers());\n+                expressionOptimizerManager,\n+                taskManagerConfig).getPlanningTimeOptimizers());\n         return planOptimizers.build();\n     }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\nindex 4a07a25abbd04..6ebe9120e8a1a 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n@@ -253,7 +253,8 @@ public void testDefaults()\n                 .setNativeExecutionScaleWritersThreadsEnabled(false)\n                 .setEnhancedCTESchedulingEnabled(true)\n                 .setExpressionOptimizerName(\"default\")\n-                .setExcludeInvalidWorkerSessionProperties(false));\n+                .setExcludeInvalidWorkerSessionProperties(false)\n+                .setAddExchangeBelowPartialAggregationOverGroupId(false));\n     }\n \n     @Test\n@@ -456,6 +457,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"enhanced-cte-scheduling-enabled\", \"false\")\n                 .put(\"expression-optimizer-name\", \"custom\")\n                 .put(\"exclude-invalid-worker-session-properties\", \"true\")\n+                .put(\"optimizer.add-exchange-below-partial-aggregation-over-group-id\", \"true\")\n                 .build();\n \n         FeaturesConfig expected = new FeaturesConfig()\n@@ -655,7 +657,8 @@ public void testExplicitPropertyMappings()\n                 .setNativeExecutionScaleWritersThreadsEnabled(true)\n                 .setEnhancedCTESchedulingEnabled(false)\n                 .setExpressionOptimizerName(\"custom\")\n-                .setExcludeInvalidWorkerSessionProperties(true);\n+                .setExcludeInvalidWorkerSessionProperties(true)\n+                .setAddExchangeBelowPartialAggregationOverGroupId(true);\n         assertFullMapping(properties, expected);\n     }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\nnew file mode 100644\nindex 0000000000000..5cffdb0dcd22d\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\n@@ -0,0 +1,99 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.execution.TaskManagerConfig;\n+import com.facebook.presto.spi.WarningCollector;\n+import com.facebook.presto.sql.planner.assertions.BasePlanTest;\n+import com.facebook.presto.sql.planner.assertions.PlanAssert;\n+import com.facebook.presto.sql.planner.plan.GroupIdNode;\n+import com.facebook.presto.testing.LocalQueryRunner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import io.airlift.units.DataSize;\n+import org.testng.annotations.Test;\n+\n+import static com.facebook.presto.SystemSessionProperties.ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID;\n+import static com.facebook.presto.SystemSessionProperties.MERGE_AGGREGATIONS_WITH_AND_WITHOUT_FILTER;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.anyTree;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.exchange;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.expression;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.node;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.project;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.tableScan;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.LOCAL;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.REMOTE_STREAMING;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Type.REPARTITION;\n+import static io.airlift.units.DataSize.Unit.KILOBYTE;\n+import static io.airlift.units.DataSize.Unit.MEGABYTE;\n+\n+public class TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet\n+        extends BasePlanTest\n+{\n+    public TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet()\n+    {\n+        super(TestLogicalAddExchangesBelowPartialAggregationOverGroupIdRuleSet::setup);\n+    }\n+\n+    private static LocalQueryRunner setup()\n+    {\n+        // We set available max-partial-aggregation-memory to a low value to allow the rule to trigger for the TPCH tiny scale factor\n+        TaskManagerConfig taskManagerConfig = new TaskManagerConfig().setMaxPartialAggregationMemoryUsage(DataSize.succinctDataSize(1, KILOBYTE));\n+        return createQueryRunner(ImmutableMap.of(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, \"true\"), taskManagerConfig);\n+    }\n+\n+    @Test\n+    public void testRollup()\n+    {\n+        assertDistributedPlan(\"SELECT orderkey, suppkey, partkey, sum(quantity) from lineitem GROUP BY ROLLUP(orderkey, suppkey, partkey)\",\n+                anyTree(node(GroupIdNode.class,\n+                        // Since 'orderkey' will be the variable with the highest frequency, we repartition on it\n+                        anyTree(exchange(LOCAL, REPARTITION, ImmutableList.of(), ImmutableSet.of(\"orderkey\"),\n+                                exchange(REMOTE_STREAMING, REPARTITION, ImmutableList.of(), ImmutableSet.of(\"orderkey\"),\n+                                        anyTree(tableScan(\"lineitem\", ImmutableMap.of(\"orderkey\", \"orderkey\")))))))));\n+    }\n+\n+    @Test\n+    public void testNegativeCases()\n+    {\n+        // MERGE_AGGREGATIONS_WITH_AND_WITHOUT_FILTER adds a Project for an 'expr' that is pass-through through the GroupIdNode node\n+        // The Rule does not apply when such a variable is used in an Aggregation but not in the GroupId grouping set\n+        Session enableMergeAggregationWithAndWithoutFilter = Session.builder(getQueryRunner().getDefaultSession())\n+                .setSystemProperty(MERGE_AGGREGATIONS_WITH_AND_WITHOUT_FILTER, \"true\")\n+                .build();\n+        String sql = \"select partkey, sum(quantity), sum(quantity) filter (where discount > 0.1) from lineitem group by grouping sets((), (partkey))\";\n+        assertDistributedPlan(sql, enableMergeAggregationWithAndWithoutFilter,\n+                anyTree(node(GroupIdNode.class,\n+                        project(ImmutableMap.of(\"partkey\", expression(\"partkey\"), \"quantity\", expression(\"quantity\"), \"expr\", expression(\"discount > DOUBLE'0.1'\")),\n+                                tableScan(\"lineitem\",\n+                                        ImmutableMap.of(\"partkey\", \"partkey\", \"quantity\", \"quantity\", \"discount\", \"discount\"))))));\n+\n+        //  Rule does not apply when aggregation will be effective due to a sufficiently high max-partial-aggregation-memory\n+        TaskManagerConfig taskManagerConfig = new TaskManagerConfig().setMaxPartialAggregationMemoryUsage(DataSize.succinctDataSize(1, MEGABYTE));\n+        try (LocalQueryRunner queryRunner = createQueryRunner(ImmutableMap.of(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, \"true\"), taskManagerConfig)) {\n+            queryRunner.inTransaction(queryRunner.getDefaultSession(), transactionSession -> {\n+                Plan plan = queryRunner.createPlan(transactionSession,\n+                        \"SELECT orderkey, suppkey, partkey, sum(quantity) from lineitem GROUP BY ROLLUP(orderkey, suppkey, partkey)\",\n+                        WarningCollector.NOOP);\n+\n+                PlanAssert.assertPlan(transactionSession, queryRunner.getMetadata(), queryRunner.getStatsCalculator(), plan,\n+                        anyTree(node(GroupIdNode.class,\n+                                tableScan(\"lineitem\", ImmutableMap.of(\"orderkey\", \"orderkey\")))));\n+                return null;\n+            });\n+        }\n+    }\n+}\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java\nindex 8194b060e0d42..45deffb292fa7 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/BasePlanTest.java\n@@ -21,6 +21,7 @@\n import com.facebook.presto.common.type.TestingTypeDeserializer;\n import com.facebook.presto.common.type.TestingTypeManager;\n import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.execution.TaskManagerConfig;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.spi.ConnectorId;\n import com.facebook.presto.spi.WarningCollector;\n@@ -104,6 +105,11 @@ protected static ObjectMapper createObjectMapper()\n     }\n \n     protected static LocalQueryRunner createQueryRunner(Map<String, String> sessionProperties)\n+    {\n+        return createQueryRunner(sessionProperties, new TaskManagerConfig().setTaskConcurrency(1));\n+    }\n+\n+    protected static LocalQueryRunner createQueryRunner(Map<String, String> sessionProperties, TaskManagerConfig taskManagerConfig)\n     {\n         Session.SessionBuilder sessionBuilder = testSessionBuilder()\n                 .setCatalog(\"local\")\n@@ -112,7 +118,14 @@ protected static LocalQueryRunner createQueryRunner(Map<String, String> sessionP\n \n         sessionProperties.entrySet().forEach(entry -> sessionBuilder.setSystemProperty(entry.getKey(), entry.getValue()));\n \n-        LocalQueryRunner queryRunner = new LocalQueryRunner(sessionBuilder.build(), new FeaturesConfig(), new FunctionsConfig(), new NodeSpillConfig(), false, false, createObjectMapper());\n+        LocalQueryRunner queryRunner = new LocalQueryRunner(sessionBuilder.build(),\n+                new FeaturesConfig(),\n+                new FunctionsConfig(),\n+                new NodeSpillConfig(),\n+                false,\n+                false,\n+                createObjectMapper(),\n+                taskManagerConfig);\n \n         queryRunner.createCatalog(queryRunner.getDefaultSession().getCatalog().get(),\n                 new TpchConnectorFactory(1),\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java\nindex cdda3f16c3070..88419b759f046 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/ExchangeMatcher.java\n@@ -17,10 +17,14 @@\n import com.facebook.presto.cost.StatsProvider;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.relation.RowExpression;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n import com.facebook.presto.sql.planner.assertions.PlanMatchPattern.Ordering;\n import com.facebook.presto.sql.planner.plan.ExchangeNode;\n+import com.google.common.collect.ImmutableSet;\n \n import java.util.List;\n+import java.util.Set;\n \n import static com.facebook.presto.sql.planner.assertions.MatchResult.NO_MATCH;\n import static com.facebook.presto.sql.planner.assertions.Util.orderingSchemeMatches;\n@@ -34,12 +38,14 @@ final class ExchangeMatcher\n     private final ExchangeNode.Scope scope;\n     private final ExchangeNode.Type type;\n     private final List<Ordering> orderBy;\n+    private final Set<String> partitionedBy;\n \n-    public ExchangeMatcher(ExchangeNode.Scope scope, ExchangeNode.Type type, List<Ordering> orderBy)\n+    public ExchangeMatcher(ExchangeNode.Scope scope, ExchangeNode.Type type, List<Ordering> orderBy, Set<String> partitionedBy)\n     {\n         this.scope = scope;\n         this.type = type;\n         this.orderBy = requireNonNull(orderBy, \"orderBy is null\");\n+        this.partitionedBy = requireNonNull(partitionedBy, \"partitionedBy is null\");\n     }\n \n     @Override\n@@ -69,6 +75,18 @@ public MatchResult detailMatches(PlanNode node, StatsProvider stats, Session ses\n             }\n         }\n \n+        if (!partitionedBy.isEmpty()) {\n+            Set<String> partitionedColumns = exchangeNode.getPartitioningScheme().getPartitioning().getArguments().stream()\n+                    .map(RowExpression.class::cast)\n+                    .map(VariableReferenceExpression.class::cast)\n+                    .map(VariableReferenceExpression::getName)\n+                    .collect(ImmutableSet.toImmutableSet());\n+\n+            if (!partitionedColumns.containsAll(partitionedBy)) {\n+                return NO_MATCH;\n+            }\n+        }\n+\n         return MatchResult.match();\n     }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java\nindex d06a36ae81279..8702e3c0f7a1b 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/assertions/PlanMatchPattern.java\n@@ -512,9 +512,14 @@ public static PlanMatchPattern exchange(ExchangeNode.Scope scope, ExchangeNode.T\n     }\n \n     public static PlanMatchPattern exchange(ExchangeNode.Scope scope, ExchangeNode.Type type, List<Ordering> orderBy, PlanMatchPattern... sources)\n+    {\n+        return exchange(scope, type, orderBy, ImmutableSet.of(), sources);\n+    }\n+\n+    public static PlanMatchPattern exchange(ExchangeNode.Scope scope, ExchangeNode.Type type, List<Ordering> orderBy, Set<String> partitionedBy, PlanMatchPattern... sources)\n     {\n         return node(ExchangeNode.class, sources)\n-                .with(new ExchangeMatcher(scope, type, orderBy));\n+                .with(new ExchangeMatcher(scope, type, orderBy, partitionedBy));\n     }\n \n     public static PlanMatchPattern union(PlanMatchPattern... sources)\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\nnew file mode 100644\nindex 0000000000000..4239877e95318\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet.java\n@@ -0,0 +1,224 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.planner.iterative.rule;\n+\n+import com.facebook.presto.cost.PlanNodeStatsEstimate;\n+import com.facebook.presto.cost.TaskCountEstimator;\n+import com.facebook.presto.cost.VariableStatsEstimate;\n+import com.facebook.presto.execution.TaskManagerConfig;\n+import com.facebook.presto.spi.plan.Partitioning;\n+import com.facebook.presto.spi.plan.PartitioningScheme;\n+import com.facebook.presto.spi.plan.PlanNode;\n+import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.relation.VariableReferenceExpression;\n+import com.facebook.presto.sql.planner.assertions.GroupIdMatcher;\n+import com.facebook.presto.sql.planner.iterative.rule.test.BaseRuleTest;\n+import com.facebook.presto.sql.planner.iterative.rule.test.RuleAssert;\n+import com.facebook.presto.sql.planner.iterative.rule.test.RuleTester;\n+import com.facebook.presto.sql.planner.plan.GroupIdNode;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+\n+import static com.facebook.presto.SystemSessionProperties.ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID;\n+import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.spi.plan.AggregationNode.Step.PARTIAL;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.aggregation;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.exchange;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.expression;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.node;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.project;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.singleGroupingSet;\n+import static com.facebook.presto.sql.planner.assertions.PlanMatchPattern.values;\n+import static com.facebook.presto.sql.planner.plan.AssignmentUtils.identityAssignments;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.LOCAL;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Scope.REMOTE_STREAMING;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Type.GATHER;\n+import static com.facebook.presto.sql.planner.plan.ExchangeNode.Type.REPARTITION;\n+\n+public class TestAddExchangesBelowPartialAggregationOverGroupIdRuleSet\n+        extends BaseRuleTest\n+{\n+    private static AddExchangesBelowPartialAggregationOverGroupIdRuleSet.AddExchangesBelowExchangePartialAggregationGroupId belowExchangeRule(RuleTester ruleTester)\n+    {\n+        TaskCountEstimator taskCountEstimator = new TaskCountEstimator(() -> 4);\n+        TaskManagerConfig taskManagerConfig = new TaskManagerConfig();\n+        return new AddExchangesBelowPartialAggregationOverGroupIdRuleSet(\n+                taskCountEstimator,\n+                taskManagerConfig,\n+                ruleTester.getMetadata()\n+        ).belowExchangeRule();\n+    }\n+\n+    private static AddExchangesBelowPartialAggregationOverGroupIdRuleSet.AddExchangesBelowProjectionPartialAggregationGroupId belowProjectionRule(RuleTester ruleTester)\n+    {\n+        TaskCountEstimator taskCountEstimator = new TaskCountEstimator(() -> 4);\n+        TaskManagerConfig taskManagerConfig = new TaskManagerConfig();\n+        return new AddExchangesBelowPartialAggregationOverGroupIdRuleSet(\n+                taskCountEstimator,\n+                taskManagerConfig,\n+                ruleTester.getMetadata()\n+        ).belowProjectionRule();\n+    }\n+\n+    @DataProvider\n+    public static Object[][] testDataProvider()\n+    {\n+        return new Object[][] {\n+                {1000.0, 10_000.0, 1_000_000.0, \"groupingKey3\"},\n+                {1000.0, 2_000_000.0, 1_000_000.0, \"groupingKey2\"},\n+                {1000.0, 1000.0, 1000.0, \"groupingKey1\"}\n+        };\n+    }\n+\n+    @DataProvider\n+    public static Object[][] testDataProviderMissingStats()\n+    {\n+        return new Object[][] {\n+                {Double.NaN, 10_000.0, 1_000_000.0},\n+                {1000.0, Double.NaN, 1_000_000.0},\n+                {1000.0, 10_000.0, Double.NaN}\n+        };\n+    }\n+\n+    @Test(dataProvider = \"testDataProvider\")\n+    public void testAddExchangesWithoutProjection(double groupingKey1NDV, double groupingKey2NDV, double groupingKey3NDV, String expectedRepartitionSymbol)\n+    {\n+        buildRuleAssert(groupingKey1NDV, groupingKey2NDV, groupingKey3NDV, false)\n+                .matches(exchange(\n+                        REMOTE_STREAMING,\n+                        GATHER,\n+                        aggregation(\n+                                singleGroupingSet(ImmutableList.of(\"groupingKey1\", \"groupingKey2\", \"groupingKey3\", \"groupId\")),\n+                                ImmutableMap.of(),\n+                                ImmutableList.of(),\n+                                ImmutableMap.of(),\n+                                Optional.empty(),\n+                                PARTIAL,\n+                                node(GroupIdNode.class,\n+                                        exchange(\n+                                                LOCAL,\n+                                                REPARTITION,\n+                                                ImmutableList.of(),\n+                                                ImmutableSet.of(expectedRepartitionSymbol),\n+                                                exchange(\n+                                                        REMOTE_STREAMING,\n+                                                        REPARTITION,\n+                                                        ImmutableList.of(),\n+                                                        ImmutableSet.of(expectedRepartitionSymbol),\n+                                                        values(\"groupingKey1\", \"groupingKey2\", \"groupingKey3\"))))\n+                                        .with(new GroupIdMatcher(ImmutableList.of(\n+                                                ImmutableList.of(\"groupingKey1\", \"groupingKey2\"),\n+                                                ImmutableList.of(\"groupingKey1\", \"groupingKey3\")), ImmutableMap.of(), \"groupId\")))));\n+    }\n+\n+    @Test(dataProvider = \"testDataProvider\")\n+    public void testAddExchangesWithProjection(double groupingKey1NDV, double groupingKey2NDV, double groupingKey3NDV, String expectedRepartitionSymbol)\n+    {\n+        buildRuleAssert(groupingKey1NDV, groupingKey2NDV, groupingKey3NDV, true)\n+                .matches(exchange(\n+                        REMOTE_STREAMING,\n+                        GATHER,\n+                        project(\n+                                ImmutableMap.of(\n+                                        \"groupingKey1\", expression(\"groupingKey1\"),\n+                                        \"groupingKey2\", expression(\"groupingKey2\"),\n+                                        \"groupingKey3\", expression(\"groupingKey3\")),\n+                                aggregation(\n+                                        singleGroupingSet(ImmutableList.of(\"groupingKey1\", \"groupingKey2\", \"groupingKey3\", \"groupId\")),\n+                                        ImmutableMap.of(),\n+                                        ImmutableList.of(),\n+                                        ImmutableMap.of(),\n+                                        Optional.empty(),\n+                                        PARTIAL,\n+                                        node(GroupIdNode.class,\n+                                                exchange(\n+                                                        LOCAL,\n+                                                        REPARTITION,\n+                                                        ImmutableList.of(),\n+                                                        ImmutableSet.of(expectedRepartitionSymbol),\n+                                                        exchange(\n+                                                                REMOTE_STREAMING,\n+                                                                REPARTITION,\n+                                                                ImmutableList.of(),\n+                                                                ImmutableSet.of(expectedRepartitionSymbol),\n+                                                                values(\"groupingKey1\", \"groupingKey2\", \"groupingKey3\"))))\n+                                                .with(new GroupIdMatcher(ImmutableList.of(\n+                                                        ImmutableList.of(\"groupingKey1\", \"groupingKey2\"),\n+                                                        ImmutableList.of(\"groupingKey1\", \"groupingKey3\")), ImmutableMap.of(), \"groupId\"))))));\n+    }\n+\n+    @Test(dataProvider = \"testDataProviderMissingStats\")\n+    public void testDoesNotFireIfAnySourceSymbolIsMissingStats(double groupingKey1NDV, double groupingKey2NDV, double groupingKey3NDV)\n+    {\n+        buildRuleAssert(groupingKey1NDV, groupingKey2NDV, groupingKey3NDV, true).doesNotFire();\n+        buildRuleAssert(groupingKey1NDV, groupingKey2NDV, groupingKey3NDV, false).doesNotFire();\n+    }\n+\n+    @Test\n+    public void testDoesNotFireIfSessionPropertyIsDisabled()\n+    {\n+        buildRuleAssert(1000D, 1000D, 1000D, false)\n+                .setSystemProperty(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, \"false\")\n+                .doesNotFire();\n+    }\n+\n+    private RuleAssert buildRuleAssert(double groupingKey1NDV, double groupingKey2NDV, double groupingKey3NDV, boolean withProjection)\n+    {\n+        RuleTester ruleTester = tester();\n+        String groupIdSourceId = \"groupIdSourceId\";\n+        return ruleTester.assertThat(withProjection ? belowProjectionRule(ruleTester) : belowExchangeRule(ruleTester))\n+                .setSystemProperty(ADD_EXCHANGE_BELOW_PARTIAL_AGGREGATION_OVER_GROUP_ID, \"true\")\n+                .overrideStats(groupIdSourceId, PlanNodeStatsEstimate\n+                        .builder()\n+                        .setOutputRowCount(100_000_000)\n+                        .addVariableStatistics(ImmutableMap.of(\n+                                new VariableReferenceExpression(Optional.empty(), \"groupingKey1\", BIGINT), VariableStatsEstimate.builder().setDistinctValuesCount(groupingKey1NDV).build(),\n+                                new VariableReferenceExpression(Optional.empty(), \"groupingKey2\", BIGINT), VariableStatsEstimate.builder().setDistinctValuesCount(groupingKey2NDV).build(),\n+                                new VariableReferenceExpression(Optional.empty(), \"groupingKey3\", BIGINT), VariableStatsEstimate.builder().setDistinctValuesCount(groupingKey3NDV).build()))\n+                        .build())\n+                .on(p -> {\n+                    VariableReferenceExpression groupingKey1 = p.variable(\"groupingKey1\", BIGINT);\n+                    VariableReferenceExpression groupingKey2 = p.variable(\"groupingKey2\", BIGINT);\n+                    VariableReferenceExpression groupingKey3 = p.variable(\"groupingKey3\", BIGINT);\n+                    VariableReferenceExpression groupId = p.variable(\"groupId\", BIGINT);\n+\n+                    PlanNode partialAgg = p.aggregation(builder -> builder\n+                            .singleGroupingSet(groupingKey1, groupingKey2, groupingKey3, groupId)\n+                            .step(PARTIAL)\n+                            .source(p.groupId(\n+                                    ImmutableList.of(\n+                                            ImmutableList.of(groupingKey1, groupingKey2),\n+                                            ImmutableList.of(groupingKey1, groupingKey3)),\n+                                    ImmutableList.of(),\n+                                    groupId,\n+                                    p.values(new PlanNodeId(groupIdSourceId), groupingKey1, groupingKey2, groupingKey3))));\n+\n+                    return p.exchange(\n+                            exchangeBuilder -> exchangeBuilder\n+                                    .scope(REMOTE_STREAMING)\n+                                    .partitioningScheme(new PartitioningScheme(Partitioning.create(\n+                                            FIXED_ARBITRARY_DISTRIBUTION,\n+                                            ImmutableList.of()),\n+                                            ImmutableList.copyOf(ImmutableList.of(groupingKey1, groupingKey2, groupingKey3, groupId))))\n+                                    .addInputsSet(groupingKey1, groupingKey2, groupingKey3, groupId)\n+                                    .addSource(withProjection ? p.project(identityAssignments(partialAgg.getOutputVariables()), partialAgg) : partialAgg));\n+                });\n+    }\n+}\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java\nindex aa633c33a8624..6e541d36606aa 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/iterative/rule/test/PlanBuilder.java\n@@ -76,6 +76,7 @@\n import com.facebook.presto.sql.planner.plan.AssignUniqueId;\n import com.facebook.presto.sql.planner.plan.EnforceSingleRowNode;\n import com.facebook.presto.sql.planner.plan.ExchangeNode;\n+import com.facebook.presto.sql.planner.plan.GroupIdNode;\n import com.facebook.presto.sql.planner.plan.IndexJoinNode;\n import com.facebook.presto.sql.planner.plan.IndexSourceNode;\n import com.facebook.presto.sql.planner.plan.LateralJoinNode;\n@@ -97,6 +98,7 @@\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collection;\n import java.util.HashMap;\n import java.util.LinkedHashMap;\n import java.util.List;\n@@ -126,8 +128,10 @@\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n import static java.lang.String.format;\n import static java.util.Collections.emptyList;\n+import static java.util.function.Function.identity;\n \n public class PlanBuilder\n {\n@@ -1047,4 +1051,29 @@ public PlanNodeIdAllocator getIdAllocator()\n     {\n         return idAllocator;\n     }\n+\n+    public GroupIdNode groupId(List<List<VariableReferenceExpression>> groupingSets, List<VariableReferenceExpression> aggregationArguments, VariableReferenceExpression groupIdSymbol, PlanNode source)\n+    {\n+        Map<VariableReferenceExpression, VariableReferenceExpression> groupingColumns = groupingSets.stream()\n+                .flatMap(Collection::stream)\n+                .distinct()\n+                .collect(toImmutableMap(identity(), identity()));\n+        return groupId(groupingSets, groupingColumns, aggregationArguments, groupIdSymbol, source);\n+    }\n+\n+    public GroupIdNode groupId(List<List<VariableReferenceExpression>> groupingSets,\n+            Map<VariableReferenceExpression, VariableReferenceExpression> groupingColumns,\n+            List<VariableReferenceExpression> aggregationArguments,\n+            VariableReferenceExpression groupIdSymbol,\n+            PlanNode source)\n+    {\n+        return new GroupIdNode(\n+                Optional.empty(),\n+                idAllocator.getNextId(),\n+                source,\n+                groupingSets,\n+                groupingColumns,\n+                aggregationArguments,\n+                groupIdSymbol);\n+    }\n }\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\nindex bedd15a5d9e10..cdc974260c6f3 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\n@@ -22,6 +22,7 @@\n import com.facebook.presto.cost.CostComparator;\n import com.facebook.presto.cost.TaskCountEstimator;\n import com.facebook.presto.execution.QueryManagerConfig;\n+import com.facebook.presto.execution.TaskManagerConfig;\n import com.facebook.presto.metadata.InMemoryNodeManager;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.nodeManager.PluginNodeManager;\n@@ -576,7 +577,8 @@ private QueryExplainer getQueryExplainer()\n                 featuresConfig,\n                 new ExpressionOptimizerManager(\n                         new PluginNodeManager(new InMemoryNodeManager()),\n-                        queryRunner.getMetadata().getFunctionAndTypeManager()))\n+                        queryRunner.getMetadata().getFunctionAndTypeManager()),\n+                new TaskManagerConfig())\n                 .getPlanningTimeOptimizers();\n         return new QueryExplainer(\n                 optimizers,\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23999",
    "pr_id": 23999,
    "issue_id": 23998,
    "repo": "prestodb/presto",
    "problem_statement": "Support Google compressed string polylines \n## Expected Behavior or Use Case\r\n\r\nAdd geospatial functions to work with (i.e. transform) the popular Google polyline string encoding as used in Google Maps and other products to and from the existing Presto geometry constructs like ST_Polyline, ST_Point.\r\n\r\nThese are efficiently encoded as per:\r\nhttps://developers.google.com/maps/documentation/utilities/polylinealgorithm\r\n\r\n## Presto Component, Service, or Connector\r\n\r\nGeospatial functions\r\n\r\n## Possible Implementation\r\n\r\nA possible implementation for decoding might look like as suggested in PR:  #23999 , where a google maps string is decoded to an array of ST_Points. Alternatively, we could transform it into an ESRI polyline.\r\n\r\n```\r\npresto> SELECT google_polyline_decode('_p~iF~ps|U_ulLnnqC_mqNvxq`@')\r\n                                _col0\r\n----------------------------------------------------------------------\r\n [POINT (38.5 -120.2), POINT (40.7 -120.95), POINT (43.252 -126.453)]\r\n(1 row)\r\n\r\n```\r\n\r\nA possible implementation for encoding might look like as suggested in PR:  #23999 , where an array of ST_Points is encoded to a google polyline string.\r\n\r\n```\r\npresto> SELECT google_polyline_encode(ARRAY[ST_Point (38.5, -120.2), ST_Point (40.7, -120.95), ST_Point (43.252, -126.453)])\r\n\r\n            _col0\r\n-----------------------------\r\n _p~iF~ps|U_ulLnnqC_mqNvxq`@\r\n(1 row)\r\n\r\n```\r\n\r\n## Context\r\n\r\nThis is an increasingly popular format [1], [2] for storing complex maps routes efficiently.\r\n\r\nFor example routes of Uber trips are efficiently stored using this format. \r\n\r\n[1] https://github.com/frederickjansen/polyline/stargazers\r\n[2] https://github.com/urschrei/pypolyline/stargazers \r\n[3] https://stackoverflow.com/search?q=google+polyline ",
    "issue_word_count": 226,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/geospatial/GeoFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java"
    ],
    "base_commit": "ec60c1377a0a2ff4b2f0ebce6d831b7eca71fddf",
    "head_commit": "fd4a8a5a89b2c9d5dea9a35eb9637a61dc0a9afa",
    "repo_url": "https://github.com/prestodb/presto/pull/23999",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23999",
    "dockerfile": "",
    "pr_merged_at": "2024-11-21T20:24:02.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/geospatial/GeoFunctions.java b/presto-main/src/main/java/com/facebook/presto/geospatial/GeoFunctions.java\nindex 0708941139982..e8e600b865aae 100644\n--- a/presto-main/src/main/java/com/facebook/presto/geospatial/GeoFunctions.java\n+++ b/presto-main/src/main/java/com/facebook/presto/geospatial/GeoFunctions.java\n@@ -28,6 +28,7 @@\n import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.type.IntegerType;\n import com.facebook.presto.common.type.KdbTreeType;\n+import com.facebook.presto.common.type.StandardTypes;\n import com.facebook.presto.geospatial.serde.EsriGeometrySerde;\n import com.facebook.presto.geospatial.serde.GeometrySerializationType;\n import com.facebook.presto.spi.PrestoException;\n@@ -55,6 +56,7 @@\n import org.locationtech.jts.linearref.LengthIndexedLine;\n import org.locationtech.jts.operation.distance.DistanceOp;\n \n+import java.text.StringCharacterIterator;\n import java.util.ArrayList;\n import java.util.EnumSet;\n import java.util.Iterator;\n@@ -133,6 +135,8 @@ public final class GeoFunctions\n             .build();\n     private static final int NUMBER_OF_DIMENSIONS = 3;\n     private static final Block EMPTY_ARRAY_OF_INTS = IntegerType.INTEGER.createFixedSizeBlockBuilder(0).build();\n+    private static final long DEFAULT_POLYLINE_PRECISION_EXPONENT = 5;\n+    private static final long MINIMUM_POLYLINE_PRECISION_EXPONENT = 1;\n \n     private GeoFunctions() {}\n \n@@ -1341,4 +1345,112 @@ public Slice next()\n             }\n         };\n     }\n+\n+    @Description(\"Decodes a Google Polyline string into an array of Points\")\n+    @ScalarFunction(\"google_polyline_decode\")\n+    @SqlType(\"array(\" + GEOMETRY_TYPE_NAME + \")\")\n+    public static Block gMapsPolylineDecode(@SqlType(StandardTypes.VARCHAR) Slice polyline) throws PrestoException\n+    {\n+        return googlePolylineDecodePrecision(polyline, DEFAULT_POLYLINE_PRECISION_EXPONENT);\n+    }\n+\n+    @Description(\"Decodes a Google Polyline string into an array of Points, specifying encoded precision\")\n+    @ScalarFunction(\"google_polyline_decode\")\n+    @SqlType(\"array(\" + GEOMETRY_TYPE_NAME + \")\")\n+    public static Block googlePolylineDecodePrecision(@SqlType(StandardTypes.VARCHAR) Slice polyline, @SqlType(INTEGER) final long precisionExponent) throws PrestoException\n+    {\n+        if (precisionExponent < MINIMUM_POLYLINE_PRECISION_EXPONENT) {\n+            throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Polyline precision must be greater or equal to \" + MINIMUM_POLYLINE_PRECISION_EXPONENT);\n+        }\n+\n+        double precision = Math.pow(10.0, (double) precisionExponent);\n+        List<org.locationtech.jts.geom.Point> points = new ArrayList<>();\n+        int lat = 0;\n+        int lng = 0;\n+\n+        StringCharacterIterator encodedStringIterator = new StringCharacterIterator(polyline.toStringUtf8());\n+        while (encodedStringIterator.current() != StringCharacterIterator.DONE) {\n+            lat += decodeNextDelta(encodedStringIterator);\n+            lng += decodeNextDelta(encodedStringIterator);\n+            points.add(createJtsPoint((double) lat / precision, (double) lng / precision));\n+        }\n+\n+        final BlockBuilder blockBuilder = GEOMETRY.createBlockBuilder(null, points.size());\n+        points.forEach(p -> GEOMETRY.writeSlice(blockBuilder, serialize(p)));\n+        return blockBuilder.build();\n+    }\n+\n+    // implements decoding of the encoding method specified by\n+    // https://developers.google.com/maps/documentation/utilities/polylinealgorithm\n+    private static int decodeNextDelta(StringCharacterIterator encodedStringIterator)\n+    {\n+        int character;\n+        int shift = 0;\n+        int result = 0;\n+\n+        do {\n+            character = encodedStringIterator.current();\n+            if (character == StringCharacterIterator.DONE) {\n+                throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Input is not a valid Google polyline string\");\n+            }\n+            character -= 0x3f;\n+            result |= (character & 0x1f) << shift;\n+            shift += 5;\n+            encodedStringIterator.next();\n+        } while (character >= 0x20);\n+        return ((result & 1) != 0 ? ~(result >> 1) : (result >> 1));\n+    }\n+\n+    @Description(\"Encodes an array of ST_Points into a Google Polyline\")\n+    @ScalarFunction(\"google_polyline_encode\")\n+    @SqlType(VARCHAR)\n+    public static Slice googlePolylineEncode(@SqlType(\"array(\" + GEOMETRY_TYPE_NAME + \")\") Block points) throws PrestoException\n+    {\n+        return googlePolylineEncodePrecision(points, DEFAULT_POLYLINE_PRECISION_EXPONENT);\n+    }\n+\n+    @Description(\"Encodes an array of ST_Points into a Google Polyline, specifying encoded precision\")\n+    @ScalarFunction(\"google_polyline_encode\")\n+    @SqlType(VARCHAR)\n+    public static Slice googlePolylineEncodePrecision(@SqlType(\"array(\" + GEOMETRY_TYPE_NAME + \")\") Block points, @SqlType(INTEGER) final long precisionExponent) throws PrestoException\n+    {\n+        if (precisionExponent < MINIMUM_POLYLINE_PRECISION_EXPONENT) {\n+            throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Polyline precision must be greater or equal to \" + MINIMUM_POLYLINE_PRECISION_EXPONENT);\n+        }\n+        final double precision = Math.pow(10.0, (double) precisionExponent);\n+        final CoordinateSequence coordinateSequence = readPointCoordinates(points, \"gMapsPolylineEncodePrecision\", false);\n+        StringBuilder stringBuilder = new StringBuilder();\n+        long prevX = 0;\n+        long prevY = 0;\n+        for (int i = 0; i < coordinateSequence.size(); i++) {\n+            long x = Math.round(coordinateSequence.getX(i) * precision);\n+            long y = Math.round(coordinateSequence.getY(i) * precision);\n+            if (i == 0) {\n+                encodeNextDelta(x, stringBuilder);\n+                encodeNextDelta(y, stringBuilder);\n+            }\n+            else {\n+                encodeNextDelta(x - prevX, stringBuilder);\n+                encodeNextDelta(y - prevY, stringBuilder);\n+            }\n+            prevX = x;\n+            prevY = y;\n+        }\n+        return utf8Slice(stringBuilder.toString());\n+    }\n+\n+    // implements encoding of the encoding method specified by\n+    // https://developers.google.com/maps/documentation/utilities/polylinealgorithm\n+    private static void encodeNextDelta(long delta, StringBuilder stringBuilder)\n+    {\n+        delta <<= 1;\n+        if (delta < 0) {\n+            delta = ~delta;\n+        }\n+        while (delta >= 0x20) {\n+            stringBuilder.append(Character.toChars((int) (((delta & 0x1f) | 0x20)) + 0x3f));\n+            delta >>= 5;\n+        }\n+        stringBuilder.append(Character.toChars((int) (delta + 0x3f)));\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java b/presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java\nindex 2f51a61a1a404..b651f9188161b 100644\n--- a/presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java\n+++ b/presto-main/src/test/java/com/facebook/presto/geospatial/TestGeoFunctions.java\n@@ -1371,4 +1371,69 @@ private void assertInvalidGeometryJson(String json)\n     {\n         assertInvalidFunction(\"geometry_from_geojson('\" + json + \"')\", \"Invalid GeoJSON:.*\");\n     }\n+\n+    @Test\n+    public void testGooglePolylineDecode()\n+    {\n+        // Google's standard example\n+        assertFunction(\"google_polyline_decode('_p~iF~ps|U_ulLnnqC_mqNvxq`@')\",\n+                new ArrayType(GEOMETRY),\n+                ImmutableList.of(\"POINT (38.5 -120.2)\", \"POINT (40.7 -120.95)\", \"POINT (43.252 -126.453)\"));\n+\n+        /* more complex scenario:\n+            (1) precision in the input floats higher than supported\n+            (2) duplicate points in a row\n+            (3) line crosses back over itself (not very significant, but hey)\n+            (4) line terminates at the origin (non consecutive duplicate points)\n+                37.78327388736858, -122.43876656873093\n+                37.7588492882026, -122.43533334119186\n+                37.76373485345523, -122.41027078015671\n+                37.76780591132951, -122.42537698132858\n+                37.76780591132951, -122.42537698132858\n+                37.76834870211408, -122.45421609265671\n+                37.78327388736858, -122.43876656873093\n+         */\n+        assertFunction(\"google_polyline_decode('mpreFhyhjVrwCoTo]s{CoXl}A??kBfsDg|Aq_B')\",\n+                new ArrayType(GEOMETRY),\n+                ImmutableList.of(\n+                        \"POINT (37.78327 -122.43877)\",\n+                        \"POINT (37.75885 -122.43533)\",\n+                        \"POINT (37.76373 -122.41027)\",\n+                        \"POINT (37.76781 -122.42538)\",\n+                        \"POINT (37.76781 -122.42538)\",\n+                        \"POINT (37.76835 -122.45422)\",\n+                        \"POINT (37.78327 -122.43877)\"));\n+\n+        assertInvalidFunction(\"google_polyline_decode('A')\", INVALID_FUNCTION_ARGUMENT, \"Input is not a valid Google polyline string\");\n+\n+        assertFunction(\"google_polyline_decode('_izlhA~rlgdF_{geC~ywl@_kwzCn`{nI', 6)\",\n+                new ArrayType(GEOMETRY),\n+                ImmutableList.of(\"POINT (38.5 -120.2)\", \"POINT (40.7 -120.95)\", \"POINT (43.252 -126.453)\"));\n+        assertInvalidFunction(\"google_polyline_decode('_p~iF~ps|U_ulLnnqC_mqNvxq`@', 0) \", INVALID_FUNCTION_ARGUMENT, \"Polyline precision must be greater or equal to 1\");\n+    }\n+\n+    @Test\n+    public void testGooglePolylineEncode()\n+    {\n+        // Google's standard example\n+        assertFunction(\"google_polyline_encode(ARRAY[ST_Point(38.5, -120.2), ST_Point(40.7, -120.95), ST_Point(43.252, -126.453)])\",\n+                VARCHAR, \"_p~iF~ps|U_ulLnnqC_mqNvxq`@\");\n+        assertInvalidFunction(\"google_polyline_encode(ARRAY[ST_Point(37.78327, -122.43877)], 0)\", INVALID_FUNCTION_ARGUMENT, \"Polyline precision must be greater or equal to 1\");\n+\n+        // the more complex example in the decode tests\n+        assertFunction(\n+                new StringBuilder(\"google_polyline_encode(\")\n+                        .append(\"ARRAY[\")\n+                        .append(\"ST_Point(37.78327, -122.43877),\")\n+                        .append(\"ST_Point(37.75885, -122.43533),\")\n+                        .append(\"ST_Point(37.76373, -122.41027),\")\n+                        .append(\"ST_Point(37.76781, -122.42538),\")\n+                        .append(\"ST_Point(37.76781, -122.42538),\")\n+                        .append(\"ST_Point(37.76835, -122.45422),\")\n+                        .append(\"ST_Point(37.78327, -122.43877)\")\n+                        .append(\"])\")\n+                        .toString(),\n+                VARCHAR,\n+                \"mpreFhyhjVrwCoTo]s{CoXl}A??kBfsDg|Aq_B\");\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23968",
    "pr_id": 23968,
    "issue_id": 23827,
    "repo": "prestodb/presto",
    "problem_statement": "Skip adding `java-worker` related session properties in a C++ cluster\nWith the proposed addition of https://github.com/prestodb/presto/pull/23045, session properties not relevant to the presto clusters will be hidden. \r\nHowever, in a C++ only cluster we still allow `java-worker` related properties to be loaded. \r\nUsers should be able to skip adding the `java-worker` related session properties in a C++ cluster if needed.",
    "issue_word_count": 69,
    "test_files_count": 5,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/admin/properties.rst",
      "presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java",
      "presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java",
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesExcludingInvalidProperties.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesIncludingInvalidProperties.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesExcludingInvalidProperties.java",
      "presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesIncludingInvalidProperties.java"
    ],
    "base_commit": "1b51466641d82817e80fb63ed15a09e9e8388964",
    "head_commit": "c3dad1589b9bd0264727370e20d8e85c1bf041d2",
    "repo_url": "https://github.com/prestodb/presto/pull/23968",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23968",
    "dockerfile": "",
    "pr_merged_at": "2025-02-05T23:03:09.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/admin/properties.rst b/presto-docs/src/main/sphinx/admin/properties.rst\nindex 0669e89b216b5..3da3ad4ab3570 100644\n--- a/presto-docs/src/main/sphinx/admin/properties.rst\n+++ b/presto-docs/src/main/sphinx/admin/properties.rst\n@@ -82,6 +82,17 @@ be executed within a single node.\n \n The corresponding session property is :ref:`admin/properties-session:\\`\\`single_node_execution_enabled\\`\\``.\n \n+``exclude-invalid-worker-session-properties``\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+* **Type:** ``boolean``\n+* **Default value:** ``false``\n+\n+When ``exclude-invalid-worker-session-properties`` is ``true``, worker session properties that are\n+incompatible with the cluster type are excluded. For example, when ``native-execution-enabled``\n+is ``true``, java-worker only session properties are excluded and the native-worker only\n+session properties are included.\n+\n .. _tuning-memory:\n \n Memory Management Properties\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java b/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\nindex 881588658003e..3071bd0e5d0f3 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/ServerMainModule.java\n@@ -811,9 +811,19 @@ public ListeningExecutorService createResourceManagerExecutor(ResourceManagerCon\n         // Worker session property providers\n         MapBinder<String, WorkerSessionPropertyProvider> mapBinder =\n                 newMapBinder(binder, String.class, WorkerSessionPropertyProvider.class);\n-        mapBinder.addBinding(\"java-worker\").to(JavaWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n-        if (!serverConfig.isCoordinatorSidecarEnabled()) {\n-            mapBinder.addBinding(\"native-worker\").to(NativeWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n+        if (featuresConfig.isNativeExecutionEnabled()) {\n+            if (!serverConfig.isCoordinatorSidecarEnabled()) {\n+                mapBinder.addBinding(\"native-worker\").to(NativeWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n+            }\n+            if (!featuresConfig.isExcludeInvalidWorkerSessionProperties()) {\n+                mapBinder.addBinding(\"java-worker\").to(JavaWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n+            }\n+        }\n+        else {\n+            mapBinder.addBinding(\"java-worker\").to(JavaWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n+            if (!featuresConfig.isExcludeInvalidWorkerSessionProperties()) {\n+                mapBinder.addBinding(\"native-worker\").to(NativeWorkerSessionPropertyProvider.class).in(Scopes.SINGLETON);\n+            }\n         }\n \n         // Node manager binding\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\nindex 0bf25c1fac98d..033c6b26f489d 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/FeaturesConfig.java\n@@ -289,6 +289,8 @@ public class FeaturesConfig\n     private boolean includeValuesNodeInConnectorOptimizer = true;\n \n     private boolean eagerPlanValidationEnabled;\n+\n+    private boolean setExcludeInvalidWorkerSessionProperties;\n     private int eagerPlanValidationThreadPoolSize = 20;\n \n     private boolean prestoSparkExecutionEnvironment;\n@@ -2930,4 +2932,17 @@ public FeaturesConfig setExpressionOptimizerName(String expressionOptimizerName)\n         this.expressionOptimizerName = expressionOptimizerName;\n         return this;\n     }\n+\n+    @Config(\"exclude-invalid-worker-session-properties\")\n+    @ConfigDescription(\"Exclude worker session properties from invalid clusters\")\n+    public FeaturesConfig setExcludeInvalidWorkerSessionProperties(boolean setExcludeInvalidWorkerSessionProperties)\n+    {\n+        this.setExcludeInvalidWorkerSessionProperties = setExcludeInvalidWorkerSessionProperties;\n+        return this;\n+    }\n+\n+    public boolean isExcludeInvalidWorkerSessionProperties()\n+    {\n+        return this.setExcludeInvalidWorkerSessionProperties;\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\nindex e7b407858f6cc..4a07a25abbd04 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/analyzer/TestFeaturesConfig.java\n@@ -252,7 +252,8 @@ public void testDefaults()\n                 .setSingleNodeExecutionEnabled(false)\n                 .setNativeExecutionScaleWritersThreadsEnabled(false)\n                 .setEnhancedCTESchedulingEnabled(true)\n-                .setExpressionOptimizerName(\"default\"));\n+                .setExpressionOptimizerName(\"default\")\n+                .setExcludeInvalidWorkerSessionProperties(false));\n     }\n \n     @Test\n@@ -454,6 +455,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"native-execution-scale-writer-threads-enabled\", \"true\")\n                 .put(\"enhanced-cte-scheduling-enabled\", \"false\")\n                 .put(\"expression-optimizer-name\", \"custom\")\n+                .put(\"exclude-invalid-worker-session-properties\", \"true\")\n                 .build();\n \n         FeaturesConfig expected = new FeaturesConfig()\n@@ -652,7 +654,8 @@ public void testExplicitPropertyMappings()\n                 .setSingleNodeExecutionEnabled(true)\n                 .setNativeExecutionScaleWritersThreadsEnabled(true)\n                 .setEnhancedCTESchedulingEnabled(false)\n-                .setExpressionOptimizerName(\"custom\");\n+                .setExpressionOptimizerName(\"custom\")\n+                .setExcludeInvalidWorkerSessionProperties(true);\n         assertFullMapping(properties, expected);\n     }\n \n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\nindex 13443d5219198..40c999cf9de78 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n@@ -59,6 +59,7 @@ public static Map<String, String> getNativeSidecarProperties()\n     {\n         return ImmutableMap.<String, String>builder()\n                 .put(\"coordinator-sidecar-enabled\", \"true\")\n+                .put(\"exclude-invalid-worker-session-properties\", \"true\")\n                 .put(\"presto.default-namespace\", \"native.default\")\n                 .build();\n     }\n\ndiff --git a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\nindex ba1d134c1167b..91f18f18259db 100644\n--- a/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n+++ b/presto-native-sidecar-plugin/src/test/java/com/facebook/presto/sidecar/TestNativeSidecarPlugin.java\n@@ -85,14 +85,7 @@ public void testShowSession()\n     @Test\n     public void testSetJavaWorkerSessionProperty()\n     {\n-        @Language(\"SQL\") String setSession = \"SET SESSION aggregation_spill_enabled=false\";\n-        MaterializedResult setSessionResult = computeActual(setSession);\n-        assertEquals(\n-                setSessionResult.toString(),\n-                \"MaterializedResult{rows=[[true]], \" +\n-                        \"types=[boolean], \" +\n-                        \"setSessionProperties={aggregation_spill_enabled=false}, \" +\n-                        \"resetSessionProperties=[], updateType=SET SESSION}\");\n+        assertQueryFails(\"SET SESSION aggregation_spill_enabled=false\", \"line 1:1: Session property aggregation_spill_enabled does not exist\");\n     }\n \n     @Test\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesExcludingInvalidProperties.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesExcludingInvalidProperties.java\nnew file mode 100644\nindex 0000000000000..6732acd3c32e2\n--- /dev/null\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesExcludingInvalidProperties.java\n@@ -0,0 +1,56 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.tests;\n+\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.Test;\n+\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestSetWorkerSessionPropertiesExcludingInvalidProperties\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return DistributedQueryRunner.builder(testSessionBuilder().build())\n+                .setSingleCoordinatorProperty(\"exclude-invalid-worker-session-properties\", \"true\")\n+                .build();\n+    }\n+\n+    @Test\n+    public void testSetSessionInvalidNativeWorkerSessionProperty()\n+    {\n+        // SET SESSION on a native-worker session property\n+        assertQueryFails(\"SET SESSION native_expression_max_array_size_in_reduce=50000\", \"line 1:1: Session property native_expression_max_array_size_in_reduce does not exist\");\n+    }\n+\n+    @Test\n+    public void testSetSessionValidJavaWorkerSessionProperty()\n+    {\n+        // SET SESSION on a java-worker session property\n+        @Language(\"SQL\") String setSession = \"SET SESSION distinct_aggregation_spill_enabled=false\";\n+        MaterializedResult setSessionResult = computeActual(setSession);\n+        assertEquals(\n+                setSessionResult.toString(),\n+                \"MaterializedResult{rows=[[true]], \" +\n+                        \"types=[boolean], \" +\n+                        \"setSessionProperties={distinct_aggregation_spill_enabled=false}, \" +\n+                        \"resetSessionProperties=[], updateType=SET SESSION}\");\n+    }\n+}\n\ndiff --git a/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesIncludingInvalidProperties.java b/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesIncludingInvalidProperties.java\nnew file mode 100644\nindex 0000000000000..2e66c59a1e59d\n--- /dev/null\n+++ b/presto-tests/src/test/java/com/facebook/presto/tests/TestSetWorkerSessionPropertiesIncludingInvalidProperties.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.tests;\n+\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.QueryRunner;\n+import org.intellij.lang.annotations.Language;\n+import org.testng.annotations.Test;\n+\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestSetWorkerSessionPropertiesIncludingInvalidProperties\n+        extends AbstractTestQueryFramework\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return DistributedQueryRunner.builder(testSessionBuilder().build()).build();\n+    }\n+\n+    @Test\n+    public void testSetSessionValidNativeWorkerSessionProperty()\n+    {\n+        // SET SESSION on a native-worker session property\n+        @Language(\"SQL\") String setSession = \"SET SESSION native_expression_max_array_size_in_reduce=50000\";\n+        MaterializedResult setSessionResult = computeActual(setSession);\n+        assertEquals(\n+                setSessionResult.toString(),\n+                \"MaterializedResult{rows=[[true]], \" +\n+                        \"types=[boolean], \" +\n+                        \"setSessionProperties={native_expression_max_array_size_in_reduce=50000}, \" +\n+                        \"resetSessionProperties=[], updateType=SET SESSION}\");\n+    }\n+\n+    @Test\n+    public void testSetSessionValidJavaWorkerSessionProperty()\n+    {\n+        // SET SESSION on a java-worker session property\n+        @Language(\"SQL\") String setSession = \"SET SESSION distinct_aggregation_spill_enabled=false\";\n+        MaterializedResult setSessionResult = computeActual(setSession);\n+        assertEquals(\n+                setSessionResult.toString(),\n+                \"MaterializedResult{rows=[[true]], \" +\n+                        \"types=[boolean], \" +\n+                        \"setSessionProperties={distinct_aggregation_spill_enabled=false}, \" +\n+                        \"resetSessionProperties=[], updateType=SET SESSION}\");\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23956",
    "pr_id": 23956,
    "issue_id": 23840,
    "repo": "prestodb/presto",
    "problem_statement": "[parquet] Use deterministic data in parquet.batchreader.decoders.TestValuesDecoders\nCurrently, all of these tests utilize randomness to generate page data for decoder verification. This can introduce test flakiness.\r\n\r\nWe should re-write these tests to use a deterministic set of values which encompasses the possible edge cases but prevents test failures",
    "issue_word_count": 51,
    "test_files_count": 3,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java",
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java",
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java"
    ],
    "pr_changed_test_files": [
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java",
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java",
      "presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java"
    ],
    "base_commit": "e549fa1bd5e88a5a7fe22430b43cb525b980306f",
    "head_commit": "8cc6878529b925638c01c14d85b169b2e87bdc55",
    "repo_url": "https://github.com/prestodb/presto/pull/23956",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23956",
    "dockerfile": "",
    "pr_merged_at": "2024-12-12T15:33:58.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java\nindex d081d227480f0..92fa0986523c7 100644\n--- a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java\n+++ b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestFlatDefinitionLevelDecoder.java\n@@ -21,11 +21,10 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n-import java.util.Random;\n \n import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.addDLRLEBlock;\n import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.addDLValues;\n-import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.randomValues;\n+import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.fillValues;\n import static java.lang.Math.min;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.fail;\n@@ -35,19 +34,18 @@ public class TestFlatDefinitionLevelDecoder\n     private static int valueCount;\n     private static int nonNullCount;\n     private static byte[] pageBytes;\n-    private static List<Integer> expectedValues = new ArrayList<>();\n+    private static final List<Integer> expectedValues = new ArrayList<>();\n \n     @BeforeClass\n     public void setup()\n             throws IOException\n     {\n-        Random random = new Random(200);\n         RunLengthBitPackingHybridEncoder encoder = TestParquetUtils.getSimpleDLEncoder();\n \n         addDLRLEBlock(1, 50, encoder, expectedValues);\n-        addDLValues(randomValues(random, 457, 1), encoder, expectedValues);\n+        addDLValues(fillValues(457, 1), encoder, expectedValues);\n         addDLRLEBlock(0, 37, encoder, expectedValues);\n-        addDLValues(randomValues(random, 186, 1), encoder, expectedValues);\n+        addDLValues(fillValues(186, 1), encoder, expectedValues);\n \n         valueCount = expectedValues.size();\n         for (Integer value : expectedValues) {\n\ndiff --git a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java\nindex 9168934f92133..9d8a6a2818c95 100644\n--- a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java\n+++ b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestParquetUtils.java\n@@ -13,7 +13,6 @@\n  */\n package com.facebook.presto.parquet.batchreader.decoders;\n \n-import org.apache.commons.lang3.RandomStringUtils;\n import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime;\n import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils;\n import org.apache.parquet.bytes.BytesUtils;\n@@ -31,7 +30,6 @@\n import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n-import java.util.Random;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n@@ -80,16 +78,16 @@ public static void addDLValues(Iterator<Integer> values, RunLengthBitPackingHybr\n         }\n     }\n \n-    public static Iterator<Integer> randomValues(Random random, int numValues, int maxValue)\n+    public static Iterator<Integer> fillValues(int numValues, int maxValue)\n     {\n         List<Integer> values = new ArrayList<>();\n         for (int i = 0; i < numValues; i++) {\n-            values.add(random.nextInt(maxValue + 1));\n+            values.add(maxValue);\n         }\n         return values.iterator();\n     }\n \n-    public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, Random random, List<Object> addedValues)\n+    public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, List<Object> addedValues, int valueInt, long valueLong, int positiveUpperBoundedInt)\n     {\n         ValuesWriter writer;\n \n@@ -103,15 +101,14 @@ public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits,\n         switch (valueSizeBits) {\n             case 1: {\n                 for (int i = 0; i < valueCount; i++) {\n-                    int value = random.nextInt(2);\n-                    writer.writeInteger(value);\n-                    addedValues.add(value);\n+                    writer.writeInteger(positiveUpperBoundedInt);\n+                    addedValues.add(positiveUpperBoundedInt);\n                 }\n                 break;\n             }\n             case -1: {\n                 for (int i = 0; i < valueCount; i++) {\n-                    String valueStr = RandomStringUtils.random(random.nextInt(10), 0, 0, true, true, null, random);\n+                    String valueStr = \"4nY\" + valueCount;\n                     byte[] valueUtf8 = valueStr.getBytes(StandardCharsets.UTF_8);\n                     writer.writeBytes(Binary.fromConstantByteArray(valueUtf8, 0, valueUtf8.length));\n                     addedValues.add(valueStr);\n@@ -120,23 +117,21 @@ public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits,\n             }\n             case 32: {\n                 for (int i = 0; i < valueCount; i++) {\n-                    int value = random.nextInt();\n-                    writer.writeInteger(value);\n-                    addedValues.add(value);\n+                    writer.writeInteger(valueInt);\n+                    addedValues.add(valueInt);\n                 }\n                 break;\n             }\n             case 64: {\n                 for (int i = 0; i < valueCount; i++) {\n-                    long value = random.nextLong();\n-                    writer.writeLong(value);\n-                    addedValues.add(value);\n+                    writer.writeLong(valueLong);\n+                    addedValues.add(valueLong);\n                 }\n                 break;\n             }\n             case 96: {\n                 for (int i = 0; i < valueCount; i++) {\n-                    long millisValue = Long.valueOf(random.nextInt(1572281176) * 1000);\n+                    long millisValue = positiveUpperBoundedInt * 1000L;\n                     NanoTime nanoTime = NanoTimeUtils.getNanoTime(new Timestamp(millisValue), false);\n                     writer.writeLong(nanoTime.getTimeOfDayNanos());\n                     writer.writeInteger(nanoTime.getJulianDay());\n@@ -146,12 +141,10 @@ public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits,\n             }\n             case 128:\n                 for (int i = 0; i < valueCount; i++) {\n-                    long value = random.nextLong();\n-                    writer.writeLong(value);\n-                    addedValues.add(value);\n-                    value = random.nextLong();\n-                    writer.writeLong(value);\n-                    addedValues.add(value);\n+                    writer.writeLong(valueLong);\n+                    addedValues.add(valueLong);\n+                    writer.writeLong(valueLong);\n+                    addedValues.add(valueLong);\n                 }\n                 break;\n             default:\n@@ -166,19 +159,19 @@ public static byte[] generatePlainValuesPage(int valueCount, int valueSizeBits,\n         }\n     }\n \n-    public static byte[] generateDictionaryIdPage2048(int maxValue, Random random, List<Integer> addedValues)\n+    public static byte[] generateDictionaryIdPage2048(int maxValue, List<Integer> addedValues, int fillerValue)\n     {\n         RunLengthBitPackingHybridEncoder encoder = getDictionaryDataPageEncoder(maxValue);\n \n         addDLRLEBlock(maxValue / 2, 50, encoder, addedValues);\n-        addDLValues(randomValues(random, 457, maxValue), encoder, addedValues);\n+        addDLValues(fillValues(457, fillerValue), encoder, addedValues);\n         addDLRLEBlock(0, 37, encoder, addedValues);\n-        addDLValues(randomValues(random, 186, maxValue), encoder, addedValues);\n-        addDLValues(randomValues(random, 289, maxValue), encoder, addedValues);\n+        addDLValues(fillValues(186, fillerValue), encoder, addedValues);\n+        addDLValues(fillValues(289, fillerValue), encoder, addedValues);\n         addDLRLEBlock(maxValue - 1, 76, encoder, addedValues);\n-        addDLValues(randomValues(random, 789, maxValue), encoder, addedValues);\n+        addDLValues(fillValues(789, fillerValue), encoder, addedValues);\n         addDLRLEBlock(maxValue - 1, 137, encoder, addedValues);\n-        addDLValues(randomValues(random, 27, maxValue), encoder, addedValues);\n+        addDLValues(fillValues(27, fillerValue), encoder, addedValues);\n \n         checkState(addedValues.size() == 2048);\n \n\ndiff --git a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java\nindex c94a68e8d19e3..e42e9af8987da 100644\n--- a/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java\n+++ b/presto-parquet/src/test/java/com/facebook/presto/parquet/batchreader/decoders/TestValuesDecoders.java\n@@ -52,19 +52,16 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n-import java.util.Random;\n import java.util.stream.Collectors;\n \n import static com.facebook.presto.parquet.ParquetEncoding.PLAIN_DICTIONARY;\n-import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.generateDictionaryIdPage2048;\n-import static com.facebook.presto.parquet.batchreader.decoders.TestParquetUtils.generatePlainValuesPage;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n import static java.lang.Math.min;\n-import static org.apache.parquet.bytes.BytesUtils.UTF8;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.parquet.bytes.BytesUtils.getWidthFromMaxInt;\n import static org.testng.Assert.assertEquals;\n \n-public class TestValuesDecoders\n+public abstract class TestValuesDecoders\n {\n     private static Int32ValuesDecoder int32Plain(byte[] pageBytes)\n     {\n@@ -215,7 +212,7 @@ private static void binaryBatchReadWithSkipHelper(int batchSize, int skipSize, i\n             decoder.readIntoBuffer(byteBuffer, 0, offsets, 0, valueBuffer);\n \n             for (int i = 0; i < readBatchSize; i++) {\n-                byte[] expected = ((String) expectedValues.get(inputOffset + i)).getBytes(UTF8);\n+                byte[] expected = ((String) expectedValues.get(inputOffset + i)).getBytes(UTF_8);\n                 byte[] actual = Arrays.copyOfRange(byteBuffer, offsets[i], offsets[i + 1]);\n                 assertEquals(expected, actual);\n             }\n@@ -365,6 +362,10 @@ private static void booleanBatchReadWithSkipHelper(int batchSize, int skipSize,\n         }\n     }\n \n+    public abstract byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, List<Object> addedValues);\n+\n+    public abstract byte[] generateDictionaryIdPage2048(int maxValue, List<Integer> addedValues);\n+\n     @Test\n     public void testInt32Plain()\n             throws IOException\n@@ -372,7 +373,7 @@ public void testInt32Plain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 32, new Random(89), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 32, expectedValues);\n \n         int32BatchReadWithSkipHelper(valueCount, 0, valueCount, int32Plain(pageBytes), expectedValues); // read all values in one batch\n         int32BatchReadWithSkipHelper(29, 0, valueCount, int32Plain(pageBytes), expectedValues);\n@@ -388,14 +389,13 @@ public void testInt32Plain()\n     public void testInt32RLEDictionary()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 32, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 32, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -421,7 +421,7 @@ public void testBinaryPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, -1, new Random(113), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, -1, expectedValues);\n \n         binaryBatchReadWithSkipHelper(valueCount, 0, valueCount, binaryPlain(pageBytes), expectedValues); // read all values in one batch\n         binaryBatchReadWithSkipHelper(29, 0, valueCount, binaryPlain(pageBytes), expectedValues);\n@@ -437,14 +437,13 @@ public void testBinaryPlain()\n     public void testBinaryRLEDictionary()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = TestParquetUtils.generatePlainValuesPage(dictionarySize, -1, random, dictionary);\n-        byte[] dataPage = TestParquetUtils.generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, -1, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -470,7 +469,7 @@ public void testInt64Plain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 64, new Random(89), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 64, expectedValues);\n \n         int64BatchReadWithSkipHelper(valueCount, 0, valueCount, int64Plain(pageBytes), expectedValues); // read all values in one batch\n         int64BatchReadWithSkipHelper(29, 0, valueCount, int64Plain(pageBytes), expectedValues);\n@@ -496,14 +495,13 @@ public void testInt64Plain()\n     public void testInt64RLEDictionary()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 64, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 64, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -539,7 +537,7 @@ public void testTimestampPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 96, new Random(83), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 96, expectedValues);\n \n         timestampBatchReadWithSkipHelper(valueCount, 0, valueCount, timestampPlain(pageBytes), expectedValues); // read all values in one batch\n         timestampBatchReadWithSkipHelper(29, 0, valueCount, timestampPlain(pageBytes), expectedValues);\n@@ -555,14 +553,13 @@ public void testTimestampPlain()\n     public void testTimestampRLEDictionary()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 96, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 96, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -587,7 +584,7 @@ public void testBooleanPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 1, new Random(83), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 1, expectedValues);\n \n         booleanBatchReadWithSkipHelper(valueCount, 0, valueCount, booleanPlain(pageBytes), expectedValues); // read all values in one batch\n         booleanBatchReadWithSkipHelper(29, 0, valueCount, booleanPlain(pageBytes), expectedValues);\n@@ -602,16 +599,12 @@ public void testBooleanPlain()\n     @Test\n     public void testBooleanRLE()\n     {\n-        Random random = new Random(111);\n         int valueCount = 2048;\n         List<Integer> values = new ArrayList<>();\n \n-        byte[] dataPage = generateDictionaryIdPage2048(1, random, values);\n+        byte[] dataPage = generateDictionaryIdPage2048(1, values);\n \n-        List<Object> expectedValues = new ArrayList<>();\n-        for (Integer value : values) {\n-            expectedValues.add(value.intValue());\n-        }\n+        List<Object> expectedValues = new ArrayList<>(values);\n \n         booleanBatchReadWithSkipHelper(valueCount, 0, valueCount, booleanRLE(dataPage), expectedValues);\n         booleanBatchReadWithSkipHelper(29, 0, valueCount, booleanRLE(dataPage), expectedValues);\n@@ -630,7 +623,7 @@ public void testInt32ShortDecimalPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 32, new Random(83), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 32, expectedValues);\n         int32ShortDecimalBatchReadWithSkipHelper(valueCount, 0, valueCount, int32ShortDecimalPlain(pageBytes), expectedValues); // read all values in one batch\n         int32ShortDecimalBatchReadWithSkipHelper(29, 0, valueCount, int32ShortDecimalPlain(pageBytes), expectedValues);\n         int32ShortDecimalBatchReadWithSkipHelper(89, 0, valueCount, int32ShortDecimalPlain(pageBytes), expectedValues);\n@@ -648,7 +641,7 @@ public void testInt64ShortDecimalPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 64, new Random(83), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 64, expectedValues);\n         int64ShortDecimalBatchReadWithSkipHelper(valueCount, 0, valueCount, int64ShortDecimalPlain(pageBytes), expectedValues); // read all values in one batch\n         int64ShortDecimalBatchReadWithSkipHelper(29, 0, valueCount, int64ShortDecimalPlain(pageBytes), expectedValues);\n         int64ShortDecimalBatchReadWithSkipHelper(89, 0, valueCount, int64ShortDecimalPlain(pageBytes), expectedValues);\n@@ -663,14 +656,13 @@ public void testInt64ShortDecimalPlain()\n     public void testInt32ShortDecimalRLE()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 32, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 32, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -693,14 +685,13 @@ public void testInt32ShortDecimalRLE()\n     public void testInt64ShortDecimalRLE()\n             throws IOException\n     {\n-        Random random = new Random(83);\n         int valueCount = 2048;\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 64, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 64, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -726,7 +717,7 @@ public void testUuidPlainPlain()\n         int valueCount = 2048;\n         List<Object> expectedValues = new ArrayList<>();\n \n-        byte[] pageBytes = generatePlainValuesPage(valueCount, 128, new Random(83), expectedValues);\n+        byte[] pageBytes = generatePlainValuesPage(valueCount, 128, expectedValues);\n         // page is read assuming in big endian, so we need to flip the bytes around when comparing read values\n         expectedValues = expectedValues.stream()\n                 .map(Long.class::cast)\n@@ -749,13 +740,12 @@ public void testUuidRLEDictionary()\n             throws IOException\n     {\n         int valueCount = 2048;\n-        Random random = new Random(83);\n         int dictionarySize = 29;\n         List<Object> dictionary = new ArrayList<>();\n         List<Integer> dictionaryIds = new ArrayList<>();\n \n-        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 128, random, dictionary);\n-        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, random, dictionaryIds);\n+        byte[] dictionaryPage = generatePlainValuesPage(dictionarySize, 128, dictionary);\n+        byte[] dataPage = generateDictionaryIdPage2048(dictionarySize - 1, dictionaryIds);\n \n         List<Object> expectedValues = new ArrayList<>();\n         for (Integer dictionaryId : dictionaryIds) {\n@@ -780,4 +770,84 @@ public void testUuidRLEDictionary()\n         uuidBatchReadWithSkipHelper(89, 29, valueCount, uuidRle(dataPage, dictionarySize, binaryDictionary), expectedValues);\n         uuidBatchReadWithSkipHelper(1024, 1024, valueCount, uuidRle(dataPage, dictionarySize, binaryDictionary), expectedValues);\n     }\n+\n+    public static class TestValueDecodersArbitrary\n+            extends TestValuesDecoders\n+    {\n+        public static final int ARBITRARY_VALUE = 237;\n+\n+        @Override\n+        public byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, List<Object> addedValues)\n+        {\n+            int positiveUpperBoundedInt = getPositiveUpperBoundedInt(valueSizeBits);\n+            return TestParquetUtils.generatePlainValuesPage(valueCount, valueSizeBits, addedValues, ARBITRARY_VALUE, ARBITRARY_VALUE * (1L << 31), positiveUpperBoundedInt);\n+        }\n+\n+        @Override\n+        public byte[] generateDictionaryIdPage2048(int maxValue, List<Integer> addedValues)\n+        {\n+            return TestParquetUtils.generateDictionaryIdPage2048(maxValue, addedValues, ARBITRARY_VALUE % maxValue);\n+        }\n+\n+        private int getPositiveUpperBoundedInt(int valueSizeBits)\n+        {\n+            if (valueSizeBits == 1) {\n+                return ARBITRARY_VALUE % 2;\n+            }\n+            if (valueSizeBits == 96) {\n+                return ARBITRARY_VALUE % 1572281176;\n+            }\n+            return ARBITRARY_VALUE;\n+        }\n+    }\n+\n+    public static class TestValueDecodersLowerBounded\n+            extends TestValuesDecoders\n+    {\n+        @Override\n+        public byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, List<Object> addedValues)\n+        {\n+            return TestParquetUtils.generatePlainValuesPage(valueCount, valueSizeBits, addedValues, Integer.MIN_VALUE, 0L, getPositiveUpperBoundedInt());\n+        }\n+\n+        private int getPositiveUpperBoundedInt()\n+        {\n+            return 0;\n+        }\n+\n+        @Override\n+        public byte[] generateDictionaryIdPage2048(int maxValue, List<Integer> addedValues)\n+        {\n+            return TestParquetUtils.generateDictionaryIdPage2048(maxValue, addedValues, getPositiveUpperBoundedInt());\n+        }\n+    }\n+\n+    public static class TestValueDecodersUpperBounded\n+            extends TestValuesDecoders\n+    {\n+        private static int getPositiveUpperBoundedInt(int valueSizeBits)\n+        {\n+            int positiveUpperBoundedInt = Integer.MAX_VALUE;\n+            if (valueSizeBits == 1) {\n+                positiveUpperBoundedInt = 1;\n+            }\n+            if (valueSizeBits == 96) {\n+                positiveUpperBoundedInt = 1572281175;\n+            }\n+            return positiveUpperBoundedInt;\n+        }\n+\n+        @Override\n+        public byte[] generatePlainValuesPage(int valueCount, int valueSizeBits, List<Object> addedValues)\n+        {\n+            int positiveUpperBoundedInt = getPositiveUpperBoundedInt(valueSizeBits);\n+            return TestParquetUtils.generatePlainValuesPage(valueCount, valueSizeBits, addedValues, Integer.MAX_VALUE, Long.MAX_VALUE, positiveUpperBoundedInt);\n+        }\n+\n+        @Override\n+        public byte[] generateDictionaryIdPage2048(int maxValue, List<Integer> addedValues)\n+        {\n+            return TestParquetUtils.generateDictionaryIdPage2048(maxValue, addedValues, Math.abs(maxValue));\n+        }\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23952",
    "pr_id": 23952,
    "issue_id": 23908,
    "repo": "prestodb/presto",
    "problem_statement": "TestPrestoNativeHiveExternalTableTpchQueriesParquet: Schema not empty: tpch\nTestPrestoNativeHiveExternalTableTpchQueriesParquet test is flaky when running on CI\r\n\r\n```\r\n[ERROR] Failures: \r\n[ERROR]   TestPrestoNativeHiveExternalTableTpchQueriesParquet>AbstractTestNativeHiveExternalTableTpchQueries.tearDown:117->AbstractTestQueryFramework.assertUpdate:260->AbstractTestQueryFramework.assertUpdate:265 ? Runtime Schema not empty: tpch\r\n[INFO] \r\n[ERROR] Tests run: 41, Failures: 1, Errors: 0, Skipped: 0\r\n```\r\n\r\n\r\nIt looks like the `tpch` schema is used elsewhere in other tests running concurrently with `TestPrestoNativeHiveExternalTableTpchQueriesParquet`. We should consider having a dedicated schema for this test if CREATE / DROP is necessary.",
    "issue_word_count": 73,
    "test_files_count": 4,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "pr_changed_test_files": [
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java"
    ],
    "base_commit": "12652e47d9cdb020fdc9d2daa557b839be73c5e3",
    "head_commit": "3cb14054632d58352a9f35d12b8438c34ca986b0",
    "repo_url": "https://github.com/prestodb/presto/pull/23952",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23952",
    "dockerfile": "",
    "pr_merged_at": "2024-11-11T15:32:56.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java\nindex 2e4e0ff6826a5..22a3f2a5c891b 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.nativeworker;\n \n+import com.facebook.presto.Session;\n import com.facebook.presto.hive.HiveType;\n import com.facebook.presto.hive.metastore.Column;\n import com.facebook.presto.testing.QueryRunner;\n@@ -31,7 +32,7 @@\n import java.util.Optional;\n \n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createCustomer;\n-import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitem;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitemStandard;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createNationWithFormat;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createOrders;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createPart;\n@@ -39,19 +40,28 @@\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createRegion;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createSupplier;\n import static com.facebook.presto.nativeworker.PrestoNativeQueryRunnerUtils.createExternalTable;\n+import static com.facebook.presto.nativeworker.PrestoNativeQueryRunnerUtils.createSchemaIfNotExist;\n import static com.facebook.presto.nativeworker.SymlinkManifestGeneratorUtils.cleanupSymlinkData;\n import static com.facebook.presto.tpch.TpchMetadata.getPrestoType;\n+import static java.lang.String.format;\n \n public abstract class AbstractTestNativeHiveExternalTableTpchQueries\n         extends AbstractTestNativeTpchQueries\n {\n     private static final String HIVE = \"hive\";\n-    private static final String TPCH = \"tpch\";\n+    private static final String TPCH_SCHEMA = \"tpch_schema\";\n+    private static final String TPCH_EXTERNAL_SCHEMA = \"tpch_external_schema\";\n     private static final String SYMLINK_FOLDER = \"symlink_tables_manifests\";\n     private static final String HIVE_DATA = \"hive_data\";\n     private static final ImmutableList<String> TPCH_TABLES = ImmutableList.of(\"orders\", \"lineitem\", \"nation\",\n             \"customer\", \"part\", \"partsupp\", \"region\", \"supplier\");\n \n+    @Override\n+    public Session getSession()\n+    {\n+        return Session.builder(super.getSession()).setCatalog(HIVE).setSchema(TPCH_EXTERNAL_SCHEMA).build();\n+    }\n+\n     /**\n      * Returns the Hive type string corresponding to a given TPCH type.\n      * Currently only supports conversion for \"integer\" TPCH type.\n@@ -77,13 +87,17 @@ private static String getHiveTypeString(String tpchType)\n      * @param tableName the name of the TPCH table\n      * @return a list of Column objects representing the columns of the table\n      */\n-    private static List<Column> getTpchTableColumns(String tableName)\n+    private static List<Column> getTpchTableColumns(String tableName, boolean castDateToVarchar)\n     {\n         TpchTable<?> table = TpchTable.getTable(tableName);\n         ColumnNaming columnNaming = ColumnNaming.SIMPLIFIED;\n         ImmutableList.Builder<Column> columns = ImmutableList.builder();\n         for (TpchColumn<? extends TpchEntity> column : table.getColumns()) {\n-            columns.add(new Column(columnNaming.getName(column), HiveType.valueOf(getHiveTypeString(getPrestoType(column).getDisplayName())), Optional.empty(), Optional.empty()));\n+            HiveType hiveType = HiveType.valueOf(getHiveTypeString(getPrestoType(column).getDisplayName()));\n+            if (castDateToVarchar && hiveType.getHiveTypeName().toString().equals(\"date\")) {\n+                hiveType = HiveType.valueOf(\"string\");\n+            }\n+            columns.add(new Column(columnNaming.getName(column), hiveType, Optional.empty(), Optional.empty()));\n         }\n         return columns.build();\n     }\n@@ -92,17 +106,19 @@ private static List<Column> getTpchTableColumns(String tableName)\n     protected void createTables()\n     {\n         QueryRunner javaQueryRunner = (QueryRunner) getExpectedQueryRunner();\n-        createOrders(javaQueryRunner);\n-        createLineitem(javaQueryRunner);\n-        createNationWithFormat(javaQueryRunner, \"PARQUET\");\n-        createCustomer(javaQueryRunner);\n-        createPart(javaQueryRunner);\n-        createPartSupp(javaQueryRunner);\n-        createRegion(javaQueryRunner);\n-        createSupplier(javaQueryRunner);\n+        createSchemaIfNotExist(javaQueryRunner, TPCH_SCHEMA);\n+        Session session = Session.builder(super.getSession()).setCatalog(HIVE).setSchema(TPCH_SCHEMA).build();\n+        createOrders(session, javaQueryRunner, true);\n+        createLineitemStandard(session, javaQueryRunner);\n+        createNationWithFormat(session, javaQueryRunner, \"PARQUET\");\n+        createCustomer(session, javaQueryRunner);\n+        createPart(session, javaQueryRunner);\n+        createPartSupp(session, javaQueryRunner);\n+        createRegion(session, javaQueryRunner);\n+        createSupplier(session, javaQueryRunner);\n \n         for (String tableName : TPCH_TABLES) {\n-            createExternalTable(javaQueryRunner, TPCH, tableName, getTpchTableColumns(tableName));\n+            createExternalTable(javaQueryRunner, TPCH_SCHEMA, tableName, getTpchTableColumns(tableName, true), TPCH_EXTERNAL_SCHEMA);\n         }\n     }\n \n@@ -111,11 +127,12 @@ public void tearDown()\n     {\n         QueryRunner javaQueryRunner = (QueryRunner) getExpectedQueryRunner();\n         for (String tableName : TPCH_TABLES) {\n-            dropTableIfExists(javaQueryRunner, HIVE, TPCH, tableName);\n+            dropTableIfExists(javaQueryRunner, HIVE, TPCH_EXTERNAL_SCHEMA, tableName);\n+            dropTableIfExists(javaQueryRunner, HIVE, TPCH_SCHEMA, tableName);\n         }\n \n-        // https://github.com/prestodb/presto/issues/23908\n-        // assertUpdate(format(\"DROP SCHEMA IF EXISTS %s.%s\", HIVE, TPCH));\n+        assertUpdate(format(\"DROP SCHEMA IF EXISTS %s.%s\", HIVE, TPCH_SCHEMA));\n+        assertUpdate(format(\"DROP SCHEMA IF EXISTS %s.%s\", HIVE, TPCH_EXTERNAL_SCHEMA));\n \n         File dataDirectory = ((DistributedQueryRunner) javaQueryRunner).getCoordinator().getDataDirectory().resolve(HIVE_DATA).toFile();\n         Path symlinkTableDataPath = dataDirectory.toPath().getParent().resolve(SYMLINK_FOLDER);\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java\nindex a4b308efb0650..0879ef620a383 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeIcebergTpchQueries.java\n@@ -16,7 +16,7 @@\n import com.facebook.presto.testing.QueryRunner;\n \n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createCustomer;\n-import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitemForIceberg;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitemStandard;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createNationWithFormat;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createOrders;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createPart;\n@@ -32,7 +32,7 @@ public abstract class AbstractTestNativeIcebergTpchQueries\n     protected void createTables()\n     {\n         QueryRunner queryRunner = (QueryRunner) getExpectedQueryRunner();\n-        createLineitemForIceberg(queryRunner);\n+        createLineitemStandard(queryRunner);\n         createOrders(queryRunner);\n         createNationWithFormat(queryRunner, storageFormat);\n         createCustomer(queryRunner);\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\nindex 1d2711170183d..cfe11534c15fd 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/NativeQueryRunnerUtils.java\n@@ -13,6 +13,7 @@\n  */\n package com.facebook.presto.nativeworker;\n \n+import com.facebook.presto.Session;\n import com.facebook.presto.testing.QueryRunner;\n import com.google.common.collect.ImmutableMap;\n \n@@ -99,7 +100,7 @@ public static void createAllTables(QueryRunner queryRunner, boolean castDateToVa\n      */\n     public static void createAllIcebergTables(QueryRunner queryRunner)\n     {\n-        createLineitemForIceberg(queryRunner);\n+        createLineitemStandard(queryRunner);\n         createOrders(queryRunner);\n         createNationWithFormat(queryRunner, ICEBERG_DEFAULT_STORAGE_FORMAT);\n         createCustomer(queryRunner);\n@@ -131,10 +132,15 @@ public static void createLineitem(QueryRunner queryRunner, boolean castDateToVar\n                 \"FROM tpch.tiny.lineitem\");\n     }\n \n-    public static void createLineitemForIceberg(QueryRunner queryRunner)\n+    public static void createLineitemStandard(QueryRunner queryRunner)\n     {\n-        if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"lineitem\")) {\n-            queryRunner.execute(\"CREATE TABLE lineitem AS \" +\n+        createLineitemStandard(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createLineitemStandard(Session session, QueryRunner queryRunner)\n+    {\n+        if (!queryRunner.tableExists(session, \"lineitem\")) {\n+            queryRunner.execute(session, \"CREATE TABLE lineitem AS \" +\n                     \"SELECT orderkey, partkey, suppkey, linenumber, quantity, extendedprice, discount, tax, \" +\n                     \"   returnflag, linestatus, cast(shipdate as varchar) as shipdate, cast(commitdate as varchar) as commitdate, \" +\n                     \"   cast(receiptdate as varchar) as receiptdate, shipinstruct, shipmode, comment \" +\n@@ -149,9 +155,14 @@ public static void createOrders(QueryRunner queryRunner)\n \n     public static void createOrders(QueryRunner queryRunner, boolean castDateToVarchar)\n     {\n-        queryRunner.execute(\"DROP TABLE IF EXISTS orders\");\n+        createOrders(queryRunner.getDefaultSession(), queryRunner, castDateToVarchar);\n+    }\n+\n+    public static void createOrders(Session session, QueryRunner queryRunner, boolean castDateToVarchar)\n+    {\n+        queryRunner.execute(session, \"DROP TABLE IF EXISTS orders\");\n         String orderDate = castDateToVarchar ? \"cast(orderdate as varchar) as orderdate\" : \"orderdate\";\n-        queryRunner.execute(\"CREATE TABLE orders AS \" +\n+        queryRunner.execute(session, \"CREATE TABLE orders AS \" +\n                 \"SELECT orderkey, custkey, orderstatus, totalprice, \" + orderDate + \", \" +\n                 \"   orderpriority, clerk, shippriority, comment \" +\n                 \"FROM tpch.tiny.orders\");\n@@ -192,20 +203,26 @@ public static void createNation(QueryRunner queryRunner)\n \n     public static void createNationWithFormat(QueryRunner queryRunner, String storageFormat)\n     {\n+        createNationWithFormat(queryRunner.getDefaultSession(), queryRunner, storageFormat);\n+    }\n+\n+    public static void createNationWithFormat(Session session, QueryRunner queryRunner, String storageFormat)\n+    {\n+        queryRunner.execute(session, \"DROP TABLE IF EXISTS nation\");\n         if (storageFormat.equals(\"PARQUET\") && !queryRunner.tableExists(queryRunner.getDefaultSession(), \"nation\")) {\n-            queryRunner.execute(\"CREATE TABLE nation AS SELECT * FROM tpch.tiny.nation\");\n+            queryRunner.execute(session, \"CREATE TABLE nation AS SELECT * FROM tpch.tiny.nation\");\n         }\n \n         if (storageFormat.equals(\"ORC\") && !queryRunner.tableExists(queryRunner.getDefaultSession(), \"nation\")) {\n-            queryRunner.execute(\"CREATE TABLE nation AS SELECT * FROM tpch.tiny.nation\");\n+            queryRunner.execute(session, \"CREATE TABLE nation AS SELECT * FROM tpch.tiny.nation\");\n         }\n \n         if (storageFormat.equals(\"JSON\") && !queryRunner.tableExists(queryRunner.getDefaultSession(), \"nation_json\")) {\n-            queryRunner.execute(\"CREATE TABLE nation_json WITH (FORMAT = 'JSON') AS SELECT * FROM tpch.tiny.nation\");\n+            queryRunner.execute(session, \"CREATE TABLE nation_json WITH (FORMAT = 'JSON') AS SELECT * FROM tpch.tiny.nation\");\n         }\n \n         if (storageFormat.equals(\"TEXTFILE\") && !queryRunner.tableExists(queryRunner.getDefaultSession(), \"nation_text\")) {\n-            queryRunner.execute(\"CREATE TABLE nation_text WITH (FORMAT = 'TEXTFILE') AS SELECT * FROM tpch.tiny.nation\");\n+            queryRunner.execute(session, \"CREATE TABLE nation_text WITH (FORMAT = 'TEXTFILE') AS SELECT * FROM tpch.tiny.nation\");\n         }\n     }\n \n@@ -249,9 +266,14 @@ public static void createPrestoBenchTables(QueryRunner queryRunner)\n     }\n \n     public static void createCustomer(QueryRunner queryRunner)\n+    {\n+        createCustomer(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createCustomer(Session session, QueryRunner queryRunner)\n     {\n         if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"customer\")) {\n-            queryRunner.execute(\"CREATE TABLE customer AS \" +\n+            queryRunner.execute(session, \"CREATE TABLE customer AS \" +\n                     \"SELECT custkey, name, address, nationkey, phone, acctbal, comment, mktsegment \" +\n                     \"FROM tpch.tiny.customer\");\n         }\n@@ -366,23 +388,38 @@ public static void createPrestoBenchCustomer(QueryRunner queryRunner)\n     }\n \n     public static void createPart(QueryRunner queryRunner)\n+    {\n+        createPart(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createPart(Session session, QueryRunner queryRunner)\n     {\n         if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"part\")) {\n-            queryRunner.execute(\"CREATE TABLE part AS SELECT * FROM tpch.tiny.part\");\n+            queryRunner.execute(session, \"CREATE TABLE part AS SELECT * FROM tpch.tiny.part\");\n         }\n     }\n \n     public static void createPartSupp(QueryRunner queryRunner)\n+    {\n+        createPartSupp(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createPartSupp(Session session, QueryRunner queryRunner)\n     {\n         if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"partsupp\")) {\n-            queryRunner.execute(\"CREATE TABLE partsupp AS SELECT * FROM tpch.tiny.partsupp\");\n+            queryRunner.execute(session, \"CREATE TABLE partsupp AS SELECT * FROM tpch.tiny.partsupp\");\n         }\n     }\n \n     public static void createRegion(QueryRunner queryRunner)\n     {\n-        if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"region\")) {\n-            queryRunner.execute(\"CREATE TABLE region AS SELECT * FROM tpch.tiny.region\");\n+        createRegion(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createRegion(Session session, QueryRunner queryRunner)\n+    {\n+        if (!queryRunner.tableExists(session, \"region\")) {\n+            queryRunner.execute(session, \"CREATE TABLE region AS SELECT * FROM tpch.tiny.region\");\n         }\n     }\n \n@@ -403,8 +440,13 @@ public static void createTableToTestHiddenColumns(QueryRunner queryRunner)\n \n     public static void createSupplier(QueryRunner queryRunner)\n     {\n-        if (!queryRunner.tableExists(queryRunner.getDefaultSession(), \"supplier\")) {\n-            queryRunner.execute(\"CREATE TABLE supplier AS SELECT * FROM tpch.tiny.supplier\");\n+        createSupplier(queryRunner.getDefaultSession(), queryRunner);\n+    }\n+\n+    public static void createSupplier(Session session, QueryRunner queryRunner)\n+    {\n+        if (!queryRunner.tableExists(session, \"supplier\")) {\n+            queryRunner.execute(session, \"CREATE TABLE supplier AS SELECT * FROM tpch.tiny.supplier\");\n         }\n     }\n \n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\nindex 534e85f9c1bb2..94f72266d4cc2 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n@@ -180,11 +180,19 @@ public static QueryRunner createJavaQueryRunner(Optional<Path> baseDataDirectory\n         return queryRunner;\n     }\n \n-    public static void createExternalTable(QueryRunner queryRunner, String schemaName, String tableName, List<Column> columns)\n+    public static void createSchemaIfNotExist(QueryRunner queryRunner, String schemaName)\n+    {\n+        ExtendedHiveMetastore metastore = getFileHiveMetastore((DistributedQueryRunner) queryRunner);\n+        if (!metastore.getDatabase(METASTORE_CONTEXT, schemaName).isPresent()) {\n+            metastore.createDatabase(METASTORE_CONTEXT, createDatabaseMetastoreObject(schemaName));\n+        }\n+    }\n+\n+    public static void createExternalTable(QueryRunner queryRunner, String sourceSchemaName, String tableName, List<Column> columns, String targetSchemaName)\n     {\n         ExtendedHiveMetastore metastore = getFileHiveMetastore((DistributedQueryRunner) queryRunner);\n         File dataDirectory = ((DistributedQueryRunner) queryRunner).getCoordinator().getDataDirectory().resolve(HIVE_DATA).toFile();\n-        Path hiveTableDataPath = dataDirectory.toPath().resolve(schemaName).resolve(tableName);\n+        Path hiveTableDataPath = dataDirectory.toPath().resolve(sourceSchemaName).resolve(tableName);\n         Path symlinkTableDataPath = dataDirectory.toPath().getParent().resolve(SYMLINK_FOLDER).resolve(tableName);\n \n         try {\n@@ -194,11 +202,9 @@ public static void createExternalTable(QueryRunner queryRunner, String schemaNam\n             throw new PrestoException(() -> CREATE_ERROR_CODE, \"Failed to create symlink manifest file for table: \" + tableName, e);\n         }\n \n-        if (!metastore.getDatabase(METASTORE_CONTEXT, schemaName).isPresent()) {\n-            metastore.createDatabase(METASTORE_CONTEXT, createDatabaseMetastoreObject(schemaName));\n-        }\n-        if (!metastore.getTable(METASTORE_CONTEXT, schemaName, tableName).isPresent()) {\n-            metastore.createTable(METASTORE_CONTEXT, createHiveSymlinkTable(schemaName, tableName, columns, symlinkTableDataPath.toString()), PRINCIPAL_PRIVILEGES, emptyList());\n+        createSchemaIfNotExist(queryRunner, targetSchemaName);\n+        if (!metastore.getTable(METASTORE_CONTEXT, targetSchemaName, tableName).isPresent()) {\n+            metastore.createTable(METASTORE_CONTEXT, createHiveSymlinkTable(targetSchemaName, tableName, columns, symlinkTableDataPath.toString()), PRINCIPAL_PRIVILEGES, emptyList());\n         }\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23947",
    "pr_id": 23947,
    "issue_id": 23235,
    "repo": "prestodb/presto",
    "problem_statement": "testQueryHeartbeat is flaky\nAnother test that looks fundamentally probabilistic and flaky. We really should stop writing and including these.\r\n\r\n```\r\n2024-07-17T00:52:18.1917691Z [ERROR] Tests run: 8391, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 1,346.493 s <<< FAILURE! - in TestSuite\r\n2024-07-17T00:52:18.1920380Z [ERROR] com.facebook.presto.resourcemanager.TestResourceManagerClusterStatusSender.testQueryHeartbeat  Time elapsed: 1.008 s  <<< FAILURE!\r\n2024-07-17T00:52:18.1923186Z java.lang.AssertionError: Expect number of heartbeats to fall within target range (10), +/- 50%.  Was: 4 expected [true] but found [false]\r\n2024-07-17T00:52:18.1942673Z \tat org.testng.Assert.fail(Assert.java:110)\r\n2024-07-17T00:52:18.1943656Z \tat org.testng.Assert.failNotEquals(Assert.java:1413)\r\n2024-07-17T00:52:18.1944462Z \tat org.testng.Assert.assertTrue(Assert.java:56)\r\n2024-07-17T00:52:18.2040828Z \tat \r\n```",
    "issue_word_count": 139,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java"
    ],
    "base_commit": "0e489de17538910d142ef79aa8cce5206e1f5741",
    "head_commit": "16dacdcbb4ce69dbc2d47bc3bb0bc45ee2dcee3e",
    "repo_url": "https://github.com/prestodb/presto/pull/23947",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23947",
    "dockerfile": "",
    "pr_merged_at": "2024-11-07T16:11:32.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java b/presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java\nindex a3381a2a28ed9..ab28b1bcc895a 100644\n--- a/presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java\n+++ b/presto-main/src/test/java/com/facebook/presto/resourcemanager/TestResourceManagerClusterStatusSender.java\n@@ -112,7 +112,7 @@ public void testNodeStatus()\n                 format(\"Expect number of heartbeats to fall within target range (%s), +/- 50%%.  Was: %s\", TARGET_HEARTBEATS, nodeHeartbeats));\n     }\n \n-    @Test(timeOut = 6_000)\n+    @Test(timeOut = 10_000)\n     public void testQueryHeartbeat()\n             throws Exception\n     {\n@@ -122,8 +122,14 @@ public void testQueryHeartbeat()\n         Thread.sleep(SLEEP_DURATION);\n \n         int queryHeartbeats = resourceManagerClient.getQueryHeartbeats();\n-        assertTrue(queryHeartbeats > TARGET_HEARTBEATS * 0.5 && queryHeartbeats <= TARGET_HEARTBEATS * 1.5,\n-                format(\"Expect number of heartbeats to fall within target range (%s), +/- 50%%.  Was: %s\", TARGET_HEARTBEATS, queryHeartbeats));\n+        assertTrue(queryHeartbeats > 0, \"Expected at least one query heartbeat\");\n+\n+        Thread.sleep(SLEEP_DURATION);\n+\n+        int newQueryHeartbeats = resourceManagerClient.getQueryHeartbeats();\n+        assertTrue(\n+                newQueryHeartbeats > queryHeartbeats,\n+                format(\"Expected at least one subsequent query heartbeat, previous: %s, current: %s\", queryHeartbeats, newQueryHeartbeats));\n \n         // Completing the query stops the heartbeats\n         queryExecution.complete();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23942",
    "pr_id": 23942,
    "issue_id": 23767,
    "repo": "prestodb/presto",
    "problem_statement": "QuickStatsProvider should use ExtendedHiveMetastore\nQuickStatsProvider class uses a passed in `SemiTransactionalHiveMetastore` for the following metastore calls\r\n- `getTable`\r\n- `getPartitionsByNames`\r\n\r\nThe provider builds quick stats in background tasks, so if the primary caller thread is aborted (say due to an [inline build timeout](https://github.com/prestodb/presto/blob/176acba1b57eda39de07f9990cad948035494f9d/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java#L261)), the background build can crash out if the transaction the query was running is completed.\r\n\r\nThis can result in errors like -\r\n```\r\n2024-10-03T07:13:05.270Z\tERROR\tquick-stats-bg-fetch-154\tcom.facebook.presto.hive.statistics.QuickStatsProvider\tError while building quick stats for partition : ss_sold_date_sk=2451530\r\n\r\njava.util.concurrent.ExecutionException: java.lang.IllegalStateException: Tried to access metastore after transaction has been committed/aborted\r\n\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\r\n\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)\r\n\tat com.facebook.presto.hive.statistics.QuickStatsProvider.getQuickStats(QuickStatsProvider.java:261)\r\n\tat com.facebook.presto.hive.statistics.QuickStatsProvider.lambda$getQuickStats$3(QuickStatsProvider.java:164)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)\r\n\tat com.facebook.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:78)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.IllegalStateException: Tried to access metastore after transaction has been committed/aborted\r\n\r\n\tat com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore.checkReadable(SemiTransactionalHiveMetastore.java:1993)\r\n\r\n\tat com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore.getTable(SemiTransactionalHiveMetastore.java:202)\r\n\tat com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore.getTable(SemiTransactionalHiveMetastore.java:197)\r\n\tat com.facebook.presto.hive.statistics.QuickStatsProvider.buildQuickStats(QuickStatsProvider.java:317)\r\n\tat com.facebook.presto.hive.statistics.QuickStatsProvider.lambda$getQuickStats$4(QuickStatsProvider.java:239)\r\n\t... 5 more\r\n```\r\n\r\nThis can result in quick stats never being built for a partition if all queries are short in duration\r\n\r\n## Fix\r\nInstead of passing in a `SemiTransactionalHiveMetastore`, we could instantiate the QuickStatsProvider with a `ExtendedHiveMetastore` in [its binder](https://github.com/prestodb/presto/blob/92b27b6c54bd8f28fb411f59eba0038023730a2f/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java#L380-L397)\r\n\r\n\r\n",
    "issue_word_count": 358,
    "test_files_count": 9,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/statistics/MetastoreHiveStatisticsProvider.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/statistics/ParquetQuickStatsBuilder.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsBuilder.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java"
    ],
    "pr_changed_test_files": [
      "presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java"
    ],
    "base_commit": "1a1fdd8958ec6eb11142ccd070f6ff19f84964db",
    "head_commit": "efebb00f052cff15294359fa3b06fb0b4bed109c",
    "repo_url": "https://github.com/prestodb/presto/pull/23942",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23942",
    "dockerfile": "",
    "pr_merged_at": "2024-11-08T05:03:41.000Z",
    "patch": "diff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java b/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java\nindex c4b7af083a20f..23cd40cb39594 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HiveClientModule.java\n@@ -24,6 +24,7 @@\n import com.facebook.presto.hive.cache.HiveCachingHdfsConfiguration;\n import com.facebook.presto.hive.datasink.DataSinkFactory;\n import com.facebook.presto.hive.datasink.OutputStreamDataSinkFactory;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.HiveMetastoreCacheStats;\n import com.facebook.presto.hive.metastore.HivePartitionMutator;\n import com.facebook.presto.hive.metastore.MetastoreCacheStats;\n@@ -377,7 +378,9 @@ public ParquetMetadataSource createParquetMetadataSource(ParquetCacheConfig parq\n \n     @Singleton\n     @Provides\n-    public QuickStatsProvider createQuickStatsProvider(HdfsEnvironment hdfsEnvironment,\n+    public QuickStatsProvider createQuickStatsProvider(\n+            ExtendedHiveMetastore metastore,\n+            HdfsEnvironment hdfsEnvironment,\n             DirectoryLister directoryLister,\n             HiveClientConfig hiveClientConfig,\n             NamenodeStats nameNodeStats,\n@@ -385,7 +388,8 @@ public QuickStatsProvider createQuickStatsProvider(HdfsEnvironment hdfsEnvironme\n             MBeanExporter exporter)\n     {\n         ParquetQuickStatsBuilder parquetQuickStatsBuilder = new ParquetQuickStatsBuilder(fileFormatDataSourceStats, hdfsEnvironment, hiveClientConfig);\n-        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(hdfsEnvironment,\n+        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(metastore,\n+                hdfsEnvironment,\n                 directoryLister,\n                 hiveClientConfig,\n                 nameNodeStats,\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/MetastoreHiveStatisticsProvider.java b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/MetastoreHiveStatisticsProvider.java\nindex f27ccfc26b9f7..b1d50bd556281 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/MetastoreHiveStatisticsProvider.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/MetastoreHiveStatisticsProvider.java\n@@ -140,7 +140,7 @@ private Map<String, PartitionStatistics> getPartitionsStatistics(ConnectorSessio\n             PartitionStatistics tableStatistics = metastore.getTableStatistics(metastoreContext, table.getSchemaName(), table.getTableName());\n             if (isQuickStatsEnabled(session) &&\n                     (tableStatistics.equals(empty()) || tableStatistics.getColumnStatistics().isEmpty())) {\n-                tableStatistics = quickStatsProvider.getQuickStats(session, metastore, table, metastoreContext, UNPARTITIONED_ID.getPartitionName());\n+                tableStatistics = quickStatsProvider.getQuickStats(session, table, metastoreContext, UNPARTITIONED_ID.getPartitionName());\n             }\n             return ImmutableMap.of(UNPARTITIONED_ID.getPartitionName(), tableStatistics);\n         }\n@@ -156,7 +156,7 @@ private Map<String, PartitionStatistics> getPartitionsStatistics(ConnectorSessio\n                     .map(Map.Entry::getKey)\n                     .collect(toImmutableList());\n \n-            Map<String, PartitionStatistics> partitionQuickStats = quickStatsProvider.getQuickStats(session, metastore, table, metastoreContext, partitionsWithNoStats);\n+            Map<String, PartitionStatistics> partitionQuickStats = quickStatsProvider.getQuickStats(session, table, metastoreContext, partitionsWithNoStats);\n \n             HashMap<String, PartitionStatistics> mergedMap = new HashMap<>(partitionStatistics);\n             mergedMap.putAll(partitionQuickStats);\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/ParquetQuickStatsBuilder.java b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/ParquetQuickStatsBuilder.java\nindex 8ed20d43a54e6..a101f8b8919d7 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/ParquetQuickStatsBuilder.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/ParquetQuickStatsBuilder.java\n@@ -27,9 +27,9 @@\n import com.facebook.presto.hive.HiveFileContext;\n import com.facebook.presto.hive.HiveFileInfo;\n import com.facebook.presto.hive.PartitionNameWithVersion;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n import com.facebook.presto.hive.metastore.Partition;\n-import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n import com.facebook.presto.hive.metastore.StorageFormat;\n import com.facebook.presto.hive.metastore.Table;\n import com.facebook.presto.parquet.ParquetDataSource;\n@@ -283,7 +283,7 @@ public ThreadPoolExecutorMBean getExecutor()\n     }\n \n     @Override\n-    public PartitionQuickStats buildQuickStats(ConnectorSession session, SemiTransactionalHiveMetastore metastore,\n+    public PartitionQuickStats buildQuickStats(ConnectorSession session, ExtendedHiveMetastore metastore,\n             SchemaTableName table, MetastoreContext metastoreContext, String partitionId, Iterator<HiveFileInfo> files)\n     {\n         requireNonNull(session);\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsBuilder.java b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsBuilder.java\nindex e6a4ba06166f2..5cfa7c9f96495 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsBuilder.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsBuilder.java\n@@ -15,8 +15,8 @@\n package com.facebook.presto.hive.statistics;\n \n import com.facebook.presto.hive.HiveFileInfo;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n-import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.spi.SchemaTableName;\n \n@@ -25,6 +25,6 @@\n @FunctionalInterface\n public interface QuickStatsBuilder\n {\n-    PartitionQuickStats buildQuickStats(ConnectorSession session, SemiTransactionalHiveMetastore metastore, SchemaTableName table,\n+    PartitionQuickStats buildQuickStats(ConnectorSession session, ExtendedHiveMetastore metastore, SchemaTableName table,\n             MetastoreContext metastoreContext, String partitionId, Iterator<HiveFileInfo> files);\n }\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java\nindex 65c64bc00d54f..fff00dd53f654 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/statistics/QuickStatsProvider.java\n@@ -28,10 +28,10 @@\n import com.facebook.presto.hive.NamenodeStats;\n import com.facebook.presto.hive.PartitionNameWithVersion;\n import com.facebook.presto.hive.filesystem.ExtendedFileSystem;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n import com.facebook.presto.hive.metastore.Partition;\n import com.facebook.presto.hive.metastore.PartitionStatistics;\n-import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n import com.facebook.presto.hive.metastore.Table;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.spi.SchemaTableName;\n@@ -101,10 +101,16 @@ public class QuickStatsProvider\n     private final Cache<String, PartitionStatistics> partitionToStatsCache;\n     private final NamenodeStats nameNodeStats;\n     private final TimeStat buildDuration = new TimeStat(MILLISECONDS);\n+    private final ExtendedHiveMetastore metastore;\n \n-    public QuickStatsProvider(HdfsEnvironment hdfsEnvironment, DirectoryLister directoryLister, HiveClientConfig hiveClientConfig, NamenodeStats nameNodeStats,\n+    public QuickStatsProvider(ExtendedHiveMetastore metastore,\n+            HdfsEnvironment hdfsEnvironment,\n+            DirectoryLister directoryLister,\n+            HiveClientConfig hiveClientConfig,\n+            NamenodeStats nameNodeStats,\n             List<QuickStatsBuilder> statsBuilderStrategies)\n     {\n+        this.metastore = metastore;\n         this.hdfsEnvironment = hdfsEnvironment;\n         this.directoryLister = directoryLister;\n         this.recursiveDirWalkerEnabled = hiveClientConfig.getRecursiveDirWalkerEnabled();\n@@ -151,7 +157,7 @@ public Map<String, Instant> getInProgressBuildsSnapshot()\n         return inProgressBuilds.entrySet().stream().collect(toImmutableMap(Map.Entry::getKey, v -> v.getValue().getBuildStart()));\n     }\n \n-    public Map<String, PartitionStatistics> getQuickStats(ConnectorSession session, SemiTransactionalHiveMetastore metastore, SchemaTableName table,\n+    public Map<String, PartitionStatistics> getQuickStats(ConnectorSession session, SchemaTableName table,\n             MetastoreContext metastoreContext, List<String> partitionIds)\n     {\n         if (!isQuickStatsEnabled(session)) {\n@@ -161,7 +167,7 @@ public Map<String, PartitionStatistics> getQuickStats(ConnectorSession session,\n         CompletableFuture<PartitionStatistics>[] partitionQuickStatCompletableFutures = new CompletableFuture[partitionIds.size()];\n         for (int counter = 0; counter < partitionIds.size(); counter++) {\n             String partitionId = partitionIds.get(counter);\n-            partitionQuickStatCompletableFutures[counter] = supplyAsync(() -> getQuickStats(session, metastore, table, metastoreContext, partitionId), backgroundFetchExecutor);\n+            partitionQuickStatCompletableFutures[counter] = supplyAsync(() -> getQuickStats(session, table, metastoreContext, partitionId), backgroundFetchExecutor);\n         }\n \n         try {\n@@ -203,7 +209,7 @@ public Map<String, PartitionStatistics> getQuickStats(ConnectorSession session,\n         return result.build();\n     }\n \n-    public PartitionStatistics getQuickStats(ConnectorSession session, SemiTransactionalHiveMetastore metastore, SchemaTableName table,\n+    public PartitionStatistics getQuickStats(ConnectorSession session, SchemaTableName table,\n             MetastoreContext metastoreContext, String partitionId)\n     {\n         if (!isQuickStatsEnabled(session)) {\n@@ -236,7 +242,7 @@ public PartitionStatistics getQuickStats(ConnectorSession session, SemiTransacti\n         // If not, atomically initiate a call to build quick stats in a background thread\n         AtomicReference<CompletableFuture<PartitionStatistics>> partitionStatisticsCompletableFuture = new AtomicReference<>();\n         inProgressBuilds.computeIfAbsent(partitionKey, (key) -> {\n-            CompletableFuture<PartitionStatistics> fetchFuture = supplyAsync(() -> buildQuickStats(partitionKey, partitionId, session, metastore, table, metastoreContext), backgroundFetchExecutor);\n+            CompletableFuture<PartitionStatistics> fetchFuture = supplyAsync(() -> buildQuickStats(partitionKey, partitionId, session, table, metastoreContext), backgroundFetchExecutor);\n             partitionStatisticsCompletableFuture.set(fetchFuture);\n \n             return new InProgressBuildInfo(fetchFuture, Instant.now());\n@@ -282,7 +288,7 @@ public PartitionStatistics getQuickStats(ConnectorSession session, SemiTransacti\n         else {\n             // The quick stats inline fetch was pre-empted by another thread\n             // We get the up-to-date value by calling getQuickStats again\n-            return getQuickStats(session, metastore, table, metastoreContext, partitionId);\n+            return getQuickStats(session, table, metastoreContext, partitionId);\n         }\n     }\n \n@@ -311,7 +317,7 @@ private long getQuickStatsInlineBuildTimeoutMillis(ConnectorSession session)\n         return getQuickStatsInlineBuildTimeout(session).toMillis();\n     }\n \n-    private PartitionStatistics buildQuickStats(String partitionKey, String partitionId, ConnectorSession session, SemiTransactionalHiveMetastore metastore, SchemaTableName table,\n+    private PartitionStatistics buildQuickStats(String partitionKey, String partitionId, ConnectorSession session, SchemaTableName table,\n             MetastoreContext metastoreContext)\n     {\n         Table resolvedTable = metastore.getTable(metastoreContext, table.getSchemaName(), table.getTableName()).get();\n",
    "test_patch": "diff --git a/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java b/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\nindex 7b9e4dc481216..96e94bbac65da 100644\n--- a/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\n+++ b/presto-hive-hadoop2/src/test/java/com/facebook/presto/hive/s3select/S3SelectTestHelper.java\n@@ -176,7 +176,7 @@ public S3SelectTestHelper(String host,\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 columnConverterProvider,\n-                new QuickStatsProvider(hdfsEnvironment, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastoreClient, hdfsEnvironment, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(config));\n         transactionManager = new HiveTransactionManager();\n         splitManager = new HiveSplitManager(\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\nindex c4f1f468b544f..d622e36da67e5 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveClient.java\n@@ -1042,7 +1042,7 @@ protected final void setup(String databaseName, HiveClientConfig hiveClientConfi\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 DEFAULT_COLUMN_CONVERTER_PROVIDER,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastoreClient, HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(false));\n \n         transactionManager = new HiveTransactionManager();\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\nindex a692ba3d9ee44..14aec5fbdb822 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/AbstractTestHiveFileSystem.java\n@@ -224,7 +224,7 @@ protected void setup(String host, int port, String databaseName, BiFunction<Hive\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 columnConverterProvider,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastoreClient, HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(config));\n \n         transactionManager = new HiveTransactionManager();\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\nindex 9276948481c7d..2f71ece88ff5e 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveCommitHandleOutput.java\n@@ -259,7 +259,7 @@ private HiveMetadata getHiveMetadata(TestingExtendedHiveMetastore metastore, Hiv\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastore, HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(false));\n         return hiveMetadataFactory.get();\n     }\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java\nindex ecac18af0e649..e0e5be3b47cd0 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveMetadataFileFormatEncryptionSettings.java\n@@ -140,7 +140,7 @@ public void setup()\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, HiveTestUtils.DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastore, HDFS_ENVIRONMENT, HiveTestUtils.DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(false));\n \n         metastore.createDatabase(METASTORE_CONTEXT, Database.builder()\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\nindex bc4954831cac4..b7f2d0e58408e 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestHiveSplitManager.java\n@@ -499,8 +499,9 @@ private void assertRedundantColumnDomains(Range predicateRange, PartitionStatist\n                 new HiveHdfsConfiguration(new HdfsConfigurationInitializer(hiveClientConfig, new MetastoreClientConfig()), ImmutableSet.of(), hiveClientConfig),\n                 new MetastoreClientConfig(),\n                 new NoHdfsAuthentication());\n+        TestingExtendedHiveMetastore metastore = new TestingExtendedHiveMetastore(TEST_TABLE, partitionWithStatistics);\n         HiveMetadataFactory metadataFactory = new HiveMetadataFactory(\n-                new TestingExtendedHiveMetastore(TEST_TABLE, partitionWithStatistics),\n+                metastore,\n                 hdfsEnvironment,\n                 new HivePartitionManager(FUNCTION_AND_TYPE_MANAGER, hiveClientConfig),\n                 DateTimeZone.forOffsetHours(1),\n@@ -531,7 +532,7 @@ private void assertRedundantColumnDomains(Range predicateRange, PartitionStatist\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastore, HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(false));\n \n         HiveSplitManager splitManager = new HiveSplitManager(\n@@ -646,8 +647,9 @@ public void testEncryptionInformation()\n                 new NoHdfsAuthentication());\n         HiveEncryptionInformationProvider encryptionInformationProvider = new HiveEncryptionInformationProvider(ImmutableList.of(new TestDwrfEncryptionInformationSource()));\n \n+        TestingExtendedHiveMetastore metastore = new TestingExtendedHiveMetastore(testTable, partitionWithStatistics);\n         HiveMetadataFactory metadataFactory = new HiveMetadataFactory(\n-                new TestingExtendedHiveMetastore(testTable, partitionWithStatistics),\n+                metastore,\n                 hdfsEnvironment,\n                 new HivePartitionManager(FUNCTION_AND_TYPE_MANAGER, hiveClientConfig),\n                 DateTimeZone.forOffsetHours(1),\n@@ -678,7 +680,7 @@ public void testEncryptionInformation()\n                 new HivePartitionStats(),\n                 new HiveFileRenamer(),\n                 HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n-                new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n+                new QuickStatsProvider(metastore, HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of()),\n                 new HiveTableWritabilityChecker(false));\n \n         HiveSplitManager splitManager = new HiveSplitManager(\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java\nindex cb828381e7429..b9dcdf6979db1 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestMetastoreHiveStatisticsProvider.java\n@@ -26,6 +26,7 @@\n import com.facebook.presto.hive.OrcFileWriterConfig;\n import com.facebook.presto.hive.ParquetFileWriterConfig;\n import com.facebook.presto.hive.PartitionNameWithVersion;\n+import com.facebook.presto.hive.TestingExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.DateStatistics;\n import com.facebook.presto.hive.metastore.DecimalStatistics;\n import com.facebook.presto.hive.metastore.DoubleStatistics;\n@@ -107,7 +108,7 @@ public class TestMetastoreHiveStatisticsProvider\n     private static final HiveColumnHandle PARTITION_COLUMN_1 = new HiveColumnHandle(\"p1\", HIVE_STRING, VARCHAR.getTypeSignature(), 0, PARTITION_KEY, Optional.empty(), Optional.empty());\n     private static final HiveColumnHandle PARTITION_COLUMN_2 = new HiveColumnHandle(\"p2\", HIVE_LONG, BIGINT.getTypeSignature(), 1, PARTITION_KEY, Optional.empty(), Optional.empty());\n \n-    private static final QuickStatsProvider quickStatsProvider = new QuickStatsProvider(HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of());\n+    private static final QuickStatsProvider quickStatsProvider = new QuickStatsProvider(new TestingExtendedHiveMetastore(), HDFS_ENVIRONMENT, DO_NOTHING_DIRECTORY_LISTER, new HiveClientConfig(), new NamenodeStats(), ImmutableList.of());\n \n     @Test\n     public void testGetPartitionsSample()\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\nindex 91289906c5bc5..2d88a15e49bc7 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestParquetQuickStatsBuilder.java\n@@ -23,11 +23,12 @@\n import com.facebook.presto.hive.HiveFileInfo;\n import com.facebook.presto.hive.HiveHdfsConfiguration;\n import com.facebook.presto.hive.MetastoreClientConfig;\n-import com.facebook.presto.hive.TestingSemiTransactionalHiveMetastore;\n+import com.facebook.presto.hive.TestingExtendedHiveMetastore;\n import com.facebook.presto.hive.authentication.NoHdfsAuthentication;\n import com.facebook.presto.hive.filesystem.ExtendedFileSystem;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n-import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n+import com.facebook.presto.hive.metastore.PrincipalPrivileges;\n import com.facebook.presto.hive.metastore.Storage;\n import com.facebook.presto.hive.metastore.Table;\n import com.facebook.presto.spi.ConnectorSession;\n@@ -36,6 +37,7 @@\n import com.google.common.base.Stopwatch;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableMultimap;\n import com.google.common.collect.ImmutableSet;\n import io.airlift.slice.Slice;\n import io.airlift.units.Duration;\n@@ -80,7 +82,7 @@ public class TestParquetQuickStatsBuilder\n     public static final String TEST_TABLE = \"quick_stats\";\n     private ParquetQuickStatsBuilder parquetQuickStatsBuilder;\n     private MetastoreContext metastoreContext;\n-    private SemiTransactionalHiveMetastore metastore;\n+    private ExtendedHiveMetastore metastore;\n     private HdfsEnvironment hdfsEnvironment;\n     private HiveClientConfig hiveClientConfig;\n     private MetastoreClientConfig metastoreClientConfig;\n@@ -203,10 +205,6 @@ private void setUp()\n                 Optional.empty(),\n                 Optional.empty());\n \n-        TestingSemiTransactionalHiveMetastore mock = TestingSemiTransactionalHiveMetastore.create();\n-        mock.addTable(TEST_SCHEMA, TEST_TABLE, table, ImmutableList.of());\n-        metastore = mock;\n-\n         metastoreContext = new MetastoreContext(SESSION.getUser(),\n                 SESSION.getQueryId(),\n                 Optional.empty(),\n@@ -217,6 +215,10 @@ private void setUp()\n                 HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n                 SESSION.getWarningCollector(),\n                 SESSION.getRuntimeStats());\n+        ExtendedHiveMetastore mock = new TestingExtendedHiveMetastore();\n+        mock.createTable(metastoreContext, table, new PrincipalPrivileges(ImmutableMultimap.of(), ImmutableMultimap.of()), ImmutableList.of());\n+        metastore = mock;\n+\n         hiveClientConfig = new HiveClientConfig();\n         metastoreClientConfig = new MetastoreClientConfig();\n         // Use HiveUtils#createTestHdfsEnvironment to ensure that PrestoS3FileSystem is used for s3a paths\n\ndiff --git a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\nindex 1ca41914026f3..31166b5142ce4 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/statistics/TestQuickStatsProvider.java\n@@ -13,38 +13,28 @@\n  */\n package com.facebook.presto.hive.statistics;\n \n-import com.facebook.presto.hive.ColumnConverterProvider;\n import com.facebook.presto.hive.DirectoryLister;\n-import com.facebook.presto.hive.HdfsConfiguration;\n-import com.facebook.presto.hive.HdfsConfigurationInitializer;\n import com.facebook.presto.hive.HdfsEnvironment;\n import com.facebook.presto.hive.HiveClientConfig;\n-import com.facebook.presto.hive.HiveColumnConverterProvider;\n-import com.facebook.presto.hive.HiveHdfsConfiguration;\n import com.facebook.presto.hive.MetastoreClientConfig;\n import com.facebook.presto.hive.NamenodeStats;\n-import com.facebook.presto.hive.PartitionNameWithVersion;\n-import com.facebook.presto.hive.authentication.NoHdfsAuthentication;\n+import com.facebook.presto.hive.TestingExtendedHiveMetastore;\n import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n-import com.facebook.presto.hive.metastore.HivePartitionMutator;\n import com.facebook.presto.hive.metastore.MetastoreContext;\n import com.facebook.presto.hive.metastore.Partition;\n import com.facebook.presto.hive.metastore.PartitionStatistics;\n-import com.facebook.presto.hive.metastore.SemiTransactionalHiveMetastore;\n+import com.facebook.presto.hive.metastore.PartitionWithStatistics;\n+import com.facebook.presto.hive.metastore.PrincipalPrivileges;\n import com.facebook.presto.hive.metastore.Storage;\n import com.facebook.presto.hive.metastore.Table;\n-import com.facebook.presto.hive.metastore.thrift.BridgingHiveMetastore;\n-import com.facebook.presto.hive.metastore.thrift.HiveCluster;\n-import com.facebook.presto.hive.metastore.thrift.TestingHiveCluster;\n-import com.facebook.presto.hive.metastore.thrift.ThriftHiveMetastore;\n import com.facebook.presto.spi.ConnectorSession;\n import com.facebook.presto.spi.SchemaTableName;\n import com.facebook.presto.spi.session.PropertyMetadata;\n import com.facebook.presto.testing.TestingConnectorSession;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableMultimap;\n import com.google.common.collect.ImmutableSet;\n-import com.google.common.util.concurrent.ListeningExecutorService;\n import io.airlift.units.Duration;\n import org.testng.annotations.BeforeTest;\n import org.testng.annotations.Test;\n@@ -56,10 +46,8 @@\n import java.util.Optional;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n import java.util.concurrent.TimeUnit;\n \n-import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.hive.HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER;\n import static com.facebook.presto.hive.HiveSessionProperties.QUICK_STATS_BACKGROUND_BUILD_TIMEOUT;\n@@ -75,12 +63,9 @@\n import static com.facebook.presto.hive.metastore.StorageFormat.fromHiveStorageFormat;\n import static com.facebook.presto.hive.statistics.PartitionQuickStats.convertToPartitionStatistics;\n import static com.facebook.presto.spi.session.PropertyMetadata.booleanProperty;\n-import static com.google.common.base.Preconditions.checkArgument;\n-import static com.google.common.util.concurrent.MoreExecutors.listeningDecorator;\n import static java.util.Collections.emptyIterator;\n import static java.util.concurrent.CompletableFuture.allOf;\n import static java.util.concurrent.CompletableFuture.supplyAsync;\n-import static java.util.concurrent.Executors.newCachedThreadPool;\n import static java.util.concurrent.ForkJoinPool.commonPool;\n import static java.util.concurrent.TimeUnit.MILLISECONDS;\n import static java.util.concurrent.TimeUnit.SECONDS;\n@@ -130,7 +115,7 @@ public class TestQuickStatsProvider\n     private final HiveClientConfig hiveClientConfig = new HiveClientConfig().setRecursiveDirWalkerEnabled(true);\n     private HdfsEnvironment hdfsEnvironment;\n     private DirectoryLister directoryListerMock;\n-    private SemiTransactionalHiveMetastore metastoreMock;\n+    private ExtendedHiveMetastore metastoreMock;\n     private MetastoreContext metastoreContext;\n     private PartitionQuickStats mockPartitionQuickStats;\n     private PartitionStatistics expectedPartitionStats;\n@@ -190,7 +175,9 @@ public void setUp()\n                 Optional.empty(),\n                 Optional.empty());\n \n-        metastoreMock = MockSemiTransactionalHiveMetastore.create(mockTable, mockPartition);\n+        metastoreMock = new TestingExtendedHiveMetastore();\n+        metastoreMock.createTable(metastoreContext, mockTable, new PrincipalPrivileges(ImmutableMultimap.of(), ImmutableMultimap.of()), ImmutableList.of());\n+        metastoreMock.addPartitions(metastoreContext, TEST_SCHEMA, TEST_TABLE, ImmutableList.of(new PartitionWithStatistics(mockPartition, \"TEST_PARTITION\", empty())));\n \n         directoryListerMock = (fileSystem, table2, path, partition, namenodeStats, hiveDirectoryContext) -> emptyIterator();\n \n@@ -210,12 +197,12 @@ public void testReadThruCaching()\n     {\n         QuickStatsBuilder quickStatsBuilderMock = (session, metastore, table, metastoreContext, partitionId, files) -> mockPartitionQuickStats;\n \n-        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n+        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(metastoreMock, hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n                 ImmutableList.of(quickStatsBuilderMock));\n \n         // Execute\n         ImmutableList<String> testPartitions1 = ImmutableList.of(\"partition1\", \"partition2\", \"partition3\");\n-        Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(SESSION, metastoreMock,\n+        Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(SESSION,\n                 new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions1);\n \n         // Verify only one call was made for each test partition\n@@ -224,14 +211,14 @@ public void testReadThruCaching()\n         quickStats.values().forEach(ps -> assertEquals(ps, expectedPartitionStats));\n \n         // For subsequent calls for the same partitions that are already cached, no new calls are mode to the quick stats builder\n-        quickStatsProvider.getQuickStats(SESSION, metastoreMock,\n+        quickStatsProvider.getQuickStats(SESSION,\n                 new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions1);\n \n         // For subsequent calls with a mix of old and new partitions, we only see calls to the quick stats builder for the new partitions\n         ImmutableList<String> testPartitions2 = ImmutableList.of(\"partition4\", \"partition5\", \"partition6\");\n         ImmutableList<String> testPartitionsMix = ImmutableList.<String>builder().addAll(testPartitions1).addAll(testPartitions2).build();\n \n-        quickStats = quickStatsProvider.getQuickStats(SESSION, metastoreMock,\n+        quickStats = quickStatsProvider.getQuickStats(SESSION,\n                 new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitionsMix);\n         assertEquals(quickStats.entrySet().size(), testPartitionsMix.size());\n         assertTrue(quickStats.keySet().containsAll(testPartitionsMix));\n@@ -254,7 +241,7 @@ public void testConcurrentFetchForSamePartition()\n             return mockPartitionQuickStats;\n         };\n \n-        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n+        QuickStatsProvider quickStatsProvider = new QuickStatsProvider(metastoreMock, hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n                 ImmutableList.of(longRunningQuickStatsBuilderMock));\n \n         List<String> testPartitions = ImmutableList.of(\"partition1\", \"partition2\", \"partition3\");\n@@ -265,10 +252,10 @@ public void testConcurrentFetchForSamePartition()\n \n             ConnectorSession session = getSession(\"600ms\", \"0ms\");\n             // Execute two concurrent calls for the same partitions; wait for them to complete\n-            CompletableFuture<Map<String, PartitionStatistics>> future1 = supplyAsync(() -> quickStatsProvider.getQuickStats(session, metastoreMock,\n+            CompletableFuture<Map<String, PartitionStatistics>> future1 = supplyAsync(() -> quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions), commonPool());\n \n-            CompletableFuture<Map<String, PartitionStatistics>> future2 = supplyAsync(() -> quickStatsProvider.getQuickStats(session, metastoreMock,\n+            CompletableFuture<Map<String, PartitionStatistics>> future2 = supplyAsync(() -> quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions), commonPool());\n \n             allOf(future1, future2).join();\n@@ -300,7 +287,7 @@ else if (partitionStatistics2.equals(empty())) {\n             }\n \n             // Future calls for the same partitions will return from cached partition stats with valid values\n-            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session, metastoreMock,\n+            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions);\n \n             // Verify only one call was made for each test partition\n@@ -315,10 +302,10 @@ else if (partitionStatistics2.equals(empty())) {\n \n             ConnectorSession session = getSession(\"300ms\", \"300ms\");\n             // Execute two concurrent calls for the same partitions; wait for them to complete\n-            CompletableFuture<Map<String, PartitionStatistics>> future1 = supplyAsync(() -> quickStatsProvider.getQuickStats(session, metastoreMock,\n+            CompletableFuture<Map<String, PartitionStatistics>> future1 = supplyAsync(() -> quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions), commonPool());\n \n-            CompletableFuture<Map<String, PartitionStatistics>> future2 = supplyAsync(() -> quickStatsProvider.getQuickStats(session, metastoreMock,\n+            CompletableFuture<Map<String, PartitionStatistics>> future2 = supplyAsync(() -> quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions), commonPool());\n \n             allOf(future1, future2).join();\n@@ -350,12 +337,12 @@ public void quickStatsBuildTimeIsBounded()\n         };\n \n         {\n-            QuickStatsProvider quickStatsProvider = new QuickStatsProvider(hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n+            QuickStatsProvider quickStatsProvider = new QuickStatsProvider(metastoreMock, hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n                     ImmutableList.of(longRunningQuickStatsBuilderMock));\n             // Create a session where an inline build will occur for any newly requested partition\n             ConnectorSession session = getSession(\"300ms\", \"0ms\");\n             List<String> testPartitions = ImmutableList.copyOf(mockPerPartitionStatsFetchTimes.keySet());\n-            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session, metastoreMock,\n+            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, testPartitions);\n             Map<String, Instant> inProgressBuildsSnapshot = quickStatsProvider.getInProgressBuildsSnapshot();\n \n@@ -376,10 +363,10 @@ public void quickStatsBuildTimeIsBounded()\n             // Create a session where no inline builds will occur for any requested partition; empty() quick stats will be returned\n             ConnectorSession session = getSession(\"0ms\", \"0ms\");\n \n-            QuickStatsProvider quickStatsProvider = new QuickStatsProvider(hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n+            QuickStatsProvider quickStatsProvider = new QuickStatsProvider(metastoreMock, hdfsEnvironment, directoryListerMock, hiveClientConfig, new NamenodeStats(),\n                     ImmutableList.of((session1, metastore, table, metastoreContext, partitionId, files) -> mockPartitionQuickStats));\n \n-            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session, metastoreMock,\n+            Map<String, PartitionStatistics> quickStats = quickStatsProvider.getQuickStats(session,\n                     new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, ImmutableList.of(\"p5\", \"p6\"));\n \n             assertEquals(quickStats.size(), 2);\n@@ -392,7 +379,7 @@ public void quickStatsBuildTimeIsBounded()\n                     .maxAttempts(10)\n                     .exponentialBackoff(new Duration(20D, MILLISECONDS), new Duration(500D, MILLISECONDS), new Duration(2, SECONDS), 2.0)\n                     .run(\"waitForQuickStatsBuild\", () -> {\n-                        Map<String, PartitionStatistics> quickStatsAfter = quickStatsProvider.getQuickStats(session, metastoreMock,\n+                        Map<String, PartitionStatistics> quickStatsAfter = quickStatsProvider.getQuickStats(session,\n                                 new SchemaTableName(TEST_SCHEMA, TEST_TABLE), metastoreContext, ImmutableList.of(\"p5\", \"p6\"));\n \n                         try {\n@@ -407,61 +394,4 @@ public void quickStatsBuildTimeIsBounded()\n                     });\n         }\n     }\n-\n-    public static class MockSemiTransactionalHiveMetastore\n-            extends SemiTransactionalHiveMetastore\n-    {\n-        private final Table mockTable;\n-        private final Partition mockPartition;\n-\n-        private MockSemiTransactionalHiveMetastore(HdfsEnvironment hdfsEnvironment,\n-                ExtendedHiveMetastore delegate,\n-                ListeningExecutorService renameExecutor,\n-                boolean skipDeletionForAlter,\n-                boolean skipTargetCleanupOnRollback,\n-                boolean undoMetastoreOperationsEnabled,\n-                ColumnConverterProvider columnConverterProvider,\n-                Table mockTable, Partition mockPartition)\n-        {\n-            super(hdfsEnvironment, delegate, renameExecutor, skipDeletionForAlter, skipTargetCleanupOnRollback, undoMetastoreOperationsEnabled, columnConverterProvider);\n-            this.mockPartition = mockPartition;\n-            this.mockTable = mockTable;\n-        }\n-\n-        public static MockSemiTransactionalHiveMetastore create(Table mockTable, Partition mockPartition)\n-        {\n-            // none of these values matter, as we never use them\n-            HiveClientConfig config = new HiveClientConfig();\n-            MetastoreClientConfig metastoreClientConfig = new MetastoreClientConfig();\n-            HdfsConfiguration hdfsConfiguration = new HiveHdfsConfiguration(new HdfsConfigurationInitializer(config, metastoreClientConfig), ImmutableSet.of(), config);\n-            HdfsEnvironment hdfsEnvironment = new HdfsEnvironment(hdfsConfiguration, metastoreClientConfig, new NoHdfsAuthentication());\n-            HiveCluster hiveCluster = new TestingHiveCluster(metastoreClientConfig, \"dummy\", 1000);\n-            ColumnConverterProvider columnConverterProvider = HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER;\n-            ExtendedHiveMetastore delegate = new BridgingHiveMetastore(new ThriftHiveMetastore(hiveCluster, metastoreClientConfig, hdfsEnvironment), new HivePartitionMutator());\n-            ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(\"hive-%s\"));\n-            ListeningExecutorService renameExecutor = listeningDecorator(executor);\n-\n-            return new MockSemiTransactionalHiveMetastore(hdfsEnvironment, delegate, renameExecutor, false, false, true,\n-                    columnConverterProvider, mockTable, mockPartition);\n-        }\n-\n-        @Override\n-        public synchronized Optional<Partition> getPartition(MetastoreContext metastoreContext, String databaseName, String tableName, List<String> partitionValues)\n-        {\n-            return Optional.of(mockPartition);\n-        }\n-\n-        @Override\n-        public synchronized Map<String, Optional<Partition>> getPartitionsByNames(MetastoreContext metastoreContext, String databaseName, String tableName, List<PartitionNameWithVersion> partitionNames)\n-        {\n-            checkArgument(partitionNames.size() == 1, \"Expected caller to only pass in a single partition to fetch\");\n-            return ImmutableMap.of(partitionNames.get(0).getPartitionName(), Optional.of(mockPartition));\n-        }\n-\n-        @Override\n-        public Optional<Table> getTable(MetastoreContext metastoreContext, String databaseName, String tableName)\n-        {\n-            return Optional.of(mockTable);\n-        }\n-    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23890",
    "pr_id": 23890,
    "issue_id": 23887,
    "repo": "prestodb/presto",
    "problem_statement": "array_intersect returns non-deterministic result for different orders\nFor single parameter array<array<T>>, the [implementation](https://github.com/prestodb/presto/blob/52cdfff12cfc8acba704d376bdcc1f343da7056d/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java#L71) is order dependent and will give different result for the same set of elements.\r\n\r\nExample\r\n```\r\nSELECT array_intersect(ARRAY[null, ARRAY[1], null, ARRAY[1]])  \r\n```\r\nresult [1]\r\n\r\n```\r\nSELECT array_intersect(ARRAY[null, ARRAY[1], ARRAY[1], null])  \r\n```\r\nresult null\r\n",
    "issue_word_count": 73,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java"
    ],
    "base_commit": "1c0fc175a73f60ababb6fba26472c3d5df7ebadf",
    "head_commit": "8c1e3ea9a3b595135b721080b016b55fba291349",
    "repo_url": "https://github.com/prestodb/presto/pull/23890",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23890",
    "dockerfile": "",
    "pr_merged_at": "2024-10-29T23:45:40.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java\nindex b9c7b1268f306..d977420d28339 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArrayIntersectFunction.java\n@@ -68,6 +68,6 @@ public static Block intersect(\n     @SqlType(\"array<T>\")\n     public static String arrayIntersectArray()\n     {\n-        return \"RETURN reduce(input, null, (s, x) -> IF((s IS NULL), x, array_intersect(s, x)), (s) -> s)\";\n+        return \"RETURN reduce(input, IF((cardinality(input) = 0), ARRAY[], input[1]), (s, x) -> array_intersect(s, x), (s) -> s)\";\n     }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java\nindex 528eb9521144f..7a77020585434 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestArrayIntersectFunction.java\n@@ -215,6 +215,10 @@ public void testDuplicates()\n     public void testSqlFunctions()\n     {\n         assertFunction(\"array_intersect(ARRAY[ARRAY[1, 3, 5], ARRAY[2, 3, 5], ARRAY[3, 3, 3, 6]])\", new ArrayType(INTEGER), ImmutableList.of(3));\n+        assertFunction(\"array_intersect(ARRAY[null, ARRAY[], ARRAY[1, 2, 3]])\", new ArrayType(INTEGER), null);\n+        assertFunction(\"array_intersect(ARRAY[ARRAY[], null, ARRAY[1, 2, 3]])\", new ArrayType(INTEGER), null);\n+        assertFunction(\"array_intersect(ARRAY[])\", new ArrayType(UNKNOWN), ImmutableList.of());\n+        assertFunction(\"array_intersect(null)\", new ArrayType(UNKNOWN), null);\n         assertFunction(\"array_intersect(ARRAY[ARRAY[], ARRAY[1, 2, 3]])\", new ArrayType(INTEGER), ImmutableList.of());\n         assertFunction(\"array_intersect(ARRAY[ARRAY[1, 2, 3], null])\", new ArrayType(INTEGER), null);\n         assertFunction(\"array_intersect(ARRAY[ARRAY[DOUBLE'1.1', DOUBLE'2.2', DOUBLE'3.3'], ARRAY[DOUBLE'1.1', DOUBLE'3.4'], ARRAY[DOUBLE'1.0', DOUBLE'1.1', DOUBLE'1.2']])\", new ArrayType(DOUBLE), ImmutableList.of(1.1));\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java\nindex 5e02f43295a40..05d82512c4777 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestNanQueries.java\n@@ -46,6 +46,10 @@ public abstract class AbstractTestNanQueries\n     public static final String SIMPLE_DOUBLE_ARRAY_COLUMN = \"simple_double_array\";\n     public static final String SIMPLE_REAL_ARRAY_COLUMN = \"simple_real_array\";\n \n+    public static final String ARRAY_TABLE_NAME_NO_NULL = \"array_nans_table_no_null\";\n+    public static final String SIMPLE_DOUBLE_ARRAY_COLUMN_NO_NULL = \"simple_double_array_no_null\";\n+    public static final String SIMPLE_REAL_ARRAY_COLUMN_NO_NULL = \"simple_real_array_no_null\";\n+\n     public static final String MAP_TABLE_NAME = \"map_nans_table\";\n     public static final String DOUBLE_MAP_COLUMN = \"double_map\";\n     public static final String REAL_MAP_COLUMN = \"real_map\";\n@@ -96,7 +100,19 @@ public void setup()\n                 \"(ARRAY[DOUBLE '0', DOUBLE '1', nan(), DOUBLE '-1', nan(), DOUBLE '1', DOUBLE '1', DOUBLE'0'], ARRAY [REAL '0', REAL '1', CAST(nan() AS REAL), REAL '-1', CAST(nan() AS REAL), REAL '1', REAL '1', REAL '0'])) \" +\n                 \"AS t (\" + SIMPLE_DOUBLE_ARRAY_COLUMN + \", \" + SIMPLE_REAL_ARRAY_COLUMN + \")\";\n \n+        @Language(\"SQL\") String createArrayTableNoNullQuery = \"\" +\n+                \"CREATE TABLE \" + ARRAY_TABLE_NAME_NO_NULL + \" AS \" +\n+                \"SELECT * FROM (VALUES \" +\n+                \"(ARRAY[nan(), DOUBLE '0', DOUBLE '1', DOUBLE '-1'], ARRAY[cast(nan() AS REAL), REAL '0', REAL '1', REAL '-1']), \" +\n+                \"(ARRAY[ DOUBLE '0', nan(), DOUBLE '1', DOUBLE '-1'], ARRAY[REAL '0', CAST(nan() AS REAL),  REAL '1', REAL '-1']), \" +\n+                \"(ARRAY[ DOUBLE '0',  DOUBLE '1', DOUBLE '-1', nan()], ARRAY[REAL '0', REAL '1', REAL '-1',  CAST(nan() AS REAL)]), \" +\n+                \"(ARRAY[null, nan(), DOUBLE '200'], ARRAY[null, CAST(nan() AS REAL), REAL '200']), \" +\n+                \"(ARRAY[nan(), nan()], ARRAY[CAST(nan() AS REAL), CAST(nan() AS REAL)]), \" +\n+                \"(ARRAY[DOUBLE '0', DOUBLE '1', nan(), DOUBLE '-1', nan(), DOUBLE '1', DOUBLE '1', DOUBLE'0'], ARRAY [REAL '0', REAL '1', CAST(nan() AS REAL), REAL '-1', CAST(nan() AS REAL), REAL '1', REAL '1', REAL '0'])) \" +\n+                \"AS t (\" + SIMPLE_DOUBLE_ARRAY_COLUMN_NO_NULL + \", \" + SIMPLE_REAL_ARRAY_COLUMN_NO_NULL + \")\";\n+\n         assertUpdate(createArrayTableQuery, 7);\n+        assertUpdate(createArrayTableNoNullQuery, 6);\n \n         @Language(\"SQL\") String createMapTableQuery = \"\" +\n                 \"CREATE TABLE \" + MAP_TABLE_NAME + \" AS \" +\n@@ -728,6 +744,9 @@ public void testDoubleArrayIntersect2()\n         // Test the array of arrays function signature\n         assertQueryWithSameQueryRunner(\n                 format(\"SELECT array_sort(array_intersect(array_agg(%s))) FROM %s\", SIMPLE_DOUBLE_ARRAY_COLUMN, ARRAY_TABLE_NAME),\n+                \"SELECT NULL\");\n+        assertQueryWithSameQueryRunner(\n+                format(\"SELECT array_sort(array_intersect(array_agg(%s))) FROM %s\", SIMPLE_DOUBLE_ARRAY_COLUMN_NO_NULL, ARRAY_TABLE_NAME_NO_NULL),\n                 \"SELECT * FROM (VALUES (ARRAY[nan()]))\");\n     }\n \n@@ -737,6 +756,9 @@ public void testRealArrayIntersect2()\n         // Test the array of arrays function signature\n         assertQueryWithSameQueryRunner(\n                 format(\"SELECT array_sort(array_intersect(array_agg(%s))) FROM %s\", SIMPLE_REAL_ARRAY_COLUMN, ARRAY_TABLE_NAME),\n+                \"SELECT NULL\");\n+        assertQueryWithSameQueryRunner(\n+                format(\"SELECT array_sort(array_intersect(array_agg(%s))) FROM %s\", SIMPLE_REAL_ARRAY_COLUMN_NO_NULL, ARRAY_TABLE_NAME_NO_NULL),\n                 \"SELECT * FROM (VALUES (ARRAY[CAST(nan() AS REAL)]))\");\n     }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23884",
    "pr_id": 23884,
    "issue_id": 23833,
    "repo": "prestodb/presto",
    "problem_statement": "Error 'The scope is invalid' when connecting to Polaris using OAuth2 credentials\nWhen we configure the `iceberg.rest.auth.oauth2.credential` property to connect to a Polaris catalog we get the error `The scope is invalid`. \r\nSince the Presto code does not set the 'scope' property, the Iceberg code is using 'catalog' as scope and the OAuth2 endpoint is considering it an invalid scope.\r\n\r\n## Your Environment\r\n* Presto  0.289\r\n* Storage S3\r\n* Connector used: Iceberg connector with Rest Catalog\r\n* Trace:\r\n```\r\nCaused by: org.apache.iceberg.exceptions.BadRequestException: Malformed request: invalid_scope: The scope is invalid\r\n    at org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:254)\r\n    at org.apache.iceberg.rest.ErrorHandlers$OAuthErrorHandler.accept(ErrorHandlers.java:228)\r\n    at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:201)\r\n    at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:313)\r\n    at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:252)\r\n    at org.apache.iceberg.rest.HTTPClient.postForm(HTTPClient.java:399)\r\n    at org.apache.iceberg.rest.auth.OAuth2Util.fetchToken(OAuth2Util.java:216)\r\n    at org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:189)\r\n    at org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:78)\r\n    at org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)\r\n    at com.facebook.presto.iceberg.IcebergNativeCatalogFactory.lambda$getCatalog$0(IcebergNativeCatalogFactory.java:83)\r\n```\r\n\r\n## Expected Behavior\r\nThe request to obtain the OAuth2 token should be valid.\r\n\r\n## Current Behavior\r\nThe following error is obtained:\r\n`  Query failed (#20241015_085300_00005_qxhdt): org.apache.iceberg.exceptions.BadRequestException: Malformed request: invalid_scope: The scope is invalid`\r\n\r\n\r\n## Possible Solution\r\nA new configuration property to set the scope\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\nThis is the properties file of the Iceberg catalog:\r\n```\r\nconnector.name=iceberg\r\niceberg.catalog.type=rest\r\niceberg.rest.uri=<polaris URI>\r\niceberg.rest.auth.type=OAUTH2\r\niceberg.rest.auth.oauth2.credential=<key:secret>\r\niceberg.catalog.warehouse=<catalog name>\r\n```\r\n\r\n\r\n## Context\r\nWe cannot connect to the Polaris catalog using the `iceberg.rest.auth.oauth2.credential` property.\r\nAlternatively, we can connect to Polaris with `iceberg.rest.auth.oauth2.token`, but without the token refresh mechanism it is a bit cumbersome.\r\n\r\n",
    "issue_word_count": 354,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java"
    ],
    "base_commit": "c8a90e5f492c7323280c7b4f9c3c07323e531913",
    "head_commit": "8516cfcc08f7d28c55a5336f12f5998b11aec349",
    "repo_url": "https://github.com/prestodb/presto/pull/23884",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23884",
    "dockerfile": "",
    "pr_merged_at": "2024-10-24T17:13:21.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 55eb20acf9281..c29fb5be18fdb 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -219,6 +219,11 @@ Property Name                                        Description\n ``iceberg.rest.auth.oauth2.token``                   The Bearer token to use for OAUTH2 authentication.\n                                                      Example: ``SXVLUXUhIExFQ0tFUiEK``\n \n+``iceberg.rest.auth.oauth2.scope``                   The scope to use for OAUTH2 authentication.\n+                                                     This property is only applicable when using\n+                                                     ``iceberg.rest.auth.oauth2.credential``.\n+                                                     Example: ``PRINCIPAL_ROLE:ALL``\n+\n ``iceberg.rest.session.type``                        The session type to use when communicating with the REST catalog.\n                                                      Available values are ``NONE`` or ``USER`` (default: ``NONE``).\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\nindex 4a8be67c1c9db..ca363e793ace3 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\n@@ -50,6 +50,7 @@\n import static org.apache.iceberg.rest.auth.OAuth2Properties.CREDENTIAL;\n import static org.apache.iceberg.rest.auth.OAuth2Properties.JWT_TOKEN_TYPE;\n import static org.apache.iceberg.rest.auth.OAuth2Properties.OAUTH2_SERVER_URI;\n+import static org.apache.iceberg.rest.auth.OAuth2Properties.SCOPE;\n import static org.apache.iceberg.rest.auth.OAuth2Properties.TOKEN;\n \n public class IcebergRestCatalogFactory\n@@ -124,6 +125,7 @@ protected Map<String, String> getCatalogProperties(ConnectorSession session)\n                 }\n                 catalogConfig.getCredential().ifPresent(credential -> properties.put(CREDENTIAL, credential));\n                 catalogConfig.getToken().ifPresent(token -> properties.put(TOKEN, token));\n+                catalogConfig.getScope().ifPresent(scope -> properties.put(SCOPE, scope));\n             }\n         });\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\nindex fe0d1a5522cb3..e46dfda2109c9 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\n@@ -28,6 +28,7 @@ public class IcebergRestConfig\n     private String authenticationServerUri;\n     private String credential;\n     private String token;\n+    private String scope;\n \n     @NotNull\n     public Optional<String> getServerUri()\n@@ -108,6 +109,19 @@ public IcebergRestConfig setToken(String token)\n         return this;\n     }\n \n+    public Optional<String> getScope()\n+    {\n+        return Optional.ofNullable(scope);\n+    }\n+\n+    @Config(\"iceberg.rest.auth.oauth2.scope\")\n+    @ConfigDescription(\"The scope to use for OAUTH2 authentication\")\n+    public IcebergRestConfig setScope(String scope)\n+    {\n+        this.scope = scope;\n+        return this;\n+    }\n+\n     public boolean credentialOrTokenExists()\n     {\n         return credential != null || token != null;\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\nindex a332a8f6e6fc0..e21f70992e57c 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\n@@ -35,6 +35,7 @@ public void testDefaults()\n                 .setAuthenticationServerUri(null)\n                 .setCredential(null)\n                 .setToken(null)\n+                .setScope(null)\n                 .setSessionType(null));\n     }\n \n@@ -47,6 +48,7 @@ public void testExplicitPropertyMappings()\n                 .put(\"iceberg.rest.auth.oauth2.uri\", \"http://localhost:yyy\")\n                 .put(\"iceberg.rest.auth.oauth2.credential\", \"key:secret\")\n                 .put(\"iceberg.rest.auth.oauth2.token\", \"SXVLUXUhIExFQ0tFUiEK\")\n+                .put(\"iceberg.rest.auth.oauth2.scope\", \"PRINCIPAL_ROLE:ALL\")\n                 .put(\"iceberg.rest.session.type\", \"USER\")\n                 .build();\n \n@@ -56,6 +58,7 @@ public void testExplicitPropertyMappings()\n                 .setAuthenticationServerUri(\"http://localhost:yyy\")\n                 .setCredential(\"key:secret\")\n                 .setToken(\"SXVLUXUhIExFQ0tFUiEK\")\n+                .setScope(\"PRINCIPAL_ROLE:ALL\")\n                 .setSessionType(USER);\n \n         assertFullMapping(properties, expected);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23882",
    "pr_id": 23882,
    "issue_id": 23881,
    "repo": "prestodb/presto",
    "problem_statement": "Unauthorized User Can Execute USE Command on Schema or Catalog\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nA user with any role who has not been granted access to a specific catalog or schema is still able to execute the USE <schema> command. The expected behavior is that such users should receive an \"Access Denied\" error when attempting to access a catalog or schema for which they lack permission.\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: Latest\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nUsers should receive an \"Access Denied\" error when accessing a catalog or schema without permission.\r\n\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\nThe USE command succeeds without an error.\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Add a user with the User role\r\n2. Do not grant user1 access to the catalog.\r\n3. Connect to the catalog using user1 in Presto:\r\n./presto-cli-0.289-SNAPSHOT-executable.jar --user user1 --password\r\n4. Execute the USE command successfully.\r\n\r\n",
    "issue_word_count": 216,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/execution/UseTask.java",
      "presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java"
    ],
    "base_commit": "47b25c18ca1406ba9e76266914a3b2425722793a",
    "head_commit": "1ed9830b97cd6f67ea9778e72ca9ee73dcda22ea",
    "repo_url": "https://github.com/prestodb/presto/pull/23882",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23882",
    "dockerfile": "",
    "pr_merged_at": "2024-11-06T17:14:57.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\nindex cf3925581455a..bf6f10c8c2448 100644\n--- a/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/UseTask.java\n@@ -14,24 +14,34 @@\n package com.facebook.presto.execution;\n \n import com.facebook.presto.Session;\n+import com.facebook.presto.common.CatalogSchemaName;\n+import com.facebook.presto.common.transaction.TransactionId;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.analyzer.MetadataResolver;\n import com.facebook.presto.spi.security.AccessControl;\n+import com.facebook.presto.spi.security.AccessControlContext;\n+import com.facebook.presto.spi.security.AccessDeniedException;\n+import com.facebook.presto.spi.security.Identity;\n import com.facebook.presto.sql.analyzer.SemanticException;\n import com.facebook.presto.sql.tree.Expression;\n import com.facebook.presto.sql.tree.Use;\n import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableSet;\n import com.google.common.util.concurrent.ListenableFuture;\n \n import java.util.List;\n \n import static com.facebook.presto.spi.StandardErrorCode.NOT_FOUND;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyCatalogAccess;\n import static com.facebook.presto.sql.analyzer.SemanticErrorCode.CATALOG_NOT_SPECIFIED;\n import static com.google.common.util.concurrent.Futures.immediateFuture;\n+import static java.util.Locale.ENGLISH;\n \n public class UseTask\n         implements SessionTransactionControlTask<Use>\n {\n+    String catalog;\n     @Override\n     public String getName()\n     {\n@@ -49,11 +59,38 @@ public ListenableFuture<?> execute(\n     {\n         Session session = stateMachine.getSession();\n \n+        TransactionId transactionId = session.getTransactionId().get();\n+\n+        Identity identity = session.getIdentity();\n+\n+        AccessControlContext context = session.getAccessControlContext();\n+\n         checkCatalogAndSessionPresent(statement, session);\n \n         checkAndSetCatalog(statement, metadata, stateMachine, session);\n \n-        stateMachine.setSetSchema(statement.getSchema().getValueLowerCase());\n+        String schema = statement.getSchema().getValue();\n+\n+        stateMachine.setSetSchema(schema);\n+\n+        if (!hasCatalogAccess(identity, context, catalog, accessControl)) {\n+            denyCatalogAccess(catalog);\n+        }\n+\n+        CatalogSchemaName name = new CatalogSchemaName(catalog, schema);\n+\n+        MetadataResolver metadataresolver = metadata.getMetadataResolver(session);\n+        if (!metadataresolver.schemaExists(name)) {\n+            throw new PrestoException(NOT_FOUND, \"Schema does not exist: \" + name);\n+        }\n+\n+        if (!hasSchemaAccess(transactionId, identity, context, catalog, schema, accessControl)) {\n+            throw new AccessDeniedException(\"Cannot access schema: \" + name);\n+        }\n+\n+        if (statement.getCatalog().isPresent()) {\n+            stateMachine.setSetCatalog(catalog);\n+        }\n \n         return immediateFuture(null);\n     }\n@@ -67,12 +104,24 @@ private void checkCatalogAndSessionPresent(Use statement, Session session)\n \n     private void checkAndSetCatalog(Use statement, Metadata metadata, QueryStateMachine stateMachine, Session session)\n     {\n-        if (statement.getCatalog().isPresent()) {\n-            String catalog = statement.getCatalog().get().getValueLowerCase();\n+        if (statement.getCatalog().isPresent() || session.getCatalog().isPresent()) {\n+            catalog = statement.getCatalog()\n+                    .map(identifier -> identifier.getValue().toLowerCase(ENGLISH))\n+                    .orElseGet(() -> session.getCatalog().get());\n             if (!metadata.getCatalogHandle(session, catalog).isPresent()) {\n                 throw new PrestoException(NOT_FOUND, \"Catalog does not exist: \" + catalog);\n             }\n             stateMachine.setSetCatalog(catalog);\n         }\n     }\n+\n+    private boolean hasCatalogAccess(Identity identity, AccessControlContext context, String catalog, AccessControl accessControl)\n+    {\n+        return !accessControl.filterCatalogs(identity, context, ImmutableSet.of(catalog)).isEmpty();\n+    }\n+\n+    private boolean hasSchemaAccess(TransactionId transactionId, Identity identity, AccessControlContext context, String catalog, String schema, AccessControl accessControl)\n+    {\n+        return !accessControl.filterSchemas(transactionId, identity, context, catalog, ImmutableSet.of(schema)).isEmpty();\n+    }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java b/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java\nnew file mode 100644\nindex 0000000000000..6238f30066ced\n--- /dev/null\n+++ b/presto-main/src/test/java/com/facebook/presto/execution/TestUseTask.java\n@@ -0,0 +1,173 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.connector.MockConnectorFactory;\n+import com.facebook.presto.metadata.Catalog;\n+import com.facebook.presto.metadata.CatalogManager;\n+import com.facebook.presto.metadata.MetadataManager;\n+import com.facebook.presto.spi.ConnectorId;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.security.AccessControl;\n+import com.facebook.presto.spi.security.AccessDeniedException;\n+import com.facebook.presto.spi.security.AllowAllAccessControl;\n+import com.facebook.presto.spi.security.DenyAllAccessControl;\n+import com.facebook.presto.spi.security.Identity;\n+import com.facebook.presto.sql.analyzer.SemanticException;\n+import com.facebook.presto.sql.tree.Identifier;\n+import com.facebook.presto.sql.tree.Use;\n+import com.facebook.presto.testing.TestingConnectorContext;\n+import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.Test;\n+\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+\n+import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static com.facebook.presto.SessionTestUtils.TEST_SESSION;\n+import static com.facebook.presto.execution.TaskTestUtils.createQueryStateMachine;\n+import static com.facebook.presto.metadata.MetadataManager.createTestMetadataManager;\n+import static com.facebook.presto.spi.ConnectorId.createInformationSchemaConnectorId;\n+import static com.facebook.presto.spi.ConnectorId.createSystemTablesConnectorId;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_FOUND;\n+import static com.facebook.presto.testing.TestingSession.testSessionBuilder;\n+import static com.facebook.presto.transaction.InMemoryTransactionManager.createTestTransactionManager;\n+import static java.util.Collections.emptyList;\n+import static java.util.concurrent.Executors.newCachedThreadPool;\n+import static org.testng.Assert.assertEquals;\n+\n+public class TestUseTask\n+{\n+    private final ExecutorService executor = newCachedThreadPool(daemonThreadsNamed(\"test-%s\"));\n+    private CatalogManager catalogManager;\n+    private Catalog testCatalog;\n+    private TransactionManager transactionManager;\n+    private MetadataManager metadata = createTestMetadataManager();\n+    MockConnectorFactory.Builder builder = MockConnectorFactory.builder();\n+    MockConnectorFactory mockConnectorFactory = builder.withListSchemaNames(connectorSession -> ImmutableList.of(\"test_schema\"))\n+            .build();\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        executor.shutdownNow();\n+    }\n+\n+    @Test\n+    public void testUse()\n+    {\n+        Use use = new Use(Optional.of(identifier(\"test_catalog\")), identifier(\"test_schema\"));\n+        String sqlString = \"USE test_catalog.test_schema\";\n+        executeUse(use, sqlString, TEST_SESSION);\n+    }\n+\n+    @Test\n+    public void testUseNoCatalog()\n+    {\n+        Use use = new Use(Optional.empty(), identifier(\"test_schema\"));\n+        String sqlString = \"USE test_schema\";\n+        Session session = testSessionBuilder()\n+                .setCatalog(null)\n+                .setSchema(null)\n+                .build();\n+        try {\n+            executeUse(use, sqlString, session);\n+        }\n+        catch (SemanticException e) {\n+            assertEquals(e.getMessage(), \"Catalog must be specified when session catalog is not set\");\n+        }\n+    }\n+\n+    @Test\n+    public void testUseInvalidCatalog()\n+    {\n+        Use use = new Use(Optional.of(identifier(\"invalid_catalog\")), identifier(\"test_schema\"));\n+        String sqlString = \"USE invalid_catalog.test_schema\";\n+        try {\n+            executeUse(use, sqlString, TEST_SESSION);\n+        }\n+        catch (PrestoException e) {\n+            assertEquals(e.getErrorCode(), NOT_FOUND.toErrorCode());\n+            assertEquals(e.getMessage(), \"Catalog does not exist: invalid_catalog\");\n+        }\n+    }\n+\n+    @Test\n+    public void testUseInvalidSchema()\n+    {\n+        Use use = new Use(Optional.of(identifier(\"test_catalog\")), identifier(\"invalid_schema\"));\n+        String sqlString = \"USE test_catalog.invalid_schema\";\n+        Session session = testSessionBuilder()\n+                .setSchema(\"invalid_schema\")\n+                .build();\n+        try {\n+            executeUse(use, sqlString, session);\n+        }\n+        catch (PrestoException e) {\n+            assertEquals(e.getErrorCode(), NOT_FOUND.toErrorCode());\n+            assertEquals(e.getMessage(), \"Schema does not exist: test_catalog.invalid_schema\");\n+        }\n+    }\n+\n+    @Test\n+    public void testUseAccessDenied()\n+    {\n+        Use use = new Use(Optional.of(identifier(\"test_catalog\")), identifier(\"test_schema\"));\n+        String sqlString = \"USE test_catalog.test_schema\";\n+        Session session = testSessionBuilder()\n+                .setIdentity(new Identity(\"user\", Optional.empty()))\n+                .build();\n+        AccessControl accessControl = new DenyAllAccessControl();\n+        try {\n+            executeUse(use, sqlString, session);\n+        }\n+        catch (AccessDeniedException e) {\n+            assertEquals(e.getMessage(), \"Cannot access schema: test_catalog.test_schema\");\n+        }\n+    }\n+\n+    private void executeUse(Use use, String sqlString, Session session)\n+    {\n+        executeUse(use, sqlString, session, new AllowAllAccessControl());\n+    }\n+\n+    private void executeUse(Use use, String sqlString, Session session, AccessControl accessControl)\n+    {\n+        Connector testConnector = mockConnectorFactory.create(\"test\", ImmutableMap.of(), new TestingConnectorContext());\n+        catalogManager = new CatalogManager();\n+        String catalogName = \"test_catalog\";\n+        ConnectorId connectorId = new ConnectorId(catalogName);\n+        catalogManager.registerCatalog(new Catalog(\n+                catalogName,\n+                connectorId,\n+                testConnector,\n+                createInformationSchemaConnectorId(connectorId),\n+                testConnector,\n+                createSystemTablesConnectorId(connectorId),\n+                testConnector));\n+        transactionManager = createTestTransactionManager(catalogManager);\n+        metadata = createTestMetadataManager(transactionManager);\n+        QueryStateMachine stateMachine = createQueryStateMachine(sqlString, session, false, transactionManager, executor, metadata);\n+        UseTask useTask = new UseTask();\n+        useTask.execute(use, transactionManager, metadata, accessControl, stateMachine, emptyList());\n+    }\n+    private Identifier identifier(String name)\n+    {\n+        return new Identifier(name);\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23850",
    "pr_id": 23850,
    "issue_id": 23849,
    "repo": "prestodb/presto",
    "problem_statement": "Internal Error in Presto When Querying Elastic search Index with No Mapped Columns\nWhen querying an Elasticsearch index that has no defined columns, Presto throws an internal error. Elasticsearch allows the creation of indices without any mapped columns, which causes Presto to fail when attempting to retrieve data from such indices. \r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: latest\r\n* Data source and connector used: ElasticSearch\r\n* Debug logs :\r\n\r\n> Query 20241017_031656_00002_9vw2m failed: Internal error\r\n> java.util.NoSuchElementException\r\n>         at java.base/java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:721)\r\n>         at java.base/java.util.LinkedHashMap$LinkedValueIterator.next(LinkedHashMap.java:746)\r\n>         at com.facebook.presto.elasticsearch.client.ElasticsearchClient.lambda$getIndexMetadata$8(ElasticsearchClient.java:495)\r\n>         at com.facebook.presto.elasticsearch.client.ElasticsearchClient.doRequest(ElasticsearchClient.java:726)\r\n>         at com.facebook.presto.elasticsearch.client.ElasticsearchClient.getIndexMetadata(ElasticsearchClient.java:485)\r\n>         at com.facebook.presto.elasticsearch.ElasticsearchMetadata.makeInternalTableMetadata(ElasticsearchMetadata.java:202)\r\n>         at com.facebook.presto.elasticsearch.ElasticsearchMetadata.makeInternalTableMetadata(ElasticsearchMetadata.java:197)\r\n>         at com.facebook.presto.elasticsearch.ElasticsearchMetadata.getColumnHandles(ElasticsearchMetadata.java:377)\r\n>         at com.facebook.presto.metadata.MetadataManager.getColumnHandles(MetadataManager.java:507)\r\n>         at com.facebook.presto.metadata.MetadataManager$1.getColumnHandles(MetadataManager.java:1383)\r\n>         at com.facebook.presto.util.MetadataUtils.lambda$getTableColumnMetadata$3(MetadataUtils.java:97)\r\n>         at com.facebook.presto.common.RuntimeStats.profileNanos(RuntimeStats.java:136)\r\n>         at com.facebook.presto.util.MetadataUtils.getTableColumnMetadata(MetadataUtils.java:95)\r\n>         at com.facebook.presto.util.MetadataUtils.getTableColumnsMetadata(MetadataUtils.java:54)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:1332)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:359)\r\n>         at com.facebook.presto.sql.tree.Table.accept(Table.java:60)\r\n>         at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:373)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.analyzeFrom(StatementAnalyzer.java:2789)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:1727)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:359)\r\n>         at com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:138)\r\n>         at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:373)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:381)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:1166)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:359)\r\n>         at com.facebook.presto.sql.tree.Query.accept(Query.java:105)\r\n>         at com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:373)\r\n>         at com.facebook.presto.sql.analyzer.StatementAnalyzer.analyze(StatementAnalyzer.java:351)\r\n>         at com.facebook.presto.sql.analyzer.Analyzer.analyzeSemantic(Analyzer.java:117)\r\n>         at com.facebook.presto.sql.analyzer.BuiltInQueryAnalyzer.analyze(BuiltInQueryAnalyzer.java:93)\r\n>         at com.facebook.presto.execution.SqlQueryExecution.<init>(SqlQueryExecution.java:210)\r\n>         at com.facebook.presto.execution.SqlQueryExecution.<init>(SqlQueryExecution.java:111)\r\n>         at com.facebook.presto.execution.SqlQueryExecution$SqlQueryExecutionFactory.createQueryExecution(SqlQueryExecution.java:990)\r\n>         at com.facebook.presto.dispatcher.LocalDispatchQueryFactory.lambda$createDispatchQuery$0(LocalDispatchQueryFactory.java:170)\r\n>         at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\r\n>         at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:75)\r\n>         at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\r\n>         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n>         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n>         at java.base/java.lang.Thread.run(Thread.java:834)\r\n\r\n\r\n## Expected Behavior\r\nInstead of throwing an internal error, Presto should display a meaningful error message when querying.\r\n\r\n## Current Behavior\r\n\r\nGetting the Internal Error.\r\n<img width=\"612\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b3b9991a-4ff8-429a-bac4-eb2444ceef59\">\r\n\r\n\r\nThe code iterates through the mappings (columns) but lacks an exception handler for cases where an element is not found. Adding appropriate exception handling can address this issue.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create an index in Elasticsearch using the command: PUT /test_empty_profile.\r\n2. In Presto, establish a connection to Elasticsearch.\r\n3. Execute a query on the table test_empty_profile using SELECT.\r\n4. Observe that it throws an internal error.\r\n",
    "issue_word_count": 685,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-elasticsearch/src/main/java/com/facebook/presto/elasticsearch/client/ElasticsearchClient.java",
      "presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java"
    ],
    "pr_changed_test_files": [
      "presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java"
    ],
    "base_commit": "1c0fc175a73f60ababb6fba26472c3d5df7ebadf",
    "head_commit": "4a1df653718a523895224f10518ea8cb0da23046",
    "repo_url": "https://github.com/prestodb/presto/pull/23850",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23850",
    "dockerfile": "",
    "pr_merged_at": "2024-11-01T17:49:20.000Z",
    "patch": "diff --git a/presto-elasticsearch/src/main/java/com/facebook/presto/elasticsearch/client/ElasticsearchClient.java b/presto-elasticsearch/src/main/java/com/facebook/presto/elasticsearch/client/ElasticsearchClient.java\nindex 55caaf8fef42b..4740310c16788 100644\n--- a/presto-elasticsearch/src/main/java/com/facebook/presto/elasticsearch/client/ElasticsearchClient.java\n+++ b/presto-elasticsearch/src/main/java/com/facebook/presto/elasticsearch/client/ElasticsearchClient.java\n@@ -492,6 +492,10 @@ public IndexMetadata getIndexMetadata(String index)\n                     // Older versions of ElasticSearch supported multiple \"type\" mappings\n                     // for a given index. Newer versions support only one and don't\n                     // expose it in the document. Here we skip it if it's present.\n+\n+                    if (!mappings.elements().hasNext()) {\n+                        return new IndexMetadata(new IndexMetadata.ObjectType(ImmutableList.of()));\n+                    }\n                     mappings = mappings.elements().next();\n                 }\n \n",
    "test_patch": "diff --git a/presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java b/presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java\nindex fc15c6daf5d5e..d4ff79bcdd31e 100644\n--- a/presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java\n+++ b/presto-elasticsearch/src/test/java/com/facebook/presto/elasticsearch/TestElasticsearchIntegrationSmokeTest.java\n@@ -805,4 +805,12 @@ private void createIndex(String indexName, @Language(\"JSON\") String mapping)\n         client.getLowLevelClient()\n                 .performRequest(\"PUT\", \"/\" + indexName, ImmutableMap.of(), new NStringEntity(mapping, ContentType.APPLICATION_JSON));\n     }\n+\n+    @Test\n+    public void testEmptyIndexNoMappings()\n+            throws IOException\n+    {\n+        client.getLowLevelClient().performRequest(\"PUT\", \"/emptyindex\");\n+        assertQueryFails(\"SELECT * FROM emptyindex\", \"line 1:8: SELECT \\\\* not allowed from relation that has no columns\");\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23845",
    "pr_id": 23845,
    "issue_id": 23730,
    "repo": "prestodb/presto",
    "problem_statement": "arrays_overlap return different results after swapping parameters for array types\n```\r\nSELECT\r\n    ARRAYS_OVERLAP(ARRAY[ARRAY[1, 2], ARRAY[1, NULL]], ARRAY[ARRAY[1, 2]])\r\n=> true\r\nSELECT\r\n    ARRAYS_OVERLAP(ARRAY[ARRAY[1, NULL], ARRAY[1, 2]], ARRAY[ARRAY[1, 2]])\r\n=> throw\r\n```\r\n\r\n## Expected Behavior\r\nBoth queries should return the same result\r\n\r\n## Current Behavior\r\nThey return different results\r\n\r\n## Possible Solution\r\nSwitch completely to use \"[set based implementation](https://github.com/prestodb/presto/blob/14da6b7754aba39b5aae3203c94b1e0dabedfba3/presto-main/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java#L73)\" for arrays_overlap \r\n",
    "issue_word_count": 84,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main-base/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java",
      "presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java"
    ],
    "pr_changed_test_files": [
      "presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java"
    ],
    "base_commit": "26e1b66f9baf6e81a5c2335faa114ef484fd89a9",
    "head_commit": "15dd7cfe2455faba905f0bac54591e60eae631f1",
    "repo_url": "https://github.com/prestodb/presto/pull/23845",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23845",
    "dockerfile": "",
    "pr_merged_at": "2025-04-30T07:41:06.000Z",
    "patch": "diff --git a/presto-main-base/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java b/presto-main-base/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java\nindex 468cb2911de01..c395ff7703fb1 100644\n--- a/presto-main-base/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java\n+++ b/presto-main-base/src/main/java/com/facebook/presto/operator/scalar/ArraysOverlapFunction.java\n@@ -18,11 +18,19 @@\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.operator.aggregation.TypedSet;\n import com.facebook.presto.spi.function.Description;\n+import com.facebook.presto.spi.function.OperatorDependency;\n import com.facebook.presto.spi.function.ScalarFunction;\n import com.facebook.presto.spi.function.SqlNullable;\n import com.facebook.presto.spi.function.SqlType;\n import com.facebook.presto.spi.function.TypeParameter;\n \n+import java.lang.invoke.MethodHandle;\n+\n+import static com.facebook.presto.common.function.OperatorType.IS_DISTINCT_FROM;\n+import static com.facebook.presto.common.type.TypeUtils.readNativeValue;\n+import static com.facebook.presto.util.Failures.internalError;\n+import static com.google.common.base.Defaults.defaultValue;\n+\n @ScalarFunction(\"arrays_overlap\")\n @Description(\"Returns true if arrays have common elements\")\n public final class ArraysOverlapFunction\n@@ -36,6 +44,7 @@ private ArraysOverlapFunction() {}\n     @SqlNullable\n     public static Boolean arraysOverlap(\n             @TypeParameter(\"T\") Type elementType,\n+            @OperatorDependency(operator = IS_DISTINCT_FROM, argumentTypes = {\"T\", \"T\"}) MethodHandle elementIsDistinctFrom,\n             @SqlType(\"array(T)\") Block leftArray,\n             @SqlType(\"array(T)\") Block rightArray)\n     {\n@@ -59,8 +68,17 @@ public static Boolean arraysOverlap(\n                     hasNull = true;\n                     continue;\n                 }\n-                if (elementType.equalTo(leftArray, i, rightArray, j)) {\n-                    return true;\n+                try {\n+                    boolean firstValueNull = leftArray.isNull(i);\n+                    Object firstValue = firstValueNull ? defaultValue(elementType.getJavaType()) : readNativeValue(elementType, leftArray, i);\n+                    boolean secondValueNull = rightArray.isNull(j);\n+                    Object secondValue = secondValueNull ? defaultValue(elementType.getJavaType()) : readNativeValue(elementType, rightArray, j);\n+                    if (!(boolean) elementIsDistinctFrom.invoke(firstValue, firstValueNull, secondValue, secondValueNull)) {\n+                        return true;\n+                    }\n+                }\n+                catch (Throwable t) {\n+                    throw internalError(t);\n                 }\n             }\n         }\n",
    "test_patch": "diff --git a/presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java b/presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java\nindex 64ed4a5fc4921..5d37e94329b63 100644\n--- a/presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java\n+++ b/presto-main-base/src/test/java/com/facebook/presto/type/TestArrayOperators.java\n@@ -1295,6 +1295,8 @@ public void testArraysOverlap()\n         assertFunction(\"ARRAYS_OVERLAP(ARRAY [NULL, 3], ARRAY [2, 1])\", BooleanType.BOOLEAN, null);\n         assertFunction(\"ARRAYS_OVERLAP(ARRAY [3, NULL], ARRAY [2, 1])\", BooleanType.BOOLEAN, null);\n         assertFunction(\"ARRAYS_OVERLAP(ARRAY [3, NULL], ARRAY [2, 1, NULL])\", BooleanType.BOOLEAN, null);\n+        assertFunction(\"ARRAYS_OVERLAP(ARRAY [ARRAY [1, 2], ARRAY [1, NULL]], ARRAY [ARRAY [1, 2]])\", BooleanType.BOOLEAN, true);\n+        assertFunction(\"ARRAYS_OVERLAP(ARRAY [ARRAY [1, NULL], ARRAY [1, 2]], ARRAY [ARRAY [1, 2]])\", BooleanType.BOOLEAN, true);\n \n         assertFunction(\"ARRAYS_OVERLAP(ARRAY [CAST(1 AS BIGINT), 2], ARRAY [NULL, CAST(2 AS BIGINT)])\", BooleanType.BOOLEAN, true);\n         assertFunction(\"ARRAYS_OVERLAP(ARRAY [CAST(1 AS BIGINT), 2], ARRAY [CAST(2 AS BIGINT), NULL])\", BooleanType.BOOLEAN, true);\n@@ -1405,8 +1407,8 @@ public void testComparison()\n         assertFunction(\"ARRAY [1, 2, null] != ARRAY [1, 2, null]\", BOOLEAN, null);\n         assertFunction(\"ARRAY [1, 2, null] != ARRAY [1, null]\", BOOLEAN, true);\n         assertFunction(\"ARRAY [1, 3, null] != ARRAY [1, 2, null]\", BOOLEAN, true);\n-        assertFunction(\"ARRAY [ARRAY[1], ARRAY[null], ARRAY[2]] != ARRAY [ARRAY[1], ARRAY[2], ARRAY[3]]\", BOOLEAN, true);\n-        assertFunction(\"ARRAY [ARRAY[1], ARRAY[null], ARRAY[3]] != ARRAY [ARRAY[1], ARRAY[2], ARRAY[3]]\", BOOLEAN, null);\n+        assertFunction(\"ARRAY [ARRAY [1], ARRAY [null], ARRAY [2]] != ARRAY [ARRAY [1], ARRAY [2], ARRAY [3]]\", BOOLEAN, true);\n+        assertFunction(\"ARRAY [ARRAY [1], ARRAY [null], ARRAY [3]] != ARRAY [ARRAY [1], ARRAY [2], ARRAY [3]]\", BOOLEAN, null);\n \n         assertFunction(\"ARRAY [10, 20, 30] < ARRAY [10, 20, 40, 50]\", BOOLEAN, true);\n         assertFunction(\"ARRAY [10, 20, 30] >= ARRAY [10, 20, 40, 50]\", BOOLEAN, false);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23841",
    "pr_id": 23841,
    "issue_id": 23838,
    "repo": "prestodb/presto",
    "problem_statement": "TestQueues.testEagerPlanValidation() is flaky\nExample failure: https://github.com/prestodb/presto/actions/runs/11350506950/job/31569394356?pr=23837\r\n\r\n```\r\nError:  Tests run: 77, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 469.432 s <<< FAILURE! - in TestSuite\r\nError:  com.facebook.presto.execution.TestQueues.testEagerPlanValidation  Time elapsed: 46.685 s  <<< FAILURE!\r\norg.testng.internal.thread.ThreadTimeoutException: Method com.facebook.presto.execution.TestQueues.testEagerPlanValidation() didn't finish within the time-out 240000\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeWithTimeoutWithNoExecutor(MethodInvocationHelper.java:347)\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeWithTimeout(MethodInvocationHelper.java:294)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:679)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```",
    "issue_word_count": 187,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java"
    ],
    "pr_changed_test_files": [
      "presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java"
    ],
    "base_commit": "5f3835ef76afe93f966b25f502ba503924e21068",
    "head_commit": "a894bc94c3580f19ee231ff21fd555692f6ea130",
    "repo_url": "https://github.com/prestodb/presto/pull/23841",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23841",
    "dockerfile": "",
    "pr_merged_at": "2024-10-16T13:30:37.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java b/presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java\nindex c03ee3b3104e8..3b15e0aec0587 100644\n--- a/presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java\n+++ b/presto-tests/src/test/java/com/facebook/presto/execution/TestQueues.java\n@@ -401,11 +401,12 @@ public void testEagerPlanValidation()\n         QueryId secondQuery = createQuery(queryRunner, secondSession, LONG_LASTING_QUERY);\n         waitForQueryState(queryRunner, secondQuery, QUEUED);\n \n+        // Force failure during plan validation after queuing has begun\n+        triggerValidationFailure.set(true);\n+\n         Session thirdSession = builder.setQueryId(QueryId.valueOf(\"20240930_203743_00003_33333\")).build();\n         QueryId thirdQuery = createQuery(queryRunner, thirdSession, LONG_LASTING_QUERY);\n \n-        // Force failure during plan validation after queuing has begun\n-        triggerValidationFailure.set(true);\n         waitForQueryState(queryRunner, thirdQuery, FAILED);\n \n         DispatchManager dispatchManager = queryRunner.getCoordinator().getDispatchManager();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23765",
    "pr_id": 23765,
    "issue_id": 23763,
    "repo": "prestodb/presto",
    "problem_statement": "Round function returns incorrect results for large floats\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n\r\n```\r\nSELECT\r\n    c0 AS value,\r\n    \"round\"(c0, 1) AS rounded\r\nFROM (\r\n    VALUES\r\n        (CAST(1E19 AS REAL))\r\n) t(c0);\r\n```\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\n```\r\n value  |    rounded    \r\n--------+---------------\r\n 1.0E19 | 1.0E19\r\n(1 row)\r\n```\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n```\r\n value  |    rounded    \r\n--------+---------------\r\n 1.0E19 | -8.4467441E18 \r\n(1 row)\r\n```\r\nIssue only occurs when casting to real and using scientific notation.\r\n",
    "issue_word_count": 123,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java"
    ],
    "base_commit": "832b071b3c74f65a6fe8386dc296b61f17192a7a",
    "head_commit": "a6af7d6fe2d31f414296b15809f8cf6da14cc92c",
    "repo_url": "https://github.com/prestodb/presto/pull/23765",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23765",
    "dockerfile": "",
    "pr_merged_at": "2024-10-10T14:52:51.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\nindex 69f7eb3d07b05..231e8db1b0aa8 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/MathFunctions.java\n@@ -1136,7 +1136,7 @@ public static long roundFloat(@SqlType(StandardTypes.REAL) long num, @SqlType(St\n         catch (ArithmeticException e) {\n             // Use BigDecimal if the value is out of the range of long.\n             BigDecimal bigDecimal = new BigDecimal(numInFloat);\n-            return floatToRawIntBits(bigDecimal.setScale((int) decimals, HALF_UP).longValue());\n+            return floatToRawIntBits(bigDecimal.setScale((int) decimals, HALF_UP).floatValue());\n         }\n     }\n \n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\nindex a296d6a6a1d81..a5ab98de85fa3 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestMathFunctions.java\n@@ -709,6 +709,19 @@ public void testSecureRandom()\n         assertInvalidFunction(\"secure_random(DECIMAL '5.0', DECIMAL '-5.0')\", \"upper bound must be greater than lower bound\");\n     }\n \n+    @Test\n+    public void testRoundForUnderlyingValueOutOfRange()\n+    {\n+        // Round data of `REAL` type with underlying value out of long range should work well.\n+        // See issue https://github.com/prestodb/presto/issues/23763\n+        assertFunction(\"round(REAL '1.0E19', 1)\", REAL, 1.0E19f);\n+        assertFunction(\"round(REAL '1.0E19', 10)\", REAL, 1.0E19f);\n+        assertFunction(\"round(REAL '1.0E19', 100)\", REAL, 1.0E19f);\n+        assertFunction(\"round(REAL '9999999999999999999.9', 1)\", REAL, 9999999999999999999.9f);\n+        assertFunction(\"round(REAL '9999999999999999999.99', 10)\", REAL, 9999999999999999999.99f);\n+        assertFunction(\"round(REAL '9999999999999999999.999', 100)\", REAL, 9999999999999999999.999f);\n+    }\n+\n     @Test\n     public void testRound()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23749",
    "pr_id": 23749,
    "issue_id": 23748,
    "repo": "prestodb/presto",
    "problem_statement": "Add support for ALTER VIEW RENAME TO statement\nAdd support for SQL statement ALTER VIEW <view_name> RENAME TO <new_view_name>. \r\n\r\n## Expected Behavior or Use Case\r\nAlters the name of an existing view to a new name.\r\n\r\n## Presto Component, Service, or Connector\r\nIceberg Connector in Presto\r\n\r\n## Possible Implementation\r\n\r\nAddressed in PR: https://github.com/prestodb/presto/pull/23749\r\n\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n![image](https://github.com/user-attachments/assets/d615dae3-5435-4ee7-ba7a-9ddf1eef6065)\r\n\r\n\r\n## Context\r\nAdding support to ALTER VIEW RENAME TO statement helps to alter the name of an existing view to a new name.",
    "issue_word_count": 94,
    "test_files_count": 10,
    "non_test_files_count": 40,
    "pr_changed_files": [
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java",
      "presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/utils/StatementUtils.java",
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-docs/src/main/sphinx/connector/memory.rst",
      "presto-docs/src/main/sphinx/sql.rst",
      "presto-docs/src/main/sphinx/sql/alter-view.rst",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/MetastoreUtil.java",
      "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java",
      "presto-hive/src/main/java/com/facebook/presto/hive/security/SystemTableAwareAccessControl.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java",
      "presto-main/src/main/java/com/facebook/presto/execution/RenameViewTask.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java",
      "presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java",
      "presto-main/src/main/java/com/facebook/presto/security/AccessControlManager.java",
      "presto-main/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java",
      "presto-main/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java",
      "presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java",
      "presto-main/src/main/java/com/facebook/presto/util/PrestoDataDefBindingHelper.java",
      "presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java",
      "presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java",
      "presto-memory/src/main/java/com/facebook/presto/plugin/memory/MemoryMetadata.java",
      "presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java",
      "presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java",
      "presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4",
      "presto-parser/src/main/java/com/facebook/presto/sql/SqlFormatter.java",
      "presto-parser/src/main/java/com/facebook/presto/sql/parser/AstBuilder.java",
      "presto-parser/src/main/java/com/facebook/presto/sql/tree/AstVisitor.java",
      "presto-parser/src/main/java/com/facebook/presto/sql/tree/RenameView.java",
      "presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java",
      "presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ReadOnlyAccessControl.java",
      "presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java",
      "presto-plugin-toolkit/src/test/resources/table.json",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/AccessDeniedException.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/AllowAllAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/DenyAllAccessControl.java",
      "presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java",
      "presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java",
      "presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java",
      "presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java",
      "presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java",
      "presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java",
      "presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java",
      "presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java",
      "presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java",
      "presto-plugin-toolkit/src/test/resources/table.json"
    ],
    "base_commit": "e416ff11482b92f812c2c47ef5ea0f0b19d85393",
    "head_commit": "5a61649d7b8ab069f6ec19ad1b47a5995f63e113",
    "repo_url": "https://github.com/prestodb/presto/pull/23749",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23749",
    "dockerfile": "",
    "pr_merged_at": "2024-12-17T14:58:57.000Z",
    "patch": "diff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\nindex 66b6d73302598..d5492e6bb6932 100644\n--- a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/SemanticErrorCode.java\n@@ -83,6 +83,8 @@ public enum SemanticErrorCode\n     VIEW_IS_STALE,\n     VIEW_IS_RECURSIVE,\n     MATERIALIZED_VIEW_IS_RECURSIVE,\n+    MISSING_VIEW,\n+    VIEW_ALREADY_EXISTS,\n \n     NON_NUMERIC_SAMPLE_PERCENTAGE,\n \n\ndiff --git a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/utils/StatementUtils.java b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/utils/StatementUtils.java\nindex 0a2b34bebef10..bb16839f87288 100644\n--- a/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/utils/StatementUtils.java\n+++ b/presto-analyzer/src/main/java/com/facebook/presto/sql/analyzer/utils/StatementUtils.java\n@@ -51,6 +51,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Revoke;\n import com.facebook.presto.sql.tree.RevokeRoles;\n@@ -134,6 +135,7 @@ private StatementUtils() {}\n         builder.put(AddConstraint.class, QueryType.DATA_DEFINITION);\n         builder.put(AlterColumnNotNull.class, QueryType.DATA_DEFINITION);\n         builder.put(CreateView.class, QueryType.DATA_DEFINITION);\n+        builder.put(RenameView.class, QueryType.DATA_DEFINITION);\n         builder.put(TruncateTable.class, QueryType.DATA_DEFINITION);\n         builder.put(DropView.class, QueryType.DATA_DEFINITION);\n         builder.put(CreateMaterializedView.class, QueryType.DATA_DEFINITION);\n\ndiff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 86f83bb3674c6..6644c2ade0afb 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -1209,6 +1209,15 @@ For example, to set `commit_retries` to 6 for the table `iceberg.web.page_views_\n \n     ALTER TABLE iceberg.web.page_views_v2 SET PROPERTIES (commit_retries = 6);\n \n+ALTER VIEW\n+^^^^^^^^^^\n+\n+Alter view operations to alter the name of an existing view to a new name is supported in the Iceberg connector.\n+\n+.. code-block:: sql\n+\n+    ALTER VIEW iceberg.web.page_views RENAME TO iceberg.web.page_new_views;\n+\n TRUNCATE\n ^^^^^^^^\n \n\ndiff --git a/presto-docs/src/main/sphinx/connector/memory.rst b/presto-docs/src/main/sphinx/connector/memory.rst\nindex 0a49cc4a6290a..041282c60d84f 100644\n--- a/presto-docs/src/main/sphinx/connector/memory.rst\n+++ b/presto-docs/src/main/sphinx/connector/memory.rst\n@@ -92,6 +92,15 @@ To delete an existing table:\n \n .. note:: After using ``DROP TABLE``, memory is not released immediately. It is released after the next write access to the memory connector.\n \n+ALTER VIEW\n+^^^^^^^^^^\n+\n+Alter view operations to alter the name of an existing view to a new name is supported in the Memory connector.\n+\n+.. code-block:: sql\n+\n+    ALTER VIEW memory.default.nation RENAME TO memory.default.new_nation;\n+\n Memory Connector Limitations\n ----------------------------\n \n\ndiff --git a/presto-docs/src/main/sphinx/sql.rst b/presto-docs/src/main/sphinx/sql.rst\nindex 88a4613a27512..84b7ced26d1b7 100644\n--- a/presto-docs/src/main/sphinx/sql.rst\n+++ b/presto-docs/src/main/sphinx/sql.rst\n@@ -10,6 +10,7 @@ This chapter describes the SQL syntax used in Presto.\n     sql/alter-function\n     sql/alter-schema\n     sql/alter-table\n+    sql/alter-view\n     sql/analyze\n     sql/call\n     sql/commit\n\ndiff --git a/presto-docs/src/main/sphinx/sql/alter-view.rst b/presto-docs/src/main/sphinx/sql/alter-view.rst\nnew file mode 100644\nindex 0000000000000..dfe57193bfbb4\n--- /dev/null\n+++ b/presto-docs/src/main/sphinx/sql/alter-view.rst\n@@ -0,0 +1,42 @@\n+==========\n+ALTER VIEW\n+==========\n+\n+Synopsis\n+--------\n+\n+.. code-block:: sql\n+\n+    ALTER VIEW [IF EXISTS] old_view_name RENAME TO new_view_name;\n+\n+Description\n+-----------\n+\n+The ``ALTER VIEW [IF EXISTS] RENAME TO`` statement renames an existing view to a\n+new name. This allows you to change the name of a view without having to drop\n+and recreate it. The view's definition, security settings, and dependencies\n+remain unchanged; only the name of the view is updated.\n+\n+The optional ``IF EXISTS`` clause prevents an error from being raised if the\n+view does not exist. Instead, no action is taken, and a notice is issued.\n+\n+Renaming a view does not affect the data or structure of the underlying\n+query used to define the view. Any permissions or dependencies on the\n+view are retained, and queries or applications using the old name must\n+be updated to use the new name.\n+\n+Examples\n+--------\n+\n+Rename the view ``users`` to ``people``::\n+\n+    ALTER VIEW users RENAME TO people;\n+\n+Rename the view ``users`` to ``people`` if view ``users`` exists::\n+\n+    ALTER VIEW IF EXISTS users RENAME TO people;\n+\n+See Also\n+--------\n+\n+:doc:`create-view`\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/MetastoreUtil.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/MetastoreUtil.java\nindex 28cb2b84017d5..87cd7e7e061ca 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/MetastoreUtil.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/MetastoreUtil.java\n@@ -1026,6 +1026,11 @@ public static boolean isIcebergTable(Map<String, String> tableParameters)\n         return ICEBERG_TABLE_TYPE_VALUE.equalsIgnoreCase(tableParameters.get(ICEBERG_TABLE_TYPE_NAME));\n     }\n \n+    public static boolean isIcebergView(Table table)\n+    {\n+        return \"true\".equalsIgnoreCase(table.getParameters().get(PRESTO_VIEW_FLAG));\n+    }\n+\n     public static PrincipalPrivileges buildInitialPrivilegeSet(String tableOwner)\n     {\n         PrestoPrincipal owner = new PrestoPrincipal(USER, tableOwner);\n\ndiff --git a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java\nindex 5ed409e353669..560bf7dd8f847 100644\n--- a/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java\n+++ b/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java\n@@ -99,6 +99,7 @@\n import static com.facebook.presto.hive.metastore.MetastoreUtil.getHiveBasicStatistics;\n import static com.facebook.presto.hive.metastore.MetastoreUtil.getPartitionNamesWithEmptyVersion;\n import static com.facebook.presto.hive.metastore.MetastoreUtil.isIcebergTable;\n+import static com.facebook.presto.hive.metastore.MetastoreUtil.isIcebergView;\n import static com.facebook.presto.hive.metastore.MetastoreUtil.makePartName;\n import static com.facebook.presto.hive.metastore.MetastoreUtil.toPartitionValues;\n import static com.facebook.presto.hive.metastore.MetastoreUtil.updateStatisticsParameters;\n@@ -511,13 +512,11 @@ public synchronized MetastoreOperationResult renameTable(MetastoreContext metast\n         requireNonNull(tableName, \"tableName is null\");\n         requireNonNull(newDatabaseName, \"newDatabaseName is null\");\n         requireNonNull(newTableName, \"newTableName is null\");\n-\n         Table table = getRequiredTable(metastoreContext, databaseName, tableName);\n         getRequiredDatabase(metastoreContext, newDatabaseName);\n-        if (isIcebergTable(table)) {\n+        if (isIcebergTable(table) && !isIcebergView(table)) {\n             throw new PrestoException(NOT_SUPPORTED, \"Rename not supported for Iceberg tables\");\n         }\n-\n         // verify new table does not exist\n         verifyTableNotExists(metastoreContext, newDatabaseName, newTableName);\n \n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java b/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\nindex 2277d1dab0f53..d1b77eb6d685d 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/security/LegacyAccessControl.java\n@@ -201,6 +201,11 @@ public void checkCanCreateView(ConnectorTransactionHandle transaction, Connector\n     {\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java b/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\nindex 6fd805e34c5e2..f55247fb69397 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/security/SqlStandardAccessControl.java\n@@ -70,6 +70,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeRoles;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectTable;\n@@ -451,6 +452,24 @@ public void checkCanCreateView(ConnectorTransactionHandle transaction, Connector\n         }\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        MetastoreContext metastoreContext = new MetastoreContext(\n+                identity, context.getQueryId().getId(),\n+                context.getClientInfo(),\n+                context.getClientTags(),\n+                context.getSource(),\n+                Optional.empty(),\n+                false,\n+                HiveColumnConverterProvider.DEFAULT_COLUMN_CONVERTER_PROVIDER,\n+                context.getWarningCollector(),\n+                context.getRuntimeStats());\n+        if (!isTableOwner(transaction, identity, metastoreContext, viewName)) {\n+            denyRenameView(viewName.toString(), newViewName.toString());\n+        }\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-hive/src/main/java/com/facebook/presto/hive/security/SystemTableAwareAccessControl.java b/presto-hive/src/main/java/com/facebook/presto/hive/security/SystemTableAwareAccessControl.java\nindex 9f2b182d39d4b..74a433e5d6356 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/security/SystemTableAwareAccessControl.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/security/SystemTableAwareAccessControl.java\n@@ -173,6 +173,12 @@ public void checkCanCreateView(ConnectorTransactionHandle transactionHandle, Con\n         delegate.checkCanCreateView(transactionHandle, identity, context, viewName);\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        delegate.checkCanRenameView(transactionHandle, identity, context, viewName, newViewName);\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\nindex 5f7e38a813751..ec75d1d7cf7de 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n@@ -421,6 +421,13 @@ public Map<SchemaTableName, ConnectorViewDefinition> getViews(ConnectorSession s\n         return views.build();\n     }\n \n+    @Override\n+    public void renameView(ConnectorSession session, SchemaTableName source, SchemaTableName target)\n+    {\n+        // Not checking if source view exists as this is already done in RenameViewTask\n+        metastore.renameTable(getMetastoreContext(session), source.getSchemaName(), source.getTableName(), target.getSchemaName(), target.getTableName());\n+    }\n+\n     @Override\n     public void dropView(ConnectorSession session, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/execution/RenameViewTask.java b/presto-main/src/main/java/com/facebook/presto/execution/RenameViewTask.java\nnew file mode 100644\nindex 0000000000000..61c3cadc8619f\n--- /dev/null\n+++ b/presto-main/src/main/java/com/facebook/presto/execution/RenameViewTask.java\n@@ -0,0 +1,79 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.execution;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.QualifiedObjectName;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.spi.WarningCollector;\n+import com.facebook.presto.spi.analyzer.ViewDefinition;\n+import com.facebook.presto.spi.security.AccessControl;\n+import com.facebook.presto.sql.analyzer.SemanticException;\n+import com.facebook.presto.sql.tree.Expression;\n+import com.facebook.presto.sql.tree.RenameView;\n+import com.facebook.presto.transaction.TransactionManager;\n+import com.google.common.util.concurrent.ListenableFuture;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.facebook.presto.metadata.MetadataUtil.createQualifiedObjectName;\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.MISSING_CATALOG;\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.MISSING_VIEW;\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.sql.analyzer.SemanticErrorCode.VIEW_ALREADY_EXISTS;\n+import static com.google.common.util.concurrent.Futures.immediateFuture;\n+\n+public class RenameViewTask\n+        implements DDLDefinitionTask<RenameView>\n+{\n+    @Override\n+    public String getName()\n+    {\n+        return \"RENAME VIEW\";\n+    }\n+\n+    public ListenableFuture<?> execute(RenameView statement, TransactionManager transactionManager, Metadata metadata, AccessControl accessControl, Session session, List<Expression> parameters, WarningCollector warningCollector)\n+    {\n+        QualifiedObjectName viewName = createQualifiedObjectName(session, statement, statement.getSource());\n+\n+        Optional<ViewDefinition> view = metadata.getMetadataResolver(session).getView(viewName);\n+        if (!view.isPresent()) {\n+            if (!statement.isExists()) {\n+                throw new SemanticException(MISSING_VIEW, statement, \"View '%s' does not exist\", viewName);\n+            }\n+            return immediateFuture(null);\n+        }\n+\n+        QualifiedObjectName target = createQualifiedObjectName(session, statement, statement.getTarget());\n+        if (!metadata.getCatalogHandle(session, target.getCatalogName()).isPresent()) {\n+            throw new SemanticException(MISSING_CATALOG, statement, \"Target catalog '%s' does not exist\", target.getCatalogName());\n+        }\n+        if (metadata.getMetadataResolver(session).getView(target).isPresent()) {\n+            throw new SemanticException(VIEW_ALREADY_EXISTS, statement, \"Target view '%s' already exists\", target);\n+        }\n+        if (!viewName.getSchemaName().equals(target.getSchemaName())) {\n+            throw new SemanticException(NOT_SUPPORTED, statement, \"View rename across schemas is not supported\");\n+        }\n+        if (!viewName.getCatalogName().equals(target.getCatalogName())) {\n+            throw new SemanticException(NOT_SUPPORTED, statement, \"View rename across catalogs is not supported\");\n+        }\n+\n+        accessControl.checkCanRenameView(session.getRequiredTransactionId(), session.getIdentity(), session.getAccessControlContext(), viewName, target);\n+\n+        metadata.renameView(session, viewName, target);\n+\n+        return immediateFuture(null);\n+    }\n+}\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java b/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\nindex 8954086f99db4..83cd4594032f5 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/DelegatingMetadataManager.java\n@@ -432,6 +432,12 @@ public void createView(Session session, String catalogName, ConnectorTableMetada\n         delegate.createView(session, catalogName, viewMetadata, viewData, replace);\n     }\n \n+    @Override\n+    public void renameView(Session session, QualifiedObjectName existingViewName, QualifiedObjectName newViewName)\n+    {\n+        delegate.renameView(session, existingViewName, newViewName);\n+    }\n+\n     @Override\n     public void dropView(Session session, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java b/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\nindex 1cc99fcce3b9b..4f3f698be42c9 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/Metadata.java\n@@ -368,6 +368,11 @@ public interface Metadata\n      */\n     void createView(Session session, String catalogName, ConnectorTableMetadata viewMetadata, String viewData, boolean replace);\n \n+    /**\n+     * Rename the specified view.\n+     */\n+    void renameView(Session session, QualifiedObjectName existingViewName, QualifiedObjectName newViewName);\n+\n     /**\n      * Drops the specified view.\n      */\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\nindex 7f9812c48f891..3041d3990fdf8 100644\n--- a/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/metadata/MetadataManager.java\n@@ -1004,6 +1004,16 @@ public void createView(Session session, String catalogName, ConnectorTableMetada\n         metadata.createView(session.toConnectorSession(connectorId), viewMetadata, viewData, replace);\n     }\n \n+    @Override\n+    public void renameView(Session session, QualifiedObjectName source, QualifiedObjectName target)\n+    {\n+        CatalogMetadata catalogMetadata = getCatalogMetadataForWrite(session, target.getCatalogName());\n+        ConnectorId connectorId = catalogMetadata.getConnectorId();\n+        ConnectorMetadata metadata = catalogMetadata.getMetadata();\n+\n+        metadata.renameView(session.toConnectorSession(connectorId), toSchemaTableName(source), toSchemaTableName(target));\n+    }\n+\n     @Override\n     public void dropView(Session session, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/security/AccessControlManager.java b/presto-main/src/main/java/com/facebook/presto/security/AccessControlManager.java\nindex d4d0125959c53..468d7d0ed2f3b 100644\n--- a/presto-main/src/main/java/com/facebook/presto/security/AccessControlManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/security/AccessControlManager.java\n@@ -514,6 +514,23 @@ public void checkCanCreateView(TransactionId transactionId, Identity identity, A\n         }\n     }\n \n+    @Override\n+    public void checkCanRenameView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName, QualifiedObjectName newViewName)\n+    {\n+        requireNonNull(context, \"context is null\");\n+        requireNonNull(viewName, \"viewName is null\");\n+        requireNonNull(newViewName, \"newViewName is null\");\n+\n+        authenticationCheck(() -> checkCanAccessCatalog(identity, context, viewName.getCatalogName()));\n+\n+        authorizationCheck(() -> systemAccessControl.get().checkCanRenameView(identity, context, toCatalogSchemaTableName(viewName), toCatalogSchemaTableName(newViewName)));\n+\n+        CatalogAccessControlEntry entry = getConnectorAccessControl(transactionId, viewName.getCatalogName());\n+        if (entry != null) {\n+            authorizationCheck(() -> entry.getAccessControl().checkCanRenameView(entry.getTransactionHandle(transactionId), identity.toConnectorIdentity(viewName.getCatalogName()), context, toSchemaTableName(viewName), toSchemaTableName(newViewName)));\n+        }\n+    }\n+\n     @Override\n     public void checkCanDropView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java b/presto-main/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\nindex 4197801047f77..a0e1ea6c315f2 100644\n--- a/presto-main/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\n+++ b/presto-main/src/main/java/com/facebook/presto/security/AllowAllSystemAccessControl.java\n@@ -192,6 +192,11 @@ public void checkCanCreateView(Identity identity, AccessControlContext context,\n     {\n     }\n \n+    @Override\n+    public void checkCanRenameView(Identity identity, AccessControlContext context, CatalogSchemaTableName view, CatalogSchemaTableName newView)\n+    {\n+    }\n+\n     @Override\n     public void checkCanDropView(Identity identity, AccessControlContext context, CatalogSchemaTableName view)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java b/presto-main/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\nindex f9bf1d5b976d8..199490f096f29 100644\n--- a/presto-main/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\n+++ b/presto-main/src/main/java/com/facebook/presto/security/FileBasedSystemAccessControl.java\n@@ -65,6 +65,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetTableProperties;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetUser;\n@@ -388,6 +389,14 @@ public void checkCanCreateView(Identity identity, AccessControlContext context,\n         }\n     }\n \n+    @Override\n+    public void checkCanRenameView(Identity identity, AccessControlContext context, CatalogSchemaTableName view, CatalogSchemaTableName newView)\n+    {\n+        if (!canAccessCatalog(identity, view.getCatalogName(), ALL)) {\n+            denyRenameView(view.toString(), newView.toString());\n+        }\n+    }\n+\n     @Override\n     public void checkCanDropView(Identity identity, AccessControlContext context, CatalogSchemaTableName view)\n     {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\nindex d9fb6707c20d5..1e4e86af06446 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n@@ -136,6 +136,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Return;\n import com.facebook.presto.sql.tree.Revoke;\n@@ -1006,6 +1007,12 @@ protected Scope visitAlterColumnNotNull(AlterColumnNotNull node, Optional<Scope>\n             return createAndAssignScope(node, scope);\n         }\n \n+        @Override\n+        protected Scope visitRenameView(RenameView node, Optional<Scope> scope)\n+        {\n+            return createAndAssignScope(node, scope);\n+        }\n+\n         @Override\n         protected Scope visitDropView(DropView node, Optional<Scope> scope)\n         {\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/util/PrestoDataDefBindingHelper.java b/presto-main/src/main/java/com/facebook/presto/util/PrestoDataDefBindingHelper.java\nindex f83e282373e0d..3170b0f62d792 100644\n--- a/presto-main/src/main/java/com/facebook/presto/util/PrestoDataDefBindingHelper.java\n+++ b/presto-main/src/main/java/com/facebook/presto/util/PrestoDataDefBindingHelper.java\n@@ -42,6 +42,7 @@\n import com.facebook.presto.execution.RenameColumnTask;\n import com.facebook.presto.execution.RenameSchemaTask;\n import com.facebook.presto.execution.RenameTableTask;\n+import com.facebook.presto.execution.RenameViewTask;\n import com.facebook.presto.execution.ResetSessionTask;\n import com.facebook.presto.execution.RevokeRolesTask;\n import com.facebook.presto.execution.RevokeTask;\n@@ -80,6 +81,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Revoke;\n import com.facebook.presto.sql.tree.RevokeRoles;\n@@ -129,6 +131,7 @@ private PrestoDataDefBindingHelper() {}\n         dataDefBuilder.put(DropTable.class, DropTableTask.class);\n         dataDefBuilder.put(TruncateTable.class, TruncateTableTask.class);\n         dataDefBuilder.put(CreateView.class, CreateViewTask.class);\n+        dataDefBuilder.put(RenameView.class, RenameViewTask.class);\n         dataDefBuilder.put(DropView.class, DropViewTask.class);\n         dataDefBuilder.put(CreateMaterializedView.class, CreateMaterializedViewTask.class);\n         dataDefBuilder.put(DropMaterializedView.class, DropMaterializedViewTask.class);\n\ndiff --git a/presto-memory/src/main/java/com/facebook/presto/plugin/memory/MemoryMetadata.java b/presto-memory/src/main/java/com/facebook/presto/plugin/memory/MemoryMetadata.java\nindex 0c8c9e8b62f99..b600eabc8ea16 100644\n--- a/presto-memory/src/main/java/com/facebook/presto/plugin/memory/MemoryMetadata.java\n+++ b/presto-memory/src/main/java/com/facebook/presto/plugin/memory/MemoryMetadata.java\n@@ -291,6 +291,21 @@ else if (views.putIfAbsent(viewName, viewData) != null) {\n         }\n     }\n \n+    @Override\n+    public synchronized void renameView(ConnectorSession session, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        checkSchemaExists(newViewName.getSchemaName());\n+        if (tableIds.containsKey(newViewName)) {\n+            throw new PrestoException(ALREADY_EXISTS, \"Table already exists: \" + newViewName);\n+        }\n+\n+        if (views.containsKey(newViewName)) {\n+            throw new PrestoException(ALREADY_EXISTS, \"View already exists: \" + newViewName);\n+        }\n+\n+        views.put(newViewName, views.remove(viewName));\n+    }\n+\n     @Override\n     public synchronized void dropView(ConnectorSession session, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4 b/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4\nindex c5b80d73eb2b3..e5ee6b44b9a40 100644\n--- a/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4\n+++ b/presto-parser/src/main/antlr4/com/facebook/presto/sql/parser/SqlBase.g4\n@@ -74,6 +74,8 @@ statement\n         | type)                                                        #createType\n     | CREATE (OR REPLACE)? VIEW qualifiedName\n             (SECURITY (DEFINER | INVOKER))? AS query                   #createView\n+    | ALTER VIEW (IF EXISTS)? from=qualifiedName\n+        RENAME TO to=qualifiedName                                     #renameView\n     | DROP VIEW (IF EXISTS)? qualifiedName                             #dropView\n     | CREATE MATERIALIZED VIEW (IF NOT EXISTS)? qualifiedName\n         (COMMENT string)?\n\ndiff --git a/presto-parser/src/main/java/com/facebook/presto/sql/SqlFormatter.java b/presto-parser/src/main/java/com/facebook/presto/sql/SqlFormatter.java\nindex ea7c192cc8592..bb853ba01a8e2 100644\n--- a/presto-parser/src/main/java/com/facebook/presto/sql/SqlFormatter.java\n+++ b/presto-parser/src/main/java/com/facebook/presto/sql/SqlFormatter.java\n@@ -82,6 +82,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Return;\n import com.facebook.presto.sql.tree.Revoke;\n@@ -689,6 +690,20 @@ protected Void visitExternalBodyReference(ExternalBodyReference node, Integer in\n             return null;\n         }\n \n+        @Override\n+        protected Void visitRenameView(RenameView node, Integer context)\n+        {\n+            builder.append(\"ALTER VIEW \");\n+            if (node.isExists()) {\n+                builder.append(\"IF EXISTS \");\n+            }\n+            builder.append(formatName(node.getSource()))\n+                    .append(\" RENAME TO \")\n+                    .append(formatName(node.getTarget()));\n+\n+            return null;\n+        }\n+\n         @Override\n         protected Void visitDropView(DropView node, Integer context)\n         {\n\ndiff --git a/presto-parser/src/main/java/com/facebook/presto/sql/parser/AstBuilder.java b/presto-parser/src/main/java/com/facebook/presto/sql/parser/AstBuilder.java\nindex f916feed3ad76..7b1dd90a57f5e 100644\n--- a/presto-parser/src/main/java/com/facebook/presto/sql/parser/AstBuilder.java\n+++ b/presto-parser/src/main/java/com/facebook/presto/sql/parser/AstBuilder.java\n@@ -127,6 +127,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Return;\n import com.facebook.presto.sql.tree.Revoke;\n@@ -486,6 +487,12 @@ public Node visitRenameColumn(SqlBaseParser.RenameColumnContext context)\n                 context.EXISTS().stream().anyMatch(node -> node.getSymbol().getTokenIndex() > context.COLUMN().getSymbol().getTokenIndex()));\n     }\n \n+    @Override\n+    public Node visitRenameView(SqlBaseParser.RenameViewContext context)\n+    {\n+        return new RenameView(getLocation(context), getQualifiedName(context.from), getQualifiedName(context.to), context.EXISTS() != null);\n+    }\n+\n     @Override\n     public Node visitAnalyze(SqlBaseParser.AnalyzeContext context)\n     {\n\ndiff --git a/presto-parser/src/main/java/com/facebook/presto/sql/tree/AstVisitor.java b/presto-parser/src/main/java/com/facebook/presto/sql/tree/AstVisitor.java\nindex 8eec8cf4936fb..8f42707192a4a 100644\n--- a/presto-parser/src/main/java/com/facebook/presto/sql/tree/AstVisitor.java\n+++ b/presto-parser/src/main/java/com/facebook/presto/sql/tree/AstVisitor.java\n@@ -627,6 +627,11 @@ protected R visitCreateView(CreateView node, C context)\n         return visitStatement(node, context);\n     }\n \n+    protected R visitRenameView(RenameView node, C context)\n+    {\n+        return visitStatement(node, context);\n+    }\n+\n     protected R visitDropView(DropView node, C context)\n     {\n         return visitStatement(node, context);\n\ndiff --git a/presto-parser/src/main/java/com/facebook/presto/sql/tree/RenameView.java b/presto-parser/src/main/java/com/facebook/presto/sql/tree/RenameView.java\nnew file mode 100644\nindex 0000000000000..f591d733ed858\n--- /dev/null\n+++ b/presto-parser/src/main/java/com/facebook/presto/sql/tree/RenameView.java\n@@ -0,0 +1,108 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.sql.tree;\n+\n+import com.google.common.collect.ImmutableList;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+import static com.google.common.base.MoreObjects.toStringHelper;\n+import static java.util.Objects.requireNonNull;\n+\n+public final class RenameView\n+        extends Statement\n+{\n+    private final QualifiedName source;\n+    private final QualifiedName target;\n+\n+    private final boolean exists;\n+\n+    public RenameView(QualifiedName source, QualifiedName target, boolean exists)\n+    {\n+        this(Optional.empty(), source, target, exists);\n+    }\n+\n+    public RenameView(NodeLocation location, QualifiedName source, QualifiedName target, boolean exists)\n+    {\n+        this(Optional.of(location), source, target, exists);\n+    }\n+\n+    private RenameView(Optional<NodeLocation> location, QualifiedName source, QualifiedName target, boolean exists)\n+    {\n+        super(location);\n+        this.source = requireNonNull(source, \"source name is null\");\n+        this.target = requireNonNull(target, \"target name is null\");\n+        this.exists = exists;\n+    }\n+\n+    public QualifiedName getSource()\n+    {\n+        return source;\n+    }\n+\n+    public QualifiedName getTarget()\n+    {\n+        return target;\n+    }\n+\n+    public boolean isExists()\n+    {\n+        return exists;\n+    }\n+\n+    @Override\n+    public <R, C> R accept(AstVisitor<R, C> visitor, C context)\n+    {\n+        return visitor.visitRenameView(this, context);\n+    }\n+\n+    @Override\n+    public List<Node> getChildren()\n+    {\n+        return ImmutableList.of();\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(source, target, exists);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if ((obj == null) || (getClass() != obj.getClass())) {\n+            return false;\n+        }\n+        RenameView o = (RenameView) obj;\n+        return Objects.equals(source, o.source) &&\n+                Objects.equals(target, o.target) &&\n+                Objects.equals(exists, o.exists);\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return toStringHelper(this)\n+                .add(\"source\", source)\n+                .add(\"target\", target)\n+                .add(\"exists\", exists)\n+                .toString();\n+    }\n+}\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\nindex 38f94530bedd5..30e99555fe432 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/AllowAllAccessControl.java\n@@ -131,6 +131,11 @@ public void checkCanCreateView(ConnectorTransactionHandle transaction, Connector\n     {\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\nindex d9d13ccdb314b..e1ee1b874815c 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/FileBasedAccessControl.java\n@@ -57,6 +57,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectTable;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetTableProperties;\n@@ -227,6 +228,14 @@ public void checkCanCreateView(ConnectorTransactionHandle transaction, Connector\n         }\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        if (!checkTablePermission(identity, viewName, OWNERSHIP) || !checkTablePermission(identity, newViewName, OWNERSHIP)) {\n+            denyRenameView(viewName.toString(), newViewName.toString());\n+        }\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\nindex bb4f74134f024..dd28ee9183d56 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingConnectorAccessControl.java\n@@ -167,6 +167,12 @@ public void checkCanCreateView(ConnectorTransactionHandle transactionHandle, Con\n         delegate().checkCanCreateView(transactionHandle, identity, context, viewName);\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        delegate().checkCanRenameView(transactionHandle, identity, context, viewName, newViewName);\n+    }\n+\n     @Override\n     public void checkCanDropView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n     {\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\nindex 0adec4a48aa32..6e9cd6a91c715 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ForwardingSystemAccessControl.java\n@@ -206,6 +206,12 @@ public void checkCanCreateView(Identity identity, AccessControlContext context,\n         delegate().checkCanCreateView(identity, context, view);\n     }\n \n+    @Override\n+    public void checkCanRenameView(Identity identity, AccessControlContext context, CatalogSchemaTableName view, CatalogSchemaTableName newView)\n+    {\n+        delegate().checkCanRenameView(identity, context, view, newView);\n+    }\n+\n     @Override\n     public void checkCanDropView(Identity identity, AccessControlContext context, CatalogSchemaTableName view)\n     {\n\ndiff --git a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ReadOnlyAccessControl.java b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ReadOnlyAccessControl.java\nindex def67490ab690..30022c2079858 100644\n--- a/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ReadOnlyAccessControl.java\n+++ b/presto-plugin-toolkit/src/main/java/com/facebook/presto/plugin/base/security/ReadOnlyAccessControl.java\n@@ -25,6 +25,7 @@\n import java.util.Set;\n \n import static com.facebook.presto.spi.security.AccessDeniedException.denyGrantTablePrivilege;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyTruncateTable;\n \n@@ -71,6 +72,12 @@ public void checkCanCreateViewWithSelectFromColumns(ConnectorTransactionHandle t\n         // allow\n     }\n \n+    @Override\n+    public void checkCanRenameView(ConnectorTransactionHandle transaction, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        denyRenameView(viewName.toString(), newViewName.toString());\n+    }\n+\n     @Override\n     public void checkCanSetCatalogSessionProperty(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, String propertyName)\n     {\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\nindex 62f7208301ee1..e5621d458c0b0 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorAccessControl.java\n@@ -44,6 +44,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeRoles;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectColumns;\n@@ -270,6 +271,16 @@ default void checkCanCreateView(ConnectorTransactionHandle transactionHandle, Co\n         denyCreateView(viewName.toString());\n     }\n \n+    /**\n+     * Check if identity is allowed to rename the specified view in this catalog.\n+     *\n+     * @throws com.facebook.presto.spi.security.AccessDeniedException if not allowed\n+     */\n+    default void checkCanRenameView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        denyRenameView(viewName.toString(), newViewName.toString());\n+    }\n+\n     /**\n      * Check if identity is allowed to drop the specified view in this catalog.\n      *\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\nindex 06ebb4acbc12d..d31c12b68e23b 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/ConnectorMetadata.java\n@@ -561,6 +561,14 @@ default void createView(ConnectorSession session, ConnectorTableMetadata viewMet\n         throw new PrestoException(NOT_SUPPORTED, \"This connector does not support creating views\");\n     }\n \n+    /**\n+     * Rename the specified view\n+     */\n+    default void renameView(ConnectorSession session, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        throw new PrestoException(NOT_SUPPORTED, \"This connector does not support renaming views\");\n+    }\n+\n     /**\n      * Drop the specified view.\n      */\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\nindex f1e5407176141..fa1aeb780f691 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/connector/classloader/ClassLoaderSafeConnectorMetadata.java\n@@ -474,6 +474,14 @@ public void createView(ConnectorSession session, ConnectorTableMetadata viewMeta\n         }\n     }\n \n+    @Override\n+    public void renameView(ConnectorSession session, SchemaTableName viewName, SchemaTableName newViewName)\n+    {\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(classLoader)) {\n+            delegate.renameView(session, viewName, newViewName);\n+        }\n+    }\n+\n     @Override\n     public ColumnHandle getDeleteRowIdColumnHandle(ConnectorSession session, ConnectorTableHandle tableHandle)\n     {\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\nindex c75f3ee58e248..131c0af1577d2 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessControl.java\n@@ -193,6 +193,13 @@ default AuthorizedIdentity selectAuthorizedIdentity(Identity identity, AccessCon\n      */\n     void checkCanCreateView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName);\n \n+    /**\n+     * Check if identity is allowed to rename the specified view.\n+     *\n+     * @throws com.facebook.presto.spi.security.AccessDeniedException if not allowed\n+     */\n+    void checkCanRenameView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName, QualifiedObjectName newViewName);\n+\n     /**\n      * Check if identity is allowed to drop the specified view.\n      *\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessDeniedException.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessDeniedException.java\nindex 1766e1b961998..5992554dd887f 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessDeniedException.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/AccessDeniedException.java\n@@ -251,6 +251,16 @@ public static void denyCreateViewWithSelect(String sourceName, ConnectorIdentity\n         throw new AccessDeniedException(format(\"View owner '%s' cannot create view that selects from %s%s\", identity.getUser(), sourceName, formatExtraInfo(extraInfo)));\n     }\n \n+    public static void denyRenameView(String viewName, String newViewName)\n+    {\n+        denyRenameView(viewName, newViewName, null);\n+    }\n+\n+    public static void denyRenameView(String viewName, String newViewName, String extraInfo)\n+    {\n+        throw new AccessDeniedException(format(\"Cannot rename view from %s to %s%s\", viewName, newViewName, formatExtraInfo(extraInfo)));\n+    }\n+\n     public static void denyDropView(String viewName)\n     {\n         denyDropView(viewName, null);\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/AllowAllAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/AllowAllAccessControl.java\nindex 70458ba83ef03..f07a4e71abc11 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/AllowAllAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/AllowAllAccessControl.java\n@@ -145,6 +145,11 @@ public void checkCanCreateView(TransactionId transactionId, Identity identity, A\n     {\n     }\n \n+    @Override\n+    public void checkCanRenameView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName, QualifiedObjectName newViewName)\n+    {\n+    }\n+\n     @Override\n     public void checkCanDropView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/DenyAllAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/DenyAllAccessControl.java\nindex 0fdbbe5576b61..72b4dbabfc843 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/DenyAllAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/DenyAllAccessControl.java\n@@ -47,6 +47,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeRoles;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectColumns;\n@@ -207,6 +208,12 @@ public void checkCanCreateView(TransactionId transactionId, Identity identity, A\n         denyCreateView(viewName.toString());\n     }\n \n+    @Override\n+    public void checkCanRenameView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName, QualifiedObjectName newViewName)\n+    {\n+        denyRenameView(viewName.toString(), newViewName.toString());\n+    }\n+\n     @Override\n     public void checkCanDropView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java b/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\nindex 7d209ce6b392d..fc5e8cbf6805f 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/security/SystemAccessControl.java\n@@ -42,6 +42,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRevokeTablePrivilege;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectColumns;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetCatalogSessionProperty;\n@@ -300,6 +301,16 @@ default void checkCanCreateView(Identity identity, AccessControlContext context,\n         denyCreateView(view.toString());\n     }\n \n+    /**\n+     * Check if identity is allowed to rename the specified view in a catalog.\n+     *\n+     * @throws AccessDeniedException if not allowed\n+     */\n+    default void checkCanRenameView(Identity identity, AccessControlContext context, CatalogSchemaTableName view, CatalogSchemaTableName newView)\n+    {\n+        denyRenameView(view.toString(), newView.toString());\n+    }\n+\n     /**\n      * Check if identity is allowed to drop the specified view in a catalog.\n      *\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\nindex 4cb866e1072bd..4b18df2cacce7 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergMetadataListing.java\n@@ -153,4 +153,25 @@ public void testTableValidation()\n         assertQuerySucceeds(\"SELECT * FROM iceberg.test_schema.iceberg_table1\");\n         assertQueryFails(\"SELECT * FROM iceberg.test_schema.hive_table\", \"Not an Iceberg table: test_schema.hive_table\");\n     }\n+\n+    @Test\n+    public void testRenameView()\n+    {\n+        assertQuerySucceeds(\"CREATE TABLE iceberg.test_schema.iceberg_test_table (_string VARCHAR, _integer INTEGER)\");\n+        assertUpdate(\"CREATE VIEW iceberg.test_schema.test_view_to_be_renamed AS SELECT * FROM iceberg.test_schema.iceberg_test_table\");\n+        assertUpdate(\"ALTER VIEW IF EXISTS iceberg.test_schema.test_view_to_be_renamed RENAME TO iceberg.test_schema.test_view_renamed\");\n+        assertUpdate(\"CREATE VIEW iceberg.test_schema.test_view2_to_be_renamed AS SELECT * FROM iceberg.test_schema.iceberg_test_table\");\n+        assertUpdate(\"ALTER VIEW iceberg.test_schema.test_view2_to_be_renamed RENAME TO iceberg.test_schema.test_view2_renamed\");\n+        assertQuerySucceeds(\"SELECT * FROM iceberg.test_schema.test_view_renamed\");\n+        assertQuerySucceeds(\"SELECT * FROM iceberg.test_schema.test_view2_renamed\");\n+        assertUpdate(\"DROP VIEW iceberg.test_schema.test_view_renamed\");\n+        assertUpdate(\"DROP VIEW iceberg.test_schema.test_view2_renamed\");\n+        assertUpdate(\"DROP TABLE iceberg.test_schema.iceberg_test_table\");\n+    }\n+    @Test\n+    public void testRenameViewIfNotExists()\n+    {\n+        assertQueryFails(\"ALTER VIEW iceberg.test_schema.test_rename_view_not_exist RENAME TO iceberg.test_schema.test_renamed_view_not_exist\", \"line 1:1: View 'iceberg.test_schema.test_rename_view_not_exist' does not exist\");\n+        assertQuerySucceeds(\"ALTER VIEW IF EXISTS iceberg.test_schema.test_rename_view_not_exist RENAME TO iceberg.test_schema.test_renamed_view_not_exist\");\n+    }\n }\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\nindex 3656b5d734936..af9e69895b349 100644\n--- a/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n+++ b/presto-main/src/main/java/com/facebook/presto/testing/LocalQueryRunner.java\n@@ -70,6 +70,7 @@\n import com.facebook.presto.execution.QueryManagerConfig;\n import com.facebook.presto.execution.RenameColumnTask;\n import com.facebook.presto.execution.RenameTableTask;\n+import com.facebook.presto.execution.RenameViewTask;\n import com.facebook.presto.execution.ResetSessionTask;\n import com.facebook.presto.execution.RollbackTask;\n import com.facebook.presto.execution.ScheduledSplit;\n@@ -210,6 +211,7 @@\n import com.facebook.presto.sql.tree.Prepare;\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Rollback;\n import com.facebook.presto.sql.tree.SetProperties;\n@@ -578,6 +580,7 @@ private LocalQueryRunner(Session defaultSession, FeaturesConfig featuresConfig,\n                 .put(DropMaterializedView.class, new DropMaterializedViewTask())\n                 .put(RenameColumn.class, new RenameColumnTask())\n                 .put(RenameTable.class, new RenameTableTask())\n+                .put(RenameView.class, new RenameViewTask())\n                 .put(ResetSession.class, new ResetSessionTask())\n                 .put(SetSession.class, new SetSessionTask())\n                 .put(Prepare.class, new PrepareTask(sqlParser))\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java b/presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\nindex 1db74c076d0b2..fa2e681c1878d 100644\n--- a/presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\n+++ b/presto-main/src/main/java/com/facebook/presto/testing/TestingAccessControlManager.java\n@@ -50,6 +50,7 @@\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameColumn;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameSchema;\n import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameTable;\n+import static com.facebook.presto.spi.security.AccessDeniedException.denyRenameView;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySelectColumns;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetCatalogSessionProperty;\n import static com.facebook.presto.spi.security.AccessDeniedException.denySetSystemSessionProperty;\n@@ -73,6 +74,7 @@\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.RENAME_COLUMN;\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.RENAME_SCHEMA;\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.RENAME_TABLE;\n+import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.RENAME_VIEW;\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.SELECT_COLUMN;\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.SET_SESSION;\n import static com.facebook.presto.testing.TestingAccessControlManager.TestingPrivilegeType.SET_TABLE_PROPERTIES;\n@@ -286,6 +288,17 @@ public void checkCanCreateView(TransactionId transactionId, Identity identity, A\n         }\n     }\n \n+    @Override\n+    public void checkCanRenameView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName, QualifiedObjectName newViewName)\n+    {\n+        if (shouldDenyPrivilege(identity.getUser(), viewName.getObjectName(), RENAME_VIEW)) {\n+            denyRenameView(viewName.toString(), newViewName.toString());\n+        }\n+        if (denyPrivileges.isEmpty()) {\n+            super.checkCanRenameView(transactionId, identity, context, viewName, newViewName);\n+        }\n+    }\n+\n     @Override\n     public void checkCanDropView(TransactionId transactionId, Identity identity, AccessControlContext context, QualifiedObjectName viewName)\n     {\n@@ -383,7 +396,7 @@ public enum TestingPrivilegeType\n         CREATE_TABLE, DROP_TABLE, RENAME_TABLE, INSERT_TABLE, DELETE_TABLE, TRUNCATE_TABLE, UPDATE_TABLE,\n         ADD_COLUMN, DROP_COLUMN, RENAME_COLUMN, SELECT_COLUMN,\n         ADD_CONSTRAINT, DROP_CONSTRAINT,\n-        CREATE_VIEW, DROP_VIEW, CREATE_VIEW_WITH_SELECT_COLUMNS, SET_TABLE_PROPERTIES,\n+        CREATE_VIEW, RENAME_VIEW, DROP_VIEW, CREATE_VIEW_WITH_SELECT_COLUMNS, SET_TABLE_PROPERTIES,\n         SET_SESSION\n     }\n \n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java b/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\nindex 48210df7d5594..49a6051c22362 100644\n--- a/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\n+++ b/presto-main/src/test/java/com/facebook/presto/metadata/AbstractMockMetadata.java\n@@ -464,6 +464,12 @@ public void createView(Session session, String catalogName, ConnectorTableMetada\n         throw new UnsupportedOperationException();\n     }\n \n+    @Override\n+    public void renameView(Session session, QualifiedObjectName source, QualifiedObjectName target)\n+    {\n+        throw new UnsupportedOperationException();\n+    }\n+\n     @Override\n     public void dropView(Session session, QualifiedObjectName viewName)\n     {\n\ndiff --git a/presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java b/presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\nindex 7039ba042697d..767e0a06d35b3 100644\n--- a/presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\n+++ b/presto-main/src/test/java/com/facebook/presto/security/TestAccessControlManager.java\n@@ -458,6 +458,12 @@ public void checkCanCreateView(ConnectorTransactionHandle transactionHandle, Con\n             throw new UnsupportedOperationException();\n         }\n \n+        @Override\n+        public void checkCanRenameView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName, SchemaTableName newViewName)\n+        {\n+            throw new UnsupportedOperationException();\n+        }\n+\n         @Override\n         public void checkCanDropView(ConnectorTransactionHandle transactionHandle, ConnectorIdentity identity, AccessControlContext context, SchemaTableName viewName)\n         {\n\ndiff --git a/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java b/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java\nindex 1a1c200935f2a..acc108c57a76b 100644\n--- a/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java\n+++ b/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemoryMetadata.java\n@@ -226,6 +226,8 @@ public void testViews()\n                 test2,\n                 ImmutableList.of(new ColumnMetadata(\"a\", BIGINT)));\n \n+        SchemaTableName test3 = new SchemaTableName(\"test\", \"test_view3\");\n+\n         // create schema\n         metadata.createSchema(SESSION, \"test\", ImmutableMap.of());\n \n@@ -265,8 +267,14 @@ public void testViews()\n         views = metadata.getViews(SESSION, new SchemaTablePrefix(\"test\"));\n         assertEquals(views.keySet(), ImmutableSet.of(test2));\n \n+        // rename second view\n+        metadata.renameView(SESSION, test2, test3);\n+\n+        Map<?, ?> result = metadata.getViews(SESSION, new SchemaTablePrefix(\"test\"));\n+        assertTrue(result.containsKey(test3));\n+\n         // drop second view\n-        metadata.dropView(SESSION, test2);\n+        metadata.dropView(SESSION, test3);\n \n         views = metadata.getViews(SESSION, new SchemaTablePrefix(\"test\"));\n         assertTrue(views.isEmpty());\n\ndiff --git a/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java b/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java\nindex 35c31c556540e..4e0a436852913 100644\n--- a/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java\n+++ b/presto-memory/src/test/java/com/facebook/presto/plugin/memory/TestMemorySmoke.java\n@@ -185,6 +185,28 @@ public void testViews()\n         assertQueryFails(\"DROP VIEW test_view\", \"line 1:1: View 'memory.default.test_view' does not exist\");\n     }\n \n+    @Test\n+    public void testRenameView()\n+    {\n+        @Language(\"SQL\") String query = \"SELECT orderkey, orderstatus, totalprice / 2 half FROM orders\";\n+\n+        assertUpdate(\"CREATE VIEW test_view_to_be_renamed AS \" + query);\n+        assertUpdate(\"ALTER VIEW test_view_to_be_renamed RENAME TO test_view_renamed\");\n+        assertQuery(\"SELECT * FROM test_view_renamed\", query);\n+\n+        assertUpdate(\"CREATE SCHEMA test_different_schema\");\n+        assertQueryFails(\"ALTER VIEW test_view_renamed RENAME TO test_different_schema.test_view_renamed\", \"line 1:1: View rename across schemas is not supported\");\n+        assertUpdate(\"DROP VIEW test_view_renamed\");\n+        assertUpdate(\"DROP SCHEMA test_different_schema\");\n+    }\n+\n+    @Test\n+    public void testRenameViewIfNotExists()\n+    {\n+        assertQueryFails(\"ALTER VIEW test_rename_view_not_exist RENAME TO test_renamed_view_not_exist\", \"line 1:1: View 'memory.default.test_rename_view_not_exist' does not exist\");\n+        assertQuerySucceeds(\"ALTER VIEW IF EXISTS test_rename_view_not_exist RENAME TO test_renamed_view_not_exist\");\n+    }\n+\n     private List<QualifiedObjectName> listMemoryTables()\n     {\n         return getQueryRunner().listTables(getSession(), \"memory\", \"default\");\n\ndiff --git a/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java b/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\nindex ce3715b8c296f..68b590834653d 100644\n--- a/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\n+++ b/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\n@@ -110,6 +110,7 @@\n import com.facebook.presto.sql.tree.RenameColumn;\n import com.facebook.presto.sql.tree.RenameSchema;\n import com.facebook.presto.sql.tree.RenameTable;\n+import com.facebook.presto.sql.tree.RenameView;\n import com.facebook.presto.sql.tree.ResetSession;\n import com.facebook.presto.sql.tree.Return;\n import com.facebook.presto.sql.tree.Revoke;\n@@ -1505,6 +1506,13 @@ public void testTruncateTable()\n         assertStatement(\"TRUNCATE TABLE a.b.c\", new TruncateTable(QualifiedName.of(\"a\", \"b\", \"c\")));\n     }\n \n+    @Test\n+    public void testRenameView()\n+    {\n+        assertStatement(\"ALTER VIEW a RENAME TO b\", new RenameView(QualifiedName.of(\"a\"), QualifiedName.of(\"b\"), false));\n+        assertStatement(\"ALTER VIEW IF EXISTS a RENAME TO b\", new RenameView(QualifiedName.of(\"a\"), QualifiedName.of(\"b\"), true));\n+    }\n+\n     @Test\n     public void testDropView()\n     {\n\ndiff --git a/presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java b/presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java\nindex 1c081887df656..daf64d750d421 100644\n--- a/presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java\n+++ b/presto-plugin-toolkit/src/test/java/com/facebook/presto/plugin/base/security/TestFileBasedAccessControl.java\n@@ -69,7 +69,12 @@ public void testTableRules()\n         accessControl.checkCanDeleteFromTable(TRANSACTION_HANDLE, user(\"bob\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"));\n         accessControl.checkCanSelectFromColumns(TRANSACTION_HANDLE, user(\"joe\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), ImmutableSet.of());\n         accessControl.checkCanCreateViewWithSelectFromColumns(TRANSACTION_HANDLE, user(\"bob\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), ImmutableSet.of());\n+        accessControl.checkCanRenameTable(TRANSACTION_HANDLE, user(\"admin\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), new SchemaTableName(\"aliceschema\", \"newbobtable\"));\n+        accessControl.checkCanRenameTable(TRANSACTION_HANDLE, user(\"alice\"), CONTEXT, new SchemaTableName(\"aliceschema\", \"alicetable\"), new SchemaTableName(\"aliceschema\", \"newalicetable\"));\n+        accessControl.checkCanRenameView(TRANSACTION_HANDLE, user(\"admin\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobview\"), new SchemaTableName(\"aliceschema\", \"newbobview\"));\n+        accessControl.checkCanRenameView(TRANSACTION_HANDLE, user(\"alice\"), CONTEXT, new SchemaTableName(\"aliceschema\", \"aliceview\"), new SchemaTableName(\"aliceschema\", \"newaliceview\"));\n         accessControl.checkCanDropTable(TRANSACTION_HANDLE, user(\"admin\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"));\n+        assertDenied(() -> accessControl.checkCanRenameTable(TRANSACTION_HANDLE, user(\"bob\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), new SchemaTableName(\"bobschema\", \"newbobtable\")));\n         accessControl.checkCanSetTableProperties(TRANSACTION_HANDLE, user(\"admin\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), ImmutableMap.of());\n         accessControl.checkCanSetTableProperties(TRANSACTION_HANDLE, user(\"alice\"), CONTEXT, new SchemaTableName(\"aliceSchema\", \"aliceTable\"), ImmutableMap.of());\n         assertDenied(() -> accessControl.checkCanInsertIntoTable(TRANSACTION_HANDLE, user(\"alice\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\")));\n@@ -79,6 +84,8 @@ public void testTableRules()\n         assertDenied(() -> accessControl.checkCanSelectFromColumns(TRANSACTION_HANDLE, user(\"admin\"), CONTEXT, new SchemaTableName(\"secret\", \"secret\"), ImmutableSet.of()));\n         assertDenied(() -> accessControl.checkCanSelectFromColumns(TRANSACTION_HANDLE, user(\"joe\"), CONTEXT, new SchemaTableName(\"secret\", \"secret\"), ImmutableSet.of()));\n         assertDenied(() -> accessControl.checkCanCreateViewWithSelectFromColumns(TRANSACTION_HANDLE, user(\"joe\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobtable\"), ImmutableSet.of()));\n+        assertDenied(() -> accessControl.checkCanRenameView(TRANSACTION_HANDLE, user(\"bob\"), CONTEXT, new SchemaTableName(\"bobschema\", \"bobview\"), new SchemaTableName(\"bobschema\", \"newbobview\")));\n+        assertDenied(() -> accessControl.checkCanRenameView(TRANSACTION_HANDLE, user(\"alice\"), CONTEXT, new SchemaTableName(\"aliceschema\", \"alicetable\"), new SchemaTableName(\"bobschema\", \"newalicetable\")));\n     }\n \n     @Test\n\ndiff --git a/presto-plugin-toolkit/src/test/resources/table.json b/presto-plugin-toolkit/src/test/resources/table.json\nindex 18b76320fc162..2ebef52bdcdb6 100644\n--- a/presto-plugin-toolkit/src/test/resources/table.json\n+++ b/presto-plugin-toolkit/src/test/resources/table.json\n@@ -9,6 +9,11 @@\n       \"schema\": \".*\",\n       \"privileges\": [\"SELECT\", \"INSERT\", \"DELETE\", \"OWNERSHIP\"]\n     },\n+    {\n+      \"user\": \"alice\",\n+      \"schema\": \"aliceschema\",\n+      \"privileges\": [\"SELECT\", \"INSERT\", \"DELETE\", \"OWNERSHIP\"]\n+    },\n     {\n       \"user\": \"bob\",\n       \"schema\": \"bobschema\",\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23739",
    "pr_id": 23739,
    "issue_id": 23491,
    "repo": "prestodb/presto",
    "problem_statement": "Iceberg REST configurable OAuth Endpoint\n## Expected Behavior or Use Case\r\n[Recent changes in Iceberg REST](https://github.com/apache/iceberg/pull/10603/files#diff-02549ca620d020dc9ead80088cc14e311e12a69651fa8d394cd41a4308debb2e) deprecated the oauth2 endpoint as part of the REST spec. Instead, the OAuth2 endpoint of the IdP should be used.\r\n\r\n**Extract from the updated spec:**\r\nThe oauth/tokens endpoint is DEPRECATED for REMOVAL. It is not recommended to implement this endpoint, unless you are fully aware of the potential security implications.\r\n\r\nAll clients are encouraged to explicitly set the configuration property oauth2-server-uri to the correct OAuth endpoint.\r\n\r\n## Presto Component, Service, or Connector\r\nIceberg REST\r\n\r\n## Possible Implementation\r\nIt would be great to make the oauth2 server uri configurable by adding a `iceberg.rest.auth.oauth2.server-uri` configuration for the Iceberg Rest Catalog.\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 170,
    "test_files_count": 2,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java"
    ],
    "base_commit": "83cb0f6ffbfeffa74a027ad5f5ec73b39adaf36c",
    "head_commit": "3f78bf7f87d97ce3d4dc8fb11b4d098dbf2de297",
    "repo_url": "https://github.com/prestodb/presto/pull/23739",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23739",
    "dockerfile": "",
    "pr_merged_at": "2024-10-01T17:41:31.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 727ba555f0fb7..b743a130e2dec 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -210,6 +210,9 @@ Property Name                                        Description\n                                                      Available values are ``NONE`` or ``OAUTH2`` (default: ``NONE``).\n                                                      ``OAUTH2`` requires either a credential or token.\n \n+``iceberg.rest.auth.oauth2.uri``                     OAUTH2 server endpoint URI.\n+                                                     Example: ``https://localhost:9191``\n+\n ``iceberg.rest.auth.oauth2.credential``              The credential to use for OAUTH2 authentication.\n                                                      Example: ``key:secret``\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\nindex ababf3b2e2cbd..771b8338f7817 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestCatalogFactory.java\n@@ -35,6 +35,7 @@\n import static java.util.Objects.requireNonNull;\n import static org.apache.iceberg.CatalogProperties.URI;\n import static org.apache.iceberg.rest.auth.OAuth2Properties.CREDENTIAL;\n+import static org.apache.iceberg.rest.auth.OAuth2Properties.OAUTH2_SERVER_URI;\n import static org.apache.iceberg.rest.auth.OAuth2Properties.TOKEN;\n \n public class IcebergRestCatalogFactory\n@@ -67,6 +68,11 @@ protected Map<String, String> getCatalogProperties(ConnectorSession session)\n \n         catalogConfig.getAuthenticationType().ifPresent(type -> {\n             if (type == OAUTH2) {\n+                // The oauth2/tokens endpoint of the REST catalog spec has been deprecated and will\n+                // be removed in Iceberg 2.0 (https://github.com/apache/iceberg/pull/10603)\n+                // TODO auth server URI will eventually need to be made a required property\n+                catalogConfig.getAuthenticationServerUri().ifPresent(authServerUri -> properties.put(OAUTH2_SERVER_URI, authServerUri));\n+\n                 if (!catalogConfig.credentialOrTokenExists()) {\n                     throw new IllegalStateException(\"iceberg.rest.auth.oauth2 requires either a credential or a token\");\n                 }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\nindex b00c68f09aca5..fe0d1a5522cb3 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/rest/IcebergRestConfig.java\n@@ -25,6 +25,7 @@ public class IcebergRestConfig\n     private String serverUri;\n     private SessionType sessionType;\n     private AuthenticationType authenticationType;\n+    private String authenticationServerUri;\n     private String credential;\n     private String token;\n \n@@ -68,6 +69,19 @@ public IcebergRestConfig setAuthenticationType(AuthenticationType authentication\n         return this;\n     }\n \n+    public Optional<String> getAuthenticationServerUri()\n+    {\n+        return Optional.ofNullable(authenticationServerUri);\n+    }\n+\n+    @Config(\"iceberg.rest.auth.oauth2.uri\")\n+    @ConfigDescription(\"The URI to connect to the OAUTH2 server\")\n+    public IcebergRestConfig setAuthenticationServerUri(String authServerUri)\n+    {\n+        this.authenticationServerUri = authServerUri;\n+        return this;\n+    }\n+\n     public Optional<String> getCredential()\n     {\n         return Optional.ofNullable(credential);\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\nindex 730a58d862c5b..a332a8f6e6fc0 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergRestConfig.java\n@@ -32,6 +32,7 @@ public void testDefaults()\n         assertRecordedDefaults(ConfigAssertions.recordDefaults(IcebergRestConfig.class)\n                 .setServerUri(null)\n                 .setAuthenticationType(null)\n+                .setAuthenticationServerUri(null)\n                 .setCredential(null)\n                 .setToken(null)\n                 .setSessionType(null));\n@@ -43,6 +44,7 @@ public void testExplicitPropertyMappings()\n         Map<String, String> properties = ImmutableMap.<String, String>builder()\n                 .put(\"iceberg.rest.uri\", \"http://localhost:xxx\")\n                 .put(\"iceberg.rest.auth.type\", \"OAUTH2\")\n+                .put(\"iceberg.rest.auth.oauth2.uri\", \"http://localhost:yyy\")\n                 .put(\"iceberg.rest.auth.oauth2.credential\", \"key:secret\")\n                 .put(\"iceberg.rest.auth.oauth2.token\", \"SXVLUXUhIExFQ0tFUiEK\")\n                 .put(\"iceberg.rest.session.type\", \"USER\")\n@@ -51,6 +53,7 @@ public void testExplicitPropertyMappings()\n         IcebergRestConfig expected = new IcebergRestConfig()\n                 .setServerUri(\"http://localhost:xxx\")\n                 .setAuthenticationType(OAUTH2)\n+                .setAuthenticationServerUri(\"http://localhost:yyy\")\n                 .setCredential(\"key:secret\")\n                 .setToken(\"SXVLUXUhIExFQ0tFUiEK\")\n                 .setSessionType(USER);\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java\nindex 65c34bc31bbee..d74c11c4ad063 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/rest/TestIcebergSmokeRest.java\n@@ -29,6 +29,7 @@\n import com.facebook.presto.testing.QueryRunner;\n import com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.rest.RESTCatalog;\n import org.assertj.core.util.Files;\n import org.testng.annotations.AfterClass;\n import org.testng.annotations.BeforeClass;\n@@ -42,12 +43,15 @@\n import static com.facebook.presto.iceberg.FileFormat.PARQUET;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.ICEBERG_CATALOG;\n import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergTable;\n+import static com.facebook.presto.iceberg.rest.AuthenticationType.OAUTH2;\n import static com.facebook.presto.iceberg.rest.IcebergRestTestUtil.getRestServer;\n import static com.facebook.presto.iceberg.rest.IcebergRestTestUtil.restConnectorProperties;\n import static com.google.common.io.MoreFiles.deleteRecursively;\n import static com.google.common.io.RecursiveDeleteOption.ALLOW_INSECURE;\n import static java.lang.String.format;\n+import static org.apache.iceberg.rest.auth.OAuth2Properties.OAUTH2_SERVER_URI;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.testng.Assert.assertEquals;\n \n @Test\n public class TestIcebergSmokeRest\n@@ -106,12 +110,11 @@ protected QueryRunner createQueryRunner()\n                 Optional.of(warehouseLocation.toPath()));\n     }\n \n-    protected IcebergNativeCatalogFactory getCatalogFactory()\n+    protected IcebergNativeCatalogFactory getCatalogFactory(IcebergRestConfig restConfig)\n     {\n         IcebergConfig icebergConfig = new IcebergConfig()\n                 .setCatalogType(REST)\n-                .setCatalogWarehouse(warehouseLocation.getAbsolutePath().toString());\n-        IcebergRestConfig restConfig = new IcebergRestConfig().setServerUri(serverUri);\n+                .setCatalogWarehouse(warehouseLocation.getAbsolutePath());\n \n         return new IcebergRestCatalogFactory(\n                 icebergConfig,\n@@ -125,7 +128,8 @@ protected IcebergNativeCatalogFactory getCatalogFactory()\n     @Override\n     protected Table getIcebergTable(ConnectorSession session, String schema, String tableName)\n     {\n-        return getNativeIcebergTable(getCatalogFactory(),\n+        IcebergRestConfig restConfig = new IcebergRestConfig().setServerUri(serverUri);\n+        return getNativeIcebergTable(getCatalogFactory(restConfig),\n                 session,\n                 SchemaTableName.valueOf(schema + \".\" + tableName));\n     }\n@@ -192,4 +196,20 @@ public void testMetadataDeleteOnTableWithUnsupportedSpecsWhoseDataAllDeleted(Str\n             super.testMetadataDeleteOnTableWithUnsupportedSpecsWhoseDataAllDeleted(version, mode);\n         }\n     }\n+\n+    @Test\n+    public void testSetOauth2ServerUriPropertyI()\n+    {\n+        String authEndpoint = \"http://localhost:8888\";\n+        IcebergRestConfig restConfig = new IcebergRestConfig()\n+                .setServerUri(serverUri)\n+                .setAuthenticationType(OAUTH2)\n+                .setToken(\"SXVLUXUhIExFQ0tFUiEK\")\n+                .setAuthenticationServerUri(authEndpoint);\n+\n+        IcebergRestCatalogFactory catalogFactory = (IcebergRestCatalogFactory) getCatalogFactory(restConfig);\n+        RESTCatalog catalog = (RESTCatalog) catalogFactory.getCatalog(getSession().toConnectorSession());\n+\n+        assertEquals(catalog.properties().get(OAUTH2_SERVER_URI), authEndpoint);\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23703",
    "pr_id": 23703,
    "issue_id": 23702,
    "repo": "prestodb/presto",
    "problem_statement": "Error when select count(*) operation in presto\n`Select count(*)` returns `java.lang.ArrayIndexOutOfBoundsException: 0` for Prometheus\r\n\r\n<img width=\"1255\" alt=\"image\" src=\"https://github.com/user-attachments/assets/dc6797b7-84d5-4a4f-9f37-aeabfb60025e\">\r\n",
    "issue_word_count": 33,
    "test_files_count": 2,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusRecordCursor.java",
      "presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusStandardizedRow.java",
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java",
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java"
    ],
    "pr_changed_test_files": [
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java",
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java"
    ],
    "base_commit": "6efcf0f4567c2e1f249ee96051e81feb76dd7af8",
    "head_commit": "e70004d047fea7b95476ce2a5b7ba4b0fa061b32",
    "repo_url": "https://github.com/prestodb/presto/pull/23703",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23703",
    "dockerfile": "",
    "pr_merged_at": "2024-10-16T12:32:19.000Z",
    "patch": "diff --git a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusRecordCursor.java b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusRecordCursor.java\nindex 92adbe587f0a4..fd4f58f6d403c 100644\n--- a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusRecordCursor.java\n+++ b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusRecordCursor.java\n@@ -270,7 +270,7 @@ private Object getFieldValue(int field)\n         int columnIndex = fieldToColumnIndex[field];\n         switch (columnIndex) {\n             case 0:\n-                return fields.getLabels();\n+                return getBlockFromMap(columnHandles.get(columnIndex).getColumnType(), fields.getLabels());\n             case 1:\n                 return fields.getTimestamp();\n             case 2:\n@@ -284,13 +284,12 @@ private void checkFieldType(int field, Type expected)\n         Type actual = getType(field);\n         checkArgument(actual.equals(expected), \"Expected field %s to be type %s but is %s\", field, expected, actual);\n     }\n-\n     private List<PrometheusStandardizedRow> prometheusResultsInStandardizedForm(List<PrometheusMetricResult> results)\n     {\n         return results.stream().map(result ->\n                 result.getTimeSeriesValues().getValues().stream().map(prometheusTimeSeriesValue ->\n                         new PrometheusStandardizedRow(\n-                                getBlockFromMap(columnHandles.get(0).getColumnType(), metricHeaderToMap(result.getMetricHeader())),\n+                                result.getMetricHeader(),\n                                 prometheusTimeSeriesValue.getTimestamp(),\n                                 Double.parseDouble(prometheusTimeSeriesValue.getValue())))\n                         .collect(Collectors.toList()))\n\ndiff --git a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusStandardizedRow.java b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusStandardizedRow.java\nindex ba6ab3e2aeaaf..afac77afc5c31 100644\n--- a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusStandardizedRow.java\n+++ b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusStandardizedRow.java\n@@ -13,26 +13,25 @@\n  */\n package com.facebook.presto.plugin.prometheus;\n \n-import com.facebook.presto.common.block.Block;\n-\n import java.time.Instant;\n+import java.util.Map;\n \n import static java.util.Objects.requireNonNull;\n \n public class PrometheusStandardizedRow\n {\n-    private final Block labels;\n+    private final Map<String, String> labels;\n     private final Instant timestamp;\n     private final Double value;\n \n-    public PrometheusStandardizedRow(Block labels, Instant timestamp, Double value)\n+    public PrometheusStandardizedRow(Map<String, String> labels, Instant timestamp, Double value)\n     {\n         this.labels = requireNonNull(labels, \"labels is null\");\n         this.timestamp = requireNonNull(timestamp, \"timestamp is null\");\n         this.value = requireNonNull(value, \"value is null\");\n     }\n \n-    public Block getLabels()\n+    public Map<String, String> getLabels()\n     {\n         return labels;\n     }\n",
    "test_patch": "diff --git a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java\nindex 7910f65023eca..d55b7412b61d0 100644\n--- a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java\n+++ b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusMetricsIntegration.java\n@@ -118,4 +118,13 @@ public void testPushDown()\n         MaterializedResult results = runner.execute(session, \"SELECT * FROM prometheus.default.up WHERE timestamp > (NOW() - INTERVAL '15' SECOND)\").toTestTypes();\n         assertEquals(results.getRowCount(), 1);\n     }\n+    @Test(priority = 3, dependsOnMethods = \"testConfirmMetricAvailableAndCheckUp\")\n+    public void testCountQuery()\n+    {\n+        MaterializedResult countResult = runner.execute(session, \"SELECT COUNT(*) FROM prometheus.default.up\").toTestTypes();\n+        assertEquals(countResult.getRowCount(), 1);\n+        MaterializedRow countRow = countResult.getMaterializedRows().get(0);\n+        long countValue = (Long) countRow.getField(0);\n+        assert countValue >= 1 : \"Expected COUNT(*) to be >= 1, but got \" + countValue;\n+    }\n }\n\ndiff --git a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java\nindex 843d8426dd1e5..e551afa785399 100644\n--- a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java\n+++ b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusRecordSet.java\n@@ -36,6 +36,7 @@\n import static com.facebook.presto.plugin.prometheus.PrometheusRecordCursor.getBlockFromMap;\n import static com.facebook.presto.plugin.prometheus.PrometheusRecordCursor.getMapFromBlock;\n import static com.facebook.presto.plugin.prometheus.TestPrometheusTable.TYPE_MANAGER;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n import static java.time.Instant.ofEpochMilli;\n import static org.testng.Assert.assertEquals;\n import static org.testng.Assert.assertFalse;\n@@ -64,7 +65,8 @@ public void testCursorSimple()\n         List<PrometheusStandardizedRow> actual = new ArrayList<>();\n         while (cursor.advanceNextPosition()) {\n             actual.add(new PrometheusStandardizedRow(\n-                    (Block) cursor.getObject(0),\n+                    getMapFromBlock(varcharMapType, (Block) cursor.getObject(0)).entrySet().stream()\n+                            .collect(toImmutableMap(entry -> (String) entry.getKey(), entry -> (String) entry.getValue())),\n                     ((Instant) cursor.getObject(1)),\n                     cursor.getDouble(2)));\n             assertFalse(cursor.isNull(0));\n@@ -72,19 +74,19 @@ public void testCursorSimple()\n             assertFalse(cursor.isNull(2));\n         }\n         List<PrometheusStandardizedRow> expected = ImmutableList.<PrometheusStandardizedRow>builder()\n-                .add(new PrometheusStandardizedRow(getBlockFromMap(varcharMapType,\n-                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\")), ofEpochMilli(1565962969044L), 1.0))\n-                .add(new PrometheusStandardizedRow(getBlockFromMap(varcharMapType,\n-                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\")), ofEpochMilli(1565962984045L), 1.0))\n-                .add(new PrometheusStandardizedRow(getBlockFromMap(varcharMapType,\n-                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\")), ofEpochMilli(1565962999044L), 1.0))\n-                .add(new PrometheusStandardizedRow(getBlockFromMap(varcharMapType,\n-                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\")), ofEpochMilli(1565963014044L), 1.0))\n+                .add(new PrometheusStandardizedRow(\n+                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\"), ofEpochMilli(1565962969044L), 1.0))\n+                .add(new PrometheusStandardizedRow(\n+                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\"), ofEpochMilli(1565962984045L), 1.0))\n+                .add(new PrometheusStandardizedRow(\n+                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\"), ofEpochMilli(1565962999044L), 1.0))\n+                .add(new PrometheusStandardizedRow(\n+                        ImmutableMap.of(\"instance\", \"localhost:9090\", \"__name__\", \"up\", \"job\", \"prometheus\"), ofEpochMilli(1565963014044L), 1.0))\n                 .build();\n         List<PairLike<PrometheusStandardizedRow, PrometheusStandardizedRow>> pairs = Streams.zip(actual.stream(), expected.stream(), PairLike::new)\n                 .collect(Collectors.toList());\n         pairs.forEach(pair -> {\n-            assertEquals(getMapFromBlock(varcharMapType, pair.getFirst().getLabels()), getMapFromBlock(varcharMapType, pair.getSecond().getLabels()));\n+            assertEquals(getMapFromBlock(varcharMapType, getBlockFromMap(varcharMapType, pair.getFirst().getLabels())), getMapFromBlock(varcharMapType, getBlockFromMap(varcharMapType, pair.getSecond().getLabels())));\n             assertEquals(pair.getFirst().getTimestamp(), pair.getSecond().getTimestamp());\n             assertEquals(pair.getFirst().getValue(), pair.getSecond().getValue());\n         });\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23701",
    "pr_id": 23701,
    "issue_id": 23686,
    "repo": "prestodb/presto",
    "problem_statement": "CI / product-tests-specific-environment2, `Product Tests Specific 2.8` for sql-server is flaky\nhttps://github.com/prestodb/presto/actions/runs/10945815176/job/30390900107\r\n\r\nThe flakiness seems related to the sql-server setup: `The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".`\r\n\r\ncc @agrawalreetika\r\n\r\n```\r\n2024-09-20 00:02:49 SEVERE: cannot initialize test suite\r\njava.lang.RuntimeException: java.sql.SQLException: Cannot create PoolableConnectionFactory (The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".)\r\n\tat com.google.common.base.Throwables.propagate(Throwables.java:249)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.openConnection(JdbcQueryExecutor.java:62)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.getConnection(JdbcQueryExecutor.java:90)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.AbstractTableManager.getTableNames(AbstractTableManager.java:63)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.AbstractTableManager.dropStaleMutableTables(AbstractTableManager.java:52)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.TableRequirementFulfiller.createTable(TableRequirementFulfiller.java:75)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.DistinctOps$1$2.accept(DistinctOps.java:175)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1553)\r\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\r\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.TableRequirementFulfiller.fulfill(TableRequirementFulfiller.java:60)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.lambda$doFulfillment$0(TestInitializationListener.java:326)\r\n\tat io.prestodb.tempto.context.TestContextDsl.runWithTestContext(TestContextDsl.java:51)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.doFulfillment(TestInitializationListener.java:324)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.onStart(TestInitializationListener.java:189)\r\n\tat org.testng.TestRunner.fireEvent(TestRunner.java:938)\r\n\tat org.testng.TestRunner.beforeRun(TestRunner.java:630)\r\n\tat org.testng.TestRunner.run(TestRunner.java:596)\r\n\tat org.testng.SuiteRunner.runTest(SuiteRunner.java:429)\r\n\tat org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423)\r\n\tat org.testng.SuiteRunner.privateRun(SuiteRunner.java:383)\r\n\tat org.testng.SuiteRunner.run(SuiteRunner.java:326)\r\n\tat org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)\r\n\tat org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95)\r\n\tat org.testng.TestNG.runSuitesSequentially(TestNG.java:1249)\r\n\tat org.testng.TestNG.runSuitesLocally(TestNG.java:1169)\r\n\tat org.testng.TestNG.runSuites(TestNG.java:1092)\r\n\tat org.testng.TestNG.run(TestNG.java:1060)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.run(TemptoRunner.java:94)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.runTempto(TemptoRunner.java:67)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.runTempto(TemptoRunner.java:55)\r\n\tat com.facebook.presto.tests.TemptoProductTestRunner.main(TemptoProductTestRunner.java:27)\r\nCaused by: java.sql.SQLException: Cannot create PoolableConnectionFactory (The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:2291)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createDataSource(BasicDataSource.java:2038)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.getConnection(BasicDataSource.java:1533)\r\n\tat io.prestodb.tempto.query.JdbcConnectionsPool.connectionFor(JdbcConnectionsPool.java:37)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.openConnection(JdbcQueryExecutor.java:59)\r\n\t... 38 more\r\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\r\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\r\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\r\n\tat org.apache.commons.dbcp2.DriverConnectionFactory.createConnection(DriverConnectionFactory.java:39)\r\n\tat org.apache.commons.dbcp2.PoolableConnectionFactory.makeObject(PoolableConnectionFactory.java:256)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.validateConnectionFactory(BasicDataSource.java:2301)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:2287)\r\n\t... 42 more\r\n\r\nError: Exception in thread \"main\" java.lang.RuntimeException: java.sql.SQLException: Cannot create PoolableConnectionFactory (The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".)\r\n\tat com.google.common.base.Throwables.propagate(Throwables.java:249)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.openConnection(JdbcQueryExecutor.java:62)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.getConnection(JdbcQueryExecutor.java:90)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.AbstractTableManager.getTableNames(AbstractTableManager.java:63)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.AbstractTableManager.dropStaleMutableTables(AbstractTableManager.java:52)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.TableRequirementFulfiller.createTable(TableRequirementFulfiller.java:75)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.DistinctOps$1$2.accept(DistinctOps.java:175)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\r\n\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n\tat java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1553)\r\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\r\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\r\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)\r\n\tat io.prestodb.tempto.internal.fulfillment.table.TableRequirementFulfiller.fulfill(TableRequirementFulfiller.java:60)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.lambda$doFulfillment$0(TestInitializationListener.java:326)\r\n\tat io.prestodb.tempto.context.TestContextDsl.runWithTestContext(TestContextDsl.java:51)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.doFulfillment(TestInitializationListener.java:324)\r\n\tat io.prestodb.tempto.internal.initialization.TestInitializationListener.onStart(TestInitializationListener.java:189)\r\n\tat org.testng.TestRunner.fireEvent(TestRunner.java:938)\r\n\tat org.testng.TestRunner.beforeRun(TestRunner.java:630)\r\n\tat org.testng.TestRunner.run(TestRunner.java:596)\r\n\tat org.testng.SuiteRunner.runTest(SuiteRunner.java:429)\r\n\tat org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423)\r\n\tat org.testng.SuiteRunner.privateRun(SuiteRunner.java:383)\r\n\tat org.testng.SuiteRunner.run(SuiteRunner.java:326)\r\n\tat org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)\r\n\tat org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95)\r\n\tat org.testng.TestNG.runSuitesSequentially(TestNG.java:1249)\r\n\tat org.testng.TestNG.runSuitesLocally(TestNG.java:1169)\r\n\tat org.testng.TestNG.runSuites(TestNG.java:1092)\r\n\tat org.testng.TestNG.run(TestNG.java:1060)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.run(TemptoRunner.java:94)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.runTempto(TemptoRunner.java:67)\r\n\tat io.prestodb.tempto.runner.TemptoRunner.runTempto(TemptoRunner.java:55)\r\n\tat com.facebook.presto.tests.TemptoProductTestRunner.main(TemptoProductTestRunner.java:27)\r\nCaused by: java.sql.SQLException: Cannot create PoolableConnectionFactory (The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:2291)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createDataSource(BasicDataSource.java:2038)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.getConnection(BasicDataSource.java:1533)\r\n\tat io.prestodb.tempto.query.JdbcConnectionsPool.connectionFor(JdbcConnectionsPool.java:37)\r\n\tat io.prestodb.tempto.query.JdbcQueryExecutor.openConnection(JdbcQueryExecutor.java:59)\r\n\t... 38 more\r\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host sqlserver, port 1433 has failed. Error: \"sqlserver. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(SQLServerException.java:226)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.ConvertConnectExceptionToSQLServerException(SQLServerException.java:277)\r\n\tat com.microsoft.sqlserver.jdbc.SocketFinder.findSocket(IOBuffer.java:2379)\r\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.open(IOBuffer.java:652)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2379)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2042)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:1889)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1120)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:700)\r\n\tat org.apache.commons.dbcp2.DriverConnectionFactory.createConnection(DriverConnectionFactory.java:39)\r\n\tat org.apache.commons.dbcp2.PoolableConnectionFactory.makeObject(PoolableConnectionFactory.java:256)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.validateConnectionFactory(BasicDataSource.java:2301)\r\n\tat org.apache.commons.dbcp2.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:[2287](https://github.com/prestodb/presto/actions/runs/10946344534/job/30392645141#step:14:2288))\r\n\t... 42 more\r\n```\r\n",
    "issue_word_count": 1628,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml"
    ],
    "pr_changed_test_files": [
      "presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml"
    ],
    "base_commit": "95d24399ac931c236986acbe24285849ca557f22",
    "head_commit": "b14f536931d76c4dbcb50b2e4235249edfc3de62",
    "repo_url": "https://github.com/prestodb/presto/pull/23701",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23701",
    "dockerfile": "",
    "pr_merged_at": "2024-09-23T13:53:15.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml b/presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml\nindex 15e220a77c2f3..aec9eaef896f4 100644\n--- a/presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml\n+++ b/presto-product-tests/conf/docker/singlenode-sqlserver/docker-compose.yml\n@@ -2,7 +2,7 @@ services:\n \n   sqlserver:\n     hostname: sqlserver\n-    image: 'mcr.microsoft.com/mssql/server:2017-CU13'\n+    image: 'mcr.microsoft.com/mssql/server:2022-latest'\n     ports:\n       - '1433:1433'\n     environment:\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23674",
    "pr_id": 23674,
    "issue_id": 23660,
    "repo": "prestodb/presto",
    "problem_statement": "Query with scalar subquery having mathematical function returns 0 rows\nIn case when a query involves scalar subquery with mathematical function and the subquery returns no rows, the entire query returns no rows. This only happens in cases with mathematical functions present in the subquery. \r\n \r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: 0.290-SNAPSHOT (latest master)\r\n* Storage (HDFS/S3/GCS..):\r\n* Data source and connector used: hive connector\r\n* Deployment (Cloud or On-prem): local machine\r\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\r\n\r\n## Expected Behavior\r\nThe scalar subquery should return null and other selected columns should return proper values.\r\n\r\n## Current Behavior\r\nWhen the scalar subquery returns no rows, the entire query returns 0 rows.\r\n\r\n## Possible Solution\r\nThe query plan in such cases is introducing an empty Values Node in case `simplify_plan_with_empty_input` is set to true (It is true by default). If it is false, PickTableLayoutForPredicate rule removes TableScan node for the columns not in the subquery. Also an additional FilterNode with false predicate gets introduced. \r\n\r\n## Steps to Reproduce\r\nRun the below simple query - \r\n```\r\nselect\r\n  comment,\r\n  (\r\n    select\r\n      name\r\n    from\r\n      tpch.tiny.nation\r\n    where\r\n      nationkey = 1\r\n      and regionkey = 1\r\n      and width_bucket(nationkey, 1, 100, 10) = 2\r\n  )\r\nfrom\r\n  tpch.tiny.nation;\r\n```\r\n\r\n## Screenshots (if appropriate)\r\nOutputs\r\n<img width=\"1598\" alt=\"Screenshot 2024-09-16 at 10 15 32 PM\" src=\"https://github.com/user-attachments/assets/0d17bdf3-9c6f-43af-92cb-f7b073fe8e9b\">\r\n\r\n\r\n\r\nWhen `simplify_plan_with_empty_input` is true\r\n<img width=\"1672\" alt=\"Screenshot 2024-09-16 at 10 10 17 PM\" src=\"https://github.com/user-attachments/assets/47db3db2-df43-44e0-8797-c27aed58068c\">\r\n\r\n\r\nWhen `simplify_plan_with_empty_input` is false\r\n<img width=\"1710\" alt=\"Screenshot 2024-09-16 at 10 10 02 PM\" src=\"https://github.com/user-attachments/assets/705b3c60-bf56-4a1e-a52a-74b4ce7d58ca\">\r\n\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n",
    "issue_word_count": 322,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/sql/planner/EffectivePredicateExtractor.java",
      "presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "base_commit": "26fd8dbd976e2b4341558c953e54bcfbad0a6198",
    "head_commit": "c88ea852efbb5cd1d24fe3b386d020bdaa0c43e8",
    "repo_url": "https://github.com/prestodb/presto/pull/23674",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23674",
    "dockerfile": "",
    "pr_merged_at": "2024-09-21T02:32:48.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/sql/planner/EffectivePredicateExtractor.java b/presto-main/src/main/java/com/facebook/presto/sql/planner/EffectivePredicateExtractor.java\nindex 9e439fb863c40..a59b64b09f856 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/planner/EffectivePredicateExtractor.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/planner/EffectivePredicateExtractor.java\n@@ -60,6 +60,7 @@\n \n import static com.facebook.presto.common.function.OperatorType.EQUAL;\n import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n+import static com.facebook.presto.expressions.LogicalRowExpressions.FALSE_CONSTANT;\n import static com.facebook.presto.expressions.LogicalRowExpressions.TRUE_CONSTANT;\n import static com.facebook.presto.expressions.LogicalRowExpressions.extractConjuncts;\n import static com.facebook.presto.spi.relation.SpecialFormExpression.Form.IS_NULL;\n@@ -418,9 +419,10 @@ private RowExpression pullExpressionThroughVariables(RowExpression expression, C\n             for (RowExpression conjunct : new EqualityInference.Builder(functionManger).nonInferableConjuncts(expression)) {\n                 if (determinismEvaluator.isDeterministic(conjunct)) {\n                     RowExpression rewritten = equalityInference.rewriteExpression(conjunct, in(variables));\n-                    if (rewritten != null) {\n+                    if (rewritten != null && (hasVariableReferences(rewritten) || rewritten.equals(FALSE_CONSTANT))) {\n                         effectiveConjuncts.add(rewritten);\n                     }\n+                    // If equality inference has reduced the predicate to an expression referring to only constants, it does not make sense to pull this predicate up\n                 }\n             }\n \n@@ -428,5 +430,10 @@ private RowExpression pullExpressionThroughVariables(RowExpression expression, C\n \n             return logicalRowExpressions.combineConjuncts(effectiveConjuncts.build());\n         }\n+\n+        private static boolean hasVariableReferences(RowExpression rowExpression)\n+        {\n+            return !VariablesExtractor.extractUnique(rowExpression).isEmpty();\n+        }\n     }\n }\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java b/presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java\nindex a6d672435af38..4e76ddd5d271c 100644\n--- a/presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java\n+++ b/presto-main/src/test/java/com/facebook/presto/sql/planner/TestEffectivePredicateExtractor.java\n@@ -238,6 +238,26 @@ public void testProject()\n                         equals(DV, EV)));\n     }\n \n+    @Test\n+    public void testProjectOverFilterWithNoReferencedAssignments()\n+    {\n+        PlanNode node = new ProjectNode(newId(),\n+                filter(baseTableScan,\n+                        and(\n+                                equals(call(\"mod\",\n+                                        metadata.getFunctionAndTypeManager().lookupFunction(\"mod\", fromTypes(BIGINT, BIGINT)),\n+                                        BIGINT,\n+                                        ImmutableList.of(CV, bigintLiteral(5L))), bigintLiteral(-1L)),\n+                                equals(CV, bigintLiteral(10L)))),\n+                assignment(DV, AV));\n+\n+        RowExpression effectivePredicate = effectivePredicateExtractor.extract(node);\n+\n+        // The filter predicate is reduced to `CV = 10 AND mod(10,5) = -1`\n+        // Since we have no references to `CV` in the assignments however, neither of these conjuncts is pulled up through the Project\n+        assertEquals(effectivePredicate, TRUE_CONSTANT);\n+    }\n+\n     @Test\n     public void testTopN()\n     {\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\nindex 8e6d34006c2c9..a2f9190a258f9 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n@@ -1668,6 +1668,17 @@ public void testInUncorrelatedSubquery()\n                 \"SELECT TRUE, 2\");\n     }\n \n+    @Test\n+    public void testUncorrelatedSubqueryWithEmptyResult()\n+    {\n+        assertQuery(\n+                \"SELECT regionkey, (select name from nation where false) from region\",\n+                \"SELECT regionkey, NULL from region\");\n+        assertQuery(\n+                \"SELECT regionkey, (select name from nation where nationkey = 5 and mod(nationkey,5) = 1) from region\",\n+                \"SELECT regionkey, NULL from region\");\n+    }\n+\n     @Test\n     public void testChecksum()\n     {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23666",
    "pr_id": 23666,
    "issue_id": 23648,
    "repo": "prestodb/presto",
    "problem_statement": "Presto 0.289 cannot be downloaded from Maven central\nWhile successfully uploaded, the Presto sever distribution is too large and cannot be downloaded.  https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.289/presto-server-0.289.tar.gz returns a 503 error.\r\n\r\nIt seems we have tripped over an imaginary limit.  According to Sonatype, [the limit is 1GB](https://central.sonatype.org/publish/publish-portal-upload/), however our packages have regularly exceeded 1GB for years.  0.289 is around 2GB, so it seems 2GB is the real hard limit.\r\n\r\nWe should aim to reduce the Presto distribution size to 1GB or less.\r\n\r\nThere are multiple potential solutions to this problem:\r\n\r\n1) The Presto distribution has a lot of redundancy between plugins.  Each plugin's dependencies are repeated, resulting in hundreds of megabytes of duplication.  If the generated TAR used hard links, then we could cut the distribution size to well under half, and give us many more years of breathing room.\r\n\r\nWe cannot do this right now because we use the Maven assembly plugin, which simply does not support the creation of hard links.  We used to use the Maven Provision plugin, which does support this, but we migrated off of it in #12698.  We could investigate moving back onto the Provisio plugin to solve this problem.\r\n\r\n2) We can start to offload dependencies from the core distribution of Presto and let individual users of dependencies package it manually.  While this might be inconvenient for users, most dependencies are updated slowly, and can be lazily upgraded.  #23647 removes some not commonly used plugins from our distribution.\r\n\r\nIn either case, after fixing this issue, we should add a Presto-side check that ensures the artifacts don't grow past the stated limit in Sonatype, 1GB, and the solution should bring us to well below 1GB.",
    "issue_word_count": 307,
    "test_files_count": 1,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "pom.xml",
      "presto-product-tests/conf/docker/common/compose-commons.sh",
      "presto-server/pom.xml",
      "presto-server/src/main/assembly/presto.xml",
      "presto-server/src/main/java/com/facebook/presto/Dummy.java",
      "presto-server/src/main/provisio/presto.xml"
    ],
    "pr_changed_test_files": [
      "presto-product-tests/conf/docker/common/compose-commons.sh"
    ],
    "base_commit": "26fd8dbd976e2b4341558c953e54bcfbad0a6198",
    "head_commit": "24052e2a087a406bbd00a87301b9c2fd1d6c7e9f",
    "repo_url": "https://github.com/prestodb/presto/pull/23666",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23666",
    "dockerfile": "",
    "pr_merged_at": "2024-09-19T17:39:05.000Z",
    "patch": "diff --git a/pom.xml b/pom.xml\nindex e53fa8717e359..3985475d962a6 100644\n--- a/pom.xml\n+++ b/pom.xml\n@@ -2368,7 +2368,8 @@\n                     <groupId>org.sonatype.plugins</groupId>\n                     <artifactId>nexus-staging-maven-plugin</artifactId>\n                     <version>${dep.nexus-staging-plugin.version}</version>\n-                    <extensions>true</extensions>\n+                    <!-- This plugin is not registered as an extension because it will fail to activate\n+                         concurrently with the Maven Provisio plugin, as that has its own deployment lifecycle -->\n                     <configuration>\n                         <serverId>ossrh</serverId>\n                         <nexusUrl>https://oss.sonatype.org/</nexusUrl>\n@@ -2397,6 +2398,13 @@\n                 <artifactId>presto-maven-plugin</artifactId>\n             </plugin>\n \n+            <plugin>\n+                <groupId>ca.vanzyl.provisio.maven.plugins</groupId>\n+                <artifactId>provisio-maven-plugin</artifactId>\n+                <version>1.0.18</version>\n+                <extensions>true</extensions>\n+            </plugin>\n+\n             <plugin>\n                 <groupId>org.apache.maven.plugins</groupId>\n                 <artifactId>maven-compiler-plugin</artifactId>\n@@ -2576,9 +2584,31 @@\n             <id>deploy-to-ossrh</id>\n             <build>\n                 <plugins>\n+                    <plugin>\n+                        <groupId>org.apache.maven.plugins</groupId>\n+                        <artifactId>maven-deploy-plugin</artifactId>\n+                        <configuration>\n+                            <skip>true</skip>\n+                        </configuration>\n+                    </plugin>\n                     <plugin>\n                         <groupId>org.sonatype.plugins</groupId>\n                         <artifactId>nexus-staging-maven-plugin</artifactId>\n+                        <!-- This plugin must be configured manually (\"Maven 2\" style in the docs) due to the usage of the\n+                             Maven Provisio plugin, which includes its own deployment lifecycle -->\n+                        <executions>\n+                            <execution>\n+                                <id>default-deploy</id>\n+                                <phase>deploy</phase>\n+                                <goals>\n+                                    <goal>deploy</goal>\n+                                </goals>\n+                            </execution>\n+                        </executions>\n+                        <configuration>\n+                            <serverId>ossrh</serverId>\n+                            <nexusUrl>https://oss.sonatype.org/</nexusUrl>\n+                        </configuration>\n                     </plugin>\n                     <plugin>\n                         <groupId>org.apache.maven.plugins</groupId>\n\ndiff --git a/presto-server/pom.xml b/presto-server/pom.xml\nindex a638792efa738..deeff2d162f34 100644\n--- a/presto-server/pom.xml\n+++ b/presto-server/pom.xml\n@@ -10,6 +10,7 @@\n \n     <artifactId>presto-server</artifactId>\n     <name>presto-server</name>\n+    <packaging>provisio</packaging>\n \n     <properties>\n         <air.main.basedir>${project.parent.basedir}</air.main.basedir>\n@@ -22,425 +23,41 @@\n         <!-- Launcher properties -->\n         <main-class>com.facebook.presto.server.PrestoServer</main-class>\n         <process-name>${project.artifactId}</process-name>\n-    </properties>\n-\n-    <dependencies>\n-        <!-- lib -->\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-main</artifactId>\n-        </dependency>\n-\n-        <!-- The scope of all of the following dependencies are provided to avoid them being added to lib -->\n-        <!-- bin -->\n-        <dependency>\n-            <groupId>com.facebook.airlift</groupId>\n-            <artifactId>launcher</artifactId>\n-            <version>${dep.packaging.version}</version>\n-            <classifier>bin</classifier>\n-            <type>tar.gz</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.airlift</groupId>\n-            <artifactId>launcher</artifactId>\n-            <version>${dep.packaging.version}</version>\n-            <classifier>properties</classifier>\n-            <type>tar.gz</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <!-- plugins -->\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-resource-group-managers</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-password-authenticators</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-session-property-managers</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-node-ttl-fetchers</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-cluster-ttl-providers</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-jmx</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-cassandra</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-pinot</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-example-http</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-hive-hadoop2</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-memory</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-blackhole</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-kafka</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-kudu</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-atop</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-ml</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-mysql</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-singlestore</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-hana</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-oracle</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-bigquery</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n \n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-prometheus</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-postgresql</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-redshift</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-sqlserver</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-redis</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-tpch</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-tpcds</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-teradata-functions</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-mongodb</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-local-file</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-accumulo</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-thrift-connector</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-elasticsearch</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-druid</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-iceberg</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-function-namespace-managers</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-hive-function-namespace</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-delta</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-hudi</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-        <dependency>\n-            <groupId>com.facebook.presto</groupId>\n-            <artifactId>presto-clickhouse</artifactId>\n-            <version>${project.version}</version>\n-            <type>zip</type>\n-            <scope>provided</scope>\n-        </dependency>\n-\n-    </dependencies>\n+        <skipTakariLifecyclePlugin>false</skipTakariLifecyclePlugin>\n+    </properties>\n \n     <build>\n         <plugins>\n             <plugin>\n-                <groupId>org.apache.maven.plugins</groupId>\n-                <artifactId>maven-dependency-plugin</artifactId>\n-                <executions>\n-                    <execution>\n-                        <id>unpack-plugins</id>\n-                        <phase>prepare-package</phase>\n-                        <goals>\n-                            <goal>unpack-dependencies</goal>\n-                        </goals>\n-                        <configuration>\n-                            <skip>false</skip>\n-                            <!-- take care of all the plugin types -->\n-                            <includeTypes>zip</includeTypes>\n-                            <includeScope>provided</includeScope>\n-                        </configuration>\n-                    </execution>\n-                    <execution>\n-                        <id>unpack-launcher</id>\n-                        <phase>prepare-package</phase>\n-                        <goals>\n-                            <goal>unpack-dependencies</goal>\n-                        </goals>\n-                        <configuration>\n-                            <skip>false</skip>\n-                            <includeArtifactIds>launcher</includeArtifactIds>\n-                            <includeScope>provided</includeScope>\n-                            <outputDirectory>${project.build.directory}/dependency/launcher</outputDirectory>\n-                        </configuration>\n-                    </execution>\n-                </executions>\n-            </plugin>\n-\n-            <plugin>\n-                <groupId>org.apache.maven.plugins</groupId>\n-                <artifactId>maven-assembly-plugin</artifactId>\n-                <executions>\n-                    <execution>\n-                        <id>bin</id>\n-                        <phase>package</phase>\n-                        <goals>\n-                            <goal>single</goal>\n-                        </goals>\n-                        <configuration>\n-                            <formats>\n-                                <format>dir</format>\n-                                <format>tar.gz</format>\n-                            </formats>\n-                            <descriptors>\n-                                <descriptor>src/main/assembly/presto.xml</descriptor>\n-                            </descriptors>\n-                            <finalName>presto-server-${project.version}</finalName>\n-                            <appendAssemblyId>false</appendAssemblyId>\n-                        </configuration>\n-                    </execution>\n-                </executions>\n+                <groupId>ca.vanzyl.provisio.maven.plugins</groupId>\n+                <artifactId>provisio-maven-plugin</artifactId>\n             </plugin>\n         </plugins>\n+        <pluginManagement>\n+            <plugins>\n+                <plugin>\n+                    <groupId>io.takari.maven.plugins</groupId>\n+                    <artifactId>takari-lifecycle-plugin</artifactId>\n+                    <version>1.10.1</version>\n+                    <extensions>false</extensions>\n+                    <configuration>\n+                        <proc>none</proc>\n+                        <skip>${skipTakariLifecyclePlugin}</skip>\n+                    </configuration>\n+                </plugin>\n+            </plugins>\n+        </pluginManagement>\n     </build>\n+\n+    <profiles>\n+        <profile>\n+            <id>deploy-to-ossrh</id>\n+            <properties>\n+                <!-- takari-lifecycle-plugin is added by the provisio plugin.  During deployment to Sonatype, it should be disabled because this project\n+                     uses the nexus-staging-maven-plugin for deployment -->\n+                <skipTakariLifecyclePlugin>true</skipTakariLifecyclePlugin>\n+            </properties>\n+        </profile>\n+    </profiles>\n </project>\n\ndiff --git a/presto-server/src/main/assembly/presto.xml b/presto-server/src/main/assembly/presto.xml\ndeleted file mode 100644\nindex c81bd9e5b10fe..0000000000000\n--- a/presto-server/src/main/assembly/presto.xml\n+++ /dev/null\n@@ -1,220 +0,0 @@\n-<assembly xmlns=\"http://maven.apache.org/ASSEMBLY/2.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-          xsi:schemaLocation=\"http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd\">\n-    <id>presto-server</id>\n-    <includeBaseDirectory>true</includeBaseDirectory>\n-\n-    <files>\n-        <file>\n-            <source>README.txt</source>\n-        </file>\n-        <file>\n-            <source>NOTICE</source>\n-        </file>\n-    </files>\n-\n-    <dependencySets>\n-        <!-- lib -->\n-        <dependencySet>\n-            <useProjectArtifact>false</useProjectArtifact>\n-            <scope>runtime</scope>\n-            <outputDirectory>lib</outputDirectory>\n-            <useTransitiveDependencies>true</useTransitiveDependencies>\n-        </dependencySet>\n-    </dependencySets>\n-\n-    <fileSets>\n-        <!-- bin -->\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/launcher/bin</directory>\n-            <excludes>\n-                <exclude>launcher</exclude>\n-                <exclude>launcher.py</exclude>\n-                <exclude>launcher.properties</exclude>\n-            </excludes>\n-            <outputDirectory>bin</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/launcher/bin</directory>\n-            <includes>\n-                <include>launcher.properties</include>\n-            </includes>\n-            <outputDirectory>bin</outputDirectory>\n-            <filtered>true</filtered>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/launcher/bin</directory>\n-            <includes>\n-                <include>launcher</include>\n-                <include>launcher.py</include>\n-            </includes>\n-            <outputDirectory>bin</outputDirectory>\n-            <fileMode>0755</fileMode>\n-        </fileSet>\n-\n-        <!-- plugins -->\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-resource-group-managers-${project.version}</directory>\n-            <outputDirectory>plugin/resource-group-managers</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-password-authenticators-${project.version}</directory>\n-            <outputDirectory>plugin/password-authenticators</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-session-property-managers-${project.version}</directory>\n-            <outputDirectory>plugin/session-property-managers</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-node-ttl-fetchers-${project.version}</directory>\n-            <outputDirectory>plugin/ttl-fetchers</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-cluster-ttl-providers-${project.version}</directory>\n-            <outputDirectory>plugin/cluster-ttl-providers</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-function-namespace-managers-${project.version}</directory>\n-            <outputDirectory>plugin/function-namespace-managers</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-jmx-${project.version}</directory>\n-            <outputDirectory>plugin/jmx</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-cassandra-${project.version}</directory>\n-            <outputDirectory>plugin/cassandra</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-pinot-${project.version}</directory>\n-            <outputDirectory>plugin/pinot</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-example-http-${project.version}</directory>\n-            <outputDirectory>plugin/example-http</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-hana-${project.version}</directory>\n-            <outputDirectory>plugin/hana</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-hive-hadoop2-${project.version}</directory>\n-            <outputDirectory>plugin/hive-hadoop2</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-memory-${project.version}</directory>\n-            <outputDirectory>plugin/memory</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-blackhole-${project.version}</directory>\n-            <outputDirectory>plugin/blackhole</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-kafka-${project.version}</directory>\n-            <outputDirectory>plugin/kafka</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-kudu-${project.version}</directory>\n-            <outputDirectory>plugin/kudu</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-atop-${project.version}</directory>\n-            <outputDirectory>plugin/atop</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-ml-${project.version}</directory>\n-            <outputDirectory>plugin/ml</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-mysql-${project.version}</directory>\n-            <outputDirectory>plugin/mysql</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-singlestore-${project.version}</directory>\n-            <outputDirectory>plugin/singlestore</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-oracle-${project.version}</directory>\n-            <outputDirectory>plugin/oracle</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-postgresql-${project.version}</directory>\n-            <outputDirectory>plugin/postgresql</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-prometheus-${project.version}</directory>\n-            <outputDirectory>plugin/prometheus</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-redshift-${project.version}</directory>\n-            <outputDirectory>plugin/redshift</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-sqlserver-${project.version}</directory>\n-            <outputDirectory>plugin/sqlserver</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-redis-${project.version}</directory>\n-            <outputDirectory>plugin/redis</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-tpch-${project.version}</directory>\n-            <outputDirectory>plugin/tpch</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-tpcds-${project.version}</directory>\n-            <outputDirectory>plugin/tpcds</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-teradata-functions-${project.version}</directory>\n-            <outputDirectory>plugin/teradata-functions</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-mongodb-${project.version}</directory>\n-            <outputDirectory>plugin/mongodb</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-local-file-${project.version}</directory>\n-            <outputDirectory>plugin/localfile</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-accumulo-${project.version}</directory>\n-            <outputDirectory>plugin/accumulo</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-thrift-connector-${project.version}</directory>\n-            <outputDirectory>plugin/presto-thrift</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-elasticsearch-${project.version}</directory>\n-            <outputDirectory>plugin/presto-elasticsearch</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-druid-${project.version}</directory>\n-            <outputDirectory>plugin/presto-druid</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-bigquery-${project.version}</directory>\n-            <outputDirectory>plugin/presto-bigquery</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-iceberg-${project.version}</directory>\n-            <outputDirectory>plugin/iceberg</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-hive-function-namespace-${project.version}</directory>\n-            <outputDirectory>plugin/hive-function-namespace</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-delta-${project.version}</directory>\n-            <outputDirectory>plugin/delta</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-hudi-${project.version}</directory>\n-            <outputDirectory>plugin/hudi</outputDirectory>\n-        </fileSet>\n-        <fileSet>\n-            <directory>${project.build.directory}/dependency/presto-clickhouse-${project.version}</directory>\n-            <outputDirectory>plugin/clickhouse</outputDirectory>\n-        </fileSet>\n-    </fileSets>\n-</assembly>\n\ndiff --git a/presto-server/src/main/java/com/facebook/presto/Dummy.java b/presto-server/src/main/java/com/facebook/presto/Dummy.java\ndeleted file mode 100644\nindex 363367c5c9f66..0000000000000\n--- a/presto-server/src/main/java/com/facebook/presto/Dummy.java\n+++ /dev/null\n@@ -1,21 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package com.facebook.presto;\n-\n-/**\n- * This class exists to force the creation of a jar for the presto-server module. This is needed to deploy the presto-server module to nexus.\n- */\n-public class Dummy\n-{\n-}\n\ndiff --git a/presto-server/src/main/provisio/presto.xml b/presto-server/src/main/provisio/presto.xml\nnew file mode 100644\nindex 0000000000000..4e82ac23b9a1f\n--- /dev/null\n+++ b/presto-server/src/main/provisio/presto.xml\n@@ -0,0 +1,274 @@\n+<runtime>\n+    <!-- Target -->\n+    <archive name=\"${project.artifactId}-${project.version}.tar.gz\" hardLinkIncludes=\"**/*.jar\" />\n+\n+    <!-- Notices -->\n+    <fileSet to=\"/\">\n+        <directory path=\"${basedir}\">\n+            <include>NOTICE</include>\n+            <include>README.txt</include>\n+        </directory>\n+    </fileSet>\n+\n+    <!-- Launcher -->\n+    <artifactSet to=\"bin\">\n+        <artifact id=\"com.facebook.airlift:launcher:tar.gz:bin:${dep.packaging.version}\">\n+            <unpack />\n+        </artifact>\n+        <artifact id=\"com.facebook.airlift:launcher:tar.gz:properties:${dep.packaging.version}\">\n+            <unpack filter=\"true\" />\n+        </artifact>\n+    </artifactSet>\n+\n+    <!-- Server -->\n+    <artifactSet to=\"lib\">\n+        <artifact id=\"${project.groupId}:presto-main:${project.version}\" />\n+    </artifactSet>\n+\n+    <!-- Plugins -->\n+    <artifactSet to=\"plugin/resource-group-managers\">\n+        <artifact id=\"${project.groupId}:presto-resource-group-managers:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/password-authenticators\">\n+        <artifact id=\"${project.groupId}:presto-password-authenticators:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/session-property-managers\">\n+        <artifact id=\"${project.groupId}:presto-session-property-managers:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/ttl-fetchers\">\n+        <artifact id=\"${project.groupId}:presto-node-ttl-fetchers:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/cluster-ttl-providers\">\n+        <artifact id=\"${project.groupId}:presto-cluster-ttl-providers:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/function-namespace-managers\">\n+        <artifact id=\"${project.groupId}:presto-function-namespace-managers:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/jmx\">\n+        <artifact id=\"${project.groupId}:presto-jmx:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/cassandra\">\n+        <artifact id=\"${project.groupId}:presto-cassandra:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/pinot\">\n+        <artifact id=\"${project.groupId}:presto-pinot:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/example-http\">\n+        <artifact id=\"${project.groupId}:presto-example-http:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/hana\">\n+        <artifact id=\"${project.groupId}:presto-hana:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/hive-hadoop2\">\n+        <artifact id=\"${project.groupId}:presto-hive-hadoop2:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/memory\">\n+        <artifact id=\"${project.groupId}:presto-memory:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/blackhole\">\n+        <artifact id=\"${project.groupId}:presto-blackhole:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/kafka\">\n+        <artifact id=\"${project.groupId}:presto-kafka:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/kudu\">\n+        <artifact id=\"${project.groupId}:presto-kudu:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/atop\">\n+        <artifact id=\"${project.groupId}:presto-atop:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/ml\">\n+        <artifact id=\"${project.groupId}:presto-ml:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/mysql\">\n+        <artifact id=\"${project.groupId}:presto-mysql:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/singlestore\">\n+        <artifact id=\"${project.groupId}:presto-singlestore:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/oracle\">\n+        <artifact id=\"${project.groupId}:presto-oracle:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/postgresql\">\n+        <artifact id=\"${project.groupId}:presto-postgresql:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/prometheus\">\n+        <artifact id=\"${project.groupId}:presto-prometheus:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/redshift\">\n+        <artifact id=\"${project.groupId}:presto-redshift:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/sqlserver\">\n+        <artifact id=\"${project.groupId}:presto-sqlserver:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/redis\">\n+        <artifact id=\"${project.groupId}:presto-redis:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/tpch\">\n+        <artifact id=\"${project.groupId}:presto-tpch:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/tpcds\">\n+        <artifact id=\"${project.groupId}:presto-tpcds:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/teradata-functions\">\n+        <artifact id=\"${project.groupId}:presto-teradata-functions:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/mongodb\">\n+        <artifact id=\"${project.groupId}:presto-mongodb:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/localfile\">\n+        <artifact id=\"${project.groupId}:presto-local-file:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/accumulo\">\n+        <artifact id=\"${project.groupId}:presto-accumulo:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/presto-thrift\">\n+        <artifact id=\"${project.groupId}:presto-thrift-connector:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/presto-elasticsearch\">\n+        <artifact id=\"${project.groupId}:presto-elasticsearch:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/presto-druid\">\n+        <artifact id=\"${project.groupId}:presto-druid:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/presto-bigquery\">\n+        <artifact id=\"${project.groupId}:presto-bigquery:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/iceberg\">\n+        <artifact id=\"${project.groupId}:presto-iceberg:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/hive-function-namespace\">\n+        <artifact id=\"${project.groupId}:presto-hive-function-namespace:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/delta\">\n+        <artifact id=\"${project.groupId}:presto-delta:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/hudi\">\n+        <artifact id=\"${project.groupId}:presto-hudi:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+\n+    <artifactSet to=\"plugin/clickhouse\">\n+        <artifact id=\"${project.groupId}:presto-clickhouse:zip:${project.version}\">\n+            <unpack />\n+        </artifact>\n+    </artifactSet>\n+</runtime>\n",
    "test_patch": "diff --git a/presto-product-tests/conf/docker/common/compose-commons.sh b/presto-product-tests/conf/docker/common/compose-commons.sh\nindex 0b1119bf2546b..5a87e5e1bc79a 100644\n--- a/presto-product-tests/conf/docker/common/compose-commons.sh\n+++ b/presto-product-tests/conf/docker/common/compose-commons.sh\n@@ -33,7 +33,7 @@ export HADOOP_BASE_IMAGE=${HADOOP_BASE_IMAGE:-\"prestodb/hdp2.6-hive\"}\n \n if [[ -z \"${PRESTO_SERVER_DIR:-}\" ]]; then\n     source \"${PRODUCT_TESTS_ROOT}/target/classes/presto.env\"\n-    PRESTO_SERVER_DIR=\"${PROJECT_ROOT}/presto-server/target/presto-server-${PRESTO_VERSION}/presto-server-${PRESTO_VERSION}/\"\n+    PRESTO_SERVER_DIR=\"${PROJECT_ROOT}/presto-server/target/presto-server-${PRESTO_VERSION}/\"\n fi\n export_canonical_path PRESTO_SERVER_DIR\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23656",
    "pr_id": 23656,
    "issue_id": 23514,
    "repo": "prestodb/presto",
    "problem_statement": "Function to split an IP prefix into subnets\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\r\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\r\n\r\nThis is a proposal for a function `ip_prefix_subnets` that, given an input `ip_prefix` and a subnet `prefix_length`, splits the IP prefix into subnets of the input prefix length. The function will return an array of new prefixes of the new input prefix length.\r\n\r\nThe legal ranges for IPv4 prefix lengths are [0, 32] and [0, 128] for IPv6. If the supplied prefix length falls outside these ranges for IP version of the input IP address. Also, the new input prefix length must be longer (more-specific) than the prefix length of the input prefix. If either of these conditions fail then then a PrestoException will be thrown with an `INVALID_FUNCTION_ARGUMENT` code. \r\n\r\nThis function is conceptually a compliment to the `ip_prefix_collapse` function which combines prefixes together into larger IP blocks. \r\n\r\n## Presto Component, Service, or Connector\r\nA new user-facing function.\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\nWhile there are a variety of use cases for this function, one of the most useful in an analytical context is helping to perform efficient joins across IP prefix datasets where the primary join key, the IP prefix, data is not all a fixed length. The canonical example of this is joining against IP metadata such as geolocation, ASN, or traffic volume where IP prefixes need to be normalized to a standard length for efficient joins.\r\n\r\n",
    "issue_word_count": 292,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/functions/ip.rst",
      "presto-main/src/main/java/com/facebook/presto/operator/scalar/IpPrefixFunctions.java",
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java"
    ],
    "base_commit": "1a4339ef9b38ab2ddd3b5fefc542c654b7e73a42",
    "head_commit": "014609ab2d21b4bc9fe6b9d76102e43c592ce587",
    "repo_url": "https://github.com/prestodb/presto/pull/23656",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23656",
    "dockerfile": "",
    "pr_merged_at": "2024-09-20T22:56:38.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/functions/ip.rst b/presto-docs/src/main/sphinx/functions/ip.rst\nindex f5e637afb50bb..cc685be916dde 100644\n--- a/presto-docs/src/main/sphinx/functions/ip.rst\n+++ b/presto-docs/src/main/sphinx/functions/ip.rst\n@@ -70,3 +70,11 @@ IP Functions\n         SELECT is_private_ip(IPADDRESS '157.240.200.99'); -- false\n         SELECT is_private_ip(IPADDRESS '2a03:2880:f031:12:face:b00c:0:2'); -- false\n \n+.. function:: ip_prefix_subnets(ip_prefix, prefix_length) -> array(ip_prefix)\n+\n+    Returns the subnets of ``ip_prefix`` of size ``prefix_length``. ``prefix_length`` must be valid ([0, 32] for IPv4\n+    and [0, 128] for IPv6) or the query will fail and raise an error. An empty array is returned if ``prefix_length``\n+    is shorter (that is, less specific) than ``ip_prefix``. ::\n+\n+        SELECT IP_PREFIX_SUBNETS(IPPREFIX '192.168.1.0/24', 25); -- [{192.168.1.0/25}, {192.168.1.128/25}]\n+        SELECT IP_PREFIX_SUBNETS(IPPREFIX '2a03:2880:c000::/34', 36); -- [{2a03:2880:c000::/36}, {2a03:2880:d000::/36}, {2a03:2880:e000::/36}, {2a03:2880:f000::/36}]\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/operator/scalar/IpPrefixFunctions.java b/presto-main/src/main/java/com/facebook/presto/operator/scalar/IpPrefixFunctions.java\nindex b47e8f48ce04e..5f7f9a27d19f7 100644\n--- a/presto-main/src/main/java/com/facebook/presto/operator/scalar/IpPrefixFunctions.java\n+++ b/presto-main/src/main/java/com/facebook/presto/operator/scalar/IpPrefixFunctions.java\n@@ -52,6 +52,8 @@ public final class IpPrefixFunctions\n {\n     private static final BigInteger TWO = BigInteger.valueOf(2);\n \n+    private static final Block EMPTY_BLOCK = IPPREFIX.createBlockBuilder(null, 0).build();\n+\n     /**\n      * Our definitions for what IANA considers not \"globally reachable\" are taken from the docs at\n      * https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml and\n@@ -290,6 +292,71 @@ public static boolean isPrivateIpAddress(@SqlType(StandardTypes.IPADDRESS) Slice\n         return false;\n     }\n \n+    @Description(\"Split the input prefix into subnets the size of the new prefix length.\")\n+    @ScalarFunction(\"ip_prefix_subnets\")\n+    @SqlType(\"array(IPPREFIX)\")\n+    public static Block ipPrefixSubnets(@SqlType(StandardTypes.IPPREFIX) Slice prefix, @SqlType(StandardTypes.BIGINT) long newPrefixLength)\n+    {\n+        boolean inputIsIpV4 = isIpv4(prefix);\n+\n+        if (newPrefixLength < 0 || (inputIsIpV4 && newPrefixLength > 32) || (!inputIsIpV4 && newPrefixLength > 128)) {\n+            throw new PrestoException(INVALID_FUNCTION_ARGUMENT, \"Invalid prefix length for IPv\" + (inputIsIpV4 ? \"4\" : \"6\") + \": \" + newPrefixLength);\n+        }\n+\n+        int inputPrefixLength = getPrefixLength(prefix);\n+        // An IP prefix is a 'network', or group of contiguous IP addresses. The common format for describing IP prefixes is\n+        // uses 2 parts separated by a '/': (1) the IP address part and the (2) prefix length part (also called subnet size or CIDR).\n+        // For example, in 9.255.255.0/24, 9.255.255.0 is the IP address part and 24 is the prefix length.\n+        // The prefix length describes how many IP addresses the prefix contains in terms of the leading number of bits required. A higher number of bits\n+        // means smaller number of IP addresses. Subnets inherently mean smaller groups of IP addresses.\n+        // We can only disaggregate a prefix if the prefix length is the same length or longer (more-specific) than the length of the input prefix.\n+        // E.g., if the input prefix is 9.255.255.0/24, the prefix length can be /24, /25, /26, etc... but not 23 or larger value than 24.\n+\n+        int newPrefixCount = 0;  // if inputPrefixLength > newPrefixLength, there are no new prefixes and we will return an empty array.\n+        if (inputPrefixLength <= newPrefixLength) {\n+            // Next, count how many new prefixes we will generate. In general, every difference in prefix length doubles the number new prefixes.\n+            // For example if we start with 9.255.255.0/24, and want to split into /25s, we would have 2 new prefixes. If we wanted to split into /26s,\n+            // we would have 4 new prefixes, and /27 would have 8 prefixes etc....\n+            newPrefixCount = 1 << (newPrefixLength - inputPrefixLength);  // 2^N\n+        }\n+\n+        if (newPrefixCount == 0) {\n+            return EMPTY_BLOCK;\n+        }\n+\n+        BlockBuilder blockBuilder = IPPREFIX.createBlockBuilder(null, newPrefixCount);\n+\n+        if (newPrefixCount == 1) {\n+            IPPREFIX.writeSlice(blockBuilder, prefix); // just return the original prefix in an array\n+            return blockBuilder.build(); // returns empty or single entry\n+        }\n+\n+        int ipVersionMaxBits = inputIsIpV4 ? 32 : 128;\n+        BigInteger newPrefixIpCount = TWO.pow(ipVersionMaxBits - (int) newPrefixLength);\n+\n+        Slice startingIpAddressAsSlice = ipSubnetMin(prefix);\n+        BigInteger currentIpAddress = toBigInteger(startingIpAddressAsSlice);\n+\n+        try {\n+            for (int i = 0; i < newPrefixCount; i++) {\n+                InetAddress asInetAddress = bigIntegerToIpAddress(currentIpAddress);\n+                Slice ipPrefixAsSlice = castFromVarcharToIpPrefix(utf8Slice(InetAddresses.toAddrString(asInetAddress) + \"/\" + newPrefixLength));\n+                IPPREFIX.writeSlice(blockBuilder, ipPrefixAsSlice);\n+                currentIpAddress = currentIpAddress.add(newPrefixIpCount);   // increment to start of next new prefix\n+            }\n+        }\n+        catch (UnknownHostException ex) {\n+            throw new PrestoException(GENERIC_INTERNAL_ERROR, \"Unable to convert \" + currentIpAddress + \" to IP prefix\", ex);\n+        }\n+\n+        return blockBuilder.build();\n+    }\n+\n+    private static int getPrefixLength(Slice ipPrefix)\n+    {\n+        return ipPrefix.getByte(IPPREFIX.getFixedSize() - 1) & 0xFF;\n+    }\n+\n     private static List<Slice> generateMinIpPrefixes(BigInteger firstIpAddress, BigInteger lastIpAddress, int ipVersionMaxBits)\n     {\n         List<Slice> ipPrefixSlices = new ArrayList<>();\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java\nindex a895fdd7e1c50..21f2b615887c9 100644\n--- a/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java\n+++ b/presto-main/src/test/java/com/facebook/presto/operator/scalar/TestIpPrefixFunctions.java\n@@ -287,4 +287,37 @@ public void testIsPrivateIpNull()\n     {\n         assertFunction(\"IS_PRIVATE_IP(NULL)\", BOOLEAN, null);\n     }\n+\n+    @Test\n+    public void testIpPrefixSubnets()\n+    {\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.1.0/24', 25)\", new ArrayType(IPPREFIX), ImmutableList.of(\"192.168.1.0/25\", \"192.168.1.128/25\"));\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.0.0/24', 26)\", new ArrayType(IPPREFIX), ImmutableList.of(\"192.168.0.0/26\", \"192.168.0.64/26\", \"192.168.0.128/26\", \"192.168.0.192/26\"));\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '2A03:2880:C000::/34', 37)\",\n+                new ArrayType(IPPREFIX),\n+                ImmutableList.of(\"2a03:2880:c000::/37\", \"2a03:2880:c800::/37\", \"2a03:2880:d000::/37\", \"2a03:2880:d800::/37\", \"2a03:2880:e000::/37\", \"2a03:2880:e800::/37\", \"2a03:2880:f000::/37\", \"2a03:2880:f800::/37\"));\n+    }\n+\n+    @Test\n+    public void testIpPrefixSubnetsReturnSelf()\n+    {\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.1.0/24', 24)\", new ArrayType(IPPREFIX), ImmutableList.of(\"192.168.1.0/24\"));\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '2804:431:b000::/38', 38)\", new ArrayType(IPPREFIX), ImmutableList.of(\"2804:431:b000::/38\"));\n+    }\n+\n+    @Test\n+    public void testIpPrefixSubnetsNewPrefixLengthLongerReturnsEmpty()\n+    {\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.0.0/24', 23)\", new ArrayType(IPPREFIX), ImmutableList.of());\n+        assertFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '64:ff9b::17/64', 48)\", new ArrayType(IPPREFIX), ImmutableList.of());\n+    }\n+\n+    @Test\n+    public void testIpPrefixSubnetsInvalidPrefixLengths()\n+    {\n+        assertInvalidFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.0.0/24', -1)\", \"Invalid prefix length for IPv4: -1\");\n+        assertInvalidFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '192.168.0.0/24', 33)\", \"Invalid prefix length for IPv4: 33\");\n+        assertInvalidFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '64:ff9b::17/64', -1)\", \"Invalid prefix length for IPv6: -1\");\n+        assertInvalidFunction(\"IP_PREFIX_SUBNETS(IPPREFIX '64:ff9b::17/64', 129)\", \"Invalid prefix length for IPv6: 129\");\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23655",
    "pr_id": 23655,
    "issue_id": 23654,
    "repo": "prestodb/presto",
    "problem_statement": "TestIcebergRegisterAndUnregisterProcedure.testUnregisterTable() is flaky\nExample failure: https://github.com/prestodb/presto/actions/runs/10862285429/job/30145155157?pr=23645\r\n\r\n```\r\n[ERROR] Tests run: 3199, Failures: 1, Errors: 0, Skipped: 34, Time elapsed: 1,874.556 s <<< FAILURE! - in TestSuite\r\n[ERROR] com.facebook.presto.iceberg.procedure.TestIcebergRegisterAndUnregisterProcedure.testUnregisterTable  Time elapsed: 0.013 s  <<< FAILURE!\r\njava.lang.AssertionError: expected [false] but found [true]\r\n\tat org.testng.Assert.fail(Assert.java:110)\r\n\tat org.testng.Assert.failNotEquals(Assert.java:1413)\r\n\tat org.testng.Assert.assertFalse(Assert.java:78)\r\n\tat org.testng.Assert.assertFalse(Assert.java:88)\r\n\tat com.facebook.presto.iceberg.procedure.TestIcebergRegisterAndUnregisterProcedure.testUnregisterTable(TestIcebergRegisterAndUnregisterProcedure.java:400)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:135)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:673)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:220)\r\n\tat org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50)\r\n\tat org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:945)\r\n\tat org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:193)\r\n\tat org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146)\r\n\tat org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```",
    "issue_word_count": 242,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java"
    ],
    "base_commit": "626bb99edeb0bae7dc6078649e36feb243f4fcbb",
    "head_commit": "8cf30558944586de3b1bf99f8d2186894861d391",
    "repo_url": "https://github.com/prestodb/presto/pull/23655",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23655",
    "dockerfile": "",
    "pr_merged_at": "2024-09-14T23:19:43.000Z",
    "patch": "",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java\nindex a3f0c66fb0e83..ae1148b6cd1a0 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestIcebergRegisterAndUnregisterProcedure.java\n@@ -392,7 +392,7 @@ public void testRegisterTableWithInvalidLocation()\n     @Test\n     public void testUnregisterTable()\n     {\n-        String tableName = \"unregister\";\n+        String tableName = \"unregister_positional_args\";\n         assertUpdate(\"CREATE TABLE \" + tableName + \" (id integer, value integer)\");\n \n         // Unregister table with procedure\n@@ -403,7 +403,7 @@ public void testUnregisterTable()\n     @Test\n     public void testUnregisterTableWithNamedArguments()\n     {\n-        String tableName = \"unregister\";\n+        String tableName = \"unregister_named_args\";\n         assertUpdate(\"CREATE TABLE \" + tableName + \" (id integer, value integer)\");\n \n         // Unregister table with procedure\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23635",
    "pr_id": 23635,
    "issue_id": 23608,
    "repo": "prestodb/presto",
    "problem_statement": "Make blockSize configurable for Symlink Tables Code Path\nMake blockSize configurable for Symlink Tables Code Path\r\n\r\n## Expected Behavior or Use Case\r\nSplit generation for the Symlink table is handled via the Hadoop library [here](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java#L189)\r\n\r\nCurrently, s3 default block size is 32MB [here](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L169) which is not configurable. \r\nAnd for the hdfs file system default block size is 128MB as mentioned [here](https://github.com/apache/hadoop/blob/release-2.7.4-RC0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java#L39)\r\n\r\nAs in [FileInputFormat](https://github.com/apache/hadoop/blob/release-2.7.4-RC0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L347) split size is calculated based on these **computeSplitSize(goalSize, minSize, blockSize)**\r\n\r\nSo it would be better to make this splitSize configurable even from Presto.\r\n\r\nSetting `org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE` to  Presto property `getMaxSplitSize(session).toBytes()` in Symlink table configuration [block](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java#L185)\r\n```\r\n            Configuration configuration = targetFilesystem.getConf();\r\n            configuration.set(SPLIT_MINSIZE, Long.toString(getMaxSplitSize(session).toBytes()));\r\n```\r\n\r\n## Presto Component, Service, or Connector\r\npresto-hive\r\n\r\n## Possible Implementation\r\nSetting `org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE` to  Presto property `getMaxSplitSize(session).toBytes()` in Symlink table configuration [block](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java#L185)\r\n```\r\n            Configuration configuration = targetFilesystem.getConf();\r\n            configuration.set(SPLIT_MINSIZE, Long.toString(getMaxSplitSize(session).toBytes()));\r\n```\r\n\r\n## Example Screenshots (if appropriate):\r\nAdding sample results with tpc-ds query with sf1k data on s3. For s3 default block size is 32MB [here](https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L169)\r\n\r\n**Here Base run** is for s3 default block size is 32MB.\r\n**Target run** is for s3 default block size is 256MB.\r\n\r\n![image](https://github.com/user-attachments/assets/8011e07b-03c8-4c97-8431-103653b9f0c1)\r\n\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 408,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java",
      "presto-hive/src/test/java/com/facebook/presto/hive/TestCustomInputSplits.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/TestCustomInputSplits.java"
    ],
    "base_commit": "70188806a9a47d45cbbf6ef727962991c5b4054a",
    "head_commit": "6eb0bf023a25840999ebdb0d422e240afccdab5f",
    "repo_url": "https://github.com/prestodb/presto/pull/23635",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23635",
    "dockerfile": "",
    "pr_merged_at": "2024-12-18T14:50:46.000Z",
    "patch": "diff --git a/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java b/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java\nindex 08ab8a7b5de72..6a4052656ff46 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/StoragePartitionLoader.java\n@@ -64,6 +64,7 @@\n import static com.facebook.presto.hive.HiveErrorCode.HIVE_UNSUPPORTED_FORMAT;\n import static com.facebook.presto.hive.HiveMetadata.shouldCreateFilesForMissingBuckets;\n import static com.facebook.presto.hive.HiveSessionProperties.getMaxInitialSplitSize;\n+import static com.facebook.presto.hive.HiveSessionProperties.getMaxSplitSize;\n import static com.facebook.presto.hive.HiveSessionProperties.isFileSplittable;\n import static com.facebook.presto.hive.HiveSessionProperties.isOrderBasedExecutionEnabled;\n import static com.facebook.presto.hive.HiveSessionProperties.isSkipEmptyFilesEnabled;\n@@ -111,6 +112,7 @@ public class StoragePartitionLoader\n     private final Deque<Iterator<InternalHiveSplit>> fileIterators;\n     private final boolean schedulerUsesHostAddresses;\n     private final boolean partialAggregationsPushedDown;\n+    private static final String SPLIT_MINSIZE = \"mapreduce.input.fileinputformat.split.minsize\";\n \n     public StoragePartitionLoader(\n             Table table,\n@@ -185,6 +187,7 @@ private ListenableFuture<?> handleSymlinkTextInputFormat(ExtendedFileSystem fs,\n             JobConf targetJob = toJobConf(targetFilesystem.getConf());\n             targetJob.setInputFormat(TextInputFormat.class);\n             targetInputFormat.configure(targetJob);\n+            targetJob.set(SPLIT_MINSIZE, Long.toString(getMaxSplitSize(session).toBytes()));\n             FileInputFormat.setInputPaths(targetJob, targetPath);\n             InputSplit[] targetSplits = targetInputFormat.getSplits(targetJob, 0);\n \n@@ -214,6 +217,7 @@ private ListenableFuture<?> handleGetSplitsFromInputFormat(Configuration configu\n         FileInputFormat.setInputPaths(jobConf, path);\n         // SerDes parameters and Table parameters passing into input format\n         fromProperties(schema).forEach(jobConf::set);\n+        jobConf.set(SPLIT_MINSIZE, Long.toString(getMaxSplitSize(session).toBytes()));\n         InputSplit[] splits = inputFormat.getSplits(jobConf, 0);\n \n         return addSplitsToSource(splits, splitFactory, hiveSplitSource, stopped);\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/TestCustomInputSplits.java b/presto-hive/src/test/java/com/facebook/presto/hive/TestCustomInputSplits.java\nnew file mode 100644\nindex 0000000000000..5f6c9061080ff\n--- /dev/null\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/TestCustomInputSplits.java\n@@ -0,0 +1,80 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.hive;\n+\n+import com.google.common.io.Resources;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.HadoopExtendedFileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertNotNull;\n+\n+@Test(singleThreaded = true)\n+public class TestCustomInputSplits\n+{\n+    private static final String SPLIT_MINSIZE = \"mapreduce.input.fileinputformat.split.minsize\";\n+    private static final String LOCAL_BLOCK_SIZE = \"fs.local.block.size\";\n+    private static final String DFS_BLOCK_SIZE = \"dfs.blocksize\";\n+\n+    @Test\n+    public void testCustomSplitSize() throws Exception\n+    {\n+        long splitSize = 1000;\n+        long blockSize = 500;\n+        String filePath = Resources.getResource(\"addressbook.parquet\").getFile();\n+        Path path = new Path(filePath);\n+\n+        TextInputFormat targetInputFormat = new DummyTextInputFormat();\n+        JobConf jobConf = new JobConf();\n+        FileInputFormat.setInputPaths(jobConf, path);\n+\n+        jobConf.set(\"fs.defaultFS\", \"file:///\");\n+        jobConf.setClass(\"fs.file.impl\", TestingFileSystem.class, FileSystem.class);\n+        jobConf.setClass(\"fs.hdfs.impl\", TestingFileSystem.class, FileSystem.class);\n+        jobConf.setBoolean(\"fs.file.impl.disable.cache\", true);\n+        jobConf.setLong(LOCAL_BLOCK_SIZE, blockSize);\n+        jobConf.setLong(DFS_BLOCK_SIZE, blockSize);\n+        jobConf.set(SPLIT_MINSIZE, Long.toString(splitSize));\n+\n+        InputSplit[] targetSplits = targetInputFormat.getSplits(jobConf, 0);\n+\n+        assertNotNull(targetSplits);\n+        assertEquals(targetSplits.length, 4);\n+    }\n+\n+    public class DummyTextInputFormat\n+            extends TextInputFormat\n+    {\n+        protected boolean isSplitable(FileSystem fs, Path file)\n+        {\n+            return true;\n+        }\n+    }\n+\n+    public static class TestingFileSystem\n+            extends HadoopExtendedFileSystem\n+    {\n+        public TestingFileSystem()\n+        {\n+            super(new LocalFileSystem());\n+        }\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23629",
    "pr_id": 23629,
    "issue_id": 23613,
    "repo": "prestodb/presto",
    "problem_statement": "Query fail with error `failed: at index 1`\n<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: 0.288 docker image\r\n* Storage (HDFS/S3/GCS..):\r\n* Data source and connector used: Memory connector\r\n* Deployment (Cloud or On-prem): On-prem\r\n* [Pastebin](https://pastebin.com/) link to the complete debug logs: https://pastebin.com/Duaea2PE\r\n\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nShould  return true or provide a more meaningful error message\r\n\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\nreturns error `Query 20240910_100645_00342_v3b4v failed: at index 1`\r\n\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix/reason for the bug or a workaround -->\r\n\r\n## Steps to Reproduce\r\nThe select query returns an error:\r\n```\r\nselect (CASE (CASE true WHEN (true) THEN (true) ELSE coalesce((false = ANY (VALUES true)), (true)) END ) WHEN false THEN true END );\r\n\r\n```\r\n\r\nThe subquery  returns true:\r\n```\r\nselect CASE true WHEN (true) THEN (true) ELSE coalesce((false = ANY (VALUES true)), (true)) END ;\r\n\r\n```\r\n\r\nChanging `false = ANY (VALUES true)), false` also results in no errors:\r\n```\r\nselect (CASE (CASE true WHEN (true) THEN (true) ELSE coalesce((false), (true)) END ) WHEN false THEN true END );\r\n\r\n```\r\n\r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nTest case generated from SQLancer\r\n\r\n",
    "issue_word_count": 227,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/RewriteCaseToMap.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "pr_changed_test_files": [
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java"
    ],
    "base_commit": "e4b8abb1eba532ee024da1b09bd50cde7a0eab3a",
    "head_commit": "2eb45b53abb473d7edcfffb505e8dfc117701ce0",
    "repo_url": "https://github.com/prestodb/presto/pull/23629",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23629",
    "dockerfile": "",
    "pr_merged_at": "2024-09-20T15:00:03.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/RewriteCaseToMap.java b/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/RewriteCaseToMap.java\nindex 23c4e149cacef..30e4cedcba064 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/RewriteCaseToMap.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/planner/iterative/rule/RewriteCaseToMap.java\n@@ -231,6 +231,10 @@ else if (!curCheck.equals(checkExpr)) {\n                 }\n             }\n \n+            if (checkExpr == null) {\n+                return node;\n+            }\n+\n             // Here we have all values!\n             RowExpression mapLookup = makeMapAndAccess(whens, thens, checkExpr);\n \n",
    "test_patch": "diff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\nindex 8e6d34006c2c9..77777da9b9d75 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueries.java\n@@ -7419,6 +7419,7 @@ public void testCaseToMapOptimization()\n         assertQuery(\"select x, case x when 1 then 1 when 2 then 2 else 3 end from (select x from (values 1, 2, 3, 4) t(x))\");\n         assertQuery(\"select x, case when x=1 then 1 when x=2 then 2 else 3 end from (select x from (values 1, 2, 3, 4) t(x))\");\n         assertQuery(\"select x, case when x=1 then 1 when x in (2, 3) then 2 else 3 end from (select x from (values 1, 2, 3, 4) t(x))\");\n+        assertQuery(\"select case (case true when true then true else coalesce(false = any (values true), true) end) when false then true end limit 1\");\n \n         // disable the feature and test to make sure it doesn't fire\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23606",
    "pr_id": 23606,
    "issue_id": 23564,
    "repo": "prestodb/presto",
    "problem_statement": "Round bytes to Gigabytes in error messages\nSaw this lately:\r\n\r\ncom.facebook.presto.spi.PrestoException: TaskUpdate size of 860492511 Bytes has exceeded the limit of 524288000 Bytes\r\n\r\nNo major problem. The error is accurate and expected given what I was doing. It's just hard to read. It would be easier to read if such large numbers were rounded to gigabytes and/or megabytes before being shown to users. E.g.\r\n\r\ncom.facebook.presto.spi.PrestoException: TaskUpdate size of 8 GB has exceeded the limit of 5 GB.\r\n\r\nstart looking in presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java around line 885 for the relevant code\r\n\r\n",
    "issue_word_count": 110,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java",
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java"
    ],
    "pr_changed_test_files": [
      "presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java"
    ],
    "base_commit": "1f74d4d96191775abde662e64a246ff1f2dd56bf",
    "head_commit": "eae6618ab7d1afc0c34a683040c91efba7d6e5bb",
    "repo_url": "https://github.com/prestodb/presto/pull/23606",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23606",
    "dockerfile": "",
    "pr_merged_at": "2024-09-10T15:54:02.000Z",
    "patch": "diff --git a/presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java b/presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java\nindex 0d113a9e1d54a..95febdbfdd8b7 100644\n--- a/presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java\n+++ b/presto-main/src/main/java/com/facebook/presto/server/remotetask/HttpRemoteTask.java\n@@ -75,6 +75,7 @@\n import com.google.common.util.concurrent.ListenableFuture;\n import com.google.common.util.concurrent.SettableFuture;\n import com.sun.management.ThreadMXBean;\n+import io.airlift.units.DataSize;\n import io.airlift.units.Duration;\n import it.unimi.dsi.fastutil.longs.LongArrayList;\n import org.joda.time.DateTime;\n@@ -226,6 +227,7 @@ public final class HttpRemoteTask\n     private final HandleResolver handleResolver;\n     private final int maxTaskUpdateSizeInBytes;\n     private final int maxUnacknowledgedSplits;\n+    private final DataSize maxTaskUpdateDataSize;\n \n     private final TableWriteInfo tableWriteInfo;\n \n@@ -325,6 +327,7 @@ public HttpRemoteTask(\n             this.handleResolver = handleResolver;\n             this.tableWriteInfo = tableWriteInfo;\n             this.maxTaskUpdateSizeInBytes = maxTaskUpdateSizeInBytes;\n+            this.maxTaskUpdateDataSize = DataSize.succinctBytes(this.maxTaskUpdateSizeInBytes);\n             this.maxUnacknowledgedSplits = getMaxUnacknowledgedSplitsPerTask(session);\n             checkArgument(maxUnacknowledgedSplits > 0, \"maxUnacknowledgedSplits must be > 0, found: %s\", maxUnacknowledgedSplits);\n \n@@ -882,7 +885,7 @@ private synchronized void sendUpdate()\n         taskUpdateRequestSize.add(taskUpdateRequestJson.length);\n \n         if (taskUpdateRequestJson.length > maxTaskUpdateSizeInBytes) {\n-            failTask(new PrestoException(EXCEEDED_TASK_UPDATE_SIZE_LIMIT, format(\"TaskUpdate size of %d Bytes has exceeded the limit of %d Bytes\", taskUpdateRequestJson.length, maxTaskUpdateSizeInBytes)));\n+            failTask(new PrestoException(EXCEEDED_TASK_UPDATE_SIZE_LIMIT, getExceededTaskUpdateSizeMessage(taskUpdateRequestJson)));\n         }\n \n         if (fragment.isPresent()) {\n@@ -928,6 +931,12 @@ private synchronized void sendUpdate()\n                 executor);\n     }\n \n+    private String getExceededTaskUpdateSizeMessage(byte[] taskUpdateRequestJson)\n+    {\n+        DataSize taskUpdateSize = DataSize.succinctBytes(taskUpdateRequestJson.length);\n+        return format(\"TaskUpdate size of %s has exceeded the limit of %s\", taskUpdateSize.toString(), this.maxTaskUpdateDataSize.toString());\n+    }\n+\n     private synchronized List<TaskSource> getSources()\n     {\n         return Stream.concat(tableScanPlanNodeIds.stream(), remoteSourcePlanNodeIds.stream())\n",
    "test_patch": "diff --git a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\nindex 5b8b3e24dcd00..c9e96617fa878 100644\n--- a/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\n+++ b/presto-main/src/test/java/com/facebook/presto/server/remotetask/TestHttpRemoteTask.java\n@@ -72,6 +72,7 @@\n import com.google.inject.Injector;\n import com.google.inject.Module;\n import com.google.inject.Provides;\n+import io.airlift.units.DataSize;\n import io.airlift.units.Duration;\n import org.testng.annotations.DataProvider;\n import org.testng.annotations.Test;\n@@ -90,6 +91,7 @@\n import javax.ws.rs.core.MediaType;\n import javax.ws.rs.core.UriInfo;\n \n+import java.lang.reflect.Method;\n import java.net.URI;\n import java.util.HashMap;\n import java.util.Map;\n@@ -222,6 +224,66 @@ public void testHTTPRemoteTaskSize()\n         assertTrue(httpRemoteTaskFactory.getTaskUpdateRequestSize() > 0);\n     }\n \n+    @Test(timeOut = 50000)\n+    public void testHTTPRemoteBadTaskSize()\n+            throws Exception\n+    {\n+        AtomicLong lastActivityNanos = new AtomicLong(System.nanoTime());\n+        TestingTaskResource testingTaskResource = new TestingTaskResource(lastActivityNanos, FailureScenario.NO_FAILURE);\n+        boolean useThriftEncoding = false;\n+        DataSize maxDataSize = DataSize.succinctBytes(1024);\n+        InternalCommunicationConfig internalCommunicationConfig = new InternalCommunicationConfig()\n+                .setThriftTransportEnabled(useThriftEncoding)\n+                .setMaxTaskUpdateSize(maxDataSize);\n+\n+        HttpRemoteTaskFactory httpRemoteTaskFactory = createHttpRemoteTaskFactory(testingTaskResource, useThriftEncoding, internalCommunicationConfig);\n+\n+        RemoteTask remoteTask = createRemoteTask(httpRemoteTaskFactory);\n+        testingTaskResource.setInitialTaskInfo(remoteTask.getTaskInfo());\n+        remoteTask.start();\n+        waitUntilIdle(lastActivityNanos);\n+        httpRemoteTaskFactory.stop();\n+\n+        assertTrue(remoteTask.getTaskStatus().getState().isDone(), format(\"TaskStatus is not in a done state: %s\", remoteTask.getTaskStatus()));\n+        assertEquals(getOnlyElement(remoteTask.getTaskStatus().getFailures()).getMessage(), \"TaskUpdate size of 1.97kB has exceeded the limit of 1kB\");\n+    }\n+\n+    @Test(dataProvider = \"getUpdateSize\")\n+    public void testGetExceededTaskUpdateSizeListMessage(int updateSizeInBytes, int maxDataSizeInBytes,\n+                                                         String expectedMessage) throws Exception\n+    {\n+        AtomicLong lastActivityNanos = new AtomicLong(System.nanoTime());\n+        TestingTaskResource testingTaskResource = new TestingTaskResource(lastActivityNanos, FailureScenario.NO_FAILURE);\n+        boolean useThriftEncoding = false;\n+        DataSize maxDataSize = DataSize.succinctBytes(maxDataSizeInBytes);\n+        InternalCommunicationConfig internalCommunicationConfig = new InternalCommunicationConfig()\n+                .setThriftTransportEnabled(useThriftEncoding)\n+                .setMaxTaskUpdateSize(maxDataSize);\n+        HttpRemoteTaskFactory httpRemoteTaskFactory = createHttpRemoteTaskFactory(testingTaskResource, useThriftEncoding, internalCommunicationConfig);\n+        RemoteTask remoteTask = createRemoteTask(httpRemoteTaskFactory);\n+\n+        Method targetMethod = HttpRemoteTask.class.getDeclaredMethod(\"getExceededTaskUpdateSizeMessage\", new Class[]{byte[].class});\n+        targetMethod.setAccessible(true);\n+        byte[] taskUpdateRequestJson = new byte[updateSizeInBytes];\n+        String message = (String) targetMethod.invoke(remoteTask, new Object[]{taskUpdateRequestJson});\n+        assertEquals(message, expectedMessage);\n+    }\n+\n+    @DataProvider(name = \"getUpdateSize\")\n+    protected Object[][] getUpdateSize()\n+    {\n+        return new Object[][] {\n+                {2000, 1000, \"TaskUpdate size of 1.95kB has exceeded the limit of 1000B\"},\n+                {2000, 1024, \"TaskUpdate size of 1.95kB has exceeded the limit of 1kB\"},\n+                {5000, 4 * 1024, \"TaskUpdate size of 4.88kB has exceeded the limit of 4kB\"},\n+                {2 * 1024, 1024, \"TaskUpdate size of 2kB has exceeded the limit of 1kB\"},\n+                {1024 * 1024, 512 * 1024, \"TaskUpdate size of 1MB has exceeded the limit of 512kB\"},\n+                {16 * 1024 * 1024, 8 * 1024 * 1024, \"TaskUpdate size of 16MB has exceeded the limit of 8MB\"},\n+                {485 * 1000 * 1000, 1024 * 1024 * 512, \"TaskUpdate size of 462.53MB has exceeded the limit of 512MB\"},\n+                {1024 * 1024 * 1024, 1024 * 1024 * 512, \"TaskUpdate size of 1GB has exceeded the limit of 512MB\"},\n+                {860492511, 524288000, \"TaskUpdate size of 820.63MB has exceeded the limit of 500MB\"}};\n+    }\n+\n     private void runTest(FailureScenario failureScenario, boolean useThriftEncoding)\n             throws Exception\n     {\n@@ -272,6 +334,13 @@ private RemoteTask createRemoteTask(HttpRemoteTaskFactory httpRemoteTaskFactory)\n \n     private static HttpRemoteTaskFactory createHttpRemoteTaskFactory(TestingTaskResource testingTaskResource, boolean useThriftEncoding)\n             throws Exception\n+    {\n+        InternalCommunicationConfig internalCommunicationConfig = new InternalCommunicationConfig().setThriftTransportEnabled(useThriftEncoding);\n+        return createHttpRemoteTaskFactory(testingTaskResource, useThriftEncoding, internalCommunicationConfig);\n+    }\n+\n+    private static HttpRemoteTaskFactory createHttpRemoteTaskFactory(TestingTaskResource testingTaskResource, boolean useThriftEncoding, InternalCommunicationConfig internalCommunicationConfig)\n+            throws Exception\n     {\n         Bootstrap app = new Bootstrap(\n                 new JsonModule(),\n@@ -347,7 +416,7 @@ private HttpRemoteTaskFactory createHttpRemoteTaskFactory(\n                                 metadataUpdatesJsonCodec,\n                                 metadataUpdatesSmileCodec,\n                                 new RemoteTaskStats(),\n-                                new InternalCommunicationConfig().setThriftTransportEnabled(useThriftEncoding),\n+                                internalCommunicationConfig,\n                                 createTestMetadataManager(),\n                                 new TestQueryManager(),\n                                 new HandleResolver(),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23589",
    "pr_id": 23589,
    "issue_id": 22032,
    "repo": "prestodb/presto",
    "problem_statement": "Add support for fast forwarding branches\nFast forwarding allows updates that are made to a branch to be integrated back into the main table.  This just updates the data files which have been subsequently added to be integrated back into the main table.\r\n\r\nWe should have a procedure in the Iceberg connector which fast forwards one branch to another. \r\n\r\nSee Spark implementation: https://github.com/apache/iceberg/pull/8081\r\n\r\nPart of #22025\r\n\r\n## Expected Behavior or Use Case\r\n\r\n`CALL iceberg.system.fast_forward('schema', 'table', 'main', 'audit-branch');`\r\n\r\n## Presto Component, Service, or Connector\r\nIceberg connector\r\n\r\n## Possible Implementation\r\n\r\nNew system procedure in the Iceberg connector\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 143,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/procedure/FastForwardBranchProcedure.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestFastForwardBranchProcedure.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestFastForwardBranchProcedure.java"
    ],
    "base_commit": "f393c602230bd30f0934557ec908e3c29aee72fb",
    "head_commit": "a87dd74d06967cc4d6a3975565a7ff8caaf9f757",
    "repo_url": "https://github.com/prestodb/presto/pull/23589",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23589",
    "dockerfile": "",
    "pr_merged_at": "2024-09-06T20:42:58.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 3f8a215bf7110..3a8e26d1046cd 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -917,6 +917,36 @@ Examples:\n \n     CALL iceberg.system.remove_orphan_files(schema => 'db', table_name => 'sample');\n \n+Fast Forward Branch\n+^^^^^^^^^^^^^^^^^^^\n+\n+This procedure advances the current snapshot of the specified branch to a more recent snapshot from another branch without replaying any intermediate snapshots.\n+``branch`` can be fast-forwarded up to the ``to`` snapshot if ``branch`` is an ancestor of ``to``.\n+\n+The following arguments are available:\n+\n+===================== ========== =============== =======================================================================\n+Argument Name         required   type            Description\n+===================== ========== =============== =======================================================================\n+``schema``            ✔️         string          Schema of the table to update\n+\n+``table_name``        ✔️         string          Name of the table to update\n+\n+``branch``            ✔️         string          The branch you want to fast-forward\n+\n+``to``                ✔️         string          The branch you want to fast-forward to\n+===================== ========== =============== =======================================================================\n+\n+Examples:\n+\n+* Fast-forward the ``dev`` branch to the latest snapshot of the ``main`` branch ::\n+\n+    CALL iceberg.system.fast_forward('schema_name', 'table_name', 'dev', 'main');\n+\n+* Given the branch named ``branch1`` does not exist yet, create a new branch named ``branch1``  and set it's current snapshot equal to the latest snapshot of the ``main`` branch ::\n+\n+    CALL iceberg.system.fast_forward('schema_name', 'table_name', 'branch1', 'main');\n+\n SQL Support\n -----------\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\nindex 37e89912eb164..dbcf17febc4f9 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergCommonModule.java\n@@ -40,6 +40,7 @@\n import com.facebook.presto.iceberg.nessie.IcebergNessieConfig;\n import com.facebook.presto.iceberg.optimizer.IcebergPlanOptimizerProvider;\n import com.facebook.presto.iceberg.procedure.ExpireSnapshotsProcedure;\n+import com.facebook.presto.iceberg.procedure.FastForwardBranchProcedure;\n import com.facebook.presto.iceberg.procedure.RegisterTableProcedure;\n import com.facebook.presto.iceberg.procedure.RemoveOrphanFiles;\n import com.facebook.presto.iceberg.procedure.RollbackToSnapshotProcedure;\n@@ -159,6 +160,7 @@ public void setup(Binder binder)\n         procedures.addBinding().toProvider(UnregisterTableProcedure.class).in(Scopes.SINGLETON);\n         procedures.addBinding().toProvider(ExpireSnapshotsProcedure.class).in(Scopes.SINGLETON);\n         procedures.addBinding().toProvider(RemoveOrphanFiles.class).in(Scopes.SINGLETON);\n+        procedures.addBinding().toProvider(FastForwardBranchProcedure.class).in(Scopes.SINGLETON);\n         procedures.addBinding().toProvider(SetCurrentSnapshotProcedure.class).in(Scopes.SINGLETON);\n \n         // for orc\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/procedure/FastForwardBranchProcedure.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/procedure/FastForwardBranchProcedure.java\nnew file mode 100644\nindex 0000000000000..96898ad485397\n--- /dev/null\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/procedure/FastForwardBranchProcedure.java\n@@ -0,0 +1,75 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.procedure;\n+\n+import com.facebook.presto.iceberg.IcebergMetadataFactory;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.spi.connector.ConnectorMetadata;\n+import com.facebook.presto.spi.procedure.Procedure;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.Table;\n+\n+import javax.inject.Inject;\n+import javax.inject.Provider;\n+\n+import java.lang.invoke.MethodHandle;\n+\n+import static com.facebook.presto.common.block.MethodHandleUtil.methodHandle;\n+import static com.facebook.presto.common.type.StandardTypes.VARCHAR;\n+import static com.facebook.presto.iceberg.IcebergUtil.getIcebergTable;\n+import static java.util.Objects.requireNonNull;\n+\n+public class FastForwardBranchProcedure\n+        implements Provider<Procedure>\n+{\n+    private static final MethodHandle FAST_FORWARD = methodHandle(\n+            FastForwardBranchProcedure.class,\n+            \"fastForwardToBranch\",\n+            ConnectorSession.class,\n+            String.class,\n+            String.class,\n+            String.class,\n+            String.class);\n+\n+    private final IcebergMetadataFactory metadataFactory;\n+\n+    @Inject\n+    public FastForwardBranchProcedure(IcebergMetadataFactory metadataFactory)\n+    {\n+        this.metadataFactory = requireNonNull(metadataFactory, \"metadataFactory is null\");\n+    }\n+\n+    @Override\n+    public Procedure get()\n+    {\n+        return new Procedure(\n+                \"system\",\n+                \"fast_forward\",\n+                ImmutableList.of(\n+                        new Procedure.Argument(\"schema\", VARCHAR),\n+                        new Procedure.Argument(\"table_name\", VARCHAR),\n+                        new Procedure.Argument(\"branch\", VARCHAR),\n+                        new Procedure.Argument(\"to\", VARCHAR)),\n+                FAST_FORWARD.bindTo(this));\n+    }\n+\n+    public void fastForwardToBranch(ConnectorSession clientSession, String schemaName, String tableName, String fromBranch, String targetBranch)\n+    {\n+        SchemaTableName schemaTableName = new SchemaTableName(schemaName, tableName);\n+        ConnectorMetadata metadata = metadataFactory.create();\n+        Table icebergTable = getIcebergTable(metadata, clientSession, schemaTableName);\n+        icebergTable.manageSnapshots().fastForwardBranch(fromBranch, targetBranch).commit();\n+    }\n+}\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestFastForwardBranchProcedure.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestFastForwardBranchProcedure.java\nnew file mode 100644\nindex 0000000000000..deaf56d8066cd\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/procedure/TestFastForwardBranchProcedure.java\n@@ -0,0 +1,278 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg.procedure;\n+\n+import com.facebook.presto.iceberg.IcebergConfig;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.AbstractTestQueryFramework;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.nio.file.Path;\n+import java.util.Map;\n+\n+import static com.facebook.presto.iceberg.CatalogType.HADOOP;\n+import static com.facebook.presto.iceberg.IcebergQueryRunner.createIcebergQueryRunner;\n+import static com.facebook.presto.iceberg.IcebergQueryRunner.getIcebergDataDirectoryPath;\n+import static java.lang.String.format;\n+\n+public class TestFastForwardBranchProcedure\n+        extends AbstractTestQueryFramework\n+{\n+    public static final String ICEBERG_CATALOG = \"test_hadoop\";\n+    public static final String TEST_SCHEMA = \"tpch\";\n+\n+    @Override\n+    protected QueryRunner createQueryRunner()\n+            throws Exception\n+    {\n+        return createIcebergQueryRunner(ImmutableMap.of(), HADOOP, ImmutableMap.of());\n+    }\n+\n+    public void createTable(String tableName)\n+    {\n+        assertUpdate(\"CREATE TABLE IF NOT EXISTS \" + tableName + \" (id integer, value VARCHAR)\");\n+    }\n+\n+    public void dropTable(String tableName)\n+    {\n+        assertQuerySucceeds(\"DROP TABLE IF EXISTS \" + TEST_SCHEMA + \".\" + tableName);\n+    }\n+\n+    @Test\n+    public void testFastForwardBranchUsingPositionalArgs()\n+    {\n+        String tableName = \"fast_forward_table_test\";\n+        createTable(tableName);\n+        try {\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (2, 'b')\", 1);\n+\n+            Table table = loadTable(tableName);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch\").commit();\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (3, 'c')\", 1);\n+            table.refresh();\n+\n+            assertQuery(\"SELECT * FROM \" + tableName + \" ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+\n+            String fromBranch = \"testBranch\";\n+            String toBranch = \"main\";\n+            assertUpdate(format(\"CALL system.fast_forward('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, fromBranch, toBranch));\n+\n+            // now testBranch branch should have 3 entries same as main\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testFastForwardBranchUsingNamedArgs()\n+    {\n+        String tableName = \"fast_forward_table_arg_test\";\n+        createTable(tableName);\n+        try {\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (2, 'b')\", 1);\n+\n+            Table table = loadTable(tableName);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch\").commit();\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (3, 'c')\", 1);\n+            table.refresh();\n+\n+            assertQuery(\"SELECT * FROM \" + tableName + \" ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+\n+            String fromBranch = \"testBranch\";\n+            String toBranch = \"main\";\n+            assertUpdate(format(\"CALL system.fast_forward(schema => '%s', branch => '%s', to => '%s', table_name => '%s')\",\n+                    TEST_SCHEMA, fromBranch, toBranch, tableName));\n+\n+            // now testBranch branch should have 3 entries same as main\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testFastForwardWhenTargetIsNotAncestorFails()\n+    {\n+        String tableName = \"fast_forward_table_fail_test\";\n+        createTable(tableName);\n+        try {\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (2, 'b')\", 1);\n+\n+            Table table = loadTable(tableName);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch1\").commit();\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (3, 'c')\", 1);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch2\").commit();\n+            table.refresh();\n+\n+            assertQuery(\"SELECT * FROM \" + tableName + \" ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch1' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch2' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+\n+            String fromBranch = \"testBranch2\";\n+            String toBranch = \"testBranch1\";\n+            // this should fail since fromBranch is not ancestor of toBranch\n+            assertQueryFails(format(\"CALL system.fast_forward('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, fromBranch, toBranch),\n+                    \"Cannot fast-forward: testBranch2 is not an ancestor of testBranch1\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testInvalidFastForwardBranchCases()\n+    {\n+        assertQueryFails(\"CALL system.fast_forward('test_table', branch => 'main', to => 'newBranch')\",\n+                \"line 1:1: Named and positional arguments cannot be mixed\");\n+        assertQueryFails(\"CALL custom.fast_forward('test_table', 'main', 'newBranch')\",\n+                \"Procedure not registered: custom.fast_forward\");\n+        assertQueryFails(\"CALL system.fast_forward('test_table', 'main')\",\n+                \"line 1:1: Required procedure argument 'branch' is missing\");\n+        assertQueryFails(\"CALL system.fast_forward('', 'main', 'newBranch')\",\n+                \"line 1:1: Required procedure argument 'to' is missing\");\n+    }\n+\n+    @Test\n+    public void testFastForwardNonExistingToRefFails()\n+    {\n+        String tableName = \"sample_table\";\n+        createTable(tableName);\n+        try {\n+            String fromBranch = \"main\";\n+            String toBranch = \"non_existing_branch\";\n+            assertQueryFails(format(\"CALL system.fast_forward(branch => '%s', to => '%s', table_name => '%s', schema => '%s')\",\n+                            fromBranch, toBranch, tableName, TEST_SCHEMA),\n+                    \"Ref does not exist: non_existing_branch\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testFastForwardNonMain()\n+    {\n+        String tableName = \"fast_forward_table_nonmain_test\";\n+        createTable(tableName);\n+        try {\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (2, 'b')\", 1);\n+\n+            Table table = loadTable(tableName);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch1\").commit();\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (3, 'c')\", 1);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch2\").commit();\n+            table.refresh();\n+\n+            assertQuery(\"SELECT * FROM \" + tableName + \" ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch1' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch2' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+\n+            String fromBranch = \"testBranch1\";\n+            String toBranch = \"testBranch2\";\n+            assertUpdate(format(\"CALL system.fast_forward('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, fromBranch, toBranch));\n+\n+            // now testBranch1 branch should have 3 entries same as testBranch2\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch1' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    @Test\n+    public void testFastForwardNonExistingBranch()\n+    {\n+        String tableName = \"fast_forward_table_non_existing_test\";\n+        createTable(tableName);\n+        try {\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (1, 'a')\", 1);\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (2, 'b')\", 1);\n+\n+            Table table = loadTable(tableName);\n+            table.refresh();\n+\n+            table.manageSnapshots().createBranch(\"testBranch1\").commit();\n+            assertUpdate(\"INSERT INTO \" + tableName + \" VALUES (3, 'c')\", 1); // main branch here\n+            table.refresh();\n+\n+            assertQuery(\"SELECT * FROM \" + tableName + \" ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'testBranch1' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+\n+            String fromBranch = \"non_existing_branch\"; // non existing branch\n+            String toBranch = \"main\";\n+            assertUpdate(format(\"CALL system.fast_forward('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, fromBranch, toBranch));\n+\n+            // New branch non_existing_branch would be created and it should have 3 entries same as main branch\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'non_existing_branch' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b'), (3, 'c')\");\n+\n+            String fromBranch1 = \"non_existing_branch1\"; // non existing branch\n+            String toBranch1 = \"testBranch1\";\n+            assertUpdate(format(\"CALL system.fast_forward('%s', '%s', '%s', '%s')\", TEST_SCHEMA, tableName, fromBranch1, toBranch1));\n+\n+            // New branch non_existing_branch1 would be created and it should have 2 entries same as testBranch1 branch\n+            assertQuery(\"SELECT * FROM \" + tableName + \" FOR SYSTEM_VERSION AS OF 'non_existing_branch1' ORDER BY id\", \" VALUES (1, 'a'), (2, 'b')\");\n+        }\n+        finally {\n+            dropTable(tableName);\n+        }\n+    }\n+\n+    private Table loadTable(String tableName)\n+    {\n+        Catalog catalog = CatalogUtil.loadCatalog(HadoopCatalog.class.getName(), ICEBERG_CATALOG, getProperties(), new Configuration());\n+        return catalog.loadTable(TableIdentifier.of(TEST_SCHEMA, tableName));\n+    }\n+\n+    private Map<String, String> getProperties()\n+    {\n+        File metastoreDir = getCatalogDirectory();\n+        return ImmutableMap.of(\"warehouse\", metastoreDir.toString());\n+    }\n+\n+    private File getCatalogDirectory()\n+    {\n+        Path dataDirectory = getDistributedQueryRunner().getCoordinator().getDataDirectory();\n+        Path catalogDirectory = getIcebergDataDirectoryPath(dataDirectory, HADOOP.name(), new IcebergConfig().getFileFormat(), false);\n+        return catalogDirectory.toFile();\n+    }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23558",
    "pr_id": 23558,
    "issue_id": 23555,
    "repo": "prestodb/presto",
    "problem_statement": "Support enabling SSL for Prometheus connector\n<!--- Provide a general summary of the feature request or improvement in the Title above -->\r\n<!--- Look through existing open and closed feature proposals to see if someone has asked for the feature before -->\r\n\r\n## Expected Behavior or Use Case\r\n<!--- Tell us how it should work -->\r\nPrometheus should support ssl connections.\r\n## Presto Component, Service, or Connector\r\n<!--- Tell us to which service or component this request is related to -->\r\nPresto-prometheus\r\n## Possible Implementation\r\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\nEnabling SSL support for the Prometheus connector will allow us to align with security best practices, ensuring that our monitoring and metrics infrastructure is both robust and secure.",
    "issue_word_count": 156,
    "test_files_count": 1,
    "non_test_files_count": 4,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/prometheus.rst",
      "presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusClient.java",
      "presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusConnectorConfig.java",
      "presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusErrorCode.java",
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java"
    ],
    "pr_changed_test_files": [
      "presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java"
    ],
    "base_commit": "a8c1ee397a2ab74e5e673edaef156baf2c900114",
    "head_commit": "a5d5405a8ef3a60ec6cf7608a6b5120b5ca6a77c",
    "repo_url": "https://github.com/prestodb/presto/pull/23558",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23558",
    "dockerfile": "",
    "pr_merged_at": "2024-09-26T20:05:45.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/prometheus.rst b/presto-docs/src/main/sphinx/connector/prometheus.rst\nindex 317076e13fc0d..035be908cb7af 100644\n--- a/presto-docs/src/main/sphinx/connector/prometheus.rst\n+++ b/presto-docs/src/main/sphinx/connector/prometheus.rst\n@@ -25,6 +25,10 @@ replacing the properties as appropriate:\n     prometheus.max-query-duration=1h\n     prometheus.cache-ttl=30s\n     prometheus.bearer-token-file=/path/to/bearer/token/file\n+    prometheus.tls.enabled=true\n+    prometheus.tls.truststore-path=/path/to/truststore\n+    prometheus.tls.truststore-password=truststorePassword\n+    verify-host-name=true\n \n Configuration Properties\n ------------------------\n@@ -39,6 +43,10 @@ Property Name                                   Description\n ``prometheus.max-query-duration``        Width of overall query to Prometheus, will be divided into query-chunk-duration queries\n ``prometheus.cache-ttl``                 How long the config values are cached\n ``prometheus.bearer-token-file``         File holding bearer token for access to Prometheus\n+``prometheus.tls.enabled``               Enable or disable TLS for securing communication with Prometheus\n+``prometheus.tls.truststore-path``       Path to the trust store containing the SSL certificates\n+``prometheus.tls.truststore-password``   Password to access the trust store for TLS verification\n+``verify-host-name``                     Enable or disable hostname verification in the SSL certificate\n ======================================== ============================================================================================\n \n Not Exhausting Your Presto Available Heap\n\ndiff --git a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusClient.java b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusClient.java\nindex 179cf1c7ac605..555e5aba9e789 100644\n--- a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusClient.java\n+++ b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusClient.java\n@@ -28,12 +28,24 @@\n import okhttp3.Response;\n \n import javax.inject.Inject;\n+import javax.net.ssl.HostnameVerifier;\n+import javax.net.ssl.SSLContext;\n+import javax.net.ssl.SSLHandshakeException;\n+import javax.net.ssl.SSLPeerUnverifiedException;\n+import javax.net.ssl.TrustManagerFactory;\n+import javax.net.ssl.X509TrustManager;\n \n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n import java.net.URISyntaxException;\n+import java.net.URL;\n import java.nio.file.Files;\n+import java.security.KeyManagementException;\n+import java.security.KeyStore;\n+import java.security.KeyStoreException;\n+import java.security.NoSuchAlgorithmException;\n+import java.security.cert.CertificateException;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n@@ -42,6 +54,7 @@\n import static com.facebook.presto.common.type.TimestampWithTimeZoneType.TIMESTAMP_WITH_TIME_ZONE;\n import static com.facebook.presto.common.type.VarcharType.VARCHAR;\n import static com.facebook.presto.plugin.prometheus.PrometheusColumn.mapType;\n+import static com.facebook.presto.plugin.prometheus.PrometheusErrorCode.PROMETHEUS_SECURE_COMMUNICATION_ERROR;\n import static com.facebook.presto.plugin.prometheus.PrometheusErrorCode.PROMETHEUS_TABLES_METRICS_RETRIEVE_ERROR;\n import static com.facebook.presto.plugin.prometheus.PrometheusErrorCode.PROMETHEUS_UNKNOWN_ERROR;\n import static java.nio.charset.StandardCharsets.UTF_8;\n@@ -57,13 +70,14 @@ public class PrometheusClient\n     private final Optional<File> bearerTokenFile;\n     private final Supplier<Map<String, Object>> tableSupplier;\n     private final Type varcharMapType;\n+    private PrometheusConnectorConfig config;\n \n     private static final Logger log = Logger.get(PrometheusClient.class);\n \n     @Inject\n     public PrometheusClient(PrometheusConnectorConfig config, JsonCodec<Map<String, Object>> metricCodec, TypeManager typeManager)\n     {\n-        requireNonNull(config, \"config is null\");\n+        this.config = requireNonNull(config, \"config is null\");\n         requireNonNull(metricCodec, \"metricCodec is null\");\n         requireNonNull(typeManager, \"typeManager is null\");\n \n@@ -148,17 +162,54 @@ public byte[] fetchUri(URI uri)\n     {\n         Request.Builder requestBuilder = new Request.Builder().url(uri.toString());\n         getBearerAuthInfoFromFile().map(bearerToken -> requestBuilder.header(\"Authorization\", \"Bearer \" + bearerToken));\n-\n         Response response;\n         try {\n-            response = httpClient.newCall(requestBuilder.build()).execute();\n-            if (response.isSuccessful() && response.body() != null) {\n-                return response.body().bytes();\n+            if (config.isTlsEnabled()) {\n+                OkHttpClient httpClient;\n+                HostnameVerifier hostnameVerifier = (hostname, session) -> true;\n+                if (!config.getVerifyHostName()) {\n+                    httpClient = new OkHttpClient.Builder()\n+                            .hostnameVerifier(hostnameVerifier)\n+                            .sslSocketFactory(getSSLContext().getSocketFactory(), (X509TrustManager) getTrustManagerFactory().getTrustManagers()[0])\n+                            .build();\n+                }\n+                else {\n+                    httpClient = new OkHttpClient.Builder()\n+                            .sslSocketFactory(getSSLContext().getSocketFactory(), (X509TrustManager) getTrustManagerFactory().getTrustManagers()[0])\n+                            .build();\n+                }\n+                response = httpClient.newCall(requestBuilder.build()).execute();\n+                if (response.isSuccessful() && response.body() != null) {\n+                    return response.body().bytes();\n+                }\n+            }\n+            else {\n+                response = httpClient.newCall(requestBuilder.build()).execute();\n+                if (response.isSuccessful() && response.body() != null) {\n+                    return response.body().bytes();\n+                }\n             }\n         }\n+        catch (SSLHandshakeException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"An SSL handshake error occurred while establishing a secure connection. Try the following measures to resolve the error:\\n\\n\" + \"- Upload a valid SSL certificate for authentication\\n- Verify the expiration status of the uploaded certificate.\\n- If you are connecting with SSL, enable SSL on both ends of the connection.\\n\", e);\n+        }\n+        catch (SSLPeerUnverifiedException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Peer verification failed. These measures might resolve the issue \\n\" +\n+                    \"- Add correct Hostname in the SSL certificate's SAN list \\n\" +\n+                    \"- The certificate chain might be incomplete. Check your SSL certificate\\n\", e);\n+        }\n         catch (IOException e) {\n             throw new PrestoException(PROMETHEUS_UNKNOWN_ERROR, \"Error reading metrics\", e);\n         }\n+        catch (NoSuchAlgorithmException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Requested cryptographic algorithm is not available\", e);\n+        }\n+        catch (KeyStoreException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Keystore operation error\", e);\n+        }\n+        catch (KeyManagementException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Key management operation error\", e);\n+        }\n \n         throw new PrestoException(PROMETHEUS_UNKNOWN_ERROR, \"Bad response \" + response.code() + response.message());\n     }\n@@ -174,4 +225,33 @@ private Optional<String> getBearerAuthInfoFromFile()\n             }\n         });\n     }\n+\n+    private SSLContext getSSLContext()\n+            throws NoSuchAlgorithmException, KeyStoreException, KeyManagementException\n+    {\n+        SSLContext sslContext = SSLContext.getInstance(\"TLS\");\n+        sslContext.init(null, getTrustManagerFactory().getTrustManagers(), new java.security.SecureRandom());\n+        return sslContext;\n+    }\n+\n+    public TrustManagerFactory getTrustManagerFactory()\n+            throws KeyStoreException\n+    {\n+        KeyStore truststore = KeyStore.getInstance(KeyStore.getDefaultType());\n+        try {\n+            truststore.load(new URL(\"file://\" + config.getTrustStorePath()).openStream(), config.getTruststorePassword().toCharArray());\n+            TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm());\n+            trustManagerFactory.init(truststore);\n+            return trustManagerFactory;\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(PROMETHEUS_UNKNOWN_ERROR, \"I/O Error\", e);\n+        }\n+        catch (NoSuchAlgorithmException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Requested cryptographic algorithm is not available\", e);\n+        }\n+        catch (CertificateException e) {\n+            throw new PrestoException(PROMETHEUS_SECURE_COMMUNICATION_ERROR, \"Error while parsing or validating the certificate\", e);\n+        }\n+    }\n }\n\ndiff --git a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusConnectorConfig.java b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusConnectorConfig.java\nindex db8d1d3912059..dda73ed5fe81c 100644\n--- a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusConnectorConfig.java\n+++ b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusConnectorConfig.java\n@@ -36,6 +36,10 @@ public class PrometheusConnectorConfig\n     private Duration maxQueryRangeDuration = new Duration(1, TimeUnit.HOURS);\n     private Duration cacheDuration = new Duration(30, TimeUnit.SECONDS);\n     private File bearerTokenFile;\n+    private boolean tlsEnabled;\n+    private String trustStorePath;\n+    private String truststorePassword;\n+    private boolean verifyHostName;\n \n     @NotNull\n     public URI getPrometheusURI()\n@@ -115,4 +119,51 @@ public void checkConfig()\n             throw new ConfigurationException(ImmutableList.of(new Message(\"prometheus.max-query-duration must be greater than prometheus.query-chunk-duration\")));\n         }\n     }\n+    public boolean isTlsEnabled()\n+    {\n+        return tlsEnabled;\n+    }\n+\n+    @Config(\"prometheus.tls.enabled\")\n+    public PrometheusConnectorConfig setTlsEnabled(boolean tlsEnabled)\n+    {\n+        this.tlsEnabled = tlsEnabled;\n+        return this;\n+    }\n+\n+    public String getTrustStorePath()\n+    {\n+        return trustStorePath;\n+    }\n+\n+    @Config(\"prometheus.tls.truststore-path\")\n+    public PrometheusConnectorConfig setTrustStorePath(String path)\n+    {\n+        this.trustStorePath = path;\n+        return this;\n+    }\n+\n+    public String getTruststorePassword()\n+    {\n+        return truststorePassword;\n+    }\n+\n+    @Config(\"prometheus.tls.truststore-password\")\n+    public PrometheusConnectorConfig setTruststorePassword(String password)\n+    {\n+        this.truststorePassword = password;\n+        return this;\n+    }\n+\n+    public boolean getVerifyHostName()\n+    {\n+        return verifyHostName;\n+    }\n+\n+    @Config(\"verify-host-name\")\n+    public PrometheusConnectorConfig setVerifyHostName(boolean val)\n+    {\n+        this.verifyHostName = val;\n+        return this;\n+    }\n }\n\ndiff --git a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusErrorCode.java b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusErrorCode.java\nindex fe1edd4be60e0..606ba0a90435e 100644\n--- a/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusErrorCode.java\n+++ b/presto-prometheus/src/main/java/com/facebook/presto/plugin/prometheus/PrometheusErrorCode.java\n@@ -26,7 +26,8 @@ public enum PrometheusErrorCode\n     PROMETHEUS_UNKNOWN_ERROR(0, EXTERNAL),\n     PROMETHEUS_TABLES_METRICS_RETRIEVE_ERROR(1, USER_ERROR),\n     PROMETHEUS_PARSE_ERROR(2, EXTERNAL),\n-    PROMETHEUS_OUTPUT_ERROR(3, EXTERNAL);\n+    PROMETHEUS_OUTPUT_ERROR(3, EXTERNAL),\n+    PROMETHEUS_SECURE_COMMUNICATION_ERROR(4, EXTERNAL);\n \n     private final ErrorCode errorCode;\n \n",
    "test_patch": "diff --git a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java\nindex fe602e996605b..b2e49fb9007ff 100644\n--- a/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java\n+++ b/presto-prometheus/src/test/java/com/facebook/presto/plugin/prometheus/TestPrometheusConnectorConfig.java\n@@ -39,7 +39,11 @@ public void testDefaults()\n                 .setQueryChunkSizeDuration(Duration.valueOf(\"10m\"))\n                 .setMaxQueryRangeDuration(Duration.valueOf(\"1h\"))\n                 .setCacheDuration(Duration.valueOf(\"30s\"))\n-                .setBearerTokenFile(null));\n+                .setBearerTokenFile(null)\n+                .setTlsEnabled(false)\n+                .setTruststorePassword(null)\n+                .setVerifyHostName(false)\n+                .setTrustStorePath(null));\n     }\n \n     @Test\n@@ -51,6 +55,10 @@ public void testExplicitPropertyMappings()\n                 .put(\"prometheus.max-query-duration\", \"1095d\")\n                 .put(\"prometheus.cache-ttl\", \"60s\")\n                 .put(\"prometheus.bearer-token-file\", \"/tmp/bearer_token.txt\")\n+                .put(\"prometheus.tls.enabled\", \"true\")\n+                .put(\"prometheus.tls.truststore-password\", \"password\")\n+                .put(\"prometheus.tls.truststore-path\", \"/tmp/path/truststore\")\n+                .put(\"verify-host-name\", \"true\")\n                 .build();\n \n         URI uri = URI.create(\"file://test.json\");\n@@ -60,6 +68,10 @@ public void testExplicitPropertyMappings()\n         expected.setMaxQueryRangeDuration(Duration.valueOf(\"1095d\"));\n         expected.setCacheDuration(Duration.valueOf(\"60s\"));\n         expected.setBearerTokenFile(new File(\"/tmp/bearer_token.txt\"));\n+        expected.setTlsEnabled(true);\n+        expected.setTruststorePassword(\"password\");\n+        expected.setTrustStorePath(\"/tmp/path/truststore\");\n+        expected.setVerifyHostName(true);\n \n         assertFullMapping(properties, expected);\n     }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23539",
    "pr_id": 23539,
    "issue_id": 22029,
    "repo": "prestodb/presto",
    "problem_statement": "Add support for querying branches and tags in Iceberg\nIceberg metadata includes branches and tags.  The Iceberg connector should have a way of querying these, however, our current time travel implementation (added by #20991) only supports snapshot versions or points in time.  We can modify the time travel syntax to also allow the querying of snapshot references, either tags or branches.\r\n\r\nPart of #22025\r\n\r\n## Expected Behavior or Use Case\r\n`select * from tab1 FOR SYSTEM_VERSION AS OF 'branch-name'`\r\n`select * from tab1 FOR SYSTEM_VERSION AS OF 'tag-name'`\r\n\r\n## Presto Component, Service, or Connector\r\nIceberg connector, with minor changes in parser and SPI to allow String references of tags and branches.\r\n\r\n## Possible Implementation\r\n<!--- Not obligatory, suggest ideas of how to implement the addition or change -->\r\n\r\n## Example Screenshots (if appropriate):\r\n\r\n## Context\r\n<!--- Why do you need this feature or improvement? What is your use case? What are you trying to accomplish? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->",
    "issue_word_count": 164,
    "test_files_count": 3,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java",
      "presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java",
      "presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java"
    ],
    "pr_changed_test_files": [
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java",
      "presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java"
    ],
    "base_commit": "2bceaa02b982f2b10d4d577fac9b3c8944c36ace",
    "head_commit": "e066ac7fef6023bc76f445b2341e7db44b332a6e",
    "repo_url": "https://github.com/prestodb/presto/pull/23539",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23539",
    "dockerfile": "",
    "pr_merged_at": "2024-09-03T19:07:25.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 38ac101b31f81..d9c412723d0ff 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -1340,7 +1340,7 @@ Time Travel\n Iceberg and Presto Iceberg connector support time travel via table snapshots\n identified by unique snapshot IDs. The snapshot IDs are stored in the ``$snapshots``\n metadata table. You can rollback the state of a table to a previous snapshot ID.\n-It also supports time travel query using VERSION (SYSTEM_VERSION) and TIMESTAMP (SYSTEM_TIME) options.\n+It also supports time travel query using SYSTEM_VERSION (VERSION) and SYSTEM_TIME (TIMESTAMP) options.\n \n Example Queries\n ^^^^^^^^^^^^^^^\n@@ -1522,6 +1522,40 @@ In the following query, the expression CURRENT_TIMESTAMP returns the current tim\n             10 | united states |         1 | comment\n     (1 row)\n \n+Querying branches and tags\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Iceberg supports branches and tags which are named references to snapshots.\n+\n+Query Iceberg table by specifying the branch name:\n+\n+.. code-block:: sql\n+\n+    SELECT * FROM nation FOR SYSTEM_VERSION AS OF 'testBranch';\n+\n+.. code-block:: text\n+\n+     nationkey |      name     | regionkey | comment\n+    -----------+---------------+-----------+---------\n+            10 | united states |         1 | comment\n+            20 | canada        |         2 | comment\n+            30 | mexico        |         3 | comment\n+    (3 rows)\n+\n+Query Iceberg table by specifying the tag name:\n+\n+.. code-block:: sql\n+\n+    SELECT * FROM nation FOR SYSTEM_VERSION AS OF 'testTag';\n+\n+.. code-block:: text\n+\n+     nationkey |      name     | regionkey | comment\n+    -----------+---------------+-----------+---------\n+            10 | united states |         1 | comment\n+            20 | canada        |         2 | comment\n+    (3 rows)\n+\n Type mapping\n ------------\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex 788787d756582..0d097596e9a0b 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -21,6 +21,7 @@\n import com.facebook.presto.common.type.SqlTimestampWithTimeZone;\n import com.facebook.presto.common.type.TimestampWithTimeZoneType;\n import com.facebook.presto.common.type.TypeManager;\n+import com.facebook.presto.common.type.VarcharType;\n import com.facebook.presto.hive.HivePartition;\n import com.facebook.presto.hive.HiveWrittenPartitions;\n import com.facebook.presto.hive.NodeVersion;\n@@ -978,8 +979,9 @@ private static long getSnapshotIdForTableVersion(Table table, ConnectorTableVers\n             throw new PrestoException(NOT_SUPPORTED, \"Unsupported table version expression type: \" + tableVersion.getVersionExpressionType());\n         }\n         if (tableVersion.getVersionType() == VersionType.VERSION) {\n+            long snapshotId;\n             if (tableVersion.getVersionExpressionType() instanceof BigintType) {\n-                long snapshotId = (long) tableVersion.getTableVersion();\n+                snapshotId = (long) tableVersion.getTableVersion();\n                 if (table.snapshot(snapshotId) == null) {\n                     throw new PrestoException(ICEBERG_INVALID_SNAPSHOT_ID, \"Iceberg snapshot ID does not exists: \" + snapshotId);\n                 }\n@@ -990,6 +992,13 @@ private static long getSnapshotIdForTableVersion(Table table, ConnectorTableVers\n                     return getSnapshotIdTimeOperator(table, table.snapshot(snapshotId).timestampMillis(), VersionOperator.LESS_THAN);\n                 }\n             }\n+            else if (tableVersion.getVersionExpressionType() instanceof VarcharType) {\n+                String branchOrTagName = ((Slice) tableVersion.getTableVersion()).toStringUtf8();\n+                if (!table.refs().containsKey(branchOrTagName)) {\n+                    throw new PrestoException(ICEBERG_INVALID_SNAPSHOT_ID, \"Could not find Iceberg table branch or tag: \" + branchOrTagName);\n+                }\n+                return table.refs().get(branchOrTagName).snapshotId();\n+            }\n             throw new PrestoException(NOT_SUPPORTED, \"Unsupported table version expression type: \" + tableVersion.getVersionExpressionType());\n         }\n         throw new PrestoException(NOT_SUPPORTED, \"Unsupported table version type: \" + tableVersion.getVersionType());\n\ndiff --git a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\nindex d0703c0eccf0a..49297ff101fc6 100644\n--- a/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n+++ b/presto-main/src/main/java/com/facebook/presto/sql/analyzer/StatementAnalyzer.java\n@@ -29,6 +29,7 @@\n import com.facebook.presto.common.type.RowType;\n import com.facebook.presto.common.type.TimestampWithTimeZoneType;\n import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.common.type.VarcharType;\n import com.facebook.presto.metadata.Metadata;\n import com.facebook.presto.metadata.OperatorNotFoundException;\n import com.facebook.presto.spi.ColumnHandle;\n@@ -1421,9 +1422,9 @@ private Optional<TableHandle> processTableVersion(Table table, QualifiedObjectNa\n                 }\n             }\n             if (tableVersionType == VERSION) {\n-                if (!(stateExprType instanceof BigintType)) {\n+                if (!(stateExprType instanceof BigintType || stateExprType instanceof VarcharType)) {\n                     throw new SemanticException(TYPE_MISMATCH, stateExpr,\n-                            \"Type %s is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\",\n+                            \"Type %s is invalid. Supported table version AS OF/BEFORE expression type is BIGINT or VARCHAR\",\n                             stateExprType.getDisplayName());\n                 }\n             }\n",
    "test_patch": "diff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\nindex 07b4928d99a88..9012547098bb0 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedTestBase.java\n@@ -1539,20 +1539,25 @@ public void testDecimal(boolean decimalVectorReaderEnabled)\n     @Test\n     public void testRefsTable()\n     {\n-        assertUpdate(\"CREATE TABLE test_table_references (id BIGINT)\");\n-        assertUpdate(\"INSERT INTO test_table_references VALUES (0), (1), (2)\", 3);\n+        assertUpdate(\"CREATE TABLE test_table_references (id1 BIGINT, id2 BIGINT)\");\n+        assertUpdate(\"INSERT INTO test_table_references VALUES (0, 00), (1, 10), (2, 20)\", 3);\n \n         Table icebergTable = loadTable(\"test_table_references\");\n         icebergTable.manageSnapshots().createBranch(\"testBranch\").commit();\n \n-        assertUpdate(\"INSERT INTO test_table_references VALUES (0), (1), (2)\", 3);\n+        assertUpdate(\"INSERT INTO test_table_references VALUES (3, 30), (4, 40), (5, 50)\", 3);\n \n         assertEquals(icebergTable.refs().size(), 2);\n         icebergTable.manageSnapshots().createTag(\"testTag\", icebergTable.currentSnapshot().snapshotId()).commit();\n \n         assertEquals(icebergTable.refs().size(), 3);\n+        assertUpdate(\"INSERT INTO test_table_references VALUES (6, 60), (7, 70), (8, 80)\", 3);\n         assertQuery(\"SELECT count(*) FROM \\\"test_table_references$refs\\\"\", \"VALUES 3\");\n \n+        assertQuery(\"SELECT count(*) FROM test_table_references FOR SYSTEM_VERSION AS OF 'testBranch'\", \"VALUES 3\");\n+        assertQuery(\"SELECT count(*) FROM test_table_references FOR SYSTEM_VERSION AS OF 'testTag'\", \"VALUES 6\");\n+        assertQuery(\"SELECT count(*) FROM test_table_references FOR SYSTEM_VERSION AS OF 'main'\", \"VALUES 9\");\n+\n         assertQuery(\"SELECT * from \\\"test_table_references$refs\\\" where name = 'testBranch' and type = 'BRANCH'\",\n                 format(\"VALUES('%s', '%s', %s, %s, %s, %s)\",\n                         \"testBranch\",\n@@ -1570,6 +1575,17 @@ public void testRefsTable()\n                         icebergTable.refs().get(\"testTag\").maxRefAgeMs(),\n                         icebergTable.refs().get(\"testTag\").minSnapshotsToKeep(),\n                         icebergTable.refs().get(\"testTag\").maxSnapshotAgeMs()));\n+\n+        // test branch & tag access when schema is changed\n+        assertUpdate(\"ALTER TABLE test_table_references DROP COLUMN id2\");\n+        assertUpdate(\"ALTER TABLE test_table_references ADD COLUMN id2_new BIGINT\");\n+\n+        // since current table schema is changed from col id2 to id2_new\n+        assertQuery(\"SELECT * FROM test_table_references where id1=1\", \"VALUES(1, NULL)\");\n+        assertQuery(\"SELECT * FROM test_table_references FOR SYSTEM_VERSION AS OF 'testBranch' where id1=1\", \"VALUES(1, NULL)\");\n+        // Currently Presto returns current table schema for any previous snapshot access https://github.com/prestodb/presto/issues/23553\n+        // otherwise querying a tag uses the snapshot's schema https://iceberg.apache.org/docs/nightly/branching/#schema-selection-with-branches-and-tags\n+        assertQuery(\"SELECT * FROM test_table_references FOR SYSTEM_VERSION AS OF 'testTag' where id1=1\", \"VALUES(1, NULL)\");\n     }\n \n     @Test\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java\nindex 3acb4629cdb53..8da55ce029aae 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTableVersion.java\n@@ -187,6 +187,8 @@ public void testTableVersionMisc()\n     {\n         // Alias cases - SYSTEM_TIME and SYSTEM_VERSION\n         assertQuery(\"SELECT desc FROM \" + tableName1 + \" FOR SYSTEM_VERSION AS OF \" + tab1VersionId1 + \" ORDER BY 1\", \"VALUES 'aaa'\");\n+        assertQuery(\"SELECT count(*) FROM \" + tableName1 + \" FOR SYSTEM_VERSION AS OF 'main'\", \"VALUES 3\");\n+        assertQuery(\"SELECT desc FROM \" + tableName1 + \" FOR SYSTEM_VERSION AS OF 'main'\" + \" ORDER BY 1\", \"VALUES ('aaa'), ('bbb'), ('ccc')\");\n         assertQuery(\"SELECT desc FROM \" + tableName1 + \" FOR SYSTEM_TIME AS OF TIMESTAMP \" + \"'\" + tab1Timestamp1 + \"'\" + \" ORDER BY 1\", \"VALUES 'aaa'\");\n         assertQuery(\"SELECT desc FROM \" + tableName1 + \" FOR SYSTEM_TIME AS OF CURRENT_TIMESTAMP ORDER BY 1\", \"VALUES ('aaa'), ('bbb'), ('ccc')\");\n         assertQuery(\"SELECT SUM(id) FROM \" + tableName1 + \" FOR SYSTEM_VERSION AS OF \" + tab1VersionId2, \"VALUES 3\");\n@@ -272,10 +274,10 @@ public void testTableVersionMisc()\n     @Test\n     public void testTableVersionErrors()\n     {\n-        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF 100\", \".* Type integer is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\");\n-        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF 'bad'\", \".* Type varchar\\\\(3\\\\) is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\");\n-        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF CURRENT_DATE\", \".* Type date is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\");\n-        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF CURRENT_TIMESTAMP\", \".* Type timestamp with time zone is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\");\n+        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF 100\", \".* Type integer is invalid. Supported table version AS OF/BEFORE expression type is BIGINT or VARCHAR\");\n+        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF 'bad'\", \"Could not find Iceberg table branch or tag: bad\");\n+        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF CURRENT_DATE\", \".* Type date is invalid. Supported table version AS OF/BEFORE expression type is BIGINT or VARCHAR\");\n+        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF CURRENT_TIMESTAMP\", \".* Type timestamp with time zone is invalid. Supported table version AS OF/BEFORE expression type is BIGINT or VARCHAR\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF id\", \".* cannot be resolved\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF (SELECT 10000000)\", \".* Constant expression cannot contain a subquery\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION AS OF NULL\", \"Table version AS OF/BEFORE expression cannot be NULL for .*\");\n@@ -296,7 +298,7 @@ public void testTableVersionErrors()\n \n         assertQueryFails(\"SELECT desc FROM \" + tableName1 + \" FOR VERSION BEFORE \" + tab1VersionId1 + \" ORDER BY 1\", \"No history found based on timestamp for table \\\"test_tt_schema\\\".\\\"test_table_version_tab1\\\"\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR TIMESTAMP BEFORE TIMESTAMP \" + \"'\" + tab2Timestamp1 + \"' - INTERVAL '1' MONTH\", \"No history found based on timestamp for table \\\"test_tt_schema\\\".\\\"test_table_version_tab2\\\"\");\n-        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION BEFORE 100\", \".* Type integer is invalid. Supported table version AS OF/BEFORE expression type is BIGINT\");\n+        assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION BEFORE 100\", \".* Type integer is invalid. Supported table version AS OF/BEFORE expression type is BIGINT or VARCHAR\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR VERSION BEFORE \" + tab2VersionId1 + \" - \" + tab2VersionId1, \"Iceberg snapshot ID does not exists: 0\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR TIMESTAMP BEFORE 'bad'\", \".* Type varchar\\\\(3\\\\) is invalid. Supported table version AS OF/BEFORE expression type is Timestamp with Time Zone.\");\n         assertQueryFails(\"SELECT desc FROM \" + tableName2 + \" FOR TIMESTAMP BEFORE NULL\", \"Table version AS OF/BEFORE expression cannot be NULL for .*\");\n\ndiff --git a/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java b/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\nindex 329b1e9703c09..7b5bba37d60a4 100644\n--- a/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\n+++ b/presto-parser/src/test/java/com/facebook/presto/sql/parser/TestSqlParser.java\n@@ -3144,6 +3144,18 @@ public void testSelectWithAsOfVersion()\n                         Optional.empty(),\n                         Optional.empty()));\n \n+        assertStatement(\"SELECT * FROM table1 FOR VERSION AS OF 'branch-name'\",\n+                simpleQuery(\n+                        selectList(new AllColumns()),\n+                        new Table(new NodeLocation(1, 15), QualifiedName.of(\"table1\"),\n+                                new TableVersionExpression(VERSION, TableVersionOperator.EQUAL, new StringLiteral(\"branch-name\"))),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty()));\n+\n         assertStatement(\"SELECT * FROM table1 FOR SYSTEM_VERSION AS OF 8772871542276440693\",\n                 simpleQuery(\n                         selectList(new AllColumns()),\n@@ -3168,6 +3180,18 @@ public void testSelectWithAsOfVersion()\n                         Optional.empty(),\n                         Optional.empty()));\n \n+        assertStatement(\"SELECT * FROM table1 FOR VERSION AS OF 'branch-name' WHERE (c1 = 100)\",\n+                simpleQuery(\n+                        selectList(new AllColumns()),\n+                        new Table(new NodeLocation(1, 15), QualifiedName.of(\"table1\"),\n+                                new TableVersionExpression(VERSION, TableVersionOperator.EQUAL, new StringLiteral(\"branch-name\"))),\n+                        Optional.of(new ComparisonExpression(EQUAL, new Identifier(\"c1\"), new LongLiteral(\"100\"))),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty(),\n+                        Optional.empty()));\n+\n         assertStatement(\"SELECT * FROM table1 FOR VERSION AS OF 8772871542276440693, table2 FOR VERSION AS OF 123456789012345\",\n                 simpleQuery(selectList(new AllColumns()),\n                         new Join(Join.Type.IMPLICIT,\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23536",
    "pr_id": 23536,
    "issue_id": 22580,
    "repo": "prestodb/presto",
    "problem_statement": "[native] Delta table access via Symlink table using Hive Connector\n Delta table access via Symlink table using Hive Connector in Prestissimo\r\n\r\n## Expected Behavior or Use Case\r\nDelta table can be accessed using hive connector by registering them as a symlink tables in HMS. https://docs.delta.io/latest/presto-integration.html\r\nThis is to extend this same support for Prestissimo using Hive Connector.\r\n\r\n## Presto Component, Service, or Connector\r\npresto-native-execution\r\n\r\n## Possible Implementation\r\nCurrenlty mapping for inputformat and serde are missing in Prestissimo to access symlink tables. We need to add that mapping in PrestoToVeloxConnector.cpp to successfully access Delta tables.\r\n\r\n## Context\r\nDelta table can be accessed using hive connector by registering them as a symlink tables in HMS. https://docs.delta.io/latest/presto-integration.html\r\nThis is to extend this same support for Prestissimo using Hive Connector.",
    "issue_word_count": 139,
    "test_files_count": 6,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/presto-cpp.rst",
      "presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java",
      "presto-native-execution/pom.xml",
      "presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/SymlinkManifestGeneratorUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeHiveExternalTableTpchQueriesParquet.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/SymlinkManifestGeneratorUtils.java",
      "presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeHiveExternalTableTpchQueriesParquet.java",
      "presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java"
    ],
    "base_commit": "12afd2664b7e33b82332dbe7e4d81878470cc449",
    "head_commit": "f3ea758ef272608265da79237530ed60d74c25af",
    "repo_url": "https://github.com/prestodb/presto/pull/23536",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23536",
    "dockerfile": "",
    "pr_merged_at": "2024-10-24T15:57:37.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/presto-cpp.rst b/presto-docs/src/main/sphinx/presto-cpp.rst\nindex 0f9ed62a12ced..120bcd4a02a4a 100644\n--- a/presto-docs/src/main/sphinx/presto-cpp.rst\n+++ b/presto-docs/src/main/sphinx/presto-cpp.rst\n@@ -45,6 +45,10 @@ Only specific connectors are supported in the Presto C++ evaluation engine.\n \n * Hive connector for reads and writes, including CTAS, are supported.\n \n+* Hive Connector supports Delta Lake table access using Symlink table in Prestissimo.\n+  For more information about Symlink tables, see `Presto, Trino, and Athena to Delta Lake integration using\n+  manifests <https://docs.delta.io/latest/presto-integration.html>`_.\n+\n * Iceberg tables are supported only for reads.\n \n * Iceberg connector supports both V1 and V2 tables, including tables with delete files.\n\ndiff --git a/presto-native-execution/pom.xml b/presto-native-execution/pom.xml\nindex 1318524adac4e..df39f43a5739b 100644\n--- a/presto-native-execution/pom.xml\n+++ b/presto-native-execution/pom.xml\n@@ -18,6 +18,16 @@\n     </properties>\n \n     <dependencies>\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-tpch</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift.tpch</groupId>\n+            <artifactId>tpch</artifactId>\n+        </dependency>\n+\n         <dependency>\n             <groupId>com.google.guava</groupId>\n             <artifactId>guava</artifactId>\n@@ -154,6 +164,11 @@\n             </exclusions>\n         </dependency>\n \n+        <dependency>\n+            <groupId>com.facebook.presto.hive</groupId>\n+            <artifactId>hive-apache</artifactId>\n+        </dependency>\n+\n         <dependency>\n             <groupId>com.facebook.presto</groupId>\n             <artifactId>presto-hive</artifactId>\n\ndiff --git a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\nindex faa4f93eaf67a..3c8970da7549d 100644\n--- a/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\n+++ b/presto-native-execution/presto_cpp/main/types/PrestoToVeloxConnector.cpp\n@@ -77,6 +77,13 @@ dwio::common::FileFormat toVeloxFileFormat(\n     } else if (format.serDe == \"org.apache.hive.hcatalog.data.JsonSerDe\") {\n       return dwio::common::FileFormat::JSON;\n     }\n+  } else if (\n+      format.inputFormat ==\n+      \"org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\") {\n+    if (format.serDe ==\n+        \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\") {\n+      return dwio::common::FileFormat::PARQUET;\n+    }\n   } else if (format.inputFormat == \"com.facebook.alpha.AlphaInputFormat\") {\n     // ALPHA has been renamed in Velox to NIMBLE.\n     return dwio::common::FileFormat::NIMBLE;\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java b/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\nindex a6e842b29b627..c19ee4951bb1f 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/HiveQueryRunner.java\n@@ -302,7 +302,7 @@ public static List<String> getAllTpcdsTableNames()\n         return tables.build();\n     }\n \n-    private static ExtendedHiveMetastore getFileHiveMetastore(DistributedQueryRunner queryRunner)\n+    public static ExtendedHiveMetastore getFileHiveMetastore(DistributedQueryRunner queryRunner)\n     {\n         File dataDirectory = queryRunner.getCoordinator().getDataDirectory().resolve(\"hive_data\").toFile();\n         HiveClientConfig hiveClientConfig = new HiveClientConfig();\n@@ -362,7 +362,7 @@ private static void setupLogging()\n         logging.setLevel(\"parquet.hadoop\", WARN);\n     }\n \n-    private static Database createDatabaseMetastoreObject(String name)\n+    public static Database createDatabaseMetastoreObject(String name)\n     {\n         return Database.builder()\n                 .setDatabaseName(name)\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java\nnew file mode 100644\nindex 0000000000000..dadce6334ed1b\n--- /dev/null\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/AbstractTestNativeHiveExternalTableTpchQueries.java\n@@ -0,0 +1,128 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.nativeworker;\n+\n+import com.facebook.presto.hive.HiveType;\n+import com.facebook.presto.hive.metastore.Column;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.DistributedQueryRunner;\n+import com.facebook.presto.tpch.ColumnNaming;\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.tpch.TpchColumn;\n+import io.airlift.tpch.TpchEntity;\n+import io.airlift.tpch.TpchTable;\n+import org.testng.annotations.AfterClass;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createCustomer;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createLineitem;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createNationWithFormat;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createOrders;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createPart;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createPartSupp;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createRegion;\n+import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.createSupplier;\n+import static com.facebook.presto.nativeworker.PrestoNativeQueryRunnerUtils.createExternalTable;\n+import static com.facebook.presto.nativeworker.SymlinkManifestGeneratorUtils.cleanupSymlinkData;\n+import static com.facebook.presto.tpch.TpchMetadata.getPrestoType;\n+import static java.lang.String.format;\n+\n+public abstract class AbstractTestNativeHiveExternalTableTpchQueries\n+        extends AbstractTestNativeTpchQueries\n+{\n+    private static final String HIVE = \"hive\";\n+    private static final String TPCH = \"tpch\";\n+    private static final String SYMLINK_FOLDER = \"symlink_tables_manifests\";\n+    private static final String HIVE_DATA = \"hive_data\";\n+    private static final ImmutableList<String> TPCH_TABLES = ImmutableList.of(\"orders\", \"lineitem\", \"nation\",\n+            \"customer\", \"part\", \"partsupp\", \"region\", \"supplier\");\n+\n+    /**\n+     * Returns the Hive type string corresponding to a given TPCH type.\n+     * Currently only supports conversion for \"integer\" TPCH type.\n+     *\n+     * @param tpchType the TPCH type as a string\n+     * @return the corresponding Hive type as a string\n+     */\n+    private static String getHiveTypeString(String tpchType)\n+    {\n+        switch (tpchType) {\n+            case \"integer\":\n+                return \"int\";\n+            default:\n+                return tpchType;\n+        }\n+    }\n+\n+    /**\n+     * Retrieves a list of columns for a specified TPCH table.\n+     * The method looks up the TPCH table by name and converts its columns to Hive-compatible\n+     * column representations.\n+     *\n+     * @param tableName the name of the TPCH table\n+     * @return a list of Column objects representing the columns of the table\n+     */\n+    private static List<Column> getTpchTableColumns(String tableName)\n+    {\n+        TpchTable<?> table = TpchTable.getTable(tableName);\n+        ColumnNaming columnNaming = ColumnNaming.SIMPLIFIED;\n+        ImmutableList.Builder<Column> columns = ImmutableList.builder();\n+        for (TpchColumn<? extends TpchEntity> column : table.getColumns()) {\n+            columns.add(new Column(columnNaming.getName(column), HiveType.valueOf(getHiveTypeString(getPrestoType(column).getDisplayName())), Optional.empty(), Optional.empty()));\n+        }\n+        return columns.build();\n+    }\n+\n+    @Override\n+    protected void createTables()\n+    {\n+        QueryRunner javaQueryRunner = (QueryRunner) getExpectedQueryRunner();\n+        createOrders(javaQueryRunner);\n+        createLineitem(javaQueryRunner);\n+        createNationWithFormat(javaQueryRunner, \"PARQUET\");\n+        createCustomer(javaQueryRunner);\n+        createPart(javaQueryRunner);\n+        createPartSupp(javaQueryRunner);\n+        createRegion(javaQueryRunner);\n+        createSupplier(javaQueryRunner);\n+\n+        for (String tableName : TPCH_TABLES) {\n+            createExternalTable(javaQueryRunner, TPCH, tableName, getTpchTableColumns(tableName));\n+        }\n+    }\n+\n+    @AfterClass(alwaysRun = true)\n+    public void tearDown()\n+    {\n+        QueryRunner javaQueryRunner = (QueryRunner) getExpectedQueryRunner();\n+        for (String tableName : TPCH_TABLES) {\n+            dropTableIfExists(javaQueryRunner, HIVE, TPCH, tableName);\n+        }\n+        assertUpdate(format(\"DROP SCHEMA IF EXISTS %s.%s\", HIVE, TPCH));\n+\n+        File dataDirectory = ((DistributedQueryRunner) javaQueryRunner).getCoordinator().getDataDirectory().resolve(HIVE_DATA).toFile();\n+        Path symlinkTableDataPath = dataDirectory.toPath().getParent().resolve(SYMLINK_FOLDER);\n+        try {\n+            cleanupSymlinkData(symlinkTableDataPath);\n+        }\n+        catch (IOException e) {\n+            // ignore\n+        }\n+    }\n+}\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\nindex 69bb2433b3f18..32527ef53c0aa 100644\n--- a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/PrestoNativeQueryRunnerUtils.java\n@@ -14,33 +14,54 @@\n package com.facebook.presto.nativeworker;\n \n import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.common.ErrorCode;\n import com.facebook.presto.functionNamespace.FunctionNamespaceManagerPlugin;\n import com.facebook.presto.functionNamespace.json.JsonFileBasedFunctionNamespaceManagerFactory;\n import com.facebook.presto.hive.HiveQueryRunner;\n+import com.facebook.presto.hive.metastore.Column;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+import com.facebook.presto.hive.metastore.PrincipalPrivileges;\n+import com.facebook.presto.hive.metastore.Storage;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.hive.metastore.Table;\n import com.facebook.presto.iceberg.FileFormat;\n+import com.facebook.presto.spi.PrestoException;\n import com.facebook.presto.testing.QueryRunner;\n import com.facebook.presto.tests.DistributedQueryRunner;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableMultimap;\n import com.google.common.io.Resources;\n+import org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;\n+import org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n \n+import java.io.File;\n import java.io.IOException;\n import java.io.UncheckedIOException;\n import java.net.URI;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.nio.file.Paths;\n+import java.util.List;\n import java.util.Optional;\n import java.util.OptionalInt;\n import java.util.UUID;\n import java.util.function.BiFunction;\n \n+import static com.facebook.presto.common.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.hive.HiveQueryRunner.METASTORE_CONTEXT;\n+import static com.facebook.presto.hive.HiveQueryRunner.createDatabaseMetastoreObject;\n+import static com.facebook.presto.hive.HiveQueryRunner.getFileHiveMetastore;\n import static com.facebook.presto.hive.HiveTestUtils.getProperty;\n+import static com.facebook.presto.hive.metastore.PrestoTableType.EXTERNAL_TABLE;\n import static com.facebook.presto.iceberg.IcebergQueryRunner.createIcebergQueryRunner;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerHiveProperties;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerIcebergProperties;\n import static com.facebook.presto.nativeworker.NativeQueryRunnerUtils.getNativeWorkerSystemProperties;\n+import static com.facebook.presto.nativeworker.SymlinkManifestGeneratorUtils.createSymlinkManifest;\n import static java.lang.String.format;\n+import static java.util.Collections.emptyList;\n import static java.util.Objects.requireNonNull;\n import static org.testng.Assert.assertTrue;\n \n@@ -50,11 +71,20 @@ public class PrestoNativeQueryRunnerUtils\n     public static final String REMOTE_FUNCTION_UDS = \"remote_function_server.socket\";\n     public static final String REMOTE_FUNCTION_JSON_SIGNATURES = \"remote_function_server.json\";\n     public static final String REMOTE_FUNCTION_CATALOG_NAME = \"remote\";\n+    public static final String HIVE_DATA = \"hive_data\";\n \n     protected static final String ICEBERG_DEFAULT_STORAGE_FORMAT = \"PARQUET\";\n \n     private static final Logger log = Logger.get(PrestoNativeQueryRunnerUtils.class);\n     private static final String DEFAULT_STORAGE_FORMAT = \"DWRF\";\n+    private static final String SYMLINK_FOLDER = \"symlink_tables_manifests\";\n+    private static final PrincipalPrivileges PRINCIPAL_PRIVILEGES = new PrincipalPrivileges(ImmutableMultimap.of(), ImmutableMultimap.of());\n+    private static final ErrorCode CREATE_ERROR_CODE = new ErrorCode(123, \"CREATE_ERROR_CODE\", INTERNAL_ERROR);\n+\n+    private static final StorageFormat STORAGE_FORMAT_SYMLINK_TABLE = StorageFormat.create(\n+            ParquetHiveSerDe.class.getName(),\n+            SymlinkTextInputFormat.class.getName(),\n+            HiveIgnoreKeyTextOutputFormat.class.getName());\n     private PrestoNativeQueryRunnerUtils() {}\n \n     public static QueryRunner createQueryRunner(boolean addStorageFormatToPath)\n@@ -147,6 +177,28 @@ public static QueryRunner createJavaQueryRunner(Optional<Path> baseDataDirectory\n         return queryRunner;\n     }\n \n+    public static void createExternalTable(QueryRunner queryRunner, String schemaName, String tableName, List<Column> columns)\n+    {\n+        ExtendedHiveMetastore metastore = getFileHiveMetastore((DistributedQueryRunner) queryRunner);\n+        File dataDirectory = ((DistributedQueryRunner) queryRunner).getCoordinator().getDataDirectory().resolve(HIVE_DATA).toFile();\n+        Path hiveTableDataPath = dataDirectory.toPath().resolve(schemaName).resolve(tableName);\n+        Path symlinkTableDataPath = dataDirectory.toPath().getParent().resolve(SYMLINK_FOLDER).resolve(tableName);\n+\n+        try {\n+            createSymlinkManifest(hiveTableDataPath, symlinkTableDataPath);\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(() -> CREATE_ERROR_CODE, \"Failed to create symlink manifest file for table: \" + tableName, e);\n+        }\n+\n+        if (!metastore.getDatabase(METASTORE_CONTEXT, schemaName).isPresent()) {\n+            metastore.createDatabase(METASTORE_CONTEXT, createDatabaseMetastoreObject(schemaName));\n+        }\n+        if (!metastore.getTable(METASTORE_CONTEXT, schemaName, tableName).isPresent()) {\n+            metastore.createTable(METASTORE_CONTEXT, createHiveSymlinkTable(schemaName, tableName, columns, symlinkTableDataPath.toString()), PRINCIPAL_PRIVILEGES, emptyList());\n+        }\n+    }\n+\n     public static QueryRunner createJavaIcebergQueryRunner(boolean addStorageFormatToPath)\n             throws Exception\n     {\n@@ -518,4 +570,24 @@ public static void setupJsonFunctionNamespaceManager(QueryRunner queryRunner, St\n                         \"function-implementation-type\", \"CPP\",\n                         \"json-based-function-manager.path-to-function-definition\", jsonDefinitionPath));\n     }\n+\n+    private static Table createHiveSymlinkTable(String databaseName, String tableName, List<Column> columns, String location)\n+    {\n+        return new Table(\n+                databaseName,\n+                tableName,\n+                \"hive\",\n+                EXTERNAL_TABLE,\n+                new Storage(STORAGE_FORMAT_SYMLINK_TABLE,\n+                        \"file:\" + location,\n+                        Optional.empty(),\n+                        false,\n+                        ImmutableMap.of(),\n+                        ImmutableMap.of()),\n+                columns,\n+                ImmutableList.of(),\n+                ImmutableMap.of(),\n+                Optional.empty(),\n+                Optional.empty());\n+    }\n }\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/SymlinkManifestGeneratorUtils.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/SymlinkManifestGeneratorUtils.java\nnew file mode 100644\nindex 0000000000000..44405e329ebaf\n--- /dev/null\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/SymlinkManifestGeneratorUtils.java\n@@ -0,0 +1,105 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.nativeworker;\n+\n+import com.google.common.io.MoreFiles;\n+import com.google.common.io.RecursiveDeleteOption;\n+\n+import java.io.BufferedWriter;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Utility class for generating and managing symlink manifest tables.\n+ * This class provides methods to create symlink manifest files, clean up symlink data,\n+ * and handle TPCH-related table columns and types.\n+ */\n+public final class SymlinkManifestGeneratorUtils\n+{\n+    private SymlinkManifestGeneratorUtils() {}\n+\n+    /**\n+     * Creates a symlink manifest file at the specified directory.\n+     * This method collects all file paths under the hiveTableLocation and writes them\n+     * to a symlink manifest file named \"symlink_manifest\" inside the specified symlinkManifestDir.\n+     *\n+     * @param hiveTableLocation the root directory of the Hive table files\n+     * @param symlinkManifestDir the directory where the symlink manifest should be created\n+     * @throws IOException if an I/O error occurs while creating the directories or writing the manifest\n+     */\n+    public static void createSymlinkManifest(Path hiveTableLocation, Path symlinkManifestDir) throws IOException\n+    {\n+        if (Files.notExists(symlinkManifestDir.getParent())) {\n+            Files.createDirectory(symlinkManifestDir.getParent());\n+        }\n+\n+        if (Files.notExists(symlinkManifestDir)) {\n+            Files.createDirectory(symlinkManifestDir);\n+        }\n+\n+        Path manifestFilePath = symlinkManifestDir.resolve(\"symlink_manifest\");\n+        try (BufferedWriter writer = Files.newBufferedWriter(manifestFilePath, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) {\n+            List<String> fileList = new ArrayList<>();\n+            collectDataFiles(hiveTableLocation, fileList);\n+\n+            for (String filePath : fileList) {\n+                writer.write(filePath);\n+                writer.write(System.lineSeparator());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Recursively collects all non-hidden file paths in the source directory.\n+     * If a directory is encountered, the method is called recursively to collect files in subdirectories.\n+     *\n+     * @param sourceDir the directory to start collecting files from\n+     * @param fileList the list where the collected file paths are stored\n+     * @throws IOException if an I/O error occurs while accessing the directory or files\n+     */\n+    private static void collectDataFiles(Path sourceDir, List<String> fileList) throws IOException\n+    {\n+        try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir)) {\n+            for (Path entry : stream) {\n+                if (!Files.isHidden(entry)) {\n+                    if (Files.isDirectory(entry)) {\n+                        collectDataFiles(entry, fileList);\n+                    }\n+                    else {\n+                        fileList.add(entry.toString());\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Cleans up (deletes) all files and subdirectories inside the specified directory.\n+     * This method walks through the directory tree and deletes files in reverse order to ensure\n+     * that directories are deleted after their contents.\n+     *\n+     * @param directory the root directory to clean up\n+     * @throws IOException if an I/O error occurs during file deletion\n+     */\n+\n+    public static void cleanupSymlinkData(Path directory) throws IOException\n+    {\n+        MoreFiles.deleteRecursively(directory, RecursiveDeleteOption.ALLOW_INSECURE);\n+    }\n+}\n\ndiff --git a/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeHiveExternalTableTpchQueriesParquet.java b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeHiveExternalTableTpchQueriesParquet.java\nnew file mode 100644\nindex 0000000000000..37184cbd6c448\n--- /dev/null\n+++ b/presto-native-execution/src/test/java/com/facebook/presto/nativeworker/TestPrestoNativeHiveExternalTableTpchQueriesParquet.java\n@@ -0,0 +1,35 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.nativeworker;\n+\n+import com.facebook.presto.testing.ExpectedQueryRunner;\n+import com.facebook.presto.testing.QueryRunner;\n+import org.testng.annotations.Test;\n+\n+@Test(groups = {\"parquet\"})\n+public class TestPrestoNativeHiveExternalTableTpchQueriesParquet\n+        extends AbstractTestNativeHiveExternalTableTpchQueries\n+{\n+    @Override\n+    protected QueryRunner createQueryRunner() throws Exception\n+    {\n+        return PrestoNativeQueryRunnerUtils.createNativeQueryRunner(true, \"PARQUET\");\n+    }\n+\n+    @Override\n+    protected ExpectedQueryRunner createExpectedQueryRunner() throws Exception\n+    {\n+        return PrestoNativeQueryRunnerUtils.createJavaQueryRunner(\"PARQUET\");\n+    }\n+}\n\ndiff --git a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\nindex 1757ba65e8d5a..b2a4c607ee618 100644\n--- a/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\n+++ b/presto-tests/src/main/java/com/facebook/presto/tests/AbstractTestQueryFramework.java\n@@ -619,4 +619,9 @@ public interface ExpectedQueryRunnerSupplier\n         ExpectedQueryRunner get()\n                 throws Exception;\n     }\n+\n+    public static void dropTableIfExists(QueryRunner queryRunner, String catalogName, String schemaName, String tableName)\n+    {\n+        queryRunner.execute(format(\"DROP TABLE IF EXISTS %s.%s.%s\", catalogName, schemaName, tableName));\n+    }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "prestodb__presto-23534",
    "pr_id": 23534,
    "issue_id": 23529,
    "repo": "prestodb/presto",
    "problem_statement": "Iceberg `timestamptz` should map to Presto `TIMESTAMP WITH TIME ZONE` type\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Presto version used: Any\r\n* Storage (HDFS/S3/GCS..): NA\r\n* Data source and connector used: Iceberg\r\n* Deployment (Cloud or On-prem): NA\r\n* [Pastebin](https://pastebin.com/) link to the complete debug logs:\r\n\r\n## Expected Behavior\r\nCurrently, the Presto Iceberg connector does not differentiate between `timestamp` and `timestamptz`, mapping both to `TIMESTAMP`.  This is incorrect, especially when `deprecated.legacy-timestamp` is set to `false`, because `timestamptz` represents a point in time value, whereas Presto's `TIMESTAMP` represents a timestamp that is not a point in time.\r\n\r\n## Current Behavior\r\nAll Iceberg `timestamp` and `timestamptz`s are being mapped to `TIMESTAMP`\r\n\r\n## Possible Solution\r\nImprove the type mapping to use `TIMESTAMP WITH TIME ZONE`\r\n\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1.\r\n2.\r\n3.\r\n4.\r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n",
    "issue_word_count": 191,
    "test_files_count": 5,
    "non_test_files_count": 14,
    "pr_changed_files": [
      "presto-docs/src/main/sphinx/connector/iceberg.rst",
      "presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java",
      "presto-iceberg/src/main/java/com/facebook/presto/iceberg/TypeConverter.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTypes.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/Decoders.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/ValuesDecoder.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/delta/Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/plain/Int64TimeAndTimestampMicrosPlainValuesDecoder.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/rle/Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java",
      "presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java"
    ],
    "pr_changed_test_files": [
      "presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTypes.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java",
      "presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java"
    ],
    "base_commit": "44641296d4108c2c17f619c2e98b512148fccfe7",
    "head_commit": "0ce3cc8736b7267d9355d080401968c40fb1819c",
    "repo_url": "https://github.com/prestodb/presto/pull/23534",
    "swe_url": "https://swe-bench-plus.turing.com/repos/prestodb__presto/23534",
    "dockerfile": "",
    "pr_merged_at": "2025-01-30T16:10:10.000Z",
    "patch": "diff --git a/presto-docs/src/main/sphinx/connector/iceberg.rst b/presto-docs/src/main/sphinx/connector/iceberg.rst\nindex 0e156b005a2cd..8408736f92abc 100644\n--- a/presto-docs/src/main/sphinx/connector/iceberg.rst\n+++ b/presto-docs/src/main/sphinx/connector/iceberg.rst\n@@ -1775,6 +1775,10 @@ Map of Iceberg types to the relevant PrestoDB types:\n     - ``TIME``\n   * - ``TIMESTAMP``\n     - ``TIMESTAMP``\n+  * - ``TIMESTAMP``\n+    - ``TIMESTAMP_WITH_TIMEZONE``\n+  * - ``STRING``\n+    - ``VARCHAR``\n   * - ``UUID``\n     - ``UUID``\n   * - ``LIST``\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java\nindex 3f05a48d1219a..285a9cf863b5f 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java\n@@ -30,6 +30,7 @@\n import com.facebook.presto.common.type.RowType;\n import com.facebook.presto.common.type.TimeType;\n import com.facebook.presto.common.type.TimestampType;\n+import com.facebook.presto.common.type.TimestampWithTimeZoneType;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.common.type.UuidType;\n import com.facebook.presto.common.type.VarbinaryType;\n@@ -47,6 +48,7 @@\n import static com.facebook.presto.common.predicate.Marker.Bound.ABOVE;\n import static com.facebook.presto.common.predicate.Marker.Bound.BELOW;\n import static com.facebook.presto.common.predicate.Marker.Bound.EXACTLY;\n+import static com.facebook.presto.common.type.DateTimeEncoding.unpackMillisUtc;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.getPushedDownSubfield;\n import static com.facebook.presto.iceberg.IcebergColumnHandle.isPushedDownSubfield;\n import static com.facebook.presto.parquet.ParquetTypeUtils.columnPathFromSubfield;\n@@ -203,6 +205,10 @@ private static Object getIcebergLiteralValue(Type type, Marker marker)\n             return MILLISECONDS.toMicros((Long) marker.getValue());\n         }\n \n+        if (type instanceof TimestampWithTimeZoneType) {\n+            return MILLISECONDS.toMicros(unpackMillisUtc((Long) marker.getValue()));\n+        }\n+\n         if (type instanceof VarcharType) {\n             return ((Slice) marker.getValue()).toStringUtf8();\n         }\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\nindex 4fa63f09e5606..457896af447dd 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergAbstractMetadata.java\n@@ -154,7 +154,6 @@\n import static com.facebook.presto.iceberg.IcebergUtil.tryGetProperties;\n import static com.facebook.presto.iceberg.IcebergUtil.tryGetSchema;\n import static com.facebook.presto.iceberg.IcebergUtil.validateTableMode;\n-import static com.facebook.presto.iceberg.IcebergUtil.verifyTypeSupported;\n import static com.facebook.presto.iceberg.PartitionFields.getPartitionColumnName;\n import static com.facebook.presto.iceberg.PartitionFields.getTransformTerm;\n import static com.facebook.presto.iceberg.PartitionFields.toPartitionFields;\n@@ -693,10 +692,6 @@ public void addColumn(ConnectorSession session, ConnectorTableHandle tableHandle\n \n         Type columnType = toIcebergType(column.getType());\n \n-        if (columnType.equals(Types.TimestampType.withZone())) {\n-            throw new PrestoException(NOT_SUPPORTED, format(\"Iceberg column type %s is not supported\", columnType));\n-        }\n-\n         IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n         verify(handle.getIcebergTableName().getTableType() == DATA, \"only the data table can have columns added\");\n         Table icebergTable = getIcebergTable(session, handle.getSchemaTableName());\n@@ -754,8 +749,6 @@ public ConnectorInsertTableHandle beginInsert(ConnectorSession session, Connecto\n         Table icebergTable = getIcebergTable(session, table.getSchemaTableName());\n         validateTableMode(session, icebergTable);\n \n-        verifyTypeSupported(icebergTable.schema());\n-\n         return beginIcebergTableInsert(session, table, icebergTable);\n     }\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\nindex f50d930e17966..4ae5daeab8527 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHiveMetadata.java\n@@ -124,7 +124,6 @@\n import static com.facebook.presto.iceberg.IcebergUtil.populateTableProperties;\n import static com.facebook.presto.iceberg.IcebergUtil.toHiveColumns;\n import static com.facebook.presto.iceberg.IcebergUtil.tryGetProperties;\n-import static com.facebook.presto.iceberg.IcebergUtil.verifyTypeSupported;\n import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n import static com.facebook.presto.iceberg.PartitionSpecConverter.toPrestoPartitionSpec;\n import static com.facebook.presto.iceberg.SchemaConverter.toPrestoSchema;\n@@ -307,8 +306,6 @@ public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, Con\n \n         Schema schema = toIcebergSchema(tableMetadata.getColumns());\n \n-        verifyTypeSupported(schema);\n-\n         PartitionSpec partitionSpec = parsePartitionFields(schema, getPartitioning(tableMetadata.getProperties()));\n \n         MetastoreContext metastoreContext = getMetastoreContext(session);\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\nindex a36759cd7a136..43a812b9d3dd0 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergNativeMetadata.java\n@@ -67,7 +67,6 @@\n import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergTable;\n import static com.facebook.presto.iceberg.IcebergUtil.getNativeIcebergView;\n import static com.facebook.presto.iceberg.IcebergUtil.populateTableProperties;\n-import static com.facebook.presto.iceberg.IcebergUtil.verifyTypeSupported;\n import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n import static com.facebook.presto.iceberg.PartitionSpecConverter.toPrestoPartitionSpec;\n import static com.facebook.presto.iceberg.SchemaConverter.toPrestoSchema;\n@@ -308,8 +307,6 @@ public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, Con\n \n         Schema schema = toIcebergSchema(tableMetadata.getColumns());\n \n-        verifyTypeSupported(schema);\n-\n         PartitionSpec partitionSpec = parsePartitionFields(schema, getPartitioning(tableMetadata.getProperties()));\n         FileFormat fileFormat = getFileFormat(tableMetadata.getProperties());\n \n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\nindex c509a814640af..09a879702e4a0 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergUtil.java\n@@ -457,13 +457,6 @@ public static void validateTableMode(ConnectorSession session, org.apache.iceber\n         }\n     }\n \n-    public static void verifyTypeSupported(Schema schema)\n-    {\n-        if (schema.columns().stream().anyMatch(column -> Types.TimestampType.withZone().equals(column.type()))) {\n-            throw new PrestoException(NOT_SUPPORTED, format(\"Iceberg column type %s is not supported\", Types.TimestampType.withZone()));\n-        }\n-    }\n-\n     public static Map<String, String> createIcebergViewProperties(ConnectorSession session, String prestoVersion)\n     {\n         return ImmutableMap.<String, String>builder()\n\ndiff --git a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/TypeConverter.java b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/TypeConverter.java\nindex 9897897cc8042..ca8db778ae87c 100644\n--- a/presto-iceberg/src/main/java/com/facebook/presto/iceberg/TypeConverter.java\n+++ b/presto-iceberg/src/main/java/com/facebook/presto/iceberg/TypeConverter.java\n@@ -61,6 +61,7 @@\n import static com.facebook.presto.common.type.RealType.REAL;\n import static com.facebook.presto.common.type.SmallintType.SMALLINT;\n import static com.facebook.presto.common.type.TimestampType.TIMESTAMP;\n+import static com.facebook.presto.common.type.TimestampWithTimeZoneType.TIMESTAMP_WITH_TIME_ZONE;\n import static com.facebook.presto.common.type.TinyintType.TINYINT;\n import static com.facebook.presto.common.type.VarbinaryType.VARBINARY;\n import static com.facebook.presto.hive.HiveType.HIVE_BINARY;\n@@ -118,6 +119,10 @@ public static Type toPrestoType(org.apache.iceberg.types.Type type, TypeManager\n             case TIME:\n                 return TimeType.TIME;\n             case TIMESTAMP:\n+                Types.TimestampType timestampType = (Types.TimestampType) type.asPrimitiveType();\n+                if (timestampType.shouldAdjustToUTC()) {\n+                    return TIMESTAMP_WITH_TIME_ZONE;\n+                }\n                 return TimestampType.TIMESTAMP;\n             case STRING:\n                 return VarcharType.createUnboundedVarcharType();\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/Decoders.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/Decoders.java\nindex e51cf0c00fdea..235a42a522c17 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/Decoders.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/Decoders.java\n@@ -62,6 +62,7 @@\n import org.apache.parquet.bytes.ByteBufferInputStream;\n import org.apache.parquet.column.ColumnDescriptor;\n import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;\n \n import java.io.ByteArrayInputStream;\n@@ -128,10 +129,14 @@ private static ValuesDecoder createValuesDecoder(ColumnDescriptor columnDescript\n                 case FLOAT:\n                     return new Int32PlainValuesDecoder(buffer, offset, length);\n                 case INT64: {\n-                    if (isTimeStampMicrosType(columnDescriptor) || isTimeMicrosType(columnDescriptor)) {\n+                    if (isTimeStampMicrosType(columnDescriptor)) {\n+                        LogicalTypeAnnotation.TimestampLogicalTypeAnnotation typeAnnotation = (LogicalTypeAnnotation.TimestampLogicalTypeAnnotation) columnDescriptor.getPrimitiveType().getLogicalTypeAnnotation();\n+                        boolean withTimezone = typeAnnotation.isAdjustedToUTC();\n+                        return new Int64TimeAndTimestampMicrosPlainValuesDecoder(buffer, offset, length, withTimezone);\n+                    }\n+                    if (isTimeMicrosType(columnDescriptor)) {\n                         return new Int64TimeAndTimestampMicrosPlainValuesDecoder(buffer, offset, length);\n                     }\n-\n                     if (isShortDecimalType(columnDescriptor)) {\n                         return new Int64ShortDecimalPlainValuesDecoder(buffer, offset, length);\n                     }\n@@ -184,8 +189,13 @@ else if (isUuidType(columnDescriptor)) {\n                     return new Int32RLEDictionaryValuesDecoder(bitWidth, inputStream, (IntegerDictionary) dictionary);\n                 }\n                 case INT64: {\n-                    if (isTimeStampMicrosType(columnDescriptor) || isTimeMicrosType(columnDescriptor)) {\n-                        return new Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(bitWidth, inputStream, (LongDictionary) dictionary);\n+                    if (isTimeStampMicrosType(columnDescriptor)) {\n+                        LogicalTypeAnnotation.TimestampLogicalTypeAnnotation typeAnnotation = (LogicalTypeAnnotation.TimestampLogicalTypeAnnotation) columnDescriptor.getPrimitiveType().getLogicalTypeAnnotation();\n+                        boolean withTimezone = typeAnnotation.isAdjustedToUTC();\n+                        return new Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(bitWidth, inputStream, (LongDictionary) dictionary, withTimezone);\n+                    }\n+                    if (isTimeMicrosType(columnDescriptor)) {\n+                        return new Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(bitWidth, inputStream, (LongDictionary) dictionary, false);\n                     }\n                     if (isShortDecimalType(columnDescriptor)) {\n                         return new Int64RLEDictionaryValuesDecoder(bitWidth, inputStream, (LongDictionary) dictionary);\n@@ -227,10 +237,14 @@ else if (isUuidType(columnDescriptor)) {\n                     return new Int32DeltaBinaryPackedValuesDecoder(valueCount, inputStream);\n                 }\n                 case INT64: {\n-                    if (isTimeStampMicrosType(columnDescriptor) || isTimeMicrosType(columnDescriptor)) {\n-                        return new Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder(valueCount, inputStream);\n+                    if (isTimeStampMicrosType(columnDescriptor)) {\n+                        LogicalTypeAnnotation.TimestampLogicalTypeAnnotation typeAnnotation = (LogicalTypeAnnotation.TimestampLogicalTypeAnnotation) columnDescriptor.getPrimitiveType().getLogicalTypeAnnotation();\n+                        boolean withTimezone = typeAnnotation.isAdjustedToUTC();\n+                        return new Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder(valueCount, inputStream, withTimezone);\n+                    }\n+                    if (isTimeMicrosType(columnDescriptor)) {\n+                        return new Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder(valueCount, inputStream, false);\n                     }\n-\n                     if (isShortDecimalType(columnDescriptor)) {\n                         ValuesReader parquetReader = getParquetReader(encoding, columnDescriptor, valueCount, inputStream);\n                         return new Int64ShortDecimalDeltaValuesDecoder(parquetReader);\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/ValuesDecoder.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/ValuesDecoder.java\nindex b3944cfe9cd76..96a83d83c9026 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/ValuesDecoder.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/ValuesDecoder.java\n@@ -64,6 +64,11 @@ void skip(int length)\n                 throws IOException;\n     }\n \n+    interface PackFunction\n+    {\n+        long pack(long millis);\n+    }\n+\n     interface TimestampValuesDecoder\n             extends ValuesDecoder\n     {\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/delta/Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/delta/Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder.java\nindex d422c8d5ebcca..a8c7b8a6442d2 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/delta/Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/delta/Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder.java\n@@ -20,6 +20,8 @@\n \n import java.io.IOException;\n \n+import static com.facebook.presto.common.type.DateTimeEncoding.packDateTimeWithZone;\n+import static com.facebook.presto.common.type.TimeZoneKey.UTC_KEY;\n import static java.util.concurrent.TimeUnit.MICROSECONDS;\n \n /**\n@@ -34,11 +36,14 @@ public class Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder\n \n     private final DeltaBinaryPackingValuesReader innerReader;\n \n-    public Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder(int valueCount, ByteBufferInputStream bufferInputStream)\n+    private final PackFunction packFunction;\n+\n+    public Int64TimeAndTimestampMicrosDeltaBinaryPackedValuesDecoder(int valueCount, ByteBufferInputStream bufferInputStream, boolean withTimezone)\n             throws IOException\n     {\n         innerReader = new DeltaBinaryPackingValuesReader();\n         innerReader.initFromPage(valueCount, bufferInputStream);\n+        this.packFunction = withTimezone ? millis -> packDateTimeWithZone(millis, UTC_KEY) : millis -> millis;\n     }\n \n     @Override\n@@ -46,7 +51,8 @@ public void readNext(long[] values, int offset, int length)\n     {\n         int endOffset = offset + length;\n         for (int i = offset; i < endOffset; i++) {\n-            values[i] = MICROSECONDS.toMillis(innerReader.readLong());\n+            long curValue = MICROSECONDS.toMillis(innerReader.readLong());\n+            values[i] = packFunction.pack(curValue);\n         }\n     }\n \n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/plain/Int64TimeAndTimestampMicrosPlainValuesDecoder.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/plain/Int64TimeAndTimestampMicrosPlainValuesDecoder.java\nindex cc78f9e981d55..96b76d5f20878 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/plain/Int64TimeAndTimestampMicrosPlainValuesDecoder.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/plain/Int64TimeAndTimestampMicrosPlainValuesDecoder.java\n@@ -17,6 +17,8 @@\n import com.facebook.presto.parquet.batchreader.decoders.ValuesDecoder.Int64TimeAndTimestampMicrosValuesDecoder;\n import org.openjdk.jol.info.ClassLayout;\n \n+import static com.facebook.presto.common.type.DateTimeEncoding.packDateTimeWithZone;\n+import static com.facebook.presto.common.type.TimeZoneKey.UTC_KEY;\n import static com.google.common.base.Preconditions.checkArgument;\n import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.concurrent.TimeUnit.MICROSECONDS;\n@@ -31,11 +33,19 @@ public class Int64TimeAndTimestampMicrosPlainValuesDecoder\n \n     private int bufferOffset;\n \n+    private final PackFunction packFunction;\n+\n     public Int64TimeAndTimestampMicrosPlainValuesDecoder(byte[] byteBuffer, int bufferOffset, int length)\n+    {\n+        this(byteBuffer, bufferOffset, length, false);\n+    }\n+\n+    public Int64TimeAndTimestampMicrosPlainValuesDecoder(byte[] byteBuffer, int bufferOffset, int length, boolean withTimezone)\n     {\n         this.byteBuffer = byteBuffer;\n         this.bufferOffset = bufferOffset;\n         this.bufferEnd = bufferOffset + length;\n+        this.packFunction = withTimezone ? millis -> packDateTimeWithZone(millis, UTC_KEY) : millis -> millis;\n     }\n \n     @Override\n@@ -49,10 +59,10 @@ public void readNext(long[] values, int offset, int length)\n         int localBufferOffset = bufferOffset;\n \n         while (offset < endOffset) {\n-            values[offset++] = MICROSECONDS.toMillis(BytesUtils.getLong(localByteBuffer, localBufferOffset));\n+            long valueMillis = MICROSECONDS.toMillis(BytesUtils.getLong(localByteBuffer, localBufferOffset));\n+            values[offset++] = packFunction.pack(valueMillis);\n             localBufferOffset += 8;\n         }\n-\n         bufferOffset = localBufferOffset;\n     }\n \n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/rle/Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/rle/Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder.java\nindex e628a08292a9b..c94d88f896805 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/rle/Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/batchreader/decoders/rle/Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder.java\n@@ -21,6 +21,8 @@\n import java.io.IOException;\n import java.io.InputStream;\n \n+import static com.facebook.presto.common.type.DateTimeEncoding.packDateTimeWithZone;\n+import static com.facebook.presto.common.type.TimeZoneKey.UTC_KEY;\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n import static io.airlift.slice.SizeOf.sizeOf;\n@@ -34,10 +36,18 @@ public class Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder\n \n     private final LongDictionary dictionary;\n \n-    public Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(int bitWidth, InputStream inputStream, LongDictionary dictionary)\n+    private final PackFunction packFunction;\n+\n+    public Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(int bitWidth, InputStream inputStream, LongDictionary dictionary, boolean withTimezone)\n     {\n         super(Integer.MAX_VALUE, bitWidth, inputStream);\n         this.dictionary = dictionary;\n+        this.packFunction = withTimezone ? millis -> packDateTimeWithZone(millis, UTC_KEY) : millis -> millis;\n+    }\n+\n+    public Int64TimeAndTimestampMicrosRLEDictionaryValuesDecoder(int bitWidth, InputStream inputStream, LongDictionary dictionary)\n+    {\n+        this(bitWidth, inputStream, dictionary, false);\n     }\n \n     @Override\n@@ -60,7 +70,7 @@ public void readNext(long[] values, int offset, int length)\n                     final int rleValue = currentValue;\n                     final long rleDictionaryValue = MICROSECONDS.toMillis(dictionary.decodeToLong(rleValue));\n                     while (destinationIndex < endIndex) {\n-                        values[destinationIndex++] = rleDictionaryValue;\n+                        values[destinationIndex++] = packFunction.pack(rleDictionaryValue);\n                     }\n                     break;\n                 }\n@@ -69,7 +79,8 @@ public void readNext(long[] values, int offset, int length)\n                     final LongDictionary localDictionary = dictionary;\n                     for (int srcIndex = currentBuffer.length - currentCount; destinationIndex < endIndex; srcIndex++) {\n                         long dictionaryValue = localDictionary.decodeToLong(localBuffer[srcIndex]);\n-                        values[destinationIndex++] = MICROSECONDS.toMillis(dictionaryValue);\n+                        long millisValue = MICROSECONDS.toMillis(dictionaryValue);\n+                        values[destinationIndex++] = packFunction.pack(millisValue);\n                     }\n                     break;\n                 }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\nindex a6b152381622c..0204878bf0d00 100644\n--- a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/ParquetWriters.java\n@@ -29,6 +29,7 @@\n import com.facebook.presto.parquet.writer.valuewriter.RealValueWriter;\n import com.facebook.presto.parquet.writer.valuewriter.TimeValueWriter;\n import com.facebook.presto.parquet.writer.valuewriter.TimestampValueWriter;\n+import com.facebook.presto.parquet.writer.valuewriter.TimestampWithTimezoneValueWriter;\n import com.facebook.presto.parquet.writer.valuewriter.UuidValuesWriter;\n import com.facebook.presto.spi.PrestoException;\n import com.google.common.collect.ImmutableList;\n@@ -57,6 +58,7 @@\n import static com.facebook.presto.common.type.SmallintType.SMALLINT;\n import static com.facebook.presto.common.type.TimeType.TIME;\n import static com.facebook.presto.common.type.TimestampType.TIMESTAMP;\n+import static com.facebook.presto.common.type.TimestampWithTimeZoneType.TIMESTAMP_WITH_TIME_ZONE;\n import static com.facebook.presto.common.type.TinyintType.TINYINT;\n import static com.facebook.presto.common.type.UuidType.UUID;\n import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n@@ -211,6 +213,9 @@ private static PrimitiveValueWriter getValueWriter(ValuesWriter valuesWriter, Ty\n         if (TIMESTAMP.equals(type)) {\n             return new TimestampValueWriter(valuesWriter, type, parquetType);\n         }\n+        if (TIMESTAMP_WITH_TIME_ZONE.equals(type)) {\n+            return new TimestampWithTimezoneValueWriter(valuesWriter, type, parquetType);\n+        }\n         if (TIME.equals(type)) {\n             return new TimeValueWriter(valuesWriter, type, parquetType);\n         }\n\ndiff --git a/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java\nnew file mode 100644\nindex 0000000000000..358310a333ae9\n--- /dev/null\n+++ b/presto-parquet/src/main/java/com/facebook/presto/parquet/writer/valuewriter/TimestampWithTimezoneValueWriter.java\n@@ -0,0 +1,51 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.parquet.writer.valuewriter;\n+\n+import com.facebook.presto.common.block.Block;\n+import com.facebook.presto.common.type.Type;\n+import org.apache.parquet.column.values.ValuesWriter;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+import static com.facebook.presto.common.type.DateTimeEncoding.unpackMillisUtc;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+\n+public class TimestampWithTimezoneValueWriter\n+        extends PrimitiveValueWriter\n+{\n+    private final Type type;\n+    private final boolean writeMicroseconds;\n+\n+    public TimestampWithTimezoneValueWriter(ValuesWriter valuesWriter, Type type, PrimitiveType parquetType)\n+    {\n+        super(parquetType, valuesWriter);\n+        this.type = requireNonNull(type, \"type is null\");\n+        this.writeMicroseconds = parquetType.isPrimitive() && parquetType.getOriginalType() == OriginalType.TIMESTAMP_MICROS;\n+    }\n+\n+    @Override\n+    public void write(Block block)\n+    {\n+        for (int i = 0; i < block.getPositionCount(); i++) {\n+            if (!block.isNull(i)) {\n+                long value = unpackMillisUtc(type.getLong(block, i));\n+                long scaledValue = writeMicroseconds ? MILLISECONDS.toMicros(value) : value;\n+                getValueWriter().writeLong(scaledValue);\n+                getStatistics().updateStats(scaledValue);\n+            }\n+        }\n+    }\n+}\n",
    "test_patch": "diff --git a/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java b/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\nindex 8afaded8b96e3..e8c88af69a7df 100644\n--- a/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\n+++ b/presto-hive/src/test/java/com/facebook/presto/hive/parquet/AbstractTestParquetReader.java\n@@ -43,9 +43,10 @@\n import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n import org.apache.parquet.format.Statistics;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.LogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.MessageTypeParser;\n import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Types;\n import org.joda.time.DateTimeZone;\n import org.testng.annotations.BeforeClass;\n import org.testng.annotations.Test;\n@@ -999,8 +1000,12 @@ public void testDecimalBackedByINT32()\n     public void testTimestampMicrosBackedByINT64()\n             throws Exception\n     {\n-        org.apache.parquet.schema.MessageType parquetSchema =\n-                MessageTypeParser.parseMessageType(\"message ts_micros { optional INT64 test (TIMESTAMP_MICROS); }\");\n+        LogicalTypeAnnotation annotation = LogicalTypeAnnotation.timestampType(false, LogicalTypeAnnotation.TimeUnit.MICROS);\n+        MessageType parquetSchema = Types.buildMessage()\n+                .primitive(PrimitiveType.PrimitiveTypeName.INT64, OPTIONAL)\n+                .as(annotation)\n+                .named(\"test\")\n+                .named(\"ts_micros\");\n         ContiguousSet<Long> longValues = longsBetween(1_000_000, 1_001_000);\n         ImmutableList.Builder<SqlTimestamp> expectedValues = new ImmutableList.Builder<>();\n         for (Long value : longValues) {\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\nindex 074a36e43ebb8..a308202869338 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/IcebergDistributedSmokeTestBase.java\n@@ -89,11 +89,11 @@ public void testTimestamp()\n     @Test\n     public void testTimestampWithTimeZone()\n     {\n-        assertQueryFails(\"CREATE TABLE test_timestamp_with_timezone (x timestamp with time zone)\", \"Iceberg column type timestamptz is not supported\");\n-        assertQueryFails(\"CREATE TABLE test_timestamp_with_timezone (x) AS SELECT TIMESTAMP '1969-12-01 00:00:00.000000 UTC'\", \"Iceberg column type timestamptz is not supported\");\n-        assertUpdate(\"CREATE TABLE test_timestamp_with_timezone (x timestamp)\");\n-        assertQueryFails(\"ALTER TABLE test_timestamp_with_timezone ADD COLUMN y timestamp with time zone\", \"Iceberg column type timestamptz is not supported\");\n+        assertQuerySucceeds(\"CREATE TABLE test_timestamp_with_timezone (x) AS SELECT TIMESTAMP '1969-12-01 00:00:00.000000 UTC'\");\n+        assertQuerySucceeds(\"ALTER TABLE test_timestamp_with_timezone ADD COLUMN y timestamp with time zone\");\n         dropTable(getSession(), \"test_timestamp_with_timezone\");\n+\n+        assertQueryFails(\"CREATE TABLE test_timestamp_with_timezone (x) WITH ( format = 'ORC') AS SELECT TIMESTAMP '1969-12-01 00:00:00.000000 UTC'\", \"Unsupported Type: timestamp with time zone\");\n     }\n \n     @Test\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTypes.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTypes.java\nnew file mode 100644\nindex 0000000000000..28265c9289c9b\n--- /dev/null\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestIcebergTypes.java\n@@ -0,0 +1,121 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.Session;\n+import com.facebook.presto.common.type.TimestampType;\n+import com.facebook.presto.common.type.TimestampWithTimeZoneType;\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.testing.MaterializedResult;\n+import com.facebook.presto.testing.MaterializedRow;\n+import com.facebook.presto.testing.QueryRunner;\n+import com.facebook.presto.tests.AbstractTestQueryFramework;\n+import com.google.common.collect.ImmutableMap;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.util.List;\n+\n+import static com.facebook.presto.hive.HiveCommonSessionProperties.PARQUET_BATCH_READ_OPTIMIZATION_ENABLED;\n+import static com.facebook.presto.iceberg.IcebergQueryRunner.createIcebergQueryRunner;\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestIcebergTypes\n+        extends AbstractTestQueryFramework\n+{\n+    protected QueryRunner createQueryRunner() throws Exception\n+    {\n+        return createIcebergQueryRunner(ImmutableMap.of(), ImmutableMap.of());\n+    }\n+\n+    @DataProvider(name = \"testTimestampWithTimezone\")\n+    public Object[][] createTestTimestampWithTimezoneData()\n+    {\n+        return new Object[][] {\n+                {Session.builder(getSession())\n+                        .setCatalogSessionProperty(\"iceberg\", PARQUET_BATCH_READ_OPTIMIZATION_ENABLED, \"true\")\n+                        .build()},\n+                {Session.builder(getSession())\n+                        .setCatalogSessionProperty(\"iceberg\", PARQUET_BATCH_READ_OPTIMIZATION_ENABLED, \"false\")\n+                        .build()}\n+        };\n+    }\n+\n+    @Test(dataProvider = \"testTimestampWithTimezone\")\n+    public void testTimestampWithTimezone(Session session)\n+    {\n+        QueryRunner runner = getQueryRunner();\n+        String timestamptz = \"TIMESTAMP '1984-12-08 00:10:00 America/Los_Angeles'\";\n+        String timestamp = \"TIMESTAMP '1984-12-08 00:10:00'\";\n+\n+        dropTableIfExists(runner, session.getCatalog().get(), session.getSchema().get(), \"test_timestamptz\");\n+        assertQuerySucceeds(session, \"CREATE TABLE test_timestamptz(a TIMESTAMP WITH TIME ZONE, b TIMESTAMP, c TIMESTAMP WITH TIME ZONE)\");\n+\n+        String row = \"(\" + timestamptz + \", \" + timestamp + \", \" + timestamptz + \")\";\n+        for (int i = 0; i < 10; i++) {\n+            assertUpdate(session, \"INSERT INTO test_timestamptz values \" + row, 1);\n+        }\n+\n+        MaterializedResult initialRows = runner.execute(session, \"SELECT * FROM test_timestamptz\");\n+\n+        List<Type> types = initialRows.getTypes();\n+        assertTrue(types.get(0) instanceof TimestampWithTimeZoneType);\n+        assertTrue(types.get(1) instanceof TimestampType);\n+\n+        List<MaterializedRow> rows = initialRows.getMaterializedRows();\n+        for (int i = 0; i < 10; i++) {\n+            assertEquals(\"[1984-12-08T08:10Z[UTC], 1984-12-08T00:10, 1984-12-08T08:10Z[UTC]]\", rows.get(i).toString());\n+        }\n+\n+        dropTableIfExists(runner, session.getCatalog().get(), session.getSchema().get(), \"test_timestamptz_partition\");\n+        assertQuerySucceeds(session, \"CREATE TABLE test_timestamptz_partition(a TIMESTAMP WITH TIME ZONE, b TIMESTAMP, c TIMESTAMP WITH TIME ZONE) \" +\n+                \"WITH (PARTITIONING = ARRAY['b'])\");\n+        assertUpdate(session, \"INSERT INTO test_timestamptz_partition (a, b, c) SELECT a, b, c FROM test_timestamptz\", 10);\n+\n+        MaterializedResult partitionRows = runner.execute(session, \"SELECT * FROM test_timestamptz\");\n+\n+        List<Type> partitionTypes = partitionRows.getTypes();\n+        assertTrue(partitionTypes.get(0) instanceof TimestampWithTimeZoneType);\n+        assertTrue(partitionTypes.get(1) instanceof TimestampType);\n+\n+        rows = partitionRows.getMaterializedRows();\n+        for (int i = 0; i < 10; i++) {\n+            assertEquals(\"[1984-12-08T08:10Z[UTC], 1984-12-08T00:10, 1984-12-08T08:10Z[UTC]]\", rows.get(i).toString());\n+        }\n+\n+        String earlyTimestamptz = \"TIMESTAMP '1980-12-08 00:10:00 America/Los_Angeles'\";\n+        dropTableIfExists(runner, session.getCatalog().get(), session.getSchema().get(), \"test_timestamptz_filter\");\n+        assertQuerySucceeds(session, \"CREATE TABLE test_timestamptz_filter(a TIMESTAMP WITH TIME ZONE)\");\n+\n+        for (int i = 0; i < 5; i++) {\n+            assertUpdate(session, \"INSERT INTO test_timestamptz_filter VALUES (\" + earlyTimestamptz + \")\", 1);\n+        }\n+        for (int i = 0; i < 5; i++) {\n+            assertUpdate(session, \"INSERT INTO test_timestamptz_filter VALUES (\" + timestamptz + \")\", 1);\n+        }\n+\n+        MaterializedResult lateRows = runner.execute(session, \"SELECT a FROM test_timestamptz_filter WHERE a > \" + earlyTimestamptz);\n+        assertEquals(lateRows.getMaterializedRows().size(), 5);\n+\n+        MaterializedResult lateRowsFromEquals = runner.execute(session, \"SELECT a FROM test_timestamptz_filter WHERE a = \" + timestamptz);\n+        com.facebook.presto.testing.assertions.Assert.assertEquals(lateRows, lateRowsFromEquals);\n+\n+        MaterializedResult earlyRows = runner.execute(session, \"SELECT a FROM test_timestamptz_filter WHERE a < \" + timestamptz);\n+        assertEquals(earlyRows.getMaterializedRows().size(), 5);\n+\n+        MaterializedResult earlyRowsFromEquals = runner.execute(session, \"SELECT a FROM test_timestamptz_filter WHERE a = \" + earlyTimestamptz);\n+        com.facebook.presto.testing.assertions.Assert.assertEquals(earlyRows, earlyRowsFromEquals);\n+    }\n+}\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java\nindex cf0392c07df2c..a1eed697d5830 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestNestedFieldConverter.java\n@@ -38,6 +38,8 @@\n import static com.facebook.presto.common.type.DoubleType.DOUBLE;\n import static com.facebook.presto.common.type.IntegerType.INTEGER;\n import static com.facebook.presto.common.type.RealType.REAL;\n+import static com.facebook.presto.common.type.TimestampType.TIMESTAMP;\n+import static com.facebook.presto.common.type.TimestampWithTimeZoneType.TIMESTAMP_WITH_TIME_ZONE;\n import static com.facebook.presto.common.type.VarbinaryType.VARBINARY;\n import static com.facebook.presto.common.type.VarcharType.createUnboundedVarcharType;\n import static com.facebook.presto.iceberg.NestedFieldConverter.toIcebergNestedField;\n@@ -176,6 +178,12 @@ protected static PrestoIcebergNestedField prestoIcebergNestedField(\n             case \"date\":\n                 prestoType = DATE;\n                 break;\n+            case \"timestamp\":\n+                prestoType = TIMESTAMP;\n+                break;\n+            case \"timestamptz\":\n+                prestoType = TIMESTAMP_WITH_TIME_ZONE;\n+                break;\n             case \"nested\":\n                 prestoType = RowType.from(ImmutableList.of(\n                         RowType.field(\"int\", INTEGER),\n@@ -239,6 +247,12 @@ protected static Types.NestedField nestedField(int id, String name)\n             case \"date\":\n                 icebergType = Types.DateType.get();\n                 break;\n+            case \"timestamp\":\n+                icebergType = Types.TimestampType.withoutZone();\n+                break;\n+            case \"timestamptz\":\n+                icebergType = Types.TimestampType.withZone();\n+                break;\n             case \"nested\":\n                 icebergType = nested();\n                 break;\n\ndiff --git a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java\nindex adbbde80977af..8d276765c7441 100644\n--- a/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java\n+++ b/presto-iceberg/src/test/java/com/facebook/presto/iceberg/TestSchemaConverter.java\n@@ -92,7 +92,9 @@ protected static PrestoIcebergSchema prestoIcebergSchema(TypeManager typeManager\n                 prestoIcebergNestedField(9, \"varchar\", typeManager),\n                 prestoIcebergNestedField(10, \"varbinary\", typeManager),\n                 prestoIcebergNestedField(11, \"row\", typeManager),\n-                prestoIcebergNestedField(12, \"date\", typeManager)));\n+                prestoIcebergNestedField(12, \"date\", typeManager),\n+                prestoIcebergNestedField(13, \"timestamp\", typeManager),\n+                prestoIcebergNestedField(14, \"timestamptz\", typeManager)));\n \n         Map<String, Integer> columnNameToIdMapping = getColumnNameToIdMapping();\n \n@@ -114,11 +116,13 @@ private static Map<String, Integer> getColumnNameToIdMapping()\n         columnNameToIdMapping.put(\"varbinary\", 10);\n         columnNameToIdMapping.put(\"row\", 11);\n         columnNameToIdMapping.put(\"date\", 12);\n-        columnNameToIdMapping.put(\"array.element\", 13);\n-        columnNameToIdMapping.put(\"map.key\", 14);\n-        columnNameToIdMapping.put(\"map.value\", 15);\n-        columnNameToIdMapping.put(\"row.int\", 16);\n-        columnNameToIdMapping.put(\"row.varchar\", 17);\n+        columnNameToIdMapping.put(\"timestamp\", 13);\n+        columnNameToIdMapping.put(\"timestamptz\", 14);\n+        columnNameToIdMapping.put(\"array.element\", 15);\n+        columnNameToIdMapping.put(\"map.key\", 16);\n+        columnNameToIdMapping.put(\"map.value\", 17);\n+        columnNameToIdMapping.put(\"row.int\", 18);\n+        columnNameToIdMapping.put(\"row.varchar\", 19);\n \n         return columnNameToIdMapping;\n     }\n@@ -137,7 +141,9 @@ protected static Schema schema()\n                 nestedField(9, \"varchar\"),\n                 nestedField(10, \"varbinary\"),\n                 nestedField(11, \"row\"),\n-                nestedField(12, \"date\")));\n+                nestedField(12, \"date\"),\n+                nestedField(13, \"timestamp\"),\n+                nestedField(14, \"timestamptz\")));\n \n         Type schemaAsStruct = Types.StructType.of(fields);\n         AtomicInteger nextFieldId = new AtomicInteger(1);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  }
]